<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">KQA Pro: A Large Diagnostic Dataset for Complex Question Answering over Knowledge Base</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-07-08">8 Jul 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiaxin</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shulin</forename><surname>Cao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liangming</forename><surname>Pan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yutong</forename><surname>Xiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lei</forename><surname>Hou</surname></persName>
							<email>houlei@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
							<email>lijuanzi@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
							<email>hanwangzhang@ntu.edu.sg</email>
							<affiliation key="aff2">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bin</forename><surname>He</surname></persName>
							<affiliation key="aff3">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kobe</forename><surname>Bryant</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Vanessa</forename><forename type="middle">Laine</forename><surname>Bryant</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yao</forename><surname>Ming</surname></persName>
						</author>
						<title level="a" type="main">KQA Pro: A Large Diagnostic Dataset for Complex Question Answering over Knowledge Base</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-07-08">8 Jul 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2007.03875v1[cs.CL]</idno>
					<note type="submission">height: 198 centimetre; mass: 96 kilogram; date of birth: 23 August 1978; date of death: 26 January 2020; occupation: basketball player; date of birth: 5 May 1982; family name: Bryant; 12 September 1980; occupation: basketball player, entrepreneur; Shanghai population: 23,390,000 population: 24,152,700 point in time: 2016 point in time: 2015 place of birth United States of America inception: 4 July 1776 short name: USA motto text: In God We Trust population: 325,145,963 point in time: 2015 country of citizenship start time: 2001 end time: 2020</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Complex question answering over knowledge base (Complex KBQA) is challenging because it requires the compositional reasoning capability. Existing benchmarks have three shortcomings that limit the development of Complex KBQA: 1) they only provide QA pairs without explicit reasoning processes; 2) questions are either generated by templates, leading to poor diversity, or on a small scale; and 3) they mostly only consider the relations among entities but not attributes. To this end, we introduce KQA Pro, a large-scale dataset for Complex KBQA. We generate questions, SPARQLs, and functional programs with recursive templates and then paraphrase the questions by crowdsourcing, giving rise to around 120K diverse instances. The SPARQLs and programs depict the reasoning processes in various manners, which can benefit a large spectrum of QA methods. We contribute a unified codebase and conduct extensive evaluations for baselines and state-of-the-arts: a blind GRU obtains 31.58%, the best model achieves only 35.15%, and humans top at 97.5%, which offers great research potential to fill the gap.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Thanks to the recent advances in deep models, especially large-scale unsupervised representation learning <ref type="bibr" target="#b6">[7]</ref>, question answering of simple questions over knowledge base (KBQA), i.e., singlerelation factoid questions <ref type="bibr" target="#b4">[5]</ref>, begin to saturate <ref type="bibr">[24; 35; 16]</ref>. However, tackling complex questions (Complex KBQA) is still an ongoing challenge, due to the unsatisfied capability of compositional reasoning. To promote the community development, several benchmarks are proposed for Complex KBQA, including LC-QuAD2.0 <ref type="bibr" target="#b10">[11]</ref>, ComplexWebQuestions <ref type="bibr" target="#b31">[32]</ref>, MetaQA <ref type="bibr" target="#b37">[38]</ref>, CSQA <ref type="bibr" target="#b25">[26]</ref>, and so on. However, they suffer from the following shortcomings: 1) Most of them only provide QA pairs without explicit reasoning processes, making it challenging for models to learn the compositional reasoning. Some researchers try to learn the reasoning processes with reinforcement learning <ref type="bibr">[21; 27; 2]</ref> and searching <ref type="bibr" target="#b12">[13]</ref>. However, the prohibitively huge search space hinders both the performance and speed, especially when the question complexity increases. For example, Saha et al. <ref type="bibr" target="#b26">[27]</ref> achieved a 96.52% F1 score on simple questions in CSQA, whereas only 0.33% on complex questions that require comparative count. We believe that an intermediate supervision is needed for learning the compositional reasoning, mimicking the learning process of human beings <ref type="bibr" target="#b14">[15]</ref>.</p><p>2) Questions are weak in diversity and scale. For example, MetaQA <ref type="bibr" target="#b37">[38]</ref> contains about 400K instances, but the questions are generated using just 36 templates; LC-QuAD2.0 <ref type="bibr" target="#b10">[11]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Knowledge Base</head><p>Figure <ref type="figure">1</ref>: Example of our knowledge base and questions. Our knowledge base includes multiple types of knowledge, making the knowledge comprehension more challenging. Our questions are paired with executable SPARQL queries and functional programs.</p><p>plexWebQuestions <ref type="bibr" target="#b31">[32]</ref> have human-written questions that are fluent and diverse, but the scale is less than 40K, which is far from satisfactory.</p><p>3) Most existing datasets only consider the relations among entities, ignoring the attributes that also contain rich information, such as the height of athlete and the area of country. As a result, systems developed based on them are limited in application and even not able to handle questions like How tall is Kobe Bryant? and Kobe Bryant and Yao Ming, who is taller?.</p><p>To address the above problems, we create KQA Pro, a large-scale benchmark of Complex KBQA. In KQA Pro, questions are firstly generated by templates and then paraphrased by crowdsourcing. We design a template kit to generate textual questions, SPARQLs, and programs at the same time. The programs are composed of reusable functions, which define basic operations on the knowledge base, such as Find, Relate, and Query in Fig. <ref type="figure">1</ref>. By composing these functions into programs, we can model the compositional reasoning process of any complex question. To get diverse and complex questions in a scalable way, we propose a recursive generation strategy, in which we can extend one question to many by recursively unfolding the entities (e.g., replacing "Kobe Bryant" with "the spouse of Vanessa Laine Bryant" to get Question 2 in Fig. <ref type="figure">1</ref>). KQA Pro takes a dense subset of Wikidata <ref type="bibr" target="#b33">[34]</ref> as the knowledge base and preserves all its knowledge of different types, including relational (e.g., spouse), literal (e.g., height), and high-level (e.g., the starting time of a marriage relationship).</p><p>Despite the additional supervision of SPARQLs and programs, KQA Pro is still very challenging. KQA Pro questions involve varied reasoning skills, including multi-hop reasoning, comparisons between quantity values and between time values, and set operations. The knowledge base contains many ambiguous entities that have the same name, which makes it difficult for the model to understand the questions correctly, especially when they are in natural language. We build a unified codebase to implement baselines and state-of-the-art models of Complex KBQA. It is demonstrated that even with the extra information of SPARQLs and programs, the best model can only achieve 35.15%, revealing a large gap from the human performance, 97.5%.</p><p>2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Simple KBQA</head><p>The simple KBQA task is also named as factoid question answering. A simple question over knowledge base usually consists of a subject entity and a single relation, which can be mapped into the knowledge base and then retrieve the answer entity or value directly. Widely-used datasets of simple KBQA include WebQuestions <ref type="bibr" target="#b2">[3]</ref>, SimpleQuestions <ref type="bibr" target="#b4">[5]</ref>, WikiMovies <ref type="bibr" target="#b21">[22]</ref>, FreebaseQA <ref type="bibr" target="#b17">[18]</ref>, and etc. <ref type="bibr" target="#b23">[24]</ref> analyze the upper bound of the SimpleQuestions dataset and introduce a strong baseline, claiming that the dataset is nearly solved. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Complex KBQA</head><p>Compared with simple questions, complex questions usually require the reasoning capability, including multi-hop inference, quantitative comparison, set operation, and etc. Widely-used datasets of Complex KBQA include LC-QuAD2.0 <ref type="bibr" target="#b10">[11]</ref>, ComplexWebQuestions <ref type="bibr" target="#b31">[32]</ref>, MetaQA <ref type="bibr" target="#b37">[38]</ref>, and CSQA <ref type="bibr" target="#b25">[26]</ref>. Table <ref type="table" target="#tab_1">1</ref> lists these datasets and their features. They all lack the intermediate reasoning processes, which we believe are very important for machines to learn the compositional reasoning. LC-QuAD2.0 and ComplexWebQuestions have natural questions and SPARQLs. However, their scale is less than 40K, which is far from satisfactory. Compared with them, our KQA Pro serves as a more well-rounded benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Complex QA in other domains</head><p>Complex QA that requires the reasoning capability has been a hot research topic recently. In the visual domain, based on the large-scale datasets CLEVR <ref type="bibr" target="#b18">[19]</ref> and GQA <ref type="bibr" target="#b16">[17]</ref>, which both provide the functional programs as additional supervision, many models that push the boundaries of vision and language reasoning are proposed <ref type="bibr">[1; 29]</ref>. In the textual domain, the widely-used DROP <ref type="bibr" target="#b9">[10]</ref> and HOTPOTQA <ref type="bibr" target="#b35">[36]</ref> provide multi-hop QA pairs but lack annotations of the reasoning process.</p><p>Existing models learn implicit reasoning based on graph neural networks <ref type="bibr">[8; 12]</ref> or heuristics <ref type="bibr" target="#b13">[14]</ref>.</p><p>In this paper, we focus on the domain of structural knowledge base.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data Collection</head><p>To build KQA Pro dataset, we first extract a dense subset from Wikidata <ref type="bibr" target="#b33">[34]</ref> as our knowledge base, then generate questions with proposed recursive templates, and finally rewrite questions into natural language with the help of crowdsourcing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Knowledge Base Selection</head><p>Popular knowledge bases such as Freebase <ref type="bibr" target="#b3">[4]</ref> and Wikidata <ref type="bibr" target="#b33">[34]</ref> consist of millions of entities and triples. If we directly take them as our knowledge base, most of entities will be never used in our questions and the huge amount of knowledge may cause both time and space issues. So we extracted a subset of Wikidata as our knowledge base.</p><p>Specifically, we took the entities of FB15k-237 <ref type="bibr" target="#b32">[33]</ref>, a popular subset of Freebase, as seed, and then aligned them with Wikidata via Freebase IDs<ref type="foot" target="#foot_0">1</ref> , so that we could extract their rich literal attributes (e.g., height) and high-level knowledge (e.g., the starting time of a marriage relationship) from Wikidata. Besides, we added other 3,000 entities with the same name as one of FB15k-237 entities, to further increase the difficulty of disambiguation. For the relational knowledge (e.g., spouse), we manually merged the predicates of FB15k-237 (e.g., /people/person/spouse_s./people/marriage/spouse) and Wikidata (e.g., spouse), obtaining 363 predicates totally.</p><p>We used two special predicates of Wikidata, instance of and subclass of, to distinguish concepts (e.g., athlete) from entities (e.g., Yaoming). All entities that appeared as object of instance of, or either subject or object of subclass of, were regarded as concepts. We built the hierarchical structure for concepts based on subclass of triples (e.g., basketball player is a child concept of athlete). Entities were ensured to have at least one direct concept.</p><p>Finally, we filtered out some useless attributes about images, Wikidata pages, and etc. The statistics of our final knowledge base is listed in Table <ref type="table" target="#tab_2">2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Question Generation</head><p>To generate compositional complex questions in a scalable manner, we sample from simple operations, which correspond to functions, and then compose them into complex questions, which correspond to programs. The snippets of textual questions, SPARQLs, and programs are sampled and assembled together, guaranteeing that they have the same semantics.</p><p>Specifically, our generation strategy consists of two stages: locating and asking. In the first stage, we locate a single entity or an entity set in a recursive way (e.g., basketball players whose height is greater than 190 centimetres). In the second stage, we ask something about the target entity or entities (e.g., how many).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Locating Stage</head><p>We propose several strategies to locate an entity or an entity set:</p><p>• Entity name: It locates a single entity. For example, Yaoming.</p><p>• Concept name: It locates an entity set belonging to the given concept. For example, basketball players. • Concept and literal condition: It selects the entity or entities that belong to the given concept and satisfy the given literal condition, which may include quantitative comparisons, temporal comparisons, and high-level restrictions. For example, the cities whose population is greater than 23,000,000 in 2016. It can also locate a single entity if there is only one entity in KB satisfying the condition. An entity belongs to all its ancestor concepts, so a high-level concept like athlete will locate more entities than a low-level one like basketball player. • Concept and relational condition: It selects the entity or entities that belong to the given concept and satisfy the given relational condition, which may include high-level restrictions. For example, the basketball players who were born in Shanghai. • Recursive multi-hop condition: It replaces the entity of a relational condition with a more detailed description, which should uniquely locate the certain entity. For example, suppose Shanghai can be uniquely located by the city whose population is greater than 23,000,000 in 2016, then we can unfold it in the previous example and get a multi-hop condition: the basketball players who were born in the city whose population is greater than 23,000,000 in 2016. If we replace Shanghai with another relational condition, such as the city that is the financial center of China, then we can unfold China recursively and get description with more hops. • Intersection of two conditions: For example, the basketball players whose height is greater than 200 centimetres and less than 210 centimetres. • Union of two conditions: For example, the basketball players who were born in Shanghai or New York.</p><p>With these strategies, we can either randomly generate a condition to locate entities aimlessly, or find a condition that uniquely locates a target entity by going through its relevant knowledge.</p><p>For quantitative comparisons, we support 4 operations: equal, not equal, greater than, and less than.</p><p>For convenience, we only support comparisons between values with the same unit. For temporal comparison, we also support 4 operations: in (e.g., 01/01/1990 in 1990), not in, before, and after. We ignore the time and only consider the date and the year. For high-level restriction, we regard it as an additional literal condition (e.g., the point in time of the population statement is 2016) and use it to narrow the entity range (e.g., by specifying the time when one had multiple spouses).</p><p>We manually annotate textual templates for each attribute (e.g., height) and each predicate (e.g., spouse) to make sure the description is fluent, so that workers can understand the questions correctly when paraphrasing. For example, for the predicate member of, we create two templates, S is the member of O and O has a member S, to describe the triple (S, member of, O) forward and backward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Asking Stage</head><p>After locating an entity or an entity set, we construct the question by asking something about it. We design 9 question types:</p><p>• QueryName: Asking the entity name. The template<ref type="foot" target="#foot_1">2</ref> is Who/What is &lt;E&gt;?, where &lt;E&gt; is a description which locates a unique entity. For example, Who is the human that is the spouse of Kobe Bryant?.</p><p>• Count: Asking the account of an entity set. The template is How many &lt;E&gt;?, where &lt;E&gt; is a description of an entity set. For example, How many basketball players whose height is greater than 200 centimetres?.</p><p>• SelectAmong: Selecting the entity with the maximum or minimum attribute value from an entity set. For example, Among basketball players whose height is greater than 200 centimetres, which one has the largest mass?.</p><p>• SelectBetween: Selecting the entity with the larger or smaller attribute value from two entities. For example, Which one has the larger mass, Kobe Bryant or Yaoming?. • QueryAttribute: Asking the attribute value. High-level restrictions are randomly added in parentheses to disambiguate the answer. For example, For Shanghai, what is its population (the point in time is 2016)?.</p><p>• Verify: Asking whether the attribute value satisfies the condition. For example, For the human that is the spouse of Kobe Bryant, is his/her height greater than 180 centimetres?.</p><p>High-level restrictions are also randomly added. For temporal attribute, in/after/before/not in will be used as the comparison descriptions.</p><p>• HighLevelLiteral: Asking high-level knowledge about a literal fact. For example, For Shanghai, its population is 24,152,700, what is the point in time?.</p><p>• Relation: Asking the relation between two entities. For example, What is the relation from Kobe Bryant to United States of America?.</p><p>• HighLevelRelational: Asking high-level knowledge about a relational fact. For example, Kobe Bryant is the spouse of Vanessa Laine Bryant, what is the end time?.</p><p>We randomly generate lots of questions, and only preserve those whose answer is unique. For example, if Bob has multiple height values that correspond to different time points, we will drop the questions like What is the height of Bob? because it leads to more than one answers. For an entity set including multiple entities, we do not ask its member names. Instead, we ask its size or select one target from it. As a result, our dataset can easily take the accuracy as the metric, which we believe will benefit the development and comparison of models.</p><p>Besides the open-domain setting, we also support the multiple-choice setting by sampling 10 candidate answers (containing the correct one) for each question. The candidates are ensured to be as similar as possible to the correct answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">SPARQLs</head><p>To represent the high-level knowledge, we create a virtual node for those literal facts and relational facts with high-level restrictions. For example, to denote the start time of the relational fact (Kobe Bryant, spouse, Vanessa Laine Bryant), we create a node _BN which connects to the subject, the predicate, and the object with three special edges, and then add (_BN, start time, 2001) into the graph. Besides, we create a virtual node for each literal attribute to represent the value and the unit at the same time. For example, to represent the height of Yaoming we need to add (Yaoming, height, _BN), (_BN, value, 229), (_BN, unit, centimetre) into the graph. We build a SPARQL engine with Virtuoso<ref type="foot" target="#foot_2">3</ref> to execute and check our generated SPARQL queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Functions and Programs</head><p>Inspired by <ref type="bibr" target="#b18">[19]</ref> and <ref type="bibr" target="#b16">[17]</ref>, We design 27 compositional functions and describe the reasoning process of our questions by composing them into programs. Each function has two kinds of input. The textual inputs come from the question, and the functional inputs come from the output of previous functions.</p><p>Here we give two functions for examples, leaving the complete instructions in Appendix:</p><p>• Relate. It has 2 textual inputs, representing the predicate and the direction (i.e., forward or backward, meaning the output is object or subject). The functional input is a unique entity. The output is an set of entities that hold the specific relation with the input entity. For example, in Question 1 of Fig. We make sure that all of our questions are answerable and their answers are unique based on our knowledge base. We check the correctness of generated SPARQLs and programs by executing the engines (the program engine is rule-based), ensuring that 99.99% of them can obtain the gold answer. We release the template-formed questions on Amazon Mechanical Turk (AMT) and ask the workers to paraphrase them without changing the original meaning, with a cost of 0.1$ per question. The program flowchart is visualized to help workers understand the complex questions. We allow workers to mark a question as confusing if they cannot understand it or find some logical errors in it. These instances will be removed from our dataset.</p><p>After paraphrasing, we evaluate the quality by 5 other workers, with a cost of 0.02$ per annotation. They are asked to check whether the paraphrase keeps the original meaning and give an additional fluency score. A paraphrase is approved only when it is marked the same as the machine-generated one by at least 3 workers and obtains an enough fluency score. The paraphrases that have a very small edit distance with the machine question are rejected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Dataset Analysis</head><p>Our KQA Pro dataset consists of 117,970 instances. We split it to training/validation/test set by 8/1/1, resulting in three sets including 94,376/11,797/11,797 instances. Fig. <ref type="figure" target="#fig_0">2</ref> shows sunburst for first 4 words in questions. We can see that questions usually start with "what", "which", "how many", "when", "is" and "does". Frequent topics include "person", "movie", "country", "university", and etc. (a) Top 20 most occurring answers in KQApro. The most frequent one is "yes", which is the answer of about half of type Verify.</p><p>(e) Question length distribution for complex KBQA dataset. We can see that KQApro questions have a wide range of lengths and are longer on average than all others. SelectAmong accounts for the least fraction (4.6%), while others account for more or less than 10%.</p><p>Taking the questions that locate the target entity by the link from another entity or from some attribute values as multi-hop questions, they cover 73.7% of KQA Pro, much more than single-hop questions that locate the target directly by the entity name. We compare the question length distribution of different Complex KBQA datasets in Fig. <ref type="figure" target="#fig_2">3</ref>(e). We observe that on average, our KQA Pro has longer questions than others, implying that our dataset is more challenging. In KQA Pro, the average length of questions/programs/SPARQLs is 14.95/4.79/35.52 respectively. Fig. <ref type="figure" target="#fig_2">3</ref>(d) shows the program length distribution. Most of our problems (28.42%) can be solved by 4 functional steps. Some extreme complicated ones (1.24%) need more than 10 steps.</p><p>There are 24,724 unique answers in KQA Pro. We show the top 20 most frequent answers and their fractions in Fig. <ref type="figure" target="#fig_2">3</ref>(a). "yes" and "no" are the most frequent answers, because they cover all questions of type Verify. "0", "1", "2", "3", and other quantity answers are for questions of type Count, which account for 11.5% according to Fig. <ref type="figure" target="#fig_2">3(b)</ref>. About 30% answers of the test set are not visible in the training set, making the dataset more challenging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Existing models of Complex KBQA fall into 3 categories: 1) Multi-hop path searching <ref type="bibr">[38; 39; 25]</ref>, which start from a topic entity and find a sequential relation path to predict the target entity. They are limited to relational knowledge and unable to handle quantitative questions. 2) Wealy-supervised program induction [21; 13; 27; 2], which learn to decompose complex questions into compositional functions with reinforcement learning or searching. Functions have strong reasoning ability, however, the huge search space usually causes issues of running time and performance. 3) Query parsing <ref type="bibr">[9; 30; 37; 31]</ref>, which convert questions into structured queries (e.g., SPARQLs) and then feed into the query engine.</p><p>We build a unified codebase to implement state-of-the-arts of these methods, including SRN <ref type="bibr" target="#b24">[25]</ref>, the best one of multi-hop path searching method; Weakly-Supervised Program, which reimplements <ref type="bibr" target="#b26">[27]</ref> on our functions; and Supervised SPARQL, a seq2seq model with attention mechanism based on <ref type="bibr" target="#b8">[9]</ref>. Thanks to our provided ground-truth programs, we also incorporate Supervised Program, which learns the program parser in a supervised manner, to compare with the previous weaklysupervised setting. Besides, as graph neural networks have made great progress in machine reading comprehension <ref type="bibr">[8; 12]</ref>, we take RGCN, i.e., relational graph convolutional networks <ref type="bibr" target="#b27">[28]</ref>, as another state-of-the-art method. As to baselines, we use Blind GRU <ref type="bibr" target="#b5">[6]</ref>, which ignores the KB and just simply encodes the question to predict answer, and KVMemNet <ref type="bibr" target="#b21">[22]</ref>, a widely-used baseline method in KBQA task. To compare computers with Human, we sample 200 instances from the test set, and ask human experts to answer them by searching our knowledge base.</p><p>As each question of KQA Pro has a unique answer, which may be an entity name, an attribute value, a predicate name, a number, "yes", or "no", we directly use the accuracy as the evaluation metric.</p><p>Results are shown in Table <ref type="table" target="#tab_4">3</ref>. Supervised SPARQL achieves an overall 35.15% accuracy, which is the best of all machine results. However, it is still very far from the human performance, 97.5%, indicating that even with the additional information of programs and SPARQLs, KQA Pro is still very challenging. Weakly-Supervised Program cannot converge due to the huge search space (i.e., dozens of functions and thousands of function inputs), even though we have applied many tricks like grammar restrictions. SRN is not able to handle literal knowledge, so we select questions containing only entities and predicates for its training and testing, including 5004 and 649 questions respectively. RGCN achieves the best performance in Multi-hop and High-level questions, demonstrating its strong reasoning capbility. However, it is weak in quantitative questions, i.e.,multiple-choice setting Comparison and Count. For these questions, models using programs or SPARQLs perform much better, because they handle quantities explicitly.</p><p>For those methods that output a probability distribution over the answer vocabulary, we take the 10 candidate choices as a mask and get results of the multiple-choice setting. RGCN achieves the highest accuracy, 54.03%. The results of the multiple-choice setting is consistent with the open-ended setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We introduce KQA Pro, a new benchmark of Complex KBQA with following features: 1) with explicit reasoning process, including SPARQLs and programs; 2) large-scale, with about 120K natural questions; 3) with rich kinds of knowledge, including relational, literal, and high-level. We create a unified codebase to implement the baselines and state-of-the-arts of Complex KBQA. Extensive experiments reveal a huge gap between machines and humans, demonstrating that KQA Pro is very challenging. KQA Pro is the first KBQA benchmark that provides the explicit reasoning process for complex questions. We hope that these additional information can help machines develop the compositional reasoning ability and learn to tackle complex questions like a human.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>As a novel dataset, this work will benefit the research community of Complex KBQA and push the development of QA models with more powerful reasoning capability. Besides, KQA Pro can also serve as a large-scale language-to-SPARQL translation dataset. The proposed recursive generation strategy provides a scalable manner for generating multi-hop questions, and can be easily applied in other domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KQA Pro -Appendix Generation Details Locating Stage</head><p>To make the quantitative comparisons more natural, we construct some quantity grids (e.g., height of 190, 200, and etc) as an alternative of specific values (e.g., 198). We will sample from grids for a literal condition of greater than or less than (e.g., whose height is larger than 200 centimetres), while use a specific value for equal or not equal (e.g., whose height is equal to 198 centimitre).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Asking Stage</head><p>We list the complete templates of our 9 question types:</p><p>• QueryName: Who is &lt;E&gt;? for an entity belonging to the concept Human, and What is &lt;E&gt;? for others, where &lt;E&gt; is a description which locates a unique entity. For example, Who is the human that is the spouse of Kobe Bryant?. • Count: asking the account of an entity set. The template is How many &lt;E&gt;?, where &lt;E&gt; is a description of an entity set. For example, How many basketball players whose height is greater than 200 centimetres?. • SelectAmong: selecting the entity with the maximum or minimum attribute value from an entity set. The template is Among &lt;E&gt;, which one has the largest/smallest &lt;A&gt;?, where &lt;E&gt; is an entity set and &lt;A&gt; is a quantitative attribute. For example, Among basketball players whose height is greater than 200 centimetres, which one has the largest mass?. • SelectBetween: selecting the entity with the larger or smaller attribute value from two entities. The template is Which one has the larger/smaller &lt;A&gt;, &lt;E1&gt; or &lt;E2&gt;?, where &lt;E1&gt; and &lt;E2&gt; describe two unique entities respectively and &lt;A&gt; is a quantitative attribute. For example, Which one has the larger mass, Kobe Bryant or Yaoming?. • QueryAttribute: asking the attribute value. The template is For &lt;E&gt;, what is his/her/its &lt;A&gt;?, where &lt;A&gt; is an attribute key and &lt;E&gt; is an unique entity. High-level restrictions are randomly added in parentheses to disambiguate the answer. For example, For Shanghai, what is its population (the point in time is 2016)?. • Verify: asking whether the attribute value satisfies the condition. The template is For &lt;E&gt;, is his/her/its &lt;A&gt; equal to/greater than/less than/not equal to &lt;V&gt;?, where &lt;E&gt; is a unique entity, &lt;A&gt; is an attribute key, and &lt;V&gt; is a possible attribute value. For example, For the human that is the spouse of Kobe Bryant, is his/her height greater than 180 centimetres?. High-level restrictions are also randomly added. For temporal attribute, in/after/before/not in will be used as the comparison descriptions.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Functions</head><p>Before introducing our functions, we first clarify some necessary items.</p><p>• Entities: A set of entities, which can be the output or functional input of a function.</p><p>• Entity: When the entity set has a unique element, we get an Entity.</p><p>• Name: A string that denotes the name of an entity or a concept.</p><p>• Key: An attribute key, e.g., height.</p><p>• Value: An attribute value, e.g., 200 centimetres.</p><p>• Op: The comparative operation of quantitative values. It is one of {=, =, &lt;, &gt;} when comparing two values, one of {greater, less} in the function SelectBetween, and one of {largest, smallest} in the function SelectAmong. • Pred: A predicate, e.g., spouse.</p><p>• Dir: forward or backward, denoting the direction of predicate.</p><p>• Fact: A literal fact, e.g., (Yao Ming, height, 229 centimetres), or a relational fact, e.g., (Kobe Bryant, spouse, Vanessa Laine Bryant). • Qualifier: High-level knowledge is also named as qualifiers.</p><p>• QKey: A qualifier key, e.g., start time of the fact (Kobe Bryant, spouse, Vanessa Laine Bryant). • QValue: A qualifier value. Table <ref type="table" target="#tab_6">4</ref> list our functions and their explanations. Note that we define specific functions for different attribute types (i.e., string, number, date, and year), because the comparison of these types are quite different.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>More Examples</head><p>Our instances consist of 5 components: textual question, SPARQL, program, 10 choices, and gold answer. We show examples of different question types in Fig. <ref type="figure">4</ref> and Fig. <ref type="figure" target="#fig_4">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Settings</head><p>We used 300-dimensional Glove <ref type="bibr" target="#b22">[23]</ref> as pretrained word embeddings. For all models, we used the optimizer Adam <ref type="bibr" target="#b19">[20]</ref>. The learning rates were selected over {0.00001, 0.0001, 0.001, 0.001, 0.01} through grid search. The optimal learning rate is 0.00001 for SRN and 0.001 for other models.</p><p>Blind GRU. Blind GRU ignores KB and only takes original question as input. Given the word sequence of a question, we encoded it into a 1024-d vector with a 2-layer bidirectional GRU, and then fed the hidden vector into a 2-layer MLP as the classifier. We collected all candidate choices of our dataset, whose number is 81,406, as the classes.</p><p>KVMemNet. The memory cell of KVMemNet is formed as a key-value pair. Let (s, r, o) ∈ K represent a KB triple. The distributed key k ∈ R 2d is formed by concatenating the embeddings s ∈ R d and r ∈ R d of subject entity s and relation r respectively. The embedding o ∈ R d of object entity o is treated as its value v. The basic idea of KVMemNet is a multi-hop context updating process, which takes the question embedding as the initial context and updates the context with new evidence collected by attention over memory cells. The final context vector is used to predict the answer. Formally, the context vector c t for hop t is calculated as</p><formula xml:id="formula_0">c t = Linear t (c t−1 + (k,v)∈M Sof tmax(c t−1 • k)v)</formula><p>, where c 0 is computed by encoding the question with a Bi-GRU, and M is the set of memory cells.</p><p>Following <ref type="bibr" target="#b21">[22]</ref>, we pre-select a small subset of the KB for each question. Specifically, we retrieved 1,000 key-value pairs where the key shares at least one word with the question with frequency &lt; 1000 (to ignore stop words). For high-level knowledge, we concatenated the fact and the qualifier key as the memory key, e.g., "Kobe Bryant spouse Vanessa Laine Bryant start time". SRN. SRN formulates multi-relation question answering as a sequential decision problem in reinforcement learning, whose Markov decision process(MDP) is defined as a tuple (S, A, P, R) consisting of four parts: (1) State space S. State S t = (q, e s , e t , h t ) consists of four elements at each time step t. q refers to the question. e s is the topic entity of q. e t is the focused entity at time step t and e 0 = e s . h t records the action history and h 0 = ∅. (2) Action space A. At time step t, given the focused entity e t , A(S t ) contains all outgoing edges of e t in knowledge graph K, i.e., A(S t ) = {(r, e) (e t , r, e) ∈ K}. In addition, a self-loop edge is added to action A(S t ). Path search is unrolled for a fixed number of steps T , which is 3 in our experiment. (3) Transition P . The agent selects action A t = (r , e ) according to the probability distribution over candidate actions at time step t. Then, the state will be changed to S t+1 = (q, e s , e , h t+1 ) where h t+1 = h t ∪ {A t }. (4) Reward R. The agent receives a positive terminal reward 1 when it arrives at a correct answer node in the end or a negative reward 0 otherwise. In addition, a potential-based intermediate reward is supplied for reward shaping. SRN can only handle relational knowledge. It must start from a topic entity and terminate with a predicted entity. So we filter out questions that contain literal knowledge or high-level knowledge, retaining 5004 and 649 questions as its training set and test set. Specifically, we retained the questions with Find as the first function and What as the last function. The textual input of the first function Find was used as the topic entity. Sup. Program. Supervised Program adopts an attention-enhanced encoder-decoder model to parse the question into a sequence of (function, textual inputs, functional inputs). For function, we directly feed the attentional hidden vector into a classifier. For textual inputs, as there are at most 3 textual inputs for a function (shown in Table <ref type="table" target="#tab_6">4</ref>), we use 3 different classifiers to predict them. A special token "&lt;PAD&gt;" is used to pad functions with less than 3 textual inputs. Textual inputs share the same vocabulary with question words, whose size is 77,494. For functional inputs, we consider each previous function and predict whether the current function depends on it or not using a binary classifier. During inference, function pairs with a probability &gt; 0.5 will build the dependency. Note that a function has at most 2 functional inputs, so we just take the top 2 if there are multiple previous functions &gt; 0.5 with the current one.</p><p>Weak. Program. Weakly-Supervised Program use the same model with Supervised Program, but learn it with reinforcement learning instead of supervised learning. Program search is unrolled for a fixed number of steps T , which is 20 in our experiment. The agent receives a positive terminal reward 1 if an executor executes the searched program and get a correct answer. Otherwise, the terminal reward is 0. Note that the search space of Weak. Program is very huge, i.e., there are 20 steps, at each step there are 27 functions, several functional inputs, and 77,494 textual inputs. To help converge, following <ref type="bibr" target="#b26">[27]</ref>, we applied beam search, grammar restriction (i.e., sampling only feasible actions), and auxiliary reward. However, unfortunately, we found it still cannot converge. We think it is because our search space is much huger than <ref type="bibr" target="#b26">[27]</ref>. Their questions only care about relational knowledge and thus can be solved with less functions and less inputs than ours. Sup. SPARQL. An attention-based encoder-decoder model is used to parse questions into corresponding SPARQLs. Specifically, we used a 2-layer GRU to encode the input question, used another 2-layer GRU to decode hidden vectors, enhanced the hidden vector by attention over the encoder states, and finally fed it into an MLP classifier. We delimited SPARQL into sequence by blanks or tabs, leading to a vocabulary of size 45,693. RGCN. To build the graph, we take entities as nodes, take relations between them as edges, and take predicates as edge labels. We concatenate the literal attributes of an entity into a sequence as the node description. For simplicity, we ignore the high-level knowledge. Given a question, we first initialize node vectors by fusing the information of node descriptions and the question, then conduct RGCN to update the node features, and finally aggregate features to predict answer. Specifically, we take the question embedding as the attention key, take the word embeddings of a node description as the attention values, and compute the weighted sum as the initial node vector. As to the feature aggregation, we still use the attention mechanism, i.e., summing node features up with the question embedding as attention key.</p><p>Our RGCN implementation is based on DGL,<ref type="foot" target="#foot_3">4</ref> a high performance Python package for deep learning on graphs. Due to the memory limit, we set the graph layer to 1 and set the hidden dimension of nodes and edges to 32.</p><p>Question: When did the big city whose postal code is 54000 have a population of 104072? SPARQL: SELECT DISTINCT ?qpv WHERE { ?e &lt;pred:instance_of&gt; ?c . ?c &lt;pred:name&gt; "big city" . ?e &lt;postal_code&gt; ? pv_1 . ?pv_1 &lt;pred:value&gt; "54000" . ?e &lt;population&gt; ?pv . ?pv &lt;pred:unit&gt; "1" . ?pv &lt;pred:value&gt; "104072"^^xsd:double . <ref type="bibr">[</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer: 154</head><p>Question: What is the academic degree Vladimir Nabokov achieved at Trinity College (the one that is the education place of George V) ? SPARQL: SELECT DISTINCT ?qpv WHERE { ?e_1 &lt;pred:name&gt; "Vladimir Nabokov" . ?e_2 &lt;pred:name&gt; "Trinity College" . ?e_3 &lt;educated_at&gt; ?e_2 . ?e_3 &lt;pred:name&gt; "George V" . ?e_1 &lt;educated_at&gt; ?e_2 . <ref type="bibr">[</ref>  Question: Among the feature films with a publication date after 2003, which one has the smallest duration? SPARQL: SELECT ?e WHERE { ?e &lt;pred:instance_of&gt; ?c . ?c &lt;pred:name&gt; "feature film" . ?e &lt;publication_date&gt; ? pv_1 . ?pv_1 &lt;pred:year&gt; ?v_1 . FILTER <ref type="bibr">( ?v_1 &gt; 2003 )</ref> . ?e &lt;duration&gt; ?pv . ?pv &lt;pred:value&gt; ?v . } ORDER BY ?v LIMIT 1  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Program:</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Distribution of first 4 question words.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Ratio of multi-hop questions and single-hop questions. (d) Distribution of program lengths.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Question, answer, and program statistics of KQA Pro.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Alice in Wonderland; Pirates of the Caribbean: Dead Man's Chest; Wallace &amp; Gromit: The Curse of the Were-Rabbit; Bedtime Stories; Secretariat; The Sorcerer's Apprentice; Enchanted; Old Dogs; Harry Potter and the Prisoner of Azkaban; Prince of Persia: The Sands of Time Answer:Wallace &amp; Gromit: The Curse of the Were-Rabbit</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: More examples of our instance: (question, SPARQL, program, choices, answer).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>and Com-</figDesc><table><row><cell cols="4">Question 1: How many people does the Yao Ming's birth place have in</cell></row><row><cell></cell><cell>2016?</cell><cell></cell><cell></cell></row><row><cell cols="4">SPARQL: SELECT ?v WHERE { ?e_1 &lt;name&gt; "Yao Ming" . ?e_1 &lt;place_of_birth&gt; ?</cell></row><row><cell></cell><cell cols="3">e. ?e &lt;population&gt; ?v . [ &lt;fact_h&gt; ?e ; &lt;fact_r&gt; &lt;population&gt; ; &lt;fact_t&gt; ?</cell></row><row><cell></cell><cell cols="2">v ] &lt;point_in_time&gt; 2016 . }</cell><cell></cell></row><row><cell>Program:</cell><cell>Find Yao Ming</cell><cell>Relate place of birth</cell><cell>Query population point in time=2016</cell></row><row><cell>Program:</cell><cell></cell><cell>Find</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Yao Ming</cell><cell>Select</cell></row><row><cell></cell><cell></cell><cell></cell><cell>height</cell></row><row><cell></cell><cell>Bryant Vanessa Laine Find</cell><cell>spouse Relate</cell><cell>greater</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison with existing datasets of Complex KBQA.</figDesc><table><row><cell>Dataset</cell><cell>multiple kinds of knowledge</cell><cell>number of questions</cell><cell cols="3">natural language SPARQLs programs</cell></row><row><cell>LC-QuAD2.0[11]</cell><cell></cell><cell>30k</cell><cell></cell><cell></cell><cell>×</cell></row><row><cell>ComplexWebQuestions[32]</cell><cell></cell><cell>35k</cell><cell></cell><cell></cell><cell>×</cell></row><row><cell>MetaQA[38]</cell><cell>×</cell><cell>400k</cell><cell>×</cell><cell>×</cell><cell>×</cell></row><row><cell>CSQA[26]</cell><cell>×</cell><cell>1.6M</cell><cell>×</cell><cell>×</cell><cell>×</cell></row><row><cell>KQA Pro (ours)</cell><cell></cell><cell>120k</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Statistics of our knowledge base. Top section are the numbers of concepts, entities, unique entity names, predicates, and attributes. Bottom section are the numbers of different types of knowledge.</figDesc><table><row><cell># Con.</cell><cell># Ent.</cell><cell cols="3"># Name # Pred. # Attr.</cell></row><row><cell>794</cell><cell>16,960</cell><cell>14,471</cell><cell>363</cell><cell>846</cell></row><row><cell cols="5"># Relational facts # Literal facts # High-level facts</cell></row><row><cell>415,334</cell><cell></cell><cell>174,539</cell><cell></cell><cell>309,407</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>1, the function Relate(place of birth, forward) locates Shanghai from Yao Ming (the direction is omitted for simplicity in the figure). • SelectBetween. It has 2 textual inputs, the attribute key and the operation (i.e., greater or less), and 2 functional inputs, representing 2 unique entities. The output is the entity with the greater or smaller attribute value. For example, in Question 2 of Fig. 1, the function Select(height, greater) selects the taller one from Yao Ming and Kobe Bryant (function name is simplified in the figure).</figDesc><table /><note>Based on the proposed recursive templates, we can generate a large amount of instances with 5 components: (template-formed question, SPARQL, program, 10 candidate choices, gold answer).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Accuracy of different models on KQA Pro test set. We categorize the test questions based on their golden program to measure fine-grained ability of models. Specifically, Multi-hop means multi-hop questions, High-level means questions containing high-level knowledge, Comparison means quantitative or temporal comparison between two or more entities, Logical means logical union or intersection, Count means questions that ask the number of target entities, and Verify means questions that take "yes" or "no" as answer.</figDesc><table><row><cell>Model</cell><cell>Overall</cell><cell>Multi-</cell><cell>High-</cell><cell>Compari-</cell><cell>Logical</cell><cell>Count</cell><cell>Verify</cell></row><row><cell></cell><cell></cell><cell>hop</cell><cell>level</cell><cell>son</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">Open-Ended Setting</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Blind GRU</cell><cell>31.58</cell><cell>30.08</cell><cell>26.91</cell><cell>17.40</cell><cell>30.70</cell><cell>35.44</cell><cell>64.30</cell></row><row><cell>KVMemNet</cell><cell>13.61</cell><cell>14.36</cell><cell>18.11</cell><cell>0.14</cell><cell>11.75</cell><cell>21.82</cell><cell>51.04</cell></row><row><cell>RGCN</cell><cell>34.00</cell><cell>32.95</cell><cell>27.22</cell><cell>28.48</cell><cell>33.81</cell><cell>38.07</cell><cell>64.50</cell></row><row><cell>SRN</cell><cell>-</cell><cell>10.49</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Sup. Program</cell><cell>32.42</cell><cell>26.92</cell><cell>14.07</cell><cell>53.09</cell><cell>32.61</cell><cell>41.61</cell><cell>30.73</cell></row><row><cell>Weak. Program</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Sup. SPARQL</cell><cell>35.15</cell><cell>31.95</cell><cell>15.10</cell><cell>64.64</cell><cell>30.16</cell><cell>49.74</cell><cell>58.15</cell></row><row><cell></cell><cell></cell><cell cols="3">Multiple-Choice Setting</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Blind GRU</cell><cell>52.00</cell><cell>49.45</cell><cell>59.55</cell><cell>41.91</cell><cell>52.82</cell><cell>40.71</cell><cell>65.06</cell></row><row><cell>KVMemNet</cell><cell>38.33</cell><cell>36.52</cell><cell>53.70</cell><cell>18.71</cell><cell>36.60</cell><cell>26.34</cell><cell>57.87</cell></row><row><cell>RGCN</cell><cell>54.03</cell><cell>52.43</cell><cell>60.82</cell><cell>46.25</cell><cell>57.67</cell><cell>50.53</cell><cell>66.11</cell></row><row><cell></cell><cell></cell><cell cols="3">Human Performance</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Human</cell><cell>97.50</cell><cell>97.24</cell><cell>95.65</cell><cell>100.00</cell><cell>98.18</cell><cell>83.33</cell><cell>95.24</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>•</head><label></label><figDesc>HighLevelLiteral: asking high-level knowledge about a literal fact. The template is For &lt;E&gt;, his/her/its &lt;A&gt; is &lt;V&gt;, what is the &lt;H&gt;?, where &lt;E&gt; is a unique entity, &lt;A&gt; is an attribute key, &lt;V&gt; is the corresponding attribute value, and &lt;H&gt; is a high-level key. For example, For Shanghai, its population is 24,152,700, what is the point in time?.• Relation: asking the relation between two entities. The template is What is the relation from &lt;E1&gt; to &lt;E2&gt;?, where &lt;E1&gt; and &lt;E2&gt; describe two unique entities. For example, What is the relation from Kobe Bryant to United States of America?.</figDesc><table /><note>• HighLevelRelational: asking high-level knowledge about a relational fact. The template is &lt;E1&gt; is &lt;P&gt; of &lt;E2&gt;, what is the &lt;H&gt;?, where (&lt;E1&gt;, &lt;P&gt;, &lt;E2&gt;) is a triple, and &lt;H&gt; is a high-level key. For example, Kobe Bryant is the spouse of Vanessa Laine Bryant, what is the end time?.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Details of our 27 functions.</figDesc><table><row><cell>Function</cell><cell>Functional Inputs × Textual Inputs →</cell><cell>Description</cell><cell>Example (only show textual in-</cell></row><row><cell></cell><cell>Outputs</cell><cell></cell><cell>puts)</cell></row><row><cell>FindAll</cell><cell>() × () → (Entities)</cell><cell>Return all entities in KB</cell><cell>-</cell></row><row><cell>Find</cell><cell>() × (Name) → (Entities)</cell><cell>Return all entities with the given name</cell><cell>Find(Kobe Bryant)</cell></row><row><cell>FilterConcept</cell><cell>(Entities) × (Name) → (Entities)</cell><cell>Find those belonging to the given con-</cell><cell>FilterConcept(Athlete)</cell></row><row><cell></cell><cell></cell><cell>cept from the input entities</cell><cell></cell></row><row><cell>FilterStr</cell><cell>(Entities) × (Key, Value) → (Entities,</cell><cell>Find entities with the given attribute</cell><cell>FilterStr(gender, male)</cell></row><row><cell></cell><cell>Facts)</cell><cell>value, whose type is string, return enti-</cell><cell></cell></row><row><cell></cell><cell></cell><cell>ties and corresponding facts</cell><cell></cell></row><row><cell>FilterNum</cell><cell>(Entities) × (Key, Value, Op) → (Enti-</cell><cell>Similar to FilterStr, except that the at-</cell><cell>FilterNum(height, 200 cen-</cell></row><row><cell></cell><cell>ties, Facts)</cell><cell>tribute type is number</cell><cell>timetres, &gt;)</cell></row><row><cell>FilterYear</cell><cell>(Entities) × (Key, Value, Op) → (Enti-</cell><cell>Similar to FilterStr, except that the at-</cell><cell>FilterYear(birthday, 1980, =)</cell></row><row><cell></cell><cell>ties, Facts)</cell><cell>tribute type is year</cell><cell></cell></row><row><cell>FilterDate</cell><cell>(Entities) × (Key, Value, Op) → (Enti-</cell><cell>Similar to FilterStr, except that the at-</cell><cell>FilterDate(birthday, 1980-06-</cell></row><row><cell></cell><cell>ties, Facts)</cell><cell>tribute type is date</cell><cell>01, &lt;)</cell></row><row><cell>QFilterStr</cell><cell>(Entities, Facts) × (QKey, QValue) →</cell><cell>Filter out entities and corresponding</cell><cell>QFilterStr(language, English)</cell></row><row><cell></cell><cell>(Entities, Facts)</cell><cell>facts with a qualifier condition of string</cell><cell></cell></row><row><cell></cell><cell></cell><cell>type</cell><cell></cell></row><row><cell>QFilterNum</cell><cell>(Entities, Facts) × (QKey, QValue, Op)</cell><cell>Similar to QFilterStr, except that the</cell><cell>QFilterNum(bonus, 20000 dol-</cell></row><row><cell></cell><cell>→ (Entities, Facts)</cell><cell>qualifier type is number</cell><cell>lars, &gt;)</cell></row><row><cell>QFilterYear</cell><cell>(Entities, Facts) × (QKey, QValue, Op)</cell><cell>Similar to QFilterStr, except that the</cell><cell>QFilterYear(start time, 1980,</cell></row><row><cell></cell><cell>→ (Entities, Facts)</cell><cell>qualifier type is year</cell><cell>=)</cell></row><row><cell>QFilterDate</cell><cell>(Entities, Facts) × (QKey, QValue, Op)</cell><cell>Similar to QFilterStr, except that the</cell><cell>QFilterDate(start time, 1980-</cell></row><row><cell></cell><cell>→ (Entities, Facts)</cell><cell>qualifier type is date</cell><cell>06-01, &lt;)</cell></row><row><cell>Relate</cell><cell>(Entity) × (Pred, Dir) → (Entities,</cell><cell>Find entities that have a specific relation</cell><cell>Relate(capital, forward)</cell></row><row><cell></cell><cell>Facts)</cell><cell>with the given entity</cell><cell></cell></row><row><cell>And</cell><cell>(Entities, Entities) × () → (Entities)</cell><cell>Return the intersection of two entity sets</cell><cell>-</cell></row><row><cell>Or</cell><cell>(Entities, Entities) × () → (Entities)</cell><cell>Return the union of two entity sets</cell><cell>-</cell></row><row><cell>What</cell><cell>(Entity) × () → string</cell><cell>Return the entity name</cell><cell>-</cell></row><row><cell>Count</cell><cell>(Entities) × () → number</cell><cell>Return the number of entities</cell><cell>-</cell></row><row><cell>SelectBetween</cell><cell>(Entity, Entity) × (Key, Op) → string</cell><cell>From the two entities, find the one whose</cell><cell>SelectBetween(height,</cell></row><row><cell></cell><cell></cell><cell>attribute value is greater or less and re-</cell><cell>greater)</cell></row><row><cell></cell><cell></cell><cell>turn its name</cell><cell></cell></row><row><cell>SelectAmong</cell><cell>(Entities) × (Key, Op) → string</cell><cell>From the entity set, find the one whose</cell><cell>SelectAmong(height, largest)</cell></row><row><cell></cell><cell></cell><cell>attribute value is the largest or smallest</cell><cell></cell></row><row><cell></cell><cell></cell><cell>and return its name</cell><cell></cell></row><row><cell>QueryAttr</cell><cell>(Entity) × (Key) → string</cell><cell>Return the attribute value of the entity</cell><cell>QueryAttr(height)</cell></row><row><cell>QueryAttrUnder-</cell><cell>(Entity) × (Key, QKey, QValue) →</cell><cell>Return the attribute value, whose corre-</cell><cell>QueryAttrUnderCondition(</cell></row><row><cell>Condition</cell><cell>string</cell><cell>sponding fact should satisfy the qualifier</cell><cell>population, point in time,</cell></row><row><cell></cell><cell></cell><cell>condition</cell><cell>2016)</cell></row><row><cell>VerifyStr</cell><cell>(Value) × (Value) → boolean</cell><cell>Return whether the output of Query-</cell><cell>VerifyStr(male)</cell></row><row><cell></cell><cell></cell><cell>Attr or QueryAttrUnderCondition and</cell><cell></cell></row><row><cell></cell><cell></cell><cell>the given value are equal as string</cell><cell></cell></row><row><cell>VerifyNum</cell><cell>(Value) × (Value, Op) → boolean</cell><cell>Return whether the two numbers satisfy</cell><cell>VerifyNum(20000 dollars, &gt;)</cell></row><row><cell></cell><cell></cell><cell>the condition</cell><cell></cell></row><row><cell>VerifyYear</cell><cell>(Value) × (Value, Op) → boolean</cell><cell>Return whether the two years satisfy the</cell><cell>VerifyYear(1980, &gt;)</cell></row><row><cell></cell><cell></cell><cell>condition</cell><cell></cell></row><row><cell>VerifyDate</cell><cell>(Value) × (Value, Op) → boolean</cell><cell>Return whether the two dates satisfy the</cell><cell>VerifyDate(1980-06-01, &gt;)</cell></row><row><cell></cell><cell></cell><cell>condition</cell><cell></cell></row><row><cell>QueryRelation</cell><cell>(Entity, Entity) × () → string</cell><cell>Return the predicate between two enti-</cell><cell>QueryRelation(Kobe Bryant,</cell></row><row><cell></cell><cell></cell><cell>ties</cell><cell>America)</cell></row><row><cell>QueryAttr-</cell><cell>(Entity) × (Key, Value, QKey) → string</cell><cell>Return the qualifier value of the fact (En-</cell><cell>QueryAttrQualifier(population,</cell></row><row><cell>Qualifier</cell><cell></cell><cell>tity, Key, Value)</cell><cell>23390000, point in time)</cell></row><row><cell>QueryRelation-</cell><cell>(Entity, Entity) × (Pred, QKey) → string</cell><cell>Return the qualifier value of the fact (En-</cell><cell>QueryRelationQualifier(spouse,</cell></row><row><cell>Qualifier</cell><cell></cell><cell>tity, Pred, Entity)</cell><cell>start time)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>&lt;pred:fact_h&gt; ?e ; &lt;pred:fact_r&gt; &lt;population&gt; ; &lt;pred:fact_t&gt; ?pv ] &lt;point_in_time&gt; ?qpv . } What number of sovereign states have a population not equal to 7600000000? SPARQL: SELECT (COUNT(DISTINCT ?e) AS ?count) WHERE { ?e &lt;pred:instance_of&gt; ?c . ?c &lt;pred:name&gt; "sovereign state" . ?e &lt;population&gt; ?pv . ?pv &lt;pred:unit&gt; "1" . ?pv &lt;pred:value&gt; ?v . FILTER ( ?v != "7600000000"^^xsd:double ) . }</figDesc><table><row><cell>Program:</cell><cell>FindAll</cell><cell>FilterStr postal code 54000</cell><cell>FilterConcept big city</cell><cell>QueryAttrQualifier population 104072 point in time</cell></row><row><cell>Choices:</cell><cell cols="4">1980-04-01;1868-01-01;2008-11-12;1790-01-01;1964-12-01;2010-08-11;1772-12-01;2013</cell></row><row><cell></cell><cell cols="2">-01-01;1861;1810-01-01</cell><cell></cell><cell></cell></row><row><cell>Answer:</cell><cell>2013-01-01</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Question: Program:</cell><cell>FindAll</cell><cell>FilterNum population 7600000000</cell><cell>FilterConcept sovereign state</cell><cell>Count</cell></row><row><cell></cell><cell></cell><cell>≠</cell><cell></cell><cell></cell></row><row><cell>Choices:</cell><cell cols="2">145;146;147;148;149;150;151;152;153;154</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>&lt;pred:fact_h&gt; ?e_1 ; &lt;pred:fact_r&gt; &lt;educated_at&gt; ; &lt;pred:fact_t&gt; ?e_2 ] &lt;academic_degree&gt; ?qpv . }</figDesc><table><row><cell>Program:</cell><cell></cell><cell>Find</cell></row><row><cell></cell><cell></cell><cell>Vladimir Nabokov</cell></row><row><cell></cell><cell>Find George V</cell><cell>Relate educated at forward</cell><cell>QueryRelationQualifier educated at academic degree</cell></row><row><cell></cell><cell></cell><cell>And</cell></row><row><cell></cell><cell></cell><cell>Find</cell></row><row><cell></cell><cell></cell><cell>Trinity College</cell></row><row><cell>Choices:</cell><cell cols="3">Bachelor of Arts;Juris Doctor;bachelor's degree;Master of Laws;Bachelor of Fine</cell></row><row><cell></cell><cell cols="3">Arts;Doctor of Philosophy;Bachelor of Science;Bachelor of General Studies;Doctor of</cell></row><row><cell></cell><cell>Medicine;doctorate</cell><cell></cell></row><row><cell>Answer:</cell><cell>Bachelor of Arts</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Wikidata is one of the largest collaboratively edited knowledge graphs. It provides the Freebase ID for most of its entities, but the predicates are not aligned.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">We only introduce the templates of QueryName and Count, and leave others in Appendix.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">https://github.com/openlink/virtuoso-opensource</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">https://github.com/dmlc/dgl</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Question: Is the name of the person that was educated at high school Laura Linney?? SPARQL: ASK { ?e &lt;pred:instance_of&gt; ?c . ?c &lt;pred:name&gt; "human" . ?e &lt;educated_at&gt; ?e_1 . ?e_1 &lt;pred:name&gt; "high school" . ?e &lt;name_in_native_language&gt; ?pv . ?pv &lt;pred:value&gt; "Laura Linney" . }</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Program:</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>QueryAttr name in native language</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Find high school</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relate educated at backward</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VerifyStr</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Laura Linney</head><p>FilterConcept human Question: Which area has higher elevation (above sea level), Baghdad or Jerusalem (the one whose population is 75200)? SPARQL: SELECT ?e WHERE { { ?e &lt;pred:name&gt; "Baghdad" . } UNION { ?e &lt;pred:name&gt; "Jerusalem" . ?e &lt;population&gt; ?pv_1 . ?pv_1 &lt;pred:unit&gt; "1" . ?pv_1 &lt;pred:value&gt; "75200"^^xsd:double . } ?e &lt;elevation_above_sea_level&gt; ?pv . ?pv &lt;pred:value&gt; ?v . } ORDER BY DESC(?v) <ref type="bibr">LIMIT</ref>  1906-02-05</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question:</head><p>For what was John Houseman (who is in the Jewish ethnic group) nominated for an Academy Award for Best Picture? SPARQL: SELECT DISTINCT ?qpv WHERE { ?e_1 &lt;pred:name&gt; "John Houseman" . ?e_1 &lt;ethnic_group&gt; ?e_3 . ?e_3 &lt;pred:name&gt; "Jewish people" . ?e_2 &lt;pred:name&gt; "Academy Award for Best Picture" . ?e_1 &lt;nominated_for&gt; ? e_2 . <ref type="bibr">[</ref>   </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural module networks</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural program induction for kbqa without gold programs or query annotations</title>
		<author>
			<persName><forename type="first">Ghulam</forename><surname>Ahmed Ansari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amrita</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishwajeet</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohan</forename><surname>Bhambhani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumen</forename><surname>Chakrabarti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic parsing on freebase from question-answer pairs</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
				<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Large-scale simple question answering with memory networks</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02075</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cognitive graph for multi-hop reading comprehension at scale</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qibin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Language to logical form with neural attention</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs</title>
		<author>
			<persName><forename type="first">Dheeru</forename><surname>Dua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Stanovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Lc-quad 2.0: A large dataset for complex question answering over wikidata and dbpedia</title>
		<author>
			<persName><forename type="first">Mohnish</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debayan</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Abdelkawi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Semantic Web Conference</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="69" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Hierarchical graph network for multi-hop question answering</title>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.03631</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dialog-to-action: Conversational question answering over a large-scale knowledge base</title>
		<author>
			<persName><forename type="first">Daya</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.04971</idno>
		<title level="m">Neural module networks for reasoning over text</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">How children learn</title>
		<author>
			<persName><forename type="first">John</forename><surname>Holt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Hachette UK</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding based question answering</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingcheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Gqa: A new dataset for real-world visual reasoning and compositional question answering</title>
		<author>
			<persName><forename type="first">A</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09506</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Freebaseqa: A new factoid qa data set matching trivia-style question-answer pairs with freebase</title>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dekun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Clevr: A diagnostic dataset for compositional language and elementary visual reasoning</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural symbolic machines: Learning semantic parsers on freebase with weak supervision</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Forbus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Key-value memory networks for directly reading documents</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName><surname>Amir-Hossein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Simplequestions nearly solved: A new upperbound and baseline approach</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Petrochuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Stepwise reasoning for multi-relation question answering over knowledge graph with weak supervision</title>
		<author>
			<persName><forename type="first">Yunqi</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Complex sequential question answering: Towards learning to converse over linked question answer pairs with a knowledge graph</title>
		<author>
			<persName><forename type="first">Amrita</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vardaan</forename><surname>Pahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mitesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Khapra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarath</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName><surname>Chandar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Complex program induction for querying knowledge bases in the absence of gold programs</title>
		<author>
			<persName><forename type="first">Amrita</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ghulam</forename><surname>Ahmed Ansari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Laddha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumen</forename><surname>Chakrabarti</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Transactions of the Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Semantic Web Conference</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Explainable and explicit visual reasoning over scene graphs</title>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Neural machine translation for query construction and composition</title>
		<author>
			<persName><forename type="first">Tommaso</forename><surname>Soru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edgard</forename><surname>Marx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">André</forename><surname>Valdestilhas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Esteves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Moussallem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustavo</forename><surname>Publio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.10478</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Sparqa: Skeleton-based semantic parsing for complex questions over knowledge bases</title>
		<author>
			<persName><forename type="first">Yawei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingling</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuzhong</forename><surname>Qu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The web as a knowledge-base for answering complex questions</title>
		<author>
			<persName><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Representing text for joint embedding of text and knowledge bases</title>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pallavi</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Gamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Wikidata: a free collaborative knowledge base</title>
		<author>
			<persName><forename type="first">Denny</forename><surname>Vrandečić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Krötzsch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning representation mapping for relation detection in knowledge base question answering</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongxiang</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaixiang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianbing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.09600</idno>
		<title level="m">Hotpotqa: A dataset for diverse, explainable multi-hop question answering</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dagmar</forename><surname>Gromann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Rudolph</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.09302</idno>
		<title level="m">Neural machine translating from natural language to sparql</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Variational reasoning for question answering with knowledge graph</title>
		<author>
			<persName><forename type="first">Yuyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">An interpretable reasoning network for multi-relation question answering</title>
		<author>
			<persName><forename type="first">Mantong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
