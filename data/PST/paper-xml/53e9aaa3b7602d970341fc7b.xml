<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">YCSB++ : Benchmarking and Performance Debugging Advanced Features in Scalable Table Stores</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Swapnil</forename><surname>Patil</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Milo</forename><surname>Polte</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kai</forename><surname>Ren</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wittawat</forename><surname>Tantisiriroj</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lin</forename><surname>Xiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Julio</forename><surname>López</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Garth</forename><surname>Gibson</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Adam</forename><surname>Fuchs</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">National Security Agency</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Billie</forename><surname>Rinaldi</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">National Security Agency</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">YCSB++ : Benchmarking and Performance Debugging Advanced Features in Scalable Table Stores</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6CB38D5A38E430E19DF6084947D55E01</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T17:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3.4 [Information Storage and Retrieval]: Systems and Software-performance evaluation</term>
					<term>H.2.4 [Database Management]: Systems-distributed and parallel databases</term>
					<term>D.2.5 [Software Engineering]: Testing and Debuggingtesting tools, diagnostics Scalable Table Stores, Benchmarking, YCSB, NoSQL</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Inspired by Google's BigTable, a variety of scalable, semistructured, weak-semantic table stores have been developed and optimized for different priorities such as query speed, ingest speed, availability, and interactivity. As these systems mature, performance benchmarking will advance from measuring the rate of simple workloads to understanding and debugging the performance of advanced features such as ingest speed-up techniques and function shipping filters from client to servers. This paper describes YCSB++, a set of extensions to the Yahoo! Cloud Serving Benchmark (YCSB) to improve performance understanding and debugging of these advanced features. YCSB++ includes multi-tester coordination for increased load and eventual consistency measurement, multi-phase workloads to quantify the consequences of work deferment and the benefits of anticipatory configuration optimization such as B-tree pre-splitting or bulk loading, and abstract APIs for explicit incorporation of advanced features in benchmark tests. To enhance performance debugging, we customized an existing cluster monitoring tool to gather the internal statistics of YCSB++, table stores, system services like HDFS, and operating systems, and to offer easy post-test correlation and reporting of performance behaviors. YCSB++ features are illustrated in case studies of two BigTable-like table stores, Apache HBase and Accumulo, developed to emphasize high ingest rates and finegrained security.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The past few years have seen an emergence of large-scale table stores that are more simple and lightweight, and provide higher scalability and availability than traditional relational databases <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b45">46]</ref>. Table stores, such as BigTable <ref type="bibr" target="#b11">[12]</ref>, Dynamo <ref type="bibr" target="#b16">[17]</ref>, HBase <ref type="bibr" target="#b26">[27]</ref> and Cassandra <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b32">33]</ref>, are an intrinsic part of Internet services. Not only are these stores used by data-intensive applications, such as business analytics and scientific data analysis <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b44">45]</ref>, but they are also used by critical systems infrastructure; for example, the next generation Google file system, called Colossus, stores all file system metadata in BigTable <ref type="bibr" target="#b19">[20]</ref>.</p><p>This growing adoption, coupled with spiraling scalability and tightening performance requirements, has led to the inclusion of a range of (often re-invented) optimization features that significantly increase the complexity of understanding the behavior and performance of the system. Table stores that began with a simple table model and single-row transactions have extensions with new mechanisms for consistency, bulk insertions, concurrency, data partitioning, indexing, and query analysis.</p><p>A key functionality enhancement for applications that continuously capture petabytes into a table is to increase the speed of ingest <ref type="bibr" target="#b44">[45]</ref>. Typically data is ingested in a table using iterative insertions or bulk insertions. Iterative insertions add new data through single row "insert" or "update" operations that are often optimized using techniques such as client-side buffering, disabling logs <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b42">43]</ref>, relying on fast storage devices <ref type="bibr" target="#b48">[49]</ref>, and indexing structures optimized for high-speed inserts <ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b37">38]</ref>. Bulk loads bypass the regular insertion code path by converting existing datasets from their external storage format to the format of the native table store so that insertion bypasses the normal insert code path. Proposals to speed up bulk loading include using optimization frameworks to pre-split partitions <ref type="bibr" target="#b46">[47]</ref> and running Hadoop jobs to parallelize data loading <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b27">28]</ref>.</p><p>Another useful feature is the ability to run distributed computations directly on data stored at table store servers instead of clients. BigTable co-processors allow arbitrary application code to run directly on tablet servers even when the table is growing and expanding over multiple servers <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15]</ref>. HBase plans to use a similar technique for server-side filtering and fine-grained access control <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b39">40]</ref>. Such a server-side execution model, inspired from early work in parallel databases <ref type="bibr" target="#b17">[18]</ref>, is designed to drastically reduce the amount of data shipped to the client. This significantly improves performance, particularly of scan operations with an application-defined filter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Extensions to the YCSB framework</head><p>Observations in HBase and Accumulo Distributed testing using multiple YCSB client nodes ZooKeeper-based barrier synchronization for multiple YCSB clients to coordinate start and end of different tests Distributed setup benefits multi-client, multi-phase testing (to evaluate weak consistency and table pre-splits)</p><p>Distributed event notification using ZooKeeper to understand the cost (measured as read-after-write latency) of weak consistency</p><p>Both HBase and Accumulo support strong consistency, but using client-side batch writing for higher throughput results in weak consistency with higher read-after-write latency as batch sizes increase Ingest-intensive workload extensions External Hadoop tool that formats data to be inserted into a format used natively by the table store servers Bulk insertion delivers the highest data ingest rate of all ingestion techniques, but the servers may end up doing expensive load-balancing A new workload executor for externally pre-splitting the key space into variable-sized and fixed-size ranges.</p><p>Ingest throughput of Accumulo increases by 20% but if range partitioning is not known a priori the servers may incur expensive re-balancing and merging overhead Offloading functions to the DB servers New workload executor that generates "deterministic" data to allow use of appropriate filters and DB client API extensions to send filters to servers Server-side filtering benefits HBase and Accumulo only when the client scans enough data (more than 10 MB) to mask network and disk I/O overhead Fine grained access control New workload generator and API extensions to DB clients to test both schema-level and cell-level access control models (HBase does not support access control <ref type="bibr" target="#b26">[27]</ref> but Accumulo does) Accumulo's access control increases the size of the table and may reduce insert throughput (if client CPU is saturated) or scan throughput (when server returns ACLs with the data) in proportion to controls imposed Table <ref type="table">1</ref>: Summary of contributions -For each advanced functionality that YCSB++ benchmarks, this table describes the techniques implemented in YCSB and the key observations from our HBase and Accumulo case studies.</p><p>The profusion of table stores calls for developing effective benchmarking tools, and the Yahoo! Cloud Serving Benchmark (YCSB) has answered this call successfully. YCSB is a great framework for measuring the basic performance of several popular table stores including HBase, Voldemort, Cassandra and MongoDB <ref type="bibr" target="#b13">[14]</ref>. YCSB has an abstraction layer for adapting to the API of a specific table store, for gathering widely recognized performance metrics and for generating a mix of workloads. Although it is useful for characterizing the baseline performance of simple workloads, such as single-row insertions, lookups or deletions, YCSB lacks support for benchmarking advanced table store functionality. Advanced features make a table store attractive for a wide range of use cases, but their complex interactions can be very hard to benchmark, debug and understand, especially when a store exhibits poor performance.</p><p>Our goal is to extend the scope of table store benchmarking in YCSB to support complex features and optimizations. In this paper, we present a systematic approach to benchmark advanced functionality in a distributed manner, and implement our techniques as extensible modules in the YCSB framework. We do not modify the table stores under evaluation, but the abstraction layer adapting a specific table store to a benchmarking oriented API for a specific advanced function may be simple or complex, depending on the capabilities of the underlying table store.</p><p>Table <ref type="table">1</ref> summarizes the key contributions of this paper. The first contribution is a set of benchmarking techniques to measure and understand five advanced features: weak consistency, bulk insertions, table pre-splitting, server-side filtering and fine-grained access control. The second contribution is implementing these techniques, which we collectively call YCSB++, as extensible modules in the YCSB framework. Our final contribution is the experience of analyzing these features in two table stores, HBASE <ref type="bibr" target="#b26">[27]</ref> and Accumulo, both inspired by BigTable and exhibiting most or all of YCSB++ features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">YCSB++ DESIGN</head><p>In this section, we present an overview of table stores, including HBase and Accumulo, followed by the design and implementation of advanced functionality benchmarking techniques in YCSB++.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overview of table stores</head><p>HBase and Accumulo are scalable semi-structured table stores that store data in a multi-dimensional sorted map where keys are tuples of the form {row, column, timestamp}. Both are inspired by Google's BigTable system <ref type="bibr" target="#b11">[12]</ref>. HBase is being developed as a part of the open-source Apache Hadoop project <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref> and Accumulo is being developed by the U.S. National Security Agency. <ref type="foot" target="#foot_0">1</ref> Both are written in Java and layered on top of the Hadoop distributed file system (HDFS) <ref type="bibr" target="#b5">[6]</ref>. They support efficient storage and retrieval of structured data, including range queries, and allow using tables as input and output for MapReduce jobs. Other features in these systems pertinent to YCSB++ include automatic load-balancing and partitioning, data compression and server-side user-defined function such as regular expression filtering. To avoid confusion from terminology differences in HBase and Accumulo, the rest of this paper uses terminology from the Google BigTable paper <ref type="bibr" target="#b11">[12]</ref>.</p><p>At a high-level, each table is indexed as a B-tree in which all records are stored in leaf nodes called tablets. An HBase or Accumulo installation consists of tablet servers running on all nodes in the cluster, and each tablet server handles requests for several tablets. A tablet consists of rows in a contiguous range in the key space and is represented (on disk) as one or more files stored in HDFS. Each table store represents these files in their respective custom formats (BigTable uses an SSTable format, HBase uses an HFile format and Accumulo uses an RFile format) which we will refer as store files. In all cases, store files are sorted, indexed, and used with bloom filters to make negative lookups faster <ref type="bibr" target="#b11">[12]</ref>. Both HBase and Accumulo provide columnar abstractions that allow users to group a set of columns into a locality group. Each locality group is stored in its separate store file in HDFS; this enables efficient scan performance by avoiding excess data fetches (from other columns) <ref type="bibr" target="#b47">[48]</ref>. These table stores use a master server that manages schema details and assigns tablets to tablet servers in a load-balanced manner.</p><p>When a table is first created, it has a single tablet, the root of the B-tree, managed by one tablet server. Inserts are sent to an appropriate tablet server guided by the cached state about non-leaf nodes of the B-tree. The leaf tablet server logs mutation operations and buffers all requests in an in-memory buffer called memstore. When this memstore fills up, the tablet server flushes recently written entries to create a store file in HDFS; this process is called minor compaction. As the table grows, the memstore fills up again and is flushed to create another store file. Reads not specified in a memstore may have to search many store files for the requested entries. This use of multiple store files representing mutations from a particular time period is inspired by the classic log-structured merge tree (LSM-tree) <ref type="bibr" target="#b37">[38]</ref>. Once a tablet exceeds a threshold size, the tablet server splits the overflowing tablet (and its key range) by creating a new tablet on another tablet server and transferring the rows that belong to the key range of the new tablet. This process is called a split. A large table may have large number of tablets and each tablet may have many store files. To control the number of store files that may be accessed to service a read request, major compaction operations are used to merge store files into fewer store files. All files are stored in HDFS and these table stores rely on HDFS for durability and availability of data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Additional features in Accumulo</head><p>The design and implementation of Accumulo has several features that are different from other open-source table stores. Perhaps the most unique feature in Accumulo is the iterator framework that embeds user-programmed functionality into the different LSM-tree stages. Figure <ref type="figure" target="#fig_1">1</ref> shows how iterators fit in the tablet server architecture of Accumulo and enable in-situ processing during otherwise necessary I/O operations. For example, iterators can operate during minor compactions by using the memstore data as input to generate on-disk store files comprised of some transformation of the input such as statistics or additional indices. is either another Iterator or the main query function. This single parentage property with no loops means that the collection of Iterators forms a tree topology.</p><p>As additional background needed to fully characterize Iterators, we define the term context to be the group of cells that a processing element considers at one time in order to determine its output. For example, a Reducer is a processing element that operates within the context is a single key and all of the values associated with it. 3 Given our definition of key and value, we can enumerate a set of useful contexts over which we can define functions. These contexts are shown in figure <ref type="figure" target="#fig_2">2</ref>.</p><p>A Full Context allows operations over an ordered set of unrestricted key/value pairs, a Row Context allows operations over an ordered set of key/value pairs which all have the same row (i.e., a single row in IcyTable), etc. In a Full Context a processing element would be able to see all of the data in a table, while in a Value Context a processing element would only see one key/value pair at a time. With IcyTable, the Full Context is only available on the client side, since rows are partitioned across tablet servers and tablet servers do not have a built-in mechanism to share data for query. However, all of the other contexts are available on the server-side, and we have many examples of useful functions that can be encoded in those contexts. 3 For the purpose of this discussion we ignore the possibility of the Reducer object carrying state between calls to reduce().</p><p>Conceptually, an Iterator is a processing element th the form Row Context ⇒ key, value. In other words erator takes all of the data in a row and provides an o stream of key/value pairs. The following represents sioning Iterator, an iterator that filters the set of key pairs down to the most recent timestamped version key as follows:</p><p>1: method init(Iterator src) 2: rp, fp, qp, visp ← ∅ 3:</p><p>s ← src 4: method next() 5: repeat 6:</p><p>((r, f, q, vis, t), v) ← s.next() 7:</p><p>until (rp, fp, qp, visp) = (r, f, q, vis) 8:</p><p>(rp, fp, qp, visp) ← (r, f, q, vis) 9:</p><p>return ((r, f, q, vis, t), v)</p><p>This Versioning Iterator is initialized with a sour ator that it uses as the source for all of its key/value In its next method, it skips over any repeated vers keys after the first. In doing so, it limits itself to sion Context. For ease of programming, we also pro number of specializations of the Iterator that allow explicitly encode functions on more narrow context Aggregating Iterator, discussed in Section 4.1, uses a gable Aggregator object to encode computations with Version Context, mapping a stream of values to a value. Figure <ref type="figure" target="#fig_3">3</ref> gives a taxonomy of some different It and the contexts in which they operate.</p><p>At the leaves of the Iterator Tree are simple Iterato seek to a location in the key space and read key/valu in order from sorted files or the in-memory map. The of these leaves is always a Multiple Iterator, which pe a merge of the data provided by several sources and p a single, sorted view. Several layers of Iterators on the leaves, in every Iterator Tree, are constructed of t system code. These trusted Iterators ensure safe, secu consistent access to data available on the tablet, perf functions such as cell-level security and key deletio Iterators read one or more sorted sequences of key-value pairs and output an ordered stream of key-value pairs. The input stream can be transformed in various ways depending on the context of transformation used by an iterator. The context of an iterator is the degree of commonality among the key-value pairs that it uses as input for a given operation. For example, a version context is defined as a set of key-value pairs that share the same row and column but have different timestamps and values. A versioning iterator operates within that version context and pares down the input set to the N key-value pairs with the most recent timestamps. An aggregating iterator also operates within a version context, and it replaces all key-value pairs in the set with a new key-value pair whose value is an aggregate function (e.g. sum) of the key-value pairs in the context.</p><p>Accumulo can also create iterator trees by chaining different iterators together such that the output from one iterator serves as the input to another iterator. These iterators can be organized as a hierarchy, comprising of parent and child iterators that can create a user-defined data processing pipeline to support stream processing, incremental bulk processing, and partitioned join operations. Iterators can perform the basic operations of a query language, such as selection, projection, and set intersection and union within a partition. Trees of these types of iterators are used to implement scalable information retrieval systems with highly expressive query languages. Other user-defined iterators can be used to efficiently encode complex, online statistical aggregation functions that are crucial for big-data analytics.</p><p>Another feature unique to Accumulo is fine-grained celllevel access control. To the best of our knowledge, Accumulo is the only table store that provides cell-level access control by associating an access control list (ACL) with every cell. This is different from Bigtable, which uses tablelevel and column family-level access control mechanisms <ref type="bibr" target="#b11">[12]</ref>. HBase proposes to support a coarse-grained schema-level access control mechanism that will store and check the ACLs only at a schema (or metadata) level <ref type="bibr" target="#b29">[30]</ref>. Currently, Accumulo uses cell-level access control only for reads and additional schema-level access control for both read and write operations. To support cell-level ACLs, Accumulo uses a key specifier comprised of the tuple {row, column family, column qualifier, visibility, timestamp} where the visibility portion is an encoded and-or tree of authorizations.</p><p>The authors of Accumulo report that it has been demon-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Client nodes</head><p>Workload parameter le</p><formula xml:id="formula_0">-! R/W mix! -! RecordSize! -! DataSet! -! …!</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Extensions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HBase</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACCUMULO</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Other DBs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Storage Servers</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stats Workload Executor</head><p>Client Threads</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DB Clients</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>New workloads</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>API ext</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Phase Processing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>YCSB Client (with our extensions)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ganglia monitoring</head><p>Hadoop, HDFS and OS metrics YCSB metrics</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>YCSB Client Coordination</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ZooKeeper-based barrier sync and event noti cation</head><p>Command-line parameters strated across diverse hardware configurations and multiple levels of scale and, in a variety of usability tests, it has been successful at handing very complex data-sets with highspeed, efficient ingest and concurrent query workloads. An open-source release of Accumulo has been offered to the Apache Software Foundation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">YCSB background</head><p>The Yahoo! Cloud Serving Benchmark (YCSB) is a popular extensible framework designed to compare different table stores under identical synthetic workloads <ref type="bibr" target="#b13">[14]</ref>; the different modules in YCSB are shown as light boxes in Figure <ref type="figure" target="#fig_2">2</ref>.</p><p>The workload executor module loads test data and generates operations that will be specialized and issued by a DB client to a table store. The default YCSB workload issues mixes of basic operations including reads, updates, deletes and scans. In YCSB, read operations may read() a single row or scan() a range of consecutive rows and update operations may either insert() a new row or update() an existing one. Operations are issued one at a time per client thread and their distributions are based on parameters specified in the workload parameter file for a benchmark. The YCSB distribution includes five default workload files (called <ref type="figure">Workloads A, B, C, D</ref> and<ref type="figure">E</ref> ) that generate specific readintensive, update-intensive and scan-intensive workloads.</p><p>The current YCSB distribution provides DB client modules with wrappers for HBase, Cassandra <ref type="bibr" target="#b0">[1]</ref>, MongoDB <ref type="bibr" target="#b1">[2]</ref> and Voldemort <ref type="bibr" target="#b2">[3]</ref>; YCSB++ adds a new client for Accumulo. For a given table store, its DB client converts a 'generic' operation issued by the workload executor to an operation specific for that table store. In an HBase cluster, for example, if the workload executor generates a read() operation, the HBase DB client issues a get() operation to the HBase servers.</p><p>YCSB starts executing a benchmark using a pool of client threads that call the workload executor to issue operations and then report the measured performance to the stats module. Users can specify the size of the work generating thread pool, the table store being evaluated and the workload parameter file as command line parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Extensions in YCSB++</head><p>YCSB's excellent modular structure makes it natural for us to integrate advanced functionality testing mechanisms as YCSB extensions. Our YCSB++ extensions are shown as dark shaded boxes in Figure <ref type="figure" target="#fig_2">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Parallel testing</head><p>The first extension in YCSB++ enables multiple clients, on different machines, to coordinate start and end of benchmarking tests. This modification is necessary because YCSB was designed to run on a single node and just one instance of YCSB, even with hundreds of threads, may limit its ability to test large deployments of table stores effectively. YCSB++ controls execution of different workload generator instances through distributed coordination and event notification using Apache ZooKeeper, a service that provides distributed synchronization and group membership <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b51">52]</ref>. ZooKeeper is already used in HBase and Accumulo deployments.</p><p>YCSB++ implements a new class, called ZKCoordination, that provides two abstractions -barrier-synchronization and producer-consumer -through ZooKeeper. We added four new parameters to the workload parameter file: a status flag, the ZooKeeper server address, a barrier-sync variable, and the size of the client coordination group. The status flag checks whether coordination is needed among the clients. Each coordination instance has a unique barrier-sync variable to track the number of processes entering or leaving a barrier. ZooKeeper uses a hierarchical namespace for synchronization and, for each barrier-sync variable specified by YCSB++, creates a corresponding "barrier" directory in its namespace. Whenever a new YCSB++ client starts, it joins the barrier by contacting the ZooKeeper server that in turn creates a new entry, corresponding to the client's identifier, in the barrier directory. The number of entries in a barrier directory indicates the number of clients that have joined the barrier. If all the clients have joined the barrier, ZooKeeper sends these clients a callback message to start executing the benchmark; if not, YCSB++ clients block and wait for more clients to join. After the test (or one phase) completes, YCSB++ clients notify ZooKeeper about leaving the barrier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Weak consistency</head><p>Table stores provide high throughput and high availability by eliminating expensive features, particularly the strong ACID transactional guarantees found in traditional relational databases. Based on the CAP theorem, some table stores tolerate network Partitions and provide high Availability by giving up on strong Consistency guarantees <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b21">22]</ref>. Systems may offer "loose" or "weak" consistency semantics, such as eventual consistency <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b49">50]</ref>, in which acknowledged changes are not seen by other clients for significant time delays. This lag in change visibility may introduce challenges that programmers may need to explicitly handle in their applications (i.e., coping with possibly stale data). YCSB++ measures the time lag from one client completing an insert until a different client can successfully observe the value.</p><p>To evaluate this time to consistency, YCSB++ uses asynchronous directed coordination between multiple clients enabled by the producer-consumer abstraction in the aforementioned ZKCoordination module. YCSB++ clients interested in benchmarking weak consistency specify three properties in the workload parameter file: a status flag to check if a client is a producer or a consumer, the ZooKeeper server address, and a reference to a shared queue datastructure in ZooKeeper. Synchronized access to this queue is provided by ZooKeeper: for each queue, ZooKeeper creates a directory in its hierarchical namespace and adds (or removes) a file in this directory for every key inserted in (or deleted from) the queue. Clients that insert or update records are "producers" who add keys of recently inserted records in the ZooKeeper queue. The "consumer" clients register a callback on this queue at start-up. On receiving a notification from ZooKeeper about new elements, "consumers" remove a key from the queue then read it from the table store. If the attempt to read this key fails, the "consumer" will put the key back on the queue and try reading the next available key. Excessive use of ZooKeeper for inter-client coordination may affect the performance of the benchmark; we avoid this issue by sampling a small fraction (1%) of the inserted keys for read-after-write measurements. The "read-after-write" time lag for key K is the difference from the time a "consumer" first tries to read the new key until the first time it successfully reads that key from the table store server; we only report the lag for keys that needed more than one read attempt. We did not measure the time from "producer" write to "consumer' read in order to avoid cluster-wide clock synchronization challenges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">Table pre-splitting for fast ingest</head><p>Recall that both HBase and Accumulo distribute a table over multiple tablets. Because these stores use B-tree indices, each tablet has a key range associated with it and this range changes when a tablet overflows to split into two tablets. These split operations limit the performance of ingest-intensive workloads because table store implementations lock a tablet during splits and migrate a large amount of data from one tablet server to another on a different machine. During this migration, servers refuse any operation (including reads) addressed to the tablet undergoing a split (until it finishes). One way to reduce this splitting overhead is to split a table when it is empty or small into multiple key ranges based on a priori knowledge, such as key distributions, of the workload; we call this pre-splitting the table.</p><p>YCSB++ adds to the DB clients module a pre-split function that takes split points as input and invokes the servers to pre-split a table. To enable pre-splits in a benchmark, YCSB++ adds a new property in the workload parameter files that can specify either a list of variable-size ranges in the key space or a number of fixed-size partitions to divide the key space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.4">Bulk loading using Hadoop</head><p>To efficiently add massive data-sets, various table stores rely on specialized, high-throughput tools and interfaces <ref type="bibr" target="#b27">[28]</ref>. In addition to the normal insert operations, YCSB++ supports the use of these specialized bulk load mechanisms. YCSB++ invokes an external tool that directly processes the incoming data, stores it in an on-disk format native to the table store, and notifies the servers about the existence of the new and properly formatted files through an import() API call. Table store servers make the newly loaded data-set available after successfully updating internal datastructures.</p><p>Developers can create bulk loader adaptors for particular table stores by providing specific implementations for two YCSB++ components: data transformation and import operation adaptors. For the data transformation component, YCSB++ expects a Hadoop application for partitioning, potentially sorting, and storing the data in the appropriate format. The implementation of the import operation loads the formatted data using the specific interface for the particular table store. YCSB++ also implements a generic Hadoop data generator for bulk load benchmarks that can be extended and adapted to a particular store by implementing the corresponding output format adaptor in the tool.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.5">Server-side filtering</head><p>Server-side filtering offloads compute from the client to the server, possibly reducing the amount of data transmitted over the network and amount of data fetched from disk. In order to reduce the amount of data fetched from disk, YCSB++ includes the ability to break columns into locality groups. Since locality groups are often stored in separate files by tables stores, filters that test and return data from only some locality groups do less work <ref type="bibr" target="#b47">[48]</ref>. YCSB++ takes a workload parameter causing each column to be treated as a single locality group.</p><p>There are a wide range of server-side filters that could be supported and scalable table stores filtering implementations are often not as expressive as SQL. For YCSB++ we define four server-side filters, exploiting regular expressions for "pattern" parameters, that are significantly different and are supported in both HBase and Accumulo. The first filter returns the entire row if the value of the row's key matches a specified pattern, the second filter returns the entire row if the value of the row's entry for a specified column name matches a specified pattern, the third filter returns the row's key and the {column name, cell value} tuple where column name matches a specified pattern, and the fourth filter returns the row's key and the {column name, cell value} tuple where any column's entry value matches a specified pattern.</p><p>Each table store's DB client implements these four filters in whatever manner is best supported by the table store under test. That is, if the table store does not have an API capable of function shipping the filter to the server, it could fetch all possibly matching data and implement the filter in the DB client.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.6">Access control</head><p>Table stores support different types of access control mechanisms, including none at all, checks applied at the level of the entire table, checks applied conditionally to each column, column family or locality group, or checks applied to every cell. Checks applied only to the entire table or specific column sets are said to be schema-level access controls, while checks applied to every cell are said to be cell-level access controls. HBase developers are working on schema-level access control, although the main release of Hbase has no security <ref type="bibr" target="#b29">[30]</ref>. Accumulo implements both, using schema-level access control on all accesses and cell-level access controls on read accesses.</p><p>YCSB++ supports tests that specify credentials for each operation and access control lists (ACLs) to be attached to schema or cells. The DB client code for each table store implements operations specific to a credential used or an ACL set in the manner best suited to that table store. The goal of YCSB++ access control tests is to evaluate the performance consequences of using access control; our tests ex- aggerate the use of ACLs relative to table data to make performance trends more noticeable, not because we believe that this heavy use of ACLs is common.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Performance monitoring in YCSB++</head><p>There are many tools for cluster-wide monitoring and visualization such as Ganglia <ref type="bibr" target="#b36">[37]</ref>, Collectd <ref type="bibr" target="#b12">[13]</ref>, and Munin <ref type="bibr" target="#b38">[39]</ref>. These tools are designed for large scale data gathering, transport, and visualization. They make it easy to view application-agnostic metrics, such as aggregate CPU load in a cluster, but they lack support for application-specific performance monitoring and analysis. For example, virtual memory statistics for the sum of all processes running on a node or cluster are typically recorded, but we think a more useful approach is to report aggregate memory usage of a MapReduce task separate from that used by tablet servers, HDFS data servers and other non-related processes.</p><p>YCSB++ uses a custom monitoring tool, called Otus <ref type="bibr" target="#b40">[41]</ref>, that was built on top of Ganglia. Otus runs a daemon process on each cluster node that periodically collects metrics from the node's OS, from different table store components such as tablet servers and HDFS data nodes, and from YCSB++ itself. All collected metrics are stored in a central repository; users can process and analyze the collected data using a tailored web-based visualization system.</p><p>In Otus, OS-level resource utilization for individual processes is obtained from the Linux /proc file system; these metrics include per-process CPU usage, memory usage, and disk and network I/O activities. By inspecting commandline invocation data from /proc and aggregating stats for process groups derived from other invocations, Otus differentiates logical functions in a node. Table store related metrics, such as the number of tablets and store files, are extracted directly from the table store services to provide information about the inner workings of these systems. Otus can currently extract metrics from HBase and Accumulo, and adding support for another table store involves writing Python scripts to extract the desired metrics in whatever manner that table store uses to dynamically report metrics <ref type="bibr" target="#b40">[41]</ref>. We also extended the YCSB++ stats module to periodically send (using UDP) performance metrics to Otus.</p><p>By storing the collected data in a central repository and providing a flexible web interface to access the benchmark data, users can obtain and correlate fine-grained time series information of different metrics coming from different service layers and within a service. Figure <ref type="figure" target="#fig_3">3</ref> shows a sample output from Otus that combines simultaneous display of three metrics collected during an experiment: HDFS data node CPU utilization, tablet server CPU utilization and the number of store files in the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">ANALYSIS</head><p>All our experiments are performed on sub-clusters of the 64-node "OpenCloud" cluster at CMU. Each node has a 2.8 GHz dual quad core CPU, 16 GB RAM, 10 Gbps Ethernet NIC and four Seagate 7200 RPM SATA disk drives. These machines were drawn from two racks of 32 nodes each with an Arista 7148S top-of-the-rack switch. Both rack switches are connected to an Force10 4810 head-end switch using six 10 Gbps uplinks each. Each node was running Debian Lenny 2.6.32-5 Linux distribution with the XFS file system managing the test disks.</p><p>Our experiments were performed using Hadoop-0.20.1 (that includes HDFS) and HBase-0.90.2 which use the Java SE Runtime 1.6.0. HDFS was configured with a single dedicated metadata server and 6 data servers. Both HBase and Accumulo were running on this HDFS configuration with one master and 6 region servers -a configuration similar to the original YCSB paper. <ref type="bibr" target="#b13">[14]</ref>. The test data in these table stores was stored in table that used the default YCSB schema where each row is 1 KB in size and comprises of ten columns of 100 bytes each; this schema was used for all experiments except server-side filtering (in Section 3.5) and access control (in Section 3.6).</p><p>The rest of this section shows how YCSB++ was used to study the performance behavior of advanced functionality in HBase and Accumulo. We use the Otus performance monitor (Section 2.4) to understand the observed performance of all software and hardware components in the cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Effect of batch writing</head><p>Both HBase and Accumulo coalesce application writes in a client-side buffer before sending them to a server because batching multiple writes together improves the write throughput by avoiding a round-trip latency in sending each write to the server. To understand the benefits of batching for different write buffer sizes, we configure two 6-node clusters, one for HBase and other for Accumulo, that are both layered on an HDFS instance. We use 6 separate machines as YCSB++ clients that insert 9 million rows each in a single table; the YCSB++ clients for Accumulo use 50 threads each, while the YCSB++ clients for HBase use 4 threads each. <ref type="foot" target="#foot_2">2</ref>Figure <ref type="figure" target="#fig_4">4</ref> shows the insert throughput (measured as the number of rows inserted per second) with four different batch sizes. All numbers are an average of two runs with negligible variance. Results are most dramatic for Accumulo, where more than a factor of two increase in insert throughput can be obtained with larger write batching, but HBase also sees almost a factor of two increase with large batch size when the offered load from the client is large.</p><p>Graphs like Figure <ref type="figure" target="#fig_4">4</ref> are useful to the developers of a ta- ble store both to confirm that a mechanism such as batch writing achieves greater insert throughput and to point out where other effects impact the desired result. For example, HBase with 10KB batches sees lower throughput at higher offered load and Accumulo with 1 client and 50 threads aggregate sees slightly decreasing throughput with larger batches. Figure <ref type="figure" target="#fig_5">5</ref> begins to shed light on the latter situation; 9 million inserts of 1 KB rows in batches of 10 KB (10 rows) to 10 MB (10,000 rows) fully saturates the client most of the time, so little throughput can be gained from more efficiency in the server or lower per insert latency. In fact, the two periods of significant decrease in utilization in the client suggests looking more deeply at non-continuous processes in the server (such as tablet splitting and major compactions of store files, which, for example, are seen to be large sources of slowdown in Section 3.4).</p><p>Consider the most significant throughout change in Figure <ref type="figure" target="#fig_4">4</ref>, Accumulo with high offered load sees its throughout increase from near 20,000 rows per second to over 40,000 rows per second when the batch size goes from 10 KB to 100 KB then sees only small increases for larger batches. Figures 6 shows how the server CPU utilization with 100 KB batches is approaching saturation, reducing the benefit of larger batches from both increasing the client efficiency at generating load and increasing the server efficiency at processing load to only increasing throughput with increased server efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Weak consistency due to batch writing</head><p>Although batching improves throughput, it has an important side-effect: data inconsistency. Even for table stores like HBase and Accumulo that support strong consistency, newly written objects are locally buffered and are not sent to the server until the buffer is full or a time-out on the buffer expires. Such delayed writes can violate the readafter-write consistency expected by many applications, i.e. a client, who is notified by another client that some write has been completed, may fail to read the data written by that operation.</p><p>We evaluate the cost of batch writing using the producerconsumer abstraction in YCSB++ with a 2-client setup. Client C1 inserts 1 million rows in an empty table, randomly selects 1% of these inserts and enqueues them at the  Figure <ref type="figure">6</ref>: Six Accumulo servers begin to saturate when 300 threads (spread on six clients) insert records at maximum speed using a 100 KB batch buffer (Figure <ref type="figure" target="#fig_4">4</ref>).</p><p>ZooKeeper server. The second client C2 dequeues keys inserted in the ZooKeeper queue and attempts to read the rows associated with those keys. We estimate the "readafter-write" time lag as the time difference between when C2 first attempts to read a key and when it first successfully reads that key. This under-estimates by the time from write at C1 to dequeue at C2 and over-estimates by the time in the ZooKeeper queue of the last unsuccessful read, but neither of these should be more than a few milliseconds. Figure <ref type="figure" target="#fig_7">7</ref> shows a cumulative distribution of the estimated time lag observed by client C2 for different batch sizes. This data excludes the (zero) time lag of keys that are read successfully the first time C2 tries to do so. Out of the 10,000 keys that C2 tries to read, less than 1% keys experience a non-zero lag when using a 10 KB batch in both HBase and Accumulo. The fraction of keys that experience a non-zero lag increases with larger batch sizes: 1.2% and 7.4% of the keys experience a lag for a 100 KB batch size in Accumulo and HBase respectively, 14% and 17% for a 1 MB batch size, and 33% and 23% for a 10 MB batch size. This fraction of keys that see non-zero lag increases with batch size because smaller batches fill up more quickly and are flushed to the server more often, while larger batches take longer to fill and are flushed less often.</p><p>For the developer or administrator of the table store, these tests give insight into the expected scale of delayed creates. For the smallest batch size (10 KB), HBase has a median lag of 100 ms and a maximum lag of 150 seconds, while Accumulo has an order of magnitude higher median (about 900 ms) and an order of magnitude lower maximum lag (about 10 seconds). However, the time lag for both table stores is similar for all larger batch sizes; the largest batch size (10 MB), for example, has a median lag of approximately 140 seconds and a maximum lag of approximately 200 seconds for both HBase and Accumulo.</p><p>For programmers of services that use table stores, it is important to observe that large batches may cause some keys to be visible more than 100 seconds after they were written by other clients. That is, with large batched writes, programmers must be prepared to cope with read-after-write time lags in the order of minutes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Table pre-splitting</head><p>Both HBase and Accumulo rely on a distributed B-tree that grows incrementally by splitting its leaf nodes (tablets) as the table grows. Because throughput degrades during tablet splits, these table stores provide an alternate interface to pre-split a table before it has much data. The goal for pre-splitting is (1) to migrate less data during ingest phase, and (2) to engage more tablet servers earlier on in an ingestheavy workload.</p><p>We extended YCSB++'s DB client API to offer an interface to pre-split a key range into N equal sub-ranges. The idea is that N predicts the future size of the tablets covering the key range in question. If N is too small, extra splits beyond the pre-splits will be done and not all tablet servers will be engaged early in the ingest work. If N is too large, tablets and their minor compactions will be numerous and small, leading to complex interactions with the major compaction policies.</p><p>Accumulo has a general interface for synchronously presplitting the tablets covering a range at specific key values. This is fast because multiple tablets in Accumulo can share the same (immutable) store files. In HBase, tablet pre-splitting is deferred until the next major compaction on that tablet, and then the tablet is divided into exactly two new tablets, optimizing the work of splitting and major compaction together. Unfortunately, when the YCSB++ DB client code invokes many pre-splits in HBase and the corresponding major compactions, which is an uncommon workload for HBase, it becomes unstable. The results in this section are all taken from Accumulo experiments.  It is tempting to evaluate ingest speed by the time until the last client returns from submitting the last row to the table store, but this underestimates the churn the table store may continue to experience as it splits and compacts tablets after the clients think ingest is complete. One way to measure this churn is through a light load of query and update operations after the inserts are done; YCSB++ enables this through multiple phase tests, using its multi-client coordination techniques to synchronize all client threads on the current phase.</p><p>The six phases used in our pre-split experiments are described in Table <ref type="table" target="#tab_1">2</ref>. Phase ( <ref type="formula">5</ref>) is a 300 second idle period designed to encourage a table store waiting for an idle period to do its pending work, so phase (6) repeats the light query and update load to expose the impact of such pending work. In these experiments, we use three YCSB++ clients and reduce the main memory each tablet server uses for memstore to 1 GB to engage compaction work more frequently.</p><p>Figure <ref type="figure">8</ref> shows the duration of Phase ( <ref type="formula">1</ref>), which loads 6 million rows with keys in the [0, 12 × 10 9 ] range into an empty table, and Phase (3), which loads 48 million rows with keys in the [0, 72 × 10 6 ] range into a table that is pre-split (in Phase ( <ref type="formula">2</ref>)) into different numbers of fixed-size partitions. This figure reports the "slowest" and the "fastest" completion times of the three YCSB++ clients used for this experiment. After pre-splitting the key range [0, 72 × 10 sized partitions the duration of Phase ( <ref type="formula">1</ref>) is reduced from from about 1,800 seconds to 1,600 seconds, and after presplitting it into 143 or more partitions the duration reduces even further to about 1,500 seconds -a 20% improvement in completion time. Pre-splitting the [0, 72 × 10 6 ] key range into less than 143 tablets does not inhibit most further splits because the split threshold is 256 MB in store files. But as little as 17 pre-splits ensures that all tablet servers are engaged during the subsequent insert phases. The duration of insert phases is only part of the effort expended by table stores. Phases (4), ( <ref type="formula">5</ref>) and ( <ref type="formula">6</ref>) explore the behavior after the insert phases. In Phase ( <ref type="formula">6</ref>), read latencies are all about 7-10 ms. However, read latency for operations in Phase (4), which happens immediately after the table attains 54 million rows, is dependent on compactions happening concurrently in the table store. Figure <ref type="figure" target="#fig_8">9</ref> shows the behavior during Phase (4) for the case when the range [0, 72 × 10 6 ] is pre-split into 143 equal-size ranges; this figure plots read latency with the number of compaction operations on the tablet servers (collected by the Otus performance monitor). In the first 60 seconds of this measurement phase, the read latency is always more than 500 ms and as high as 1,500 ms. These slow operations correlate with a large number of major compactions that keep tablet servers busy. As the measurement phase progresses and the compactions that the tablet servers want to do complete, read latencies start to decrease. After about 200 seconds, when the tablet servers are no longer performing any compactions, monitored read operations are taking about 7 ms, which corresponds to our observed read latencies for operations performed much later in Phase <ref type="bibr" target="#b5">(6)</ref>. This large variance in response time suggest that the policies and mechanisms of compactions, like defragmentation and cleaning in logstructured file systems <ref type="bibr" target="#b43">[44]</ref>, is important future work for table store developers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Bulk loading using Hadoop</head><p>YCSB++ uses an external Hadoop/MapReduce (MR) tool to benchmark bulk inserts in HBase and Accumulo. Similar to the previous section on pre-splitting tables, we analyze the performance of bulk insertions using an eightphase experiment shown in Table <ref type="table" target="#tab_4">3</ref>. The big difference be-   tween the experiments of Section 3.3 and those in this section is the replacement of an iterative call to insert one row many times with a MapReduce job that formats all data to be inserted into a native format, stored as on-disk store files, and one call to adopt these store files in a table in the store. For both HBase and Accumulo, our formatting tool generated store files for 36 tablets, enough to reliably load balance work to 6 tablet servers, but for HBase, a limit on the size of the native store files to be imported led us to generate 8 store files per tablet. For both table stores, the time to format a bulk load is much faster than the time to insert one row at a time; this is observed from comparing the durations in Figure <ref type="figure">8</ref> to durations from start to P2 and from end of P3 to end of P5 in Figure <ref type="figure" target="#fig_1">11</ref>. And since the adoption of native store files is also very fast, more interesting issues are observed in the measurement phases. Figure <ref type="figure" target="#fig_1">10</ref> shows the read latencies during all three measurement phases in this eight-phase experiment and Figure <ref type="figure" target="#fig_1">11</ref> reports the number of store files, tablets and compactions across all phases, but we distinctly highlight the end of Phase ( <ref type="formula">2</ref>), ( <ref type="formula">3</ref>), ( <ref type="formula">5</ref>) and <ref type="bibr" target="#b7">(8)</ref>. All experiments were run three times. For Accumulo all three runs were similar and one run in shown in the graphs. For HBase two runs were similar and one very different; we show one of each.</p><p>The two different types of runs experienced when using HBase are shown in Figure <ref type="figure" target="#fig_1">10</ref> and 11, (a) when HBase decides to split during the run, or (b) when HBase does not do any splits during the run. Figure <ref type="figure" target="#fig_1">11</ref>(b) confirms that splits and compactions are not happening in this run and the read response times are constant and low (20-30 ms after all data has been loaded). In fact, all the data is attached to the same tablet and served by one tablet server. Splits induce a lot of work and interference with the measurement workload, so the Y-axes of Figures <ref type="figure" target="#fig_1">10(a</ref>) and on the right in Figure <ref type="figure" target="#fig_1">11</ref>(a) change by more than an order of magnitude. The read response time immediately after all data has been inserted peaks at 5,000 ms and does not drop below 100 ms until about 12 minutes after the insertion is complete. One might conclude that the insertion takes at least 12 minutes longer than just building and inserting the store files, or close to 25 minutes, almost as long the fastest pre-split experiment's insertion and post-insertion compactions took (in Section 3.3).</p><p>When this test is run on the Accumulo table store, splitting and compacting is more aggressive and consistent. The interference to read response time is larger in the first measurement phase, but by 3 minutes into the second measurement phase, the splitting and compaction is done, allowing the entire load to be complete in less than one-third of the time of HBase run experiencing splits. This experiment em-phasizes the importance of the policies managing splits and compactions to performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Server-side filtering</head><p>By default, read or scan operations in YCSB return all the columns associated with the respective row(s). For a large table with thousands of columns, clients may get much more data (than what they are interested in) resulting in high data processing and network transfer overheads. Filtering at the tablet servers helps minimize this overhead by not returning irrelevant data to the client.</p><p>To understand the effectiveness of server-side filtering, we use a YCSB++ test to create a data-set that has 100 times more data per row than the data-set used in prior tests: each row has 10 times as many cells (total of 100 cells) and each cell is 10 times larger in size (total of 1 KB). Moreover, each column has a dedicated locality group. This test's workload issues scan requests for one cell from each of 1, 10, 100, or 1000 rows at a randomly selected row key (this is the third type of filter described in Section 2.3.5 that returns the row's key and the {column name, cell value} tuple). We refer to these values as the "scan length" and report the client-perceived scan throughput in terms of number of rows received by the client every second. All results in this section are computed as an average of three runs and have very small variance.</p><p>Figure <ref type="figure" target="#fig_0">12</ref> shows that server-side filtering in Accumulo drastically improves client throughput only for a scan length of 1,000 rows. In fact, for all smaller scan-lengths, filtering performs much worse than without server-side filtering -a phenomenon that arises from Accumulo's scan implemen- tation. Accumulo uses a scanner object to return results of a scan operation. A scanner object, by default, can hold 1,000 rows. An Accumulo tablet server returns a scanner object to the client only when the object is filled (1,000 rows). Consequently, even if a scan request wants only a single row, which is the case for scan length 1 in Figure <ref type="figure" target="#fig_0">12</ref>, the tablet servers will continue to scan table data until the scanner object is filled. As a result, server-side filtering exacerbates load on the servers, especially for scan lengths of 1 and 10 rows, because the server has to read and filter more rows to fill the single scanner object that was requested. Instead of using scanner objects with the default size, we modified Accumulo's DB client API in YCSB++ to allow our test to size scanner objects to the expected scan length. This modification, titled in Figure <ref type="figure" target="#fig_0">12</ref> as "filtered (buffer scaled)", decreases the unneeded scanning load on the servers and results in a significant improvement for smaller scan lengths of 1 and 10 rows.</p><p>We repeat this experiment to study server-side filtering in HBase. Figure <ref type="figure" target="#fig_3">13</ref> shows that HBase, similar to Accumulo, does not benefit from server-side filtering when scan lengths are smaller than 100 rows, but filtering improves the throughput by 10 times for scan length of 1,000 rows. We also observe that HBase does not require batch size manipulation because it performs less aggressive prefetching than Accumulo.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Benchmarking access control</head><p>Because only Accumulo supports fine grained access con- trol, we could not perform a comparison with HBase. <ref type="foot" target="#foot_3">3</ref> However, YCSB++ enables testing of the costs associated with fine-grained access control in Accumulo.</p><p>Although fine-grained mechanisms like cell-level access control provide great flexibility for data security, they come at the cost of additional performance overhead: the first overhead stems from higher disk and network traffic for each access and the second overhead stems from computationally verifying credentials on each access. Both of these overheads are dependent on the size of the ACLs in terms of number of users and groups.</p><p>Although Accumulo contains optimizations for ACLs that are frequently reused, we setup experiments to benchmark the worst-case performance by using a unique ACL for each key and by making the size of the ACL three times larger than the rest of the cell itself, as shown in Table <ref type="table" target="#tab_5">4</ref>. For this experiment, we use two benchmarks -an insert workload that writes 48 million single-cell rows in an empty table and a scan workload that scans 320 million rows. Two different client configurations -one with a single client with 100 threads and other with six clients with 16 threads eachgenerate load on a 6-node Accumulo cluster. We report an average of three runs (and standard deviation) for each configuration.</p><p>Figure <ref type="figure" target="#fig_4">14</ref> shows the insert throughput, measured as the number of rows inserted per second, for different numbers of entries in each ACL (while the total size of the ACLs is constant). A value of zero entries means that no security was used. When the workload uses a single client with 100 threads, we observe that the throughput decreases with increasing number of entries in each ACL: in comparison to not using any access control, throughput drops by 24% with 4 entries in the ACL and by as much as 47% with an 11-entry ACL. This happens because the single YCSB++ client is running at almost 100% CPU utilization (as shown in Figure <ref type="figure" target="#fig_5">15</ref>) and increasing the number of entries in each ACL leads to increased computation overhead. However, using six YCSB++ clients with 16 threads each, reduces the insert throughput only by about 10%, even when there are 11 entries in the ACL.</p><p>Figure <ref type="figure" target="#fig_13">16</ref> shows the scan throughput, measured as the number of rows scanned per second, for varying number of entries in each ACL. Unlike the insert throughput, we observe that the scan throughput is not affected by using different client configurations. However, in both cases, the scan throughput drops by about 45% once fine-grained ACLs are invoked and remains same for different number of entries in an ACL. In Figure <ref type="figure" target="#fig_14">17</ref>, Otus performance monitoring shows that this degradation results from a four-fold increase in the amount on data sent from the Accumulo tablet servers to  the clients, even though the clients do not always have use for the ACLs after they are allowed data access. <ref type="foot" target="#foot_4">4</ref> In these experiments, we see that extremely complex fine-grained access controls can impact performance significantly but "only" by a factor of two.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RELATED WORK</head><p>To the best of our knowledge, this is the first work to propose systematic benchmarking techniques for advanced functionality in table stores. All advanced features of table stores discussed in this paper are inspired by decades of research and implementation in traditional databases. We focus this section on work in scalable and distributed table stores.</p><p>Weak consistency: Various studies have measured the performance impact of weaker consistency semantics used by different table stores and service providers <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b50">51]</ref>. Using a first read-after-write measurement similar to YCSB++, one study has found that 40% of reads return inconsistent results when issued right after a write <ref type="bibr" target="#b33">[34]</ref>. Other studies found that Amazon SimpleDB's eventually consistent model may cause users to experience stale reads and inter-item inconsistencies <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b50">51]</ref>. Unlike our approach which measures the time to the first successful read, the SimpleDB study also checked if subsequent reads returned stale values <ref type="bibr" target="#b50">[51]</ref>. In contrast to SimpleDB's eventual consistency which stems   from divergent replicas, both HBase and Accumulo experience weak consistency only when batch writing is enabled at the clients. Orthogonal approaches to understand weak consistency include theoretical models <ref type="bibr" target="#b30">[31]</ref> and algorithmic properties <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19]</ref>.</p><p>Ingest-intensive optimizations: The use of an external Hadoop job to format and store massive data-sets in a native tabular form understood by the table store servers has been exploited for HBase <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b27">28]</ref>. An alternate approach adopted by the PNUTS system is to use an optimization-based planning phase before inserting the data <ref type="bibr" target="#b46">[47]</ref>. This phase allows the system to gather statistics about the data-set that may lead to efficient splitting and balancing. Such an approach could be used to complement the Hadoop-based bulk load tool used in YCSB++.</p><p>Server-side filtering: Function shipping in databases has long been used in parallel database <ref type="bibr" target="#b17">[18]</ref> and this idea has appeared in active disks (in a single-node setting) <ref type="bibr" target="#b41">[42]</ref>, MapReduce (in cloud computing) <ref type="bibr" target="#b15">[16]</ref> and key-value stores (in widearea networks) <ref type="bibr" target="#b20">[21]</ref>. Because the Hadoop/MapReduce framework is built on the premise of collocating compute and data, both HBase and BigTable have proposed the use of coprocessors to allow application level code to run on the tablet servers <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b39">40]</ref>. The YCSB++ approach to testing server-side filtering focuses on regular expression based filters rather than the general abstractions proposed by HBase <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b39">40]</ref>.</p><p>Voldemort Key-Value Store <ref type="bibr" target="#b2">[3]</ref> Apache Cassandra <ref type="bibr" target="#b0">[1]</ref> MongoDB <ref type="bibr" target="#b1">[2]</ref> Weak consistency Eventual consistency semantics resulting from divergent replicas Eventual consistency semantics resulting from divergent replicas Default mode: strong data consistency (can support weak consistency for high performance)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bulk loading</head><p>No support for bulk loading Provides an interface for bypassing the RPC marshalling process and directly bulk loading into Cassandra's memtable format <ref type="bibr" target="#b8">[9]</ref> Custom bulk-load commands to import data from different file formats; no API support to directly create MongoDB data files </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION AND FUTURE WORK</head><p>Scalable table stores started with simple data models, lightweight semantics and limited functionality. Today, they feature a variety of performance optimizations, such as batch write-behind, tablet pre-split, bulk loading, and server-side filtering, as well as enhanced functionality, such as per-cell access control. Coupled with complex deferring and asynchronous online re-balancing policies, these optimizations have performance implications that are neither assured nor simple to understand, and yet are important to the goals of high ingest rate, secure scalable table stores.</p><p>Benchmarking tools like YCSB <ref type="bibr" target="#b13">[14]</ref> help with basic, singlephase workload testing of the core create-read-update-delete interfaces, but lack support for benchmarking and performance debugging advanced features. In this work, we extended YCSB's modular framework to integrate support for advanced feature testing. Our tool, called YCSB++, is a distributed multi-phase YCSB with an extended abstract table API for pre-splitting, bulk loading, server side filtering, and applying cell-level access control lists.</p><p>For more effective performance debugging, YCSB++ exposes its internal statistics to an external monitor, like Otus, where they are correlated with statistics from the table store under test and system services like file systems and MapReduce job control. Collectively comparing metrics of internal behaviors of the table store (such as compactions), the benchmark phases, and the network and CPU usage of each service, yields a powerful tool for understanding and improving scalable table store systems.</p><p>Although we evaluated YCSB++ with only two table stores, HBase and Accumulo, which have multi-dimensional distributed sorted map structures, we believe that YCSB++ can be used to test other table stores as well. Table <ref type="table" target="#tab_7">5</ref> shows the applicability of different YCSB++ extensions to three other table stores, Voldemort, Cassandra and MongoDB, that were originally supported by YCSB.</p><p>Testing weak consistency is one feature that is important for all three table stores. Although weak consistency in HBase and Accumulo stems from client-side buffering, we were able to adapt YCSB++ to measure the read-afterwrite time lag for Voldemort and Cassandra where eventual, weak consistency can result from divergent replicas. Some features like table pre-splitting and fine-grained access control are not common; for example, pre-splitting is relevant only if the underlying distributed index supports incremental growth.</p><p>YCSB++ is publicly available under the Apache License at http://www.pdl.cmu.edu/ycsb++/ for researchers and developers to use it to benchmark their table store deployments and extend it to facilitate testing of other advanced features.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>Figure 1: Diagram of data flow in a tablet. Iterator Trees are shown on the minor compaction, major/me compaction, and query data flow paths.Full Context := ( row , cf , cq, vis, time, val ) Row Context := ( row , cf , cq, vis, time, val ) CF Context := ( row , cf , cq, vis, time, val ) CQ Context := ( row , cf , cq , vis, time, val ) Version Context := ( row , cf , cq , vis, time, val ) Time Context := ( row , cf , cq , vis, time, val ) Value Context := ( row , cf , cq , vis, time, val ) Figure 2: A set of useful contexts for operations on key/value pairs. Parentheses () represent a single tuple, and angle brackets represent an ordered sequence of tuples.</figDesc><graphic coords="3,317.81,54.60,237.53,133.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Design of the Accumulo tablet server and its use of iterators for bulk and stream processing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: YCSB++ functionality testing framework -Light colored boxes show modules in YCSB v0.1.3 [14] and dark shaded boxes show our new extensions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An example of combining different logical aggregates in Otus graphs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Effect of batch size on insert rate in a 6-node HBase and Accumulo cluster.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: A single client inserting records in a 6-node Accumulo cluster becomes CPU limited resulting in underutilized servers and low overall throughput (Figure 4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: CDF of non-zero read-after-write time lag estimations for different batch sizes. The parentheses show the fraction of tests that show non-zero time lag.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Understanding read latencies in Accumulo after ingest-intensive workloads and the correlations with compactions on its servers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :Figure 11 :</head><label>1011</label><figDesc>Figure 10: Read latency during the three measurement phases in our eight-phase bulk load experiments</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 12 :Figure 13 :</head><label>1213</label><figDesc>Figure 12: Performance of server-side filtering in Accumulo for varying scan lengths in terms of rows.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 14 :Figure 15 :</head><label>1415</label><figDesc>Figure 14: Insert throughput decreases with increasing number of ACL clauses when the CPU is a limiting resource.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 16 :</head><label>16</label><figDesc>Figure16: For both client configurations, scan performance drops by about 45% when Accumulo enables per-cell access control and the throughput is not affected by the number of entries in the ACL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 17 :</head><label>17</label><figDesc>Figure 17: Significantly more bytes are sent from Accumulo servers to YCSB++ clients while scanning records with access control lists.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Six-phase experiment used to study the effects of pre-splitting in HBase and Accumulo.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Effect of pre-splitting the [0, 72×10 6 ] key range in an Accumulo table into into varying number of equal-sized ranges on completion time of ingest-intensive workloads.</figDesc><table><row><cell></cell><cell cols="6">Load 48M rows in after 6M rows preloaded</cell></row><row><cell>Completion time (seconds)</cell><cell>0 400 800 1200 1600 2000</cell><cell></cell><cell></cell><cell></cell><cell>Pre-load (slowest) Pre-load (fastest) Load (slowest) Load (fastest)</cell></row><row><cell></cell><cell>0</cell><cell>1 7 3 5</cell><cell>5 9</cell><cell>1 4 3</cell><cell>1 9 1</cell><cell>3 8 3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Number of splits added</cell></row><row><cell cols="2">Figure 8:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p><p>6 </p>] into 17 equal-</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Eight-phase experiment used to understand bulk loading in HBase and Accumulo.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Sizes of attributes associated with each cell used for ACL benchmarking.</figDesc><table><row><cell cols="2">Attributes of each cell Attribute size</cell></row><row><cell>Row Key Column family Column</cell><cell>12 bytes 3 bytes 6 bytes</cell></row><row><cell>ACL Value Timestamp</cell><cell>100 bytes 2 bytes 8 bytes</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Applicability of YCSB++ extensions for other scalable stores supported by the original YCSB distribution<ref type="bibr" target="#b13">[14]</ref>.</figDesc><table><row><cell>Table pre-splitting</cell><cell>Uses consistent hashing-based data partitioning that randomly pre-splits hash ranges across all server</cell><cell>Allows an pre-splitting ranges us-ing different partitioner modes (using both the key and the hash</cell><cell>Allows pre-splitting based on a continuous key-range, followed by load-balancing the pre-split</cell></row><row><cell></cell><cell>nodes (in an a priori manner); users cannot control this partitioning</cell><cell>of the key); but to avoid hot-spots, the node tokens may need to be</cell><cell>ranges</cell></row><row><cell></cell><cell></cell><cell>constantly adjusted</cell><cell></cell></row><row><cell>Server-side filter-</cell><cell>Simple key-value data model with no filtering support</cell><cell>Filtering based on column names and key values; no support for</cell><cell>Enables clause filtering on the servers SQL-style "where"</cell></row><row><cell>ing</cell><cell></cell><cell>user-defined matching for value-based filtering</cell><cell></cell></row><row><cell>Fine-</cell><cell>No access control support</cell><cell>Extensible authorization to con-</cell><cell>No access control support (only</cell></row><row><cell>grained ACLs</cell><cell></cell><cell>trol read/writes to a column fam-ily (no cell-level ACLs [10])</cell><cell>simple user authentication for the DB)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>An open-source release of Accumulo has been offered to the Apache Software Foundation.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>Context is not really defined for multiple sources. case we have a single key/value pair at a time from source, but the row s might not match between them</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>HBase, when configured with 50 threads per client, was unable to complete the test successfully without crashing any server during the test.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3"><p>HBase plans to add security in future releases<ref type="bibr" target="#b29">[30]</ref> </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4"><p>Some users of Accumulo use ACLs to convey interesting information to the reader.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The work in this paper is based on research supported in part by the National Science Foundation under award CCF-1019104, by the Betty and Gordon Moore Foundation, by the Qatar National Research Fund under award number NPRP 09-1116-1-172, and by grants from Google and Yahoo!. We also thank the members and companies of the PDL Consortium (including APC, EMC, Facebook, Google, Hewlett-Packard, Hitachi, IBM, Intel, Microsoft, NEC, Ne-tApp, Oracle, Panasas, Riverbed, Samsung, Seagate, STEC, Symantec, and VMware) for their interest, insight, feedback, and support.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Apache</forename><surname>Cassandra</surname></persName>
		</author>
		<ptr target="http://cassandra.apache.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<ptr target="http://www.mongodb.org/" />
		<title level="m">MongoDB</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<ptr target="http://project-voldemort.com/" />
		<title level="m">Project Voldemort: A distributed database</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Consistability: Describing usually consistent systems</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Aiyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Wylie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 4th Workshop on Hot Topics in Syetms Dependability (HotDep &apos;2008)</title>
		<meeting>of the 4th Workshop on Hot Topics in Syetms Dependability (HotDep &apos;2008)<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-12">December 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Parallel bulk Insertion for large-scale analytics applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Barbuzzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Michiardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Biersack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Boggia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 4th ACM SIGOPS/SIGACT International Workshop on Large Scale Distributed Systems and Middleware (LADIS &apos;2010)</title>
		<meeting>of the 4th ACM SIGOPS/SIGACT International Workshop on Large Scale Distributed Systems and Middleware (LADIS &apos;2010)<address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-07">July 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Borthakur</surname></persName>
		</author>
		<ptr target="http://hadoop.apache.org/core/docs/r0.16.4/hdfsdesign.html" />
		<title level="m">The Hadoop Distributed File System: Architecture and Design</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Towards robust distributed systems</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Brewer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Keynote at the 19th Annual ACM Symposium on Principles of Distributed Computing (PODC &apos;2000) on July 19, 2000 in Portland OR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Cafarella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fikes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Halevy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Madhavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Muthukrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Management Projects at Google. SIGMOD Record</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Cassandra&apos;s Binary Memtable</title>
		<author>
			<persName><forename type="first">Cassandra</forename></persName>
		</author>
		<ptr target="http://wiki.apache.org/cassandra/BinaryMemtable" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Cassandra&apos;s Extensible Authentication/Authorization</title>
		<author>
			<persName><forename type="first">Cassandra</forename></persName>
		</author>
		<ptr target="http://wiki.apache.org/cassandra/ExtensibleAuth" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Cattell</surname></persName>
		</author>
		<ptr target="http://www.cattell.net/datastores/Datastores.pdf" />
		<title level="m">Scalable SQL and NoSQL Data Stores</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bigtable: A Distributed Storage System for Structured Data</title>
		<author>
			<persName><forename type="first">F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">C</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Burrows</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fikes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gruber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 7th USENIX Symposium on Operating Systems Design and Implementation</title>
		<meeting>of the 7th USENIX Symposium on Operating Systems Design and Implementation<address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-11">2006. November 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<ptr target="http://collectd.org/" />
		<title level="m">Collectd: The system statistics collection daemon</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Benchmarking cloud serving systems with YCSB</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">F</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Silberstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sears</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 1st ACM Symposium on Cloud Computing (SOCC &apos;2010)</title>
		<meeting>of the 1st ACM Symposium on Cloud Computing (SOCC &apos;2010)<address><addrLine>Indianapolis, IN</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-06">June 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><surname>Designs</surname></persName>
		</author>
		<ptr target="http://www.cs.cornell.edu/projects/ladis2009/talks/dean-keynote-ladis2009.pdf" />
		<title level="m">Lessons and Advice from Building Large Distributed Systems. Keynote at the 3rd ACM SIGOPS International Workshop on Large Scale Distributed Systems and Middleware (LADIS &apos;2009) on October</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">MapReduce: Simplified Data Processing on Large Clusters</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 6th USENIX Symposium on Operating Systems Design and Implementation (OSDI &apos;2004)</title>
		<meeting>of the 6th USENIX Symposium on Operating Systems Design and Implementation (OSDI &apos;2004)<address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-12">December 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dynamo: Amazon&apos;s Highly Available Key-Value Store</title>
		<author>
			<persName><forename type="first">G</forename><surname>Decandia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hastorun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kakulapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lakshman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pilchin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sivasubramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vosshall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Vogels</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 21st ACM Symposium on Operating Systems Principles (SOSP &apos;2007)</title>
		<meeting>of the 21st ACM Symposium on Operating Systems Principles (SOSP &apos;2007)<address><addrLine>Stevenson, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-10">October 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Parallel database systems: the future of high performance database systems</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Dewitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Consistency Models for Replicated Data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fekete</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ramamritham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Replication</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">5959</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Storage Architecture and Challenges. Talk at the Google Faculty Summit</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fikes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010-07-29">2010 on July 29, 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Comet: An Active Distributed Key-Value Store</title>
		<author>
			<persName><forename type="first">R</forename><surname>Geambasu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kohno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 9th USENIX Symposium on Operating Systems Design and Implementation (OSDI &apos;2010)</title>
		<meeting>of the 9th USENIX Symposium on Operating Systems Design and Implementation (OSDI &apos;2010)<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-10">October 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Brewer&apos;s conjecture and the feasibility of consistent, available, partition-tolerant web services</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lynch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGACT News</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Partitioned B-trees: A user&apos;s guide</title>
		<author>
			<persName><forename type="first">G</forename><surname>Graefe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 10th Conference on Database Systems for Business, Technology and Web (BTW &apos;2003)</title>
		<meeting>of the 10th Conference on Database Systems for Business, Technology and Web (BTW &apos;2003)<address><addrLine>Leipzig, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-02">February 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">B-tree indexes for high update rates</title>
		<author>
			<persName><forename type="first">G</forename><surname>Graefe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGMOD Record</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fast Loads and Queries</title>
		<author>
			<persName><forename type="first">G</forename><surname>Graefe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kuno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions on Large-Scale Data-and Knowledge-Centered Systems II</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">6380</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName><forename type="middle">Apache</forename><surname>Hadoop</surname></persName>
		</author>
		<author>
			<persName><surname>Hadoop</surname></persName>
		</author>
		<ptr target="http://hadoop.apache.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName><forename type="middle">Apache</forename><surname>Hbase</surname></persName>
		</author>
		<author>
			<persName><surname>Hbase</surname></persName>
		</author>
		<ptr target="http://hbase.apache.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><surname>Hbase</surname></persName>
		</author>
		<author>
			<persName><surname>Hbase</surname></persName>
		</author>
		<ptr target="http://hbase.apache.org/docs/r0.89.20100621/bulk-loads.html" />
		<title level="m">Bulk Loads in HBase</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">ZooKeeper: Wait-free Coordination for Internet-scale Systems</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Konar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">P</forename><surname>Junqueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Reed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2010 USENIX Annual Technical Conference (USENIX ATC &apos;2010)</title>
		<meeting>of the 2010 USENIX Annual Technical Conference (USENIX ATC &apos;2010)<address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-06">June 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Kootz</surname></persName>
		</author>
		<ptr target="http://hbaseblog.com/2010/10/11/secure-hbase-access-controls/" />
		<title level="m">The HBase Blog -Secure HBase: Access Controls</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Kraska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hentschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kossmann</surname></persName>
		</author>
		<title level="m">Consistency Rationing in the Cloud: Pay only when it matters. Proc. of the VLDB Endowment</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Lai</surname></persName>
		</author>
		<ptr target="http://hbaseblog.com/2010/11/30/hbase-coprocessors/" />
		<title level="m">HBase Coprocessors</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cassandra -A Decentralized Structured Storage System</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lakshman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 3rd ACM SIGOPS International Workshop on Large Scale Distributed Systems and Middleware (LADIS &apos;2009)</title>
		<meeting>of the 3rd ACM SIGOPS International Workshop on Large Scale Distributed Systems and Middleware (LADIS &apos;2009)<address><addrLine>Big Sky, MT</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-10">October 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">CloudCmp: Comparing Public Cloud Providers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kandula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 9th ACM SIG-COMM Conference on Internet Measurement (IMC &apos;2009)</title>
		<meeting>of the 9th ACM SIG-COMM Conference on Internet Measurement (IMC &apos;2009)<address><addrLine>Chicago, IL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-11">November 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<ptr target="http://docs.outerthought.org/lily-docs-current/438-lily.html" />
		<title level="m">Bulk Imports in Lily</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="http://huanliu.wordpress.com/2010/03/03/the-cost-of-eventual-consistency/" />
		<title level="m">The cost of eventual consistency</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The Ganglia Distributed Monitoring System: Design, Implementation And Experience</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Massie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Culler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Parallel Computing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The logstructured merge-tree (LSM-tree)</title>
		<author>
			<persName><forename type="first">P</forename><surname>O'neil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gawlick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>O'neil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Informatica</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Munin: Graphisches Netzwerk-und System-Monitoring</title>
		<author>
			<persName><forename type="first">G</forename><surname>Pohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Renner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Open Source Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Coprocessors: Support small query language as filter on server side</title>
		<author>
			<persName><forename type="first">A</forename><surname>Purtell</surname></persName>
		</author>
		<ptr target="https://issues.apache.org/jira/browse/HBASE-1002" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Otus: Resource Attribution in Data-Intensive Clusters</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>López</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gibson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2nd International Workshop on MapReduce and its Applications (MapReduce &apos;2011)</title>
		<meeting>of the 2nd International Workshop on MapReduce and its Applications (MapReduce &apos;2011)<address><addrLine>San Jose, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06">June 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Active Disks for Large-Scale Data Processing</title>
		<author>
			<persName><forename type="first">E</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nagle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Robidoux</surname></persName>
		</author>
		<ptr target="http://www.mssqltips.com/tip.asp?tip=1185" />
		<title level="m">Minimally Logging Bulk Load Inserts into SQL Server</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The Design and Implementation of a Log-Structured File System</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rosenblum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Ousterhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer Systems (TOCS)</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1992-08">August 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Use Cases for SciDB</title>
		<author>
			<persName><surname>Scidb</surname></persName>
		</author>
		<ptr target="http://www.scidb.org/use/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Beyond Relational Databases</title>
		<author>
			<persName><forename type="first">M</forename><surname>Seltzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Efficient Bulk Insertions into a Distributed Ordered Table</title>
		<author>
			<persName><forename type="first">A</forename><surname>Silberstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">F</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yerneni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2008 ACM SIGMOD International Conference on Management of Data (SIGMOD &apos;2008)</title>
		<meeting>of the 2008 ACM SIGMOD International Conference on Management of Data (SIGMOD &apos;2008)<address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-06">June 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">C-Store: A Column Oriented DBMS</title>
		<author>
			<persName><forename type="first">M</forename><surname>Stonebraker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Batkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cherniack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Madden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>O'neil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>O'neil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rasin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zdonik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Fractal Tree Indexing in TokuDB</title>
		<author>
			<persName><surname>Tokutek</surname></persName>
		</author>
		<ptr target="http://tokutek.com/technology/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Eventually Consistent</title>
		<author>
			<persName><forename type="first">W</forename><surname>Vogels</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Queue</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Data Consistency Properties and the Trade-offs in Commercial Cloud Storages: the Consumers&apos; Perspective</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fekete</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 5th Biennial Conference on Innovative Data Systems Research (CIDR &apos;2011)</title>
		<meeting>of the 5th Biennial Conference on Innovative Data Systems Research (CIDR &apos;2011)<address><addrLine>Asilomar, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-01">January 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title/>
		<author>
			<persName><forename type="middle">Apache</forename><surname>Zookeeper</surname></persName>
		</author>
		<author>
			<persName><surname>Zookeeper</surname></persName>
		</author>
		<ptr target="http://zookeeper.apache.org/" />
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
