<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Tracking continuous emotional trends of participants during affective dyadic interactions using body language and speech information ☆</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Angeliki</forename><surname>Metallinou</surname></persName>
							<email>metallin@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Signal Analysis and Interpretation Lab (SAIL)</orgName>
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Athanasios</forename><surname>Katsamanis</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Signal Analysis and Interpretation Lab (SAIL)</orgName>
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shrikanth</forename><surname>Narayanan</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Signal Analysis and Interpretation Lab (SAIL)</orgName>
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Tracking continuous emotional trends of participants during affective dyadic interactions using body language and speech information ☆</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">47A1D52EC13149661CA628C8263011DE</idno>
					<idno type="DOI">10.1016/j.imavis.2012.08.018</idno>
					<note type="submission">Received 29 October 2011 Received in revised form 7 July 2012 Accepted 22 August 2012</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Continuous emotion tracking Dimensional emotional descriptions Gaussian Mixture Model mapping Body language Improvised dyadic interactions</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We address the problem of tracking continuous levels of a participant's activation, valence and dominance during the course of affective dyadic interactions, where participants may be speaking, listening or doing neither. To this end, we extract detailed and intuitive descriptions of each participant's body movements, posture and behavior towards his interlocutor, and speech information. We apply a Gaussian Mixture Model-based approach which computes a mapping from a set of observed audio-visual cues to an underlying emotional state. We obtain promising results for tracking trends of participants' activation and dominance values, which outperform other regression-based approaches used in the literature. Additionally, we shed light into the way expressive body language is modulated by underlying emotional states in the context of dyadic interactions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human expressive communication is characterized by the continuous interplay of multimodal information, such as facial, vocal and bodily gestures, which may convey the participant's affect. The affective state of each participant can be seen as a continuous variable that evolves with variable intensity and clarity over the course of an interaction. It can be described by certain continuous attributes (dimensions): activation, valence and dominance. Activation describes how intense is the emotional experience, valence describes the level of pleasure related to an emotion, and takes positive and negative values for pleasant and unpleasant emotions respectively, while dominance describes the level of control of a person during an emotional experience. This approach was introduced in psychology research based on evidence that humans may perceptually use such a representation to evaluate emotional situations <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref>. It may also be a more generic way to classify emotions, especially for emotional manifestations that may not have a clear categorical description.</p><p>This work addresses the problem of continuous tracking of activation, valence and dominance, when they are considered to be continuously valued. Our goal is to obtain a continuous description of each participant's underlying emotional state through the course of an improvised dyadic interaction. Our experimental setup is generic; participants express a wide variety of emotions that are not pre-defined but are elicited through their interaction, and have varying roles throughout the performance <ref type="bibr">(speaker, listener, neither)</ref>. This approach has the potential to shed light into the temporal dynamics of emotions through an interaction and highlight regions where abrupt emotional change happens. These could be viewed as regions of emotional saliency.</p><p>Our contributions could be summarized as follows:</p><p>1. We present a statistical framework to dynamically track the emotional content that is displayed over time by participants of an interaction, using bodily and vocal information. 2. We systematically examine how body language behavior is modulated by underlying emotional states in dyadic interactions. 3. We discuss the data annotation design for continuous ratings, which is a challenging problem in itself.</p><p>We apply a Gaussian Mixture Model (GMM) based methodology, originally introduced in <ref type="bibr" target="#b3">[4]</ref>, to compute an optimal statistical mapping between an underlying emotional state and an observed set of audiovisual features, both evolving through time. Extending our previous work <ref type="bibr" target="#b4">[5]</ref>, we formulate the emotion tracking problem at various time resolutions, to investigate the effect of the tracking detail on the final performance. For our experiments, we use the USC Creative IT database which contains detailed full body Motion Capture (MoCap) information in the context of expressive theatrical improvisations <ref type="bibr" target="#b5">[6]</ref>. We extract a variety of psychology-inspired body language features describing each participant's body language and relative interaction behaviors with respect to their interlocutor. We systematically examine the relevant emotional content of each feature to select body language feature sets tailored to each emotional attribute. In addition to emotion tracking, our goal is to examine the way expressive body language is modulated in order to reflect different emotional states. This allows us to revisit qualitative psychological observations from a quantitative perspective.</p><p>Finally, the data annotation design is an important part of the data preparation, since continuous tagging is a challenging task and often results in low inter-evaluator agreement. Our annotation results show that people tend to agree more on the trends rather than the absolute values of emotional attributes. This suggests that humans find it more straightforward to define emotions in relative (e.g., more activated, more dominant), rather than absolute terms (similar observations are described in <ref type="bibr" target="#b6">[7]</ref>).</p><p>Our experimental results indicate that we are better at tracking changes in emotional attributes rather than the absolute values themselves, following a similar trend as the human annotations. Furthermore, the proposed GMM based tracking method outperforms other examined methods, in terms of correlation-based performance metrics (estimating trends of attributes). For activation trends, the tracking performance is close to human agreement, while for dominance we achieve encouraging results. Body language seems to carry rich activation and dominance related information, reflected in features such as body and hand movements, orientation and approach-avoidance behaviors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>The use of dimensional representations of emotions has been adopted by many researchers but typically the dimensional values are quantized into discrete levels. However, a continuous representation may allow a more generic and flexible treatment of emotions. Examples of work that avoid discretizing the emotional dimensions include <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref> where regression approaches, such as Support Vector Regression (SVR), were used to estimate continuous dimensional attributes from speech cues of presegmented utterances.</p><p>Most of the existing literature, including works that focus on recognition of emotions as part of an emotion sequence <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>, presegment the time dimension into units for recognition, e.g., consecutive words or utterances. Few works have avoided segmenting the temporal dimension and have addressed the problem of continuously tracking emotions across time. For example, in <ref type="bibr" target="#b11">[12]</ref> the authors present continuous recognition of the emotional content of movies using a Hidden Markov Model (HMM) which classifies dimensional attributes into discrete levels.</p><p>A relatively small amount of literature treats both time and emotion variables as continuous. In <ref type="bibr" target="#b12">[13]</ref> the authors describe a multimodal system to continuously track valence and activation of a speaker, using SVR and Long-Short Term memory (LSTM) regression, with LSTM being the best performing approach. Similarly, single-modality systems were proposed in <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref> using SVR and LSTM neural networks for regression to continuously estimate valence and activation values from emotional speech. An unsupervised method for mapping the emotional content of movies in the valence-activation space was proposed in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref> using low-level audio and video cues. In our work, we propose a supervised, GMM-based methodology to continuously track an underlying emotional state using body language and speech information.</p><p>The use of multimodal information allows for a more complete description of the expressed emotion, therefore many works utilize both facial expressions and vocal cues <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>, while an increasing amount of recent literature investigates body language. In <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref> the authors use upper body language information along with facial expressions to recognize emotions, while in <ref type="bibr" target="#b12">[13]</ref> shoulder movement cues were used along with facial and vocal cues for continuous emotion tracking. In <ref type="bibr" target="#b21">[22]</ref> authors investigate a variety of upper body descriptions of movement and symmetry in order to extract a minimal representation of affective gestures. Works that examine affective full body language include <ref type="bibr" target="#b22">[23]</ref> where authors advantageously use full body motion cues, alongside facial and vocal information, and <ref type="bibr" target="#b23">[24]</ref> where authors use features describing movement quality to classify basic emotional states. In <ref type="bibr" target="#b24">[25]</ref>, authors use the setup of a body-movement-based videogame and recognize emotions such as defeat, triumph etc., using MoCap derived features. Few works have addressed body language behavior in the context of social interaction, for example the work in <ref type="bibr" target="#b25">[26]</ref>, that examines dominance and synchronization phenomena during collaborative social tasks, and <ref type="bibr" target="#b26">[27]</ref> where measures of posture are used to examine approach-avoidance behaviors during the interaction of two seated participants.</p><p>Various body language feature sets have been proposed in the literature, ranging from lower-level features such as joint angles <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b27">28]</ref>, to more interpretable features such as distances and angles between body parts <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref> and this work, to higher-level posture and movement properties (contraction index, smoothness/fluidity of motion) <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b23">24]</ref>. An overview of various body language features in the literature can be found in <ref type="bibr" target="#b30">[31]</ref>. In this work we extract a large set of interpretable body language features, which measure properties of a person's posture, motion, and body behavior with respect to the interlocutor. Although there seems to be no standard feature set for body language, several body language features in the literature measure similar qualities. For example, in <ref type="bibr" target="#b28">[29]</ref> authors measure horizontal and vertical distances between a subject's hands and shoulder, while here we compute the relative positions of a person's hands with respect to his torso.</p><p>Our work lies in the intersection of many of the above areas; we address the issue of emotion tracking when both the emotion and time dimensions are continuous, using full body language features and speech information. Body language is examined in the context of affective dyadic interactions. Additionally, our setup is generic; the examined subjects are not restricted to produce specific emotions or body gestures. On the contrary, through their improvisation a wide variety of emotional states, body language gestures and interaction dynamics are elicited in a naturalistic manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Framework overview</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>Fig. <ref type="figure" target="#fig_0">1</ref> presents a summary of our work. As illustrated in the left of Fig. <ref type="figure" target="#fig_0">1</ref>, our study relies on video, audio and MoCap data collected from two actors engaged in emotional dyadic improvisations. The center part of Fig. <ref type="figure" target="#fig_0">1</ref> describes the data processing, specifically the extraction of detailed body language and speech information from both participants, as well as the data annotation. Data annotation was performed by multiple human evaluators who were asked to continuously rate the perceived valence, activation and dominance levels of each participant during each interaction. The result is multiple emotional curves which are averaged to provide the ground truth for further experiments. After these steps, we have available for each participant various body language features x body extracted throughout the interaction, speech features x speech extracted from regions where that person is speaking, and the corresponding emotional curves y. The joint distribution P(x,y) is modeled using a Gaussian Mixture Model (GMM), where x can be a visual or audiovisual feature vector and y is one of the three emotional attributes. The conditional distribution P(y|x) is also a GMM. The GMM-based tracking approach consists of computing a mapping from the observed features to the underlying emotional curve by maximizing the conditional probability of the emotion given the features, e.g., ŷ ¼ argmaxP y x j Þ ð . In the right part of Fig. <ref type="figure" target="#fig_0">1</ref> we present an example of the resulting emotional curve estimate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Framework for continuous tracking of emotional states and emotional changes</head><p>Let x t denote the vector of body language and speech observations at time t of an interaction recording and y t be the underlying emotional attribute, namely activation, valence or dominance. One way to predict y t given x t would be by maximizing the corresponding conditional probability:</p><formula xml:id="formula_0">ŷt ¼ arg max y t P y t x t ; λ y;x ð Þ<label>ð1Þ</label></formula><p>assuming a specific model λ (y,x) for two concurrent instantiations of x and y. However, given the continuous nature of the involved variables, it would be beneficial to incorporate dynamic information in this estimation. This can be achieved by also jointly modeling the first and second temporal derivatives of y t and x t , denoted here as Δy t , Δ 2 y t and Δx t , Δ 2 x t respectively. By replacing y t with Y t ¼ y t ; Δy t ; Δ 2 y t h i T and x t with</p><formula xml:id="formula_1">X t ¼ x T t ; Δx T t ; Δ 2 x T t h i T</formula><p>, the optimal estimate ŷ ¼ y 1 ; …; y t ; …y T ½ of the emotional flow for the course of the interaction can be found as:</p><formula xml:id="formula_2">ŷ ¼ arg max y P Y X; λ Y;X ð Þ ;<label>ð2Þ</label></formula><p>where</p><formula xml:id="formula_3">X=[X 1 T ,X 2 T ,…,X t T ,…,X T T ] T is the sequence of the dynamic information-augmented features and Y=[Y 1 T ,Y 2 T ,…,Y t T ,…,Y T T</formula><p>] T the corresponding emotional attribute and its derivatives for the entire interaction. Following the paradigm that was originally introduced for voice conversion <ref type="bibr" target="#b31">[32]</ref>, we consider the model λ (Y,X) of the joint probability of (Y t ,X t ) to be a Gaussian Mixture Model (GMM):</p><formula xml:id="formula_4">P Y t ; X t λ Y;X ð Þ ¼ X M m¼1 a m N Y t T ; ; X t T h i T ; μ Y;X ð Þ m ; Σ Y;X ð Þ m<label>ð3Þ</label></formula><p>with a m , μ m (Y,X) and Σ m (Y,X) being each component's weight, mean and covariance respectively:</p><formula xml:id="formula_5">μ Y;X ð Þ m ¼ μ Y ð Þ m μ X ð Þ m " # ; Σ Y;X ð Þ m ¼ Σ YY ð Þ m Σ YX ð Þ m Σ XY ð Þ m Σ XX ð Þ m " # :<label>ð4Þ</label></formula><p>The conditional probability in Eq. ( <ref type="formula" target="#formula_2">2</ref>) can be written as <ref type="bibr" target="#b31">[32]</ref>:</p><formula xml:id="formula_6">P YjX; λ Y;X ð Þ ¼ ∑ over all m P mjX; λ Y;X ð Þ P YjX; m; λ Y;X ð Þ ≈ ∏ T t¼1 X M m¼1 P mjX t ; λ Y;X ð Þ P Y t jX t ; m; λ Y;X ð Þ<label>ð5Þ</label></formula><p>where m = [m 1 , …,m t , …,m T ] is a sequence of mixture components and:</p><formula xml:id="formula_7">P mjX t ; λ Y;X ð Þ ¼ a m N X t ; μ X ð Þ m ; Σ XX ð Þ m ∑ M i¼1 a i N X t ; μ X ð Þ i ; Σ XX ð Þ i<label>ð6Þ</label></formula><formula xml:id="formula_8">P Y t jX t ; m; λ Y;X ð Þ ¼ N Y t ; E Y ð Þ m;t ; D Y ð Þ m :<label>ð7Þ</label></formula><p>and:</p><formula xml:id="formula_9">E Y ð Þ m;t ¼ μ Y ð Þ m þ Σ YX ð Þ m Σ XX ð Þ-1 X t -μ X ð Þ m ;<label>ð8Þ</label></formula><formula xml:id="formula_10">D Y ð Þ m ¼ Σ YY ð Þ m -Σ YX ð Þ m Σ XX ð Þ-1 m Σ XY ð Þ m :<label>ð9Þ</label></formula><p>Estimation of the underlying emotional flow ŷ for the entire utterance can finally be achieved based on Eq. ( <ref type="formula" target="#formula_2">2</ref>) via Expectation Maximization as described in detail in <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b3">4]</ref>. The initial estimate is just the Minimum Mean Squared Error (MMSE) estimate based on the conditional probability distribution (5) without using dynamic information. Due to the use of dynamic information in the estimations, the final estimate at each time instant ends up being affected by the entire sequence of observations. It has been shown that in the case of a single Gaussian Model the incorporation of derivatives in an analogous scenario corresponds to fixed-lag Kalman smoothing <ref type="bibr" target="#b32">[33]</ref>. The lag depends on the window length 2L -1 over which the derivatives are approximated (second derivatives are computed by applying Eq. <ref type="bibr" target="#b9">(10)</ref> to the first derivatives):</p><formula xml:id="formula_11">Δy t ¼ ∑ θ¼L θ¼-L θ y tþθ -y t-θ À Á 2∑ θ¼L θ¼-L θ 2 :<label>ð10Þ</label></formula><p>This scheme has been successfully applied for voice conversion <ref type="bibr" target="#b31">[32]</ref>, lip movement-speech synchronization <ref type="bibr" target="#b33">[34]</ref> and acoustic to articulatory speech inversion <ref type="bibr" target="#b3">[4]</ref>. Speech inversion refers to the problem of recovering the underlying articulation during speech production from just the observed speech acoustics. In a similar way, herein, we are trying to recover the underlying emotional state as it is represented by activation, valence and dominance from the observed body language and speech observations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Database and annotation process</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Database description</head><p>We use the USC CreativeIT database which is a multimodal database that combines engineering and theatrical approaches <ref type="bibr" target="#b5">[6]</ref>. It contains a variety of dyadic theatrical improvisations and represents an opportunity to systematically study verbal and non-verbal expressions in affective interactions. Performances are either improvisations of scenes from theatrical plays or theatrical exercises where actors repeat sentences in a manner that conveys specific intent (e.g., accepting or rejecting behavior towards other). However, the actors were not instructed to produce specific emotions. Instead, we expect a variety of emotional expressions and interaction dynamics to occur as part of the performance. This design makes the emotional manifestations of the database especially challenging to analyze, since they are more subtle and diverse. The theatrical design was performed by a theater expert (director/teacher), and the participating actors were senior theater students, who first had to pass an audition. The performances were recorded under the guidance of the theater expert in order to ensure high quality performances. Further data collection details can be found in <ref type="bibr" target="#b5">[6]</ref>.</p><p>The database contains multimodal information from the vocal and body language behavior of the actors obtained through close-up microphones, Motion Capture (MoCap) cameras and HD cameras. Each actor wore a special suit and 45 MoCap markers were placed across his/her body, as illustrated in Fig. <ref type="figure" target="#fig_2">2</ref>(a) and (b). The performances were recorded by 12 Vicon MoCap cameras placed on the ceiling of the recording room, as well as two HD cameras located at each corner of the room. In this work we use data from 16 actors, 9 females and 7 males; 6 out of 8 dyads performed 6 improvisations, and the remaining two dyads performed 7 improvisations, resulting in a total of 50 improvisations. The extra improvisations were performed after the theater professor's request, who judged that those dyads' performances were excellent, and asked for an additional performance. Improvisations range from 2 to 10 min, while on average about 40% of an improvisation contains speech from one of the two participants. We capture audio-visual data from both actors in each improvisation, therefore we have a total of 100 actor-recordings. Our modeling is based on features which are extracted from MoCap and speech information. The videos of the performances have only been used for the data annotation process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Annotation process</head><p>The CreativeIT performances contain a variety of emotional manifestations. Each participant's emotional state is mapped into dimensional labels of activation, valence and dominance, which provide a continuous and generic description of the expressed emotions. Unlike speechcentric emotion databases (i.e., IEMOCAP <ref type="bibr" target="#b34">[35]</ref> and VAM <ref type="bibr" target="#b7">[8]</ref>), where it is common to segment a conversation into sentences as basic units for examining emotional content, in CreativeIT each performance is characterized by an unfolding flow of body gestures. This makes segmentation into sentences rather arbitrary. Therefore we decided to collect continuous annotations throughout each interaction, without segmenting the recordings, using the Feeltrace instrument, which allows real-time continuous annotation of video content <ref type="bibr" target="#b35">[36]</ref>. Annotations are collected for each emotional attribute and for both actors of each performance, by watching the corresponding video recordings.</p><p>The problem of emotional data annotation has been addressed in other works including <ref type="bibr" target="#b24">[25]</ref>, where authors measure evaluator agreement through repeatedly comparing evaluator subsets, and <ref type="bibr" target="#b36">[37]</ref> where the notion of implicit annotation is discussed. Here, the continuous nature of the annotation task represents an additional challenge in terms of obtaining agreement. Furthermore, recordings are long and require constant attention from the annotator, while the actors express a wide variety of emotions and have different roles throughout the interaction (speaker, listener, neither). Consequently, inter-annotator agreement is hard to achieve, as we observed in our previous work using a subset of the CreativeIT data <ref type="bibr" target="#b4">[5]</ref>. Similar challenges have also been reported in other engineering studies that use continuous annotations <ref type="bibr" target="#b11">[12]</ref>, or examine expressive body language using discrete labels <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>For our current study, we recruited psychology students, most of whom had previous experience in emotional annotation. Annotators were further trained by a short instruction session where Feeltrace was introduced and the definitions of activation, valence and dominance attributes were explained through examples. Annotators watched many recorded performances in advance in order to get an idea of the data. They performed their first annotations multiple times to familiarize themselves with Feeltrace and were later encouraged to perform each annotation as many times as needed until they were satisfied with the result. Since annotations are done real-time, there is expected to be a person-specific delay between the time that an event happens and when its emotional content is annotated. In order to reduce this delay,  we modified the Feeltrace interface so that annotators can focus on one attribute each time, rather than two attributes, as was initially proposed in <ref type="bibr" target="#b35">[36]</ref>. A snapshot of the modified Feeltrace interface for activation annotation is presented in Fig. <ref type="figure">3</ref>. The annotation is performed by moving the mouse, shown as a full circle, along the horizontal line, while watching the performance video in a separate window. 1 To further reduce person-specific delays, we also instructed annotators to watch each video multiple times and have a clear idea of the emotional content before starting the real-time annotation.</p><p>In Fig. <ref type="figure" target="#fig_3">4</ref> we present a segment of the activation annotations of an actor provided by three annotators, and their average which is used as the ground truth. Note that although annotators agree on the trends of the activation curve (mean correlation of 0.67), and recognize pronounced activation events, they do not agree on the actual activation values. Similar observations hold true for many of our obtained annotations. This suggests that people tend to agree more when describing emotions in relative terms, e.g., whether there has been an increase or decrease, rather than in absolute terms (an observation which agrees with the literature, e.g., <ref type="bibr" target="#b6">[7]</ref>). This motivated us to focus on the annotation trends, and to use correlation metrics, such as linear correlation, to measure evaluator agreement and the performance of the emotion tracking algorithms.</p><p>Seven annotators participated in total, rating overlapping portions of the database, so that each actor-recording would be rated by three or four people (88 out of the 100 actor-recordings were rated by 3 people). For computing the annotator correlations we set a cut-off threshold for defining acceptable annotator agreement. For each actor-recording, we take the union of all annotator pairs with linear correlations greater than the threshold; this annotator subset is used to compute the ground-truth for the corresponding actor-recording, by taking the average of the selected annotations. If no annotators are selected then we exclude that actor-recording from our analysis. Our threshold is empirically set to 0.45, which results in selecting 80, 84 and 73 actor-recordings for the activation, valence and dominance class respectively, out of 100 in total (the rest were excluded from further analysis). The annotator agreement measure is computed by first computing the mean of the correlations between the selected annotators per actor recording, and then computing the median over all actor recordings. Median annotator correlations reached 0.59, 0.62 and 0.60 for activation, valence and dominance respectively (these numbers are higher than the ones achieved by our previous annotation effort <ref type="bibr" target="#b4">[5]</ref>).</p><p>Our choice of averaging multiple annotations to provide ground truth is a common approach in the emotion recognition community, but could be problematic in cases when the actual attribute value is of interest, since different annotators often have different internal rating scales. Here we reduce the extent of this problem by focusing on the trends of the average curve; the trends of the evaluator curves are not affected as much by the mean operation and could be a more robust indicator of the underlying ground truth (see also Fig. <ref type="figure" target="#fig_3">4</ref>). However, effectively fusing multiple annotators' subjective judgments is an important research problem (e.g., see <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref>), and a direction for future research. Also, the issue of person-specific delays is a challenging issue and is worth further investigation in the future, e.g., by means of targeted experiments measuring such delays among annotators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Feature extraction and selection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Body language feature extraction</head><p>Our body language features are extracted from full body MoCap data (the performance videos are only used for data annotation). From now on, we will also refer to these MoCap features as visual features, since they are visually perceived. The choice of features is inspired by the psychology literature which indicates that behaviors such as looking at the interlocutor, approaching, touching, as well as body postures such as looking down, and hand gestures carry emotional information <ref type="bibr" target="#b41">[42]</ref>. The features are extracted for each person, and they are either absolute descriptions of a person's posture and movement, or relative descriptions of his body behavior with respect to his interlocutor (in the latter case data from both people are used for the feature extraction).</p><p>In total, we examine 53 body language features, extracted at the MoCap framerate (60 fps) and smoothed using a median filter. This comprehensive feature set is summarized in Table <ref type="table">1</ref>, and may contain correlated or redundant features; decorrelated feature subsets will be later chosen through feature selection. Features are extracted in a geometrical manner from the positions of the MoCap markers, by defining global and local coordinate systems and measuring 3D distances, velocities and angles. These features are potentially informative either individually or in combination with each other. Our modeling framework Fig. <ref type="figure">3</ref>. Screenshot of the modified Feeltrace interface. 1 Currently a one-dimensional version of Feeltrace is publicly available, in software Gtrace <ref type="bibr" target="#b38">[39]</ref>. This software became available when we were midway in our data annotation, and we decided to keep our own modified Feeltrace version for consistency. can exploit such feature relations, as explained in Section 6.1. The origin of the global coordinate system is roughly the center of the recording space, while local coordinate systems for each actor are defined using the four waist markers, as shown in Fig. <ref type="figure" target="#fig_2">2(c</ref>). The positions of the various body parts are illustrated in Fig. <ref type="figure" target="#fig_2">2(c)</ref>. For example, one's center is defined as the average of the four waist markers.</p><p>Certain features are particularly influenced by person-specific bodily characteristics. For example the z-coordinates of a person's upper and lower back, which may reflect crouching and sitting are influenced by the person's height. Therefore, features that are z-coordinate positions are normalized by dividing by the actor's median height in each recording. Additionally, features that are (x,y,z) positions of hands in one's coordinate system are normalized by dividing by the person's median arm length in each recording, measured by the median distance between shoulder and hand markers. All normalized features are denoted as 'norm' in Table <ref type="table">1</ref>. Apart from that, we do not perform any normalization of person-specific emotional variability, since our person-independent setup does not assume any prior information about the expressive characteristics of a test subject. Normalizing for such person-specific emotion variability would be an interesting future direction. Fig. <ref type="figure" target="#fig_6">5</ref> illustrates some example features. For instance, as shown in Fig. <ref type="figure" target="#fig_6">5</ref>(a), the position of one's center is measured in the global system to describe his location, while positions of one's hands are measured in his local coordinate system to describe his hand gestures. An individual's absolute velocity is computed from the movement of his center, while relative velocity is computed by projecting the velocity vector in the direction between the two participants (Fig. <ref type="figure" target="#fig_6">5(b)</ref>). A description of one's looking behavior relative to his interlocutor is computed from the angle between the orientation of one's head coordinate system and the direction between the heads of the participants (Fig. <ref type="figure" target="#fig_6">5(c</ref>)). A description of one's relative body orientation can be obtained similarly by looking at the people's waist coordinate systems instead. In Fig. <ref type="figure" target="#fig_6">5(d)</ref>, the angle between a person's spine and his local z-axis describes his leaning front/back behavior, while the angle between one's spine and the direction between the centers of the participants describes relative leaning behavior (towards/away). The angles of one's arms with his local x-axis, describe hand position and indicate arm crossing behavior (Fig. <ref type="figure" target="#fig_6">5</ref>(e)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Feature selection approaches</head><p>We examine a variety of feature selection approaches to select a subset of decorrelated, informative body language features, tailored to each emotional attribute.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1.">Mutual information-based and correlation-based criteria</head><p>The minimal redundancy maximal relevance criterion (mRMR), introduced in <ref type="bibr" target="#b42">[43]</ref>, selects features that maximize the mutual information (MI) between features and the ground truth, and minimize the MI between the selected features. Let S M = {x i } i =1 M be a set of M continuous body language features, y the continuous emotional attribute, and I(⋅,⋅) represents MI. Then the mRMR measure is defined:</p><formula xml:id="formula_12">mRMR I i ð Þ ¼ I x i ; y ð Þ- 1 M-1 ∑ x j ∈S M ;j≠i I x i ; x j<label>ð11Þ</label></formula><p>where</p><formula xml:id="formula_13">I x i ; y ð Þ¼ ∑ x i ∈X i ∑ y∈Y p x i ; y ð Þlog p x i ; y ð Þ p x i ð Þp y ð Þ :<label>ð12Þ</label></formula><p>Estimation of the probability distributions p(x i ), p(y), p(x i ,x j ) and p(x i ,y), which is required for computing the MI values, is performed through uniform quantization.</p><p>We also examine the selection of maximal relevance and minimal redundancy features based on correlations rather than MI values. Specifically, if we denote as C(x i ,y) and C(x i ,x j ) the linear (Pearson) correlations between a feature and the ground truth, and between the two features, respectively, we can define the correlation-based metric as follows:</p><formula xml:id="formula_14">mRMR C i ð Þ ¼ C x i ; y ð Þ- 1 M-1 ∑ x j ∈S M ;j≠i C x i ; x j :<label>ð13Þ</label></formula><p>Both approaches perform a ranking of features, where high values are preferred and they denote that the feature shares much information, or has high correlation, with the ground truth and shares little information, or has low correlation, with other selected features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2.">Fisher criterion</head><p>Alternatively, we select features that discriminate between regions of high, low and medium values of the emotional ground truth. Intuitively these features reflect different body language behaviors across regions of different emotional content. Each attribute is quantized into three levels through k-means clustering, and the features that correspond to each level are collected. Fisher criterion, denoted as F value , is the ratio between the within-class variance and the between-class</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1</head><p>Body language features extracted from actor A during his interaction with actor B. Features are denoted as individual when they describe only A's movement and posture information, and as interaction features when they describe the relative movement and posture of A with respect to his interlocutor B. Norm indicates that the corresponding feature has been normalized per actor recording. • Position of A in B's coordinate system variance for a feature, and scores highly those features that achieve small within-class variability and large between-class variability <ref type="bibr" target="#b43">[44]</ref>. While the previously described correlation and MI based methods favor the selection of feature sets with low redundancy, the Fisher criterion may lead to redundant feature sets. Therefore, we further reduce our feature set, by excluding features, such that no feature pair has a correlation higher than a threshold (here we empirically selected a threshold of 0.8).</p><p>When choosing between two competing, highly-correlated features, we pick the one with the largest F value .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Vocal feature extraction</head><p>In contrast to body language features which are extracted throughout the recording session, the acoustic features are extracted only when the actors are speaking. For this purpose, the microphone signal obtained from each actor is first manually transcribed into regions where that actor is speaking and being silent. We extract 12 Mel Frequency Cepstral Coefficients (MFCCs) along with pitch and energy, using overlapping windows of length 30 ms and framerate of 16.67 ms (same as MoCap framerate). Such features are standard for speech emotion recognition <ref type="bibr" target="#b44">[45]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Tracking emotion trends at multiple resolutions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">GMM-based tracking at frame and window level</head><p>Our GMM-based tracking approach follows the mathematical framework described in Section 3.2. Additionally, it takes into account that body language features are available throughout the interaction, while speech features are available only when the actor is speaking. Therefore, when audio-visual features are considered we compute two mappings: a visual mapping trained only with body language features and an audio-visual mapping trained with both body language and speech features. The audio-visual features are fused at the feature-level for training the audio-visual GMM. During testing, we apply the GMM mapping on overlapping windows. When only visual features are used we compute the visual mapping on each window irrespective if whether the window contains speech or not. When audio-visual features are used, we compute an audio-visual mapping for the windows where speech is present, otherwise we compute a visual mapping. Therefore, we again scan the total recording using visual information and, if available, speech information. As a result, the results of the visual and audio-visual experiments are comparable as they are computed on the same recordings, and the audio-visual results provide information about whether speech improves emotion tracking on top of the visual information.</p><p>Empirically, we confirmed that including dynamic features produces a smoother emotional trajectory estimate, since it considers a window of the emotional state and the feature vector centered at the frame of interest. In our implementation, the underlying emotional trajectory y t , t =1,…, T is estimated over consecutive overlapping windows of length 300 frames, with 150 frames overlap. Then curves obtained from neighboring windows are merged using the add-overlap algorithm, and are smoothed using a low-pass filter.</p><p>This approach computes detailed frame-by-frame emotional trajectory estimates. However, emotional states are slowly varying, therefore this degree of accuracy may not be necessary. Modeling body and speech features at such detail may lead to modeling of noise or gestures unrelated to emotion rather than emotionally informative audio-visual manifestations. This motivates the use of window-level tracking, where features and feature functionals are extracted over larger windows in an attempt to capture more meaningful emotional and gestural dynamics. In this case, the mapping function takes as input the functionals computed over a window and outputs the average emotional attribute value of that window. Specifically, we average the ground truth curves over overlapping windows of 3 s length and 2 s overlap. We also apply such windows on the audio-visual features, over which we extract a variety of statistical functionals, specifically: mean, standard deviation, median, minimum, maximum, range, skewness, kurtosis, the lower and upper quantiles (corresponding to the 25th and 75th percentiles)  and the interquantile range. Therefore, we extract a potentially richer feature description by including statistical functionals over features.</p><p>The feature vector dimensionality is reduced by PCA. We train full covariance GMMs using 4 and 2 mixtures for frame and window-level tracking respectively (the method is not sensitive to the number of Gaussian mixtures). The use of a full covariance matrix is important in order to capture relations between the various body language and speech features, and empirically leads to better performance. The joint feature-emotion GMM models were trained using the HTK Toolbox <ref type="bibr" target="#b45">[46]</ref>, while the subsequent EM equations for computing the statistical GMM-based mapping were implemented in MATLAB, based on <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Using LSTM neural networks for regression</head><p>Long Short Term Memory (LSTM) neural networks were introduced in <ref type="bibr" target="#b46">[47]</ref>, as a variant of Recurrent Neural Networks (RNN). While RNNs are able to model a certain amount of history through their cyclic connections, it has been shown that longer range history is inaccessible to RNNs since the backpropagated error either blows up or decays over time (vanishing gradient problem). LSTM networks overcome the vanishing gradient problem by storing in their hidden layers information from an arbitrarily long amount of time <ref type="bibr" target="#b46">[47]</ref>. LSTM networks have been applied in a variety of pattern recognition applications, including phoneme classification <ref type="bibr" target="#b47">[48]</ref>, audio-visual emotion classification <ref type="bibr" target="#b48">[49]</ref>, and regression for tracking continuous emotions <ref type="bibr" target="#b12">[13]</ref>. Modeling history seems to be beneficial for the problem of emotion tracking, since emotions tend to be slowly varying over time, and LSTM regression was shown to outperform Support Vector Regression (SVR) for continuously tracking valence and activation over time <ref type="bibr" target="#b12">[13]</ref>. Here, we apply LSTM networks for both the frame and the window level regression problems.</p><p>LSTM networks for regression are trained using the RNNlib Toolbox <ref type="bibr" target="#b49">[50]</ref>, without using derivative features. Including derivatives was deemed redundant since temporal information is already captured through the network. The LSTM networks consist of one hidden layer with 128 memory blocks (we also experimented with 64 and 256 memory block configurations, which performed similarly). To improve generalization low Gaussian noise was added to the training features. The produced curves are smoothed using a lowpass filter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Baseline based on simple functions of informative features</head><p>A relevant question is what would be the tracking performance if we estimated an attribute, e.g., activation, as a simple function of informative features, e.g., velocity of body, of hands, intensity of voice, leaning angle towards interlocutor etc. Indeed such approaches are common in the behavioral sciences, where for instance speech intensity and pitch are sometimes used as indicators of vocal activation <ref type="bibr" target="#b50">[51]</ref>. Along these lines, assuming that an interlocutor's emotional attributes and his audiovisual features are normalized to be roughly in the same range, we could compute an estimate of his activation by taking a functional (e.g., mean) of the most activation-informative features. If a feature is negatively correlated with activation then we multiply it with -1 beforehand. This method does not require training a model, however it assumes that we have an available set of informative features for each attribute, which can be chosen through feature selection e.g., by using the approaches of Section 5.2, or through prior knowledge. This simple baseline could be useful for cases where we have few or no annotated data.</p><p>In our implementation, we select the K most informative body features for each attribute, based on the F value , and the L most informative speech features based on correlation with the attribute (results based on the mRMR C criterion are similar and are omitted for lack of space). All the features and the emotional attributes are first normalized to have zero mean and unit standard deviation across the database, and features that are negatively correlated with the emotional attribute are multiplied with -1. Then we compute the mean, median and maximum of these features as different attribute estimates (please refer to the Appendix A for a list of the most informative body language features per attribute). For the window-level tracking, we follow the same approach using normalized statistical functionals of body and speech features extracted over windows, so as to directly compare with the methods of Sections 6.1 and 6.2. Again, we select the K most correlated functionals of body features and the L most correlated functionals of speech features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Experiments, results and discussion</head><p>Our experiments are organized in an eight-fold leave-one-dyadout cross validation. Actors belong only to one dyad, therefore this cross validation ensures that test set actors are not seen during training. Each dyad was recorded in each of eight recording days, and since the selected number of recordings per day varies, this results in 5-12 actor recordings selected for testing at each fold, while the rest are used for training. We focus primarily on tracking the underlying emotional trends, and therefore we compute the correlations between the ground truth and the estimated emotional trajectories as our primary performance metric.</p><p>The body language feature sets are selected through the correlationbased criterion mRMR C or the Fisher criterion F value (the MI-based criterion mRMR I gave slightly lower performance and is omitted). We systematically examine the effect of the number of body language features on the performance of each tracking approach, by selecting the top 5, 10, 15, ⋯ 30 features for the mRMR C criterion, and the top 10, 15, ⋯ 40 features for the F value criterion (which are later further reduced after removing highly correlated features). The performance of the GMM-based and LSTM frame-level tracking as a function of the number of selected body features is shown in Fig. <ref type="figure">6</ref>. This approach represents a principled way to select the final number of features based on visual frame-level tracking performance, although could cause some amount of overfitting. Note however that the selected number of features is not necessarily optimal for the audiovisual and window-level tracking, since some of the body language features that are left out may be important when used in combination with speech, or may have informative statistical functionals. To underscore this point, we present an example of valence tracking at Tables <ref type="table" target="#tab_0">2</ref> and<ref type="table">4</ref>, where selecting 11 features is optimal for the frame-level visual experiments but not for the audio-visual and window-level experiments, where larger feature sets, e.g. of 24 features, perform better.</p><p>For GMM-based and LSTM frame-level tracking, we select the number of body language features that leads to the best performance. We include speech information, by adding the 14 speech features in our body language feature set (feature-level fusion). We also add the first and second feature derivatives. For window-level tracking, we perform statistical functional computation on the respective optimal frame-level feature set and then Principal Component Analysis (PCA), keeping the first 50 components, which explain about 88-95% of the total variability. To prevent over smoothing, we only add first derivatives, resulting in 100 dimensional feature vectors for both the visual and the audio-visual case. Both the features and the emotional curves are z-normalized using the global means and standard deviations of the dataset.</p><p>Regarding the simple baseline described in Section 6.3 for frame or window-level tracking, we select the number of body language and speech features that empirically give the best performance, and we combine them using their mean (which tends to perform better than median and maximum). Our observation is that the performance of this simple approach saturates sooner than the other algorithms, typically around 10 or 15 features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Frame-level tracking using audio-visual information</head><p>In Table <ref type="table" target="#tab_0">2</ref>, we present the tracking performance of visual and audio-visual feature methods for the GMM-based mapping and the LSTM regression approaches. The number of selected body language features is presented in parentheses. For the simple baseline method, the selected number of K body and L speech features is presented in parentheses as (K + L). For each case, we present the median of the correlations between each estimated curve and the ground truth, as a metric of the overall performance. In the last row of Table <ref type="table" target="#tab_0">2</ref>, we also report the median inter-annotator correlations computed at the frame-level, as described in Section 4.2.</p><p>For all methods, activation tracking is the best performing task, followed by dominance, while neither of the approaches seems to adequately capture valence trends. Considering speech features increases activation (speech features generally convey activation information <ref type="bibr" target="#b51">[52]</ref>) and slightly boosts dominance tracking performance but offers no significant increase for valence. Both feature selection criteria perform comparably.</p><p>For valence, the F value resulted in selecting a relatively small body language feature set of 11 features, therefore we also tried a larger feature set of 24 features to see if extra features would increase performance at later stages. Indeed the extra features and their statistical functionals seem to slightly boost performance at window-level tracking (see results of Section 7.2 Table <ref type="table">4</ref>), however valence tracking generally remains problematic. This suggests that valence is not adequately reflected on our features, or that body language generally conveys less information about valence, compared to activation and dominance. Valence may be better reflected through other modalities; for instance facial expressions are found to discriminate valence states well <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b10">11]</ref>. Note that when annotators rated each actor's valence they had access to a variety of cues besides body language and speech, including facial expressions and lexical content, a fact that could explain their good agreement scores.  Fig. <ref type="figure">6</ref>. Frame-level tracking using body language features: performance of the various tracking approaches and feature selection algorithms (in terms of median correlation with ground truth) as a function of the number of body language features used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 3</head><p>Continuous tracking at the frame-level of activation, valence and dominance using speech cues only. We present the median correlation value between the computed emotional curve and the ground truth, computed only on speech regions.  Correlations with ground truth are 0.68 and 0.48 respectively</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Speech features only: median correlations with ground truth</head><p>Between the tracking approaches, the GMM-based mapping achieves consistently higher correlation for activation and dominance. We performed the non-parametric Wilcoxon signed-rank test to examine whether the median of the paired differences between algorithms is significantly different from zero. Specifically, we compared the GMM and LSTM approaches, given same feature selection method, the GMM approach with the simple baseline, and the LSTM approach with the simple baseline (p= 0.05). Statistically significant differences are denoted in Table <ref type="table" target="#tab_0">2</ref> with symbol ⋆ for the GMM vs LSTM comparison, † for the GMM vs simple baseline comparison and ⋄ for the LSTM vs simple baseline comparison (symbols are placed next to the method that performs better in the comparison). For example, for the frame-level tracking of activation using body language features, symbols ⋆ and † next to GMM tracking(F value ) indicate that the algorithm performs significantly better than both LSTM (F value ) and the simple baseline. Overall, the GMM-based mapping significantly outperforms both the LSTM method and the simple baseline for most activation and dominance tasks. However, LSTM tracking hardly outperforms the simple baseline, which works reasonably well for the activation and dominance tasks.</p><p>In order to examine how these methods approximate the actual values of the underlying emotional curves, we also compute the Root Mean Square Error (RMSE) between the estimated curve and the ground truth, which is defined as:</p><formula xml:id="formula_15">RMSE ŷest ; y true ð Þ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi 1 T X T i¼1 ŷest i ð Þ-y true i ð Þ ð Þ 2 v u u t :</formula><p>All methods lead to median RMSE methods between 0.8 and 1.2, with the GMM-based mapping usually having a slightly lower RMSE. Those values are considerably higher than the median RMSE values computed between the annotation curves of multiple evaluators, which are 0.37, 0.24 and 0.31 for activation, valence and dominance, respectively.</p><p>In Table <ref type="table">3</ref>, we also present results based on speech features only. Audio-only GMM-based tracking works reasonably for activation and partially for dominance, which confirms our previous observations regarding the importance of speech for activation trend tracking. Note however that these results are computed only on speech regions, therefore they are not directly comparable with the results of Table <ref type="table" target="#tab_0">2</ref>.</p><p>The behavior of the two methods is illustrated in Fig. <ref type="figure">7</ref>. In Fig. <ref type="figure">7(a)-(c</ref>), we present the multiple annotations (dashed blue lines) along with their mean (red line) which is our ground truth, for two activation and one dominance example. Fig. <ref type="figure">7(d)-(f)</ref> shows the estimated curves for GMM-based tracking, LSTM and the simple baseline respectively, for the curve of Fig. <ref type="figure">7</ref>(a). For this example, GMM-based mapping produces a curve that is smoother and has higher correlation with the ground truth than the other two methods. Fig. <ref type="figure">7(g)-(</ref>i) shows the estimated curves for Fig. <ref type="figure">7(b)</ref>, where the GMM-based performance is moderate but the method seems to track the most prominent activation trends. Finally, Fig. <ref type="figure">7(j)-(l)</ref> shows examples of dominance tracking for curve of Fig. <ref type="figure">7(c)</ref>, where all methods perform reasonably well, although the three output curves look quite different. In general, we notice that the GMM method produces smooth and flat curves, while the other two methods produce noisier curves of larger amplitudes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Window-level tracking using audio-visual information</head><p>In Table <ref type="table">4</ref> we present the performance of the low resolution tracking at the window level. The median annotation correlations are re-computed at the window level and are reported at the last row of Table <ref type="table">4</ref>. For GMM-based and LSTM tracking we utilize the empirically selected feature sets of Section 7.1, after statistical feature extraction and PCA. For the simple baseline, we present the better performing statistical functionals of K body and L speech features.</p><p>In general, we notice a significant increase from the previous results which can be attributed to the fact that we model less noise and track pronounced trends in the underlying emotional curves. Also we use a richer feature set, consisting of statistical functionals of the frame-level features. The GMM-based mapping results follow similar trends as before; activation is the best performing attribute, followed by dominance. Valence performance is still low, although when we use the Fisher criterion F value with the larger feature set our performance increases. Adding speech features considerably increases activation and dominance performance. Activation tracking reaches a median correlation of around 0.6, which is similar to the median correlations between human annotators for this task. The LSTM regression and simple baseline results follow similar trends, although median correlations are generally lower.</p><p>The statistical significance of these results is examined using the Wilcoxon signed-rank test for paired differences, following the same notation as in Section 7.1. In general, GMM-based tracking significantly outperforms the other two approaches for activation and dominance trend tracking, while LSTM and simple baseline have comparable performance, with LSTM being slightly better.</p><p>Again, when looking at the resulting curves we observe smooth and flat curves for the GMM-based method and noisier curves with bigger amplitude for the LSTM and simple baseline methods. Fig. <ref type="figure" target="#fig_10">8(a)-(c</ref>) illustrates examples of rated activation, valence and dominance respectively. In Fig. <ref type="figure" target="#fig_10">8(d)-(f</ref>) we present the window-level tracking of activation curve in Fig <ref type="figure" target="#fig_10">8(a)</ref>, where all methods perform well, while the GMM-based curve achieves the highest correlation with the ground truth. In Fig. <ref type="figure" target="#fig_10">8(g)-(</ref>i) we present less successful tracking results of the valence curve in Fig. <ref type="figure" target="#fig_10">8</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(b). The GMM-based mapping captures few of</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 4</head><p>Continuous tracking at the window-level of activation, valence and dominance using body language and speech cues. We present the median correlation value between the computed emotional curve and the ground truth.  the valence peaks, while the other two methods seem to mostly capture noise. Finally, in Fig. <ref type="figure" target="#fig_10">8</ref>(j)-(l) we present tracking of the dominance curve in <ref type="bibr">Fig 8(c)</ref>, where GMM-based tracking performs better than LSTM, which in turn outperforms the simple baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Discussion of informative body language features</head><p>This section discusses the selected body language features, to provide insights about the body language gestures, movements and postures that are informative of the underlying emotional attributes. Details about the top ranking body language features, according to F value criterion, are presented in Appendix A, Tables A.5-A.7. We omit detailed analysis of the mRMR C selected features; similar observations can be made for the activation and dominance tasks.</p><p>As seen in Table <ref type="table">A</ref>.5 for activation, many of the selected features describe absolute velocities, relative body orientation and leaning, posture and hand gestures. Highly activated subjects generally display higher arm and foot velocities (feats 4,20,21), more leaning and body orientation towards the interlocutor (feats 1,5), and more front leaning (feat 9) among others. Also many selected features describe hand gestures, for example hands tend to be further from the body (feats <ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b18">19)</ref>, further from each other <ref type="bibr" target="#b11">(12,</ref><ref type="bibr" target="#b21">22)</ref>, and raised higher <ref type="bibr" target="#b23">(24,</ref><ref type="bibr" target="#b24">25)</ref> for highly activated subjects. Also, body location in (x,y) coordinates reflects a tendency of activated participants to be at the center of the recording space (feats 11,13).</p><p>For the dominance task, according to Table A.6, many of the selected features are common with the activation features, however we notice a preference for features describing relative behaviors like velocity, leaning and orientation. For example dominant individuals tend to lean and have body orientation more towards interlocutor (feats 1,4), and move their body, arms and feet more towards interlocutor (feats <ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24)</ref>. This seems intuitive since dominance essentially captures relative (interaction) behavior. Also, dominant subjects tend to touch the interlocutor (feat 10), which brings to mind psychological observations relating touching with dominant behavior <ref type="bibr" target="#b53">[54]</ref>.</p><p>Finally, for the valence task, some features from Table <ref type="table">A</ref>.7 stand out. For instance, positively valenced subjects tend to place hands on chest (feats 22,23), or touch the interlocutor's hand (feat 15), which seem to be intuitive bodily expressions of valence. Also positively valenced subjects tend to look more towards and move towards the interlocutor (feats 9,21), and move their arms and feet more (feats 2,13,14). Also the combination of more leaning towards others (feat 20), but less front leaning (feat 19) for positive valence, indicates that positively valenced subjects tend to lean more towards the interlocutor, while negatively valenced subjects generally have a more slouched posture. Some of the above affective body language behaviors agree with the literature, for example arms being far from the body for high activation, or increased body motion for activated emotions such as anger ( [31], Table <ref type="table" target="#tab_0">2</ref>). However, direct comparisons are hard to make since most past works on body language examine pre-defined categorical emotional states rather than continuous emotional attributes. Other aspects that differentiate this work from the literature include examining dominant behaviors, which are generally less discussed, as well as the focus on interaction aspects of body language through the introduction of 'relative' body features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion and future work</head><p>We address the problem of tracking continuous emotional attributes of participants throughout affective dyadic improvisations, where participants may be listening, speaking or doing neither. To this end, we have examined interpretable features describing of a person's body language, and speech information. These descriptions complement existing literature, e.g., <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>, in capturing a wide range of full body gestures and emphasizing the aspects of body language in dyadic emotional interactions. We propose a statistical mapping approach to automatically track emotional trends based on body language and speech. Our outperforms other examined methods, such as LSTM regression <ref type="bibr" target="#b12">[13]</ref>, and produces smooth emotional curve estimates. Also, the simple baseline represents an interesting, unsupervised alternative, that is worth further investigation. Our results show promising performance for tracking trends of activation and dominance, and also suggest that body language conveys rich activation and dominance related information. For activation trend tracking our correlation-based performance is comparable to human performance. Finally, analysis of our body language features offers quantitative insights on the relations between an underlying emotional state and the displayed bodily behavior in the context of dyadic interaction. This enables us to draw connections with psychological observations regarding body language and emotion.</p><p>However, valence trend tracking remains problematic, which might indicate that our features are not adequately reflective of valence. Existing literature indicates that body posture is a better indicator of activation, although the importance of the valence dimension should not be dismissed <ref type="bibr" target="#b24">[25]</ref>. Possibly higher-level body features are required to discern valence; we have not incorporated audio-visual cues at the session level, such as the amount and length of pauses, percentage of time that an actor performs an action, turn-taking patterns etc. Such higherlevel cues may be informative of valence and dominance, and their investigation is a promising future research direction. Also note that we do not consider facial expressions, which are known to be reflective of valence <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b10">11]</ref>.</p><p>Other open questions pertain to our performance metrics; while correlation metrics and RMS errors describe different aspects of tracking performance and are currently used for evaluating systems that produce continuous estimates <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b11">12]</ref>, we may need to find more accurate measures to describe the performance of such systems. Additionally, normalizing for subject-dependent emotional variability in expressive body language is an interesting research direction that could potentially bring significant improvement. A further goal is to extend this work towards examining the produced emotional curves to detect regions of emotional saliency, and study the actual events that occur in such regions. Such vocal, bodily or interaction-based events could give us insights of what constitutes the emotional content of an interaction. x coord of A's center x abs value higher (x further from center (0,0,0) of the recording space) </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. An overview of the work presented in this paper. From left to right we depict the data collection setting, the audio-visual feature extraction and data annotation processes, as well as the GMM-based statistical mapping approach that we follow for estimating the emotional curves.</figDesc><graphic coords="3,75.52,53.18,453.19,310.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) Marker positions. (b) Actor wearing markers. (c) Definition of body parts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The positions of the Motion Capture markers and definitions of the body parts used in feature extraction.</figDesc><graphic coords="4,72.08,621.41,442.90,105.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Example of activation rating by three annotators.</figDesc><graphic coords="5,44.05,53.18,248.16,216.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>A</head><label></label><figDesc>'s body posture (individual) • A's velocity (see Fig. 5(b)) • Relative velocity of A's right/left arm w. respect to A • Velocity of A's right/left arm • Relative velocity of A's right/left foot w. respect to A • Velocity of A's right/left foot A's body posture (individual) • A's body leaning angle front/back (see Fig. 5(d)) • Head angle, looking up/down • A's body leaning angle right/left • Distance between A's right/left hand and A's chest • A's body position in global coord. system: x,y, norm z coordinates (see Fig. 5(a)) • Distance between A's right/left hand and A's right/left hip • A's right/left hand position in A's local coord. system: norm x,y,z coordinates (see Fig. 5(a)) • Angle between A's right and left hands • Distance between A's right and left hand • Norm z coordinate of A's right/left knee (indicating kneeling) • Angle of A's right/left hand with x -axis in A's system (indicating • z coordinate of A's right/left foot (indicating jumping) • Arms crossed, see Fig. 5(e) • Norm z coordinate of A's upper back (indicating upward vs crouched posture) • Norm z coordinate of A's lower back (indicating sitting down) A's distance from B (interaction) • A's distance from B • Min. distance between A's right/left hand and B's head • Min. distance between A's right/left hand and B's hands • Min. distance between A's right/left hand and B's back • Min. distance between A's right/left hand and B's torso A's velocity with respect to B (interaction) • A's relative velocity w. respect to B (see Fig. 5(b)) • Relative velocity of A's right/left foot w. respect to B • Relative velocity of A's right/left hand w. respect to B A's orientation with respect to B (interaction) • Angle of A's face w. respect to B (see Fig. 5(c)) • A's leaning angle towards or away from B (see Fig. 5(d)) • Angle of A's body w. respect to B (similar to Fig. 5(c), but for waist coord. systems)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(a) Hand positions. (b) Relative movement. (c) Looking towards or away. (d) Leaning front/back, and leaning towards/away from Other. (e) Arm angles and description of arms crossed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Examples of extracted features from MoCap markers.</figDesc><graphic coords="7,74.54,70.37,455.51,261.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Tracking of Activation Curve (a) using GMM-based mapping, with body (green) and speech+body (black) features. Correlations with ground truth are 0.74 and 0Tracking of Activation Curve (a) using LSTM regression, with body (green) and speech+body (black) features. Correlations with ground truth are 0.60 and 0Tracking of Activation Curve (a) using the simple baseline (mean), with body (green) and speech+body (black) features. Correlations with ground truth are 0Tracking of Activation Curve (b) using GMM-based mapping, with body (green) and speech+body (black) features. Correlations with ground truth are 0.25 and 0Tracking of Activation Curve (b) using LSTM regression, with body (green) and speech+body (black) features. Correlations with ground truth are 0.25 and 0Tracking of Activation Curve (b) using the simple baseline (mean), with body (green) and speech+body (black) features. Correlations with ground truth are 0Tracking of Dominance Curve (c) using GMM-based mapping, with body (green) and speech+body (black) features. Correlations with ground truth are 0.59 and 0.Tracking of Dominance Curve (c) using LSTM regression, with body (green) and speech+body (black) features. Correlations with ground truth are 0.51 and 0.Tracking of Dominance Curve (c) using the simple baseline (mean), with body (green) and speech+body (black) features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 8 .</head><label>8</label><figDesc>Fig.8. Results of the three tracking methods, GMM-based mapping, LSTM regression and the simple baseline, for activation, valence and dominance cases, for window-level tracking.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2</head><label>2</label><figDesc>Continuous tracking at the frame-level of activation, valence and dominance using body language and speech cues. We present the median correlation value between the computed emotional curve and the ground truth. Parentheses indicate the number of selected body features (K), or body and speech features (K + L).</figDesc><table><row><cell>Activation</cell><cell>Valence</cell><cell>Dominance</cell></row><row><cell>GMM-based mapping</cell><cell></cell><cell></cell></row><row><cell>0.3866</cell><cell>0.0501</cell><cell>0.1102</cell></row><row><cell>LSTM regression</cell><cell></cell><cell></cell></row><row><cell>0.2237</cell><cell>0.0609</cell><cell>0.0066</cell></row><row><cell>Simple baseline (mean)</cell><cell></cell><cell></cell></row><row><cell>0.1823 (5)</cell><cell>0.0529 (5)</cell><cell>0.0093 (5)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Body language + speech features: median correlations with ground truth Results of the three tracking methods, GMM-based mapping, LSTM regression and the simple baseline, for activation, and dominance cases, for frame-level tracking.</figDesc><table><row><cell>Feature selection</cell><cell>Activation</cell><cell>Valence</cell><cell>Dominance</cell></row><row><cell>GMM-based mapping</cell><cell></cell><cell></cell><cell></cell></row><row><cell>F value</cell><cell>0.4943 ⋆  †</cell><cell>0.1296 / 0.2061</cell><cell>0.3268  †</cell></row><row><cell>mRMR C</cell><cell>0.5169 ⋆  †</cell><cell>0.0866</cell><cell>0.3219 ⋆  †</cell></row><row><cell>LSTM regression</cell><cell></cell><cell></cell><cell></cell></row><row><cell>F value</cell><cell>0.4455</cell><cell>0.1348</cell><cell>0.2268 ⋄</cell></row><row><cell>mRMR C</cell><cell>0.4529 ⋄</cell><cell>0.1480</cell><cell>0.2835 ⋄</cell></row><row><cell>Simple baseline</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mean</cell><cell>0.3682(10)</cell><cell>0.0626(15)</cell><cell>0.0953(15)</cell></row><row><cell cols="4">Body language + speech features: median correlations with ground truth</cell></row><row><cell>Feature selection</cell><cell>Activation</cell><cell>Valence</cell><cell>Dominance</cell></row><row><cell>GMM-based mapping</cell><cell></cell><cell></cell><cell></cell></row><row><cell>F value</cell><cell>0.5979 ⋆  †</cell><cell>0.1831/0.2247</cell><cell>0.3696 ⋆  †</cell></row><row><cell>mRMR C</cell><cell>0.5837 ⋆  †</cell><cell>0.0563</cell><cell>0.3368 ⋆  †</cell></row><row><cell>LSTM regression</cell><cell></cell><cell></cell><cell></cell></row><row><cell>F value</cell><cell>0.4882</cell><cell>0.0976</cell><cell>0.2122</cell></row><row><cell>mRMR C</cell><cell>0.4934 ⋄</cell><cell>0.0878</cell><cell>0.2549</cell></row><row><cell>Simple baseline</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mean</cell><cell>0.4447(10 + 5)</cell><cell>0.1261 (15 + 5)</cell><cell>0.1837(5 + 5)</cell></row><row><cell cols="3">Median inter-annotator correlation (agreement)</cell><cell></cell></row><row><cell></cell><cell>Activation</cell><cell>Valence</cell><cell>Dominance</cell></row><row><cell></cell><cell>0.6199</cell><cell>0.6317</cell><cell>0.6200</cell></row><row><cell>Fig. 7.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table A . 5</head><label>A5</label><figDesc>Statistical analysis of the top 25 activation features, according to the F value criterion (each feature's rank according to the mRMR C criterion is included in the second column). The feature descriptions under the statistical tests column are describing high activation behavior compared to low activation behavior of a subject A. The statistical test performed is difference of means of the feature values between high and low activation classes (t-test). Statistical analysis of the top 25 dominance features, according to the F value criterion (each feature's rank according to the mRMR C criterion is included in the second column). The feature descriptions under the statistical tests column are describing high dominance behavior compared to low dominance behavior of a subject A. The statistical test performed is difference of means of the feature values between high and low dominance classes (t-test). Dominance: Comparison of high vs low dominance classes F value (and mRMR C )</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Table A.6</cell><cell></cell><cell></cell></row><row><cell cols="4">Activation: comparison of high vs low activation classes</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">F value (and mRMR C )</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Rank Rank</cell><cell>Feature</cell><cell>Description of statistical</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">F value mRMR C 1 1</cell><cell cols="2">tests results p ≈ 0 A's body lean towards/away from B More lean towards vs no</cell><cell cols="2">Rank Rank F value mRMR C</cell><cell>Feature</cell><cell>Description of statistical tests results p ≈ 0</cell></row><row><cell></cell><cell></cell><cell></cell><cell>leaning</cell><cell>1</cell><cell>7</cell><cell>Relative angle of A's body</cell><cell>Body orientation more towards</cell></row><row><cell>2</cell><cell>9</cell><cell>Norm x coord of A's right hand in A's</cell><cell>x coord higher (further</cell><cell></cell><cell></cell><cell>towards/away from B</cell><cell>other vs sideways</cell></row><row><cell></cell><cell></cell><cell>system</cell><cell>from body towards right,</cell><cell>2</cell><cell>1</cell><cell>A's head angle, up/down</cell><cell>More straight vs more downwards</cell></row><row><cell></cell><cell></cell><cell></cell><cell>see also Fig. 5(a))</cell><cell>3</cell><cell>6</cell><cell>Norm z coord of A's center</cell><cell>Higher, indicates less sitting</cell></row><row><cell>3</cell><cell>7</cell><cell cols="2">Distance of A's left hand from A's hip Greater distance</cell><cell>4</cell><cell>3</cell><cell>A's body leaning angle</cell><cell>More lean towards vs no leaning</cell></row><row><cell>4</cell><cell>6</cell><cell>Abs velocity of A's right arm</cell><cell>Higher velocity</cell><cell></cell><cell></cell><cell>towards/away from B</cell><cell></cell></row><row><cell>5</cell><cell>22</cell><cell>Relative angle of A's body towards B</cell><cell>Body orientation more</cell><cell>5</cell><cell>13</cell><cell>Distance of A's left hand</cell><cell>Greater distance, hand further</cell></row><row><cell></cell><cell></cell><cell></cell><cell>towards B vs sideways</cell><cell></cell><cell></cell><cell>from A's hip</cell><cell>away from hip</cell></row><row><cell>6</cell><cell>27</cell><cell>Norm y coord of A's left hand in A's</cell><cell>y coord higher (further</cell><cell>6</cell><cell>2</cell><cell>z coord of A's right foot</cell><cell>Lower</cell></row><row><cell></cell><cell></cell><cell>system</cell><cell>from body towards front,</cell><cell>7</cell><cell>17</cell><cell>Norm x coord of A's right</cell><cell>x coord higher (further from body</cell></row><row><cell></cell><cell></cell><cell></cell><cell>see also Fig. 5(a))</cell><cell></cell><cell></cell><cell>hand in A's system</cell><cell>towards right, see also Fig. 5(a))</cell></row><row><cell>7</cell><cell>12</cell><cell>Distance of A's right hand from A's</cell><cell>Greater distance</cell><cell>8</cell><cell>12</cell><cell>Relative velocity of A to-</cell><cell>Move more towards vs away</cell></row><row><cell></cell><cell></cell><cell>hip</cell><cell></cell><cell></cell><cell></cell><cell>wards/away from B</cell><cell></cell></row><row><cell>8</cell><cell>2</cell><cell>A's body leaning angle, left/right</cell><cell>Slightly lean right vs straight</cell><cell>9</cell><cell>24</cell><cell>Distance of A's right hand</cell><cell>Greater distance, further from hip</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(though angle in both cases is</cell><cell></cell><cell></cell><cell>from A's hip</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>close to zero)</cell><cell>10</cell><cell>48</cell><cell>Min dist between A's left</cell><cell>Smaller, indicates more touching</cell></row><row><cell>9</cell><cell>4</cell><cell>A's body leaning angle, front/back</cell><cell>More lean front vs no leaning</cell><cell></cell><cell></cell><cell>hand and B's torso</cell><cell></cell></row><row><cell>10</cell><cell>28</cell><cell>Norm y coord of A's right hand in A's</cell><cell>y coord higher (further</cell><cell>11</cell><cell>38</cell><cell>Norm z coord of A's right</cell><cell>Hand is lower</cell></row><row><cell></cell><cell></cell><cell>system</cell><cell>from body towards front,</cell><cell></cell><cell></cell><cell>hand in A's system</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>see also Fig. 5(a))</cell><cell>12</cell><cell>29</cell><cell cols="2">Distance between A's hands Hands wider apart</cell></row><row><cell>11</cell><cell>3</cell><cell>x coord of A's center</cell><cell>x abs value lower (x more</cell><cell>13</cell><cell>10</cell><cell>A's body leaning angle, left/</cell><cell>More lean right (though angle in</cell></row><row><cell></cell><cell></cell><cell></cell><cell>towards center (0,0,0) of</cell><cell></cell><cell></cell><cell>right</cell><cell>both cases is close to zero)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>the recording space)</cell><cell>14</cell><cell>28</cell><cell>Norm x coord of A's left</cell><cell>x coord lower (further from body</cell></row><row><cell>12</cell><cell>34</cell><cell>Distance between A's right and left</cell><cell>Hands wider apart</cell><cell></cell><cell></cell><cell>hand in A's system</cell><cell>towards left, see also Fig. 5(a))</cell></row><row><cell></cell><cell></cell><cell>hand</cell><cell></cell><cell>15</cell><cell>5</cell><cell>y coord of A's center</cell><cell>y abs value lower (y more towards</cell></row><row><cell>13</cell><cell>11</cell><cell>y coord of A's center</cell><cell>y abs value lower (y more</cell><cell></cell><cell></cell><cell></cell><cell>center (0,0,0) of the recording space)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>towards center (0,0,0) of the</cell><cell>16</cell><cell>36</cell><cell>Angle of A's right hand with</cell><cell>Hand more in front vs slightly</cell></row><row><cell></cell><cell></cell><cell></cell><cell>recording space)</cell><cell></cell><cell></cell><cell>x coord in A's system</cell><cell>towards left (see also Fig. 5(e))</cell></row><row><cell>14</cell><cell>14</cell><cell>Norm z coord of A's upper back</cell><cell>Higher, more upwards</cell><cell>17</cell><cell>14</cell><cell>Relative velocity of A's right</cell><cell>Move more towards vs away</cell></row><row><cell></cell><cell></cell><cell></cell><cell>posture, also indicates less</cell><cell></cell><cell></cell><cell>hand towards/away from B</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>sitting</cell><cell>18</cell><cell>45</cell><cell>Norm z coord of A's left</cell><cell>Hand is lower</cell></row><row><cell>15</cell><cell>48</cell><cell>Distance between A's right hand and</cell><cell>Smaller, more touching,</cell><cell></cell><cell></cell><cell>hand in A's system</cell><cell></cell></row><row><cell></cell><cell></cell><cell>B's back</cell><cell>could indicate hugging</cell><cell>19</cell><cell>5</cell><cell>A's body leaning angle,</cell><cell>More lean front vs slightly less lean</cell></row><row><cell></cell><cell></cell><cell></cell><cell>depending on the</cell><cell></cell><cell></cell><cell>front/back</cell><cell>front</cell></row><row><cell></cell><cell></cell><cell></cell><cell>interlocutors orientation</cell><cell>20</cell><cell>21</cell><cell>Relative velocity of A's left</cell><cell>Move more towards vs away</cell></row><row><cell>16</cell><cell>23</cell><cell>Norm z coord of A's left knee</cell><cell>Lower, may indicate</cell><cell></cell><cell></cell><cell>hand towards/away from B</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>kneeling</cell><cell>21</cell><cell>37</cell><cell>Distance between A's right</cell><cell>Greater, hand further from chest</cell></row><row><cell>17</cell><cell>8</cell><cell>A's head angle, up/down</cell><cell>More straight vs more</cell><cell></cell><cell></cell><cell>hand and A's chest</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>downwards</cell><cell>22</cell><cell>15</cell><cell>Relative velocity of A's right</cell><cell>Move more towards vs away</cell></row><row><cell>18</cell><cell>38</cell><cell>Angle of A's right hand with x coord</cell><cell>Hand more in front vs</cell><cell></cell><cell></cell><cell>foot towards/away from B</cell><cell></cell></row><row><cell></cell><cell></cell><cell>in A's system</cell><cell>slightly towards left (see</cell><cell>23</cell><cell>11</cell><cell>Norm z coord of A's upper</cell><cell>Higher, more upwards position,</cell></row><row><cell></cell><cell></cell><cell></cell><cell>also Fig. 5(e))</cell><cell></cell><cell></cell><cell>back</cell><cell>also indicates less sitting</cell></row><row><cell>19</cell><cell>37</cell><cell>Norm x coord of A's left hand in A's</cell><cell>x coord lower (further</cell><cell>24</cell><cell>16</cell><cell>Relative velocity of A's left</cell><cell>Move more towards vs away</cell></row><row><cell></cell><cell></cell><cell>system</cell><cell>from body towards left, see</cell><cell></cell><cell></cell><cell>foot towards/away from B</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>also Fig. 5(a))</cell><cell>25</cell><cell>9</cell><cell></cell><cell></cell></row><row><cell>20</cell><cell>18</cell><cell>Abs velocity of A's right foot</cell><cell>Velocity higher</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>21</cell><cell>17</cell><cell>Abs velocity of A's left foot</cell><cell>Velocity higher</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>22</cell><cell>20</cell><cell>Angle between A's hands</cell><cell>Hands wider apart</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>23</cell><cell>40</cell><cell>Distance between A's left hand and</cell><cell>Bigger distance, hand</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>A's chest</cell><cell>further from chest</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>24</cell><cell>41</cell><cell>Norm z coord of A's right hand in A's</cell><cell>Hand is higher, indicates</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>system</cell><cell>raised hand (see also</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Fig. 5(a))</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>25</cell><cell>42</cell><cell>Norm z coord of A's left hand in A's</cell><cell>Hand is higher, indicates</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>system</cell><cell>raised hand (see also</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Fig. 5(a))</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table A . 7</head><label>A7</label><figDesc>Statistical analysis of the top 25 valence features, according to the F value criterion (each feature's rank according to the mRMR C criterion is included in the second column). The feature descriptions under the statistical tests column are describing positive valence behavior compared to negative valence behavior of a subject A. The statistical test performed is difference of means of the feature values between positive and negative valence classes (t-test).</figDesc><table><row><cell cols="3">Table A.7 (continued)</cell><cell></cell></row><row><cell cols="4">Valence: Comparison of positive vs negative valence classes</cell></row><row><cell cols="3">F value (and mRMR C )</cell><cell></cell></row><row><cell cols="2">Rank Rank</cell><cell>Feature</cell><cell>Description of stat. tests results p ≈ 0</cell></row><row><cell cols="2">F value mRMR C</cell><cell></cell><cell></cell></row><row><cell>3</cell><cell>15</cell><cell>A's head angle, up/</cell><cell>Slightly more downwards vs straight</cell></row><row><cell></cell><cell></cell><cell>down</cell><cell>(though the two angles are almost the</cell></row><row><cell></cell><cell></cell><cell></cell><cell>same)</cell></row><row><cell>4</cell><cell>34</cell><cell>Distance between A's</cell><cell>Hands closer together</cell></row><row><cell></cell><cell></cell><cell>hands</cell><cell></cell></row><row><cell>5</cell><cell>41</cell><cell>Distance of A's left</cell><cell>Greater distance, further from hip</cell></row><row><cell></cell><cell></cell><cell>hand from A's hip</cell><cell></cell></row><row><cell>6</cell><cell>36</cell><cell>Distance of A's right</cell><cell>Greater distance, further from hip</cell></row><row><cell></cell><cell></cell><cell>hand from A's hip</cell><cell></cell></row><row><cell>7</cell><cell>28</cell><cell>Norm x coord of A's left</cell><cell>x coord higher (closer to body towards</cell></row><row><cell></cell><cell></cell><cell>hand in A's system</cell><cell>right, see also Fig. 5(a))</cell></row><row><cell>8</cell><cell>20</cell><cell>Norm z coord of A's</cell><cell>Lower, less upward position</cell></row><row><cell></cell><cell></cell><cell>upper back</cell><cell></cell></row><row><cell>9</cell><cell>11</cell><cell>Relative angle of A's</cell><cell>Face orientation more towards other</cell></row><row><cell></cell><cell></cell><cell>face towards B</cell><cell></cell></row><row><cell>10</cell><cell>31</cell><cell>Norm x coord of A's</cell><cell>x coord lower (closer to body towards</cell></row><row><cell></cell><cell></cell><cell>right hand in A's</cell><cell>left, see also Fig. 5(a))</cell></row><row><cell></cell><cell></cell><cell>system</cell><cell></cell></row><row><cell>11</cell><cell>30</cell><cell>Angle of A's left hand</cell><cell>Left hand more towards front rather</cell></row><row><cell></cell><cell></cell><cell>with x coord in A's</cell><cell>than left (see also Fig. 5(e))</cell></row><row><cell></cell><cell></cell><cell>system</cell><cell></cell></row><row><cell>12</cell><cell>27</cell><cell>Norm y coord of A's</cell><cell>y coord higher (further from body</cell></row><row><cell></cell><cell></cell><cell>right hand in A's</cell><cell>towards front)</cell></row><row><cell></cell><cell></cell><cell>system</cell><cell></cell></row><row><cell>13</cell><cell>21</cell><cell>Abs velocity of A's right</cell><cell>Higher velocity</cell></row><row><cell></cell><cell></cell><cell>foot</cell><cell></cell></row><row><cell>14</cell><cell>23</cell><cell>Abs velocity of A's left</cell><cell>Higher velocity</cell></row><row><cell></cell><cell></cell><cell>foot</cell><cell></cell></row><row><cell>15</cell><cell>43</cell><cell>Distance between A's</cell><cell>Lower, indicates more touching of B's</cell></row><row><cell></cell><cell></cell><cell>right hand and B's hand</cell><cell>hand</cell></row><row><cell>16</cell><cell>3</cell><cell>Norm z coord of A's left</cell><cell>Higher, indicates less kneeling</cell></row><row><cell></cell><cell></cell><cell>knee</cell><cell></cell></row><row><cell>17</cell><cell>4</cell><cell cols="2">A's direction relative to B Slightly more towards right-front of B vs</cell></row><row><cell></cell><cell></cell><cell></cell><cell>more in front</cell></row><row><cell>18</cell><cell>29</cell><cell>Norm y coord of A's left</cell><cell>y coord higher (further from body</cell></row><row><cell></cell><cell></cell><cell>hand in A's system</cell><cell>towards front)</cell></row><row><cell>19</cell><cell>19</cell><cell>A's body leaning angle,</cell><cell>Less leaning front vs more leaning front,</cell></row><row><cell></cell><cell></cell><cell>front/back</cell><cell>indicates less slouched posture</cell></row><row><cell>20</cell><cell>24</cell><cell>A's body leaning angle,</cell><cell>More leaning towards vs less leaning</cell></row><row><cell></cell><cell></cell><cell>towards/away from B</cell><cell>towards</cell></row><row><cell>21</cell><cell>12</cell><cell>Relative velocity of A</cell><cell>More moving towards vs moving away</cell></row><row><cell></cell><cell></cell><cell>towards/away from B</cell><cell></cell></row><row><cell>22</cell><cell>37</cell><cell>Distance between A's</cell><cell>Lower, indicates hand touching chest</cell></row><row><cell></cell><cell></cell><cell>right hand and A's</cell><cell></cell></row><row><cell></cell><cell></cell><cell>chest</cell><cell></cell></row><row><cell>23</cell><cell>38</cell><cell>Distance between A's</cell><cell>Lower, indicates hand touching chest</cell></row><row><cell></cell><cell></cell><cell>left hand and A's chest</cell><cell></cell></row><row><cell>24</cell><cell>30</cell><cell>Angle of A's left hand</cell><cell>Hand more towards front vs towards</cell></row><row><cell></cell><cell></cell><cell>with x coord in A's</cell><cell>right</cell></row><row><cell></cell><cell></cell><cell>system</cell><cell></cell></row><row><cell>25</cell><cell>1</cell><cell>y coord of A's center</cell><cell>y abs value bigger (y further from center</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(0,0,0) of the recording space)</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Valence: Comparison of positive vs negative valence classes</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">F value (and mRMR C )</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Rank Rank</cell><cell>Feature</cell><cell>Description of stat. tests results p ≈ 0</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">F value mRMR C</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1</cell><cell>33</cell><cell>Norm z coord of A's</cell><cell>Higher, indicates less sitting</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>lower back</cell></row><row><cell></cell><cell></cell><cell></cell><cell>2</cell><cell>42</cell><cell>Abs velocity of A's right</cell><cell>Higher velocity</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>arm</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A. Metallinou et al. / Image and Vision Computing 31 (2013) 137-152</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Top ranked body language features for emotion discrimination</head><p>In Tables A. <ref type="bibr" target="#b4">5</ref>-A.7 we present the top ranked 25 body language features for activation, valence and dominance, according to the F value criterion. Detailed results of the mRMR C criterion are omitted for lack of space, however we include the mRMR C -based rank next to each feature (notice the overlap between the features of the two criteria for activation and dominance, although not for valence). Each feature value represents a meaningful body posture. For performing statistical tests, we quantize each attribute value into 3 classes using the k-means algorithm, and collect the feature instances that correspond to the high and low classes, over the total database. For each feature, we perform a t-test to compare the mean feature value between low and high emotional attribute classes. We also include a description of the corresponding difference in body language that each feature value represents, always comparing high (or positive) versus low (or negative) attribute values. For example, the first line (feature of rank 1) of Table A.5 can be interpreted as 'more leaning towards the interlocutor when subject is characterized by high activation vs no leaning when subject is characterized by low activation'. All feature mean differences are statistically significant, although in some cases mean differences are so small that do not correspond to a recognizable difference in body language (e.g., see feat. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Evidence for a three-factor theory of emotions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mehrabian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Res. Pers</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="273" to="294" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Three dimensions of emotion</title>
		<author>
			<persName><forename type="first">H</forename><surname>Schlosberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychol. Rev</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="81" to="88" />
			<date type="published" when="1954">1954</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Affective judgment and psychophysiological response: dimensional covariation in the evaluation of pictorial stimuli</title>
		<author>
			<persName><forename type="first">M</forename><surname>Greenwald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Psychophysiol</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="51" to="64" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Statistical mapping between articulatory movements and acoustic spectrum using a Gaussian mixture model</title>
		<author>
			<persName><forename type="first">T</forename><surname>Toda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tokuda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Commun</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="215" to="227" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Tracking changes in continuous emotion states using body language and prosodic cues</title>
		<author>
			<persName><forename type="first">A</forename><surname>Metallinou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Katsamanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The USC CreativeIT database: a multimodal database of theatrical improvisation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Metallinou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Busso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Carnicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Multimodal Corpora, LREC</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Ranking-based emotion recognition for music organization and retrieval</title>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio Speech Lang. Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="762" to="774" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Primitives based estimation and evaluation of emotions in speech</title>
		<author>
			<persName><forename type="first">M</forename><surname>Grimm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mower</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kroschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Commun</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="787" to="800" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Speech emotion estimation in 3D space</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Parsons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mower</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Intl. Conf. on Multimedia &amp; Expo (ICME)</title>
		<meeting>of IEEE Intl. Conf. on Multimedia &amp; Expo (ICME)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Naturalistic affective expression classification by a multi-stage approach based on hidden Markov models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bianchi-Berthouze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACII</title>
		<meeting>of ACII</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Context-sensitive learning for enhanced audiovisual emotion classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Metallinou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Woellmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Katsamanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. of Affective Computing to Appear</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A supervised approach to movie emotion tracking</title>
		<author>
			<persName><forename type="first">N</forename><surname>Malandrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Potamianos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Evangelopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zlatintsi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Continuous prediction of spontaneous affect from multiple cues and modalities in valence-arousal space</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gunes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Affective Computing</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Abandoning emotion classes -towards continuous emotion recognition with modelling of long-range dependencies</title>
		<author>
			<persName><forename type="first">M</forename><surname>Woellmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Douglas-Cowie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cowie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Interspeech</title>
		<meeting>of Interspeech</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Combining long short-term memory and dynamic Bayesian networks for incremental emotion-sensitive artificial listening</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wollmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rigoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Top. Signal Process</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="867" to="881" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Affective video content representation and modeling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hanjalic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-Q</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="143" to="154" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Extracting moods from pictures and sounds: towards truly personalized TV</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hanjalic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process Mag</title>
		<imprint>
			<biblScope unit="page" from="90" to="100" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Multimodal emotion recognition, Handbook of Pattern Recognition and Computer Vision</title>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>World Scientific</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A survey of affect recognition methods: audio, visual, and spontaneous expressions</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Roisman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="39" to="58" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bi-modal emotion recognition from expressive face and body gestures</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gunes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Piccardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Netw. Comput. Appl</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1334" to="1345" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Automatic temporal segment detection and affect recognition from face and body display</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gunes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Piccardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst. Man Cybern. Part B Spec. Issue Hum. Comput</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="64" to="84" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Towards a minimal representation of affective gestures</title>
		<author>
			<persName><forename type="first">D</forename><surname>Glowinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Camurri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Volpe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mortillaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Affective Comput</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="106" to="118" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multimodal emotion recognition from expressive faces, body gestures and speech</title>
		<author>
			<persName><forename type="first">G</forename><surname>Castellano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kessous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Caridakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACII</title>
		<meeting>of ACII</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Recognising human emotions from body movement and gesture dynamics</title>
		<author>
			<persName><forename type="first">G</forename><surname>Castellano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Villalba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Camurri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACII</title>
		<meeting>of ACII</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Automatic recognition of non-acted affective postures</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kleinsmith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bianchi-Berthouze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Steed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst. Man Cybern. Part B</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1027" to="1038" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A system for real-time multimodal analysis of nonverbal affective social interaction in user-centric media</title>
		<author>
			<persName><forename type="first">G</forename><surname>Varni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Volpe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Camurri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="576" to="590" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Estimation of ordinal approach-avoidance labels in dyadic interactions: ordinal logistic regression approach</title>
		<author>
			<persName><forename type="first">V</forename><surname>Rozgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Katsamanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Baucom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Georgiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Detecting affect from non-stylised body motions</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bernhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACII</title>
		<meeting>of ACII</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Kleinsmith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bianchi-Berthouze</surname></persName>
		</author>
		<title level="m">Proceedings of the 2nd Intl Conf on Affective Computing and Intelligent Interaction (ACII)</title>
		<meeting>the 2nd Intl Conf on Affective Computing and Intelligent Interaction (ACII)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note>Recognizing affective dimensions from body posture</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Automatic analysis of affective postures and body motion to detect engagement with a game companion</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sanghvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Castellano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Leite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcowan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Paiva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM/IEEE Intl Conf. on Human-Robot Interaction</title>
		<meeting>of ACM/IEEE Intl Conf. on Human-Robot Interaction</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Affective body expression perception and recognition: a survey</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kleinsmith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bianchi-Berthouze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Affective Comput</title>
		<imprint>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Voice conversion based on maximum likelihood estimation of spectral parameter trajectory</title>
		<author>
			<persName><forename type="first">T</forename><surname>Toda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tokuda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio Speech Lang. Process</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="2222" to="2235" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A theoretical analysis of speech recognition based on feature trajectory models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Minami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nakamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Katagiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Interspeech</title>
		<meeting>of Interspeech</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A minimum converted trajectory error (MCTE) approach to high quality speech-to-lips conversion</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodanand</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">K</forename><surname>Soong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Interspeech</title>
		<meeting>of Interspeech</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">IEMOCAP: interactive emotional dyadic motion capture database</title>
		<author>
			<persName><forename type="first">C</forename><surname>Busso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bulut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mower</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lang. Resour. Eval</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="335" to="359" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">FEELTRACE: an instrument for recording perceived emotion in real time</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Douglas-Cowie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savvidou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mcmahon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sawey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schröder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA Workshop on Speech and Emotion</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="19" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A multimodal database for affect recognition and implicit tagging</title>
		<author>
			<persName><forename type="first">M</forename><surname>Soleymani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lichtenauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Affective Comput</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="42" to="55" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Recognizing emotion from dance movement: comparison of spectator recognition and automated techniques</title>
		<author>
			<persName><forename type="first">A</forename><surname>Camurri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Lagerlof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Volpe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Hum. Comput. Stud</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="213" to="225" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">GTrace -General trace program from Queen&apos;s, Belfast</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sawey</surname></persName>
		</author>
		<ptr target="http://www.dfki.de/schroed/feeltrace/2011" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A globally-variant locally-constant model for fusion of labels from multiple diverse experts without using reference labels</title>
		<author>
			<persName><forename type="first">K</forename><surname>Audhkhasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint/>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multi-score learning for affect recognition: the case of body postures</title>
		<author>
			<persName><forename type="first">H</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kleinsmith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bianchi-Berthouze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACII</title>
		<meeting>of ACII</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Harrigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rosenthal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Scherer</surname></persName>
		</author>
		<title level="m">The new handbook of Methods in Nonverbal Behavior Research</title>
		<imprint>
			<publisher>Oxford Univ. Press</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Feature selection based on mutual information: criteria of max-dependency, max-relevance, and min-redundancy</title>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1226" to="1238" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stork</surname></persName>
		</author>
		<title level="m">Pattern Classification</title>
		<imprint>
			<publisher>Springer-Verlag New York, Inc</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Emotional speech recognition: resources, features, and methods</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ververidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kotropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Commun</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1162" to="1181" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">S</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Evermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kershaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Odell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ollason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Valtchev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Woodland</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>The HTK Book</publisher>
			<pubPlace>Cambridge, England</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Entropic Cambridge Research Laboratory</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Framewise phoneme classification with bidirectional LSTM and other neural network architectures</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="602" to="610" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Context-sensitive multimodal emotion recognition from speech and facial expression using bidirectional LSTM modeling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wöllmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Metallinou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Interspeech</title>
		<meeting>of Interspeech<address><addrLine>Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<ptr target="http://sourceforge.net/projects/rnnl/2010" />
		<title level="m">RNNLib toolbox</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Chapter 3: vocal expression of affect</title>
		<author>
			<persName><forename type="first">P</forename><surname>Juslin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The New Handbook of Methods in Nonverbal Behavior Research</title>
		<imprint>
			<publisher>Oxford Univ. Press</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="65" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">On-line emotion recognition in a 3-D activation-valence-time continuum using acoustic and linguistic cues</title>
		<author>
			<persName><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Woellmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Douglas-Cowie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cowie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Multimodal User Interfaces</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="7" to="19" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Audio-visual emotion recognition using an emotion space concept</title>
		<author>
			<persName><forename type="first">I</forename><surname>Kanluan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grimm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kroschel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EUSIPCO</title>
		<meeting>of EUSIPCO</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Henley</surname></persName>
		</author>
		<title level="m">Body politics revisited: what do we know today? in: Gender, Power, and Communication in Human Relationships</title>
		<meeting><address><addrLine>Lawrence Erlbaum, Hillsdale, NJ</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
