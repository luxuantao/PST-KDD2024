<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Enhanced random search based incremental extreme learning machine</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2007-11-21">21 November 2007</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Guang-Bin</forename><surname>Huang</surname></persName>
							<email>egbhuang@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical and Electronic Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<addrLine>Nanyang Avenue</addrLine>
									<postCode>639798</postCode>
									<country>Singapore, Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lei</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical and Electronic Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<addrLine>Nanyang Avenue</addrLine>
									<postCode>639798</postCode>
									<country>Singapore, Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<addrLine>3 Science Drive 2</addrLine>
									<postCode>117543</postCode>
									<country>Singapore, Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Enhanced random search based incremental extreme learning machine</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2007-11-21">21 November 2007</date>
						</imprint>
					</monogr>
					<idno type="MD5">2856820E57565F96061EDEEFB59A6963</idno>
					<idno type="DOI">10.1016/j.neucom.2007.10.008</idno>
					<note type="submission">Received 27 June 2007; received in revised form 21 October 2007; accepted 25 October 2007</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T03:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Incremental extreme learning machine</term>
					<term>Convergence rate</term>
					<term>Random hidden nodes</term>
					<term>Random search</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently an incremental algorithm referred to as incremental extreme learning machine (I-ELM) was proposed by Huang et al. [G.-B. Huang, L. Chen, C.-K. Siew, Universal approximation using incremental constructive feedforward networks with random hidden nodes, IEEE Trans. Neural Networks 17(4) (2006) 879-892], which randomly generates hidden nodes and then analytically determines the output weights. Huang et al. [G.-B. Huang, L. Chen, C.-K. Siew, Universal approximation using incremental constructive feedforward networks with random hidden nodes, IEEE Trans. Neural Networks 17(4) ( <ref type="formula">2006</ref>) 879-892] have proved in theory that although additive or RBF hidden nodes are generated randomly the network constructed by I-ELM can work as a universal approximator. During our recent study, it is found that some of the hidden nodes in such networks may play a very minor role in the network output and thus may eventually increase the network complexity. In order to avoid this issue and to obtain a more compact network architecture, this paper proposes an enhanced method for I-ELM (referred to as EI-ELM). At each learning step, several hidden nodes are randomly generated and among them the hidden node leading to the largest residual error decreasing will be added to the existing network and the output weight of the network will be calculated in a same simple way as in the original I-ELM. Generally speaking, the proposed enhanced I-ELM works for the widespread type of piecewise continuous hidden nodes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In the past decades single-hidden-layer feedforward networks (SLFNs) (may or may not be neural based) have been investigated extensively from both theoretical and application aspects. SLFN network functions with n hidden nodes can be represented by</p><formula xml:id="formula_0">f n ðxÞ ¼ X n i¼1 b i g i ðxÞ ¼ X n i¼1 b i Gðx; a i ; b i Þ; a i 2 C d , x 2 C d ; b i 2 C; b i 2 C,<label>ð1Þ</label></formula><p>where g i or Gðx; a i ; b i Þ denotes the output function of the ith hidden node and b i is the (output) weight of the connection between the ith hidden node and the output node. The hidden nodes of SLFNs can be neuron alike or non-neuron alike, including additive or radial basis function (RBF) type of nodes <ref type="bibr" target="#b7">[8]</ref>, multiplicative nodes, fuzzy rules <ref type="bibr" target="#b9">[10]</ref>, fully complex nodes <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b18">19]</ref>, hinging functions <ref type="bibr" target="#b1">[2]</ref>, high-order nodes <ref type="bibr" target="#b4">[5]</ref>, ridge polynomials <ref type="bibr" target="#b21">[22]</ref>, wavelets <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, Fourier terms <ref type="bibr" target="#b5">[6]</ref>, etc.</p><p>For the specific example of the standard single-hiddenlayer feedforward neural networks, three types of hidden nodes have been mainly used:</p><p>(1) Additive hidden nodes:</p><formula xml:id="formula_1">g i ðxÞ ¼ gða i Á x þ b i Þ; a i 2 R d ; b i 2 R, (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where g is the activation function of hidden nodes. (2) RBF hidden nodes:</p><formula xml:id="formula_3">g i ðxÞ ¼ gðb i kx À a i kÞ; a i 2 R d ; b i 2 R.<label>(3)</label></formula><p>(3) Fully complex hidden nodes <ref type="bibr" target="#b14">[15]</ref>:</p><formula xml:id="formula_4">g i ðxÞ ¼ Y s i l¼1 gða il Á x þ b i Þ; a il 2 C d ; x 2 C d ; b i 2 C, b i 2 C,<label>ð4Þ</label></formula><p>where s i is an integer constant.</p><p>However, the output functions Gðx; a i ; b i Þ to be studied in this paper can be non-neuron alike and can be different from the above mentioned as long as they are nonlinear and piecewise continuous.</p><p>Conventional neural network theories <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b20">21]</ref> show that SLFNs with additive or RBF hidden nodes can work as universal approximators when all the parameters of the networks are allowed adjustable. However, tuning all the parameters of the networks may cause learning complicated and inefficient since tuning based learning may easily converge to local minima and/or may generally be very slow due to improper learning steps. Many iterative learning steps may be required by such learning algorithms in order to obtain better learning performance. It may be difficult to apply tuning methods in neural networks with nondifferentiable activation functions such as threshold networks. Interestingly, different from the common understanding on the neural networks White <ref type="bibr" target="#b23">[24]</ref> found that ''random search'' over input to hidden layer connections is computationally efficient in SLFNs with affine transformation and no additional learning is required. Unlike the conventional neural network theories, Huang et al. <ref type="bibr" target="#b7">[8]</ref> have recently proved that SLFNs with additive or RBF hidden nodes and with randomly generated hidden node parameters ða i ; b i ) <ref type="foot" target="#foot_0">1</ref> can work as universal approximators by only calculating the output weights b i linking the hidden layer to the output nodes. In such SLFNs implementations, the activation functions for additive nodes can be any bounded nonconstant piecewise continuous functions and the activation functions for RBF nodes can be any integrable piecewise continuous functions. The resulting method referred to as incremental extreme learning machines (I-ELM) <ref type="bibr" target="#b7">[8]</ref> randomly adds nodes to the hidden layer one by one and freezes the output weights of the existing hidden nodes when a new hidden node is added. The original batch learning mode extreme learning machines (ELM) <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref> and their online sequential learning mode ELM (OS-ELM) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b19">20]</ref> are developed for SLFNs with fixed network architectures. The activation functions used in ELM include differentiable <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref> and nondifferentiable <ref type="bibr" target="#b11">[12]</ref> functions, continuous <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref> and noncontinuous <ref type="bibr" target="#b11">[12]</ref> functions, etc. I-ELM <ref type="bibr" target="#b7">[8]</ref> randomly adds nodes to the hidden layer one by one and freezes the output weights of the existing hidden nodes when a new hidden node is added. I-ELM <ref type="bibr" target="#b7">[8]</ref> is a ''random search'' method in the sense that in theory the residual error of I-ELM will decrease and I-ELM moves toward the target function further whenever a hidden node is randomly added. Huang et al. <ref type="bibr" target="#b8">[9]</ref> also shows its universal approximation capability for the case of fully complex hidden nodes. Huang et al. <ref type="bibr" target="#b6">[7]</ref> recently proposed a convex optimization method based incremental method to further improve the convergence rate of I-ELM by allowing properly adjusting the output weights of the existing hidden nodes when a new hidden node is added. Huang et al. <ref type="bibr" target="#b6">[7]</ref> also extends the ELM from additive and RBF type of SLFNs to ''generalized'' SLFNs (including those which are usually not considered as the standard single-hidden layer feedforward neural networks.) where any type of piecewise continuous hidden nodes Gðx; a i ; b i Þ could be used. These hidden nodes could include additive or RBF type of nodes, multiplicative nodes, fuzzy rules, fully complex nodes, hinging functions, high-order nodes, ridge polynomials, wavelets, and Fourier terms, etc.</p><p>In this paper we propose a simple improved implementation of I-ELM in order to achieve a more compact network architecture than the original I-ELM. Similar to the original I-ELM, the improved I-ELM is fully automatic in the sense that except for target errors and the allowed maximum number of hidden nodes no control parameters need to be manually set by users. Different from the original I-ELM which is studied in the real domain, this paper simply discusses the proposed enhanced I-ELM in the complex domain which considers the real domain as its specific case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Proposed enhanced I-ELM</head><p>Let L 2 ðX Þ be a space of functions f in a measurable compact subset X of the d-dimensional space C d such that jf j 2 are integrable. For u; v 2 L 2 ðX Þ, the inner product hu; vi is defined by hu; vi ¼ R X uðxÞvðxÞ dx. The closeness between network function f n and the target function f is measured by the L 2 distance:</p><formula xml:id="formula_5">kf n À f k ¼ Z X ðf n ðxÞ À f ðxÞÞðf n ðxÞ À f ðxÞÞ dx 1=2<label>(5)</label></formula><p>In this paper, the sample input space X is always considered as a bounded measurable compact subset of the space C d . The I-ELM <ref type="bibr" target="#b7">[8]</ref> adds random nodes to the hidden layer one by one and freezes the output weights of the existing hidden nodes after a new hidden node is added. Denote the residual error of f n as e n f À f n where f 2 L 2 ðX Þ is the target function. Huang et al. <ref type="bibr" target="#b7">[8]</ref> have proved (for X 2 R d ) that: Lemma 2.1 (Huang et al., <ref type="bibr" target="#b7">[8]</ref>). Given any bounded nonconstant piecewise continuous function g : R ! R for additive nodes or any integrable piecewise continuous function g : R ! R and R R gðxÞ dxa0 for RBF nodes, for any continuous target function f and any randomly generated function sequence fg n g, lim n!1 kf À f n k ¼ 0 holds with probability one if</p><formula xml:id="formula_6">b n ¼ he nÀ1 ; g n i kg n k 2 . (<label>6</label></formula><formula xml:id="formula_7">)</formula><p>Lemma 2.1 shows that different from the common understanding in neural networks, from the function approximation point of view the hidden nodes of SLFNs need not be tuned at all. In order to train SLFNs with additive or RBF nodes one only needs to randomly assign the hidden nodes (''random searching'') and analytically calculate the output weights only. No tuning is required in the training of SLFNs. Furthermore, the universal approximation capability of ELM for fixed network architectures <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref> is the straightforward corollary of Lemma 2.1. In fact, seen from the proof of <ref type="bibr" target="#b7">[8]</ref>, Lemma 2.1 is valid if and only if Gðx; a; bÞ is piecewise continuous and spanfGðx; a; bÞ : ða; bÞ 2 C d Â Cg is dense in L 2 . Based on the same proof of <ref type="bibr" target="#b7">[8]</ref> we can further extend <ref type="bibr" target="#b7">[8]</ref> from the standard single-hidden layer feedforward neural networks (with additive or RBF hidden nodes) to ''generalized'' SLFNs</p><formula xml:id="formula_8">f n ðxÞ ¼ P n i¼1 b i Gðx; a i ; b i Þ with any type of hidden nodes Gðx; a i ; b i Þ.</formula><p>Theorem 2.1. Given a SLFN with any nonconstant piecewise continuous hidden nodes Gðx; a; bÞ, if spanfGðx; a; bÞ : ða; bÞ 2 C d Â Cg is dense in L 2 , then for any continuous target function f and any function sequence fg n ðxÞ ¼ Gðx; a n ; b n Þg randomly generated based on any continuous sampling distribution,</p><formula xml:id="formula_9">lim n!1 kf À ðf nÀ1 þ b n g n Þk ¼ 0 holds with probability one if b n ¼ he nÀ1 ; g n i kg n k 2 . (<label>7</label></formula><formula xml:id="formula_10">)</formula><p>Proof. The proof method of I-ELM <ref type="bibr" target="#b7">[8]</ref> can be adopted to prove the validity of this theorem without any major modification.</p><p>Similar to <ref type="bibr" target="#b7">[8]</ref>, for any function sequence fg n ðxÞ ¼ Gðx; a n ; b n Þg randomly generated based on any continuous sampling distribution we can prove that</p><formula xml:id="formula_11">ke n k ¼ kf À ðf nÀ1 þ b n g n Þk achieves its minimum if b n ¼ he nÀ1 ; g n i kg n k 2 . (<label>8</label></formula><formula xml:id="formula_12">)</formula><p>Furthermore, when b n ¼ he nÀ1 ;g n i kg n k 2 , the sequence fke n kg decreases and converges.</p><p>As spanfGðx; a; bÞg is dense in L 2 and Gðx; a; bÞ is a nonconstant piecewise continuous function, in order to prove lim n!þ1 ke n k ¼ 0, seen from the proof of the original I-ELM <ref type="bibr" target="#b7">[8]</ref> we only need to prove e n ? ðe nÀ1 À e n Þ.</p><p>According to formula (1), we have</p><formula xml:id="formula_13">e n ¼ f À f n ¼ e nÀ1 À b n g n .<label>(9)</label></formula><p>Furthermore, we have</p><formula xml:id="formula_14">he n ; g n i ¼ he nÀ1 À b n g n ; g n i ¼ he nÀ1 ; g n i À b n kg n k 2 . (<label>10</label></formula><formula xml:id="formula_15">)</formula><p>According to formula (8), we further have</p><formula xml:id="formula_16">he n ; g n i ¼ he nÀ1 ; g n i À he nÀ1 ; g n i kg n k 2 Á kg n k 2 ¼ he nÀ1 ; g n i À he nÀ1 ; g n i ¼ 0.<label>ð11Þ</label></formula><p>Thus, from formulas ( <ref type="formula" target="#formula_13">9</ref>) and ( <ref type="formula" target="#formula_16">11</ref>) we have he n ; e n À e nÀ1 i ¼ he n ; Àb n g n i ¼ 0, which means e n ? ðe n À e nÀ1 Þ. This completes the proof of this theorem. &amp; Different from the convex optimization based incremental extreme learning machine (CI-ELM) <ref type="bibr" target="#b6">[7]</ref>, Theorem 2.1 is valid for the case where all the output weights of the existing hidden nodes are frozen when a new hidden node is added.</p><p>It should be noted that although hidden nodes can be added randomly some newly added hidden nodes may make residual error reduce less and some newly added hidden nodes may make residual error reduce more. Based on this observation, in this paper we propose an enhanced I-ELM algorithm. In this new method, at any step, among k trial of hidden nodes, the hidden nodes with greatest residual error reduction will be added, where k is a fixed constant.</p><p>Theorem 2.2. Given an SLFN with any nonconstant piecewise continuous hidden nodes Gðx; a; bÞ, if spanfGðx; a; bÞ : ða; bÞ 2 C d Â Cg is dense in L 2 , for any continuous target function f and any randomly generated function sequence fg n g and any positive integer k,</p><formula xml:id="formula_17">lim n!1 kf À f Ã n k ¼ 0 holds with probability one if b Ã n ¼ he Ã nÀ1 ; g Ã n i kg Ã n k 2 , (<label>12</label></formula><formula xml:id="formula_18">)</formula><formula xml:id="formula_19">where f Ã n ¼ P n i¼1 b Ã i g Ã i , e Ã n ¼ f À f Ã n and g Ã n ¼ fg i jmin ðnÀ1Þkþ1pipnk kðf À f Ã nÀ1 Þ À b n g i kg.</formula><p>Proof. Given the function sequence fg n g which is randomly generated based on any continuous sampling distribution probability, for fixed k we can choose an element of the nth segment fg ðnÀ1Þkþ1 ; . . . ; g nk g as g</p><formula xml:id="formula_20">Ã n which minimizes kðf À f Ã nÀ1 Þ À b n g i k, i ¼ ðn À 1Þk þ 1; . . . ; nk. Similar to [8], we can prove that ke Ã n k ¼ kf À ðf Ã nÀ1 þ b Ã n g Ã n Þk</formula><p>achieves its minimum, and the sequence fke n kg decreases and converges if b</p><formula xml:id="formula_21">Ã n ¼ he Ã nÀ1 ;g Ã n i kg Ã n k 2 .</formula><p>As spanfGðx; a; bÞg is dense in L 2 and Gðx; a; bÞ is a nonconstant piecewise continuous function, seen from the proof of the original I-ELM <ref type="bibr" target="#b7">[8]</ref> (Lemmas II.5 and II.6 of <ref type="bibr" target="#b7">[8]</ref> are also valid for the sequence fg Ã n g 1 n¼1 ) in order to prove lim n!þ1 ke Ã n k ¼ 0 we only need to prove e Ã n ? ðe</p><formula xml:id="formula_22">Ã nÀ1 À e Ã n Þ. As e Ã n ¼ f À f Ã n ¼ e Ã nÀ1 À b Ã n g Ã n , we have he Ã n ; g Ã n i ¼ he Ã nÀ1 À b Ã n g Ã n ; g Ã n i ¼ he Ã nÀ1 ; g Ã n i À b Ã n kg Ã n k 2 . (<label>13</label></formula><formula xml:id="formula_23">)</formula><p>According to formula <ref type="bibr" target="#b11">(12)</ref>, we further have</p><formula xml:id="formula_24">he Ã n ; g Ã n i ¼ he Ã nÀ1 ; g Ã n i À he Ã nÀ1 ; g Ã n i kg Ã n k 2 Á kg Ã n k 2 ¼ he Ã nÀ1 ; g Ã n i À he Ã nÀ1 ; g Ã n i ¼ 0.<label>ð14Þ</label></formula><formula xml:id="formula_25">Thus, we have he Ã n ; e Ã n À e Ã nÀ1 i ¼ he Ã n ; Àb Ã n g Ã n i ¼ 0, which means e Ã n ? ðe Ã n À e Ã nÀ1 Þ.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>This completes the proof of this theorem. &amp;</head><p>According to Theorem 2.2, when the nth hidden node is added, the weight b n linking the new node to the output node should be chosen as</p><formula xml:id="formula_26">he Ã nÀ1 ;g Ã n i kg Ã n k 2 .</formula><p>In practice, it could not be calculated since the exact functional form of e Ã nÀ1 is unavailable. Similar to I-ELM <ref type="bibr" target="#b7">[8]</ref>, a consistent estimate of the weight b n based on the training set is (originally proposed by Kwok and Yeung <ref type="bibr" target="#b15">[16]</ref> for the case where the hidden nodes are not randomly added):</p><formula xml:id="formula_27">b n ¼ E Á H T H Á H T ¼ P N p¼1 eðpÞhðpÞ P N p¼1 h 2<label>ðpÞ</label></formula><formula xml:id="formula_28">, (<label>15</label></formula><formula xml:id="formula_29">)</formula><p>where hðpÞ is the activation of the new hidden node for the input of pth training sample and eðpÞ is the corresponding residual error before this new hidden neuron is added. H ¼ ½hð1Þ; . . . ; hðNÞ T is the activation vector of the new node for all the N training samples and E ¼ ½eð1Þ; Á Á Á ; eðNÞ T is the residual vector before this new hidden node added. In real applications, one may not really wish to get zero approximation error by adding infinite neurons to the network, a maximum number of hidden neurons is normally given. Thus, such an incremental constructive method 2,3 for SLFNs can be summarized as follows: EI-ELM Algorithm. Given a training set @ ¼ fðx i ; t i Þjx i 2 C d ; t i 2 C; i ¼ 1; . . . ; Ng, hidden node output function Gðx; a; bÞ, maximum number L max of hidden nodes, maximum number k of trials of assigning random hidden nodes at each step, and expected learning accuracy , step 1 Initialization: Let L ¼ 0 and residual error E ¼ t, where t ¼ ½t 1 ; . . ; t N T .</p><p>step 2 Learning step: while LoL max and kEk4 (a) Increase by 1 the number of hidden nodes L: </p><formula xml:id="formula_30">L ¼ L þ 1. (b) for i ¼ 1 : k (i)</formula><formula xml:id="formula_31">b ðiÞ ¼ E Á H T<label>ðiÞ</label></formula><formula xml:id="formula_32">H ðiÞ Á H T ðiÞ . (<label>16</label></formula><formula xml:id="formula_33">)</formula><p>(iii) Calculate the residual error after adding the new hidden node L:</p><formula xml:id="formula_34">E ðiÞ ¼ E À b ðiÞ Á H ðiÞ . (<label>17</label></formula><formula xml:id="formula_35">)</formula><formula xml:id="formula_36">endfor (c) Let i Ã ¼ fijmin 1pipk kE ðiÞ kg. Set E ¼ E ðiÞ , a L ¼ a ði Ã Þ , b L ¼ b ði Ã Þ , and b L ¼ b ði Ã Þ . endwhile 3.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance evaluation</head><p>In this section, the performance of the proposed enhanced incremental learning algorithm is compared with the original I-ELM on benchmark regression problems from UCI database <ref type="bibr" target="#b0">[1]</ref>. The specification of these benchmark problems are shown in Table <ref type="table" target="#tab_0">1</ref>. In our experiments, all the inputs (attributes) have been normalized into the range ½À1; 1 while the outputs (targets) have been normalized into ½0; 1. Neural networks are tested in I-ELM and its enhancements EI-ELM. Popular additive and RBF hidden nodes in the real domain space (a specific case of the  <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25]</ref> suggested to use ''random hidden nodes'' in the SLFNs augmented by connections from the input layer to output layer): <ref type="bibr" target="#b24">[25]</ref> proposed an interesting incremental learning algorithm named QuickNet. Similar to I-ELM and EI-ELM, QuickNet randomly adds hidden nodes one by one. However, unlike I-ELM and EI-ELM which only simply calculates the output weight of the newly added hidden node, QuickNet readjusts the output weights of the existing hidden nodes after a new hidden node is added which may require more computational burden than I-ELM and EI-ELM. Simply speaking, EI-ELM can be considered a further simplification of White's incremental methods <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>. 3 Instead of randomly adding hidden nodes, Kwok and Yeung <ref type="bibr" target="#b15">[16]</ref> adds the optimal hidden nodes one by one. Unlike White's works <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25]</ref> and I-ELM and EI-ELM, optimization learning is required in Kwok and Yeung <ref type="bibr" target="#b15">[16]</ref>. When a new hidden node is added both Kwok and Yeung <ref type="bibr" target="#b15">[16]</ref> and EI-ELM do not readjust the existing hidden nodes and they compute the output weights in a similar way: b</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARTICLE IN PRESS</head><formula xml:id="formula_37">f n ðxÞ ¼ y Á x þ P n i¼1 b i gða i Á x þ b i Þ, especially White</formula><formula xml:id="formula_38">Ã n ¼ he Ã nÀ1 ;g Ã n i kg Ã n k 2 .</formula><p>The newly added node g Ã n in EI-ELM will be as close to the optimal hidden node in Kwok and Yeung <ref type="bibr" target="#b15">[16]</ref> as required if the number of selecting trial k in EI-ELM is sufficiently large. I-ELM is a specific case of EI-ELM when k ¼ 1. In this sense EI-ELM has unified the I-ELM, Kwok and Yeung <ref type="bibr" target="#b15">[16]</ref>, and White's work <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>complex domain) are used. It should be noted that similar results can be obtained in the complex domain as well. For the sigmoid type of additive hidden nodes Gðx; a; bÞ ¼</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">1þe</head><p>ÀðaÁxþbÞ the hidden parameters a and b are randomly chosen from the range ½À1; 1. For the RBF hidden nodes Gðx; a; bÞ ¼ expðÀbkx À ak 2 Þ, the hidden node parameters a are randomly chosen from the range ½À1; 1 whereas the hidden node parameter b is randomly generated from the range ð0; 0:5Þ. The target error is set as ¼ 0:01. All the simulations are running in MATLAB 6.5 environment and the same PC with Pentium 4 2.99 GHZ CPU and 1G RAM. For each problem, the average results over 20 trials are obtained for EI-ELM and I-ELM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Comparison between EI-ELM and I-ELM with the same number of hidden nodes</head><p>We first compare the generalization performance of EI-ELM and I-ELM with the same number of hidden nodes. Tables <ref type="table">2</ref> and<ref type="table" target="#tab_1">3</ref> show the performance of EI-ELM and I-ELM with 200 additive and RBF nodes, respectively. The close results obtained by different algorithms are underlined and the apparent better results are shown in boldface. Seen from Tables <ref type="table">2</ref> and<ref type="table" target="#tab_1">3</ref>, the testing root mean square error (RMSE) of EI-ELM are generally much smaller than that of the original I-ELM when both EI-ELM and I-ELM use the same number of hidden nodes. It shows that EI-ELM may achieve faster convergence rate than I-ELM under the same nodes. This can also be verified from Fig. <ref type="figure" target="#fig_2">1</ref> which shows testing error curves of EI-ELM and I-ELM for both Airplane and Census cases. Furthermore, as observed from Tables <ref type="table">2</ref> and<ref type="table" target="#tab_1">3</ref>, EI-ELM obtain much smaller standard deviation of testing RMSE than I-ELM. This means that the performance of EI-ELM is stabler than I-ELM.</p><p>The training time curves for several cases are shown in Fig. <ref type="figure" target="#fig_1">2</ref> (for sigmoid additive node case). Seen from Fig. <ref type="figure" target="#fig_1">2</ref>, the spent learning time is still linearly increasing with the learning steps, which is the same as I-ELM's conclusion. Since the training time of I-ELM linearly increases with the number of hidden nodes, in theory the training time of EI-ELM should be around k times that of I-ELM when the same number of hidden nodes are added, where k is optimal trial times. In our simulations, we set k ¼ 10. The training time of EI-ELM should be almost 10 times of I-ELM. However, EI-ELM needs to seek for the most appropriate parameters from k trials, furthermore, for all the applications, we need to store some temp variables, which normally use large memory. When memory reach    <ref type="table">2</ref> and<ref type="table" target="#tab_1">3</ref> have verified our conclusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARTICLE IN PRESS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Comparison between EI-ELM and I-ELM with the different number of hidden nodes</head><p>During our simulations we also studied the case where EI-ELM and I-ELM are given different number of hidden nodes. The comparison between EI-ELM with 50 hidden nodes and I-ELM with 500 hidden nodes are shown in Tables <ref type="table" target="#tab_2">4</ref> and<ref type="table" target="#tab_3">5</ref>. Here we set k ¼ 10 and 20 for EI-ELM, respectively. As observed from Tables <ref type="table" target="#tab_2">4</ref> and<ref type="table" target="#tab_3">5</ref>, the generalization performance obtained by EI-ELM with 50 hidden nodes are usually slightly better than those obtained by I-ELM with 500 hidden nodes. It further demonstrates that EI-ELM can converge faster than I-ELM. It can also be seen that the training time spent by I-ELM with 500 hidden nodes and EI-ELM with 50 hidden nodes are in the same order when k ¼ 10, which is consistent with our previous analysis on the training time of EI-ELM. However, compact network architecture implies faster prediction time. It can be seen that EI-ELM with only 50 hidden nodes can achieve the similar results as I-ELM with 500 hidden nodes do. Fig. <ref type="figure" target="#fig_4">3</ref>     the generalization performance and its stability tend to become stable after k increases up to certain number (around 20 in Airplane case). The above observations are true for the rest cases as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>In this paper, inspired by the latest theory <ref type="bibr" target="#b7">[8]</ref>, we proposed an enhanced random search based incremental algorithm EI-ELM for I-ELM. Similar to I-ELM, EI-ELM also randomly generates hidden nodes and then analytically determines the output weights. EI-ELM works for different type of hidden node output functions instead of neural networks only. It should be noted that I-ELM is a specific case of EI-ELM when k ¼ 1. The difference between I-ELM and EI-ELM is that at each learning step among several randomly generated hidden nodes EI-ELM picks the optimal hidden node which leads to the smallest residual error. Compared with the original I-ELM, EI-ELM can achieve faster convergence rate and much more compact network architectures, which have been further verified by the simulation results on some benchmark realworld regression problems. Reasonably speaking, EI-ELM could also be applied to the CI-ELM <ref type="bibr" target="#b6">[7]</ref> straightforward, the performance of EI-ELM with convex optimization is worth investigating in the future.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARTICLE IN PRESS</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Assign random parameters ða ðiÞ ; b ðiÞ Þ for the new hidden node L according to any continuous sampling distribution probability. (ii) Calculate the output weight b ðiÞ for the new hidden node:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Table 2</head><label>2</label><figDesc>Performance comparison between EI-ELM and I-ELM (both with 200 sigmoid hidden nodes) Name EI-ELM (sigmoid, k ¼ 10) I-ELM (sigmoid, k ¼ 1) Computer activity 0.0941 0.0028 3.7511 0.1201 0.0125 0.2794 Delta ailerons 0.0445 0.0062 2.7735 0.0525 0.0078 0.2620 Delta elevators 0.0582 0.0032 3.7971 0.0740 0.0126 0.2708 Kinematics 0:1393 0.0028 3.4373 0:1418 0.0033 0.2810 Machine CPU 0:0466 0.0060 0.3112 0:0504 0.0079 0.0234 Puma 0:1840 0.0017 3.9531 0:1861 0.0041 0.3236 Pyrim 0.1414 0.0341 0.3062 0.1867 0.0628 0.0374 Servo 0.1518 0.0116 0.3235 0.1662 0.0124 0.0218</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The testing error updating curves of EI-ELM and I-ELM.</figDesc><graphic coords="5,303.04,332.99,248.40,189.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>shows the testing error curves of I-ELM and EI-ELM with sigmoid activation functions for the Abalone case. It can be seen that in order to obtain the same testing RMSE 0.09, EI-ELM only needs 21 nodes and spends 0.2722 s on training while I-ELM needs 331 nodes and spends 0.5231 s on training. Similar curves have been found for other regression cases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>3. 3 .</head><label>3</label><figDesc>Effects of number of selecting trial factor kEffects of number of selecting trials k on the performance of EI-ELM have also been investigated in our study. Figs.4 and 5show that the generalization performance and stability of EI-ELM with 100 sigmoid hidden nodes will become better when the number of selecting trials k increases for the Airplane case. Fig.6shows the testing RMSE curves of EI-ELM with increasing number of hidden nodes and different number of selecting trials k. It can be seen that the effect of number of selecting trials k onARTICLE IN PRESS</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Training time spent by EI-ELM is linearly increased with the number of hidden nodes to be added.</figDesc><graphic coords="6,43.87,251.45,248.40,195.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Testing RMSE performance comparison between EI-ELM and I-ELM (with sigmoid hidden nodes) for Abalone case.</figDesc><graphic coords="7,34.05,346.93,248.40,191.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Effect of number of selecting trials k on the generalization performance of EI-ELM in airplane case.</figDesc><graphic coords="7,302.54,346.93,249.48,191.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Effect of number of selecting trials k on the stability of the generalization performance of EI-ELM in airplane case.</figDesc><graphic coords="8,43.87,69.98,248.40,207.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Testing RMSE updating progress with new hidden nodes added and different number of selecting trials k in airplane case.</figDesc><graphic coords="8,43.87,327.48,248.40,190.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell cols="3">Specification of 16 benchmarking regression datasets</cell><cell></cell></row><row><cell>Name</cell><cell cols="2">No. of observations</cell><cell>Attributes</cell></row><row><cell></cell><cell>Training data</cell><cell>Testing data</cell><cell></cell></row><row><cell>Abalone</cell><cell>2000</cell><cell>2177</cell><cell>8</cell></row><row><cell>Ailerons</cell><cell>7154</cell><cell>6596</cell><cell>39</cell></row><row><cell>Airplane</cell><cell>450</cell><cell>500</cell><cell>9</cell></row><row><cell>Auto price</cell><cell>80</cell><cell>79</cell><cell>15</cell></row><row><cell>Bank</cell><cell>4500</cell><cell>3692</cell><cell>8</cell></row><row><cell>Boston</cell><cell>250</cell><cell>256</cell><cell>13</cell></row><row><cell>California</cell><cell>8000</cell><cell>12 640</cell><cell>8</cell></row><row><cell>Census (House8L)</cell><cell>10 000</cell><cell>12 784</cell><cell>8</cell></row><row><cell>Computer activity</cell><cell>4000</cell><cell>4192</cell><cell>12</cell></row><row><cell>Delta ailerons</cell><cell>3000</cell><cell>4129</cell><cell>5</cell></row><row><cell>Delta elevators</cell><cell>4000</cell><cell>5517</cell><cell>6</cell></row><row><cell>Kinematics</cell><cell>4000</cell><cell>4192</cell><cell>8</cell></row><row><cell>Machine CPU</cell><cell>100</cell><cell>109</cell><cell>6</cell></row><row><cell>Puma</cell><cell>4500</cell><cell>3692</cell><cell>8</cell></row><row><cell>Pyrim</cell><cell>40</cell><cell>34</cell><cell>26</cell></row><row><cell>Servo</cell><cell>80</cell><cell>87</cell><cell>4</cell></row></table><note><p><p>2 </p>White et al. in their seminal works</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell cols="4">Performance comparison between EI-ELM and I-ELM (both with 200</cell></row><row><cell>RBF hidden nodes)</cell><cell></cell><cell></cell></row><row><cell>Name</cell><cell cols="2">EI-ELM (RBF, k ¼ 10)</cell><cell>I-ELM (RBF, k ¼ 1)</cell></row><row><cell></cell><cell>Mean Dev.</cell><cell cols="2">Time (s) Mean Dev.</cell><cell>Time (s)</cell></row><row><cell>Abalone</cell><cell cols="2">0.0829 0.0027 5.6006</cell><cell>0.0938 0.0053 0.5030</cell></row><row><cell>Ailerons</cell><cell cols="2">0.0774 0.0129 36.016</cell><cell>0.1430 0.0298 3.2769</cell></row><row><cell>Airplane</cell><cell cols="2">0.0633 0.0057 0.9578</cell><cell>0.0992 0.0166 0.0751</cell></row><row><cell>Auto price</cell><cell cols="2">0.1139 0.0189 0.4031</cell><cell>0.1261 0.0255 0.0468</cell></row><row><cell>Bank</cell><cell cols="2">0.0730 0.0022 9.8079</cell><cell>0.1157 0.0097 0.7782</cell></row><row><cell>Boston</cell><cell cols="2">0.1077 0.0084 0.7972</cell><cell>0.1320 0.0126 0.0657</cell></row><row><cell>California</cell><cell cols="2">0.1503 0.0022 17.133</cell><cell>0.1731 0.0081 1.3656</cell></row><row><cell>Census (8L)</cell><cell cols="2">0.0810 0.0016 19.922</cell><cell>0.0922 0.0029 1.7928</cell></row><row><cell cols="3">Computer activity 0.1153 0.0021 10.092</cell><cell>0.1552 0.0282 0.8220</cell></row><row><cell>Delta ailerons</cell><cell cols="2">0.0448 0.0065 4.6169</cell><cell>0.0632 0.0116 0.4327</cell></row><row><cell>Delta elevators</cell><cell cols="2">0.0575 0.0047 7.3541</cell><cell>0.0790 0.0123 0.6321</cell></row><row><cell>Kinematics</cell><cell cols="2">0.1213 0.0017 8.3114</cell><cell>0.1555 0.0122 0.6953</cell></row><row><cell>Machine CPU</cell><cell cols="2">0.0554 0.0148 0.4114</cell><cell>0.0674 0.0177 0.0447</cell></row><row><cell>Puma</cell><cell cols="2">0.1752 0.0022 9.7983</cell><cell>0.1913 0.0180 0.7872</cell></row><row><cell>Pyrim</cell><cell cols="2">0.1209 0.0431 0.4423</cell><cell>0.2241 0.1752 0.0434</cell></row><row><cell>Servo</cell><cell cols="2">0.1379 0.0151 0.4031</cell><cell>0.1524 0.0200 0.0391</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4</head><label>4</label><figDesc>Performance comparison between EI-ELM with 50 Sigmoid hidden nodes and I-ELM with 500 sigmoid hidden nodes</figDesc><table><row><cell>Problems</cell><cell cols="3">EI-ELM (50 sigmoid hidden nodes)</cell><cell></cell><cell></cell><cell></cell><cell cols="3">I-ELM (500 sigmoid hidden nodes, k ¼ 1)</cell></row><row><cell></cell><cell>k ¼ 10</cell><cell></cell><cell></cell><cell>k ¼ 20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Mean</cell><cell>Dev.</cell><cell>Time (s)</cell><cell>Mean</cell><cell>Dev.</cell><cell>Time (s)</cell><cell>Mean</cell><cell>Dev.</cell><cell>Time (s)</cell></row><row><cell>Abalone</cell><cell>0:0878</cell><cell>0.0033</cell><cell>0.6506</cell><cell>0:0876</cell><cell>0.0015</cell><cell>1.5785</cell><cell>0:0876</cell><cell>0.0033</cell><cell>0.7695</cell></row><row><cell>Ailerons</cell><cell>0.0640</cell><cell>0.0066</cell><cell>2.3766</cell><cell>0.0571</cell><cell>0.0022</cell><cell>6.2519</cell><cell>0.0824</cell><cell>0.0232</cell><cell>1.8810</cell></row><row><cell>Airplane</cell><cell>0.0922</cell><cell>0.0061</cell><cell>0.1389</cell><cell>0:0862</cell><cell>0.0040</cell><cell>0.2921</cell><cell>0:0898</cell><cell>0.0067</cell><cell>0.1466</cell></row><row><cell>Auto price</cell><cell>0.0924</cell><cell>0.0112</cell><cell>0.0814</cell><cell>0.0897</cell><cell>0.0104</cell><cell>0.1658</cell><cell>0.0948</cell><cell>0.0158</cell><cell>0.0561</cell></row><row><cell>Bank</cell><cell>0.1066</cell><cell>0.0058</cell><cell>0.9965</cell><cell>0.0896</cell><cell>0.0036</cell><cell>3.1058</cell><cell>0.0757</cell><cell>0.0032</cell><cell>0.7914</cell></row><row><cell>Boston</cell><cell>0:1133</cell><cell>0.0101</cell><cell>0.1065</cell><cell>0:1102</cell><cell>0.0061</cell><cell>0.2232</cell><cell>0:1084</cell><cell>0.0096</cell><cell>0.1033</cell></row><row><cell>California</cell><cell>0:1591</cell><cell>0.0034</cell><cell>2.2423</cell><cell>0:1548</cell><cell>0.0033</cell><cell>4.9486</cell><cell>0:1543</cell><cell>0.0019</cell><cell>1.5665</cell></row><row><cell>Census (8L)</cell><cell>0:0899</cell><cell>0.0017</cell><cell>2.8655</cell><cell>0:0865</cell><cell>0.0011</cell><cell>6.1100</cell><cell>0:0871</cell><cell>0.0018</cell><cell>2.1199</cell></row><row><cell>Computer activity</cell><cell>0.1075</cell><cell>0.0057</cell><cell>0.9342</cell><cell>0.0991</cell><cell>0.0036</cell><cell>2.3311</cell><cell>0.1057</cell><cell>0.0078</cell><cell>0.7185</cell></row><row><cell>Delta ailerons</cell><cell>0:0474</cell><cell>0.0062</cell><cell>0.7006</cell><cell>0:0467</cell><cell>0.0042</cell><cell>1.4570</cell><cell>0:0468</cell><cell>0.0052</cell><cell>0.6340</cell></row><row><cell>Delta elevators</cell><cell>0.0615</cell><cell>0.0049</cell><cell>0.9502</cell><cell>0.0586</cell><cell>0.0038</cell><cell>2.5385</cell><cell>0.0640</cell><cell>0.0055</cell><cell>0.6516</cell></row><row><cell>Kinematics</cell><cell>0:1420</cell><cell>0.0029</cell><cell>0.8655</cell><cell>0:1416</cell><cell>0.0019</cell><cell>2.9017</cell><cell>0:1406</cell><cell>0.0014</cell><cell>0.7117</cell></row><row><cell>Machine CPU</cell><cell>0:0498</cell><cell>0.0155</cell><cell>0.0750</cell><cell>0:0467</cell><cell>0.0148</cell><cell>0.1577</cell><cell>0:0474</cell><cell>0.0040</cell><cell>0.0645</cell></row><row><cell>Puma</cell><cell>0:1846</cell><cell>0.0018</cell><cell>0.9856</cell><cell>0:1827</cell><cell>0.0017</cell><cell>2.7264</cell><cell>0:1856</cell><cell>0.0039</cell><cell>0.7983</cell></row><row><cell>Pyrim</cell><cell>0.1514</cell><cell>0.0419</cell><cell>0.0782</cell><cell>0.1300</cell><cell>0.0405</cell><cell>0.1533</cell><cell>0.1712</cell><cell>0.0626</cell><cell>0.0810</cell></row><row><cell>Servo</cell><cell>0.1634</cell><cell>0.0129</cell><cell>0.0795</cell><cell>0:1558</cell><cell>0.0121</cell><cell>0.1611</cell><cell>0:1589</cell><cell>0.0124</cell><cell>0.0642</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5</head><label>5</label><figDesc>Performance comparison between EI-ELM with 50 RBF hidden nodes and I-ELM with 500 RBF hidden nodes Problems EI-ELM (50 RBF hidden nodes) I-ELM (500 RBF hidden nodes, k ¼ 1)</figDesc><table><row><cell></cell><cell>k ¼ 10</cell><cell></cell><cell></cell><cell>k ¼ 20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Mean</cell><cell>Dev.</cell><cell>Time (s)</cell><cell>Mean</cell><cell>Dev.</cell><cell>Time (s)</cell><cell>Mean</cell><cell>Dev.</cell><cell>Time (s)</cell></row><row><cell>Abalone</cell><cell>0:0907</cell><cell>0.0034</cell><cell>1.4036</cell><cell>0:0871</cell><cell>0.0023</cell><cell>3.0006</cell><cell>0:0872</cell><cell>0.0022</cell><cell>1.2121</cell></row><row><cell>Ailerons</cell><cell>0.0973</cell><cell>0.0229</cell><cell>9.0306</cell><cell>0.0775</cell><cell>0.0033</cell><cell>19.071</cell><cell>0.1129</cell><cell>0.0295</cell><cell>8.1818</cell></row><row><cell>Airplane</cell><cell>0.0943</cell><cell>0.0168</cell><cell>0.2347</cell><cell>0:0813</cell><cell>0.0102</cell><cell>0.5487</cell><cell>0:0772</cell><cell>0.0082</cell><cell>0.1940</cell></row><row><cell>Auto price</cell><cell>0.1187</cell><cell>0.0159</cell><cell>0.0998</cell><cell>0.1104</cell><cell>0.0148</cell><cell>0.2110</cell><cell>0.1231</cell><cell>0.0133</cell><cell>0.1189</cell></row><row><cell>Bank</cell><cell>0.0989</cell><cell>0.0031</cell><cell>2.4460</cell><cell>0:0888</cell><cell>0.0023</cell><cell>5.4199</cell><cell>0:0843</cell><cell>0.0058</cell><cell>1.9382</cell></row><row><cell>Boston</cell><cell>0.1197</cell><cell>0.0107</cell><cell>0.1845</cell><cell>0.1171</cell><cell>0.0078</cell><cell>0.3621</cell><cell>0.1214</cell><cell>0.0103</cell><cell>0.1872</cell></row><row><cell>California</cell><cell>0:1624</cell><cell>0.0049</cell><cell>4.2339</cell><cell>0:1579</cell><cell>0.0027</cell><cell>8.8326</cell><cell>0:1582</cell><cell>0.0027</cell><cell>3.8482</cell></row><row><cell>Census (8L)</cell><cell>0:0864</cell><cell>0.0026</cell><cell>4.9858</cell><cell>0:0846</cell><cell>0.0020</cell><cell>11.796</cell><cell>0:0860</cell><cell>0.0018</cell><cell>4.8536</cell></row><row><cell>Computer activity</cell><cell>0.1295</cell><cell>0.0068</cell><cell>2.4905</cell><cell>0.1201</cell><cell>0.0024</cell><cell>5.5878</cell><cell>0.1358</cell><cell>0.0177</cell><cell>2.1267</cell></row><row><cell>Delta ailerons</cell><cell>0.0469</cell><cell>0.0067</cell><cell>1.1800</cell><cell>0.0466</cell><cell>0.0039</cell><cell>2.4763</cell><cell>0.0544</cell><cell>0.0076</cell><cell>1.0361</cell></row><row><cell>Delta elevators</cell><cell>0.0603</cell><cell>0.0049</cell><cell>1.8515</cell><cell>0.0602</cell><cell>0.0039</cell><cell>4.2506</cell><cell>0.0685</cell><cell>0.0099</cell><cell>1.5399</cell></row><row><cell>Kinematics</cell><cell>0.1346</cell><cell>0.0025</cell><cell>2.0913</cell><cell>0.1306</cell><cell>0.0019</cell><cell>4.6727</cell><cell>0.1425</cell><cell>0.0095</cell><cell>1.7042</cell></row><row><cell>Machine CPU</cell><cell>0.0622</cell><cell>0.0281</cell><cell>0.1067</cell><cell>0.0511</cell><cell>0.0114</cell><cell>0.2031</cell><cell>0.0614</cell><cell>0.0274</cell><cell>0.0875</cell></row><row><cell>Puma</cell><cell>0.1789</cell><cell>0.0020</cell><cell>2.4465</cell><cell>0.1770</cell><cell>0.0012</cell><cell>5.2821</cell><cell>0.1850</cell><cell>0.0119</cell><cell>1.9709</cell></row><row><cell>Pyrim</cell><cell>0.1214</cell><cell>0.0345</cell><cell>0.1016</cell><cell>0.0989</cell><cell>0.0286</cell><cell>0.2079</cell><cell>0.2179</cell><cell>0.1545</cell><cell>0.1071</cell></row><row><cell>Servo</cell><cell>0.1487</cell><cell>0.0133</cell><cell>0.0985</cell><cell>0:1434</cell><cell>0.0120</cell><cell>0.1958</cell><cell>0:1410</cell><cell>0.0151</cell><cell>0.0982</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>In theory<ref type="bibr" target="#b7">[8]</ref>, from the function approximation point of view the hidden node parameters of the single-hidden layer feedforward neural networks are independent of the training data and of each other.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>G.-B. Huang, L. Chen / Neurocomputing 71 (2008) 3460-3468</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Halbert White, University of California, San Diego, US, for the helpful discussions and kind clarification on his significant seminal works.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Lei Chen received the B.Sc. degree in applied mathematics and the M. Sc. degree in operational research and control theory from Northeastern University, China, in 1999 and 2002, respectively. He is currently working toward the Ph. D. degree from Nanyang Technological University, Singapore. His research interests include artificial neural networks, pattern recognition, and machine learning.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">UCI repository of machine learning databases</title>
		<author>
			<persName><forename type="first">C</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Merz</surname></persName>
		</author>
		<ptr target="hhttp://www.ics.uci.edu/$mlearn/MLRepository.htmli" />
		<imprint>
			<date type="published" when="1998">1998</date>
			<pubPlace>Irvine, USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Information and Computer Sciences, University of California</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hinging hyperplanes for regression, classification, and function approximation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="999" to="1013" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Orthonormal bases of compactly supported wavelets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Daubechies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. Pure Appl. Math</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="909" to="996" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The wavelet transform, time-frequency localization and signal analysis</title>
		<author>
			<persName><forename type="first">I</forename><surname>Daubechies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="961" to="1005" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning, invariance, and generalization in high-order neural networks</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Maxwell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Opt</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="4972" to="4978" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improved extreme learning machine for function approximation by encoding a priori information</title>
		<author>
			<persName><forename type="first">F</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="2369" to="2373" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convex incremental extreme learning machine</title>
		<author>
			<persName><forename type="first">G.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="3056" to="3062" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Universal approximation using incremental constructive feedforward networks with random hidden nodes</title>
		<author>
			<persName><forename type="first">G.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Siew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="879" to="892" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Incremental extreme learning machine with fully complex hidden nodes, Neurocomputing</title>
		<author>
			<persName><forename type="first">G.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Siew</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2007.07.025</idno>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On-line sequential extreme learning machine</title>
		<author>
			<persName><forename type="first">G.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N.-Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Saratchandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sundararajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IASTED International Conference on Computational Intelligence (CI 2005)</title>
		<meeting><address><addrLine>Calgary, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">July 4-6, 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Extreme learning machine: RBF network case</title>
		<author>
			<persName><forename type="first">G.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Siew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Conference on Control, Automation, Robotics and Vision (ICARCV 2004)</title>
		<meeting>the Eighth International Conference on Control, Automation, Robotics and Vision (ICARCV 2004)<address><addrLine>Kunming, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-12-09">6-9 December, 2004</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1029" to="1036" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Can threshold networks be trained directly?</title>
		<author>
			<persName><forename type="first">G.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Z</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Siew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Saratchandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sundararajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Systems II</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="187" to="191" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Extreme learning machine: theory and applications</title>
		<author>
			<persName><forename type="first">G.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Siew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="489" to="501" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Real-time learning capability of neural networks</title>
		<author>
			<persName><forename type="first">G.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Siew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="863" to="878" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Approximation by fully complex multilayer perseptrons</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Adali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1641" to="1666" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Objective functions for training new hidden units in constructive neural networks</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1131" to="1148" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Testing for neglected nonlinearity in time series modes: a comparison of neural network methods and standard tests</title>
		<author>
			<persName><forename type="first">T.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W J</forename><surname>Granger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Econ</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="269" to="290" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multilayer feedforward networks with a nonpolynomial activation function can approximate any function</title>
		<author>
			<persName><forename type="first">M</forename><surname>Leshno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pinkus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schocken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="861" to="867" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fully complex extreme learning machine</title>
		<author>
			<persName><forename type="first">M.-B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Saratchandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sundararajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="306" to="314" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A fast and accurate on-line sequential learning algorithm for feedforward networks</title>
		<author>
			<persName><forename type="first">N.-Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Saratchandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sundararajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1411" to="1423" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Universal approximation using radial-basisfunction networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Sandberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="246" to="257" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Approximation of multivariate functions using ridge polynomial networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Neural Networks (IJCNN&apos;2002)</title>
		<meeting>the International Joint Conference on Neural Networks (IJCNN&apos;2002)<address><addrLine>Baltimore, MD, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-06">June 2002</date>
			<biblScope unit="page" from="380" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Consistent specification testing with nuisance parameters present only under the alternative</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Stinchcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econ. Theory</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="295" to="324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An additional hidden unit test for neglected nonlinearity in multilayer feedforward networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Neural Networks</title>
		<meeting>the International Conference on Neural Networks</meeting>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="451" to="455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>White</surname></persName>
		</author>
		<title level="m">Approximate nonlinear forecasting methods</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Elliott</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">W J</forename><surname>Granger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Timmermann</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="460" to="512" />
		</imprint>
	</monogr>
	<note>Handbook of Economics Forecasting</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
