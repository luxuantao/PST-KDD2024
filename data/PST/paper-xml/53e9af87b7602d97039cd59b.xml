<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Automatic Performance Analysis of Hybrid MPI/OpenMP Applications</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Felix</forename><surname>Wolf</surname></persName>
							<email>f.wolf@fz-juelich.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Forschungszentrum Jülich Zentralinstitut für Angewandte Mathematik</orgName>
								<address>
									<postCode>52425</postCode>
									<settlement>Jülich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bernd</forename><surname>Mohr</surname></persName>
							<email>b.mohr@fz-juelich.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Forschungszentrum Jülich Zentralinstitut für Angewandte Mathematik</orgName>
								<address>
									<postCode>52425</postCode>
									<settlement>Jülich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Automatic Performance Analysis of Hybrid MPI/OpenMP Applications</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0C70241A447CCA65FFA5B5FCC8EF99E0</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T05:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The EXPERT performance-analysis environment provides a complete tracing-based solution for automatic performance analysis of MPI, OpenMP, or hybrid applications running on parallel computers with SMP nodes. EXPERT describes performance problems using a high level of abstraction in terms of execution patterns that result from an inefficient use of the underlying programming model(s). The set of supported problems can be extended to meet application-specific needs. The analysis is carried out along three interconnected dimensions: class of performance behavior, call-tree position, and thread of execution. Each dimension is arranged in a hierarchy, so that the user can investigate the behavior on varying levels of detail. All three dimensions are interactively accessible using a single integrated view.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Coupling SMP systems combines the packaging efficiencies of shared-memory multiprocessors with the scaling advantages of distributed-memory architectures. The result is a computer architecture that can scale more cost-effectively in size. Unfortunately, these systems come at the price of a more complex programming environment to deal with the different modes of parallel execution: sharedmemory multithreading vs. distributed-memory message passing. As a consequence, performance optimization becomes more difficult and creates a need for advanced performance tools that are custom made for this class of computing environments.</p><p>While performance tools exist for sharedmemory systems and for distributed-memory systems, solving performance problems on parallel computers with SMP nodes is not as simple as combining two tools. When dealing with hybrid (MPI/OpenMP) parallel executions, performance problems arise where an integrated view is required. Current state-of-the-art tools such as VGV <ref type="bibr" target="#b9">[10]</ref> can capture and visualize these integrated views, but suffer from performance-information overload, unable to abstract performance problems from detailed performance data in an integrated hybrid framework.</p><p>The EXPERT performance-analysis environment <ref type="foot" target="#foot_0">1</ref> is able to automatically detect performance problems in event traces of MPI <ref type="bibr" target="#b14">[16]</ref>, OpenMP <ref type="bibr" target="#b17">[19]</ref>, or hybrid applications running on parallel computers with SMP nodes as well as on more traditional non-SMP or single SMP systems.</p><p>Performance problems are specified in terms of execution patterns that represent situations of inefficient behavior. These patterns are input for an automatic analysis process that recognizes and quantifies the inefficient behavior in event traces. Mechanisms that hide the complex relationships within compound-event specifications allow a simple description of complex inefficient behavior on a high level of abstraction.</p><p>The analysis process automatically transforms the event traces into a three-dimensional representation of performance behavior. The first dimension is the kind of behavior. The second dimension describes the behavior's source-code location and the execution phase during which it occurs.</p><p>Finally, the third dimension gives information on the distribution of performance losses across different processes or threads. The hierarchical organization of each dimension enables the investigation of performance behavior on varying levels of granularity. Each point of the representation is uniformly mapped onto the corresponding fraction of execution time, allowing the convenient correlation of different behavior using only a single view. In addition, the set of predefined performance problems can be extended to meet individual (e.g., application-specific) needs.</p><p>The remainder of this article is organized as follows: First, we describe the overall architecture of our analysis environment in the next section. In Section 3, we present the abstraction mechanisms used to simplify the specification of complex situations representing inefficient performance behavior. After that, we introduce the actual analysis component and how it can be extended to deal with application specific requirements in Section 4. Section 5 proves our concept by applying it to two realistic codes. Finally, we consider related work and conclude the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Overall Architecture</head><p>The EXPERT performance-analysis environment is depicted in Figure <ref type="figure" target="#fig_0">1</ref>. The different components are represented as boxes with rounded corners and their inputs and outputs are represented as paper sheets with the upper-right corner turned down. The arrows illustrate the whole performance-analysis process from instrumentation to result presentation.</p><p>The EXPERT analysis process is composed of two parts: a semi-automatic multi-level instrumentation of the user application followed by an automatic analysis of the generated performance data. The first subprocess is called semi-automatic because it requires the user to slightly modify the makefile. To begin the process, the user supplies the application's source code, written in either C, C++, or Fortran, to OPARI <ref type="bibr" target="#b16">[18]</ref>, which performs automatic instrumentation of OpenMP constructs and redirection of OpenMP-library calls to instrumented wrapper functions on the sourcecode level. Instrumentation of user functions is done either on the source-code level using TAU <ref type="bibr" target="#b19">[21]</ref> or using a compiler-supplied profiling interface. Instrumentation for MPI events is accomplished with the PMPI <ref type="bibr" target="#b13">[15]</ref> wrapper library, which generates MPI-specific events by intercepting calls to MPI functions. All MPI, OpenMP, and user-  function instrumentation call the EPILOG run-time library, which provides mechanisms for buffering and trace-file creation. At the end of the instrumentation process the user has a fully instrumented executable.</p><p>Running this executable generates a trace file in the EPILOG format. After program termination, the trace file is fed into the EXPERT analyzer. The analyzer uses EARL <ref type="bibr" target="#b21">[23]</ref> to provide a high-level view of the raw trace file. We call this view the enhanced event model, and it is where the actual analysis takes place. The analyzer generates an analysis report, which serves as input for the EXPERT presenter.</p><p>Currently, the software necessary to generate event traces has been successfully installed on two parallel computers with SMP nodes: the PC-based ZAMpano <ref type="bibr" target="#b11">[13]</ref> and the HITACHI SR8000-F1 <ref type="bibr" target="#b2">[3]</ref>. Instrumentation is done using the unpublished profiling interface of the PGI <ref type="bibr">[12]</ref> compiler or of the proprietary HITACHI compiler <ref type="bibr" target="#b8">[9]</ref>, respectively. The analysis components run on Linux.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Abstraction Mechanisms</head><p>EARL maps a raw EPILOG trace of "basic" events onto the enhanced event model. The enhanced event model provides abstractions that al-low compound events representing inefficient behavior to be easily described (see <ref type="bibr" target="#b23">[25]</ref> for details). The model considers an event trace as a chronologically sorted sequence of primitive events. Depending on the event type, each event is characterized by a set of attributes. The event types are organized in a hierarchy. There are programmingmodel-independent event types representing simple region enters and exits. Types indicating pointto-point and collective communication cover the MPI model. OpenMP event types comprise fork and join operations, lock synchronization operations, and -similar to MPI -an event type indicating the collective execution of parallel constructs.</p><p>Additionally, EARL provides two types of abstractions on top of the basic part of the model:</p><formula xml:id="formula_0">• State sequences • Pointer attributes</formula><p>State sequences map individual events onto a set of events that represent one aspect of the parallel system's execution state at the moment when the event happens. An example is the message queue containing all events of sending messages currently being transferred. Pointer attributes connect corresponding events, so that one can define compound events along a path of corresponding events. An example is is an attribute pointing from a messagereceipt event to the corresponding send event. An essential part of the enhanced model is the dynamic call tree, which is computed from all region-enter and region-exit events. As an additional pointer attribute, EARL provides a link from each enter event to the first enter event visiting the same node in the call tree. This provides a simple means to associate a performance-relevant compound event with the corresponding execution phase of the parallel program.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Analysis Component</head><p>The design of the analyzer is based on the specifications and terminology presented in <ref type="bibr" target="#b5">[6]</ref>. The analyzer attempts to prove performance properties for one execution of a parallel application and to quantify them according to their influence on the performance. A performance property characterizes a class of performance behavior and is specified in terms of a compound event, which the analyzer tries to detect in an event trace. A compound event is a set of events matching a specific execution pattern, whose constituents are connected by relationships and constraints. For each property, EXPERT calculates a severity measure indicating the fraction of execution time spent on that property and, thus, allows the correlation of different properties in a single view.</p><p>The run-time events of a parallel application occur on multiple time lines -one for each control flow (i.e, thread). EXPERT regards all control flows as being mapped to different CPUs at any time, that is, processes or threads running on the same SMP node do not share a CPU. EXPERT describes the severity of a particular performance property in terms of wall-clock interval sets that may be distributed across different time lines. All interval sets are subsets of the CPU-reservation time, which is the time from the first to the last event multiplied by the number of threads. The severity is returned in percentage of the CPU-reservation time.</p><p>The analyzer is implemented in Python using EARL for trace access. Its architecture is based on the idea of separating the analysis process from the specification of the performance properties; that is, the performance properties are not hard-coded and specified separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Specification of Performance Properties</head><p>The performance properties are specified in form of patterns. Patterns are Python classes, which are responsible for detecting compound events indicating inefficient behavior. They provide a common interface making them exchangeable from the perspective of the tool. The specifications use the abstractions provided by EARL and, for this reason, are very simple.</p><p>The analysis process follows an event driven approach. EXPERT walks sequentially through the event trace and invokes for each single event callback methods of the pattern instances and supplies the event as an argument. A pattern can provide a different call-back method for each event type. The call-back method itself then tries to locate a compound event representing an inefficiency, thereby following links (i.e., pointer attributes) emanating from the supplied event or investigating state sequences. This mechanism allows the simple specification of very complex performance-relevant situations and an explanation of inefficiency that is based on the terminology of the programming model.</p><p>The common interface also provides a method to launch a configuration dialog for the input of pattern-specific parameters before the analysis process as well as a method to launch a presentation dialog for the display of pattern-specific results afterward, which allows the treatment of patternspecific performance criteria.</p><p>EXPERT organizes the performance properties in a hierarchy. The upper levels of the hierarchy (i.e., those that are closer to the root) correspond to more general behavioral aspects such as time spent in MPI functions. The deeper levels correspond to more specific situations such as time lost due to blocking communication.</p><p>Figure <ref type="figure" target="#fig_1">2</ref> shows the complete hierarchy of performance properties being currently supported by EXPERT. We shall briefly discuss some of the most interesting ones in Section 4.1.1 and 4.1.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Examples of MPI Performance Properties</head><p>Late Sender This property refers to the time wasted, when a call to a blocking receive operation (e.g, MPI Recv or MPI Wait) is posted before the corresponding send operation is executed.</p><p>Late Receiver This property refers to the inverse case. A send operation blocks until the corresponding receive operation is called. This can happen for several reasons. Either the MPI implementation is working in synchronous mode by default or the size of the message to be sent exceeds the available MPI-internal buffer space and the operation blocks until the data is transferred to the receiver.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Messages in Wrong Order</head><p>This property, which has been motivated by <ref type="bibr" target="#b10">[11]</ref>, deals with the problem of passing messages out of order. For example, the sender may send messages in a certain order, but the receiver may expect their arrival in the reverse order. The implementation locates such situations by querying the message queue each time a message is received and by looking for older messages with the same target as the current message. This situation can be a specialization of either Late Sender or Late Receiver.</p><p>Wait at N x N Collective communication operations that send data from all processes to all processes exhibit an inherent synchronization, that is, no process can finish the operation until the last process has started. The time until all processes have entered the operation is measured and used to compute the severity. Note that this property requires to identify all parts of individual collective-operation instances in the event stream.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Examples of OpenMP Performance Properties</head><p>Wait at Barrier The time spent on waiting for the last participant in implicit (i.e., compilergenerated) or explicit (i.e., user-specified) OpenMP barrier synchronization. Note that this property requires to identify all parts of individual barrier instances in the event stream.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lock Synchronization</head><p>The time a thread waits for a lock that is owned by another thread. This can occur as a result of OpenMP-library calls or at the entry of critical sections.</p><p>Idle Threads Idle times on processors caused by sequential execution before or after an OpenMP parallel region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Representation of Performance Behavior</head><p>Each applied pattern instance computes a twodimensional severity matrix, which contains the severity as a function of the node in the dynamic call tree and the location (i.e., thread). Thus, the complete performance behavior is represented using a three-dimensional matrix, where each cell contains the severity for a specific performance property, call-tree node, and location.</p><p>The first dimension describes the kind of inefficient behavior. The second dimension describes both its source-code location and the execution phase during which it occurs. Finally, the third dimension gives information on the distribution of performance losses across different processes or threads, which allows to draw additional conclusions (e.g., load imbalance, see also <ref type="bibr" target="#b22">[24]</ref>).</p><p>In addition, each of the dimensions is arranged in a hierarchy: the performance properties in a hierarchy of general and more specific ones, the calltree nodes in their evident hierarchy, and the locations in a hierarchy consisting of the levels machine, node, process, and thread. Thus, it is possible to analyze the behavior on different levels of granularity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Presentation of Performance Behavior</head><p>The user can interactively access each of the hierarchies constituting a dimension of performance  behavior using a tree browser that labels each node with a weight. EXPERT uses as weight a percentage of the application's CPU-reservation time. The weight that is actually displayed depends on the state of the node, that is, whether it is expanded or collapsed. The weight of a collapsed node represents the whole subtree associated with that node, whereas the weight of an expanded node represents only the fraction that is not covered by its descendants because the weights of its descendants are now displayed separately. This allows the analysis of performance behavior on different levels of granularity.</p><p>For example, the call tree may have a node main with two children foo and bar (Figure <ref type="figure" target="#fig_2">3</ref>). In the collapsed state, this node is labeled with the weight representing the time spent in the whole program. In the expanded state it displays only the fraction that is spent neither in foo nor in bar.</p><p>The weight is displayed simultaneously using both a numerical value as well as a colored icon. The color is taken from a spectrum representing the whole range of possible weights (i.e., 0 -100 percent). To avoid distraction, insignificant val- The trees of the different analysis dimensions are interconnected, so that the user can display the call tree with respect to a particular performance property, and the distribution across the locations with respect to a particular node in call tree. In Figure <ref type="figure" target="#fig_3">4</ref>, the selections are indicated by framed node labels. Thus, the user can investigate the performance behavior in a scalable but still accurate way along all its interconnected dimensions using only a single integrated view.</p><p>In the default mode, the display represents the severity as a percentage of the total CPUreservation time. However, always referring to the total CPU-reservation time may limit scalability because values may become very small (e.g., in the case of many locations). For this reason, the presenter offers a relative view mode. In this relative view mode, a percentage shown in a tree always refers to the selection in the left neighbor tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Extension Mechanism</head><p>EXPERT provides a large set of built-in performance properties, which cover the most frequent inefficiency situations. But sometimes the user may wish to consider application-specific metrics such as iterations or updates per second. In this case, the user can simply write another pattern class that implements an own application-specific performance property according to the common interface of all pattern classes, and place it into a plug-in module.</p><p>At startup time, EXPERT dynamically queries the module's name space and looks for newly inserted patterns from which it is now able to build instances. The new patterns are integrated into the graphical user interface and can be used like the predefined ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Examples</head><p>We tested our environment for two realistic code examples, REMO and SWEEP3D, on ZAMpano <ref type="bibr" target="#b11">[13]</ref>. Both applications are hybrid MPI/OpenMP applications. CPU reservation was done such that there was one CPU per computational thread or single-threaded process. We consider one event trace per application. Table <ref type="table">5</ref> summarizes trace-file size and overhead. The first row contains the program name, the second row shows the number of CPUs used, the third row lists the trace-file size, and the fourth row gives the execution time. To estimate the runtime overhead introduced by the instrumentation, the minimum execution time of a series of ten uninstrumented runs was compared to the minimum execution time of a series of ten instrumented runs. The result is listed in the fifth row. Finally, the last row shows the duration of the analysis process carried out on the test platform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">REMO</head><p>REMO <ref type="bibr" target="#b3">[4]</ref> is a weather forecast application of the DKRZ (Deutsches Klima Rechenzentrum). It implements a hydrostatic limited area model, which is based on the Deutschland/Europa weather forecast model of the German Meteorological Services (Deutscher Wetterdienst (DWD)). We consider an early experimental MPI/OpenMP version of the production code. The application was executed on four nodes with one process per node and four threads per process (4 processes × 4 threads).</p><p>Figure <ref type="figure" target="#fig_3">4</ref> shows the result display of REMO in the default mode, that is, all values and colors represent percentages of the total CPU-reservation time. The property view indicates that one half (i.e., 51.8 percent) of the total CPU-reservation time is idle time (i.e., Idle Threads) resulting from OpenMP sequential execution outside of parallel regions. Although during this period the idle threads actually do not execute any code, the time is mapped onto the call paths that have been executed by the master thread during this time. That is to say, for analysis and presentation purposes EXPERT assumes that outside parallel regions the slave threads "execute" the same code as their master thread. This method of call-path mapping helps to identify parts of the call tree that might be optimized in order to reduce the amount of sequential execution.</p><p>In the case of REMO, the EXPERT display (Figure <ref type="figure" target="#fig_3">4</ref>, middle) allows the easy identification of two call paths as major sources of idle times. The location view (Figure <ref type="figure" target="#fig_3">4</ref>, right) shows the distribution of the idle time across the slave threads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">SWEEP3D</head><p>The benchmark code SWEEP3D <ref type="bibr" target="#b1">[2]</ref> represents the core of a real ASCI application. It solves a 1-group time-independent discrete ordinates (Sn) 3D Cartesian (XYZ) geometry neutron transport problem. We consider an early experimental MPI/OpenMP version of the original MPI version. While MPI is responsible for parallelism by domain decomposition, OpenMP is responsible for parallelism by multitasking. The application was executed on four nodes with one process per node and four threads per process (4 processes × 4 threads). The performance behavior of SWEEP3D exhibits a weak point of hybrid programming, that is, a performance problem resulting from the combination of MPI and OpenMP. MPI calls made outside a parallel region prolong sequential execution and prevent available CPUs from being used by multiple threads. The results are shown in Table <ref type="table">5</ref>.2. The call path (a) shown in the table is responsible for most of the losses occurring due to the property Idle Threads. However, at the same time this call path exhibits a significant loss due to the property Late Sender. Note that Late Sender counts the times of the master threads, whereas Idle Threads counts the times of the slave threads (3 slaves per master). Taking this into account, reducing Late Sender by one percent would speed up the application by four percent because speeding up the master also reduces idle times of the slaves. Obviously, one rea-son for the Late Sender problem at call path (a) is receiving messages in the reverse sending order (Messages in Wrong Order). Moreover, a significant amount of time is spent on the implicit (i.e., compiler-generated) OpenMP barrier at the end of call path (b). Expanding the node of the property Implicit Barrier (Figure <ref type="figure" target="#fig_4">5</ref>, left) reveals that most of that time is lost due to the property Wait at Barrier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Miller and associates <ref type="bibr" target="#b15">[17]</ref> developed automatic on-line performance analysis according to the W 3 Search Model in the well-known Paradyn project. In contrast to our approach, the W 3 model describes performance behavior along the dimensions performance problem, program resource, and time. Performance problems are expressed in terms of a threshold and one or more metrics such as CPU time, blocking time, message rates, I/O rates, or number of active processors. The main accomplishments of EXPERT in contrast to Paradyn is the description of performance problems in terms of complex event patterns that go beyond counter-based metrics. Also, the uniform mapping of arbitrary performance behavior onto the CPUreservation time allows the correlation of different behavior in a single view. Espinosa <ref type="bibr" target="#b4">[5]</ref> implemented an automatic trace analysis tool KAPPA-PI for evaluating the performance behavior of MPI and PVM message-passing programs. Here, behavior classification is carried out in two steps. At first, a list of idle times is generated from the raw trace file using a simple metrics. Then, based on this list, a recursive inference process continuously deduces new facts on an increasing level of abstraction. Finally, recommendations on possible sources of inefficiencies are built from the facts being proved on the one hand and from the results of source-code analysis on the other hand.</p><p>Vetter <ref type="bibr" target="#b20">[22]</ref> performs automatic performance analysis of MPI point-to-point communication based on machine learning techniques. He traces individual message-passing operations and then, classifies each individual communication event using a decision tree. The decision tree has been previously trained by microbenchmarks that demonstrate both efficient as well as inefficient performance behavior. As opposed to this approach, EXPERT draws conclusions from the temporal relationships of individual events in a platformindependent way, which does not require any training prior to analysis. JavaPSL <ref type="bibr" target="#b6">[7]</ref> has been designed by Fahringer and associates to specify performance properties based on the Java programming language. Whereas EX-PERT uses Python to provide a uniform interface to performance properties, JavaPSL exploits simi-lar mechanisms of the Java language, such as polymorphism, abstract classes, and reflection. In contrast to EXPERT, which concentrates on compoundevent analysis, JavaPSL puts emphasis on the definition of performance properties based on existing ones (e.g., by defining metaproperties).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>The EXPERT tool environment provides a complete but still extensible solution for automatic performance analysis of MPI, OpenMP, or hybrid applications running on parallel computers with SMP nodes. EXPERT represents performance properties on a very high level of abstraction that goes beyond simple metrics and provides the ability to explain performance problems in terms of the underlying programming model(s). The set of performance-property specifications is embedded in a flexible architecture and can be extended to meet application-specific needs.</p><p>The performance behavior is presented along three interconnected dimensions: class of performance behavior, position within the call tree and thread of execution. The last dimensions allows even the effects of different communication patterns among subdomains to be investigated. Each dimension is arranged in a hierarchy, so that the user can view the behavior on varying levels of detail. In particular, the hierarchical structure of hybrid applications and SMP-cluster hardware is reflected this way. Each point of the representation is uniformly mapped onto the corresponding fraction of CPU-reservation time, allowing the convenient correlation of different behavior in a single integrated view. The user can access all three dimensions interactively using a scalable but still accurate tree display. Colors make it easy to identify interesting nodes even in case of large trees.</p><p>EXPERT is well suited to analyze a single trace file. But the development process of parallel applications often demands for comparison of trace files representing different execution configurations or development versions. For the future, we intend to integrate mechanisms for comparative performance analysis. In addition, we plan to improve our result presentation by integrating it with an event-trace browser such as VAMPIR <ref type="bibr" target="#b0">[1]</ref> to visualize instances of performance problems using timeline diagrams and by adding source-code displays. Finally, we will work on further improving and completing our performance-property catalog.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. EXPERT overall architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Hierarchy of performance properties.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Node of the call tree in collapsed and expanded state.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Display of performance behavior in EXPERT for REMO.</figDesc><graphic coords="7,101.16,99.29,425.22,315.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Display of performance behavior in EXPERT for SWEEP3D in the relative view mode.</figDesc><graphic coords="9,101.16,99.24,425.21,215.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 . Trace-file size and overhead.</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell>REMO</cell><cell>SWEEP3D</cell></row><row><cell>CPUs</cell><cell>16</cell><cell>16</cell></row><row><cell>Size (MB)</cell><cell>170</cell><cell>72</cell></row><row><cell>Execution time (sec)</cell><cell>37.2</cell><cell>16.5</cell></row><row><cell>Overhead (%)</cell><cell>9.7</cell><cell>6.0</cell></row><row><cell>Analysis time (h:m)</cell><cell>9 : 48</cell><cell>3 : 22</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 . Performance problems found in SWEEP3D in percentage of the total CPU- reservation time.</head><label>2</label><figDesc>→ inner auto → inner → sweep → recv real → MPI Recv (b) driver → inner auto → inner/sweep → !$omp parallel → !$omp do → !$omp ibarrier</figDesc><table><row><cell>Call Paths</cell><cell></cell><cell></cell><cell></cell></row><row><cell>(a)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Performance Property</cell><cell>Whole Program</cell><cell>(a)</cell><cell>(b)</cell></row><row><cell>Idle Threads</cell><cell>37.5</cell><cell>17.5</cell><cell></cell></row><row><cell>Communication</cell><cell>6.5</cell><cell>5.8</cell><cell></cell></row><row><cell>Late Sender</cell><cell>3.2</cell><cell>3.2</cell><cell></cell></row><row><cell>Implicit Barrier (OpenMP)</cell><cell>4.3</cell><cell></cell><cell>3.3</cell></row><row><cell>Wait at Barrier (OpenMP, implicit)</cell><cell>2.8</cell><cell></cell><cell>2.6</cell></row></table><note><p>seep3d</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The work on EXPERT is carried out as a part of the KOJAK project<ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b12">14]</ref> and is embedded in the IST working group APART<ref type="bibr" target="#b18">[20]</ref>.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Acknowledgements</head><p>We would like to thank all our partners in the IST working group APART for their contributions to this topic. We also would like to thank Arpad Kiss for providing the basic tree browser implementation, DKRZ for giving us access to their application, and Reiner Vogelsang for helping us in conducting our experiments. Finally, we would like to thank Allen Malony and Craig Soules for their helpful comments and suggestions on the language.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Performance Optimization of Parallel Programs: Tracing, Zooming, Understanding</title>
		<author>
			<persName><forename type="first">A</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Detert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Nagel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Cray User Group Meeting</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Winget</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Winget</surname></persName>
		</editor>
		<meeting>of Cray User Group Meeting<address><addrLine>Denver, CO</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995-03">March 1995</date>
			<biblScope unit="page" from="252" to="258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<ptr target="http://www.llnl.gov/ascibenchmarks/" />
		<title level="m">The ASCI sweep3d Benchmark Code</title>
		<imprint>
			<publisher>ASCI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<ptr target="http://www.lrz-muenchen.de/" />
		<title level="m">Leibnitz Rechenzentrum der Bayerischen Akademie der Wissenschaften. HITACHI SR 8000-F1</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Performance of the Parallelized Regional Climate Model REMO</title>
		<author>
			<persName><forename type="first">T</forename><surname>Diehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gülzow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Eighth ECMWF Workshop on the Use of Parallel Processors in Meterology</title>
		<meeting>of the Eighth ECMWF Workshop on the Use of Parallel essors in Meterology<address><addrLine>Reading, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-11">November 1998</date>
			<biblScope unit="page" from="181" to="191" />
		</imprint>
	</monogr>
	<note>European Centre for Medium-Range Weather Forecasts</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Automatic Performance Analysis of Parallel Programs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Espinosa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000-09">September 2000</date>
		</imprint>
		<respStmt>
			<orgName>Universitat Autonoma de Barcelona</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Knowledge Specification for Automatic Performance Analysis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Fahringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gerndt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mohr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Träff</surname></persName>
		</author>
		<idno>FZJ-ZAM-IB-2001- 08</idno>
		<imprint>
			<date type="published" when="2001-08">August 2001</date>
		</imprint>
		<respStmt>
			<orgName>ESPRIT IV Working Group APART, Forschungszentrum Jülich</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note>Revised version</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Modelling and Detecting Performance Problems for Distributed and Parallel Programs with JavaPSL</title>
		<author>
			<persName><forename type="first">T</forename><surname>Fahringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Seragiotto</forename><surname>Junior</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Conference on Supercomputers</title>
		<meeting>of the Conference on Supercomputers<address><addrLine>SC; Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-11">2001. November 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Performance Analysis for CRAY T3E</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gerndt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mohr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 7th Euromicro Workshop on Parallel and Disributed Pocessing (PDP&apos;99)</title>
		<meeting>of the 7th Euromicro Workshop on Parallel and Disributed Pocessing (PDP&apos;99)</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="241" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">HITACHI SR 8000 Compiler</title>
		<author>
			<persName><surname>Hitachi</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note type="report_type">Technical Manual</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An Integrated Performance Visualizer for MPI/OpenMP Programs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hoeflinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Nagel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rajic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vetter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 3rd European Workshop on OpenMP</title>
		<meeting>of the 3rd European Workshop on OpenMP<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Grindstone: A Test Suite for Parallel Performance Tools</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Hollingsworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Steele</surname></persName>
		</author>
		<idno>CS-TR-3703</idno>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
		<respStmt>
			<orgName>University of Maryland</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Computer Science Technical Report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Forschungszentrum</forename><surname>Jülich</surname></persName>
		</author>
		<author>
			<persName><surname>Zampano</surname></persName>
		</author>
		<ptr target="http://zampano.zam.kfa-juelich.de/" />
		<title level="m">ZAM Parallel Nodes)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">KOJAK (Kit for Objective Judgement and Knowledgebased Detection of Performance Bottlenecks)</title>
		<ptr target="http://www.fz-juelich.de/zam/kojak/" />
		<imprint/>
		<respStmt>
			<orgName>Research Centre Jülich</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<ptr target="http://www.mpi-forum.org" />
		<title level="m">Message Passing Interface Forum. MPI: A Message Passing Interface Standard</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">MPI-2: Extensions to the Message-Passing Interface</title>
		<ptr target="http://www.mpi-forum.org" />
	</analytic>
	<monogr>
		<title level="m">Message Passing Interface Forum</title>
		<imprint>
			<date type="published" when="1997-07">Juli 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The Paradyn Parallel Performance Measurement Tool</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Callaghan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Cargille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Hollingsworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Irvine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Karavanic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kunchithapadam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Newhall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="37" to="46" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Design and Prototype of a Performance Tool Interface for OpenMP</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mohr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Malony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Supercomputing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="105" to="128" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">OpenMP Architecture Review Board. OpenMP Fortran Application Program Interface -Version 2.0</title>
		<ptr target="http://www.openmp.org" />
		<imprint>
			<date type="published" when="2000-11">November 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<ptr target="http://www.fz-juelich.de/apart/" />
		<title level="m">IST Working Group APART (Automatic Performance Analysis: Real and Tools). Homepage</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Portable Profiling and Tracing for Parallel Scientific Applications using C++</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Malony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cuny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lindlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Beckman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karmesin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the SIG-METRICS Symposium on Parallel and Distributed Tools</title>
		<meeting>of the SIG-METRICS Symposium on Parallel and Distributed Tools</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998-08">August 1998</date>
			<biblScope unit="page" from="134" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Performance Analysis of Distributed Applications using Automatic Classification of Communication Inefficencies</title>
		<author>
			<persName><forename type="first">J</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 14th International Conference on Supercomputing</title>
		<meeting>of the 14th International Conference on Supercomputing<address><addrLine>Santa Fe, New Mexico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-05">May 2000</date>
			<biblScope unit="page" from="245" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">EARL -Eine programmierbare Umgebung zur Bewertung paralleler Prozesse auf Message-Passing-Systemen</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Jül-Bericht</title>
		<imprint>
			<biblScope unit="volume">3551</biblScope>
			<date type="published" when="1998-06">June 1998</date>
		</imprint>
		<respStmt>
			<orgName>RWTH Aachen, Forschungszentrum Jülich</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Automatic Performance Analysis of MPI Applications Based on Event Traces</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mohr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Conference on Parallel Computing (Euro-Par)</title>
		<meeting>of the European Conference on Parallel Computing (Euro-Par)<address><addrLine>Munich (Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-08">August 2000</date>
			<biblScope unit="page" from="123" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Specifying Performance Properties of Parallel Applications Using Compund Events. Parallel and Distributed Computing Practices (Special Issue on Monitoring Systems and Tool Interoperability)</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mohr</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In press</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
