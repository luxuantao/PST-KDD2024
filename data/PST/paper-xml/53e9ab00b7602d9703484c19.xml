<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An unsupervised approach to activity recognition and segmentation based on object-use fingerprints</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2010-02-01">1 February 2010</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Tao</forename><surname>Gu</surname></persName>
							<email>gu@imada.sdu.dk</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern Denmark</orgName>
								<address>
									<addrLine>Campusvej 55</addrLine>
									<postCode>5230</postCode>
									<settlement>Odense M</settlement>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shaxun</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">UC Davis</orgName>
								<address>
									<addrLine>One Shields Avenue</addrLine>
									<postCode>95616</postCode>
									<settlement>Davis</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xianping</forename><surname>Tao</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<addrLine>22 Hankou Road MMW Building</addrLine>
									<settlement>Nanjing, Jiangsu Province</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jian</forename><surname>Lu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<addrLine>22 Hankou Road MMW Building</addrLine>
									<settlement>Nanjing, Jiangsu Province</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An unsupervised approach to activity recognition and segmentation based on object-use fingerprints</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2010-02-01">1 February 2010</date>
						</imprint>
					</monogr>
					<idno type="MD5">247160FD570B5698DDB3BE4A78F520E8</idno>
					<idno type="DOI">10.1016/j.datak.2010.01.004</idno>
					<note type="submission">Received 7 October 2008 Received in revised form 21 January 2010 Accepted 21 January 2010</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T04:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Human activity recognition Activity trace segmentation Contrast patterns Emerging patterns Fingerprint Object-use Web mining RFID</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Human activity recognition is an important task which has many potential applications. In recent years, researchers from pervasive computing are interested in deploying on-body sensors to collect observations and applying machine learning techniques to model and recognize activities. Supervised machine learning techniques typically require an appropriate training process in which training data need to be labeled manually. In this paper, we propose an unsupervised approach based on object-use fingerprints to recognize activities without human labeling. We show how to build our activity models based on object-use fingerprints, which are sets of contrast patterns describing significant differences of object use between any two activity classes. We then propose a fingerprint-based algorithm to recognize activities. We also propose two heuristic algorithms based on object relevance to segment a trace and detect the boundary of any two adjacent activities. We develop a wearable RFID system and conduct a real-world trace collection done by seven volunteers in a smart home over a period of 2 weeks. We conduct comprehensive experimental evaluations and comparison study. The results show that our recognition algorithm achieves a precision of 91.4% and a recall 92.8%, and the segmentation algorithm achieves an accuracy of 93.1% on the dataset we collected.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years, human activity recognition, which aims to recognize the actions and goals of one or more agents from a series of observations, has become an important research direction in pervasive computing. In this paradigm, various sensors are typically deployed to collect a sequence of observations (i.e., time-series data), and these observations are used to train an appropriate activity model. The trained model can then be used to assign new observations with activity labels. There are many useful healthcare and context-aware applications. A typical healthcare application is monitoring activities of daily living (ADLs) <ref type="bibr" target="#b0">[1]</ref> for the elderly and cognitively impaired people and providing them with proactive assistance. Other application can be activity-based adaptation such as lowering TV volume when a user answers a phone call or providing instructions when a user operates unfamiliar appliances.</p><p>Recognizing activities based on sensor readings is challenging because sensor data are inherently noisy and human activities are usually performed in a non-deterministic fashion. In addition, we argue that we need to address more challenges before deploying any activity recognition system for real-life use. These challenges can be, for example, scalability -scale to a large number of activities, applicability -work efficiently in real-life situations, andadaptability -adapt to different users 0169-023X/$ -see front matter Ã“ 2010 Elsevier B.V. All rights reserved. doi:10.1016/j.datak.2010.01.004 since different individuals may perform the same activity in different ways. A key issue is to develop appropriate activity models that map low-level sensor features to high-level concepts. To obtain activity models, a straightforward method is to learn from a set of training data. Most existing approaches <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref> leverage on supervised learning techniques to construct their activity models; however, learning from training data typically requires human labeling. Application developers are required to label both the underlying sensor system and the activities associated with a set of training data. Considering a large number of activities to be recognized in our daily lives, manual labeling of training data may place a significant burden to any individual involved in data collection. Hence, supervised learning approaches may have limitations in real-life deployment where scalability, applicability and adaptability are highly concerned.</p><p>In this paper, we aim to propose a simple, unsupervised approach based on RFID-tagged objects to recognize human activities without manual labeling, making our effort towards scalability, applicability and adaptability for real-life deployment. We build our activity model based on fingerprints, which are sets of object-use based contrast patterns describing significant differences of object-use between any two activity classes. This is done by first mining a set of object terms for each activity class from the web, and then mining contrast patterns among object terms based on emerging patterns <ref type="bibr" target="#b34">[35]</ref> to contrast any two activity classes. We then propose a fingerprint-based algorithm to recognize activities. We also propose two heuristic algorithms to segment an activity trace and detect the boundary of any two adjacent activities. We evaluate the effectiveness of our proposed techniques using a real-world dataset collected by seven subjects performing 17 activities involving 132 objects in a smart home environment. The experimental and comparison results show both efficiency and robustness of our proposed system.</p><p>In summary, this paper makes the following contributions:</p><p>We propose an activity model based on object-use fingerprints which are obtained by combining mining object-use from the web and mining contrast patterns, and a fingerprint-based algorithm to recognize activities. We dissociate the recognition process from the trace segmentation process, and propose two algorithms based on objectuse relevance to segment an activity trace. We validate our algorithms using a real-world dataset collected in a smart home environment, and analyze their effectiveness and limitations through comprehensive experimental and comparison studies.</p><p>The rest of this paper is organized as follows. We discuss related work in Section 2. We describe our activity model in Section 3. We then propose our activity recognition and segmentation algorithms in Section 4, and evaluate them in Section 5. Finally, we conclude the work in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Much early work in human activity recognition <ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref> has been done in the computer vision community. They leverage on video cameras, and explore various tracking methods and spatio-temporal analysis to track moving objects and recognize people's actions from video sequences.</p><p>Researchers in pervasive computing are interested in recognizing activities using on-body sensors that directly measure human, the environment and human-object interaction. Most existing work focuses on applying supervised machine learning techniques to this task where manual labeling of training data is typically required. There are two major models for recognizing human activities from artificial intelligence: logic-based approach and probabilistic approach. Early approaches such as <ref type="bibr" target="#b35">[36]</ref> were based on logic. In this model, activities are described as a logical inference process of circumscription, and represented by a set of first-order statements called event hierarchy. However, logic-based approaches have limitations in distinguishing among consistent plans and have problems to handle uncertainty and noise in sensor data. Probabilistic models gain more popularity as sensor readings are usually noisy and human activities are typically performed in a non-deterministic fashion. Probabilistic models can be generally categorized into static classification or temporal classification. In static classification, a variety of features is first extracted from sensor readings, and then a static classifier is applied to classify different activities. Typical static classifiers include naÃ¯ve Bayes used in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b10">11]</ref>, decision tree used in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11]</ref>, k-nearest neighbor (k-NN) used in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11]</ref>, and support vector machine (SVM) used in <ref type="bibr" target="#b7">[8]</ref>. In temporal classification, state-space models are typically used to enable the inference of hidden states (e.g., activity labels) given the observations (i.e., sensor readings). We name a few examples here: hidden Markov models (HMMs) used in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref>, dynamic Bayesian networks (DBNs) used in <ref type="bibr" target="#b3">[4]</ref> and conditional random fields (CRFs) used in <ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref>. Specifically, Bao et al. <ref type="bibr" target="#b1">[2]</ref> proposed to use multiple accelerometers placed in multiple locations of the human body to detect activities. The authors applied pre-trained classifiers such as decision tree and NaÃ¯ve Bayes to recognize activities. Ward et al. <ref type="bibr" target="#b9">[10]</ref> proposed to use microphones and 3-axis accelerometers to recognize continuous activities to assemble tasks in a ''wood workshop". They applied linear discriminant analysis (LDA) on the sound segments and HMM on the acceleration data. Tapia et al. <ref type="bibr" target="#b2">[3]</ref> proposed an activity recognition system based on a set of small and simple sensors. The NaÃ¯ve Bayesian classifier was chosen to predict the activity labels. Philipose et al. <ref type="bibr" target="#b3">[4]</ref> proposed to use RFID tags attached on objects of interest and represented activities as a probabilistic object sequence. These activity models were then converted into DBNs to compute the probabilities of the activities. Lester et al. <ref type="bibr" target="#b28">[29]</ref> used the multi-modal sensor board and applied both static classifier and HMM classifier to recognize activities. A variant of CRF (i.e., skip chain CRF <ref type="bibr" target="#b32">[33]</ref>) and a variant of HMM (i.e., interleaved mixture of HMMs <ref type="bibr" target="#b36">[37]</ref>) can be used to recognize interleaving activities, and another variant of CRF (i.e., factorial CRF <ref type="bibr" target="#b31">[32]</ref>) can be used to recognize concurrent activities. However, supervised learning models typically require labeled training data for an appropriate training process.</p><p>Recent work shows an interesting direction towards an unsupervised approach to activity recognition. Perkowitz et al. <ref type="bibr" target="#b11">[12]</ref> formulated activity models by translating labeled activities into probabilistic collections of terms, and mined the definitions of activities from the web. They represented the activities as HMM-based on the object sequence used with probabilistic distributions. However, activity traces were manually segmented and input to their HMM models, and hand segmentation still requires involvement of human effort. In addition, it may cause the data belonging to the same activity being segmented into different slices or the data two adjacent activities falls in the same slice. Wyatt et al. <ref type="bibr" target="#b12">[13]</ref> built their activity models by mining the web in a wider scope. They presented a bootstrap method that can produce labeled segmentations automatically. Their model abstraction uses a large number of labeled web pages as the training set in which human efforts are involved. The segmentation in this work was based on the duration of an activity that may vary greatly from one user to another. Therefore, the accuracy may drop obviously when their activity models were applied to real-world scenarios. Furthermore, although they used the generic mined models to segment the trace into labeled instances of activities, their segmentation process is sequential in nature such that any error in one segment may affect the segmentations of the subsequent traces. Pentney et al. <ref type="bibr" target="#b13">[14]</ref> demonstrated the use of a large number of hand-entered common sense database to interpret activity traces. Wang et al. <ref type="bibr" target="#b29">[30]</ref> combined a generative common sense model of activity with a discriminative model of actions to automate feature selection. Pentney et al. <ref type="bibr" target="#b20">[21]</ref> proposed chain graphs to represent objects used and activities performed. The approach is based on combining relational databases of large common sense created by the user with techniques for information retrieval on web. Hamid et al. modeled activity as a sequence of discrete events <ref type="bibr" target="#b33">[34]</ref>, recognition is done by discovering and matching the Motif which is defined as the subsequences with similar behavior appeared frequently in timeseries data. They also proposed to represent activities as bags of n-grams to extract global structure information of activities and presented a computational framework for unsupervised activity discovery and classification <ref type="bibr" target="#b23">[24]</ref>.</p><p>As we discussed, existing unsupervised approaches <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> mine both object and object sequence using web mining to obtain activity models. However, different users usually perform their activities in their own ways, and even the same user may perform the same activity in several different ways. Under a real-world situation, it is difficult to obtain complete HMMbased models. Most likely, this has to be complemented by hand specification. Our work in this paper is motivated by the recent advancement of unsupervised approaches. We build a simply activity model based on object-use fingerprints. Compared to <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>, we only mine object terms from the web. Mining sequential data has been well studied in the data mining literature <ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref>. In this paper, we apply contrast pattern mining to sensor-based human activity recognition and mine contrast patterns (i.e., fingerprints) from object terms for each activity to maximize the discriminating power of fingerprints. Furthermore, our trace segmentation algorithms operate independently such that the segmentation process for one segment does not affect that for subsequent segments. This approach is in contrast to sequential segmentation commonly employed in previous research where an error in a segmentation process affects the subsequent ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Mining object-use contrast patterns for fingerprints</head><p>This section describes our fingerprint-based activity models. The problem of activity recognition based on object-use can be formulated as follows. Given an activity trace consisting of a sequence of observations (i.e., object-use) and assume there are p activities, our objective is to build a model that can assign each observation with the correct activity label and segment the trace. To obtain a set of fingerprints for each activity class, we first mine a set of object terms for each activity class from the web. We then mine a set of contrast patterns for each class which will be used as fingerprints for the subsequent recognition process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Mining object terms and relevance weights from the web</head><p>In the first step, given a set of activities A Â¼ fa 1 ; a 2 ; . . . ; a m g, we mine a set of object terms T Â¼ ft 1 ; t 2 ; . . . ; t p g involved in each activity a 2 A, together with their associated weights W from the web. First, we obtain a set of object terms T Â¼ ft 1 ; t 2 ; . . . ; t p g used for defining each activity from a set of instructions on the web. Similar to <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>, we extract relevant instructions for an activity on two websites: www.wikihow.com and www.ehow.com. Both websites provide comprehensive instructions for many day-to-day activities such as make tea, brush teeth and take pills.</p><p>Fig. <ref type="figure" target="#fig_0">1</ref> shows a brief description of the processes involved in extracting object terms for an activity. Firstly, a collection of html documents is transformed into plain text and stemmed using Porter's stemming algorithm <ref type="bibr" target="#b14">[15]</ref>. In stemming, morphological variants of terms (e.g., singular vs. plural) which have similar semantic interpretations are considered equivalent and reduced to their stemmed or root forms. Secondly, the number of relevant object terms is further reduced by removing object terms appearing in the stoplist <ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref> (e.g., verbs, adjectives, pronouns, adverbs, false nouns, etc.). Finally, only object terms with a mapping in the physical space are retained. The database of these objects can be easily built by extracting information from a server that stored the entire object IDs.</p><p>Next, we identify the relevance weight for each object term. Apparently, there are many object terms appearing in an activity. By observing that the occurrence frequency of an object in a particular instruction closely parallels the weight of the object in real usage, we determine the weight W of each object term t 2 a by computing its term frequency -inverse document frequency (tf-idf) <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>, shown as follows:</p><formula xml:id="formula_0">tf-idf a i :Â¼ jt a i j Ã‚ log jD a j jd a : t a i 2 d a j<label>Ã°1Ãž</label></formula><p>where tf -idf a i is the tf-idf of ith object term in a; jt a i j is the term frequency of the ith object term in a; jD a j is the total number of documents in a; jd a : t a i 2 d a j is the number of documents where the ith object term appears Ã°jt a i j -0Ãž. Eq. ( <ref type="formula" target="#formula_0">1</ref>) implies that if an object term is too common, it occurs in almost all documents and will have a very low tf-idf score. The simple factor, log jDj jd:t i 2dj , denotes words if they occur in almost all documents (too general terms) and promotes words that occur in a limited number of documents (specific terms).</p><p>Since weight computation varies significantly from one activity to another because of heterogeneous sources, to establish a common basis of comparison among relative object term weights in different activities, we define our normalization technique as follows:</p><formula xml:id="formula_1">W a i :Â¼ logÃ°tf Ã€ idf a i Ãž max n a jÂ¼1 log tf Ã€ idf a j n o<label>Ã°2Ãž</label></formula><p>where W a i is the ith object term's normalized weight in a; tf -idf a i is the tf-idf of ith object term in a; n a is the total number of object terms in a.</p><p>This normalization ensures that the topmost object term has 1.0 weight and the relative distances of succeeding object terms based on their weights do not have high variability due to the smoothing effect of the log transformation. Moreover, this transformation lessens the strong bias in weights of the topmost object terms.</p><p>We have mined 45 activity models, and Table <ref type="table" target="#tab_0">1</ref> lists a partial set of object terms and relevance weights for four activities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Mining contrast patterns from object terms</head><p>Based on object terms and their relevance weights we mined, we can build a simple model which leverages a set of top n object terms as fingerprints for each activity. However, in real-life, activities may share common objects. To maximize the discriminative power of fingerprints for each activity class, we mine a set of contrast patterns from object terms and their  relevance weights for each class, which describe the significant differences of object terms between any two activity classes.</p><p>The mining results will be used as fingerprints for activity recognition. The definition of our contrast pattern is motivated by the concept of emerging pattern which was first introduced in <ref type="bibr" target="#b34">[35</ref>]. An emerging pattern is a set of items whose frequency changes significantly from one dataset to another; it describes significant changes (differences or trends) between two classes of data. Unlike an emerging pattern where each item has the same weight, in our context, items have different weights and each item is associated with a weight value (i.e., a probability). While other forms of contrast pattern exist, e.g., minimal distinguishing subsequence patterns <ref type="bibr" target="#b41">[42]</ref>, we leverage the basis form. We define our contrast pattern as follows. Suppose that a dataset D associated with a class C. D consists of many instances, and an instance contains a set of items (i.e., an itemset X), where an item is an attribute-value pair and each item x is associated with a weight W C</p><p>x . We denote the set of all items in D as T. Definition 1. The support of an itemset X, where X Â¼ fx 1 ; x 2 ; . . . ; x m g and X # T, is defined as</p><formula xml:id="formula_2">supp D Ã°XÃž :Â¼ count D Ã°XÃž jDj Ã‚ W C X<label>Ã°3Ãž</label></formula><p>where count D Ã°XÃž is the number of instances in D containing X, and W C X is the aggregated weight of X in C, which is defined as</p><formula xml:id="formula_3">Q m xÂ¼1 W C x .</formula><p>Definition 2. Given two different classes C 1 and C 2 , the growth rate of an itemset X from C 1 to C 2 is defined as GrowthRate(X) :Â¼ 0 i fsupp 1 Ã°XÃž Â¼ 0 and supp 2 Ã°XÃž Â¼ 0 Definition 3. Given a growth rate threshold q &gt; 1, an itemset X is said to be a contrast pattern from a background class C 1 to a target class C 2 if GrowthRateÃ°XÃž P q.</p><formula xml:id="formula_4">1 if</formula><p>Definitions 2 and 3 are similar to the definitions in emerging patterns. Contrast patterns are those itemsets with large growth rates from C 1 to C 2 . A contrast pattern with high support in its target class and low support in the contrasting class can be seen as a strong signal indicating the class of a test instance containing it.</p><p>We then mine contrast patterns for each of our activity classes. For each activity, denoted as a i , we mine a set of contrast patterns to contrast its instances,D a i , against all other activity instances D 0 a i , where D 0 a i Â¼ D Ã€ D a i and D is the entire dataset which is the entire object terms mined from the web in our previous step. We refer CP a i as the contrast patterns of a i: .</p><p>We develop an algorithm to discover contrast patterns based on an emerging patterns mining algorithm described in <ref type="bibr" target="#b19">[20]</ref>, which mines closed patterns and generators simultaneously under one depth-first search scheme. After computation, we get m sets of CPs, one set per activity class, named fingerprints for this class. Table <ref type="table" target="#tab_2">2</ref> presents an example of the CPs of the make tea activity. Column 1 shows the CPs. For example, the CP {tea, teapot} has a support of 95% and a growth rate of 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Activity recognition and segmentation algorithms</head><p>We can now apply fingerprints we obtained to recognize activities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Recognizing activities using fingerprints</head><p>To recognize activities using fingerprints, given a sequence of observations (i.e., S 0$T Ãž, we first segment its sequence using a sliding window (i.e., L a i Ãž to obtain a test instance (i.e., S t$tÃ¾La i Ãž, and then test this instance against a score function for each possible activity. The score function basically combines the strength of each CP subset. In such combination, CP subsets are aggregated in a single function to maximize their discriminative power to achieve good performance. The sliding window L a i is the average length of all the instances of activity a i . The activity yielding the higher score wins out and its class label will be assigned to the test instance. The above recognition process will be performed recursively. Since each of these sequence segments corresponds to an activity label, for each pair of consecutive sequence segments, we design an algorithm to detect and adjust the boundary. This algorithm serves as a feedback loop in the recognition process aiming to label sequence segments accurately and overcome the drawback of a sliding window based our segmentation method. The score function is defined as follows. </p><formula xml:id="formula_5">f Ã°a i ; S t$tÃ¾La i Ãž :Â¼ X X # S t$tÃ¾La i ;X2CPa i growth rateÃ°XÃž growth rateÃ°XÃž Ã¾ 1 Ã‚ supp a i Ã°XÃž Ã°<label>4Ãž</label></formula><p>where supp a i Ã°XÃž is the support of X in a i , and growth rateÃ°XÃž is supp a i Ã°XÃž divided by the X's support in non-a i classes.</p><p>Using contrast patterns as fingerprints, we are able to discriminate activities since contrast patterns represent the differences between activity classes. In real-life, some activities may share similar objects, however, the weight of each object appeared in each activity will unlikely be the same. Hence, by mining contrast patterns, we are able to obtain different CP subsets with different supports and growth rates for each of these activities, and aggregate each CP subsets for classification. Algorithm 1 describes the steps involved in the activity recognition algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1. Fingerprint-based activity recognition algorithm</head><p>Input: A sequence of objects from t Â¼ 0 $ T : O Â¼ fo return a i with the highest f;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Activity trace segmentation</head><p>In Algorithm 1, we use a sliding window to get a test instance for each possible activity. Since the sliding window length of each activity is an approximation of the actual length, the segmentation may not be accurate. Moreover, any error in one segment may affect the recognition of the subsequent trace. This error may accumulate and affect the recognition accuracy seriously. This problem is referred to as the activity boundary detection problem or trace segmentation problem.</p><p>To address this problem, we propose two heuristic algorithms leveraging on object relevance weight. We have two observations. First, an object will have a higher weight value in an activity if it is important to this activity, and the same object will have a lower weight value in an activity if it has less or no relevance to this activity. Second, the weights of adjacent objects in the same activity do not vary significantly compared to the weights of two adjacent objects belonging to two different activities.</p><p>We first introduce the MaxGap algorithm. To detect the boundary between two adjacent activities, a x and a y , the MaxGap algorithm computes the difference between the weight of each object in activity a x and its weight in activity a y (RW: Relative Weight). If the object is more relevant to a x than a y , then RW will be positive while the reverse will be negative. We then compute the difference of each consecutive RW pairs (gap), and the maximumgap is the boundary for these two activities. Algorithm 2 outlines the MaxGap algorithm. The input of the algorithm is a sequence of objects and two predicted activities. The output of the algorithm is the timestamp of an object where we should segment the two activities. The complexity of MaxGap is linear.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2. The MaxGap algorithm</head><p>Input: A sequence of objects from t Â¼ 0 $ T : O Â¼ fo 1 ; o 2 ; o 3 ; . . . ; o T g; Activities: A Â¼ fa 1 ; a 2 ; a 3 ; . . . ; a m g; Predicted activities a x and a y . Output: the boundary between a x and a y . 1:</p><p>foreach Ã°x; yÃž 2 a x ; a y do 2:</p><p>for ctr = x to y do 3:</p><formula xml:id="formula_6">RW ctr Â¼ W x Ã°o ctr Ãž Ã€ W y Ã°o ctr Ãž; 4:</formula><p>for ctr = x to y-1 do 5:</p><p>GAP ctr Â¼ RW ctr Ã€ RW ctrÃ¾1 ; 6: endfor 7:</p><p>return the boundary GAP ctr such that is maximum;</p><p>In cases where the two adjacent activities share common objects, boundary detection will be complicated if the common objects are located nearby the boundary. This is because their RWs will be close to zero. The MaxGap algorithm may fail to work in this case. To address this issue, we propose the MaxGain algorithm outlined in Algorithm 3. The input is the same as that of the MaxGap algorithm. foreach Ã°x; yÃž 2 a x ; a y do 2:</p><p>for ctr = x to y do 3:</p><formula xml:id="formula_7">RW ctr Â¼ W x Ã°o ctr Ãž Ã€ W y Ã°o ctr Ãž; 4:</formula><p>for ctr = x to y do 5: upperSum = 0; lowerSum = 0; 6:</p><p>for upper = x to ctr do 7:</p><p>upperSum Ã¾ Â¼ RW upper ; 8:</p><p>for lower = ctr + 1 to y do 9: lowerSum Ã¾ Â¼ RW lower ; 10:</p><p>GAIN ctr Â¼ upperSum Ã€ lowerSum; 11: endfor 12:</p><p>return the boundary such that GAIN ctr is maximum;</p><p>We walk-through the MaxGain algorithm using an example shown in Fig. <ref type="figure" target="#fig_2">2</ref>. The first column is a segment of objects extracted from an activity trace. For each possible object x i (a candidate boundary), we compute the upperSum and lowerSum as shown in column 4 and 5, respectively. The upperSum is the sum of all RWs from coffee to x i , while the lowerSum is the sum of all RWs from x iÃ¾1 to tea. We then compute the Gain for each candidate boundary, where Gain is defined as: Gain i Â¼ upperSum i Ã€ lowerSum i for i 2 fcoffee; salt; . . . ; teag</p><p>The result is shown in the last column. The timestamp of the object with the maximum Gain value is the boundary. In the example shown in Table <ref type="table" target="#tab_4">3</ref>, the creamer object yields the maximum Gain of 5.28 which also indicates the location of the boundary in the ground truth.</p><p>Compared to MaxGap, MaxGain considers the interplay of the group of objects between two adjacent activities, while the former algorithm only makes use of the relationship between two adjacent objects. Intuitively, MaxGain tends to be more accurate and noise-tolerant, since it is ''globally optimized". When calculating the upperSum (or lowerSum) of each candidate boundary x i , we can simply add (or subtract) the RW of x iÃ€1 . Therefore, we conclude the complexity of upperSum (or lower-Sum) is O(n).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental studies</head><p>We now move to evaluate our proposed algorithms. We develop a wearable RFID system as shown in Fig. <ref type="figure" target="#fig_1">3</ref>. The RFID wristband reader (Fig. <ref type="figure" target="#fig_3">3b</ref>) incorporates a SkyeTek M1-mini RFID reader, a Crossbow Mica2Dot wireless sensor module, and a rechargeable battery. It is able to detect the presence of a tag within the range of about 7 cm. The design of our RFID wristband reader is similar to Intel's iBracelet <ref type="bibr" target="#b3">[4]</ref>. RFID tags are attached to day-to-day objects such as cup, teaspoon and book in a smart home. We have three types of tags (Fig. <ref type="figure" target="#fig_3">3a</ref>) and all operate on 13.56 MHz. In the case of metal objects, e.g., kettle, we tagged on its plastic handle. In the case of liquid objects, e.g., water, we tagged on the faucet with a special plastic handle to sense the use. Fig. <ref type="figure" target="#fig_3">3c</ref> shows a screen shot of tagged objects in the kitchen. When the user handles a tagged object, the RFID reader scans the tag ID and sends it wirelessly to a server (Fig. <ref type="figure" target="#fig_3">3d</ref>) that maps the ID to an object name based on a pre-defined table. The server runs on a Linux-based laptop PC with a programming interface board and a Mica2Dot wireless module connected through its serial port. The sever samples tag IDs at a frequency of 1 Hz and records them in a text file.</p><p>We then conduct a real-world trace collect over a period of two weeks in a smart home environment. We had seven volunteers (one female and six males) and each volunteer wore an RFID wristband reader on each of her/his hands. We tagged 132 day-to-day objects. We selected a total number of 17 activities as summarized in Table <ref type="table" target="#tab_5">4</ref>. These activities are commonly used in other work <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref>. Each day, each of them performed these activities in any order they like. Each subject was requested to perform their activities consecutively in their own ways based on their daily practices. A trace (i.e., a sequence of tag IDs) was logged each day in a server. There was only one subject performing activities at any given time to reduce annotation efforts. One of the volunteers annotated the traces to establish the ground truth. All the traces collected were annotated by hand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Comparison results</head><p>To evaluate the performance of our activity recognition algorithm, we use the following metrics   <ref type="table" target="#tab_4">3</ref>).</p><p>precision</p><formula xml:id="formula_8">Â¼ TP TP Ã¾ FP and recall Â¼ TP TP Ã¾ FN</formula><p>where TP (true positive) refers to the number of instances where an activity recognition algorithm correctly identifies an activity; FP is (false positive) refers to the number of instances where the algorithm spuriously detects an activity that does not actually occur; and FN is (false negative) denotes the number of times that an activity that actually occurred is not detected.</p><p>We compare our fingerprint-based recognition algorithm with a supervised learning model -HMM, which is commonly used in activity recognition. HMM is a generative probabilistic model consisting of a hidden variable and an observable variable at each time step. To test the HMM model, we use leave-one-out cross validation. We build a HMM model where activity labels are represented as hidden states and objects are represented as observations. The model will be trained first, and then we use the Viberbi algorithm to recover the hidden state sequence for testing.</p><p>The overall comparison result of the two models is shown in Table <ref type="table" target="#tab_6">5</ref> and the detailed result is given in Fig. <ref type="figure" target="#fig_1">3</ref>. While the learned HMM model gets the highest precision, our algorithm achieves a comparable result in terms of precision and recall. After analysis, two limitations are worth noting. First, when RFID readers failed to report tag IDs or received corrupted tag IDs, missed detection occurred. This kind of errors is a common problem in sensor-based activity recognition. Efforts to improve the reliability of sensor deployment and acquisition process require further investigation. Second, some cases of false recognition occurred when the subject touched another objects unintentionally while an activity is being performed or because of the close proximity to nearby objects. The consequence in this case will cause the objects collected around the neighborhood of these objects to carry relatively high weights in another activity. Section 5.2 evaluates this case further and discusses our potential solution.</p><p>We now compare the performances of the MaxGap and MaxGain algorithms. The performance comparison is based on the two metrics, namely: (1) Mean Absolute Error from the true Boundary (MAEB) and ( <ref type="formula" target="#formula_1">2</ref>) Mean Percentage of the true Boundaries detected (MPB). MAEB is a continuous value that measures how many objects away is the algorithm's boundary from the true boundary. A good algorithm must have MAEB value near zero. MPB, on the other hand, is the ratio between the number of true boundaries detected by the algorithm and the total number of boundaries. A good algorithm has MPB value near 100%. MAEB and MPB are summarized in as follows:</p><formula xml:id="formula_9">MAEB Â¼ P T jÂ¼1 P N iÂ¼1 jtrB ij Ã€ a lg B ij j N Ãƒ T and MPB Â¼ P T jÂ¼1 D j B j Ã‚<label>100</label></formula><formula xml:id="formula_10">T</formula><p>where trB is the boundary; algB is the algorithm's boundary; N is the total number of boundaries; T is the number of traces; D j is the number of correctly detected boundaries in a trace; B j is the total number of true boundaries in a trace.</p><p>In the evaluation, we did not include detecting boundary in cases where two consecutive activities are of the same type since it is trivial to detect such boundaries.</p><p>Table <ref type="table" target="#tab_7">6</ref> shows the comparison result. As expected, MaxGain outperforms MaxGap since MaxGain takes the consideration of Group RW (i.e., RWs of all the objects) in the neighbor of a boundary rather than the RW of two consecutive objects. From the above result, we observe two limitations. First, by analyzing the ground truth, many cases of false segmentation are caused by missed detection and false recognition in the activity recognition process as explained previously. However, the detection of each boundary is done in an independent way that the false segmentation for one boundary does not affect the segmentation for other boundaries. That is the reason we can still achieve relatively high accuracies in the presences of missed detection and false recognition during the segmentation process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Robustness and scalability</head><p>This section evaluates and discusses our concerns in the previous sections. The trace collection in this paper was performed by a number of subjects in a smart home environment. These traces provided a variety of test cases in a close-toreal-life, noisy environment to validate and evaluate the effectiveness of the proposed algorithms. However, it was done in a mock-up situation. In real-life situations, we may see increasing sensor noise in the traces we collected. To evaluate the robustness of both the activity recognition and segmentation algorithms, we conducted a number of simulation studies by manually adding additional noise to our traces. Noise was randomly selected from the entire tagged IDs. This simulation allows us to test our algorithms in cases of a subject touching other tagged objects unintentionally in the activity being performed or because of the close proximity of nearby objects. We generated a set of traces by adding different percentages of noise randomly to each trace, and compared our fingerprint-based recognition algorithm with a HMM model. The comparison result is shown in Fig. <ref type="figure" target="#fig_4">4</ref>. As expected, both two models experience performance drop; however, both decrease linearly. The precisions/recalls of our fingerprint-based algorithm and the HMM model decrease by 19.4%/18.8% and 48.5%/54.5%, respectively, when 40% of additional noise was added. The result also demonstrates the fingerprint-based recognition algorithm is more robust to sensor noise as compared to HMM since Fig. <ref type="figure" target="#fig_4">4</ref> shows that both the precision and recall of a HMM model decrease rapidly. Fig. <ref type="figure" target="#fig_5">5</ref> shows the comparison result of the MaxGap and MaxGain algorithms in the presence of additional noise. The MPBs of MaxGap and MaxGain decrease by 35.5% and 12.1%, respectively, when 40% of additional noise was added. MaxGain outperforms MaxGap since MaxGain decreases in a much lower gradient as compared to MaxGap. This is probably because Group RW resists random noise much better than RW.</p><p>Supervised learning techniques such as HMM typically require a dataset for the training process in order to build various activity models. For better scalability, we have to obtain a very large training dataset over a long period of time, resulting in huge efforts on manual labeling. On the contrary, our fingerprint-based recognition algorithm is unsupervised, and we can mine a comprehensive, large number of activity models on the web without manual labeling. Hence, our solution can achieve better scalability than a HMM model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we propose to build our activity models based on object-use fingerprints by combining web mining and data mining techniques. We mine a set of object terms form the web for each activity, and mine a set of contrast patterns as fingerprints describing significant differences of object terms between any two activity classes. We then propose fingerprint-based algorithm to recognize activities. We also address the trace segmentation problem by proposing the two algorithms, MaxGap and MaxGain, based on the comparison of the relative weights of all objects sandwiched in the two adjacent activities. We conduct real-world trace collection in a smart home environment. The experimental results demonstrate both effectiveness and robustness of our algorithms.  Though RFID has been used in both research and commercially approved contexts, the long-term effects on humans require thorough investigation and further research towards human-centric RFID <ref type="bibr" target="#b42">[43]</ref>. For our future work, we will further develop our sensor platform to include more sensor features and seek a more nature data collection which should be conducted in a real home under real-life situations. In addition, we will investigate complex issues in recognizing interleaved and concurrent activities.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Mining object terms from the web.</figDesc><graphic coords="4,89.35,54.71,368.79,137.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 3 .</head><label>3</label><figDesc>The MaxGain algorithm 1:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. (a) RFID tags, (b) RFID wristband readers, (c) RFID-tagged objects, and (d) A Linux-based server with a programming interface board and a Mica2Dot wireless module.</figDesc><graphic coords="7,56.69,54.71,427.72,91.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Detailed comparison results of our fingerprint-based recognition algorithm and HMM (Numbers in x-axis correspond to Table3).</figDesc><graphic coords="8,103.52,242.65,340.33,168.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Precision/recall of the recognition algorithms vs. different percentages of additional noise.</figDesc><graphic coords="10,165.88,54.71,212.83,141.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. MPBs of both the MaxGap and MaxGain algorithms vs. different percentages of additional noise.</figDesc><graphic coords="10,177.22,234.14,191.97,106.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>A partial view of object terms and their relevance weights.</figDesc><table><row><cell>Make tea</cell><cell></cell><cell>Make coffee</cell><cell></cell><cell>Make pasta</cell><cell></cell><cell>Fry egg</cell><cell></cell></row><row><cell>Object</cell><cell>Weight</cell><cell>Object</cell><cell>Weight</cell><cell>Object</cell><cell>Weight</cell><cell>Object</cell><cell>Weight</cell></row><row><cell>tea</cell><cell>1.00</cell><cell>coffee</cell><cell>1.00</cell><cell>pasta</cell><cell>1.00</cell><cell>egg</cell><cell>1.00</cell></row><row><cell>water</cell><cell>0.85</cell><cell>water</cell><cell>0.86</cell><cell>flour</cell><cell>0.88</cell><cell>pan</cell><cell>0.99</cell></row><row><cell>cup</cell><cell>0.83</cell><cell>cup</cell><cell>0.85</cell><cell>pepper</cell><cell>0.85</cell><cell>oil</cell><cell>0.78</cell></row><row><cell>sugar</cell><cell>0.75</cell><cell>pot</cell><cell>0.82</cell><cell>water</cell><cell>0.84</cell><cell>burner</cell><cell>0.76</cell></row><row><cell>teapot</cell><cell>0.75</cell><cell>grinder</cell><cell>0.80</cell><cell>sauce</cell><cell>0.83</cell><cell>spatula</cell><cell>0.69</cell></row><row><cell>pot</cell><cell>0.74</cell><cell>filter</cell><cell>0.79</cell><cell>tomato</cell><cell>0.81</cell><cell>lid</cell><cell>0.66</cell></row><row><cell>bowl</cell><cell>0.72</cell><cell>sugar</cell><cell>0.76</cell><cell>cheese</cell><cell>0.80</cell><cell>water</cell><cell>0.65</cell></row><row><cell>lemon</cell><cell>0.70</cell><cell>coffeemaker</cell><cell>0.73</cell><cell>garlic</cell><cell>0.80</cell><cell>bowl</cell><cell>0.60</cell></row><row><cell>kettle</cell><cell>0.70</cell><cell>creamer</cell><cell>0.72</cell><cell>oil</cell><cell>0.80</cell><cell>butter</cell><cell>0.60</cell></row><row><cell>microwave</cell><cell>0.67</cell><cell>tablespoon</cell><cell>0.72</cell><cell>pot</cell><cell>0.79</cell><cell>dish</cell><cell>0.60</cell></row></table><note><p>Note: All weights are normalized using the log smoothing function.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>supp 1 Ã°XÃž Â¼ 0 and supp 2 Ã°XÃž &gt; 0</figDesc><table><row><cell>8 &gt; &lt;</cell><cell></cell><cell></cell></row><row><cell>&gt; :</cell><cell>supp 2 Ã°XÃž supp 1 Ã°XÃž</cell><cell>otherwise</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>A subset of CPs for the make tea activity.Definition 4. Given a test instance S t$tÃ¾La i , the score function f Ã°a i ; S t$tÃ¾La i Ãž for activity a i is defined as</figDesc><table><row><cell>CPs</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc><ref type="bibr" target="#b0">1</ref> ; o 2 ; o 3 ; . . . ; o T g; Activities: A Â¼ fa 1 ; a 2 ; a 3 ; . . . ; a m g; Prediction starts at time t, Output: the activity label starts from time t 1:foreach activity a</figDesc><table><row><cell>2:</cell><cell>get instance S t$tÃ¾La i Â¼</cell><cell>StÃ¾L a i jÂ¼t o j ;</cell></row><row><cell>3:</cell><cell>compute f Ã°a i ; S t$tÃ¾La i Ãž;</cell><cell></cell></row><row><cell>4:</cell><cell>endfor</cell><cell></cell></row><row><cell>5:</cell><cell></cell><cell></cell></row></table><note><p>i ; i Â¼ 1; 2; . . . ; m do</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc>A walk-through example for the MaxGain algorithm.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc>Activities performed in our trace collection.</figDesc><table><row><cell>1</cell><cell>make coffee</cell><cell>7</cell><cell>brush teeth</cell><cell>13</cell><cell>clean dining table</cell></row><row><cell>2</cell><cell>make tea</cell><cell>8</cell><cell>wash clothes</cell><cell>14</cell><cell>play PC games</cell></row><row><cell>3</cell><cell>make pasta</cell><cell>9</cell><cell>make orange juice</cell><cell>15</cell><cell>watch TV</cell></row><row><cell>4</cell><cell>make oatmeal</cell><cell>10</cell><cell>watch DVD</cell><cell>16</cell><cell>put on make-up</cell></row><row><cell>5</cell><cell>fry eggs</cell><cell>11</cell><cell>take pills</cell><cell>17</cell><cell>use toilet</cell></row><row><cell>6</cell><cell>make phone call</cell><cell>12</cell><cell>read books</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5</head><label>5</label><figDesc>Comparison result.</figDesc><table><row><cell></cell><cell>Average precision (%)</cell><cell>Average recall (%)</cell></row><row><cell>Fingerprint-based</cell><cell>91.4</cell><cell>92.8</cell></row><row><cell>HMM</cell><cell>93.5</cell><cell>92.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6</head><label>6</label><figDesc>Comparison of the segmentation algorithms.</figDesc><table><row><cell>MAEB</cell><cell>MPB (%)</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Studies of illness in the aged. The index of ADL: a standardized measure of biological and psychological function</title>
		<author>
			<persName><forename type="first">S</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Ford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Moskowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Jaffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Association</title>
		<imprint>
			<biblScope unit="volume">185</biblScope>
			<biblScope unit="page" from="914" to="919" />
			<date type="published" when="1963">1963</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Activity recognition from user-annotated acceleration data</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Intille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Pervasive</title>
		<meeting>of Pervasive<address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-04">2004. April 2004</date>
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Activity recognition in the home using simple and ubiquitous sensors</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Tapia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Intille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Larson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Pervasive</title>
		<meeting>of Pervasive<address><addrLine>Vienna Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-04">2004. April 2004</date>
			<biblScope unit="page" from="158" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Inferring activities from interactions with objects</title>
		<author>
			<persName><forename type="first">M</forename><surname>Philipose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Fishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Perkowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>HÃ¤hnel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Pervasive Computing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="50" to="57" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Human intention detection and activity support system for ubiquitous sensor room</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Nakauchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Noguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Somwong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Matsubara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Robotics and Mechatronics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="545" to="551" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Inferring long-term user property based on users&apos; location history</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Matsuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Okazaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Izumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nakamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nishimura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hasida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nakashima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 20th International Joint Conference on Artificial Intelligence (IJCAI 2007)</title>
		<meeting>of the 20th International Joint Conference on Artificial Intelligence (IJCAI 2007)<address><addrLine>Hyderabad, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-01">January 2007</date>
			<biblScope unit="page" from="2159" to="2165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Towards less supervision in activity recognition from wearable sensors</title>
		<author>
			<persName><forename type="first">T</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 10th IEEE International Symposium on Wearable Computing (ISWC 2006)</title>
		<meeting>of the 10th IEEE International Symposium on Wearable Computing (ISWC 2006)<address><addrLine>Montreux, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-10">October 2006</date>
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised discovery of structure in activity data using multiple eigenspaces</title>
		<author>
			<persName><forename type="first">T</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Second International Workshop on Location and Context-Awareness</title>
		<meeting>of the Second International Workshop on Location and Context-Awareness<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-05">May 2006</date>
			<biblScope unit="page" from="151" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fine-grained activity recognition by aggregating abstract object usage</title>
		<author>
			<persName><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Philipose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ISWC 2005</title>
		<meeting>of ISWC 2005<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-10">October 2005</date>
			<biblScope unit="page" from="44" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Activity recognition of assembly tasks using body-worn microphones and accelerometers</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lukowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Troester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Starner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1553" to="1567" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A long-term evaluation of sensing modalities for activity recognition</title>
		<author>
			<persName><forename type="first">B</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Healey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Philipose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Munguia-Tapia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Intille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Ubicomp</title>
		<meeting>of Ubicomp<address><addrLine>Innsbruck, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-09">2007. September 2007</date>
			<biblScope unit="page" from="483" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mining models of human activities from the web</title>
		<author>
			<persName><forename type="first">M</forename><surname>Perkowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Philipose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Fishkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 13th International World Wide Web Conference</title>
		<meeting>of the 13th International World Wide Web Conference<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>WWW</publisher>
			<date type="published" when="2004-05">2004. May 2004</date>
			<biblScope unit="page" from="573" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised activity recognition using automatically mined common sense</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wyatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Philipose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Choudhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI 2005</title>
		<meeting>of AAAI 2005<address><addrLine>Pittsburgh, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-07">July 2005</date>
			<biblScope unit="page" from="21" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sensor-based understanding of daily life via large-scale use of common sense</title>
		<author>
			<persName><forename type="first">W</forename><surname>Pentney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-M</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Philipose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI<address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-07">2006. July 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An algorithm for suffix stripping</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Program</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="130" to="137" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Stop</forename><surname>Word</surname></persName>
		</author>
		<author>
			<persName><forename type="first">List</forename></persName>
		</author>
		<ptr target="&lt;http://www.dcs.gla.ac.uk/idom/ir_resources/linguistic_utils/stop_words&gt;" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">English</forename><surname>Club</surname></persName>
		</author>
		<ptr target="&lt;http://www.englishclub.com/&gt;" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title/>
		<ptr target="&lt;http://www.keepandshare.com/doc/view.php?u=12894&gt;" />
	</analytic>
	<monogr>
		<title level="j">Adjective List</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Frakes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Baeza-Yates</surname></persName>
		</author>
		<title level="m">Information Retrieval: Data Structures and Algorithms</title>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Berry</surname></persName>
		</author>
		<title level="m">Survey of Text Mining: Clustering, Classification, and Retrieval</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Mcgill</surname></persName>
		</author>
		<title level="m">Introduction to Modern Information Retrieval</title>
		<imprint>
			<publisher>McGraw-Hill</publisher>
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Robust real-time human activity recognition from tracked face displacements</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rybski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Veloso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 12th Portuguese Conference on Artificial Intelligence (EPIA 2005)</title>
		<title level="s">Lecture Notes in Artificial Intelligence</title>
		<meeting>of the 12th Portuguese Conference on Artificial Intelligence (EPIA 2005)</meeting>
		<imprint>
			<publisher>LNAI Springer</publisher>
			<date type="published" when="2005-12">December 2005</date>
			<biblScope unit="volume">3808</biblScope>
			<biblScope unit="page" from="87" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">HMM-based segmentation and recognition of human activities from video sequences</title>
		<author>
			<persName><forename type="first">F</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abdel-Mottaleb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE International Conference on Multimedia and Expo (ICME 2005)</title>
		<meeting>of IEEE International Conference on Multimedia and Expo (ICME 2005)</meeting>
		<imprint>
			<date type="published" when="2005-07">July 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A novel sequence representation for unsupervised analysis of human activities</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bobick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Isbell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Journal</title>
		<imprint>
			<biblScope unit="volume">173</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="1221" to="1244" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Parameterized modeling and recognition of activities</title>
		<author>
			<persName><forename type="first">Yaser</forename><surname>Yacoob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Sixth International Conference on Computer Vision (ICCV&apos;98)</title>
		<meeting>of the Sixth International Conference on Computer Vision (ICCV&apos;98)</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">W4: real-time surveillance of people and their activities</title>
		<author>
			<persName><forename type="first">I</forename><surname>Haritaoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Harwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Recognition and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="809" to="830" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Activity recognition and abnormality detection with the switching hidden semi-Markov model</title>
		<author>
			<persName><forename type="first">T</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Phung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venkatesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CVPR</title>
		<meeting>the CVPR</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="838" to="845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Probabilistic motion parameter models for human activity recognition</title>
		<author>
			<persName><forename type="first">Xinding</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ching-Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Manjunath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 16th International Conference on Pattern Recognition (ICPR&apos;02)</title>
		<meeting>of the 16th International Conference on Pattern Recognition (ICPR&apos;02)</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A practical approach to recognizing physical activities</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanzeem</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaetano</forename><surname>Borriello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Pervasive Computing</title>
		<meeting>of International Conference on Pervasive Computing</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Common sense based joint training of human activity recognizers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pentney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-M</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Philipose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IJCAI</title>
		<meeting>of IJCAI<address><addrLine>Hyderabad</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-01">2007. January 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Conditional random fields for activity recognition</title>
		<author>
			<persName><forename type="first">Douglas</forename><forename type="middle">L</forename><surname>Vail</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuela</forename><forename type="middle">M</forename><surname>Veloso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Autonomous Agents and Multi-agent Systems (AAMAS)</title>
		<meeting>of International Conference on Autonomous Agents and Multi-agent Systems (AAMAS)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Joint Recognition of multiple concurrent activities using factorial conditional random fields</title>
		<author>
			<persName><forename type="first">Tsu-Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chia-Chun</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jane</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Yung-Jen</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI Workshop on Plan, Activity, and Intent Recognition</title>
		<meeting>of AAAI Workshop on Plan, Activity, and Intent Recognition<address><addrLine>California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-07">July 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">CIGAR: concurrent and interleaving goal and activity recognition</title>
		<author>
			<persName><forename type="first">Derek</forename><forename type="middle">Hao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 23rd AAAI Conference on Artificial Intelligence (AAAI 2008)</title>
		<meeting>of the 23rd AAAI Conference on Artificial Intelligence (AAAI 2008)<address><addrLine>Chicago, IL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unsupervised analysis of activity sequences using event motifs</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bobick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Fourth ACM International Workshop on Video Surveillance and Sensor</title>
		<meeting>of the Fourth ACM International Workshop on Video Surveillance and Sensor</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Efficient mining of emerging patterns: discovering trends and differences</title>
		<author>
			<persName><forename type="first">Guozhu</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Fifth ACM SIGKDD Int. Conf. on Knowledge Discovery and Data Mining (KDD&apos;99)</title>
		<meeting>of the Fifth ACM SIGKDD Int. Conf. on Knowledge Discovery and Data Mining (KDD&apos;99)<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-08">August 1999</date>
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Kautz</surname></persName>
		</author>
		<title level="m">A Formal Theory of Plan Recognition</title>
		<imprint>
			<date type="published" when="1987">1987</date>
		</imprint>
		<respStmt>
			<orgName>University of Rochester</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. Dissertation</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Modeling interleaved hidden processes</title>
		<author>
			<persName><forename type="first">Niels</forename><surname>Landwehr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 25th International Conference on Machine Learning (ICML&apos;08)</title>
		<meeting>of the 25th International Conference on Machine Learning (ICML&apos;08)<address><addrLine>Helsinki, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-07">July 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mining sequential patterns</title>
		<author>
			<persName><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 11th International Conference on Data Engineering</title>
		<meeting>of the 11th International Conference on Data Engineering<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="3" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<author>
			<persName><forename type="first">Yida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ee-Peng</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">San-Yih</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Efficient mining of group patterns from user movement data</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="240" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A two-stage methodology for sequence classification based on sequential pattern mining and optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Themis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markos</forename><forename type="middle">G</forename><surname>Exarchos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Costas</forename><surname>Tsipouras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitrios</forename><forename type="middle">I</forename><surname>Papaloukas</surname></persName>
		</author>
		<author>
			<persName><surname>Fotiadis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data and Knowledge Engineering</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="467" to="487" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">FreeSpan: frequent pattern-projected sequential pattern mining</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mortazavi-Asl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Dayal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M-C</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2000 Int. Conf. Knowledge Discovery and Data Mining (KDD&apos;00)</title>
		<meeting>2000 Int. Conf. Knowledge Discovery and Data Mining (KDD&apos;00)<address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-08">August 2000</date>
			<biblScope unit="page" from="355" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Mining minimal distinguishing subsequence patterns with gap constraints</title>
		<author>
			<persName><forename type="first">Xiaonan</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guozhu</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="259" to="286" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Lend me your arms: the use and implications of human-centric RFID</title>
		<author>
			<persName><forename type="first">Amelia</forename><surname>Masters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katina</forename><surname>Michael</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronic Commerce Research and Applications</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="39" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">He has served as a technical program committee member for many international conferences such as Percom, Mobituitous, ICDCS, etc. His current research interest includes pervasive computing and wireless sensor networks</title>
		<author>
			<persName><surname>Dr</surname></persName>
		</author>
		<imprint/>
		<respStmt>
			<orgName>Department of Mathematics and Computer Science at University of Southern Denmark. He received his B. Eng. from Huazhong University of Science and Technology, and M.Sc. from Nanyang Technological University, Singapore, and Ph.D. in Computer Science from National University of Singapore</orgName>
		</respStmt>
	</monogr>
	<note>Tao Gu is currently an Assistant Professor in the</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Now he is a Ph.D. student at UC Davis, USA. His research interest includes wireless network security and pervasive computing. Xianping Tao is currently a Professor in the Department of</title>
		<author>
			<persName><forename type="first">Shaxun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">He received his M.S. and Ph.D. in Computer Science from Nanjing University in 1994 and 2001, respectively. His research interest includes software agents, middleware systems, internetware methodology and pervasive computing</title>
		<meeting><address><addrLine>China; Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005 and 2008. 2007</date>
		</imprint>
		<respStmt>
			<orgName>Computer Science from Nanjing University ; Computer Science and Technology at Nanjing University</orgName>
		</respStmt>
	</monogr>
	<note>He was a research assistant in Institute for Infocomm Research</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">He is also the Director of the State Key Laboratory for Novel Software Technology and is the Vice Director of the Institute of Software Technology at Nanjing University. His research interest includes programming methodology, pervasive computing, software agent, and middleware. He has published over 100 technical papers in the above areas</title>
	</analytic>
	<monogr>
		<title level="s">He received his B.Sc., M.Sc. and Ph.</title>
		<imprint>
			<publisher>UNU International Institute for Software Technology and ACM</publisher>
		</imprint>
		<respStmt>
			<orgName>Jian Lu is currently a Professor in the Department of Computer Science and Technology, Nanjing University ; Computer Science and Technology, Nanjing University</orgName>
		</respStmt>
	</monogr>
	<note>D. in Computer Science at Nanjing University in 1982, 1984 and 1988, respectively. He is currently a Professor in the Department of. He is a member of the Board of the. He is also the chairman of Software Engineering Professional Committee of the China Computer Federation. He has served as an organizing/program committee member for many international conferences</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
