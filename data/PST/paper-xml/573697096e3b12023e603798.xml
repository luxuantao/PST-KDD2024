<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Learning for Event-Driven Stock Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiao</forename><surname>Ding</surname></persName>
							<email>xding@ir.hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Research Center for Social Computing and Information Retrieval</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
							<email>zhang@sutd.edu.sg</email>
							<affiliation key="aff1">
								<orgName type="institution">Singapore University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
							<email>tliu@ir.hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Research Center for Social Computing and Information Retrieval</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junwen</forename><surname>Duan</surname></persName>
							<email>jwduan@ir.hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Research Center for Social Computing and Information Retrieval</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">pore University of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Learning for Event-Driven Stock Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a deep learning method for eventdriven stock market prediction. First, events are extracted from news text, and represented as dense vectors, trained using a novel neural tensor network. Second, a deep convolutional neural network is used to model both short-term and long-term influences of events on stock price movements. Experimental results show that our model can achieve nearly 6% improvements on S&amp;P 500 index prediction and individual stock prediction, respectively, compared to state-of-the-art baseline methods. In addition, market simulation results show that our system is more capable of making profits than previously reported systems trained on S&amp;P 500 stock historical data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>It has been shown that the financial market is "informationally efficient" <ref type="bibr" target="#b2">[Fama, 1965]</ref> -stock prices reflect all known information, and the price movement is in response to news or events. As web information grows, recent work has applied Natural Language Processing (NLP) techniques to explore financial news for predicting market volatility.</p><p>Pioneering work mainly uses simple features from news documents, such as bags-of-words, noun phrases, and named entities <ref type="bibr" target="#b2">[Kogan et al., 2009;</ref><ref type="bibr" target="#b2">Schumaker and Chen, 2009</ref>]. Although useful, these features do not capture structured relations, which limits their potentials. For example, representing the event "Microsoft sues Barnes &amp; Noble." using term-level features {"Microsoft", "sues", "Barnes", "Noble"} alone, it can be difficult to accurately predict the price movements of Microsoft Inc. and Barnes &amp; Noble Inc., respectively, as the unstructured terms cannot differentiate the accuser ("Microsoft") and defendant ("Barnes &amp; Noble").</p><p>Recent advances in computing power and NLP technology enables more accurate models of events with structures. Using open information extraction (Open IE) to obtain structured events representations, we find that the actor and object of events can be better captured <ref type="bibr">[Ding et al., 2014]</ref>. For example, a structured representation of the event above can be (Actor = Microsoft, Action = sues, Object = Barnes &amp; Noble). They report improvements on stock market prediction using their structured representation instead of words as features.</p><p>One disadvantage of structured representations of events is that they lead to increased sparsity, which potentially limits the predictive power. We propose to address this issue by representing structured events using event embeddings, which are dense vectors. Embeddings are trained such that similar events, such as (Actor = Nvidia fourth quarter results, Action = miss, Object = views) and (Actor = Delta profit, Action = didn't reach, Object = estimates), have similar vectors, even if they do not share common words. In theory, embeddings are appropriate for achieving good results with a density estimator (e.g. convolutional neural network), which can misbehave in high dimensions <ref type="bibr" target="#b0">[Bengio et al., 2005]</ref>. We train event embeddings using a novel neural tensor network (NTN), which can learn the semantic compositionality over event arguments by combining them multiplicatively instead of only implicitly, as with standard neural networks.</p><p>For the predictive model, we propose to use deep learning <ref type="bibr" target="#b1">[Bengio, 2009]</ref> to capture the influence of news events over a history that is longer than a day. Research shows diminishing effects of reported events on stock market volatility. For example, <ref type="bibr" target="#b7">Xie et al. [2013]</ref>, <ref type="bibr" target="#b5">Tetlock et al. [2008]</ref> and <ref type="bibr">Ding et al. [2014]</ref> show that the performance of daily prediction is better than weekly and monthly prediction. As shown in Figure <ref type="figure" target="#fig_0">1</ref>, the influences of three actual events for Google Inc. in the year 2012 was the highest on the second day, but gradually weakened over time. Despite the relatively weaker effects of long-term events, the volatility of stock markets is still affected by them. However, little previous work quantitively models combined short-term and long-term effects of events. To fill in this gap, we treat history news as daily event sequences, using a convolutional neural network (CNN) to perform semantic composition over the input event sequence, and a pooling layer to extract the most representative global features. Then a feedforward neural network is used to associate the global features with stock trends through a shared hidden layer and a output layer.</p><p>Experiments on large-scale financial news datasets from Reuters and Bloomberg show that event embeddings can effectively address the problem of event sparsity. In addition, the CNN model gives significant improvement by using longer-term event history. The accuracies of both S&amp;P 500 index prediction and individual stock prediction by our approach outperform state-of-the-art baseline methods by nearly 6%. Market simulation shows that our model is more capable of making profits compared to previous methods. To our knowledge, we are the first to use a deep learning model for event-driven stock market prediction, which gives the best reported results in the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Neural Tensor Network for Learning Event Embeddings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Event Representation and Extraction</head><p>We follow our previous work <ref type="bibr">[Ding et al., 2014]</ref> and represent an event as a tuple E = (O 1 , P, O 2 , T ), where P is the action, O 1 is the actor and O 2 is the object on which the action is performed. T is the timestamp of the event, which is mainly used for aligning stock data with news data, and not for event embeddings. For example, the event "Jan 13, 2014 -Google Acquires Smart Thermostat Maker Nest For for $3.2 billion." is modeled as: (Actor = Google, Action = acquires, Object = Nest, Time = Jan 13, 2014).</p><p>We extract structured events from free text using Open IE technology and dependency parsing. Given a sentence obtained from news text, we first use ReVerb <ref type="bibr" target="#b1">[Fader et al., 2011]</ref> to extract the candidate tuples of the event (O 1 , P , O 2 ), and then parse the sentence with <ref type="bibr">ZPar [Zhang and Clark, 2011]</ref> to extract the subject, object and predicate. We assume that O 1 , O 2 , and P should contain the subject, object, and predicate, respectively. If this is not the case, the candidate tuple is filtered out. Redundancy in large news data allows this method to capture major events with high recalls.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Event Embedding</head><p>Events are extremely sparse. Our previous work used backoff features (e.g. (O 1 , P ), (P, O 2 ), O 1 , P , O 2 ) to address this issue, and we generalized verbs into verb classes, so that similar actions become one feature <ref type="bibr">[Ding et al., 2014]</ref>. In contrast, our goal is to automatically learn embeddings for structured event tuples E = (O 1 , P, O 2 ), which draw more fundamental relations between events, even if they do not share the same action, actor or object. Our task is related to previous work on learning distributed representations of multi-relational data from knowledge bases <ref type="bibr" target="#b1">[Bordes et al., 2011;</ref><ref type="bibr" target="#b4">Socher et al., 2013]</ref>, which learns the embedding of (e 1 , R, e 2 ), where e 1 and e 2 are named entities and R is the relation type. However, learning structured event embedding has two significant differences.</p><p>First, the number of relation types in knowledge bases is limited. Hence, most previous work models a relation type by using a matrix or a tensor, and train a model for each specific relation type. However, as introduced in the previous section, we extract events based on Open IE technology, and the event types is an open set. Therefore, it is more difficult to train a specific model for each event type. To address this issue, we represent the action P as a vector, which shares the dimensionality with event arguments.</p><p>Second, the goal of relational database embedding is to be able to state whether two entities (e 1 , e 2 ) are in a certain relation R. When R is symmetric, e 1 and e 2 have interchangeable roles. In contrast, each argument of the event has a specific role, which is not interchangeable. To address this difference, we design a novel neural tensor network to embed structured events, in which the role of argument is explicitly modeled. As illustrated in Figure <ref type="figure" target="#fig_1">2</ref>, two tensors, T 1 and T 2 , are used to model the roles of O 1 and O 2 , respectively. O 1 T 1 P and P T 2 O 2 are used to construct two role-dependent embeddings R 1 and R 2 , respectively. A third tensor, T 3 , is used for semantic compositionality over R 1 and R 2 , and generate a complete structured embedding U for E = (O 1 , P, O 2 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Neural Tensor Network</head><p>The input of neural tensor network is word embeddings and the output is event embeddings. We learn the initial word representation of d-dimensions (d = 100) from large-scale financial news corpus, using the skip-gram algorithm <ref type="bibr">[Mikolov et al., 2013]</ref>. As most event arguments consist of several words, we represent the actor, action and object as the average of its word embeddings, respectively, allowing the sharing of statistical strength between the words describing each component (e.g. Nokia's mobile phone business and Nokia).</p><p>From Figure <ref type="figure" target="#fig_1">2</ref>, R 1 ∈ R d is computed by: </p><formula xml:id="formula_0">R1 = f (O T 1 T [1:k] 1 P + W O1 P + b) (1) Algorithm 1: Event Embedding Training Process Input: E = (E 1 , E 2 , • • • , E n ) a set</formula><formula xml:id="formula_1">E r ← (E r 1 , E r 2 , • • • , E r n ) 3 while E = [ ] do 4 loss ← max (0 , 1 − f (E i ) + f (E r i ) + λ Φ 2 2 5 if loss &gt; 0 then 6 Update(Φ) 7 else 8 E ← E/{E i } 9 return EELM where T [1:k] 1 ∈ R d×d×k is a tensor, and the bilinear tensor product O T 1 T [1:k] 1 P is a vector r ∈ R k</formula><p>, where each entry is computed by one slice of the tensor (</p><formula xml:id="formula_2">r i = O T 1 T [i] 1 P, i = 1, • • • , k).</formula><p>The other parameters are a standard feed-forward neural network, where W ∈ R k×2d is the weight matrix, b ∈ R k is the bias vector, and f = tanh is the activation function. R 2 and U are computed in the same way as R 1 .</p><p>We also experiment with randomly initialized word vectors as the input of NTN, which is commonly used in related work on structured embedding <ref type="bibr" target="#b1">[Bordes et al., 2011;</ref><ref type="bibr" target="#b2">Jenatton et al., 2012]</ref>. In our case, pre-trained word embeddings give slightly better results than randomly initialized embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training</head><p>We extract more than 10 million events from Reuters financial news and Bloomberg financial news as the training data for event embeddings. The training algorithm repeats for N iterations over the training examples, which is a set of event tuples E = (O 1 , P, O 2 ), extracted from the training corpus using the method in Section 2.1. In each iteration, the training procedure is shown in Algorithm 1.</p><p>We assume that event tuples in the training set should be given a higher score than corrupted tuples, in which one of the event arguments is replaced with a random argument. The corrupted event tuple is denoted as E r = (O r 1 , P, O 2 ). Specifically, we replace each word in O 1 with a random word w r in our dictionary D (it contains all the words in the training data) and derive a corrupted O r 1 . We calculate the margin loss of the above two event tuples as:</p><formula xml:id="formula_3">loss(E , E r ) = max(0 , 1 − f (E ) + f (E r )) + λ Φ 2 2 ,<label>(2)</label></formula><p>where</p><formula xml:id="formula_4">Φ = (T 1 , T 2 , T 3 , W, b) is the set of parameters. The standard L 2 regularization weight λ is set as 0.0001. If the loss loss(E, E r ) = max(0, 1 − f (E) + f (E r )</formula><p>) is equal to zero, the algorithm continues to process the next event tuple.</p><p>Otherwise, the parameters are updated to minimize the loss using the standard back-propagation (BP) algorithm. The iteration number N is set to 500. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Deep Prediction Model</head><p>We model long-term events as events over the past month, mid-term events as events over the past week, and short-term events as events on the past day of the stock price change. As shown in Figure <ref type="figure" target="#fig_2">3</ref>, the prediction model learns the effect of these three different time spans on stock prices based on the framework of a CNN.</p><p>The input to the model is a sequence of event embeddings, where events are arranged in chronological order. Embeddings of the events on each day are averaged as a single input unit (U ). The output of the model is a binary class, where Class +1 represents that the stock price will increase, and Class -1 represents that the stock price will decrease. As shown in Figure <ref type="figure" target="#fig_2">3</ref>, for long-term (left) and mid-term (middle) news, the narrow convolution operation is used to combine l (l = 3) neighbour events. It can be viewed as feature extraction based on sliding window, which can capture local information through combinations of vectors in a window.</p><p>For our task, it is necessary to utilize all local features and predict stock price movements globally. Hence, we use a max pooling layer on top of the convolutional layer, which forces the network to retain only the most useful local features produced by the convolutional layer. Note that the convolution operation is only applied to the long-term and mid-term event embeddings, because the unit of timing is one day.</p><p>Formally, given a series of input event embeddings</p><formula xml:id="formula_5">U = (U 1 , U 2 , • • • , U n ), where U i ∈ R d ,</formula><p>a one-dimensional convolution function takes the dot product of the weight vector W 1 ∈ R l with each l events (sliding window) in U to obtain a new sequence Q:</p><formula xml:id="formula_6">Qj = W T 1 U j−l+1:j<label>(3)</label></formula><p>To determine the most representative features globally, we perform a max pooling operation over Q.</p><formula xml:id="formula_7">Vj = max Q(j, •),<label>(4)</label></formula><p>where Q(j, •) is the j-th row of matrix Q. After max pooling, we obtain the feature vector V . For long-term and mid-term events, we obtain the feature vector V l and V m , respectively. For short-term events, we obtain the feature vector V s by directly using its averaged event embeddings U s . The feature layer is the combination of long-term, mid-term and shortterm feature vectors</p><formula xml:id="formula_8">V C = (V l , V m , V s ).</formula><p>To correlate the feature vector V C and stock prices, we use a feedforward neural network with one hidden layer and one output layer. Formally, let the values of the output layer be y cls (cls ∈ {+1, −1}), its input be net cls , and Y be the neuron vector of the hidden layer; then:</p><formula xml:id="formula_9">y cls = f (net cls ) = σ(W T 3 • Y ) and Y = σ(W T 2 • V C ),</formula><p>where σ is the sigmoid function, W 2 is the weight vector between the hidden layer and the feature layer, and W 3 is the weight vector between the neuron cls of the output layer and the hidden layer. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines and Proposed Models</head><p>The baselines are two state-of-the-art financial-news-based stock market prediction systems: Luss and d' <ref type="bibr">Aspremont et al. [2012]</ref> propose using bags-of-words to represent news documents, and constructing the prediction model by using Support Vector Machines (SVMs). Ding et al.</p><p>[2014] report a system that uses structured event tuples E = (O 1 , P, O 2 ) to represent news documents, and investigates the complex hidden relationships between events and stock price movements by using a standard feedforward neural network.</p><p>In contrast to the baselines, we use a neural tensor network to learn event embeddings for representing news documents, and build a prediction model based on a deep CNN. To make detailed analysis, we construct the following five models: • EB-CNN: event embeddings input and convolutional neural network prediction model (this paper).</p><p>A word embedding input (WB) consists of the sum of each word in a document; it addresses sparsity in word-based inputs, and can serve as a baseline embedding method. The standard feedforward neural network (NN) is used as a baseline to compare with the deep CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Development Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S&amp;P 500 Index Prediction</head><p>We test the influence of event embeddings by comparing them with the structured event representation <ref type="bibr">[Ding et al., 2014]</ref>, and the CNN model by comparison with the standard feedforward neural network model <ref type="bibr">[Ding et al., 2014]</ref>. The experimental results are shown in Table <ref type="table" target="#tab_3">2</ref>. We find that:</p><p>(1) Comparison between the word-embedding-based models and event-embedding-based models (e.g. WB-NN vs EB-NN and WB-CNN vs EB-CNN) confirms our previous conclusion <ref type="bibr">[Ding et al., 2014]</ref>: events are better features than words for stock market prediction.</p><p>(2) Event embedding is useful for the task of stock market prediction. Given the same prediction model (CNN or NN), the event embeddings based methods (EB-NN and EB-CNN) achieve consistently better performance than the events-based methods (E-NN and E-CNN). This is likely due to the following reasons. First, low-dimensional dense vector can effectively alleviate the problem of feature sparsity. In fact, using word embeddings of events (WB-NN) only, we can achieve better performance than <ref type="bibr">Ding et al. [2014]</ref> (E-NN). This contrast demonstrates the importance of reducing sparsity, which rivals the effect of structured information.</p><p>Second, we can learn deeper semantic relations between event embeddings, by modeling the semantic compositionality over word embeddings. For example, the two events E 1 = (Actor = Nvidia fourth quarter results, Action = miss, Object = views) in the training data and E 2 =(Actor = Delta profit,   <ref type="bibr">[2014]</ref> even after backing-off, as the actor and the object of E 1 and E 2 are different, and the predicates of these two events cannot be generalized to the same verb class. However, the semantic distance of these two event embeddings are very small in our model, even though they do not have similar word embeddings. As a result, E 1 can serve as a relative training example for predicting using E 2 in our model.</p><p>(3) CNN-based prediction models are more powerful than NN-based prediction models (e.g. WB-CNN vs WB-NN, EB-CNN vs EB-NN, and E-CNN vs E-NN). This is mainly because CNN can quantitively analyze the influence of the history events over longer terms, and can extract the most representative feature vector for the prediction model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Individual Stock Prediction</head><p>We compare our approach with the baselines on individual stock prediction using the development dataset. We use the 15 companies selected by <ref type="bibr">Ding et al. [2014]</ref> from S&amp;P 500. The list consists of samples from high-ranking, middle-ranking, and low-ranking companies from S&amp;P 500 according to the Fortune Magazine. The results are shown in Figure <ref type="figure" target="#fig_3">4</ref> (as space is limited, we only show comparison between EB-CNN and the two baselines). We find that:</p><p>(1) Our model achieves consistently better performance compared to the baseline methods, on both individual stock prediction and S&amp;P 500 index prediction.</p><p>(2) Our model achieves relatively higher improvements on those lower fortune ranking companies, for which fewer news are available. For the baseline methods, the prediction results of low-ranking companies dramatically decrease. However, our model considers the diminishing influence of monthly news and weekly news, which are important features for individual stock prediction. Hence, even without daily news, our model can also give relatively accurate prediction results.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Market Simulation</head><p>We simulate real stock trading by following the strategy proposed by Lavrenko et al. <ref type="bibr">[2000]</ref>, which mimics the behavior of a daily trader who uses our model in a simple way. If the model indicates that an individual stock price will increase the next day, the fictitious trader will invest in $10,000 worth of that stock at the opening price. After a purchase, the trader will hold the stock for one day. During the holding time, if the stock can make a profit of 2% or more, the trader sells immediately. Otherwise, at the end of the day, the trader sells the stock at the closing price. The same strategy is used for shorting, if the model indicates that an individual stock price will decrease. If the trader can buy the stock at a price 1% lower than shorted, he/she buys the stock to cover. Otherwise, the trader buys the stock at the closing price. We use the same training, development and test dataset as shown in Table <ref type="table" target="#tab_1">1</ref>, for the simulation. Table <ref type="table" target="#tab_4">3</ref> summarizes the average cumulative profits over the 15 companies in Section 4.3. These results are obtained on the development data. The cumulative earnings of our model averaged $16,785 (which means that trading $10,000 worth of stocks would result in a net profit of $16,785), which is higher than Luss and d'Aspremont <ref type="bibr">[2012]</ref>  <ref type="bibr">($8,694)</ref> and <ref type="bibr">Ding et al. [2014] ($10,456)</ref>. Except for the reasons analyzed in Sections 4.3, we notice that if there is no news reported for an individual stock on the previous day, their models cannot predict the trend of the stock price movements on a day, because they do not leverage long-term and mid-term news. This does not hurt the evaluation results of accuracy and MCC, but can hurt the real profit.</p><p>To verify the statistical significance of our earnings, we perform a randomization test <ref type="bibr">[Edgington and Onghena, 2007]</ref> by randomly buying or shorting for 1000 trials. The mean profit over the randomized test is -$9,865 and the performance of our model is significant at the 1% level.   Except for IBM, we achieve consistently better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Final Results</head><p>Better trading strategies. To further investigate the effectiveness of our model, we buy or sell stocks according to the classification probability. If the uptrend probability is higher than a threshold β, we buy $10,000 worth of the stock. If the downtrend probability is higher than β, we short $10,000 worth of the stock. Otherwise, we do not buy or short the stock. The results are shown in Figure <ref type="figure" target="#fig_4">5</ref>. We find that the best profit can be achieved when the threshold β is set as 0.7. Using this strategy, the overall profit is $82,000, significantly higher than $21,000 by using Lavrenko et al. <ref type="bibr">[2000]</ref>'s strategy. The results suggest space for further improvement. Exploration of sophisticated trading strategies are beyond the scope of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Efficient Market Hypothesis (EMH) <ref type="bibr" target="#b2">[Fama, 1965]</ref> states that the price of a security reflects all of the information available, and that everyone has a certain degree of access to the information. Despite 50 years of studies from the fields of finance, computer science and other research communities, the debate continues over what kinds of information can be useful for stock market prediction. In Artificial Intelligence (AI), three sources of information has been the most exploited for algorithmic stock market prediction.</p><p>First, some prediction techniques leverage historical and time-series data <ref type="bibr" target="#b4">[Taylor and Xu, 1997;</ref><ref type="bibr" target="#b0">Andersen and Bollerslev, 1997;</ref><ref type="bibr" target="#b5">Taylor, 2007]</ref>. Researchers believed that predictions can be made through careful averaging of historical price and volume movements and comparing them against current prices. It is also believed that there are certain high or low psychological price barriers, such as support and resistance levels. However, these methods ignore one key source of market volatility: financial news.</p><p>With advances of NLP techniques, various studies have found that financial news can dramatically affect the share price of a security <ref type="bibr" target="#b1">[Cutler et al., 1998;</ref><ref type="bibr" target="#b5">Tetlock et al., 2008;</ref><ref type="bibr">Luss and d'Aspremont, 2012;</ref><ref type="bibr" target="#b7">Xie et al., 2013;</ref><ref type="bibr" target="#b6">Wang and Hua, 2014]</ref>. Recently, we proposed using structured events to represent news, which can indicate the actors and objects of events <ref type="bibr">[Ding et al., 2014]</ref> . However, modeling complex event structures directly, their work is challenged by the new problem of sparsity. To this end, this paper proposes learning event embedding.</p><p>Apart from events, sentiment is another perspective of deep semantic analysis of news documents <ref type="bibr" target="#b1">[Das and Chen, 2007;</ref><ref type="bibr">Tetlock, 2007;</ref><ref type="bibr" target="#b5">Tetlock et al., 2008;</ref><ref type="bibr" target="#b1">Bollen et al., 2011;</ref><ref type="bibr">Si et al., 2013]</ref>. <ref type="bibr">Tetlock [2007]</ref> examines how qualitative information (i.e. the fraction of negative words in a particular news column) is incorporated in aggregate market valuations. <ref type="bibr" target="#b1">Bollen and Zeng [2011]</ref> find that large-scale collective emotions (representing public moods) on Twitter is correlated with the volatility of Dow Jones Industrial Average (DJIA). <ref type="bibr" target="#b4">Si et al. [2014]</ref> propose to regress topic-sentiment time-series and stock's price time series. Their work is orthogonal to event-driven stock market prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We demonstrated that deep learning is useful for event-driven stock price movement prediction by proposing a novel neural tensor network for learning event embeddings, and using a deep convolutional neural network to model the combined influence of long-term events and short-term events on stock price movements. Experimental results showed that eventembeddings-based document representations are better than discrete events-based methods, and deep convolutional neural network can capture longer-term influence of news event than standard feedforward neural network. In market simulation, a simple greedy strategy allowed our model to yield more profit compared with previous work.</p><p>tional <ref type="bibr">Basic Research Program (973 Program)</ref> of China via Grant 2014CB340503, the National Natural Science Foundation of China (NSFC) via Grant 61133012 and 61472107, the TL SUTD grant IGDST1403012 and SRG ISTD 2012 038 from Singapore University of Technology and Design. We are grateful to Siddharth Agrawal for discussions in working with neural tensor network.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example news influence of Google Inc.</figDesc><graphic url="image-1.png" coords="1,348.30,216.00,176.40,124.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Neural tensor network for event embeddings.</figDesc><graphic url="image-2.png" coords="2,335.70,54.00,201.60,137.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Architecture of the prediction model based on a deep convolutional neural network.</figDesc><graphic url="image-3.png" coords="3,323.10,54.00,226.80,127.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Development results of individual stock prediction (companies are named by their ticker symbols).</figDesc><graphic url="image-5.png" coords="5,321.78,54.44,218.70,114.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Influence of threshold.</figDesc><graphic url="image-6.png" coords="6,348.30,54.00,176.41,101.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>of event tuples; the model EELM Output: updated model EELM 1 random replace the event argument and got the corrupted event tuple 2</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Statistics of datasets.</figDesc><table><row><cell></cell><cell>Training</cell><cell cols="2">Development Test</cell></row><row><cell>#documents</cell><cell>442,933</cell><cell>110,733</cell><cell>110,733</cell></row><row><cell>#words</cell><cell>333,287,477</cell><cell>83,247,132</cell><cell>83,321,869</cell></row><row><cell>#events</cell><cell>295,791</cell><cell>34,868</cell><cell>35,603</cell></row><row><cell cols="2">time interval 02/10/2006 -</cell><cell>19/06/2012 -</cell><cell>22/02/2013 -</cell></row><row><cell></cell><cell>18/06/2012</cell><cell>21/02/2013</cell><cell>21/11/2013</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Development results of index prediction.</figDesc><table><row><cell>Acc</cell><cell>MCC</cell></row></table><note>1 http://ir.hit.edu.cn/∼xding/index english.htm</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Averaged profit of 15 individual companies.</figDesc><table /><note>Action = didn't reach, Object = estimates) in the development data result in different features by Ding et al.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Final results on the test dataset.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Profit compared with Lavrenko et al. [2000]. (there are 4 negative profit stocks out of 15 which are not included in this table)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Table4shows the final experimental results on the test dataset, whereLuss [2012]  is the model ofLuss and  d'Aspremont [2012]  andDing [2014]  is the model of our previous work. As space is limited, we only show the average prediction results of 15 individual stocks. The results demonstrate consistently better performance, which indicates the robustness of our model. Simulation To compare with Lavrenko et al.[2000], Table5shows the profit for eight companies (i.e., IBM, Lucent, Yahoo, Amazon, Disney, AOL, Intel and Oracle) selected by them. We use for training data news between October and December 1999, and for test data news of 40 days starting on January 3rd, 2000, which is the same with Lavrenko et al.[2000].</figDesc><table><row><cell>Market</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence(IJCAI 2015)   </note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers for their constructive comments, and gratefully acknowledge the support of the Na-</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Intraday periodicity and volatility persistence in financial markets</title>
		<author>
			<persName><forename type="first">Bollerslev</forename><forename type="middle">;</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Torben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Bollerslev</surname></persName>
		</author>
		<author>
			<persName><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
				<editor>
			<persName><forename type="first">Hugo</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Pascal</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><surname>Vincent</surname></persName>
		</editor>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="1997">1997. 1997. 2005. 2005</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="115" to="122" />
		</imprint>
	</monogr>
	<note>Non-local manifold parzen windows</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Using structured events to predict stock price movement: An empirical investigation</title>
		<author>
			<persName><surname>Bengio ; Bollen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Yoshua Bengio. Learning deep architectures for ai. Foundations and trends R in Machine Learning</title>
				<meeting><address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1998">2009. 2009. 2011. 2011. 2011. 2011. 1998. 1998. 2007. October 2014. 2007. 2007. 2011. 2011</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1535" to="1545" />
		</imprint>
	</monogr>
	<note>Proc. of EMNLP</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A latent factor model for highly multi-relational data</title>
		<author>
			<persName><forename type="first">Eugene</forename><forename type="middle">F</forename><surname>Fama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Fama</surname></persName>
		</author>
		<author>
			<persName><surname>Jenatton</surname></persName>
		</author>
		<idno type="DOI">10.1080/14697688.2012.672762):1-14</idno>
		<idno type="arXiv">arXiv:1301.3781</idno>
	</analytic>
	<monogr>
		<title level="m">Ronny Luss and Alexandre d&apos;Aspremont. Predicting abnormal returns from news using text classification</title>
				<editor>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</editor>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1965">1965. 1965. 2012. 2012. 2009. 2009. 2000. 2000. 2012. 2012. 2013. 2012. 2012. 2009. 2009</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>TOIS</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Exploiting topic based twitter sentiment for stock prediction</title>
		<author>
			<persName><forename type="first">Si</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
				<meeting>of ACL<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-08">2013. August 2013</date>
			<biblScope unit="page" from="24" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The incremental volatility information in one million foreign exchange quotations</title>
		<author>
			<persName><forename type="first">Si</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
				<meeting>of EMNLP<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">2014. 2014. 2013. 2013. 1997</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="317" to="340" />
		</imprint>
	</monogr>
	<note>Reasoning with neural tensor networks for knowledge base completion</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">More than words: Quantifying language to measure firms&apos; fundamentals</title>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><surname>Tetlock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Finance</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1437" to="1467" />
			<date type="published" when="2007">2007. 2007. 2008. 2008</date>
		</imprint>
	</monogr>
	<note>Modelling financial time series</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A semiparametric gaussian copula regression model for predicting financial risks from earnings calls</title>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">C</forename><surname>Tetlock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Tetlock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenhao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
				<meeting>of ACL</meeting>
		<imprint>
			<publisher>Wang and Hua</publisher>
			<date type="published" when="2007">2007. 2007. 2014. 2014</date>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="1155" to="1165" />
		</imprint>
	</monogr>
	<note>Giving content to investor sentiment: The role of media in the stock market</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Syntactic processing using the generalized perceptron and beam search</title>
		<author>
			<persName><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
				<meeting>of ACL<address><addrLine>Stephen Clark</addrLine></address></meeting>
		<imprint>
			<publisher>Zhang and Clark</publisher>
			<date type="published" when="2011">2013. 2013. 2011. 2011</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="105" to="151" />
		</imprint>
	</monogr>
	<note>Semantic frames to predict stock price movement</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
