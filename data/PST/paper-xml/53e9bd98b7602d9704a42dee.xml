<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Assessing the contribution of color in visual attention</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2005-05-23">23 May 2005</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Timothe</forename><surname>´e Jost</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Microtechnology</orgName>
								<orgName type="institution">University of Neucha ˆtel</orgName>
								<address>
									<postCode>CH-2000</postCode>
									<settlement>Neucha ˆtel</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Nabil</forename><surname>Ouerhani</surname></persName>
							<email>nabil.ouerhani@unine.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Microtechnology</orgName>
								<orgName type="institution">University of Neucha ˆtel</orgName>
								<address>
									<postCode>CH-2000</postCode>
									<settlement>Neucha ˆtel</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Roman</forename><surname>Von Wartburg</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Neurology</orgName>
								<orgName type="institution">University of Bern</orgName>
								<address>
									<postCode>CH-3010</postCode>
									<settlement>Inselspital, Bern</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rene</forename><surname>´mu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Neurology</orgName>
								<orgName type="institution">University of Bern</orgName>
								<address>
									<postCode>CH-3010</postCode>
									<settlement>Inselspital, Bern</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Heinz</forename><surname>Hu ¨gli</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Microtechnology</orgName>
								<orgName type="institution">University of Neucha ˆtel</orgName>
								<address>
									<postCode>CH-2000</postCode>
									<settlement>Neucha ˆtel</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Assessing the contribution of color in visual attention</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2005-05-23">23 May 2005</date>
						</imprint>
					</monogr>
					<idno type="MD5">A22FD862C9EFF750A953185B6DE0C326</idno>
					<idno type="DOI">10.1016/j.cviu.2004.10.009</idno>
					<note type="submission">Received 15 January 2004; accepted 17 October 2004</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Visual attention</term>
					<term>Saliency map</term>
					<term>Human perception</term>
					<term>Eye movements</term>
					<term>Color vision</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Visual attention is the ability of a vision system, be it biological or artificial, to rapidly detect potentially relevant parts of a visual scene, on which higher level vision tasks, such as object recognition, can focus. The saliency-based model of visual attention represents one of the main attempts to simulate this visual mechanism on computers. Though biologically inspired, this model has only been partially assessed in comparison with human behavior. Our methodology consists in comparing the computational saliency map with human eye movement patterns. This paper presents an in-depth analysis of the model by assessing the contribution of different cues to visual attention. It reports the results of a quantitative comparison of human visual attention derived from fixation patterns with visual attention as modeled by different versions of the computer model. More specifically, a one-cue gray-level model is compared to a two-cues color model. The experiments conducted with over 40 images of different nature and involving 20 human subjects assess the quantitative contribution of chromatic features in visual attention.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Visual attention is the ability of a vision system, be it biological or artificial, to rapidly detect potentially relevant parts of a visual scene, on which higher level vision tasks, such as object recognition, can focus.</p><p>It is generally agreed nowadays that under normal circumstances human eye movements are tightly coupled to visual attention. This can be partially explained by the anatomical structure of the human retina, which is composed of a high resolution central part, the fovea, and a low resolution peripheral one. Visual attention guides eye movements in order to place the fovea on the interesting parts of the scene. The foveated information can then be processed in more detail. Thanks to the availability of sophisticated eye tracking technologies, several recent works have confirmed this link between visual attention and eye movements <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref>. <ref type="bibr">Hoffman et al.</ref> suggested in <ref type="bibr" target="#b3">[4]</ref> that saccades to a location in space are preceded by a shift of visual attention to that location. Using visual search tasks, Findlay and Gilchrist concluded that when the eyes are free to move, no additional covert attentional scanning occurs, and most search tasks will be served better with overt eye scanning <ref type="bibr" target="#b4">[5]</ref>. Maioli et al. <ref type="bibr" target="#b5">[6]</ref> agree that ''There is no reason to postulate the occurrence of shifts of visuospatial attention, other than those associated with the execution of saccadic eye movements.'' Thus, eye movement recording is a suitable means for studying the temporal and spatial deployment of visual attention in most situations.</p><p>Like in human vision, visual attention can play a fundamental role in computer vision, given the high computational complexity of typical tasks <ref type="bibr" target="#b6">[7]</ref>. Thus, the paradigm of computational visual attention has been widely investigated during the last two decades, and numerous computational models of visual attention have been suggested <ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>. For a more complete overview on existing computational models of visual attention, the reader is referred to <ref type="bibr" target="#b13">[14]</ref>.</p><p>Most of these models rely on the feature integration theory presented in <ref type="bibr" target="#b14">[15]</ref>. The saliency-based model, which relies on this principle, has first been presented in <ref type="bibr" target="#b15">[16]</ref>, and has given rise to numerous software and hardware implementations <ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref>. The model starts with extracting a number of features from the scene, such as color, intensity, and orientation. Each of the extracted features gives rise to a conspicuity map which highlights conspicuous parts of the image according to this specific feature. The conspicuity maps are then combined into a final map of attention named saliency map, which topographically encodes stimulus saliency at every location of the scene. Note that the model is purely data-driven and does not require any a priori knowledge of the scene. This model has been used in a number of computer vision applications, including image compression <ref type="bibr" target="#b19">[20]</ref>, color image segmentation <ref type="bibr" target="#b20">[21]</ref>, and object tracking in dynamic environments <ref type="bibr" target="#b21">[22]</ref>.</p><p>However, and despite the fact that it is inspired by psychophysical studies, only few works have addressed the biological plausibility of the saliency-based model <ref type="bibr" target="#b22">[23]</ref>. Recently, Parkhurst et al. <ref type="bibr" target="#b23">[24]</ref> presented for the first time a quantitative comparison between the computational model and human visual attention. Using eye movement recording techniques to measure human visual attention, the authors report a relatively high correlation between human attention and the saliency map, especially when the images are presented for a relatively short time of few seconds. Although the contribution of different cues in visual attention was also addressed in that paper, the presented results did not allow a general conclusion regarding the contribution of chromatic features.</p><p>The work presented in the present paper goes further and provides an in-depth analysis of the saliency-based model by quantitatively assessing the contribution of different visual cues in computing visual attention. More specifically, it is aimed at assessing the contribution of the chromatic channels to the control of visual attention. Our hypothesis is that a model including luminance and chrominance based feature channels fares better in predicting where human observers foveate than a model based only on those features derived from luminance.</p><p>The basic idea is to compare human fixations derived from eye movement experiments with the computational maps of attention-the saliency maps-produced by two different versions of the saliency-based model. To this end, color images were presented to human subjects while their eye movements were recorded, providing information about the spatial locations of foveated image parts, as well as the duration of each fixation.</p><p>Then, two computational saliency maps were computed for the same image: a grayscale-based map and a color-based one. For computing the former, only intensity-based features like intensity itself and orientations were considered, whereas for the color-based saliency map, chromatic features were used additionally.</p><p>Another contribution of this work is the use of different metrics for comparing human and computational visual attention. The first comparison metric is a correlation coefficient computed for two maps of attention: the human map of attention, which is computed as the integral of the recorded human fixations, and the computational saliency map. The second comparison metric is a saliency difference measure between randomly picked values of a saliency map on the one hand, and fixationguided values of the same map on the other hand.</p><p>The remainder of this paper is organized as follows. Section 2 recalls basics of the saliency models. Then, Sections 3 and 4 present the experimental workflow considered in this research. Both human fixation measurement methods and comparison methods will be exposed. Finally, Section 5 presents the results, and a general conclusion follows in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Saliency models</head><p>The saliency-based model of visual attention was proposed by Koch and Ullman in <ref type="bibr" target="#b15">[16]</ref>. It is based on four major principles: visual attention acts on a multi-featured input; saliency of locations is influenced by the surrounding context; the saliency of locations is represented on a scalar map (the saliency map); and the winner-take-all and inhibition of return mechanisms are suitable to provide the locations for consecutive attentional shifts.</p><p>Several works have dealt with the realization of this model <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b16">17]</ref>. In our work, we used an implementation of the saliency-based model of visual attention that was inspired by these works. The different steps of the model are detailed below (Fig. <ref type="figure" target="#fig_0">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Feature maps</head><p>First, a number of features (1. . .j. . .n) are extracted from the scene by computing the so-called feature maps F j . Similar to Itti et al. seven different features are considered in this work. They are computed from an RGB color image and belong to two main cues, namely intensity and color.</p><p>• Intensity feature, as a weighted sum of R, G, and B channels <ref type="bibr" target="#b24">[25]</ref> </p><formula xml:id="formula_0">F 1 ¼ I ¼ 0.3 Á R þ 0.59 Á G þ 0.11 Á B.<label>ð1Þ</label></formula><p>• Four local orientation features F 4. . .7 according to the angles h 2 {0°, 45°, 90°, 135°}. Gabor filters, which approximate the receptive field impulse response of orientation-selective neurons in primary visual cortex <ref type="bibr" target="#b25">[26]</ref>, are used to compute the orientation features. Note that both even and odd phase Gabor filters are used in our implementation. • Two chromatic features based on the two color opponency filters R + G À and B + Y À where the yellow signal is defined as Y ¼ RþG 2 . Note that such chromatic opponency exists in human visual cortex <ref type="bibr" target="#b26">[27]</ref> and that the normalization of the opponency signals by I decouples hue from intensity.</p><formula xml:id="formula_1">F 2 ¼ R À G I ; F 3 ¼ B À Y I .<label>ð2Þ</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Conspicuity maps</head><p>In a second step, each feature map is transformed into its conspicuity map which highlights the parts of the scene that strongly differ, according to the feature specificity, from their surroundings. The computation of the conspicuity maps relies on three main components:</p><p>• The center-surround mechanism, implemented with a difference-of-Gaussians-filter, DoG is used to extract local activities for each feature type. • A multiscale approach to detect conspicuous regions, regardless of their sizes. The solution used in this work is based on a multi-resolution representation of images <ref type="bibr" target="#b16">[17]</ref> and computes, for each feature j, a set of conspicuity maps M j,k at different resolutions k. • A normalization and summation step during which, for each feature j, the multiscale maps M j,k are combined, in a competitive way, into a unique feature-related conspicuity map C j in accordance with the following equation:</p><formula xml:id="formula_2">C j ¼ X K k¼1 N ðM j;k Þ;<label>ð3Þ</label></formula><p>where N (AE) is a normalization operator which simulates the competition between the different scales. A detailed description of the normalization strategy is given below. Note that the summation of the multiscale maps is achieved at the coarsest resolution. Maps of finer resolutions are lowpass filtered and downsampled to the required resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Cue maps</head><p>For the comparison purposes of the present work, we group together several features j 2 J cue and we define cue conspicuity maps Ĉcue , according to the following equation:</p><formula xml:id="formula_3">Ĉcue ¼ X j2J cue N ðC j Þ.<label>ð4Þ</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Saliency map</head><p>In the third step of the attention model, the cue-related conspicuity maps Ĉcue are integrated, in a competitive manner, into a saliency map S in accordance with the following equation:</p><formula xml:id="formula_4">S ¼ X m cue¼1 N ð Ĉcue Þ;<label>ð5Þ</label></formula><p>where m is the number of the considered cues. The normalization operator N (AE) is described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Normalization for map combination</head><p>To integrate several conspicuity maps into a unique map, the normalization strategy N (AE) used in this work consists of the following <ref type="bibr" target="#b27">[28]</ref>:</p><p>1. Scale all maps to the same dynamic range to eliminate across-modality amplitude differences due to dissimilar extraction mechanisms. 2. For each map, compute the global maximum M and the average m of all other local maxima. 3. Globally multiply the map by a weight w ¼ ðM À mÞ 2 . Thus, N (AE) normalizes a conspicuity map C in accordance with the following equation:</p><formula xml:id="formula_5">N ðCÞ ¼ w Á C.<label>ð6Þ</label></formula><p>In fact, w measures how the most active locations differ from the average of local maxima of a conspicuity map. Thus, this normalization operator promotes conspicuity maps in which a small number of strong peaks of activity are present and demotes maps that contain numerous comparable peak responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Human visual attention</head><p>Under the assumption that under most circumstances, visual attention and eye movements are tightly coupled, the deployment of human visual attention is experimentally derived from the spatial pattern of fixations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Eye movement recording</head><p>Eye movements were recorded with a infrared video-based tracking system (Eye-Link, SensoMotoric Instruments GmbH, Teltow/Berlin). This system consists of a headset with a pair of infrared cameras tracking the eyes, and a third camera monitoring the screen position to compensate for any head movements. It has a temporal resolution of 250 Hz, a spatial resolution of 0.01°, and a gaze-position accuracy relative to the stimulus position of 0.5-1.0°, largely dependent on subjectsÕ fixation accuracy during calibration. As the system incorporates a head movement compensation, a chin rest was sufficient to reduce head movements and ensure constant viewing distance.</p><p>The images were presented in blocks of 10. Each image block was preceded by a 3 • 3 point grid calibration scheme.</p><p>The images were presented in a dimly lit room on a 19 in. CRT display with a resolution of 800 • 600, 24 bit color depth, and a refresh rate of 85 Hz. Active screen size was 36 • 27 cm and viewing distance 70 cm, resulting in a viewing angle of 29°• 22°. Every image was shown for 5 s, preceded by a center fixation display of 1.5 s. Image viewing was embedded in a recognition task.</p><p>Eye monitoring was conducted on-line throughout the blocks. The eye tracking data were parsed for fixations and saccades in real time, using parsing parameters proven to be useful for cognitive research thanks to the reduction of detected microsaccades and short fixations (&lt;100 ms). Remaining saccades with amplitudes less than 20 pixels (0.75°visual angle) as well as fixations shorter than 120 ms were discarded afterwards <ref type="bibr" target="#b28">[29]</ref>.</p><p>For every image and each subject i, the measurements yielded an eye trajectory T i composed of the coordinates of the successive fixations f k , expressed as image coordinates (x k ,y k )</p><formula xml:id="formula_6">T i ¼ ðf i 1 ; f i 2 ; f i 3 ; . . .Þ.<label>ð7Þ</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Human saliency map</head><p>As a global representation of the set of all fixations f i k , a human saliency map H (x) was computed, under the assumption that this map is an integral of weighted point spread functions h (x) located at the positions of the successive fixations. It is assumed that each fixation gives rise to a normally (Gaussian) distributed activity. The width r of the activity patch was chosen to approximate the size of the fovea. A weighting of h (x) as a function of the fixation duration or position k in the trajectory was not considered.</p><p>Formally, H (x) is computed according to the following equation:</p><formula xml:id="formula_7">H ðxÞ ¼ H ðx; yÞ ¼ X N subj i¼1 X f k 2T i exp ðx k À xÞ 2 þ ðy k À yÞ 2 r 2 ! ;<label>ð8Þ</label></formula><p>where (x k , y k ) are the spatial coordinates of fixation f k in image coordinates. The right part of Fig. <ref type="figure" target="#fig_2">3</ref> shows an example of human fixations superimposed on the corresponding image and the created human saliency map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Comparison metrics</head><p>Two different metrics are considered to compare human fixations and computer saliency maps: a correlation and a score measurement. An extensive description of each metrics follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Correlation of human and saliency maps</head><p>The first metric is defined by the correlation q between the computational and human saliency maps.</p><p>Let H (x) and S (x) be the human and the computational maps, respectively. The correlation coefficient q of the two maps is defined by the following equation:</p><formula xml:id="formula_8">q ¼ P x ðH ðxÞ À l H Þ Á ðSðxÞ À l S Þ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi P x ðH ðxÞ À l H Þ 2 Á P x ðSðxÞ À l S Þ 2 q ;<label>ð9Þ</label></formula><p>where l H and l S are the mean values of the two maps H (x) and S (x), respectively. The value of q lies in the [À1, 1] interval. A value of 1 indicates that both maps are exactly similar, a value of 0 indicates that both maps are totally different and a value of À1 indicates that the two maps are anti-correlated, i.e., that a salient feature in one map is not salient in the other one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Score s</head><p>The score s, also called chance-adjusted saliency by Parkhurst et al. <ref type="bibr" target="#b23">[24]</ref> is shown in Fig. <ref type="figure" target="#fig_1">2</ref> and can be written according to the following equation:</p><formula xml:id="formula_9">s ¼ s fix À s ran .<label>ð10Þ</label></formula><p>It corresponds to the difference of average values of two sets of samples from the computer saliency map S (x); s fix refers to the set of N samples taken at the recorded human fixation locations, while s ran refers to N random samples.</p><p>Considering N fixations f k from an eye trajectory T i , the value of s fix is computed according to the following equation:</p><formula xml:id="formula_10">s fix ¼ 1 N X f k 2T Sðf k Þ.<label>ð11Þ</label></formula><p>The average value of N random fixations in a saliency map S (x) is Gaussian distributed and centered at the mean value of the saliency map, l S , with an associated stan- dard error of r N ¼ r S ffiffi ffi N p , where r S is the standard deviation of the saliency map. For simplicity, we take s ran ¼ l S and we will show the values of r N as error bars on graphs.</p><p>All in all, the score s of Eq. ( <ref type="formula" target="#formula_9">10</ref>) can be expressed by</p><formula xml:id="formula_11">s ¼ 1 N X f k 2T Sðf k Þ À l S .<label>ð12Þ</label></formula><p>If the human fixations are focused on the more salient points in the saliency map, which we expect, the score should be positive. Furthermore, the better the model, the higher the similarity and the higher this score should be.</p><p>The principal difference between the s score and the correlation metric is that the former avoids relying on parameters (i.e., r as used to create the human map) while the later is more ''global'' than the score s, since it compares the overall distribution of the fixations in an image with the saliency map and not just whether fixations are found on salient points or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments and results</head><p>The experimental image dataset consisted in 41 color images containing a mix of natural scenes, fractals, and abstract art images. Most of the images (36) were shown to 20 subjects; the remaining 5 were viewed by 7 subjects. We deem this not to be crucial for the ideas presented here. As stated above, these images were presented to the subjects for 5 s apiece, resulting in an average of 290 fixations per image.</p><p>For each image of the dataset, the human map H (x) was created with the parameter r = 37 pixels, which was chosen to approximate the fovea in our experimental system, and all fixations were taken into account.</p><p>For all images, we also created a color saliency map S col ðxÞ ¼ N ð Ĉchrom Þ þ N ð Ĉint Þ and a grayscale saliency map S gray ðxÞ ¼ N ð Ĉint Þ, according to Eq. ( <ref type="formula" target="#formula_4">5</ref>). Both saliency maps are also normalized to the same dynamic range [0. . .255]. Then, a comparison of these two models with the human fixations was performed, following the metrics defined in Section 4. Fig. <ref type="figure" target="#fig_2">3</ref> shows an example of the different measurements and maps involved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Color vs. grayscale, overall results</head><p>Table <ref type="table">1</ref> presents the overall scores s and the average correlation coefficients q, over the whole experimental dataset, for both the color and grayscale models. The overall scores s were computed taking the fixations of every subject for every image of the database into account. The first five fixations were considered here since it has been suggested that, with regard to human observers, initial fixations are controlled mainly in a bottom-up manner. Finally, the resulting standard errors were computed taking both human and random saliency variances into account.</p><p>The main observation is that, based on both evaluation methods, the color model globally fares better than the grayscale one. More specifically, the color model yields an average score approximately 25% higher than the grayscale model. The worst cases considering the standard errors still yield relative increases of 15 and 7% for both metrics. This underlines the usefulness of the color channel in the model and goes toward assessing that colors have a considerable influence on visual attention. Table <ref type="table">1</ref> Mean of the similarity measurements of the two computer models S (x) vs. human behavior and their respective standard errors</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Score s q</head><p>Grayscale model S gray (x) 24.9 ± 1.21 .263 ± .027 Color model S col (x) 31.3 ± 1.17 .336 ± .025</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Influence of the number of fixations</head><p>Fig. <ref type="figure" target="#fig_3">4</ref> presents the average score s taking different numbers of fixations into account for the calculation. The overall scores were computed using the same method as presented in Section 5.1. Four cases were considered: (1) taking only the first fixation of each subject into account, (2) considering the average of the first three and then (3) five fixations, respectively, (4) taking all fixations made over the whole viewing duration into account. From a temporal point of view, these four cases correspond to the first 0.5, 1, 2, and 5 s of observation, approximately. We observe a general decrease of scores with the considered number of fixations. This suggests that those features calculated by the model as the most salient ones are also foveated first by human observers. There is one exception to this trend, in the values based on the first fixation only. This might be due to the experimental design: as the subjects had to fixate the center of the screen before any image, the location of the first fixation might have been influenced by this starting point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Model performance on individual images</head><p>Fig. <ref type="figure" target="#fig_4">5</ref> presents a few typical images of our database, sorted by their score s values on the color model. The images yielding the best results are found in the upper left; the ranking goes from left to right and top to bottom. As can be seen, the ranking based on the correlational metric would be very similar.</p><p>The principal observation here is that the resulting scores and correlation coefficients are widely spread in the value range. It is apparent that the images found on the top row generally contain a few very salient features, such as the fish, the small house or the water lily, and yield the best results. On the other hand, images that lack highly salient features, such as the abstract art or the fractal images on the bottom row, result in much lower values. Nonetheless, there is only one image (out of 41) that yields a negative value (see Fig. <ref type="figure" target="#fig_6">6</ref>); it is shown in the bottom right position in Fig. <ref type="figure" target="#fig_4">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Color vs. grayscale, individual results</head><p>Fig. <ref type="figure" target="#fig_6">6</ref> shows the correlation values q for the color and grayscale models, while Fig. <ref type="figure" target="#fig_5">7</ref> depicts the difference of score s between the colour and grayscale models for each image.</p><p>As seen on both Figs. <ref type="figure" target="#fig_6">6</ref> and<ref type="figure" target="#fig_5">7</ref>, the color model yields better results than the grayscale one in the large majority of the images. More precisely, out of 41 images, 17 yield a positive score difference over chance, while only 2 have a negative difference over chance. These results confirm the tendency showed by the average results of Sections 5.1 and 5.2 about the importance of the use of the chromatic channels in the saliency model. However, based on our hypothesis and in the perfect case, we would expect the results for the color model to be at least as good as the ones of the grayscale model, for all images. We can note that it is not always the case, since there are still a few images where the results for the color model are worse than the ones of the grayscale model. However, the difference is generally not very large and might be due to the fact that, in a few images, the color channel adds nothing but ''noise'' to the saliency map.  The road sign image permits to clearly see the influence of color in the visual attention process. Indeed, the blue road sign is not detected as very salient in the grayscale model, as opposed to the color model. When looking at human behavior, it is evident that the panel is clearly the most salient feature of the image, thus the importance of colors.</p><p>The sunflower field image is also interesting: it results in very low values with all metrics, even if one might expect the results to be much better at first sight. In fact, on such an image, subjects are mainly focusing onto the horizon and forget about the flowers, maybe because there are a lot of them and they all look the same. Another downside of the saliency model is that it tends to favor the circumference of the flowers, while humans seem to focus on their center. This is a case where the computer saliency model totally fails to reproduce human behavior, due to the human tendency to focus on the horizon. Finally, in the fractal example, the grayscale model is superior to the color model. In fact, at first sight the maps are pretty close to each other, but the color component seems only to add ''noise'' to the saliency map in this case. This is especially well visible in the lower central part of the image.</p><p>All in all, these results confirm the overall usefulness of color in the visual attention process and explain the few cases with a different behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>The work reported in this paper performs comparisons of computer models of visual attention with human attention as measured by recording eye movements of human subjects. The results follow from experiments involving over 40 images of different kinds and nature observed by 20 human subjects in most cases.</p><p>The reported comparisons rely on the comparison of a computer saliency map with the set of fixation points extracted from the eye movements. For measuring their similarity, two different metrics were used-the correlation coefficient q and the score s-which each have their own advantages.</p><p>The contribution of color in visual attention was quantitatively measured as the increase in similarity when the one-cue computer model for grayscale is replaced by the two-cues computer model for color. The similarity improvement, measured as the average on the whole dataset, is from q = .26 to .34 for the correlation coefficient, and from s = 25 to 33 for the score s. This assesses the average quantitative contribution of adding the chromaticity cue to a monochrome computer model of visual attention.</p><p>Notwithstanding their different nature, both considered metrics yielded very similar results, be it while comparing color and grayscale models or when looking at the image rankings.</p><p>A comparison of model performance when the number of considered fixation points is modified shows that on average, the few first fixations are better explained by the model than the set of all fixations of the complete eye movement record.</p><p>Finally, a more detailed analysis of the model performance shows a rather large variation of results, depending on the kind of images. On the other hand, all but one images yield positive scores and correlation coefficients, which speaks for the quality of the model itself when compared to human vision. When compared to the grayscale model, the color one also lead to better results on a large majority of the image of the dataset. All in all, the results assess the usefulness of the chromatic cue in the model of visual attention and speak for the considerable influence of color on human visual attention. anese (University of Geneva) for making available some of the pictures and the source code of their respective models which represented a source of inspiration for our implementations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Saliency-based model of visual attention. (A) The different steps of the model. (B) The conspicuity operator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Average values of the sampled saliency map: s fix and distribution of s ran .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Overview of the different measurements and maps.</figDesc><graphic coords="10,111.90,107.28,337.68,324.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Overall score s vs. number of fixations. Error bars represent plus and minus one standard error of the means.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Some typical images from the experimental set, ranked by average values for the color model. Metrics values are given in the form: score s/map correlation in percent.</figDesc><graphic coords="12,109.06,107.28,340.20,265.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Difference of scores s for the color model vs. grayscale model for each image. Error bars represent plus and minus one standard error of the difference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Correlation coefficients for individual images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Three representative examples with their corresponding human and saliency maps, s calculated with the first five fixations.</figDesc><graphic coords="14,109.06,385.67,341.28,228.96" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>T. Jost et al. / Computer Vision and Image Understanding 100 (2005) 107-123</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>T. Jost et al. / Computer Vision and Image Understanding 100 (2005) 107-123</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is partially supported by the Swiss National Science Foundation under Grant FN 64894. The authors are grateful to Koch Lab (Caltech) and Ruggero Mil-</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Shared neural control of attentional shifts and eye movements</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Kustov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">384</biblScope>
			<biblScope unit="page" from="74" to="77" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A model of eye movements and visual attention</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Salvucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Third Internat. Conf. on Cognitive Modeling</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="252" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Algorithms for defining visual regions-of-interest: comparison with eye fixations</title>
		<author>
			<persName><forename type="first">C</forename><surname>Privitera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Stark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="970" to="981" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Saccadic eye movements and visual selective attention</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Subramaniam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Percept. Psychophys</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="787" to="795" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Saccade target selection during visual search</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Findlay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Res</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="617" to="631" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The integration of parallel and serial processing mechanisms in visual search: evidence from eye movement recording</title>
		<author>
			<persName><forename type="first">C</forename><surname>Maioli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Benaglio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Siri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sosta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. J. Neurosci</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="364" to="372" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Analyzing vision at the complexity level</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Tsotsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behav. Brain Sci</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="423" to="469" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Textons, the fundamental elements in preattentive vision and perception of textures</title>
		<author>
			<persName><forename type="first">B</forename><surname>Julesz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell Syst. Tech. J</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="1619" to="1645" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">VISIT: An Efficient Computational Model of Human Visual Attention</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ahmed</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
		</imprint>
		<respStmt>
			<orgName>University of Illinois at Urbana-Champaign</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Detecting Salient Regions in an Image: From Biological Evidence to Computer implementation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Milanese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<pubPlace>Geneva, Switzerland</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Dept. of Computer Science, University of</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A prototype for data-driven visual attention</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Culhane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Tsotsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Internat. Conf. on Pattern Recognition 92</title>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="36" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Toward computational model of visual attention</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Tsotsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Early Vision and beyond</title>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">V</forename><surname>Papathomas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Chubb</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Gorea</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Kowler</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="207" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Data-and model-driven gaze control for an active-vision system</title>
		<author>
			<persName><forename type="first">G</forename><surname>Backer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mertsching</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bollmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1415" to="1429" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Computational models of visual selective attention: a review</title>
		<author>
			<persName><forename type="first">D</forename><surname>Heinke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Humphreys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Connectionist Models in Psychology</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Houghton</surname></persName>
		</editor>
		<meeting><address><addrLine>Taylor &amp; Francis, London</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A feature-integration theory of attention</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Treisman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gelade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cogn. Psychol</title>
		<imprint>
			<biblScope unit="page" from="97" to="136" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Shifts in selective visual attention: towards the underlying neural circuitry</title>
		<author>
			<persName><surname>Ch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human Neurobiol</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="219" to="227" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ch</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Computing visual attention from scene depth</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ouerhani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hugli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Internat. Conf. on Pattern Recognition (ICPRÕ00)</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="375" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A real time implementation of visual attention on a SIMD architecture</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ouerhani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hugli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Internat. J. Real Time Imaging</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="189" to="196" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adaptive color image compression based on visual attention</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ouerhani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bracamonte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hugli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ansorge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pellandini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Internat. Conf. on Image Analysis and Processing</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="416" to="421" />
		</imprint>
	</monogr>
	<note>ICIAPÕ01</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">MAPS: multiscale attention-based presegmentation of color images</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ouerhani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hugli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">th Internat. Conf. on Scale-space Theories in Computer Vision</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">2695</biblScope>
			<biblScope unit="page" from="537" to="549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A model of dynamic visual attention for object tracking in natural image sequences</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ouerhani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hugli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Internat. Conf. on Artificial and Natural Neural Network (IWANN)</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">2686</biblScope>
			<biblScope unit="page" from="702" to="709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Empirical validation of the saliency-based model of visual attention</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ouerhani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wartburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu ¨gli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Mu ¨ri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electron. Lett. Comput. Vision Image Anal</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="13" to="24" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Modeling the role of salience in the allocation of overt visual attention</title>
		<author>
			<persName><forename type="first">D</forename><surname>Parkhurst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Res</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="107" to="123" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Wyszecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Styles</surname></persName>
		</author>
		<title level="m">Color Science: Concepts and Methods, Quantitative Data and Formulae</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
	<note>second ed.</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The neural basis of visual function</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Vision and Visual Dysfunction</title>
		<meeting><address><addrLine>Boca Raton, FL</addrLine></address></meeting>
		<imprint>
			<publisher>CRC Press</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Colour tuning in human visual cortex measured with functional magnetic resonance imaging</title>
		<author>
			<persName><forename type="first">S</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wandell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">388</biblScope>
			<biblScope unit="issue">6637</biblScope>
			<biblScope unit="page" from="68" to="71" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A comparison of feature combination strategies for saliency-based visual attention systems</title>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ch</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPIE Human Vision and Electronic Imaging IV (HVEIÕ99)</title>
		<meeting><address><addrLine>San Jose, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">3644</biblScope>
			<biblScope unit="page" from="373" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Visuo-motor behaviour during complex image viewing: The influence of colour and image type</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wartburg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<pubPlace>Switzerland</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Bern</orgName>
		</respStmt>
	</monogr>
	<note>Licentiate paper, Dept. of Psychology</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
