<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Ambient Touch: Designing Tactile Interfaces for Handheld Devices</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ivan</forename><surname>Poupyrev</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Interaction Lab, Sony CSL</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shigeaki</forename><surname>Maruyama</surname></persName>
							<email>shigeaki.maruyama@jp.sony.com</email>
							<affiliation key="aff1">
								<orgName type="department">Micro Device Center</orgName>
								<address>
									<addrLine>Sony EMCS, Shinagawa 2-15-3 Konan, Minato-ku</addrLine>
									<postCode>3-14-13, 141-0022, 108-6201</postCode>
									<settlement>Higashigotanda, Tokyo, Tokyo</settlement>
									<country>Japan, Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Rekimoto</surname></persName>
							<email>rekimoto@csl.sony.co.jp</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Interaction Lab, Sony CSL</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Ambient Touch: Designing Tactile Interfaces for Handheld Devices</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5B9F9CADED213CE67FDBE905C5AB7E41</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>tactile feedback, mobile devices and interfaces</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper investigates the sense of touch as a channel for communicating with miniature handheld devices. We embedded a PDA with a TouchEngine™ -a thin, miniature lower-power tactile actuator that we have designed specifically to use in mobile interfaces (Figure <ref type="figure">1</ref>). Unlike previous tactile actuators, the TouchEngine is a universal tactile display that can produce a wide variety of tactile feelings from simple clicks to complex vibrotactile patterns. Using the TouchEngine, we began exploring the design space of interactive tactile feedback for handheld computers. Here, we investigated only a subset of this space: using touch as the ambient, background channel of interaction. We proposed a general approach to design such tactile interfaces and described several implemented prototypes. Finally, our user studies demonstrated 22% faster task completion when we enhanced handheld tilting interfaces with tactile feedback.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTRODUCTION</head><p>Miniature handheld computing devices such as mobile phones, PDAs, digital cameras, and music players are rapidly permeating into our lives today. In the near future, we will, perhaps, spend more time interacting with these devices than with conventional desktop computers equipped with familiar graphical user interfaces (GUI). Therefore, the interface design for small handheld computing devices is an important and exciting challenge for interface researchers and designers. There are many limitations and difficulties to overcome, well documented in interface literature <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36]</ref>. Many of these limitations, however, also reflect the assumptions and expectations inherited from traditional GUI, that fail in handheld devices, particularly heavy reliance on rich visual feedback and undivided attention from the user. In this work we attempt to extend beyond familiar visual interfaces and employ a relatively unexplored channel for interaction with handheld devices: the sense of touch.</p><p>There are many possibilities for tactile feedback use in mobile interfaces. Here, we are particularly interested in a small subset of this design space: using touch as the ambient, background channel of interaction. Our basic hypothesis is that in a mobile setting where the user's attention is not fixed on the computer, but on real-world tasks, such ambient tactile interfaces would become a necessary and important counterpart to the traditional visual interaction. Tactile or coetaneous sense is defined as a combination of various sensations evoked by stimulating the skin <ref type="bibr" target="#b10">[11]</ref>. In combination with kinesthesia, tactile feedback is often referred to as haptics <ref type="bibr">[4]</ref> and is crucial for us to interact with our physical environment. The importance of tactile feedback has been recognized in many fields from virtual reality <ref type="bibr" target="#b7">[8]</ref> to design of consumer electronics <ref type="bibr" target="#b0">[1]</ref>, and it is natural to extend its applications to mobile computers. Indeed, mobile devices are naturally at a close proximity to our skin, whether it is in our hand or tucked away in a pocket. Skin is the largest human sensory organ (~1.8 m 2 <ref type="bibr" target="#b10">[11]</ref>) and with the exception of water and heat regulation, most of it is unused. Mobile interfaces can utilize this. Tactile feedback also provides superior temporary discrimination, e.g. when rapidly successive data needs to be resolved, the feel of touch is about five times faster than vision <ref type="bibr" target="#b18">[19]</ref>. Hence, it allows for precise and fast motor con-Figure <ref type="figure">1</ref>: TouchEngine™ actuator -a new vibrotactile actuator for designing tactile interfaces for small handheld computing devices; it is only 0.5 mm thick trol: When we roll a pencil in our fingers, we can quickly and precisely re-adjust the 3D positions and grasping forces of our fingers by relying entirely on touch <ref type="bibr" target="#b2">[3]</ref>. Furthermore, these complex motor operations produce little cognitive load and can be performed in parallel with other activities, such as reading a newspaper. This is because large areas of the sensory cortex are devoted to processing stimuli from the skin. Moreover, a large amount of the processing occurs in the lower level of the spinal cord, where sensory and motor neuron fibers intersect <ref type="bibr" target="#b10">[11]</ref>  <ref type="foot" target="#foot_0">1</ref> . By re-directing some of the information processing from the visual channel to touch, we can take advantage of this ability to reduce the cognitive load and make it easier to operate mobile devices. The skin also acts as a powerful information pick-up channel. In the real world, it allows us to correctly judge object properties when vision fails: e.g. textures and surface variations can be accurately detected by touch <ref type="bibr" target="#b8">[9]</ref>. Geldart in 1957 <ref type="bibr" target="#b17">[18]</ref> developed a vibrotactile language called "Vibratese" and demonstrated that trained subjects were able to receive a complex message up to 38 words per minute. This and later studies <ref type="bibr" target="#b37">[38]</ref> show that with proper encoding, messages can be transmitted through the skin. We can take advantage of this when designing mobile interfaces. The message, however, does not necessarily need to be symbolic: touch has a strong emotional impact. Running a finger into a splinter, touching a cat's fur, or immersing into some unknown sticky substance all bring intense, though very different, emotional responses. Hence, touch is a very strong "break-in" sense: coetaneous sensations, especially if aroused in unusual patterns, are highly attentiondemanding <ref type="bibr" target="#b16">[17]</ref>. This has already been explored in some mobile interfaces, but there is still more room for research. To conclude, these properties make touch an ideal channel of interaction with handheld devices: it is fast, needs little conscious control, allows for information encoding, and produces strong emotional responses. This paper investigates some of the implications of using tactile feedback in mobile interfaces. We begin with a review of a related work. We then describe the design and implementation of novel tactile displays based on the TouchEngine, a miniature low-power actuator that we created specifically for small handheld devices. Unlike other previously reported actuators, the TouchEngine is a versatile device that can be used to generate a wide spectrum of tactile feelings. We continue by exploring the design space for tactile displays in mobile devices and investigate several applications using touch as the ambient, background channel for mobile communication. Finally, we report the results of the experimental studies, which demonstrated that tactile feedback resulted, on average, 22% faster task completion when used in combination with tilting interfaces in a 1D-scrolling task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RELATED WORK</head><p>The importance of tactile feedback in human-machine interaction is well recognized <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7]</ref>. Much effort has been spent on simulating realistic tactile properties of our physical environment, such as roughness of surfaces. This has been achieved by either developing new, special purpose haptic devices <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b31">32]</ref> or by adding tactile display to usual desktop devices, e.g. a mouse or touchpad <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b29">30]</ref>. In passive tactile feedback, alternatively, the user can "feel" virtual objects via manipulating physical objects with similar haptic properties, e.g. shape or surface texture. For example, in one system, a user rotates a doll's head to rotate a virtual 3D model of a human brain <ref type="bibr" target="#b23">[24]</ref>. This approach is being also explored in the tangible and graspable interface research areas <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b34">35]</ref>. Tactile interfaces were intended to enhance the user experience by making interfaces more realistic, intuitive, and easy to use. Another direction of research investigated how information can be encoded and transmitted to the user through stimulation of the skin <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b37">38]</ref>. Results from this work were used to develop devices to assist blind or deaf people, e.g. Optacon <ref type="bibr" target="#b37">[38]</ref>. Most of these devices rely on vibrotactile arrays to create a "tactile picture" that can be recognized through touch [e.g. 10]. Unfortunately, little work has been done to add tactile feedback to mobile handheld devices. To effectively utilize tactile feedback, we must initially overcome two major challenges. First, there are feasibility limitations in the current actuator technology <ref type="bibr" target="#b19">[20]</ref>. The fundamental requirements for mobile actuators are 1) small, miniature size; 2) lightweight; 3) low voltage (~5V) and low power consumption; and 4) ease in customization to allow retrofitting to devices of various sizes and forms. While technical characteristics of displays are important, we should not forget that they should also feel good. Satisfying only feasibility requirements would not produce expressive tactile displays. Although touch may not be as rich as vision, it is by no means a one-dimensional sense: we have an amazing range of tactile sensations and tactile displays should be able to evoke them. Depending on the design, the difference in feeling between tactile displays can be profound; perhaps, as profound as the difference between today's high-resolution color monitors to black and white vector displays from the 1960s. Hence, the second obstacle is human factors limitations. We still have limited understanding of the sense of touch, but we can suggest that the optimal mobile tactile display should satisfy the following requirements <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b34">35]</ref>: 1. Fast response. The minimal time in which humans can detect two consecutive tactile stimuli is about 5 ms <ref type="bibr" target="#b10">[11]</ref>. Note, that this is about five times faster then vision. Hence, we can approximate the minimum required latency of a tactile actuator to be ~5ms. A large lag would significantly reduce the quality of feeling and would not allow users to effectively regulate their motor movements or perceive complex tactile patterns. For example, vibration motors in mobile phones have significant latency and, therefore, cannot be used in interactive applications. 2. Variable intensity. Humans can discriminate a vast range of intensities of tactile stimulus. Given appropriate conditions, the palm of a hand can feel vibratory stimulus with an amplitude as low as 0.2 microns <ref type="bibr" target="#b20">[21]</ref>. On the other hand, for a comfortable tactile feeling, a much larger displacement should also be provided. Hence, the actuator should be able to generate variable skin displacements. 3. Wide frequency bandwidth. The vibration frequency has a profound effect on tactile perception. For example, single tactile pulses, or taps, are perceived differently from the sinusoidal vibration, because different coetaneous receptors respond to these patterns <ref type="bibr" target="#b10">[11]</ref>. The ideal actuator should provide variable frequencies from 1 to 1000 Hz. 4. Multitude of different wave shapes. Humans are able to distinguish a wide range of tactile wave shapes. Gault in 1924 <ref type="bibr" target="#b16">[17]</ref> converted speech into vibration and found that trained subjects were able to distinguish one out of 10 short sentences with 75% accuracy. Hence, to be effective, tactile actuators should be able to produce numerous wave shapes. We are not aware of tactile actuators that satisfy these criteria: most were designed to create a single type of tactile feeling for a narrow application. Vibration motors, for example, rotate eccentrically weighted shafts that vibrate at about 130Hz. Because its purpose is to alert the user, it is not important that there is significant latency and that only sinusoidal patterns with limited amplitudes and range of frequencies can be displayed. These limitations, however, make it impossible to use vibration motors to encode any complex tactile feelings and to use them in interactive applications where small latency is crucial. Voice coils and speakers can also be used for tactile stimulation <ref type="bibr" target="#b19">[20]</ref>, but the displacement and force they provide is low. Recently Fukamoto <ref type="bibr" target="#b15">[16]</ref> embedded a voice coil type actuator that provided sufficient force when driven at the resonant frequency. However, only a single frequency and amplitude of vibration can be produced; therefore, complex patterns of vibration cannot be encoded. Other interesting technologies are tactile matrix arrays (e.g. <ref type="bibr" target="#b11">[12]</ref>) and direct electrical stimulation <ref type="bibr" target="#b27">[28]</ref>. Matrix arrays are usually large and heavy, and require a considerable amount of voltage and power, e.g. 350V in the case of <ref type="bibr" target="#b11">[12]</ref>, making them impossible to use in mobile devices. Electrocoetaneous devices can be very small and efficient, but their feel is quite different from the familiar tactile feeling. As with any display technology, properties of tactile displays determine their interface applications. Currently few applications of mobile tactile displays are available. Vibration motors have long been used as silent alarm devices, however, because of their inherent limitations, they provide only one bit of information: an occurrence of an event, e.g. a phone call. Probably, the first interactive tactile feedback for handheld devices was implemented in the Active Click by Fukamoto <ref type="bibr" target="#b15">[16]</ref>. Active Click was designed to provide a tactile "click" feeling when users touch the graphical buttons on a touch screen. The coil actuators that he used have very low latency and the illusion of physical button pushing was strong. However, this is only a narrow application-due to the limitations we discussed above, it might be difficult to further expand the Active Click into other applications. Tactile displays have been proposed for wearable computers <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b38">39]</ref> to be used, for example, as a navigation aid. In this work, we, however, are more interested in handheld devices and more interactive applications than display of navigating information. We were inspired by designs that attempted to deviate from the traditional GUI paradigm to create interfaces explicitly tailored for mobile devices. Examples include context-aware and ambient interfaces, embodied and gesture interfaces, augmented reality, and others <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b35">36]</ref>. Most of these interfaces still center on visual interaction and assume focused attention by the user. By augmenting them with tactile displays, we want to expand these concepts leading to new mobile interfaces.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TouchEngine actuator</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Basic structure</head><p>The TouchEngine actuator is constructed as a sandwich of thin (~0.28µm) piezoceramic film with printed adhesive electrodes in between, forming an extremely thin (less then 0.5mm) beam (Figure <ref type="figure">1</ref> and Figure <ref type="figure" target="#fig_1">2</ref>). The piezoceramic material works as a solid state "muscle" by either shrinking or expanding, depending on the polarity of the applied voltage. The material on the top has an opposite polarity to that on the bottom, so when a signal is applied the entire structure bends (Figure <ref type="figure" target="#fig_1">2</ref>, left). This configuration is often called a "bending motor" actuator. Bending motors that were previously used in tactile displays consisted of only two layers (biomorphs) and required extremely high voltage (e.g. 350V in <ref type="bibr" target="#b11">[12]</ref>) making them unsuitable for mobile devices. By sandwiching multiple layers of very thin piezo film with the adhesive electrode (Figure <ref type="figure" target="#fig_1">2</ref>, middle), we can reduce the voltage required for maximum displacement to ±8-10V. Indeed, for the voltage V and the thickness of the piezoelectric layer T, the displacement D and force F for the serially connected bending motors are as follows <ref type="bibr" target="#b39">[40]</ref>:</p><formula xml:id="formula_0">3 4 and 4 31 2 31 2 V T g w F T V d l D ⋅ = = (1)</formula><p>where l and w are beam dimensions, and d 31 and g 31 are piezoelectric constants. We can see that by decreasing the thickness T, we can achieve the same displacement D with a lower voltage. This, however, also decreases the force F; we compensate this by layering multiple thin piezoceramic layers together (Figure <ref type="figure" target="#fig_1">2</ref>, middle). The resulting actuator holds unique properties. It is extremely thin, small, can be battery-operated, and manufactured in various sizes and number of layers. It is also fast allowing us to control both the amplitude and frequency of vibrations at the same time, to create a variety of tactile waveforms -this is not possible with any other actuator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Electromechanical properties</head><p>This section discusses the TouchEngine's electromechanical properties. Note that this discussion applies to any piezoceramic bending motor actuators. The relation between the signal voltage, the actuator displacement, and the force is linear (equation 1). We, how-ever, are more concerned with the actuator dynamics, particularly how fast an actuator can reach the required displacement: The greater the acceleration of the actuator, the stronger the impulse force becomes (see equation 4) and the sharper the tactile impulses can be detected by the user.</p><p>The actuator latency will depend on its electrical properties. Electrically, the piezoceramic actuator behaves as a capacitor. The typical capacitances for TouchEngine are ~3µF for a 7-layer and ~4µF for an 11-layer actuators. Consequently, the voltage across the actuator-and the displacementwould change according to the capacitor charging and discharging waveforms (Figure <ref type="figure" target="#fig_2">3</ref>). Because a capacitor reaches over 99% of it is charge in 5RC seconds, <ref type="bibr" target="#b21">[22]</ref> we can estimate the latency of an ideal actuator as:</p><formula xml:id="formula_1">τ = 5RC. (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where C is the actuator capacitance and R is the serial resistance <ref type="bibr" target="#b21">[22]</ref>. Hence, the latency of an actuator is constant and does not depend on the voltage; an actuator would reach any target displacement in the same amount of time.</p><p>For a real actuator internal resistance and inductance, material dumping, physical mounting, and limitations in the current supply would cause further delay in the displacement. Figure <ref type="figure" target="#fig_3">4</ref> shows an actual mechanical response of an 11-layer actuator measured using a laser displacement sensor. Latency of approximately 5ms can be clearly seen, which is still near optimal latency for the tactile actuator.</p><p>Increasing latency by increasing the capacitance and serial resistance can produce a softer and gentler feeling, which may be necessary in some applications. Current requirement is another very important issue because current supply significantly affects actuator response time, while in mobile devices current supply is limited. Current flows through the piezo actuator only when it bends. The actuator then stays in the bended state without draining any additional current. The current equation is:</p><formula xml:id="formula_3">dt dV C i = , (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>where C is the capacitance and V is the signal. The current is a sinusoid for a sinusoidal signal and a narrow spike for a square wave <ref type="bibr" target="#b21">[22]</ref>. This peak current can be fairly high: up to 450 mA for a 20V peak-to-peak signal. If the battery   cannot supply the maximum required current the actuator latency will increase, e.g. by integrating equation (3):</p><formula xml:id="formula_5">+ -</formula><formula xml:id="formula_6">t i C t v ⋅ = -1 ) (</formula><p>where t is time. Therefore, in case of limited current supply the actuator voltage (and displacement) changes not as a capacitive curve on Figure <ref type="figure" target="#fig_2">3</ref>, but as a linear function of time: more current can be supplied, the faster the actuator bends. Thus, current amplifiers are essential for achieving low latency. We have found that 250 mA unity-gain buffers were sufficient to achieve a latency of 5ms. While the peak current may be large, the average current is low and dependant on the signal frequency and maximum voltage. We have found that actuator require approximately 3 mW of power for a single 20V peak-to-peak square pulse, which can be easily achieved in handheld devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TouchEngine display</head><p>The actuator that we designed bends from the signal applied. The next step is to convert this mechanical motion into a detectable force. The major challenge in doing this is the very small total displacement of the actuator, less then 0.1 mm. Two strategies have been developed to utilize our actuator: direct tactile display and indirect tactile display. In direct tactile display the actuator moves a part of the device, e.g. a single button or an entire PDA screen. The basic interaction metaphor is direct touch: when the user touches a part augmented with tactile feedback, various tactile patterns communicate back. Different elements of the device can be augmented with distinct tactile feedback, however, it may be difficult and expensive to retrofit all the elements of a device with its own tactile actuators. Indirect tactile display is based on the tool metaphor: When we hit a nail with a hammer, the force is not communicated directly to our hand; we feel the effect of the impact through the handle. To achieve a similar effect, the actuator is placed anywhere within the device with a small weight attached to it (Figure <ref type="figure" target="#fig_1">2</ref>, right). Force is generated using conservation of momentum; for an isolated system with no external force, its total momentum is zero. Thus, when the actuator bends, the mass moves up or down with momentum p a and the entire device moves with equal momentum p d in the opposite direction. The force F felt by the user is the time derivative of the momentum; therefore, higher acceleration of the actuator results in a stronger force: (4)</p><p>Here, the entire handheld device acts as a tactile display.</p><p>Because our actuator moves very fast, we can create very sharp, distinct force impulses. Also, the actuator can be attached anywhere inside the device making construction of a tactile display easy, even for very small devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementing a mobile tactile display</head><p>We embedded a TouchEngine tactile display into a Sony Clié PEG-N700C Palm OS based PDA. A custom board that controls the actuator is inserted into the PDA (Figure <ref type="figure" target="#fig_4">5</ref>). We freed the space inside the PDA by removing the Memory Stick slot, Jog Dial, and pen compartment.</p><p>A basic diagram of the board is presented in Figure <ref type="figure" target="#fig_6">6</ref>. An Atmel AVR 8-bit RISC microprocessor creates different waveforms and sends them to the actuator using a 12-bit digital-to-analog converter (DAC). The power regulator (a DC-DC converter) is then used to boost the voltage from the 3.2V of the PDA battery to r12V, and the current is further amplified using 250 mA unity-gain buffers. Caution must be taken to prevent amplifier oscillations, which often occurs when driving large capacitive loads. Applications running on the Clié's Palm OS communicate with the AVR chip via a serial port. We can easily reprogram the microprocessor without removing the board from the PDA using a programming socket, so various tactile feedback applications can be prototyped on the same platform. The board can be optimized further, such as using power supplies that are much smaller and more efficient. Nevertheless, we are very satisfied with this platform as it fits entirely within the PDA, invisible to the user; provides high quality tactile feedback; and is easy to use for prototyping and evaluating mobile tactile interfaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MOBILE TACTILE INTERFACES</head><p>This section analyzes the design space of applications for mobile tactile interfaces. We do not attempt to suggest the ultimate taxonomy of mobile tactile interfaces: there are, perhaps, more applications than we can imagine at this point; other taxonomies can also be used (e.g. <ref type="bibr" target="#b30">[31]</ref>). Instead, this taxonomy is only meant to be a rough outline that can guide designers and developers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ambient displays and touch</head><p>In our analysis of tactile mobile interfaces, we were inspired by previous research on ambient and peripheral awareness interfaces <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b36">37]</ref>. They attempt to utilize human ability to receive and process information on the periphery of attention, without shifting focus from the primary task. For example, it is relatively easy for someone to note that a meeting is about to start in the next office or that the weather is about to change <ref type="bibr" target="#b9">[10]</ref>.</p><p>The premise of ambient or peripheral awareness displays is that they would first, allow users to perceive and process incoming information with little conscious effort and without need to interrupt current activities. Second, they would provide unobtrusive notification techniques to shift the user's attention when the interruption is needed. A number of ambient displays has been explored <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b36">37]</ref>, however, all of them relied on visual or aural channels to communicate information to users. We believe that touch is the perfect ambient information channel, in many cases, more powerful then visuals or sound. We already heavily rely on the tactile sense to receive information in the background: we feel the ground with our feet and unconsciously adjust how we walk when we step from the pavement to the grassy lawn; the movement from clothes informs us of changes in the wind direction; and slight vibrations of a pen help to regulate the amount of pressure applied to the paper. Tactile feedback is an extraordinary attention management technique. We may not hear someone calling our name, but we certainly react when someone touches our shoulder. Moreover, because of the reasons discussed in the introduction, most of this is done with little focused attention, almost unconsciously. Ambient tactile interfaces may be particularly useful in mobile applications. Indeed, as mobile users are often preoccupied with real world tasks; tactile displays that are always close to the body allow information to be received without interrupting the current activity to look at the visual display <ref type="bibr" target="#b36">[37]</ref>. Even when a user is focused on a mobile device, constant distractions and noise do not allow constant attention on mobile devices; hence, touch may substitute vision for short distractions. Finally, due to superior temporal processing abilities, we may be able to transfer some of the information processing from the eye to the finger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Design space of mobile tactile interfaces</head><p>We have organized the design space of mobile tactile interfaces along two dimensions (Figure <ref type="figure" target="#fig_7">7</ref>). The first dimension is the amount of cognitive load the interface imposes on the user. A large cognitive load requires more focused attention on the mobile interface at the expense of other tasks. The second dimension is the level of abstractness of information representation in tactile interfaces. The idea of abstract representation in ambient interfaces was first formulated by Pederson, et al. in the AROMA system <ref type="bibr" target="#b33">[34]</ref>. They proposed that a signal should be simplified to retain only the most significant bits of information and then re-mapped into a different abstract representation suitable for a particular ambient display and application. This idea provides an approach to tackle an important problem in ambient interfaces: how to communicate "enough" information to make the display useful, while at the same time, produce low cognitive load and create easy to use interfaces.</p><p>The lowest amount of abstraction in tactile displays is when we directly simulate the tactile feeling of real-world objects. This was investigated in VR and telepresence applications <ref type="bibr" target="#b7">[8]</ref>. A tactile interface in this case does not usually require much additional attention from the user. The exception is when the user has to perform precise identification or control only through touch, e.g. blind control.</p><p>A tactile signal can be abstracted to communicate only particular tactile properties, e.g. button presses can be simplified to a simple sinusoidal pulse to create the button "click" feeling. The entire simulation of the button physics may not be needed <ref type="bibr" target="#b15">[16]</ref>. In this case the tactile signal is in a sense a metaphor of the button; other tactile metaphors can be designed to communicate various feelings, e.g. scratching, tapping, breaking, and so on. By associating these tactile signals with interface events we can communicate the state of the mobile interface through touch. We can further increase abstractness of the tactile signal by using vibrotactile languages, such as Vibrotese, that do not resemble any realistic tactile events but can carry a complex message <ref type="bibr" target="#b37">[38]</ref>. Using them, however, drastically increases cognitive load.</p><p>Here, we are interested in the left top corner of the design space shown in Figure <ref type="figure" target="#fig_7">7</ref> that we refer to as Ambient Touch.</p><p>We investigate some of its applications in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AMBIENT TOUCH INTERFACES</head><p>This section describes several application scenarios to evaluate mobile tactile displays. Although the TouchEngine can produce a great variety of vibrotactile patterns, we used only simple square waveforms of different frequencies and intensities (e.g. Figure <ref type="figure" target="#fig_2">3</ref>). We still lack the in-depth understanding of how we can design vibration patterns to evoke specific user feelings; hence, for the initial investigation, we have chosen very simple patterns in which the feeling can be easily predicted and used in interface design.</p><p>We conducted a formal user study for one of the applications: tactile feedback for tilting interfaces. With the remaining applications, we informally tested them using our colleagues as subjects: the goal was not to collect data but evaluate the feasibility of our ideas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Notification through touch</head><p>Touch is an excellent "break-in" sense and has been used to alert people since the 1960s <ref type="bibr" target="#b18">[19]</ref>. Today, vibrotactile displays are commonly utilized for user notification in mobile phones and, recently, in handheld computers. The common problem, however, is that current tactile displays convey only a single bit of information: the simple occurrence of an event, e.g. a phone call. Because no related information is provided, such as who is calling or call urgency, the user is forced to interrupt their current activity, interact with the device, access the relevant information, and make the fol- low-up decision. This can be frustrating and dangerous, e.g. when the user checks a caller while driving. To address this problem, intelligent context-aware and ambient notification mechanisms that use sound have been proposed (e.g. <ref type="bibr" target="#b36">[37]</ref>). Tactile notification, however, is preferable when sound cannot be used, such as during meetings and in noisy environments. Tactile feedback has not been used for intelligent notification simply because no mobile tactile display was available that could produce complex vibration patterns in a short temporal event that can be remembered and recognized. We were encouraged by the studies on tactile communication <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b37">38]</ref> reporting that humans can learn to accurately distinguish different vibrotactile patterns. To experiment with tactile notification, we created six tactile pulse sequences with a different rhythm, intensity, and gradient of vibration. We used the drum as a metaphor to help users remember the patterns: simple rhythms were tapped and subjects were asked to repeat them and compare them with the same patterns reproduced on a mobile device. After a brief training period, we asked subjects to recognize the vibration patterns when the device was placed in the pocket; most subjects were able to identify them easily. We would like to further investigate tactile notification mechanisms combining them with ideas from contextual messaging, e.g. <ref type="bibr" target="#b36">[37]</ref>. We envision interfaces, where users can feel and recognize incoming information without interrupting their activity to interact directly with the device.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tactile monitoring</head><p>Notification techniques alert users about incoming events. A similar task is monitoring the status of a process, such as downloading large files or copying data to an external storage. The main difference from notification concepts is that here the user continuously monitors the time before process completion, not a single event. When a process is nearly complete, the user can prepare for the interruption in current activities, such as step aside from the sidewalk to access the mobile device. With desktop computers, this is easy to implement: the user can visually monitor the status of a process. For mobile users, visual monitoring would require periodical interruption of their current activity to access their device and check on the status, which can be frustrating and inefficient. We designed a tactile progress bar, to tactually inform the user about the process status. The first variation of the tactile progress bar simply provided the user with a short tactile impulse every 0.5 second. The intensity of the click increased as the process gets closer to completion. The evaluation revealed, however, that this design had no significant difference from the notification, because it only provided binary information on whether the process has finished or not. Absolute levels of intensity were not informative as humans have poor capabilities to judge them. We then re-designed this technique to take advantage of touch temporal discrimination abilities <ref type="bibr" target="#b18">[19]</ref>: the current process status was mapped into the time between the two clicks. As the process progresses the time between the two clicks decreases. This modification was well received by users, as it was easy to relate the tactile feedback to the current status of the process. Certainly, more study is needed to further investigate this interaction technique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tactile feedback for gestural mobile interfaces</head><p>Using gestures to interact with mobile devices have been an active research area <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b35">36]</ref>. Devices are augmented with sensors, such as tilt or pressure sensors, and users interact with them by physically manipulating the devices. For example, in tilting interfaces <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b35">36]</ref> the user can scroll through data by tilting the handheld device back and forth, in which the tilting angle controls the scrolling speed. This technique is intuitive and allows for one-handed use. Our experience with tilting interfaces, however, was not entirely satisfactory. First of all, we found that users would often overshoot the target destination, i.e. when the target is reached, users could not return the device to its neutral state in time to stop scrolling. Moreover, users often overshoot in the opposite direction, e.g. while trying to stop scrolling, the user would miss the neutral position and begin tilting in the opposite direction. This makes precise selection control quite difficult ( <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b24">25]</ref> reported similar problems). We also found that slower speeds would reduce the overshoot, but increase the time to complete tasks. Second, tilting interfaces require constant visual attention. Even a momentary distraction may result in a loss of control, leading to unpredictable results: e.g. an imperceptible twist of the hand while a user checks the name of a passing station might change the entire screen content without the user noticing. We believed that tilting interfaces could be improved by combining gesture control with tactile feedback, which would allow the user to "feel" how the information moves inside the device. It would allow the user to: 1) maintain continuous awareness of the interface state during brief interruptions to perform other tasks; 2) provide rapid feedback for tilting operations; because touch is faster then vision, we believe that it allows users to more rapidly readjust their hand to reduce overshoot than only relying on vision; 3) communicate interface states, such as reaching neutral orientation or boundaries of data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation of tilting interfaces</head><p>We used an ADXL202 two-degree accelerometer connected to unused input ports on the AVR microprocessor to measure the tilt of the Clié PDA. The sensor was set to operate at the lowest bandwidth (5Hz) to minimize noise and jitters. We sampled tilt at approximately 40 Hz. The user activates the tilt mode by pressing the back button on the left side of the PDA (Figure <ref type="figure" target="#fig_9">8</ref>). Scrolling begins when the device tilts more then 5 degrees from the initial device orientation, e.g. when the button is first pressed. Two speeds follow: data scrolls once every 500 ms when the tilt angle is between 5 and 10 degrees, and once every 250 ms when the device tilts more then 10 degrees. Scrolling was carefully synchronized with tactile feedback taking into account the time to update the visual display.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tactile applications</head><p>Two tilting applications were investigated: 2D browsing of graphical data. Figure <ref type="figure" target="#fig_9">8</ref> presents an example of browsing a Tokyo subway map. The map scrolls in the direction of the tilt where the yellow arrow indicates the direction of scrolling; The arrow increases in length as speed increases. Every time the image moves on the screen, a simple scratching tactile pattern plays, allowing the user to feel the map shifting within the device. The tactile pattern changes to a single low-intensity pulse when the image reaches its boundaries. We received positive feedback through an informal evaluation as users felt that tactile feedback made interaction easier and more enjoyable. 1D scrolling through text lists. A simple tactile "tap" occurs in the device every time one line of text is scrolled up or down. As the scrolling speed changes, the users would feel the tapping in their hand become faster or slower. The interface also produces a short buzz when the user returns the device to its initial position, reducing the overshoot when the user attempt to stop scrolling. We evaluated the text list scrolling interface in a controlled experimental study discussed in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EXPERIMENTAL STUDY</head><p>An experimental usability evaluation was conducted to investigate whether adding tactile feedback would affect user performance in tilting tasks, and if it does, how significantly. The following hypotheses were formulated prior to the experiments: H1: Tactile feedback will result in faster task completion in one-dimensional text scrolling tasks. H2: Tactile feedback will reduce overshot of target in onedimensional text scrolling tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subjects and apparatus</head><p>Ten unpaid subjects, all male, all right-handed, between 19 and 35 years old, were recruited from the laboratory subjects' pool. The experiments were conducted using a Sony Clié Palm OS 4.1 PDA running in color 160x160 resolution mode equipped with a TouchEngine feedback device and tilt sensor board described above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental task</head><p>The experimental task required subjects to scroll a text list from the start to target line, where each line is numbered (Figure <ref type="figure" target="#fig_8">9</ref>). At the start of a trial, the list is reset to the start line and the target line number is indicated at the top of the screen. The current line is underlined. To begin, the user presses the trigger button and tilts the device. Note that only the text scrolls, the position of the selection bar does not move -it remains in the middle of the screen. The task is completed when the user selects the target line and releases the button. After a delay, the next trial is presented and so on, until all trials are completed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment design and procedure</head><p>The repeated-measures within-subject experimental design was used. The independent variables are tactile feedback and distance of scrolling, defined as the number of lines from the start to the target lines with three levels: short (1 line), medium (6 lines) and large (12 lines). The start and target lines were never placed at the beginning or the end of the list, as these conditions would be easier to complete, thus, confounding experimental results. The dependent variables are completion time, i.e. the time from the moment the user presses the trigger until target selection, and overshoot, defined as the difference between the required and actual scrolled distances. The experiment began with an explanation of the techniques, experimental tasks, and procedure, followed by a 5-10 minute training session. The user was then asked to operate the device with the left hand, as the left hand is usually used to operate mobile devices. The training was followed by an experiment consisting of six sessions, three with and three without tactile feedback. Each session consisted of seven trials: two trials for each of the three distances randomized and one warm-up trial at the beginning of the session, where its data was not used in the final analysis. To control for order effect, half of the subjects started with tactile feedback while the other half started without. The TouchEngine produced sound; therefore, subjects were required to listen to music with headphones during the experiment. We believe that this closely approximates to the actual conditions of mobile device use.  After the experiment, a questionnaire was administered, in which we asked each subjects to rate the techniques on a scale from 1 to 5 (1 = very bad, 2 = bad, 3 = OK, 4 = good, and 5 = excellent) and explain their choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>A repeated-measures two-way analysis of variance (ANOVA) was performed for each of the dependent variables with tactile feedback and distance as independent variables. Averaged across conditions, tactile feedback resulted in 22% faster task completion (2.9 sec with and 3.7 without tactile feedback), which was also statistically significant F 1,9 = 15.6, p &lt; 0.003. Analyzing for each distance (Figure <ref type="figure" target="#fig_10">10</ref>) revealed that tactile feedback resulted in a 27% improvement for one-line scrolling, 26% for long distance, while for medium distance, it was only an 11% improvement 2 . The largest absolute improvement was for longdistance scrolling (1.3 second) while the smallest absolute improvement was for the medium distance (0.4 seconds, not statistically significant F 1,9 = 0.3, p &lt; 0.6). Tactile feedback was weakly significant for the overshoot distance F 1, 9 = 6.4, p &lt; 0.03: on average, it was 3.1 lines without and 2.2 lines with tactile feedback. Distance was statistically significant for both time and overshoot. User ratings were 3.9, e.g. almost good, with tactile feedback and 2.5, e.g. between bad and OK, without it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>The experimental results supported hypothesis H1: tactile feedback resulted in 22% faster task completion for tilting interfaces. The effect of tactile feedback was strongest for short and long distances. Based on our observations and user comments we found that exact positioning, e.g. to scroll to the next line, was difficult. One reason might be that the threshold angle was too small and the initial scrolling speed was too fast, hence subjects could not stop quickly enough when the list started scrolling, resulting in an overshoot. Moreover, because no tactile feedback was provided before the scrolling started, users faced difficulties in planning their movements in advance. In this situation, tactile feedback was helpful as it allowed the user to react 22% faster then with only visual feedback. 2 Conditions with no tactile feedback were taken as 100% For long distance scrolling, subjects would switch to the fastest scrolling speed in which touch was more effective as a feedback channel then vision, as it allowed users to "feel" the scrolled amount. On the medium distance, however, the user scrolled at a comfortable medium speed, which allowed precise selection, relying only on vision; hence tactile feedback did not provide significant benefits. Tactile feedback was also statistically significant for the overshoot, thus supporting hypothesis H2. However, it was not numerically significant: tactile feedback improved user performance by less then a line. One explanation might be that overshoot does not affect user performance in terms of distance, but in terms of the time that it takes the user to recover from an overshoot, re-adjust his or her hand, and correct the problem. Tactile feedback, in this case, allowed for more confident, rapid adjustment, and recovery. The ratings and comments provided by the subjects indicate that they prefer to have tactile feedback in the device. Interestingly, half of the users preferred tactile feedback not because of functional benefits, such as better coordination or faster selection, but because of a better user experience. Some suggested that emotionally it feels more comfortable and familiar. On the question of what was the most difficult element of the interface, all subjects noted that selecting the next line was the most difficult and frustrating task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CONCLUSIONS</head><p>We believe that tactile feedback can become an important feature of interfaces for future mobile devices by providing more effective, comfortable, and enjoyable interaction. Our experiments demonstrated that introducing tactile feedback resulted in a 22% improvement on user performance. This paper has only begun mapping out the design space of tactile displays for mobile devices, an area that has yet to be investigated. In our future work, we will continue our investigation of the tactile design space and search for new applications of mobile tactile feedback.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AKNOWLEDGEMENT</head><p>We are deeply indebted to Shigeru Tajima for his infinite patience while helping us with the TouchEngine circuit design, to Henry Newton-Dunn for thought-provocative and enlightening discussions, Mimi Huang for making this paper readable, James Gibson for on-screen graphics, and Sidney Fels for interesting comments. We also thank all subjects who participated in the user study and reviewers for their valuable suggestions for improving this paper. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>DESIGNING A HANDHELD TACTILE DISPLAYDevelopment of any haptic display involves: 1) Choosing or developing a haptic actuator, a transducer that converts electrical signals into mechanical motion; electrical motors are typical actuators used in haptic research. 2) Designing a tactile display that converts mechanical motion produced by the actuator into force communicated to the user. The same actuator can lead to various haptic displays with very different interactive properties. Indeed, motors have been used in haptic displays ranging from exoskeletons to small vibrators in mobile phones. 3) Developing control hardware and software. Effective operation of haptic display requires in-depth understanding of its properties. This section describes the design of the TouchEngine-a new tactile technology that includes actuator, tactile display and control hardware and software that we have developed to overcome deficiencies of current tactile displays.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2</head><label>2</label><figDesc>Figure 2 Left: The bending motor: the top layers contract and the bottom expands bending the entire actuator; Middle: A microscopic view: 18 layers of piezo and 19 layers of electrode; Right: Indirect haptic display.</figDesc><graphic coords="3,278.05,563.45,91.03,123.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Black: input signal; grey: actuator voltage: a capacitive charging/discharging curve.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Black: input signal, grey: actuator displacement (its jaggy because of the limited sensor resolution).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Control board that drives Touch-Engine actuator.</figDesc><graphic coords="4,424.11,540.33,127.27,132.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Basic diagram of the driving board.</figDesc><graphic coords="5,142.60,167.24,75.48,58.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Design space of mobile tactile interfaces</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Screen snapshot of the task at the start of a trial: the user is required to scroll from the starting position, for example, from line 5 to the target, line 7.</figDesc><graphic coords="8,101.62,300.64,155.64,155.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8</head><label>8</label><figDesc>Figure 8 Tilting interfaces: 2D browsing of a Tokyo subway map. Tilt sensor board is attached on the back.</figDesc><graphic coords="8,62.62,67.00,236.04,179.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Experimental results</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>In a striking example of this integration, it has been shown that cat walking may be entirely controlled within its spinal cord<ref type="bibr" target="#b8">[9]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>Volume 4, Issue 2</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Design innovations yearbook</title>
	</analytic>
	<monogr>
		<title level="j">Design-Zentrum Nordrhein Westfallen</title>
		<imprint>
			<date type="published" when="2000">2000. 2000</date>
			<publisher>Verlagpp</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A multi-modal mouse with tactile and force feedback</title>
		<author>
			<persName><forename type="first">M</forename><surname>Akamatsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Human-Computer Studies</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="443" to="453" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The role of compliant fingerpads in grasping and manipulation: Identification and control</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Annaswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Essays on Mathematical Robotics: The IMA Volumes in Mathematics and its Applications</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Baillieul</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Sastry</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Haptic perception of form: activity and stimulus attributes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Apelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The psychology of touch</title>
		<meeting><address><addrLine>Lawrence Erlbaum: Hillsdale, NJ</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="169" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rock&apos;n&apos;Scroll is here to stay</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bartlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics &amp; Applications</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="40" to="45" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">An evaluation of wearable information spaces. in VRAIS&apos;98</title>
		<author>
			<persName><forename type="first">M</forename><surname>Billinghurst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bowskill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Morphett</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="20" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Project GROPE-Haptic Displays for Scientific Visualization</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">P</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><surname>Jr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Ming-Ouh-Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Batter</surname></persName>
		</author>
		<author>
			<persName><surname>Killpatrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH&apos;90</title>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="177" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Force and touch feedback for virtual reality</title>
		<author>
			<persName><forename type="first">G</forename><surname>Burdea</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>John Wiley and Sons</publisher>
			<biblScope unit="volume">339</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Pen-based force display for precision manipulation in virtual environments. in VRAIS</title>
		<author>
			<persName><forename type="first">P</forename><surname>Buttolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hannaford</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="217" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Integrating the periphery and context: a new taxonomy of telematics. in Graphics Interfaces</title>
		<author>
			<persName><forename type="first">B</forename><surname>Buxton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="239" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sensory and physiological bases of touch</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cholewiak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The psychology of touch</title>
		<editor>
			<persName><forename type="first">W</forename><surname>Schiff</surname></persName>
		</editor>
		<imprint>
			<publisher>Lawrence Erlbaum</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="23" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A computer-controlled matrix system for presentation to skin of complex spatiotemporal pattern. Behavior Research Methods and Instrumentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cholewiak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sherrick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1981">1981</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="667" to="673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bricks: Laying the foundations for graspable user interfaces</title>
		<author>
			<persName><forename type="first">G</forename><surname>Fitzmaurice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Buxton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI&apos;95</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="442" to="449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Virtual reality for palmtop computers</title>
		<author>
			<persName><forename type="first">G</forename><surname>Fitzmaurice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chignell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="197" to="218" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Situated information spaces and spatially aware palmtop computers. Communication of the ACM</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Fitzmaurice</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="38" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ActiveClick: Tactile Feedback for Touch Panels</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fukumoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Toshiaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI</title>
		<title level="s">Extended Abstracts</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="page" from="121" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Progress in experiments on tactual interpretation of oral speech</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Abnormal and Social Psychology</title>
		<imprint>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="155" to="159" />
			<date type="published" when="1925">1925</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adventures in tactile literacy</title>
		<author>
			<persName><forename type="first">F</forename><surname>Geldard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Psychologist</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="115" to="124" />
			<date type="published" when="1957">1957</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Some neglected possibilities for communication</title>
		<author>
			<persName><forename type="first">F</forename><surname>Geldard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">131</biblScope>
			<biblScope unit="issue">3413</biblScope>
			<biblScope unit="page" from="1583" to="1588" />
			<date type="published" when="1960">1960</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Design of the wearbale tactile display</title>
		<author>
			<persName><forename type="first">F</forename><surname>Gemperle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Siewiorek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISWC</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The effects of a surround on the vibrotactile thresholds</title>
		<author>
			<persName><forename type="first">G</forename><surname>Gescheider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensory Process</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="115" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Basic electronics</title>
		<author>
			<persName><forename type="first">B</forename><surname>Grob</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1010">1997. 1010</date>
			<publisher>McGrow-Hill</publisher>
		</imprint>
	</monogr>
	<note>8 ed.</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Squeeze me, hold me, tilt me! An exploration of manipulative user interfaces</title>
		<author>
			<persName><forename type="first">B</forename><surname>Harrison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI&apos;98</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Passive real-world interface props for neurosurgical visualization. in CHI&apos;94</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hinckley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pausch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gobble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kassell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="452" to="458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Hinckley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sinclair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Horvitz</surname></persName>
		</author>
		<title level="m">Sensing Techniques for Mobile Interaction. in UIST&apos;2000</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="91" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Perceptualisation using a tactile mouse. in Visualization&apos;96</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Forrest</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="181" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Ullmer Tangible bits towards seamless interfaces between people, bits and atoms</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI97</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Electrocutaneous display as an interface to a virtual tactile world. in VR</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kajimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tachi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="289" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Making place&quot; to make IT work: empirical explorations of HCI for mobile CSCW</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kristoffersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ljungberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGROUP&apos;99</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="276" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A Comparison of three selection techniques for touchpads</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mackenzie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oniszczak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI&apos;98</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="336" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The design of 3D haptic widgets</title>
		<author>
			<persName><forename type="first">T</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zeleznik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Interactive 3D Graphics</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="97" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Feeling and seeing: issues in force display</title>
		<author>
			<persName><forename type="first">M</forename><surname>Minsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="235" to="241" />
		</imprint>
	</monogr>
	<note>in I3D.</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Using while moving: HCI issues in fieldwork environments</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pascoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Morse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Computer-Human Interaction</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="417" to="437" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">AROMA: abstract representation of presence supporting mutual awareness</title>
		<author>
			<persName><forename type="first">E</forename><surname>Pedersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sokoler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="51" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Developing a generic augmentedreality interface</title>
		<author>
			<persName><forename type="first">I</forename><surname>Poupyrev</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>IEEE Computer</publisher>
			<biblScope unit="page" from="44" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Tilting operations for small screen interfaces</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rekimoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UIST&apos;96</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="167" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Sawhney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmandt</surname></persName>
		</author>
		<title level="m">Nomadic Radio: Scalable and contextual notification for wearable audio messaging. in CHI&apos;99</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="96" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Vibrotactile pattern perception: some findings and applications</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sherrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Psychology of Touch, M. Heller and W. Schiff</title>
		<imprint>
			<publisher>Lawrence Erlbaum Associates</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="189" to="217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Tactual displays for wearable computing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISWC97</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="84" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Piezoelectric Actuators and Ultrasonic Motors</title>
		<author>
			<persName><forename type="first">K</forename><surname>Uchino</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Kluwer Academic Publishers</publisher>
			<biblScope unit="page">349</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
