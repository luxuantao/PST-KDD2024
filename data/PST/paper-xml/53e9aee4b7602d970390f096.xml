<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Building a Distributed Full-Text Index for the Web</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sergey</forename><surname>Melnik</surname></persName>
							<email>melnik@db.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University Stanford</orgName>
								<address>
									<postCode>94305</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sriram</forename><surname>Raghavan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University Stanford</orgName>
								<address>
									<postCode>94305</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Beverly</forename><surname>Yang</surname></persName>
							<email>byang@db.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University Stanford</orgName>
								<address>
									<postCode>94305</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hector</forename><surname>Garcia-Molina</surname></persName>
							<email>hector@db.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University Stanford</orgName>
								<address>
									<postCode>94305</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Building a Distributed Full-Text Index for the Web</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5F8912002450F845E6860004F75AD218</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3.1 [Information Systems]: Content Analysis and Indexing-Indexing; H.3.4 [Information Systems]: Systems and Software-Distributed systems Distributed indexing</term>
					<term>Text retrieval</term>
					<term>Inverted files</term>
					<term>Pipelining</term>
					<term>Embedded databases</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We identify crucial design issues in building a distributed inverted index for a large collection of Web pages. We introduce a novel pipelining technique for structuring the core index-building system that substantially reduces the index construction time. We also propose a storage scheme for creating and managing inverted files using an embedded database system. We suggest and compare different strategies for collecting global statistics from distributed inverted indexes. Finally, we present performance results from experiments on a testbed distributed indexing system that we have implemented.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Various access methods have been developed to support efficient search and retrieval over text document collections. Examples include suffix arrays <ref type="bibr" target="#b11">[11]</ref>, inverted files or inverted indexes <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b24">25]</ref>, and signature files <ref type="bibr" target="#b4">[4]</ref>. Inverted files have traditionally been the index structure of choice on the Web. Commercial search engines use custom network architectures and high-performance hardware to achieve sub-second query response times using such inverted indexes. <ref type="foot" target="#foot_0">1</ref>An inverted index over a collection of Web pages consists of a set of inverted lists, one for each occurring word (or index term). The inverted list for a term is a sorted list of locations where the term appears in the collection. A location consists of a page identifier and the position of the term within the page. When it is not necessary to track each term occurrence within a page, a location will include just a page identifier (and optionally the number of occurrences within the page). Given an index term w, and a corresponding location l, we refer to the pair (w, l) as a posting for w.</p><p>Conceptually, building an inverted index involves processing each page to extract postings, sorting the postings first on index terms and then on locations, and finally writing out the sorted postings as a collection of inverted lists on disk. When the collection is small and indexing is a rare activity, optimizing index-building is not as critical as optimizing run-time query processing and retrieval. However, with a Web-scale index, index build time also becomes a critical factor for two reasons:</p><p>Scale and growth rate. The Web is so large and growing so rapidly <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b23">24]</ref> that traditional build schemes become unmanageable, requiring huge resources and taking days to complete (and becoming more vulnerable to system failures). As a measure of comparison, the 40 million page (220 GB) WebBase repository <ref type="bibr">[8]</ref> represents only about 4% of the estimated size of the publicly indexable Web as of January 2000 <ref type="bibr" target="#b23">[24]</ref>, but is already larger than the 100 GB very large TREC-7 collection <ref type="bibr" target="#b7">[7]</ref>, the benchmark for large IR systems.</p><p>Rate of change. Since the content on the Web changes extremely rapidly <ref type="bibr" target="#b3">[3]</ref>, there is a need to periodically crawl the Web and update the inverted index. Indexes can either be updated incrementally or periodically rebuilt, after every crawl. With both approaches, the key challenge is to handle the large whole-scale changes commonly observed between successive crawls of the Web. For efficiency and simplicity, most commercial Web search systems employ the rebuilding approach <ref type="bibr">[14]</ref>. In this case, it is critical to build the index rapidly to quickly provide access to the new data.</p><p>To study and evaluate index building in the context of the special challenges imposed by the Web, we have implemented a testbed system that operates on a cluster of nodes (workstations). As we built the testbed, we encountered several challenging problems that are typically not encountered when working with smaller collections. In this paper we report on some of these issues and the experiments we conducted to optimize build times for massive collections.</p><p>In particular:</p><p>• We propose the technique of constructing a software pipeline on each indexing node to enhance performance through intra-node parallelism (Section 3).</p><p>• We argue that the use of an embedded database system (such as The Berkeley Database <ref type="bibr" target="#b16">[17]</ref>) for storing inverted files has a number of important advantages. We propose an appropriate format for inverted files that makes optimal use of the features of such a database system (Section 4).</p><p>• Any distributed system for building inverted indexes needs to address the issue of collecting global statistics (e.g., inverse document frequency -IDF ). We examine different strategies for collecting such statistics from a distributed collection (Section 5).</p><p>• For each of the above issues, wherever appropriate, we present experiments and performance studies to compare the alternatives.</p><p>We emphasize that the focus of this paper is on the actual process of building an inverted index and not on using this index to process search queries. As a result, we do not address issues such as ranking functions, relevance feedback <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b24">25]</ref>, and distributed query processing <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b20">21]</ref>.</p><p>We also wish to clarify that the focus of this paper is not on presenting a comprehensive performance or feature-list comparison of our testbed indexing system with existing systems for indexing Web and non-Web collections. Rather, we use our experience with the testbed to identify some key performance issues in building a Web-scale index and propose generic techniques that are applicable to any distributed inverted index system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">TESTBED ARCHITECTURE</head><p>Our testbed system for building inverted indexes operates on a distributed shared-nothing architecture consisting of a collection of nodes connected by a local area network. We identify three types of nodes in the system (Figure <ref type="figure" target="#fig_0">1</ref>):</p><p>Distributors. These nodes store the collection of Web pages to be indexed. Pages are gathered by a Web crawler and stored in a repository distributed across the disks of these nodes <ref type="bibr">[8]</ref>.</p><p>Indexers. These nodes execute the core of the index building engine.</p><p>Query servers. Each of these nodes stores a portion of the final inverted index and an associated lexicon. The lexicon lists all the terms in the corresponding portion of the index and their associated statistics. Depending on the organization of the index files, some or all of the query servers may be involved in answering a search query.</p><p>Note that many traditional information retrieval (IR) systems do not employ such a 3-tier architecture for building inverted indexes. In those systems, the pages or documents to be indexed are placed on disks directly attached to the machines that build the index. However, a 3-tier architecture provides significant benefits in the context of a Web search service. Note that a Web search service must perform three resource intensive tasks -crawling, indexing, and querying -simultaneously. Even as existing indexes are used to answer search queries, newer indexes (based on a more recent crawl) must be constructed, and in parallel, the crawler must begin a fresh crawl. A 3-tier architecture clearly separates these three activities by executing them on separate banks of machines, thus improving performance. This ensures that pages are indexed and made available for querying as quickly as possible, thereby maximizing index freshness.</p><p>Overview of indexing process. The inverted index is built in two stages. In the first stage, each distributor node runs a distributor process that disseminates the collection of Web pages to the indexers. Each indexer receives a mutually disjoint subset of pages and their associated identifiers. The indexers parse and extract postings from the pages, sort the postings in memory, and flush them to intermediate structures on disk.</p><p>In the second stage, these intermediate structures are merged together to create one or more inverted files and their associated lexicons. An (inverted file, lexicon) pair is generated by merging a subset of the sorted runs. Each (inverted file, lexicon) pair is transferred to one or more query servers. In this paper, for simplicity, we assume that each indexer builds only one such pair.</p><p>Distributed inverted index organization. In a distributed environment, there are two basic strategies for distributing the inverted index over a collection of query servers <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21]</ref>. One strategy is to partition the document collection so that each query server is responsible for a disjoint subset of documents in the collection (called local inverted files in <ref type="bibr" target="#b17">[18]</ref>). The other option is to partition based on the index terms so that each query server stores inverted lists only for a subset of the index terms in the collection (called global inverted files in <ref type="bibr" target="#b17">[18]</ref>). Performance studies described in <ref type="bibr" target="#b20">[21]</ref> indicate that the local inverted file organization uses system resources effectively and provides good query throughput in most cases. Hence, our testbed employs the local inverted file organization.</p><p>Testbed environment. Our indexing testbed uses a large repository of Web pages provided by the WebBase project <ref type="bibr">[8]</ref> as the test corpus for the performance experiments. The storage manager of the WebBase system receives pages from the Web crawler <ref type="bibr" target="#b3">[3]</ref> and populates the distributor nodes. The indexers and the query servers are single processor PCs with 350-500 MHz processors, 300-500 MB of main memory, and equipped with multiple IDE disks. The dis-   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PIPELINED INDEXER DESIGN</head><p>The core of the indexing system is the index-builder process that executes on each indexer. The input to the indexbuilder process is a sequence of Web pages and their associated identifiers. <ref type="foot" target="#foot_1">2</ref> The output of the index-builder is a set of sorted runs. Each sorted run contains postings extracted from a subset of the pages received by the index-builder.</p><p>The process of generating these sorted runs can logically be split into three phases, as illustrated in Figure <ref type="figure" target="#fig_1">2</ref>. We refer to these phases as loading, processing, and flushing. During the loading phase, some number of pages are read from the input stream. The processing phase involves two steps. First, the pages are parsed to remove HTML tagging, tokenized into individual terms, and stored as a set of postings in a memory buffer. In the second step, the postings are sorted in-place, first by term, and then by location. During the flushing phase, the sorted postings in the memory buffer are saved on disk as a sorted run. These three phases are executed repeatedly until the entire input stream of pages has been consumed.</p><p>Loading, processing and flushing tend to use disjoint sets of system resources. Processing is obviously CPU-intensive, whereas flushing primarily exerts secondary storage, and loading can be done directly from the network, tape, or a separate disk. Therefore, indexing performance can be improved by executing these three phases concurrently. Since the execution order of loading, processing and flushing is fixed, these three phases together form a software pipeline.</p><p>Figure <ref type="figure" target="#fig_2">3</ref> illustrates the benefits of pipelined parallelism during index construction. The figure shows a portion of an indexing process that uses three concurrent threads, operates on three reusable memory buffers, and generates six sorted runs on disk.</p><p>The key issue in pipelining is to design an execution schedule for the different indexing phases that will result in minimal overall running time (also called makespan in the scheduling literature). Our problem differs from a typical job scheduling problem <ref type="bibr" target="#b2">[2]</ref> in that we can vary the sizes of the incoming jobs, i.e., in every loading phase we can choose the number of pages to load. In the rest of this section, we describe how we make effective use of this flexibility. First, we derive, under certain simplifying assumptions, the characteristics of an optimal indexing pipeline schedule and determine the theoretical speedup achievable through pipelining. Next, we describe experiments that illustrate how observed performance gains differ from the theoretical predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Theoretical Analysis</head><p>Let us consider an indexer node that has one resource of each type: a single CPU, a single disk, and a single network connection over which to receive the pages. How should we design the pipeline shown in Figure <ref type="figure" target="#fig_1">2</ref> to minimize index construction time?</p><p>First, notice that executing concurrent phases of the same kind, such as two disk flushes, is futile, since we have only one resource of each type. Consider an index-builder that uses N executions of the pipeline to process the entire collection of pages and generate N sorted runs. By an execution of the pipeline, we refer to the sequence of three phasesloading, processing, and flushing -that transform some set of pages into a sorted run. Let Bi, i = 1 . . . N , be the buffer sizes used during these N executions. The sum È N i=1 Bi = B total is fixed for a given amount of text input and represents the total size of all the postings extracted from the pages. Our aim is to come up with a way of choosing the Bi values so as to minimize the overall running time. Now, loading and flushing take time linear in the size of the buffer. Processing time has a linear component (representing time for removing HTML and tokenizing) and a linear-logarithmic component (representing sorting time). Let li = λBi, fi = ϕBi, and pi = δBi + σBi log Bi represent the durations of the loading, flushing, and processing phases for the i th execution of the pipeline. <ref type="foot" target="#foot_2">3</ref> For large N , the overall indexing time is determined by the scarcest resource (the CPU, in Figure <ref type="figure" target="#fig_2">3</ref>) and can be approximated by Tp = max{</p><formula xml:id="formula_0">È N i=1 li, È N i=1 pi, È N i=1 fi}.</formula><p>It can be shown (see <ref type="bibr" target="#b13">[13]</ref>) that Tp is minimized when all N pipeline executions use the same buffer size B, where B = B1 . . . = BN = B total N . Let l = λB, f = ϕB, and p = δB + σB log B be the durations of the loading, processing, and flushing phases, respectively. We must choose a value of B that maximizes the speedup gained through pipelining.</p><p>We calculate speedup as follows. Pipelined execution takes time Tp = N max(l, p, f ) (6p in Figure <ref type="figure" target="#fig_2">3</ref>) and uses 3 buffers, each of size B. In comparison, sequential execution using a single buffer of size 3B will take time </p><formula xml:id="formula_1">Ts = N 3 (l ′ + p ′ + f ′ ), where l ′ = λ(3B), f ′ = ϕ(3B), and p ′ = δ(3B) + σ(3B) log (3B).</formula><formula xml:id="formula_2">θ = Ts Tp = (l + p + f ) max(l, p, f ) + σ log 3 max(λ, ϕ, δ + σ log B) = θ1 + θ2</formula><p>Now, θ1 ≥ 1 whereas θ2 ≤ σ log 3 max(λ,ϕ) &lt;&lt; 1 for typical values of λ, ϕ, and σ (refer to Table <ref type="table" target="#tab_1">1</ref>). Therefore, we ignore θ2 and concentrate on choosing the value of B that maximizes θ1. The maximum value of θ1 is 3, which is reached when l = p = f , i.e., when all three phases are of equal duration. We cannot guarantee l = f since that requires λ = ϕ. However, we can maximize θ1 by choosing p = max(l, f ) so that θ1 = 2 + min(l,f ) max(l,f ) . For example, in Figure <ref type="figure" target="#fig_2">3</ref>, the ratio between the phases is l : p : f = 3 : 4 : 2. Thus, θ1 for this setting is 3+4+2 4 = 2.25. We could improve θ1 by changing the ratio to 3:3:2, so that θ1 = 2 + 2 3 ≈ 2.67. In general, setting δB + σB log B = max{λB, ϕB}, we obtain</p><formula xml:id="formula_3">B = 2 max{λ,ϕ}-δ σ<label>(1)</label></formula><p>This expression represents the size of the postings buffer that must be used to maximize the pipeline speedup, on an indexer with a single resource of each type. In <ref type="bibr" target="#b13">[13]</ref> we generalize equation 1 to handle indexers with multiple CPUs and disks. If we use a buffer of size less than the one specified by equation 1, loading or flushing (depending on the relative magnitudes of λ and ϕ) will be the bottleneck and the processing phase will be forced to periodically wait for the other phases to complete. An analogous effect will take place for buffer sizes greater than the one prescribed by equation 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Results</head><p>To study the impact of the pipelining technique on indexing performance, we conducted a number of experiments on our testbed, using a single indexer supplied with a stream of Web pages from a distributor.</p><p>We first ran the index-builder process in measurement mode, where we recorded the execution times of the various phases and determined the values of λ, ϕ, σ, and δ (Table <ref type="table" target="#tab_1">1</ref>). Using the values of these constants in equation 1, we evaluate B to be 16 MB. Therefore, the optimal total size of the postings buffers, as predicted by our theoretical analysis, is 3B = 48 MB.</p><p>Impact of buffer size on performance. Figure <ref type="figure" target="#fig_4">4</ref> illustrates how the performance of the index-builder process varies with the size of the buffer. It highlights the importance of the analytical result as an aid in choosing the right buffer size. The optimal total buffer size based on actual experiments turned out be 40 MB. Even though the predicted optimum size differs slightly from the observed optimum, the difference in running times between the two sizes is less than 15 minutes for a 5 million page collection. For buffer sizes  No pipelining (48MB total buffer size) Pipelining (48MB total buffer size)</p><p>Figure <ref type="figure">5</ref>: Performance gain through pipelining less than 40, loading proved to be the bottleneck, and both the processing and flushing phases had to wait periodically for the loading phase to complete. However, as the buffer size increased beyond 40, the processing phase dominated the execution time as larger and larger buffers of postings had to be sorted.</p><p>Performance gain through pipelining. Figure <ref type="figure">5</ref> shows how pipelining impacts the time taken to process and generate sorted runs for a variety of input sizes. Note that for small collections of pages, the performance gain through pipelining, though noticeable, is not substantial. This is because small collections require very few pipeline executions and the overall time is dominated by the time required at startup (to load up the buffers) and shutdown (to flush the buffers). This is one of the reasons that pipelined index building has not received prior attention as most systems dealt with smaller collections. However, as collection sizes increase, the gain becomes more significant and for a collection of 5 million pages, pipelining completes almost 1.5 hours earlier than a purely sequential implementation. Our experiments showed that, in general, for large collections, a sequential index-builder is about 30-40% slower than a pipelined index-builder. Note that the observed speedup is lower than the speedup predicted by the theoretical analysis described in the previous section. That analysis was based on an "ideal pipeline," in which loading, processing and flushing do not interfere with each other in any way. In practice, however, network and disk operations do use processor cycles and access main memory. Hence, any two concurrently running phases, even of different types, do slow down each other.</p><p>Note that for a given total buffer size, pipelined execution generates sorted runs that are approximately 3 times smaller than those generated by a sequential indexer. Consequently, 3 times as many sorted runs will need to be merged in the second stage of indexing. However, other experiments described in <ref type="bibr" target="#b13">[13]</ref> show that even for very large collection sizes, the potential increase in merging time is more than offset by the time gained in the first stage through pipelining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">MANAGING INVERTED FILES IN AN EMBEDDED DATABASE SYSTEM</head><p>When building inverted indexes over massive Web-scale collections, the choice of an efficient storage format is particularly important. There have traditionally been two approaches to storing and managing inverted files; either using a custom implementation or by leveraging existing relational or object data management systems <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b6">6]</ref>.</p><p>The advantage of a custom implementation is that it enables very effective optimizations tuned to the specific operations on inverted files (e.g., caching frequently used inverted lists, compressing rarely used inverted lists using expensive methods that may take longer to decompress). Leveraging existing data management systems does not allow such fine-grained control over the implementation but reduces development time and complexity. However, the challenge lies in designing a scheme for storing inverted files that makes optimal use of the storage structures provided by the data management system. The storage scheme must be space efficient and must ensure that the basic lookup operation on an inverted file (i.e., retrieving some or all of the inverted list for a given index term) can be efficiently implemented using the access methods of the data management system.</p><p>In this section we present and compare different storage schemes for managing large inverted files in an embedded database system. To test our schemes, we used a freely available embedded database system called Berkeley DB <ref type="bibr" target="#b16">[17]</ref>, that is widely deployed in many commercial applications.</p><p>An embedded database is a library or toolkit that provides database support for applications through a well-defined programming API. Unlike traditional database systems that are designed to be accessed by applications, embedded databases are linked (at compile-time or run-time) into an application and act as its persistent storage manager. They provide device-sensitive file allocation, database access methods (such as B-trees and hash indexes), and optimized caching, with optional support for transactions, locking, and recovery. They also have the advantage of much smaller footprints compared to full-fledged client-server database systems.</p><p>In the following, we briefly the sketch the capabilities of Berkeley DB and propose a B-tree based inverted file storage scheme called the mixed-list scheme. We qualitatively and quantitatively compare the mixed-list scheme with two other schemes for storing inverted lists in Berkeley DB databases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Rationale and Implementation</head><p>Berkeley DB provides a programming library for managing (key,value) pairs, both of which can be arbitrary binary data of any length. It offers four access methods, including B-trees and linear hashing, and supports transac- tions, locking, and recovery. <ref type="foot" target="#foot_3">4</ref> We chose to use the B-tree access method since it efficiently supports prefix searches (e.g., retrieve inverted lists for all terms beginning with "pre") and has higher reference locality than hash-based indexes.</p><p>The standard organization of a B-tree based inverted file involves storing the index terms in the B-tree along with pointers to inverted lists that are stored separately. Such an organization, though easy to implement using Berkeley DB, does not fully utilize the capabilities of the database system. Since Berkeley DB efficiently handles arbitrary sized keys and values, it is more efficient to store both the index terms and their inverted lists within the database. This enables us to leverage Berkeley DB's sophisticated caching schemes while retrieving large inverted lists with a minimum number of disk operations.</p><p>Storage schemes. The challenge is to design an efficient scheme for organizing the inverted lists within the B-tree structure. We considered three schemes:</p><p>1. Full list: The key is an index term, and the value is the complete inverted list for that term.</p><p>2. Single payload: Each posting (an index term, location pair) is a separate key. <ref type="foot" target="#foot_4">5</ref> The value can either be empty or may contain additional information about the posting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Mixed list:</head><p>The key is again a posting, i.e., an index term and a location. However, the value contains a number of successive postings in sorted order, even those referring to different index terms. The postings in the value field are compressed and in every value field, the number of postings is chosen so that the length of the field is approximately the same. Note that in this scheme, the inverted list for a given index term may be spread across multiple (key,value) pairs.</p><p>Figure <ref type="figure" target="#fig_5">6</ref> illustrates the mixed-list storage scheme. For simplicity, in this example, we assume that no additional information is maintained along with each posting. However, in our actual implementation, we allocated a 2-byte Scheme Index size Zig-zag joins Hot updates single payload -- A qualitative comparison of these storage schemes is summarized in Table <ref type="table" target="#tab_2">2</ref>.</p><formula xml:id="formula_4">+ + full list +- - - mixed list +- +- +-</formula><p>Index size. The crucial factors determining index size are the number of internal pages (a function of the height of the B-tree) and the number of overflow pages. <ref type="foot" target="#foot_5">6</ref> In the single payload scheme, every posting corresponds to a new key, resulting in rapid growth in the number of internal pages of the database. For large collections, the database size becomes prohibitive even though Berkeley DB employs prefix compression on keys. At query time, many performanceimpeding disk accesses are needed. The situation is significantly better with the full list scheme. A database key is created only for every distinct term, and the value field can be well compressed. However, many terms occur only a few times in the collection whereas others may occur in almost every page. Due to large variations in the size of the value field, many overflow pages are created in the database. In comparison, with the mixed list scheme, the length of the value field is approximately constant. This limits the number of overflow pages. Moreover, the total number of keys (and hence the number of internal pages) can be further reduced by choosing a larger size for the value field. However, since the value field can contain postings of different index terms, it is not compressed as well as with full lists.</p><p>Zig-zag joins. The ability to selectively retrieve portions of an inverted list can be very useful when processing conjunctive search queries on an inverted file. For example, consider the query green AND catchflies. The term green occurs on the Web in millions of documents, whereas catchflies produces only a couple of dozen hits. A zig-zag join <ref type="bibr" target="#b5">[5]</ref> between the inverted lists for green and catchflies allows us to answer the query without reading out the complete inverted list for green. The single payload scheme provides the best support for zig-zag joins as each posting can be retrieved individually. In the full list scheme, the entire list must be retrieved to compute the join, whereas with the mixed list scheme, access to specific portions of the inverted list is available. For example, in Figure <ref type="figure" target="#fig_5">6</ref>, to retrieve locations for cat starting at 311, we do not have to read the portion of the list for locations 100-280.</p><p>The skipped-list and random inverted-list structures of <ref type="bibr" target="#b14">[15]</ref> and <ref type="bibr" target="#b15">[16]</ref> also provide selective access to portions of an inverted list, by dividing the inverted list into blocks each containing a fixed number of postings. However, those schemes assume a custom inverted file implementation and are not built on top of an existing data management system.</p><p>Hot updates. Hot updates refers to the ability to modify the index at query time. This is useful when very small changes need to be made to the index between two successive index rebuilds. For example, Web search services often allow users and organizations to register their home pages with their service. Such additions can be immediately accommodated in the index using the hot update facility, without having to defer them till the index is next rebuilt.</p><p>In all three schemes, the concurrency control mechanisms of the database can be used to support such hot updates while maintaining consistency. However, the crucial performance factor is the length of the inverted list that must be read, modified, and written back to achieve the update. Since we limit the length of the value field, hot updates are faster with mixed lists than with full lists. The single payload scheme provides the best update performance as individual postings can be accessed and modified.</p><p>Notice that all three schemes significantly benefit from the fact that the postings are first sorted and then inserted. Inserting keys into the B-tree in a random order negatively affects the page-fill factor, and expensive tree reorganization is needed. Berkeley DB is optimized for sorted insertions so that high performance and a near-one page-fill factor can be achieved in the initial index construction phase.</p><p>Table <ref type="table" target="#tab_2">2</ref> shows that the mixed-list scheme provides the best balance between small index size and support for efficient zig-zag joins. In the following section, we present a quantitative comparison of storage and retrieval efficiency for the three storage schemes discussed in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Results</head><p>The experimental data presented in this section were obtained by building an inverted index over a collection of 2 million Web pages. The collection contains 4.9 million distinct terms with a total of 312 million postings. 7  Figure <ref type="figure">7</ref> illustrates how the choice of the storage scheme affects the size of the inverted file. It shows the variation of index size with value field size, when using the mixed-list scheme. The dotted line represents the index size when the same database was stored using the full-list scheme. Note that since the value field size is not applicable to the full-list scheme, the graph is just a horizontal line. The single payload scheme can be viewed as an extreme case of the mixed scheme with value field being empty. Figure <ref type="figure">7</ref> shows that both very small and very large value fields have an adverse impact on index size. In the mixed list scheme, very small value fields will require a large number of internal database pages (and a potentially taller B-tree index) to accommodate all the postings. On the other hand, very large value fields will cause Berkeley DB to allocate a large number of 7 Only one posting was generated for all the occurrences of a term in a page.   For all of the examined storage schemes, the time to write the inverted file to disk was roughly proportional to the size of the file. Table <ref type="table" target="#tab_3">3</ref> shows how the index size (using the mixed-list scheme) varies with the size of the input collection. The index sizes listed in Table <ref type="table" target="#tab_3">3</ref> include the sum of the sizes of the inverted files and the associated lexicons. The numbers for Table <ref type="table" target="#tab_3">3</ref> were generated by using mixed-lists with the optimal value field size of 512 bytes derived from Figure <ref type="figure">7</ref>. Table <ref type="table" target="#tab_3">3</ref> shows that the mixed-list storage scheme scales very well to large collections. The size of the index is consistently below 7% the size of the input HTML text. This compares favorably with the sizes reported for the VLC2 track (which also used crawled Web pages) at TREC-7 <ref type="bibr" target="#b7">[7]</ref> where the best reported index size was approximately 7.7% the size of the input HTML. Our index sizes are also comparable to other recently reported sizes for non-Web document collections using compressed inverted files <ref type="bibr" target="#b15">[16]</ref>. Note that exact index sizes are dependent on the type and amount of information maintained along with each posting (e.g., information to handle proximity queries). However, we believe that the 2-byte payload field used in our implementation can accommodate most posting-level information normally stored in inverted indexes.</p><p>Figure <ref type="figure" target="#fig_9">8</ref> illustrates the effect of value field size on inverted list retrieval time. Once again, the dotted horizontal line represents the retrieval time when using the fixed-list scheme. Figure <ref type="figure" target="#fig_9">8</ref> was produced by generating uniformly distributed query terms and measuring the time 8 required 8 A warming-up period was allowed before the measurements to fill the database and file system cache.  to retrieve the entire inverted list for each query term. The optimal retrieval performance in the mixed-list scheme is achieved when the value field size is between 512 and 1024 bytes. Notice that (from Figures <ref type="figure">7</ref> and<ref type="figure" target="#fig_9">8</ref>) a value field size of 512 bytes results in maximum storage as well as maximum retrieval efficiency for the mixed-list scheme. Figure <ref type="figure" target="#fig_9">8</ref> also indicates that both the fixed-list and mixed-list (with optimal value field size) schemes provide comparable retrieval performance.</p><p>Note that Figure <ref type="figure" target="#fig_9">8</ref> only measures the raw inverted list retrieval performance of the different storage schemes. True query processing performance will be affected by other factors such as caching (of inverted lists), use of query processing techniques such as zig-zag joins, and the distribution of the query terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">COLLECTING GLOBAL STATISTICS</head><p>Most text-based retrieval systems use some kind of collection-wide information to increase effectiveness of retrieval <ref type="bibr" target="#b22">[23]</ref>. One popular example is the inverse document frequency (IDF) statistics used in ranking functions. The IDF of a term is the inverse of the number of documents in the collection that contain that term. If query servers have only IDF values over their local collections, then rankings would be skewed in favor of pages from query servers that return few results.</p><p>Depending on the particular global statistic, the ranking function, and the nature of the collection, it may or may not be necessary for a statistic to be computed accurately. In some cases, it suffices to estimate the global statistic from the local values at the individual query servers. However, in this section, we analyze the problem of gathering accurate collection-wide information (with minimum overhead), for the cases where this is required. We present two techniques that are capable of gathering different types of collectionwide information, though here we focus on the problem of collecting term-level global statistics, such as IDF values.<ref type="foot" target="#foot_6">9</ref>  Our approach is based on using a dedicated server, known as the statistician, for computing statistics. Having a dedicated statistician allows most computation to be done in parallel with other indexing activities. It also minimizes the number of conversations among servers, since indexers exchange statistical data with only one statistician. Local information is sent to the statistician at various stages of index creation, and the statistician returns global statistics to the indexers in the merging phase. Indexers then store the global statistics in the local lexicons. A lexicon consists of entries of the form (term, term-id, local-statistics, globalstatistics), where the terms stored in a lexicon are only those terms occurring in the associated inverted file (Section 2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Design</head><p>In order to avoid extra disk I/O, local information is sent to the statistician only when it is already in memory. We have identified two phases in which this occurs: flushingwhen sorted runs are written to disk, and merging -when sorted runs are merged to form inverted lists and the lexicon. Sending information in these two phases leads to two different strategies with various tradeoffs which are discussed in the next section. We note here only that by sending information to the statistician in these phases without additional I/O's, a huge fraction of the statistic collection is eliminated.</p><p>Sending information to the statistician is further optimized by summarizing the postings. In both identified phases, postings occur in at least partially sorted order, i.e. multiple postings for a term pass through memory in groups. Groups are condensed into (term, local aggregated information) pairs which are sent to the statistician. For example, if an indexer holds 10,000 pages that contain the term "cat", instead of sending 10,000 individual postings to the statistician, the indexer can count the postings as they pass through memory in a group and send the summary ("cat", 10000) to the statistician. The statistician receives local counts from all indexers, and aggregates these values to produce the global document frequency for "cat". This technique greatly reduces network overhead in collecting statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Statistic Gathering Strategies</head><p>Here we describe and compare the two strategies mentioned above for sending information to the statistician. Table 4 summarizes their characteristics. The column titled "Parallelism," refers to the degree of parallelism possible within each strategy.</p><p>ME Strategy (sending local information during merging). Summaries for each term are aggregated as inverted lists are created in memory, and sent to the statistician. The statistician receives parallel sorted streams of (term, local-aggregate-information) values from each indexer and merges these streams by term, aggregating the subaggregates for each term to produce global statistics. The cat: (7,2) (9,1) dog:(9,3) rat: (6,1) ( Figure <ref type="figure" target="#fig_0">10</ref>: FL strategy statistics are then sent back to the indexers in sorted term order. This approach is entirely stream based, and does not require in-memory or on-disk data structures at the statistician or indexer to store intermediate results. However, using streams means that the progress of each indexer is synchronized with that of the statistician, which in turn causes indexers to be synchronized with each other. As a result, the slowest indexer in the group becomes the bottleneck, holding back the progress of faster indexers. Figure <ref type="figure">9</ref> illustrates the ME strategy for collecting document frequency statistics for each term. Note that the bottom lexicon does not include statistics for "rat" because the term is not present in the local collection.</p><p>FL Strategy (sending local information during flushing). As sorted runs are flushed to disk, postings are summarized and the summaries sent to the statistician. Since sorted runs are accessed sequentially during processing, the statistician receives streams of summaries in globally unsorted order. To compute statistics from the unsorted streams, the statistician keeps an in-memory hash table of all terms and their statistics, and updates the statistics as summaries for a term are received. At the end of the processing phase, the statistician sorts the statistics in memory and sends them back to the indexers. Figure <ref type="figure" target="#fig_0">10</ref> illustrates the FL strategy for collecting document frequency statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experiments</head><p>To demonstrate the performance and scalability of the collection strategies, we ran the index-builder and merging processes on our testbed, using a hardware configuration consisting of four indexers. 10 We experimented with four different collection sizes -100000, 500000, 1000000, and 2000000 pages, respectively. The results are shown in Figure <ref type="figure" target="#fig_10">11</ref>, where we can see the relative overhead (defined as T 2 -T 1 T 1 where T2 is the time for full index creation with statistics collection and T1 is the time for full index creation with no statistics collection) for both strategies. In general, exper-  Furthermore, as the collection size grows, the relative overheads of both strategies decrease.</p><p>Comparison of strategies. At first glance ME might be expected to outperform FL: since the statistician receives many summary streams in FL, but only one from each indexer in ME, it performs more comparison and aggregation in FL than in ME. However, as mentioned earlier, merging progress in ME is synchronized among the servers. Hence, a good portion of computation done at the statistician cannot be done in parallel with merging activities at the indexer.</p><p>In FL, on the other hand, the indexer simply writes summaries to the network and continues with its other work. The statistician can then asynchronously process the information from the network buffer in parallel. However, not all work can be done in parallel, since the statistician consumes summaries at a slower rate than the indexer writes them to network, and the network buffer generally cannot hold all the summaries from a sorted run. Hence there is still nontrivial waiting at the indexer during flushing as summaries are sent to the statistician.</p><p>Enhancing parallelism. In the ME strategy, synchronization occurs when an indexer creates a lexicon entry and summary for a term, sends the summary to the statistician, and then waits for the global statistic to be returned so that the lexicon entry can be completed. To reduce the effect of synchronization, the merging process can instead write lexicon entries to a lexicon buffer, and a separate process will wait for global statistics and include them in the entries. In this way, the first process need not block while waiting, and both processes can operate in parallel.</p><p>Figure <ref type="figure" target="#fig_12">12</ref> shows the effect of lexicon buffer size on merging performance over a collection of a million pages. Because lexicon entries are created faster than global statistics are returned on all indexers but the slowest, the lexicon buffer often becomes full. When this occurs, the process creating lexicon entries must block until the current state changes. Because larger lexicon buffers reduce the possibility of saturation, we expect and see that initial increases in size result in large performance gains. As lexicon buffer size becomes very large, however, performance slowly deteriorates due to memory contention. Although the entire buffer need not be present in memory at any one time, the lexicon buffer is accessed cyclically; therefore LRU replacement and the fast rate at which lexicon entries are created cause buffer  Sub-linear growth of overhead. The constant decrease of the ME and FL relative overhead in Figure <ref type="figure" target="#fig_10">11</ref> is due to the fact that the number of distinct terms in a page collection is a sub-linear function of collection size. The overhead incurred by gathering statistics grows linearly with the number of terms in the collection, while the cost of index creation grows linear-logarithmically with the size of the collection. As a result, overhead of statistic collection displays sub-linear growth with respect to index creation time. This prediction is consistent with our experimental results.</p><p>However, the decreasing relative overhead for FL is subject to the constraint that the hashtable can fit in memory. Considering that a collection of a billion pages would require a hash table of roughly 5-6 GB in size, <ref type="foot" target="#foot_7">11</ref> this constraint may become a problem for very large collections. While a memory of 6 GB is not completely unreasonable, a simple alternative using only commodity hardware would be to run several statisticians in parallel, and partition the terms alphabetically between statisticians. In this way, each statistician can collect a moderately sized set of global statistics. We have not yet implemented this option in our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">RELATED WORK</head><p>Motivated by the Web, there has been recent interest in designing scalable techniques to speed up inverted index construction using distributed architectures. In <ref type="bibr" target="#b18">[19]</ref>, Ribeiro-Neto, et al. describe three techniques to efficiently build an inverted index using a distributed architecture. However, they focus on building global (partitioning index by term), rather than local (partitioning by collection), inverted files. Furthermore, they do not address issues such as global statistics collection and optimization of the indexing process on each individual node.</p><p>Our technique for structuring the index engine as a pipeline has much in common with pipelined query execution strategies employed in relational database systems <ref type="bibr" target="#b5">[5]</ref>. Chakrabarti, et al. <ref type="bibr" target="#b2">[2]</ref> present a variety of algorithms for resource scheduling with applications to scheduling pipeline stages.</p><p>There has been prior work on using relational or objectoriented data stores to manage and process inverted files <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b6">6]</ref>. Brown, et al. <ref type="bibr" target="#b1">[1]</ref> describe the architecture and performance of an IR system that uses a persistent object store to manage inverted files. Their results show that using an off-the-shelf data management facility improves the performance of an information retrieval system, primarily due to intelligent caching and device-sensitive file allocation. We experienced similar performance improvements for the same reasons by employing an embedded database system. Our storage format differs greatly from theirs because we utilize a B-tree storage system and not an object store.</p><p>Reference <ref type="bibr" target="#b22">[23]</ref> discusses the questions of when and how to maintain global statistics in a distributed text index, but their techniques only deal with challenges that arise from incremental updates. We wished to explore strategies for gathering statistics during index construction.</p><p>A great deal of work has been done on several other issues, relevant to inverted-index based information retrieval, that have not been discussed in this paper. Such issues include index compression <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b24">25]</ref>, incremental updates <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>, and distributed query performance <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSIONS</head><p>In this paper we addressed the problem of efficiently constructing inverted indexes over large collections of Web pages. We proposed a new pipelining technique to speed up index construction and showed how to choose the right buffer sizes to maximize performance. We demonstrated that for large collection sizes, the pipelining technique can speed up index construction by several hours. We proposed and compared different schemes for storing and managing inverted files using an embedded database system. We showed that an intelligent scheme for packing inverted lists in the storage structures of the database can provide performance and storage efficiency comparable to tailored inverted file implementations. Finally, we identified the key characteristics of methods for efficiently collecting global statistics from distributed inverted indexes. We proposed two such methods and compared and analyzed the tradeoffs thereof.</p><p>In the future, we intend to extend our testbed to incorporate distributed query processing and explore algorithms and caching strategies for efficiently executing queries. We also intend to experiment with indexing and querying over larger collections and integration of our text-indexing system with indexes on the link structure of the Web.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Testbed architecture</figDesc><graphic coords="2,328.32,3.63,216.31,158.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2</head><label>2</label><figDesc>Figure 2: Logical phases</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Multi-threaded execution</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>postings buffer size (Mbytes) Time to generate sorted runs from 5 million pages (hours)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Optimal buffer size</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Mixed list storage scheme</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure</head><label></label><figDesc>Figure 7: Varying value field size</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Time to retrieve inverted lists</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Overhead of statistics collection</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Varying lexicon buffer size</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Thus, in a node with a single resource of Measured constants each type, the maximal theoretical speedup that we can achieve through pipelining is (after simplification):</figDesc><table><row><cell>Constant</cell><cell>Value</cell></row><row><cell>λ</cell><cell>1.26 × 10 -3</cell></row><row><cell>ϕ</cell><cell>4.62 × 10 -4</cell></row><row><cell>δ</cell><cell>6.74 × 10 -4</cell></row><row><cell>σ</cell><cell>2.44 × 10 -5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison of storage schemespayload field, to store extra posting-level information. The top half of the figure depicts inverted lists for four successive index terms and the bottom half shows how they are stored as (key,value) pairs using the mixed-list scheme. For example, the second (key,value) pair in the figure, stores the set of postings (cat,311), (cat,328), (catch,103), (catcher,147), etc., with the first posting stored in the key and the remaining postings stored in the value. As indicated in the figure, the postings in the value are compressed by using prefix compression for the index terms and by representing successive location identifiers in terms of their numerical difference. For example, the posting (cat,328) is represented by the sequence of entries 3 &lt;an empty field&gt; 17, where 3 indicates the length of the common prefix between the words for postings (cat,311) and (cat,328), the &lt;empty field&gt; indicates that both postings refer to the same word, and 17 is the difference between the locations 328 and 311.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Mixed-list scheme index sizes overflow pages which in turn lead to a larger index. As indicated in the figure, a value field size of 512 bytes provided the best balance between these two effects. The full-list scheme results in a moderate number of both overflow pages and internal database pages. However, it still requires around 30% more storage space than the optimal mixed-list inverted file.</figDesc><table><row><cell></cell><cell cols="3">7: Varying value field size</cell></row><row><cell cols="4">Number of pages Input size Index size Index size</cell></row><row><cell>(million)</cell><cell>(GB)</cell><cell>(GB)</cell><cell>(%age)</cell></row><row><cell>0.1</cell><cell>0.81</cell><cell>0.04</cell><cell>6.17</cell></row><row><cell>0.5</cell><cell>4.03</cell><cell>0.24</cell><cell>6.19</cell></row><row><cell>2.0</cell><cell>16.11</cell><cell>0.99</cell><cell>6.54</cell></row><row><cell>5.0</cell><cell>40.28</cell><cell>2.43</cell><cell>6.33</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparing strategies mance, especially for large collections spread over many servers. Since query response times are critical, we advocate precomputing and storing statistics at the query servers during index creation.</figDesc><table><row><cell>Some authors suggest computing global statistics at query</cell></row><row><cell>time. This would require an extra round of communication</cell></row><row><cell>among the query servers to exchange local statistics. This</cell></row><row><cell>communication adversely impacts query processing perfor-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>10</head><label></label><figDesc>All indexers had the specifications listed in Section 2.</figDesc><table><row><cell></cell><cell>22</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">ME strategy</cell></row><row><cell></cell><cell>20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">FL strategy</cell></row><row><cell></cell><cell>18</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Relative overhead (%age)</cell><cell>8 10 12 14 16</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0 2</cell><cell>0.2</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell><cell>1</cell><cell>1.2</cell><cell>1.4</cell><cell>1.6</cell><cell>1.8</cell><cell>2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Collection size (in millions)</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Even though the Web link structure is being utilized to produce high-quality results, text-based retrieval continues to be the primary method for identifying the relevant pages. In most commercial search engines, a combination text and link-based methods are employed.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>The URLs are normally replaced by numeric identifiers for compactness.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>λ = λ1λ2, where λ1 is the rate at which pages can be loaded into memory from the network and λ2 is the average ratio between the size of a page and the total size of the postings generated from that page.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>All these features can be turned off, if desired, for efficiency.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>Storing the indexing term in the key and a single location in the value is not a viable option as the locations for a given term are not guaranteed to be in sorted order.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>Since values can be of arbitrary length, Berkeley DB uses overflow pages to handle large value fields.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_6"><p>Term-level refers to the fact that any gathered statistic describes only single terms, and not higher level entities such as pages or documents.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_7"><p>A billion pages will contain roughly 310 million distinct terms<ref type="bibr" target="#b13">[13]</ref>, and each term using 20 bytes of storage results in a hashtable of 5.77 GB.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Supporting full-text information retrieval with a persistent object store</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E B</forename><surname>Moss</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994-03">Mar 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Resource scheduling for parallel database and scientific applications</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Muthukrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">th ACM Symp. on Parallel Alg. and Architectures</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="329" to="335" />
			<date type="published" when="1996-06">June 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The evolution of the web and implications for an incremental crawler. 26th Intl. Conf. on Very Large Databases</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Garcia-Molina</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000-09">Sep 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Signature files: An access method for documents and its analytical performance evaluation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Christodoulakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Office Information Systems</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1984-10">October 1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Database System Implementation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Garcia-Molina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Widom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Prentice-Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Structuring text within a relation system</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Gorssman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Driscoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd Intl. Conf. on Database and Expert System Applications</title>
		<imprint>
			<date type="published" when="1992-09">Sep 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Overview of TREC-7 very large collection track</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hawking</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Seventh Text Retrieval Conf</title>
		<meeting>of the Seventh Text Retrieval Conf</meeting>
		<imprint>
			<date type="published" when="1998-11">Nov 1998</date>
			<biblScope unit="page" from="91" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">WebBase: A repository of web pages</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hirai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Garcia-Molina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Paepcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 9th Intl. WWW Conf</title>
		<meeting>of the 9th Intl. WWW Conf</meeting>
		<imprint>
			<date type="published" when="2000-05">May 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Inverted file partitioning schemes in multiple disk systems</title>
		<author>
			<persName><forename type="first">B.-S</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Omiecinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Parallel and Distributed Systems</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="142" to="153" />
			<date type="published" when="1995-02">Feb 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Accessibility of information on the web</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">400</biblScope>
			<biblScope unit="page" from="107" to="109" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Suffix arrays: A new method for on-line string searches</title>
		<author>
			<persName><forename type="first">U</forename><surname>Manber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Myers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 1st ACM-SIAM Symp. on Discrete Algorithms</title>
		<meeting>of the 1st ACM-SIAM Symp. on Discrete Algorithms</meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A design of a distributed full text retrieval system</title>
		<author>
			<persName><forename type="first">P</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">A</forename><surname>Macleod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nordin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Conf. on R&amp;D in Information Retrieval</title>
		<meeting>ACM Conf. on R&amp;D in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1986-09">Sep 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Building a distributed full-text index for the web</title>
		<author>
			<persName><forename type="first">S</forename><surname>Melnik</surname></persName>
		</author>
		<ptr target=".stanford.edu/cgi-bin/get/SIDL-WP-2000-0140" />
		<imprint>
			<date type="published" when="2000-07">July 2000</date>
		</imprint>
		<respStmt>
			<orgName>Stanford Digital Library Project</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
	<note>Available at wwwdiglib</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Self-indexing inverted files for fast text retrieval</title>
		<author>
			<persName><forename type="first">A</forename><surname>Moffat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="349" to="379" />
			<date type="published" when="1996-10">October 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Compressed inverted files with reduced decoding overheads</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ngocvo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moffat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 21st Intl. Conf. on R&amp;D in Information Retrieval</title>
		<meeting>of 21st Intl. Conf. on R&amp;D in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1998-08">Aug 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Olson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bostic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Berkeley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Summer Usenix Technical Conf</title>
		<imprint>
			<date type="published" when="1999-06">1999. Jun 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Query performance for tightly coupled distributed digital libraries</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ribeiro-Neto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barbosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd ACM Conf. on Digital Libraries</title>
		<imprint>
			<date type="published" when="1998-06">June 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient distributed algorithms to build inverted files</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ribeiro-Neto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Neubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ziviani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">22nd ACM Conf. on R&amp;D in Information Retrieval</title>
		<imprint>
			<date type="published" when="1999-08">Aug 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Information Retrieval: Data Structures and Algorithms</title>
		<author>
			<persName><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Addison-Wesley</publisher>
			<pubPlace>Massachussetts</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Query processing and inverted indices in shared-nothing document information retrieval systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tomasic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Garcia-Molina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLDB Journal</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="243" to="275" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Incremental update of inverted list for text document retrieval</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tomasic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Garcia-Molina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shoens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGMOD</title>
		<imprint>
			<date type="published" when="1994-05">1994. May 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dissemination of collection wide information in a distributed information retrieval system</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Viles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>French</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 18th Intl. ACM Conf. on R&amp;D in Information Retrieval</title>
		<meeting>18th Intl. ACM Conf. on R&amp;D in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1995-07">July 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Inktomi</forename><surname>Webmap</surname></persName>
		</author>
		<ptr target="www.inktomi.com/webmap/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Managing Gigabytes: Compressing and Indexing Documents and Images</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moffat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Bell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Morgan Kaufman Publ</publisher>
			<pubPlace>San Francisco</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An efficient indexing technique for full-text database systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zobel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moffat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sacks-Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th VLDB Conf</title>
		<imprint>
			<date type="published" when="1992-08">Aug 1992</date>
			<biblScope unit="page" from="352" to="362" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
