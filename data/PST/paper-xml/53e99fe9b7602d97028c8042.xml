<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Modeling a dynamic and uncertain world I: symbolic and probabilistic reasoning about change</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Steve</forename><surname>Hanks</surname></persName>
							<email>hanks@cs.washington.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<postCode>FR-35, 98195</postCode>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<postCode>FR-35, 98195</postCode>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Drew</forename><surname>Mcdermott</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Yale University</orgName>
								<address>
									<addrLine>Box 2158</addrLine>
									<postCode>06520</postCode>
									<settlement>Yale Station, New Haven</settlement>
									<region>CT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Modeling a dynamic and uncertain world I: symbolic and probabilistic reasoning about change</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3685EA3CC2ECC709D401462FC5611E77</idno>
					<note type="submission">Received August 1991 Revised November 1992</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T06:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Hanks, S., and D. McDermott, Modeling a dynamic and uncertain world I: symbolic and probabilistic reasoning about change, Artificial Intelligence 66 (1994) 1-55.</p><p>Intelligent agency requires some ability to predict the future. An agent must ask itself what is presently its best course of action given what it now knows about what the world will be like when it intends to act. This paper presents a system that uses a probabilistic model to reason about the effects of an agent's proposed actions on a dynamic and uncertain world, computing the probability that relevant propositions will hold at a specified point in time. The model allows for incomplete information about the world, the occurrence of exogenous (unplanned) events, unreliable sensors, and the possibility of an imperfect causal theory. The system provides an application program with answers to questions of the form "is the probability that ~ will hold in the world at time t greater than r?" It is unique among algorithms for probabilistic temporal reasoning in that it tries to limit its inference according to the proposition, time, and probability threshold provided by the application. The system will also notify the application if subsequent evidence invalidates its answer to a query.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Acting intelligently in the world requires the ability to look ahead in time in order to consider the consequences of one's actions. Planning a morning of errands may force us to decide ahead of time when to go to the grocery store, thinking ahead to the fact that we intend to buy ice cream which will melt if left in the car too long. We mop the floor so as to avoid mopping ourselves into the corner. Cooking a meal requires a great deal of looking ahead, hypothetical scheduling if you will, to make sure things get done on time and that any step in the preparation is finished before its results are needed by some other step. And a doctor, in the midst of prescribing some medication, must think carefully about the future course of the patient's illness, potential allergic, reactions, and about potential interactions among whatever drugs she is suggesting.</p><p>Looking ahead requires in turn that the agent maintain some sort of dynamic model of its worldmtracking those aspects of the world necessary for it to make good predictions and in addition remembering those aspects of the world necessary to allow it to recover from its failures and learn from its experiences.</p><p>The model will roughly consist of two pans:</p><p>• some theory of how the domain operates: rules governing expected changes, assumptions about the occurrence of events, the truth of propositions, and so on; • evidence about the current state of affairs, generally in the form of reports from sensors of some sort (whether they be mechanical, verbal, or whatever).</p><p>This work describes a "word-model manager"--a program module that stores and operates on an agent's model of its world, l Such a program is similar in spirit to temporal database managers, e.g. in <ref type="bibr" target="#b3">[4]</ref>, though we will introduce significant differences both in representation and functionality, the most significant of which is our explicit treatment of uncertainty.</p><p>The main contribution of this paper is an algorithm for probabilistic rcasoning--estimating the projected probability 2 that a proposition will be true at a point in time. We define a graphical framework for computing this probability, then point out the computational difficulty: that predicting the INote the ambiguity of the term "model", which may refer to the agent's underlying theory of the domain or to the beliefs generated from the theory and specific evidence, or even to the program that examines the theory and generates the beliefs. Context should make clear the word's sense.</p><p>2"Projected" means that the probability that a proposition will be true at time t is computed on the basis only of the world's state at time prior to t. See Section 3.6.1 for more detail. world may require instantiating the graph at every time point between the time the first piece of evidence was collected to the time of the query. Such a scheme may be both intractable and unnecessary, however. On the one hand we may want to reason about the world over a long period of time. On the other hand, it may be the case that the world only infrequently changes state in any "interesting" way (i.e. in any way relevant to the probability we are trying to compute). Our algorithm tries to identify the relevant state changes and instantiate the graph only for those propositions and at those times relevant to the prediction task at hand. By examining the propositional content of the query and our causal model of the domain we can ignore certain propositions and evidence as irrelevant, and by reasoning about the qualitative relationship of the probability to the user-supplied threshold (instead of trying to compute the exact probability), we can ignore evidence and abandon lines of reasoning when they become too tenuous to affect that relationship.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. 1. System overview</head><p>The diagram in Fig. <ref type="figure" target="#fig_0">1</ref> shows how the world-model manager is embedded in an architecture for planning and execution. 3 Note that the planner's information about the world comes entirely from the world model, and its plan commitments are posted through that module as well. 3See <ref type="bibr" target="#b13">[14]</ref> for more details.</p><p>The world model can be viewed as an interconnected network of beliefs, each of which represents a summary of the agent's information about the state of some proposition in the world at some point in time. The summary takes the form of a probability, so each belief consists of a probability distribution over the proposition's state 4 along with the time point at which the proposition's state was queried.</p><p>Beliefs are built (probabilities computed) on the basis of three sorts of information: evidence from the sensors, a causal theory that predicts changes in the proposition's state, and plan commitments that come from the planner. Computing states of the world based on plan commitments is the problem typically known as "plan projection", and is the topic of <ref type="bibr" target="#b15">[16 ]</ref>. Here we will concentrate on the process of computing probabilities based on sensory evidence and the causal theory.</p><p>Central to this work is the interaction between the planner and the world model, in which the planner posts queries to the world model and receives answers back in the form of belief data structures. The query indicates to the world-model manager what is important, when it is important, and how important the information is. The model manager tries to perform only the processing necessary to satisfy the planner's request. The planner then uses the beliefs information to make plan choices (to select new or refine existing plans).</p><p>Information about a proposition's state can change after the belief has been computed and returned to the planner, as the sensors provide more information about the world. A change in the proposition's predicted state can obviously upset plan choices, so the planner needs to be notified if subsequent information changes a belief significantly. The belief structure returned to the planner carries with it such a promise: the "owner" of the belief will be notified if information subsequently invalidates its estimate; it is up to the owner then to take appropriate action.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Issues</head><p>The main issue confronted in this paper is how to build and reason efficiently with a model of a dynamic and uncertain world. The model will have both symbolic and numeric components. Symbolic information, like rules that predict the effects of executing a particular action, is necessary to support processes like plan selection and repair. Numeric information, on the other hand, provides a concise representation of the world's uncertainty. Our job will be to integrate the two sorts of information. 4This paper will consider only binary propositions, so the distribution amounts to a single number--the probability that the proposition is (or was, or will be) true.</p><p>Consider, for example, the problem of determining whether one's car is in the parking lot downstairs. One generally knows where it was put in the morning, but the car may be stolen, or towed if one parks in a lot without a permit. <ref type="bibr" target="#b4">5</ref> One may have additional information--somebody reporting having seen the car recently, seeing a tow truck in the vicinity, parking in a highcrime area. One needs to reason about changes to the car's state over time based on evidence, specific knowledge about the domain, and knowledge about how well specific information allows predictions about the case at hand.</p><p>The causal theory mentioned in Fig. <ref type="figure" target="#fig_0">1</ref> consists of two parts. The first, mentioned above, is a set of "causal rules" that predict change in the world. These rules, similar to those seen in the literature on formal models of change ( <ref type="bibr" target="#b22">[23]</ref> for example) describe relationships of the form "for all times t, P true at t causes Q to become true at t + 1".</p><p>The numeric part of the theory quantifies the extent of our confidence in the rules and also in the reliability of reports received by the sensors.</p><p>Parameters of the former type measure the extent to which rules that predict changes in P's state (like the one above) accurately reflect the exact circumstances under which P will actually change state. Since the rules might be neither necessary nor sufficient, we need parameters of two types: the probability that P will be false even if a rule predicts that it should have become true, and the probability that P will become true even if none of the rules predicted that change.</p><p>We use probabilities to represent our confidence in evidence provided us by the execution system (via its sensors) as well: a numeric parameter attached to a symbolic sensor report measures our belief that the sensor reporting on P's state is reporting accurately--that is, that the report is an accurate reflection of the actual state of the world.</p><p>Apart from these problems of representation we are faced with a difficult computational problem, which is to cope with the staggering body of evidence that might be brought to bear in answering a query. Our causal rules are quantified over all time points, thus we might consider an infinite number of rule instances in the process of predicting P's state. Sensory evidence about P's state can extend arbitrarily far back into the past. Our task therefore will be to consider enough evidence to produce a good estimate of P's exact probability, ignoring enough evidence so that we can do so quickly. In other words, we have to build an instance of the causal theory (consisting of instances of the rules along with selected observations) appropriate to the query at hand.</p><p>The algorithm described in the paper can be described briefly as follows:</p><p>5This example is purely hypothetical, and does constitute an admission of guilt on either author's part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Inputs:</head><p>-A proposition ~, a time point t, and a probability threshold z.</p><p>-A causal theory, consisting of • a set of causal rule schemas describing the circumstances under which various propositions (including ~) change state; • a set of probability parameters characterizing the extent to which the model is considered complete, the rate at which various events occur, and so on. -Situation-specific evidence, consisting of • a set of observations, each consisting of a proposition (which might be ~), a time point, and a probability parameter characterizing our confidence in the observation's veracity. The algorithm is "interruptable" in that it always maintains an estimate of ~'s probability, and can produce that estimate at any time the application needs it (yielding a rough estimate quickly, for example). Although the algorithm will always consider more evidence in computing the probability given more time to compute, it is not convergent, in the sense that [E-P(¢t )[ does not necessarily decrease as the algorithm takes more time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.">Paper overview</head><p>Describing the model manager amounts to explaining how the module takes a query as input and ( 1 ) builds and solves an appropriate instance of the causal theory, yielding an estimate of the query proposition's probability, and (2) sets up the dependency information necessary to detect and respond appropriately to subsequent relevant information. We describe the work in the following steps:</p><p>(1) a discussion of representation issues--time, chance, and their interplay; (2) a method for computing probabilities for a proposition at a time point, given an appropriate body of evidence;</p><p>(3) a method for choosing an appropriate body of evidence for the query; (4) an example;</p><p>(5) a discussion of the central assumptions underlying the algorithm; (6) a discussion of related and future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries: notation and temporal concepts</head><p>We first need to establish some notational conventions, as well as introduce the temporal model we will exploit in later sections. We will also discuss briefly the temporal database manager module, which is used by the rest of the system. This work is covered more thoroughly in [11; 13, Chapter 2].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Temporal ontology</head><p>First of all we will adopt a point-based representation of time, meaning that the temporal individuals will denote instants in time. We will use variables like t and ti to refer to these individuals. Our analysis does not require us to commit to whether the underlying model of time is continuous or discrete, though the implementation assumes discrete time for the sake of convenience.</p><p>An interval denotes an uninterrupted stretch of time defined by its endpoints. We will write an interval as a pair [ti, t j] where ti must occur temporally no later than tj. An interval includes both of its endpoints. The function distance or d relates the instants denoted by time points. d (ti, t j) indicates the amount of time elapsed between ti and tj. We therefore enforce the following constraints:</p><formula xml:id="formula_0">Vi, j,k [d(ti, tj) + d(tj, tk) = d(ti, t~) ], Vi, j [d(ti, tj) = -d(tj, ti)], Vi [d(ti, ti) = 0],</formula><p>and introduce the notation ti-&lt;tj=-d(ti, tj) &gt;10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Temporal database management</head><p>The temporal database manager is a module that maintains a network of time points and intervals along with constraints on their temporal distances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>It provides four functions:</head><p>(1) managing the addition and deletion of points and intervals, (2) maintaining the set of constraints on inter-point distances, (3) fetching information from the database, (4) monitoring the database for the addition of new information.</p><p>Application programs can add constraints of the form low &lt;&lt;. d(ti, tj) &lt;~ high, where low and high are integers with low ~ high, and then pose queries of the form "is d(ti, tj) ~ k?" for time points ti and tj, and some integer k.</p><p>The other type of query is the "temporally scoped fetch", which returns all time points (or intervals) that fall within an application-supplied time interval, and whose tokens satisfy an application-supplied fetch criterion (supplied in the form of a function). Each time point and interval has an associated token, indicating, for example, what happened at the time point or over the interval. The database attaches no particular meaning to the tokens, using them only in the process of fetching.</p><p>The application can ask the database monitor to notify it (by calling an application-supplied function) when a point or interval gets added that ( 1 ) occurs within a particular interval of time, and (2) whose token satisfies a particular criterion. An add monitor is attached to the interval of interest and checks points and intervals as they are added to the database, notifying the application if an "interesting" one arrives. The idea is that new information might invalidate a probabilistic estimate--~0's probability was thought to be greater than ~ but new evidence shows it to be less---at which point the application should reconsider any plans that rely on ~o being true.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Propositions and probabilities</head><p>This paper is concerned with how to compute probabilities of the form P("~o true at time t"), where ~o is a proposition and t is a time point. We have already introduced the notion of a time point; now we move on to the notion of a proposition, then to a brief discussion of why we are using probabilities to represent the agent's uncertainty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1.">The form of propositions</head><p>We will use Greek letters, most often ~o, to stand for the propositions we wish to assess. "Assessing (o" means computing ~o's probability at some time point t. We will abbreviate this quantity as P(~ot). In cases where t is an indexed time point, e.g. tt, we may further abbreviate P(~t, ) as P(q~i). These propositions will in practice look like the usual AI logical propositions: (RAINY), (LOC MY-CAT MY-LIVING-ROOM), and so on--the atomic formulas in a first-order language. Propositions thus do not contain logical connectives. Instead of logical negation and conjunction, for example, probability theory speaks of complementary and joint events. If ¢ and ~u are propositions of interest (thus are events in the probability space), the events ~ and ~ are the corresponding complementary events, corresponding to (NOT ~) and (NOT ~). The axioms of probability theory dictate that P(¢) = 1 -P(~). The joint event (~,~u) corresponds to the conjunction ((p A ~). We will discuss below the problem of computing the probabilities of joint events. We allow quantification only in special domain-dependent cases, which we will discuss in <ref type="bibr" target="#b15">[16]</ref>. When we speak of propositions such as ~, however, we are talking about atomic propositions--no connectives.</p><p>Our causal model, inspired by McDermott's concept of the "pcause" rule <ref type="bibr" target="#b22">[23]</ref>, grants special semantic status to a class of propositions known as "events". Events are the propositions that effect changes in the world. The agent's actions are events. The distinction between events and "facts" (as the remaining propositions are called) is somewhat arbitrary, as Shoham points out in <ref type="bibr" target="#b25">[26,</ref><ref type="bibr">Chapter 2]</ref>. Is there a meaningful difference between the occurrence of a rainstorm and some period of time over which it is raining?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2.">Facts and events</head><p>Theories of change have typically divided the set of proposition types into facts and events. The latter are usually associated with the agent's actions. In fact most theories of change, and most planners, assume that no events except the agent's actions will ever occur.</p><p>One reason for making this distinction is to separate the intuition that if a fact F is said to be true over the interval we want to say it's true over all subintervals as well. In contrast, if an event E is said to be true over an interval we really want to say it occurs exactly once, over the whole interval, and not over any subinterval. This distinction avoids a technical problem with the causal rules: if we have a rule that says if F is true when E occurs then P becomes true, and we assert that E occurred over an interval, then we want to assert that P becomes true exactly once, and not at every time point contained within the interval.</p><p>In practice this problem rarely arises, mainly because events are almost always assumed to occur at a single point in time. We will begin by making this assumption, then relax it so that certain events (in particular the agent's actions) are allowed to occur over an interval rather than at a point in time.</p><p>A more important distinction arises in the probabilistic assumptions we make about the occurrence of events as opposed to the persistence of facts.</p><p>The first assumption is that events occur "dispassionately" in the world.</p><p>More particularly we assume that the probability that an exogenous event 6 occurs does not depend on the planning choices made by the agent. We are thus assuming a situation of uncertainty rather than one of risk or gaming.</p><p>A second assumption we make about events is a rather vague one; it is important for the efficiency of the assessment and projection algorithms but does not otherwise affect the analysis. The assumption is that causally relevant events occur, unknown to the agent, with low probability. We characterize as "chaotic" those domains in which the low probability assumption does not hold. Predicting the future becomes very difficult in chaotic situations; our algorithm becomes very slow. We will see below how this assumption of "low probability events" can be used to guide the search for important causal inferences.</p><p>Note that these assumptions--and thus the distinction between facts and events--is essentially a practical one. In fact the distinction between events and facts will usually be unimportant to our analysis; when it matters we will use the letter e or E to stand for the former and the letter ~ or P to stand for the latter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">3.3. Contextual propositions</head><p>Contextual propositions are facts we have some information about but do not reason about directly using our temporal theory; they are assumed not to change in the interval over which we model the temporal facts and events. The weather, the current temperature, a patient's age are examples.</p><p>Our implementation assumes an atemporal probabilistic model of the contextual propositions, meaning that we can ask for the probability of these propositions, and these probabilities may be interdependent, but the probabilities remain unchanged over the interval in question.</p><p>Contextual propositions will be relevant only in Sections 3.4 and 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.4.">Reasoning with probabilities</head><p>We will quantify the agent's uncertainty about the world using a probability number, thus will be speaking of assessing the probability of ~ at a time point t, abbreviated to P(~t). This approach is controversial. Some researchers of a philosophical bent contend that a probability is not a valid measure of credal commitment. Practitioners in AI argue a number of points: that probabilistic reasoning requires a tremendous amount of input data (prior probabilities and the like) that cannot reasonably be expected from an application program, that probabilistic reasoning is computationally intractable, and that a single probability number is insufficient to capture 6An exogenous event is one that is not explicitly planned by the agent. concepts, like ignorance and the value of information, that are essential components of the decision-making or planning process.</p><p>The philosophical issues begin a debate we have no wish to enter at this point; there is an excellent summary of the issues in <ref type="bibr" target="#b24">[25,</ref><ref type="bibr">Chapter 1 ]</ref>. In any case, the analysis in this chapter does not require the reader to view the computed probabilities as states of belief. 7</p><p>The second group of arguments touches on empirical issues, and we hope to show in this paper that reasoning within an application-supplied probabilistic model can be accomplished efficiently. Our contention will be that although computing exact probability values may be impractical, one may nonetheless compute approximations that (1) can be arrived at quickly, and (2) provide the agent with the information necessary to make decisions about its future actions. 8 The final point, that a single probability number does not suffice to represent concepts related to the agent's state of information, will be addressed in <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Uncertainty, evidence, and probabilistic updates</head><p>Estimating a proposition's probability can be considered in two steps:</p><p>(1) choosing an appropriate body of evidence E, and (2) computing the probability P (~0t J E).</p><p>Our implementation actually interleaves these two steps, but viewing them separately serves to isolate the issues involved in the two processes. This section treats the second step, computing a probability given a particular set of evidence. We explain the process by confronting these questions:</p><p>( 1 ) How does uncertainty manifest itself in the system? (2) What form does evidence take? (3) What is the proper update for each type of evidence?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The nature of uncertainty</head><p>There are two ways to arrive at a belief that some proposition, say ~o, holds in the world at some point in time t. The first is to observe ~0 at t either directly, or by being told by someone else, or by some other indirect evidence. We then base a belief in ~o's truth on (1) the observation or evidence itself, and 7We may occasionally lapse into language betraying the fact that we are interpreting these probabilities in exactly that way---e.g, by naming the main data structure a belief. Nonetheless, what we are doing is computing probabilities, and the reader is free to interpret them any way he or she wants.</p><p>SWellman <ref type="bibr" target="#b30">[31]</ref> advances this position as weU.</p><p>(2) the belief that the observation at time t accurately reflected ~0's state at that time.</p><p>We will assume that we will get observational evidence about ~0 only in the form of direct (but possibly inaccurate) testimony. We can also reason indirectly about ~0's state, provided we have some sense of the circumstances under which ~0 can become true. For example, knowing that the occurrence of an event e causes ~ to become true, and knowing that e actually occurred at a time t' prior to t, we can base a belief in ~0's truth on <ref type="bibr" target="#b0">(1)</ref> the belief that e causes ~0, and (2) the belief that e occurred at t', and (3) the belief that nothing happened between t' and t to cause ~0 to become false.</p><p>The two modes of reasoning are not exclusive--we may have arrived at the belief that e occurred at t' by direct observation, or by some other chain of causal inference, or both. Belief in the truth of ~o at t will be uncertain to the extent that we doubt:</p><p>• whether an observation was a good reflection of ~0's state at t; • whether some relevant e actually occurred at t';</p><p>• whether ~ indeed causes ~0;</p><p>• whether nothing actually changed ~0's state in the interim, because -maybe some other known relevant event occurred but its occurrence was unknown, -maybe some other relevant event occurred but its relevance was unknown.</p><p>To summarize the above, one is uncertain about the state of the world to the extent that one lacks confidence in the veracity of one's observations, lacks perfect knowledge of all causally relevant events, and lacks a perfect predictive model of the world.</p><p>The discussion identifies two types of evidence: reports from the sensors and predictive information from the causal rules. Each has an associated update equation--a function that incorporates the new evidence into one's belief about ~'s state at t. In other words, if one's current state of information about ~, say E, leads one to a certain probability P(q~t[ E), and a new piece of evidence e is obtained, then the update equation determines the new probability P(~ot I E, e) on the basis of the old probability and an estimate of e's veracity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Updates on sensory observations</head><p>Suppose we are reasoning about the state of ~0 at a point in time t. We are thus computing the probability P (~ot), and we have so far amassed a body of evidence E that bears on ~0, and have used it to compute a probability P(~ot I E). Then we get a sensory observation O to the effect that ~ was observed true at time t. 9</p><p>The complementary event is O, which denotes an observation that ~o is false. The question is how to update the probability P (q~t ] E) to incorporate the new piece of evidence; that is, to compute P (~ot I E, O), bearing in mind that O may or may not accurately reflect ~0's state at t.</p><p>Associated with each observation O we have several parameters: the proposition it claims to have observed, whether that proposition was true or false, the time point at which the observation occurred, and one or more parameters characterizing the relationship between ~0's state and the sensor's report about ~0's state.</p><p>The last parameters are most interesting. The sensor either was functioning properly when it made the observation, in which case we say the observation was "reliable", or it was malfunctioning, in which case we say the observation was "unreliable". Let R denote a reliable observation. The probability space then consists of the following events: all of which are relative to time t.</p><formula xml:id="formula_1">• ~ = proposition ~ is • ~ = proposition ~ is • 0 = sensor reports • 0 = sensor reports • R = observation 0 is • R = observation 0</formula><p>We can constrain the joint distribution over these events so that it corresponds to our definitions of observations and reliability. We first assume that a reliable observation of ~0 is definitive--it should determine our belief in ~0 at the time the observation was made. A reliable observation of g (i.e. 9We should note a potential confusion here among time points. Two times points are potentially relevant in considering an observation: the time at which the observation was made and the time at which the observation was posted to the database. Likewise when one makes a query like "what is ¢'s probability at t" we can consider both the time t and whatever time the application posed the query. We are not considering times of the second sort: the times at which observations are made and the times at which queries are posed. Our network of beliefs represents what the application believes now, on the basis of what information it has received to this point. We cannot therefore reason about changes to the agent's state of information over time. Although we can represent statements like "I believe that ~ was true five minutes ago but will not be true in one hour", we cannot explicitly represent statements like "five minutes ago I believed that ~ would be true in one hour, but now I do not". m O) means that we believe absolutely that ~0 is false at t. This assumption translates into the following two constraints:</p><formula xml:id="formula_2">P(¢IO, R,E) = 1,<label>(1)</label></formula><formula xml:id="formula_3">P(~oIO, R,E) = 0,<label>(2)</label></formula><p>for any body of evidence E.</p><p>It is harder to characterize the relationship between an observation and ~0 if the observation was unreliable, a malfunctioning sensor may still report either O, or O, and hence an observation may correctly report ~0's state even though it was unreliable--i.e, the sensor may do the right thing for the wrong reason.</p><p>Furthermore, we generally won't know whether any individual observation was reliable or not, and may therefore have to base our update on statistical information about the sensor's behavior. First let us assume we have a probability r = P (R) that represents the likelihood that the observation was reliable. We next assume that there is no systematic relationship between ~0's state and the observation's reliability. That is, ~0 being true in no way causes the sensor to malfunction. This assumption amounts to an assertion that R is probabilistically independent of ~0:</p><formula xml:id="formula_4">P(R I ~o) = P(R [-~) = P(R) = r.</formula><p>(</p><p>Next we have to characterize the relationship between the observation and ~o for those cases when the sensor malfunctions. This information corresponds to the parameters P (O I R, ~o) and P (OIR,-~). In some cases we may have a good model of the sensor's "failure-mode" behaviorufor example, the sensor may always report "true" when it malfunctions, in which case we would set P(OIR,~o) = P(OIR, g) = 1.0. This example introduces a reasonable assumption about unreliable observations: that when the sensor malfunctions its output is no longer sensitive to ~o's state. This assumption amounts to the assertion that P (O [ R, ~o ) = P(O I R,g). If we had less precise information about the sensor's behavior when it malfunctions, we might introduce into the model a "failure-mode" parameter f: the probability that the sensor reports "true" given that it is malfunctioning:</p><formula xml:id="formula_6">P(OIR,~o) = e(oI R,~) = f.<label>(4)</label></formula><p>In other cases we may have no explicit model of the sensor's failuremode behavior, and thus have no values for the probabilities P (O[ R, ~o) and P(O1 R,-~). The assumption we made abovemthat the sensor's failuremode behavior does not depend on q's state--is still reasonable, but we may have no f parameter available. Two other reasonable assumptions might be:</p><p>(1) That one's state of belief about ¢ should not change as a result of an unreliable sensor report. Formally we might say that P (~ I O, R) = P ((p I O, R) = P (~), though it is not immediately clear that this assumption determines a unique distribution or even that it is consistent with the rest of the constraints. (2) That our assumption about the failure-mode probabilities should add the least possible amount of information to the distribution. This is in effect the "maximum-entropy" solution <ref type="bibr" target="#b17">[ 18,</ref><ref type="bibr" target="#b18">19 ]</ref>: choose the (unique) distribution consistent with the constraints that exhibits maximum entropy (minimum information).</p><p>We show in [13, Section 3.4] that the three assumptions are equivalent: that the maximum-entropy distribution implies</p><formula xml:id="formula_7">P(OI R,~) = P(OI R,-~) = 0.5,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>which in turn implies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P(~ I O, R) = P(~ I O, R) = P(~).</head><p>In other words, the report of a malfunctioning sensor does not depend on q's state, and an unreliable observation conveys no information.</p><p>This model--the reliability probability parameter r coupled with a minimum-information assumption about the sensor's failure-mode behavior-is essentially the same as that adopted for electronic components in the literature on circuit diagnosis, e.g. <ref type="bibr" target="#b7">[8]</ref>. For the remainder of the paper we will assume a model consisting of the reliability parameter r only. Adopting a model with more parameters, e.g. the f parameter from equation ( <ref type="formula" target="#formula_6">4</ref>), would require minimal changes to the algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">The observation update equations</head><p>Now suppose we have available a prior probability p = P(~ [ E) and a reliability parameter r = P(R). We can then build the following set of constraints:</p><formula xml:id="formula_8">P(q~ I O, R,E) = 1, P(~IO, R,E) = O, e(o I E) = p, P(R [ E) = P(R [ o) = e(R [g) =r, e(o [ ~,-R,E) = P(O I -~,-R,E) = 0.5,</formula><p>which defines a unique distribution over the probability space. We can then apply Bayes' rule,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P(OIE,~o)P(¢ I E) e(~o I E,O) = P(OIE) ,</head><p>which, after expanding terms and substitution, yields</p><formula xml:id="formula_9">0.5(1 + r)p P(~oIE, O) = rp +O.5(1-r) (5)</formula><p>for positive observations, and similarly for negative observations</p><formula xml:id="formula_10">-- (1 -r)p<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P(¢IE'O)-l +r-2rp"</head><p>Note the following properties of these equations: if the observation is certainly reliable (r = 1), then P(~ I O) = 1 and P(¢ I O) = 0. A reliable observation renders the agent certain about the state of the proposition in question. If the observation is certainly unreliable (r = 0) then P (~o I O) = P(~0 l O) = p = P(~0). An unreliable observation provides no information regarding the proposition. The positive update equation is undefined in case r = 1 and p = 0; a negative update is undefined in case r = 1 and p = 1. Now assume we have an observation oi that takes place at time t; and has reliability parameter ri. We can rewrite ( <ref type="formula">5</ref>) and ( <ref type="formula" target="#formula_10">6</ref>) as a function of the prior probability p, but which depends on (a) whether oi is positive or negative and (b) the reliability parameter r~. We will call this function s~, for a sensor update at time ti and reliability parameter r~, and note that it is monotone nondecreasing in its input parameter. 10</p><formula xml:id="formula_11">0.5(1 + ri)p si(p) = rip + 0.5(1 -ri) (1 -ri)p 1 + r -2rip' if oi is positive if oi is negative. (7)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">An equivalent graph representation</head><p>Fig. <ref type="figure">2</ref> shows explicitly the probabilistic model implied by the update equations. Note that the state of ~ and the observation's reliability R jointly determine whether the observation will be true or not. ~ and R are independent, given a true or false observation. We require prior probabilities for ¢ and for R and a matrix (shown in the diagram) for all possible values 1°Note that we no longer need to condition on E. Prior information is summarized in the parameter p. The sensor update depends only on the reliability parameter, which is assumed to be independent of E. of P(OI ~a, R). The entries in this matrix are determined by the constraints and the maximum-entropy assumption introduced in the previous section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Comparison to the likelihood ratio</head><p>The approach above does sensor updates on the basis of (a) a reliability parameter r, (b) a prior probability p, and (c) additional parameters or assumptions about the sensor's behavior when it malfunctions. A more traditional update technique (see <ref type="bibr" target="#b24">[25,</ref><ref type="bibr">Chapter 2]</ref>, for example) uses two likelihood ratios instead,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P(O I ~a) 2o-P(Ol~)' P(O I¢a) P(O I ~)'</head><p>along with the same prior probability, then applies Bayes' rule as follows (this is for positive updates):</p><formula xml:id="formula_12">200 P(~a I O) -1 + 200'<label>(8)</label></formula><p>where O _ P((a)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">-P((a)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P -l-p"</head><p>As it happens, the two approaches are almost identical in their applicability. Suppose that we admit an arbitrary relationship between 0 and ~o--the following equations then hold:</p><p>--m</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P(O I ~a) --P(OI ~,R)P(R [ ~a) + e(oI ~a,R)P(R [ ~a), P(O[ ~) = P(OIg, R)P(R[g) + P(OI~,-R)P(~[ ~).</head><p>But with our assumption that reliability is independent of ~o, we can take all of the P (R I ") parameters to be equal to r, and then P (R I ") = 1 -r as well. We can perform a similar analysis for the f parameter:</p><formula xml:id="formula_13">e(o e(ol f- (l-r) -1 (l-r)</formula><p>where f is undefined (but unnecessary) if r = 1. These equations suggest certain relationships between the two update methods. They basically cover the same territory, but just parameterize it differently. Any relationship between the probabilities of ~ and O can be modeled in terms of r and f parameters, unless P(O I q~) &lt; P(O I ~), in which case it is misguided to think of O as an observation of ~ at all.</p><p>One advantage to formulating the update in terms of r and f is that it allows the case r = 1, which corresponds to a singularity in the orthodox Bayesian approach. A perfectly reliable observation fixes belief with certainty, which is impossible using equation ( <ref type="formula" target="#formula_12">8</ref>) above--the posterior probability for ~ cannot be 1. A probabilistic purist will no doubt approve of the impossibility of forcing absolute certainty with a single observation. But in the context of computational modeling it is quite useful to be able to include perfectly reliable observations as boundary conditions, thereby cutting off expensive cogitation about slight deviations from 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Updates on causal rules</head><p>Equation <ref type="bibr" target="#b6">(7)</ref> allows us to update a probability at a single point in time in response to a new piece of observational evidence, In this section we discuss the problem of how to update the probability between a point of time t and the next instant t &gt;. (If we were to assume a discrete temporal model we would sett &gt; = t+ 1.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">The structure of causal rules</head><p>Many proposals have appeared for reasoning about the dynamics of the world, variously called theories of time <ref type="bibr" target="#b22">[23]</ref>, theories of action <ref type="bibr" target="#b20">[21]</ref> or theories of change <ref type="bibr" target="#b25">[26]</ref>. The idea in all cases is the same: we identify events as the propositions that initiate change in the world; events change the state of facts.  <ref type="bibr" target="#b14">[15]</ref> ) represents the information that shooting somebody with a loaded gun usually causes that person to become dead (actually, "not alive"): 11</p><formula xml:id="formula_14">(DEFINE-RULE-SCHEMA ' GUN-SHOOTING ' (PRECONDITIONS (LOADED ?G))</formula><p>' (TRIGGER (SHOOT ?P ?G)) ' (CONSEQUENT (ALIVE ?P))</p><formula xml:id="formula_15">' (SIGN O) ' (EFFECTIVENESS O. 9) )</formula><p>A SIGN of 0 means the same thing as negating the consequent (recall that propositions cannot be negated). We will explain the EFFECTIVENESS parameter below.</p><p>These rules must be accompanied by axioms or algorithms that enforce the idea that the consequents indicated by the rules are realized at all appropriate time points--that is, for any point in time at which the TRIGGER event occurs and the PRECONDITION facts are all true, the CONSEQUENT fact is caused to be true (or false, in this case) immediately afterwards.</p><p>A first-cut interpretation of these rules is as follows: let R be a rule with precondition P, trigger event T, sign S, and consequent C. In the case S = 1 (a "positive rule") we can state, for all time points t, that e(ct&gt; IPt, Tt, Ct, E) = 1, (9) and similarly for S = 0 that</p><formula xml:id="formula_16">P(Ct&gt; I Pt, Tt, Ct, E) = 0, (<label>10</label></formula><formula xml:id="formula_17">)</formula><p>where E again is our previous body of evidence.</p><p>The equation says, in effect, that the probability that the (positive) rule will effect a change in C's state from false to true between time t and time t &gt; is 1. The equation is incorrect in that it assumes that the rule is a perfect predictor of state changes in C--that is, the rule's preconditions being true are sufficient to predict a change in C's state. The next section addresses this issue.</p><p>We can characterize a rule fully by its precondition, trigger, consequent, and sign, so let R = (pR, T R, CR, SR). We will be immediately interested only in rules for which C R = q~, for these are the only known conditions under which ~'s state can change.</p><p>Let us say that a rule "fires" at a point in time t if its precondition facts are all true at t, its trigger event occurs at t, and ~o is false at t for positive rules or true for negative rules. For positive rules R we will define and likewise for negative rules.</p><p>F S =</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">Incomplete theories</head><p>Equations ( <ref type="formula">9</ref>) and (10) imply that we know exactly the circumstances under which a rule's effects will be realized--in particular we assume that the rule's firing is sufficient to change ¢'s state. This may not always be the case: referring to the gun-firing rule, for example, it may be the case that shooting somebody with a loaded gun will usually, but not always, cause their death. We can express this notion--that the rule's stated preconditions are necessary but not sufficient for the rule to effect changeEby associating with each rule a parameter called "effectiveness". Effectiveness represents the proportion of situations in which the rule's consequent will be realized, given that it fires. Giving a rule an effectiveness parameter of p is equivalent to defining an additional precondition of" (CH,~CV. p)" which always has a probability ofp and is known to be independent of all other propositions. The independence assumption is justified here because we are quantifying the extent to which our model fails to capture certain relationships; presumably any systematic relationships would be captured in the model. The advantage of using an explicit EFFECTIVENESS parameter instead of a (CHANCE p) precondition is practical rather than semantically meaningful, so the present discussion will ignore the rule's effectiveness (assuming it is 1.0). (But see below, Section 4.1. )</p><p>Just as effectiveness represents an admission that the rule's precondition might not be restrictive enough (but we don't know exactly how to restrict it further), we might also want to represent the possibility that the precondition might be too restrictive (but we don't know exactly how to weaken it). We have no explicit parameter to represent this case--such information is coded in the application-supplied parameter A 9. `4 ~ is the probabiliBr that ~0 changed state, but the cause of that change is unknown. Section 3.4 discusses this parameter in more detail. The .4 ~ parameter does not distinguish between the case in which the unknown cause of ~o's change is a known rule that should have fired and the case in which the change's cause was not represented at all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">3. The update equation for causal rules</head><p>Now we return to the problem of tracking how belief changes from time t to time t &gt;, where our theory of change consists of a set of rules like the one above. Suppose that R 1, R 2 ..... R n are all the rules whose consequent is ~0, and the corresponding "firing" events are F 1, F 2 ..... F n. Assume for the sake of notational brevity that n = 2. </p><formula xml:id="formula_18">( F, 1 , Ft 2 , tilt F, 1 , F, 2, -f t ) P ( Ft 1 , F2, -f t Vt--S, Vt2, -f t ) e ( ~tl , FtZ, -ff t Vt 1 , Ft 2, -f t ) P ( Ft 1 , Vt 2, -f t Vt 1 , Ft 2, -fit ) P ( Ft 1 , Ft 2 , -f t E) + (11) E) + (12) E) + (13) E) + (14) E) +<label>(15)</label></formula><p>E) + ( <ref type="formula">16</ref>)</p><formula xml:id="formula_19">E) + (17) E). (<label>18</label></formula><formula xml:id="formula_20">)</formula><p>First consider the left-hand side of each addend--P(~0t, [Ftl,Ft2,~ot), P(~ot&gt; [Ftl,F2,~ot), and so on. These quantities divide into three groups; those in which none of the causal rules fire: ( <ref type="formula">14</ref>), <ref type="bibr" target="#b17">(18)</ref>; those in which exactly one of the rules fire: (12), ( <ref type="formula">13</ref>), ( <ref type="formula">16</ref>), <ref type="bibr" target="#b16">( 17)</ref>; and those in which more than one rule fires: ( 11 ), <ref type="bibr" target="#b14">(15)</ref>.</p><p>The first group represents the case in which nothing was known to cause ~o to change state between t and t &gt;. While we admit the possibility of unpredicted state changes over intervals (Section 3.4), we assume that the probability that such a change happens in the instant [t, t &gt; ] is negligible. We can therefore rewrite the left-hand sides of terms ( <ref type="formula">14</ref>) and ( <ref type="formula" target="#formula_19">18</ref>) as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P( t&gt; [ Ftl,F2, t) 1, e(¢,&gt; I F/,F?,-f,) O.</head><p>The second group of terms--( <ref type="formula">12</ref>), ( <ref type="formula">13</ref>), ( <ref type="formula">16</ref>), ( <ref type="formula">17</ref>)--represents the case where exactly one rule fires. Consider ( <ref type="formula">12</ref>) and ( <ref type="formula">16</ref>), for example:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P(q)t&gt; I Ftl,f2,(ot)P(Ftl,F2,~ot I E), P(fot&gt; [ FtI,F2,'UPt)P(Ftl,F2,'UPt I E).</head><p>The rule that does fire, R E, must either be a positive rule or a negative rule. If R a is positive we have directly from equation (9) that P(qh&gt; I Ftl,F?,gt) = P(~)t&gt; IF2) = 1.</p><p>Further, if R E is positive and did fire then ~a must have been false at t, so</p><formula xml:id="formula_21">P(F2 l ~ot) = 0 and P(F:,F2, ~, I E) = 0. Likewise, if R E is a negative rule, ?(fat&gt; I Ftl,F2,gt) = P(~0t&gt; IF2) = 0, P(Ftl,F2,gt I E) = O.</formula><p>The case of two rules firing simultaneously is trivial: R 1 and R 2 must either both be positive or both be negative, otherwise they could not both fire at the same instant. If they are both positive, then P(~o,,) = 1 since by assumption either is sufficient to make ¢a true; if both are negative, then</p><formula xml:id="formula_22">P(fpt &gt;) = O.</formula><p>Next we turn attention to computing the firing probabilities--the righthand sides of the causal update equation, for example</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P(Ftl,F2,~ot I E), P(Ftl,F2,-~t I E).</head><p>The questions to address are to what extent the rule firings interact, and to what extent rule firings depend on the previous evidence E.</p><p>First of all, as we hinted above, we will not worry about multiple rule firings: interacting positive and negative rules are not the issue here, so the danger is that we overestimate the probability of a state change by double counting evidence common to the firing of two positive (or two negative) rules.</p><p>The assumption that tends to mitigate this problem involves independence of event occurrences. In Section 2.3.2 we assumed events occurred "dispassionately" in the world: that the probability of an event occurring did not depend on that event's implications. We can formalize that assumption by assuming that an event occurs independent of the preconditions of the causal rules in which it participates. So, considering a positive role R I, we can apply these assumptions and write its firing probability as follows"</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P(Ft 1 I E) = P(Pt 1, Ttl,-~, I E) = P(Tt I I Ptl,-~t,E)P(Pt 1 I-~t,E)P(gt I E) = P(TtX)P(Pt I [ ~t,E)(1 -p),</head><p>where p is the estimate of ¢ we get from prior updates. The independence assumption on events allows us to write P ( Tt 1 I P:,-~t, E) = P ( Tt 1 ): we can assess the probability of events occurring separate from the rules in which they participate.</p><p>We will further make a Markov assumption with regard to the rule's precondition: that ~'s state at t renders all prior information irrelevant-that P(Pt ~ I ~t,E) = p(p1 I~t)-This assumption is questionable--later in the paper we will examine it in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.4.">Causal updates: summary</head><p>The assumptions above allow us to consider rule firings (relevant to ~o ) at a time point t to be mutually exclusive and exhaustive. That is, either exactly one of the rules fire, or none of the rules fire. Under these assumptions the causal update equation can be rewritten as follows: let • R + be the set of all positive rules whose consequent is ~0, • R-be the set of all negative rules whose consequent is ~0,</p><formula xml:id="formula_23">• F, + =" EReR+ P(Ft R [E),</formula><p>• F,-= ~,R~R-P( FR I E),</p><p>• p = P(~otlE), then ~o's probability at t &gt; can be computed by realizing that ~0 is true at t &gt; just in case it was true at t and no negative rules fired at t, or if ~0 was false at t and at least one positive rule fired at t:</p><formula xml:id="formula_24">P(~ot&gt; I E) = p(1-Ft-) + (1-p)Ft +. (<label>19</label></formula><formula xml:id="formula_25">)</formula><p>We will once again rewrite this update equation as a function of the prior probability p and relative to a time point ti, giving</p><formula xml:id="formula_26">ci(p) =p(1-Fi-) + (1-P)Fi +, (<label>20</label></formula><formula xml:id="formula_27">)</formula><p>where Fi + and F iare the firing probabilities for all positive and negative rules respectively, evaluated at ti. This equation, like equation <ref type="bibr" target="#b6">(7)</ref>, dictates how to update a probability given a new piece of evidence (in this case a causal rule instance). Note the difference in the forms of the equations, however: equation <ref type="bibr" target="#b6">(7)</ref> takes a prior at t and produces a posterior at t as well, whereas equation <ref type="bibr" target="#b19">(20)</ref> takes a prior at t and produces a posterior at t + 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Unpredicted changes</head><p>We discussed in the previous section the case of a causal theory in which a rule was not sufficient to predict a change in its consequent's state. The problem is the case in which all the rules are not necessary to predict a state change--that is, we must admit the possibility that a proposition might change even though no rule actually predicted that it would.</p><p>In the previous section we made the probably innocuous assumption that unpredicted change can't happen in the instant that passes between t and t &gt;, but it is unrealistic to expect the assumption to hold over longer periods of time, e.g. between times at which our sensors provide us with observations of the proposition. For example, parking one's car in a parking lot and returning three weeks later to pick it up requires that one admit the possibility that it may not still be there, even without being able to say with any precision how it might have disappeared. Leaving one's wallet on the New York subway even for a short period of time carries with it the strong possibility that its location has changed.</p><p>The point is that we may not have control over the times at which we get evidence about a proposition of interest, and therefore have to be able to reason about the possibility of change even in the absence of explicit evidence.</p><p>Assume now that we have a set of time points {t~, t2 ..... tn} such that ti "~ ti+l. Each ti will correspond to a point in time at which a sensor observation was received or at which a relevant causal rule was likely to have firedDin other words, a time point at which we can invoke one of the update rules above. For the sake of notational simplicity we will assume that the interval of time between t; and t~ is negligible, so we will speak of updating over the interval [ti, ti+l ] when we really mean [t~, ti+l ]. Recalling that ~i abbreviates ~t~, we can write this probability e(~i+l) = P(~i+I ]~i)P(~i) + e(~i+ll~i)e(~i).</p><p>(</p><p>P(~;) is the probability resulting from the previous observational or causal update. A state change between ti &gt; and ti+l is an unpredicted state change.</p><p>The usual approach to reasoning about probabilistic dynamic systems, e.g.</p><p>in <ref type="bibr" target="#b16">[17]</ref>, is to assume the probabilities e(~i+l I ~i) and e(tfli+l [ ~i) (called "transition probabilities") are available directly as inputs to the system. Usually this assumption is justified by controlling the amount of time that passes between any ti and the next ti+l. By making this interval short enough compared to the expected frequency at which ~ changes state, one can assume that ~ can change state at most once in any interval <ref type="bibr">[ti, t~+l ]</ref>.</p><p>Believing that ¢ changed state in an interval therefore implies that its state at ti+l is the opposite of what it was at ti, and of course believing that did not change state in the interval implies the belief that its state at li+l is the same as it was at ti. Predicting ~0's new state is therefore the same as predicting whether it changed state at all. When our view of assessment involves reasoning about the persistence of ~0 over longer intervals of time, however, 12 we invalidate the assumption that a state change amounts to a known transition. We may be reasonably sure that ~0 changed state at least once during a (long) interval of time, but still be unsure of ~0's state at the end of that interval.</p><p>Consider, for example, the problem of predicting whether a certain cat is currently in the living room. Suppose that it's now 4PM, and that it is known (by observation) that she was there at noon, four hours ago. We have no specific information that she has left (mainly because we don't have a very good causal model of her behavior), but we can be fairly certain at least that she has not stayed put for the whole time. That is, the probability of a state change over that four-hour period is very high. Does that mean she is not in the room? No. She might have left, then reentered, left, entered and left again, and so on. In fact, to the extent that it is likely she has left the room, our causal model loses its predictive power--an observation of her location at noon loses its predictive impact. At that point predictions must start to depend only on her "typical" behavior.</p><p>To represent this method of reasoning we introduce a new event into the probability space:</p><p>A? =--"~0 experienced at least one unpredicted state change 1,3 between ti and tj",</p><p>where ti and tj are two arbitrary time points such that ti ~_ tj and "unpredicted" means that no known causal rule firing explains the change. Then we rewrite the transition probabilities in <ref type="bibr" target="#b20">(21)</ref> to reflect the fact that ~o will be true at ti+l just in case (1) it was true at ti and nothing changed it between ti and ti+l, or (2) it was true at ti and changed, but ended up true, or </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P(¢i)(P(q~i+l I d~' )P(a~i+ [ q~i</head><formula xml:id="formula_30">) i,i + 1 , 1 + 1 -P (J~,i+ 1 I ~oi) ) ,~ ~ptA ~ ]~i) + (1-P(~i))P(q~i+l[~i,i+l, , i,i+l<label>(22)</label></formula><p>which depends on the two state-change parameters, one post-change parameter, and a prior on ~i. The equation says essentially that ~ will be true at ti+ t if and only if</p><p>• it was true at ti and there was no state change in the interim, or</p><p>• it was true at ti and there was a state change in the interim, but it ended up true anyway, or</p><p>• it was false at ti and there was a state change, and it ended up true.</p><p>Now for some ti we will summarize the necessary probability parameters </p><formula xml:id="formula_31">wi(p) = p(l -d +) + pd+bi + (1 -p)d~bi. (<label>23</label></formula><formula xml:id="formula_32">)</formula><p>It is often reasonable to assume that the two state-change probabilities are exponential and that the background probability is constant, in which case this equation can be reduced to an exponential with a single parameter (see Section 6). Our implementation makes the less restrictive assumption that background probabilities can depend on the contextual propositions but not on the temporal propositions themselves. It would therefore be more accurate to write the background probabilities explicitly in terms of the contextual propositions C, e.g. d + = P(A~,i+l[~Oi, C).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">5. The update process for fixed evidence</head><p>Recall the three update equations, corresponding to the evidential effects of sensor observations, causal rules, and "uncaused persistence" respectively: <ref type="bibr" target="#b24">(25)</ref> wi(p) = p(l -d +) + pd+bi + (1 -p)dTbi, <ref type="bibr" target="#b25">(26)</ref> which suggests the following algorithm for computing the probability P(~0n+l) based on a set of distinguished time points {ti} representing a set of causally relevant events that occurred at those times:</p><formula xml:id="formula_33">0.5(1 + ri)p ifoi is positive, rip + 0.5(1 -ri)' = (24) si (p ) ( 1 -ri )p if oi is negative. 1 + r -2rip' Ci(P) = p(1 -Fi-) + (1 -P)Fi +,</formula><p>Step 1. Compute a prior on ~0 at tl: P ~ P(~01).</p><p>Step 2. For i = 1,2 ..... n (a) compute and apply the appropriate update function <ref type="bibr">(si, ci )</ref> depending on what sort of event occurred at ti, e.g., P +--si(P) (b) "persist P forward" from ti to ti+l: compute w; for the interval [ti, ti+l] and let P 0--wi(P).</p><p>One can alternatively view this algorithm as a function AE (p), where E is the set of events that occurred at the {ti}. AE is the composition of appropriate instances of s, c, and w. The input parameter p is the prior P(q~tl). Note that AE(p) is monotone nondecreasing in p since all of its component functions are nondecreasing in their input parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1.">Conditioning on contextual propositions</head><p>We have left out one aspect of the update algorithm: the role of the contextual propositions. Recall that the equations that define the update algorithm depend on background probabilities (priors on the facts and events and probabilities for unpredicted state changes over intervals), and these background probabilities in turn depend on the contextual propositions. The same contextual proposition can figure in more than one background probability used in an update, so to avoid double counting we must explicitly condition on these propositions. Strictly speaking, then, we run the update algorithm once for each possible assignment of truth values to the contextual propositions that appear in the equations, weighting the result by the probability associated with that assignment. While this represents a potential explosion of computation time, in practice it has not been an issue: for all applications to this point (see Section 6 for example) contextual variables have not figured in the background probabilities at all. Generally a state-independent number suffices for a prior, and for the state-change probabilities we have been using an exponential function that depends only on the duration of the interval. Also note that since the states of contextual propositions do not depend on the temporal facts and events, we need calculate a probability distribution over those propositions only once.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">An equivalent graph structure</head><p>So far we have defined a model for computing probabilities for propositions that vary over time, stating this model implicitly in functional or algorithmic terms. We can also describe the model in terms of a graph <ref type="bibr" target="#b24">[25]</ref>: each node in the graph represents a quantity that can be assigned a probability and the arcs connecting nodes indicate a probabilistic dependency. Fig. <ref type="figure">3</ref> shows the structure for a simple example:</p><p>• There are two fact types, P and Q. • There are two event types, E and F. • There are two rules:</p><p>-RI: If E occurs when P is true, then Q becomes true (perhaps with some probability).</p><p>-R2: IfF occurs, then P becomes true (perhaps with some probability).</p><p>The graph shows the model's structure for a single update, between two time points, ti and ti+l. Recall that an arbitrary amount of time might separate ti and ti+l, but the expectation is that the system did not make any "interesting" state changes in the interim. Each row on the grid represents a proposition, each column represents a point in time. In addition to the facts, events, and rules we mentioned above we model the possibility that a fact is observed to be true (or false) and that an event is observed to have occurred (or not to have occurred). Each observation has an associated reliability parameter. The nodes in the rows labelled R1 and R2 represent the possibility that the rules fired at the corresponding time point.</p><p>The graph does not show the probabilistic parameters--the numbers that determine how a node's probability is computed based on the states of those nodes that point to it. This application supplies this information in the form of prior probabilities for those nodes with no incoming arcs. The application communicates whether an observation was made by providing a value of true or false or none for the appropriate obs(x) node, and also provides the reliability parameter r as a prior for the associated rel(x ) node.</p><p>The application also supplies priors for each fact and event proposition and for the state-change parameters ztx. These priors can all depend on the contextual propositions, so there should be an arrow pointing from the contextual propositions to all nodes with no other incoming arcs. (These arrows were omitted to unclutter the graph's diagram.)</p><p>The other relationships are determined by the equations developed earlier in the paper. Considering the graph from left to right, we see that updating occurs in four stages, each corresponding to one of the equations presented in this section:</p><p>• equation ( <ref type="formula">7</ref>) determines a fact's or event's state based on its prior state, whether it was observed true or false, and how reliable that observation was.</p><p>• equation ( <ref type="formula" target="#formula_24">19</ref>) determines whether a rule fired based on the states of its precondition facts and trigger event and on the rule's effectiveness parameter. • equation <ref type="bibr" target="#b19">(20)</ref> determines whether a fact will be true based on whether it was previously true and whether any of its rules fired. • equation ( <ref type="formula" target="#formula_31">23</ref>) determines whether a fact will be true based on whether it was true before and whether an unpredicted state change took place over the interval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.1.">Projected probabilities</head><p>We should note that although Fig. <ref type="figure">3</ref> illustrates the probability space and how elements of the space are related, our update algorithm is not the same as the Bayesian propagation algorithm (as described in <ref type="bibr" target="#b24">[25]</ref> for example).</p><p>The difference is that our algorithm computes probabilities only from earlier to later time points, so a proposition's state at time t can only have an effect on the system's state at times subsequent to t. A full propagation algorithm, on the other hand, also considers the effect of later evidence on earlier propositions. Suppose, for example, that P were observed to be false (fairly reliably) at time ti but was observed to be true (again fairly reliably) at a subsequent time ti+l. The full propagation algorithm would reason that P is probably true at ti+l and therefore that it must either have been true at time ti or it had to have been made true between ti and ti+l. The algorithm would therefore increase the probability that P was true at ti and also that the observation made at ti was unreliable. It would also increase the probability that one of P's causal rules fired at ti and that P changed state from false to true between ti and ti+l. These changes in turn would cause the probabilities of propositions at times prior to ti to change. In other words, the observation at ti+ 1 affects probabilities of propositions at ti and before.</p><p>Our algorithm determines P's state at ti, then at t; &gt;, then at ti+l. P would therefore be given low probability at ti &gt; based on the negative observation. That would provide a low prior for the observation-update stage at ti+l. That prior would be combined with the observational evidence at ti+l, tending to lower the posterior. But the algorithm does not then go back and revise its beliefs about P at ti based on the observation at ti+ 1--subsequent evidence has no effects on prior beliefs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">7. Time-based versus state-based models</head><p>Our equations so far tell us how to update probabilities given a set of "important" time points--those time points at which the system is likely to change state--but we have no way of reasoning about the system over some interval of time over which several changes might have occurred. In other words we have no way of identifying the important time points.</p><p>One easy way to model the system completely is to consider every time point to be potentially important. Suppose we want to project the course of the system over an interval of time [to, tn ]. One way to do so is choose some short interval of time, say J, short enough so that no event is likely to occur within any interval [t, t + J]. This interval is usually called a time mesh. We can then instantiate the model by letting the first important time point be to, the second be to + J, the third be to + 2J, and so on to tn, duplicating the graph in Fig. <ref type="figure">3</ref> at every time point. We then provide probabilities for the facts at the initial time point to and propagate that information through the graph. This method is equivalent to the usual method for reasoning about Markov random processes, and is similar to the approach suggested by Dean and Kanazawa <ref type="bibr" target="#b6">[7]</ref>. <ref type="bibr" target="#b12">13</ref> We call this approach a 13Also see Section 9.4 "time-based" approach to solving the system.</p><p>While this is an adequate formal model for solving the system it can be quite inefficient, for several reasons. First it requires reasoning about every time point whether or not anything changes between one time and the next. We would prefer to focus attention only on those times at which a change occurs and ignore those intervals (those parts of the graph) over which the system is stable. A time-based system forces us to reason about every time point, whether or not a change is likely. We therefore risk spending large amounts of processing resources propagating information through the model over intervals in which nothing happens.</p><p>Second, it requires reasoning about the state of every proposition whether or not the application program is interested in that proposition's state. We would prefer to focus attention only on those propositions that are of direct interest to the application (e.g. its goals) and to those that have a causal impact on those propositions. Third, it requires reasoning about the entire system using a level of granularity (time mesh) short enough to be appropriate for the fastest-changing proposition in the system. Reasoning about hummingbirds and boulders simultaneously requires thinking about whether the boulder has moved on a second-by-second basis.</p><p>The alternative to a time-based approach is to try to identify the propositions of interest and the time points at which they are likely to change, and instantiate only those parts of the model relevant to the specific reasoning task at hand. We call this approach a "state-based" approach because it focuses on significant changes in the system's state. To implement this approach we must identify what changes qualify as significant, and how likely they have to be to demand our attention. <ref type="bibr" target="#b13">14</ref> The next section attacks the problem of deciding which parts of the model to instantiate; in the language of the previous sections, this process can also be viewed as the process of identifying an appropriate set of evidence (observations, events, and rules) to reason about explicitly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Identifying important evidence</head><p>Analysis to this point has relied on the existence of a finite set of relevant events--sensory observations or causal rule instances--occurring at the ti points that together should determine the probability P(~On+l ). The problem remains, of course, how we collect these events in the first place.</p><p>14The statistics literature sometimes refers to the time-based approach as a "discrete-time" model and the state-based approach as a "continuous-time" model. We avoid these terms to head off confusion with the literature on temporal logics, in which discrete and continuous refer to whether time is represented by the integers or the reals.</p><p>Virtually any piece of evidence--sensory observation or causal rule instance mentioning ~o and occurring prior to tn--will have some impact on P (¢n+I). The number of actual pieces of evidence will certainly be enormous: we may have collected many sensory observations of ¢ over a long period of time, and we have to consider potential causal rule instances for every rule whose consequent is ~ at every point in time prior to tn+x. Of course if the firing probabilities of these rule instances are close to zero, we see from ( <ref type="formula">25</ref>) that they will have little effect on P (~).</p><p>The intuition that we can get away with examining only a few important events rests not only on the assumption that the important events are fairly decisive (have reliabilities or firing probabilities near 1.0, thus pretty much determine belief at that point in time) but also on the realization that we generally need only an approximation of the exact probability.</p><p>My decision whether or not to ride my bicycle to work, for example, may depend on whether or not I think it will be raining the next morning. Weighing the discomfort of getting wet against the inconvenience of having to find a parking place, I arrive at some threshold for the probability of rain, say 0.6, above which I will decide to drive. The threshold presumably comes from an analysis of the planner's current goals--see <ref type="bibr">[9]</ref>. At that point the exact probability of rain is of no interest to me; all that matters is what side of the threshold the true probability lies. And by implication, the only evidence of interest to me is some subset of the evidence that will convince me that the probability of rain is above or below that threshold. If, for example, I have a current estimate of the probability that is above the threshold, I needn't consider confirming evidence at all, since it can't possibly change my judgement relative to the threshold and thus will not affect the decision at hand.</p><p>Basically a piece of evidence can be ignored for one of three reasons: first, as above, it can be the wrong "sign"--it will only tend to confirm the current hypothesis. Second, it can be too unreliable. Recall from the update equations that sensory and causal evidence affect the estimate only to the extent that they are reliable, or likely to fire. An unreliable observation, for example, may not influence my belief enough to change it with respect to the threshold. Finally, a piece of evidence may be too remote: the farther in the past a piece of evidence occurs the more likely it is that an intervening state change will occur, which renders the evidence irrelevant. In trying to figure out where my car is, for example, I don't want to dwell on the fact that I saw it in my driveway two years ago.</p><p>All three criteria for ignoring evidence can be applied to the causal rules as well. Suppose that the query proposition is ~, and we have a rule that says event E and proposition P together cause P. Then we can compute how likely it must be for this rule to "fire" to affect our estimate of ~0, which in turn gives us bounds on how likely E's occurrence and P's truth must be to warrant our attention.</p><p>These limits suggest an algorithm for computing the probability of ~o at tn+l, relative to some threshold r:</p><p>Step 1. Start with no evidence, E = ~. (E will be the set of relevant events. )</p><p>Step 2. Estimate P (~On+l) using the algorithm from the previous section, that is, letting E = P(~0n+ 1 I E) = AE(.).</p><p>Step 3. IfE = z, perturb z slightly, letting (z .--r + e) or (z ,--z-e). <ref type="bibr" target="#b14">15</ref> Step 4. If E &gt; r, search for new evidence O (observations and rule instances) such that, if it were to be found, would change the estimate such that</p><formula xml:id="formula_34">E' = P((,Otn+l I E 0 O) &lt; v.</formula><p>Step Step 6. If no such evidence is found, report that P (~ot,+,) stands in relation to r as E does. Otherwise let E +--E t30 and go back to Step 2.</p><p>We stated the retrieval problem as that of finding a body of evidence O such that adding O to our current body of evidence E would change our estimate of ~0's probability with respect to the input threshold. In the process we might find that there is no such O, in which case we terminate.</p><p>Establishing that there is no such set requires, in the worst case, examining all possible evidence, which is exactly what we want to avoid: we would have to verify that there is no set O of size 1, of size 2, and so on, that invalidates our current estimate.</p><p>We therefore have to adopt a heuristic criterion for deciding that there is no such body of evidence (implying that our current estimate is correct). Subsequent sections adopt the following rule: we will terminate the assessment if we find no single piece of evidence that upsets the current estimate.</p><p>We base this policy on the observation that in many planning applications a single event, observation, or action often determines a proposition's state-rarely do several unlikely events conspire to make a proposition likely. The analysis in subsequent sections could instead use heuristics that consider evidence subsets of size 2 or greater; these heuristics tend to be more expensive to calculate and require searching more of the database, however, so more accurate policies carry with them a corresponding computational burden.</p><p>15Tbe rules below for bounding the search for evidence are based on z, and are undefined when E = z. In that case one might search arbitrarily carefully, since any piece of evidence, positive or negative would push E over or under the threshold z.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Bounding the reliability of evidence</head><p>We now confront the problem of how to identify the new evidence. In addition to considering evidence of the correct sign, we also want to limit consideration to evidence that is reliable enough, and recent enough, to change the current estimate.</p><p>First consider the reliability problem, and assume that E &lt; z, so we are looking only for positive evidence. The impact of a (single) piece of evidence--the extent to which it changes the probability estimate---can be measured by the appropriate s or c equation, but in either case the impact depends on the time at which that evidence is noted. It is generally the case that the more recent the evidence, the greater its impact. This will strictly be the case if the firing probabilities for the causal rules do not depend on ~0. So we can conservatively ask: how reliable must a piece of evidence, noted at tn+l, be in order that the new estimate E' would exceed r.</p><p>Consider the positive case for s, which measures the evidential impact of a positive sensory observation. We know that the current estimate of ~0 at tn+l is E. Setting the reliability parameter in equation ( <ref type="formula">7</ref>) to the desired level r*:</p><formula xml:id="formula_35">0.5(1 + r*)E &gt; z; r*E + 0.5(1 -r*)</formula><p>or, in order for a positive observation to be interesting it must have a reliability parameter that exceeds</p><formula xml:id="formula_36">T-E r* &gt; (27) E-2rE + z"</formula><p>If we were instead searching for negative observations we would look for E-r r* &gt; (28)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E-2rE + r"</head><p>Similarly for causal rules, we use equation <ref type="bibr" target="#b19">(20)</ref> </p><formula xml:id="formula_37">to get r-E f* &gt; 1 _----S-~, (<label>29</label></formula><formula xml:id="formula_38">)</formula><p>or for negative rules we use</p><formula xml:id="formula_39">f*&gt; 1-E. (<label>30</label></formula><formula xml:id="formula_40">)</formula><p>Recall that f is the probability that a particular rule fires at some time t, assuming that ~o is in the correct state. Note one detail here: we have not required that f be independent of the state of ~0 at that time, so strictly speaking we cannot use this formula for bounding f. In the case where a rule depends on a precondition ~, and C/ is true just in ease ~o is false, then f and ~0 are highly correlated and we will tend to underestimate the rule's evidential impact. We did assume, however, that the probability of trigger events occurring, and also the "effectiveness" of the rule, could both be assessed independent of ~'s state. So a conservative policy would be to consider all those rule instances for which</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>z-E P ( T ) z &gt; 1---Z-~E</head><p>where T is the rule's trigger event and X is the rule's effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Bounding the search's temporal scope</head><p>The final problem is to limit how far back in time we search for evidence. We ask the question: how far back in time would a perfectly reliable piece of evidence have to occur in order that if we were to find it (and it alone), it would change our estimate E with respect to z?</p><p>Recall that the current estimate E is based on the estimate AE (p), which we will abbreviate A (p). If currently E (p) &lt; z, we can ask two questions:</p><formula xml:id="formula_41">• what value p* will cause A (p*) &gt; z,</formula><p>• what is the latest possible time point t* -&lt; tl such that a perfectly reliable positive observation taken at t* would induce a probability at least as great as p*?</p><p>And of course the same holds if E &gt; z, except substitute "&lt;" for "&gt;" and "negative" for "positive".</p><p>The first question obviously depends on the form of the function A (p). We can most easily arrive at a p* value by using binary search and repeatedly evaluating A, since we noted that A is nondecreasing in p. The second quantity depends on the state change and background probabilities for ~o: d + , d-, and b. For certain forms of these functions we can compute exactly the time t* (see Section 6), otherwise we can compute t* by binary search as well, since zt~'. must increase as d(ti, t j) increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LJ</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Algorithm summary</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">I. Algorithm description</head><p>We can now restate the estimation algorithm from Section 4 as follows:</p><p>Inputs -A proposition ~, a time point t, a probability threshold z.</p><p>-A set of causal rule schemas such as the one described in Section 3.3.1.</p><p>-Probabilities concerning the occurrence of trigger events and on the background probabilities of facts at points in time.</p><p>-Sensory observations of various facts and events, each with an associated proposition, reliability parameter and time point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output</head><p>-An estimate E of P(~0t) with the property that ifE &lt; z then P(~ot) &lt; z, and ifE &gt; z then P(~ot) &gt; z.</p><p>-Monitors in the temporal database ensuring that the estimate will be recomputed if subsequent evidence calls this relationship into question (see next section).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Step 1. Initialize K = ~.</p><p>Step 2. Compute an estimate E = Ag (.), using the algorithm from Section 3.5.</p><p>Step 3. If E = z, then randomly set r ,--z + e.</p><p>Step 4. Determine reliability, firing, and recency bounds on evidence that would change E with respect to r, using, respectively, equation ( <ref type="formula">27</ref>) or <ref type="bibr" target="#b27">(28)</ref>, equation ( <ref type="formula" target="#formula_37">29</ref>) or <ref type="bibr" target="#b29">(30)</ref>, and the method of Section 4.2.</p><p>Step 5. Search through the temporal database for these new events.</p><p>Step 6. If none are found, report E as the estimate for P(~ot).</p><p>Step 7. Otherwise add the newly found evidence to K, and go back to Step 2.</p><p>The algorithm is heuristic in the sense that it considers only a single piece of evidence that would change its estimate in computing the reliability and recency bounds. The estimate may be incorrect only if several pieces of evidence considered jointly would change the estimate with respect to the threshold even though any piece of evidence taken alone would be too unreliable to do so.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Maintaining the belief." monitors and signals</head><p>We mentioned in the introduction that a belief, as computed by the algorithm above, also represents a promise to notify the calling program in ease subsequent evidence invalidates the relationship between the current estimate and the belief's threshold.</p><p>Providing this functionality is quite easy at this point in that the last search iteration (which yielded no evidence) also tells us what new evidence (in terms of sign, recency, and reliability) we should look for. Once the estimate is established we therefore establish add monitors in the temporal database (Section 2.2) telling it to notify the belief-computation algorithm if any such new evidence should appear. When new evidence arrives we recompute the estimate including that new evidence. If the estimate changes with respect to the threshold we call the application-supplied signal function associated with the belief; if the new estimate stands in the same relationship to the threshold as before, we re-compute the recency and reliability bounds and post new add monitors, but do not call the signal function.</p><p>We will devote the remainder of the paper to an example illustrating the algorithm, then to a discussion of the algorithm's implementation, the assumptions we have made, and related and future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Example</head><p>To illustrate the algorithm we will work through a simple example. The proposition of interest is CAT ----"the cat is now in the living room", and it is now 2:00 in the afternoon. There are two relevant sensory observations: at 9:00 this morning the cat was seen in the room. But at I:00 this afternoon a meow-like noise was sensed coming from the kitchen, which might (or might not) have been the cat. If it was, then the cat couldn't possibly have been in the living room at that time, but if the sound came from some other source, then it doesn't help me determine where the cat was. We estimate at 50% the probability that the noise really was the cat, therefore count the experience as a negative sensory observation with a reliability parameter r = 0.5.</p><p>Mainly the cat's behavior is inexplicable, but we do have one rule: when the postman comes he scares the cat out of the living room with probability about 0.6. Furthermore, the probability that the postman comes at noon is 0.8.</p><p>Finally, background knowledge about the cat leads us to estimate three parameters:</p><p>( 1 ) The probability that the cat will enter the room unobserved is roughly 1/2 in a 90-minute period. (2) The probability that the cat will leave the room unobserved is roughly 1/2 in a two-hour period. (3) The cat is in the room about 60% of the time.</p><p>We take the state-change probabilities to be exponential, with a parameter 2 such that the above half-lives are realized. That is,  where e (-0-0077)(90) = 0.5 and e (-0"00578)(120) = 0.5. (A time unit equals one minute. )</p><formula xml:id="formula_42">P(ztcAr I c-'A-T;) = 1 -e (-O'O077)d(i'j) I,J p(dCAr e(-O.OO578)d(i,J) i,j ICATi) ----1-</formula><formula xml:id="formula_43">P (POSTMAN12N) = 0.8 P(POSTMANt) = 0 (t #: 12N) Background: P (AcAT [ -c-~i) = 1 -e (-O.O077)d(ti,tj) t,J P (z~ CAT e(-O.OO77)d(ti,tj) . i,j ICATi) = 1-- P(CATj [ 3CAT)i,j " ~-0.6</formula><p>The problem is to estimate the likelihood of CAT at 2PM; we'll demonstrate with a probability threshold z = 0.4. Fig. <ref type="figure" target="#fig_7">4</ref> recapitulates the problem's parameters; Fig. <ref type="figure" target="#fig_8">5</ref> shows the exact probability of P(CATzpM) = 0.376. Analysis proceeds as follows:</p><p>Iteration 1 begins with no known observations. First we compute the current estimate, which is just the prior E = 0.6</p><p>Since E &gt; z, we search for negative observations. First we determine what the current prior would have to be in order to lower the current estimate (0.6) below the threshold (0.4). Since the current prior is the current estimate, the answer is obviously 0.4.</p><p>Next we find the time point t* such that a perfect negative observation taken at t* and persisted to 2PM would yield a posterior value at or below 0.4. Recall that if we have a perfect negative observation at t*, then P(CATt. ) = 0. We can then write We next have to compute the bound on r and f that will make a negative causal observation interesting. Recall that we do so by postulating a negative observation exactly at the query time 2PM. What can we say about the values of r and f that will lower the posterior probability to 0.4 or below? For sensory observations we can set the negative case of update equation <ref type="bibr" target="#b23">(24)</ref> equal to z and solve for r: That is, either a negative sensory observation with reliability greater than 0.38 or a causal rule instance with impact greater than 0.33 would cause us to change our mind and conclude that P (CATzpM) is actually less than 0.4.</p><p>We therefore initiate a fetch in the temporal database, searching for all negative sensory observations from II:40AM to 2PM for which r &gt;f 0.38 and for all negative causal observations in the same period for which f &gt;/ 0.33.</p><p>The search nets us k3--the negative sensory observation at 1PM, and also k2--the negative causal rule instance that occurred at t2N. Since we did find evidence as a result of the fetch, we add k2 and k3 to our set K of relevant events, and initiate another estimate/fetch cycle.</p><p>First we recompute the estimate E on the basis of the newly-enhanced K, which produces E = 0.354. And we find ourselves in the opposite position: the current estimate is below the threshold, so we have to look for positive observations that will put it back over 0.4. Now we recompute the reliability, recency, and firing bounds. First we consider E = A(p) where p is the prior at the earliest observation (in this case noon). Recall that A (p) is non-decreasing in p. We thus ask the question: how high above 0.6 would p have to be raised in order to cause A(p) to move from 0.354 to 0.4. We get the answer through binary search, repeatedly evaluating A to get p* = 0.88. Then we do a state-change analysis as above, asking the question of how recent a perfectly reliable positive observation would have to be in order to raise the probability at noon to at least 0.88. We also derive new reliability and firing bounds: r* = 0.1 and f* = 0.07. The resulting search for observations nets us nothing, however: we search only back to about l lAM, and thus exclude the 9AM positive observation. Since the search yields no new information we report that our current estimate can't change with respect to the threshold 0.4, and thus P (CAT2p~) &lt; 0.4.</p><p>Table <ref type="table">1</ref> shows the probability for all combinations of the three observations, and  <ref type="table">2</ref> note that we tend to do a lot of work if the threshold z is near the prior P (~0), and also if z is near the exact probability. The reflects two factors: a fixed cost associated with setting up and executing a fetch in the temporal database, and a variable cost associated with how far back in time the fetch covers. The magnitudes of these numbers reflect the actual implementation, of course--we find in <ref type="bibr" target="#b12">[13]</ref> that most of the time is spent in the temporal database manager module computing distances between time points, so efficiency gains in that module would improve these results considerably.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Assessing joint events</head><p>The assessment algorithm estimates the probability of simple propositions. We also need to compute probabilities for composite propositions, however, most critically for conjoined or joint events. Recall that a rule's precondition consists of a precondition, which may require that several conditions hold jointly in the world at the time the trigger event occurs. This section points out the difficulties associated with assessing joint events, and provides a method for dealing with one partic~ class of dependencies.</p><p>Suppose I want to assess the probability that my cat and my dog are both in the living room at the same time. Both tend to hang out there some fraction of the time (say 25% each), but the dog typically scares the cat out shortly after he enters the room, so rarely are they in the room toge~er for long. Probabilistically this information is usually conveyed by a conditional probability like P(CAT [ DOG) = 0.01 or some small number, which in turn implies P(CAT, DOG)= P(CAT [ DOG)P(DOG) = (0.01)(0.25) = 0.025.</p><p>Conditional probability constraints comment on interrelationships between propositions at a single point in time, while our focus to this point has been on the dynamic behavior of propositions considered in isolation.</p><p>The relationship between CAT and DOG actually summarizes the "typical" behavior of the theory, taking into account the existence of causal rules like "dog entering the room while cat is in the room causes cat to leave the room", the relative frequency of "dog enterings" to "cat enterings", other things that might scare the cat off, and so on. Put more directly, it says that if you were to view the system (look in the living room) at various points of time, under a variety of circumstances, only very rarely would you find the cat in the room if the dog was in there too.</p><p>But while we must take into account the typical, static behavior of the system, we must not at the same time ignore what we know about the situation at hand. What should we conclude if we were in the living room not 30 seconds before, and saw both animals in the room? (And what if they were both asleep?) In that case we should consider the possibility that this is one of the rare occasions in which the two animals coexist for a while.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">I. The nature of probabilistic dependencies</head><p>Given a causal theory consisting of causal rules and background probabilities, there are two ways that propositions (say (p and ~) might be interdependent:</p><p>(1) Through dependencies in the causal rules. ~ might be a precondition of a rule that affects ~,, for example, or the same event might tend to cause both propositions. Or perhaps the propositions share a rule precondition. In any case, by examining the structure of the causal rules we can identify those propositions that have an effect on both. (2) Through dependencies in the background probabilities. Recall that the state-change and post-change probabilities depend on the contextual propositions. ~ and ~ can be interdependent because their background probabilities are interdependent.</p><p>In the first case we rely on a "scenario" or "hypothetical future" structure generated by the projection algorithm to resolve the dependencies. We will formally introduce the scenario structure in <ref type="bibr" target="#b15">[16]</ref>; 16 here we will just hint at how such a structure can be used to avoid the over-or underestimation of joint probabilities in those cases where the propositions are interrelated by sharing causal rule components. We can solve restricted forms of the second problemminterdependent background probabilities--without reference to the scenario structure. We will sketch this solution later in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Dependencies in the causal rules</head><p>Returning to the cat and dog example, suppose that we have propositions CAT and DOG, and also the event E-DOG= "dog enters the living room".</p><p>The relationship between CAT and DOG shows up in the causal rules: the event E-DOG of course causes DOG. Furthermore, we have a rule that says the dog usually chases the cat out of the room: if CAT is true, then E-DOG causes CAT with some probability. If we compute P (CAT, DOG) = P (CAT)P (DOG) we will overestimate, failing to realize that the situations where DOG is likely are the ones in which E-DOG occurred, but those situations are those in which CAT is unlikely. The scenario structure makes these situations explicit, and reasons about each in isolation.</p><p>The way it works is that when P (CAT,DOG) is assessed, the system notices that the event E-DOG is relevant to both propositions. Then if, in the assessment process, it is asked to assess the probability that an E-DOG event occurs in a particular interval, it "splits the world" into two hypothetical situations, or chronicles, say Cl and c2. In Cl an E-DOG event actually occurs, and in c2 it does not. We can assign a probability to each chronicle. Then, reasoning within each chronicle separately, we notice that in cl the probability of DOG is high, but, since the "dog chases cat" causal rule probably fired, the probability of CAT is low. Likewise, in c2, the probability of DOG is low (since we have no evidence that he entered). The probability of CAT may be high or low, depending on whether we have other evidence to support CAT.</p><p>Since cl and e2 are mutually exclusive and exhaustive we can compute P ( CAT, DOG ) = P(CAT, DOG I Cl )P(cl ) + P(CAT, DOG I c2)P(c2) = P(CATICl)P(DOGICl)P(c l) + P(CATIc2)P(DOGIc2)P(c2) 16A18o~ <ref type="bibr" target="#b11">[12]</ref>.</p><p>which will turn out to be low. Note, however, that nowhere do we explicitly represent the probability P (CAT I DOG). We will call this style of reasoning, in which chronicles are introduced and assigned probabilities, "explicit reasoning by cases". Although it takes dependencies into account properly, it is computationally expensive: all probability calculations must be undertaken with respect to each separate chronicle, so frequent splitting may result in an exponential increase in the complexity of the probabilistic reasoning. We discuss this point further in Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Dependencies in the background probabilities</head><p>Recall from Section 3.4 that we allow the background probabilities to depend on the contextual (atemporal) propositions. Our algorithm (especially see Section 3.5) handles this case correctly: suppose we are jointly assessing P and Q, and their background probabilities both depend on a contextual proposition C. Further suppose that our current body of evidence is E. The algorithm correctly computes P(P, Q IE) = P(P, QIE, C)P(CIE) + P(P, QIE, C)P(CIE) (where P(CIE) = P(C) by assumption), thus avoiding the problem of double-counting the effect of C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Assumptions</head><p>The assessment algorithm embodies several assumptions both stated and unstated. They divide naturally into three groups: those concerning the temporal model, those concerning the causal rules, and those concerning the probabilistic model and its related calculations. Here we will summarize those assumptions and discuss the advisability and difficulty of relaxing them.</p><p>We made two assumptions regarding the temporal model:</p><p>• Instantaneous events. First we made a distinction between event propositions and fact propositions. Event types were then assumed to occur instantaneously (i.e. at a single point in time). We will relax this assumption in the projection algorithm, thus allowing the agent's actions to consume variable amounts of time. Relaxing the assumption is not problematic, except that the rules' syntax would have to be extended to indicate whether the consequent was realized when the event begins, or when the event ends, or at some indeterminate point during the event's execution. The third option makes the next assumption more problematic.</p><p>• Temporal separation known precisely. Although the temporal database manager is well equipped to handle imprecision in the time elapsed between time points, we assumed that the time separation among observations, and between the observations and the point of query, were known precisely. This assumption may be violated if we don't know exactly when an event's effects are realized. Relaxing this assumption turns out to be quite problematic. Imprecision in the temporal database is a problem only when it extends to imprecision in the ordering of events. If we are uncertain about the ordering of events we may at worst have to consider all possible total orderings in order to compute probabilities correctly. 17</p><p>Next we made two assumptions about the form of causal rules:</p><p>• Rule effects realized immediately. We assume that if a rule fires its consequent becomes true at the next instant in time. Relaxing this assumption to allow a delay is no problem except to the extent that the delay's length is imprecisely specified, once again bringing up the problem of imprecision in the ordering of events. • Events can't cause events. A rule's consequent can currently be a fact type but not an event type, which precludes causal chains---an exogenous event triggering a rule, which then triggers other rules subsequently.</p><p>This assumption allowed us to compute ~0's probability at t taking into account the world's state prior to t only.</p><p>All of these assumptions are common in the work on causal reasoning within a logical framework (e.g. <ref type="bibr" target="#b25">[26]</ref>). Finally we made the following assumptions about the structure of the probability space:</p><p>• Sensor reliability not systematically predictable. We assumed that an observation's reliability parameter is not correlated with other states of the world, in particular that it is independent of the assessed proposition ~0 and also of the preconditions of all rules used in assessing ~o. This assumption is crucial to our algorithm's behavior, but seems to be an intuitively reasonable one. If a sensor's reliability is correlated with other propositions involved in the assessment we can no longer guarantee a reliability bound, below which an observation has negligible evidential impact. Given the fight dependencies, even a very unreliable observation can have an arbitrarily high impact. • Rule preconditions are short-lived. This assumption indicates that a rule's precondition propositions could be expected to change state between 17See, for example, <ref type="bibr" target="#b4">[5]</ref>.</p><p>potential firings of the rule, and allows us to consider rule firings in isolation. We noted that rules representing an agent's intended actions, strung together in a plan, would typically violate this assumption, and this violation forces us to engage in explicit reasoning by cases. We confront this problem in <ref type="bibr" target="#b15">[ 16 ]</ref>. • Events occur dispassionately and predictably. We assumed that the probability with which an event occurs does not depend on the propositions that figure in the rule for which that event is a trigger. This assumption allowed us to bound the probability that a rule would fire. Obviously there are situations involving adversarial and cooperative behavior that will violate this assumption. We suspect, however, that probabilistic reasoning of the sort we have presented herein will be inappropriate in those situations anyway. Game theory and minimax reasoning provide a better formal framework. • Evidenceflows in a forward direction only. We mentioned in Section 3.6 the fact that our algorithm did not perform the usual Bayesian propagation of evidence forward and backward through its graph. This difference amounts to saying that we are doing temporal projection only, and manifests itself as the assumption that the evidence brought to bear in assessing P(~t) involves only events that occur temporally prior to t.</p><p>Our algorithm thus "walks forward in time" to compute probabilities.</p><p>The assumption means, for example, that one could simultaneously believe that ~ was very likely true at t, and that a very reliable sensor reported ~ false at t &gt;. We are currently exploring both the conceptual and the computational consequences of relaxing this assumption. Note, however, that the assumption is quite reasonable when applied to the problem of plan projection.</p><p>The last group of assumptions mainly allow us to avoid explicit reasoning by cases, as we defined it in Section 7.2--introducing hypothetical courses of events and reasoning about each in parallel. These assumptions allow us to summarize the computation's state, at each stage of the probability calculation, using a single number p which is passed from one stage to the next (as in Fig. <ref type="figure" target="#fig_8">5</ref>, for example).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Related work</head><p>Our survey of related work starts with the logical approaches to reasoning about change, then touches on several probabilistic approaches: Markov processes, expert-system frameworks, a rule-based approach, and a similar approach to approximate reasoning with probabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1.">Logical approaches</head><p>We will not attempt to summarize here the literature on logical reasoning about change--Shoham and Goyal <ref type="bibr" target="#b26">[27]</ref> provide a good starting point. Our formulation most closely resembles Shoham's <ref type="bibr" target="#b25">[26]</ref> causal theories, to which we have added probabilistic information in various places. Our algorithm for computing probability given a fixed set of information corresponds to Shoham's definition of a chronologically minimal model. 18</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2.">Markov processes</head><p>Before discussing alternative proposals in the AI literature we should mention briefly the literature on Markov processes, e.g. <ref type="bibr" target="#b16">[17]</ref>. The idea is to represent the domain by a state space, in which every state represents a complete static description of the world (or that portion of it under study). Change is described by a transition matrix: for a domain with n possible states a transition matrix is an n x n matrix whose (i,j)th entry contains the probability that the system will be in state sj at time t + J given that it was in state si at time t. We can then compute a probability distribution over possible states at time n by (1) providing a probability distribution over world states at the initial time to, and (2) multiplying this vector n times by the transition matrix.</p><p>While the representational power of these systems is the same as for our symbolic graphical models (see <ref type="bibr" target="#b23">[24]</ref>, for example), the problem with the standard Markov-process representation is that it is inconvenient. We are generally more comfortable building symbolic state descriptions than defining the system's state space as a whole, and can more easily define our causal theory using symbolic rules of change than using numeric transition matrices. Our rule-based representation is simply more appropriate for the larger task of maintaining a world model that will be used for plan generation, refinement, and repair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">3. Expert-system approaches</head><p>Cooper et al. <ref type="bibr" target="#b2">[3]</ref> confront the problem of probabilistic diagnostic reasoning, in particular determining the probability that a patient has a certain disease given that he has displayed a certain set of observable symptoms (evidence). The temporal dimension enters in that the system processes probabilities like the likelihood that a disease persists to the present given that it began at some time tx and the likelihood that a particular profile tSIn fact it corresponds exactly to chronolos/cal minimality in the case that (1) all priors arc either 0 or I, (2) all rules have an effectiveness parameter of I, (3) all observations have reliability I, and (4) the probability of an unobserved state change over an interval (the J¢ parameter) is O, of symptoms would be observed between tx and the present given that the disease began at that time.</p><p>Apart from the explicit mention of time and probabilities, there is little similarity between this work and our own. Our work is oriented primarily toward two goals: (1) to use an internal causal model (rules and event probabilities) to predict future states of the system, and (2) to limit inference in a manner appropriate to the particular problem by instantiating only relevant parts of the model. Cooper's system addresses neither of these issues. In the first place, the probabilities relate diseases and symptoms (evidence) directly, and diseases are assumed to be mutually exclusive. Therefore there is no way to reason about a scenario in which disease dl begins at time tl, eventually causes dE at some later time, and so on. In fact Assumption 7 explicitly rules out this sort of reasoning by saying essentially that the probability that disease di begins at tx does not depend on the patient's state prior to time tx.</p><p>Decisions as to what evidence to consider are made by the system designer ahead of time rather than by the system itself dynamically. The system builder fixes a time granularity and a time horizon. The system then considers all evidence occurring within the horizon. There is no explicit model of noisy evidence (in fact Assumption 5 states that the values of all evidential variables are known at all times), and there is no internal causal model corresponding to our rules, therefore no way for the system to ignore evidence that is too unreliable or too far in the past, nor is there any way to abandon causal chains as their inferences become too tenuous.</p><p>Berzuini's work--[1,2]--likewise provides a discussion of how an expert might build a graphical model that explicitly describes the time course of the system. The (human) expert describes the domain's causal structure by providing the model's state space and transition probabilities directly.</p><p>An interesting aspect of Berzuini's work is his adoption of a state-based temporal model. He suggests a continuous-time semi-Markov process to represent situations in which temporal information (when events occur, how long do propositions remain true) is imprecise, but does not provide a method for building such a model automatically. A promising area for future research is to establish the relationship between these semi-Markov models and symbolic rule-based theories in which a rule's consequent has an indeterminate delay and an event's time of occurrence is uncertain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">4. Rule-based approaches</head><p>Dean and Kanazawa <ref type="bibr" target="#b5">[6 ]</ref> present a probabilistic model essentially identical to the one that we built in Section 3.6: they too take the idea that causal rules are triggered by events and effect changes in facts. Their model has no explicit model of noisy observations, but it could easily be added.</p><p>Where the work differs from ours is in the method for building the full network. Dean and Kanazawa take the "time-based" approach: they choose a time increment J and instantiate the model at each time point, to, to + J, to + 2J ..... They suggest using approximate simulation methods to solve the network, but provide no insight into the time required to guarantee convergence. <ref type="bibr" target="#b18">19</ref> Tatman and Shachter <ref type="bibr" target="#b27">[28]</ref> propose an alternative approach that involves applying dynamic-programming techniques.</p><p>There are thus two fundamental differences between the work of Dean and Kanazawa and our own: they rely on a time-based approach (see Section 3.6), requiring them to choose an appropriate time mesh and instantiate the model at all time points, whereas we try to instantiate the model only at those time points at which the system changes significantly. Second, they propose a forward simulation solution technique that does not take into account the specific inferences required by the problem solver. The simulation solves the entire network (approximately), computing the probability of every proposition at every point in time. We try to reason only about those parts of the network that are of interest to the problem solver and only to the degree of accuracy it requires.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.5.">Belief updating by assessing impact</head><p>Weber's work <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref> is a computational theory of how to form beliefs based on statistical information. The idea is that estimating the likelihood of some proposition (say whether my car will start) suggests the problem of what information is relevant and significant to the prediction. One may have statistical information about how the car starts under a variety of circumstances: when it is cold, wet, cold and wet, parked on the street as opposed to in the garage, on weekdays as opposed to weekends, in the summer as opposed to in the winter, and so on. Some of these factors will be irrelevant, some will be redundant; as a computational matter it may be infeasible to consider all possible information.</p><p>Suppose we are estimating the likelihood of a hypothesis h, and have currently considered a set of evidence r. Now we consider whether to incorporate a new proposition f into the conditioning set--i.e, our new estimate of h will be based on r n f instead of on r. Weber defines a quantity called the impact of f on h given r, which is related to the ratio of the relative frequency of f given h N r to the relative frequency of f given N r. High positive values mean that f offers strong positive evidence for h in the context of r; high negative values indicate strong evidence that h 19Kanazawa <ref type="bibr" target="#b19">[20]</ref> in subsequent work explores the use of state-based approaches for temporal reasoning, but does not address the problem of building the graphical network from a set of rules. is false, and impact of 0 means that incorporating f will not change the estimate.</p><p>Weber's algorithm takes as input a proposition and a number a called accuracy, and performs the following loop:</p><p>(1) Initial estimate is h's prior; initial r is ~J.</p><p>(2) For all evidence f not in r, compute the impact of f on h given r.</p><p>(3) r ,--r U f* where f* is that single piece of evidence with the highest impact (in absolute value). ( <ref type="formula" target="#formula_6">4</ref>) Recompute estimate on the basis of new evidence set r (5) If limpact (f*)[ &lt; a, then terminate and return current estimate. This algorithm is similar in spirit to our own (although it is not applied explicitly to temporally qualified propositions) in that it tries to consider only that set of evidence relevant to a particular probabilistic assessment task. He also adopts the heuristic of considering only a single piece of evidence in deciding when the algorithm should terminate.</p><p>One difference is that Weber uses an accuracy value instead of a threshold. Accuracy a defines an interval [e -a, e + a ] where e is the estimate returned by the algorithm, a is the input accuracy value, and the interval is guaranteed (or very likely) to contain the exact probability. 20 Deciding degree of belief with respect to a threshold z amounts to asking which of the intervals [-c~,z] and [z,~] the exact probability lies in. <ref type="bibr">Haddawy and Hanks [9]</ref> show a decision-theoretic justification for queries of this form. One advantage of the threshold approach is that it allows us to assess the impact of certain pieces of information extremely quickly: if the current estimate is over the threshold we can ignore all positive evidence, even strongly positive evidence. Weber's algorithm would continue incorporating positive evidence, even though it would only continue to confirm the current hypothesis.</p><p>Weber <ref type="bibr" target="#b29">[30]</ref> also provides classes of "statistical causal rules" that correspond to the causal information we code in causal rules, observations, and background probabilities. He does not go on to explore whether his highest-impact-first algorithm would provide an effective inference mechanism for doing predictive inference on these theories (although he expresses pessimism in Chapter 8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I0. Conclusion</head><p>An agent's model of the world has to take into account the world's propensity to change and the agent's ignorance about its state. Doing so 2°Things are a little more complicated than that because the updates are defined in terms of logarithms.</p><p>than, an application-supplied threshold. By doing so we can consider only that evidence significant enough to change our estimate with respect to the threshold, ignoring sufficiently unreliable or old sensory evidence, and abandoning tenuous lines of causal inference.</p><p>Another way to view the work is that it takes a causal theory that defines a graphical structure over the system's state space, implicitly quantified over all time points. Instead of trying to evaluate the structure at all times the algorithm tries to identify the times at which the system changes state in relevant ways, and instantiates the graph only at those points.</p><p>Future work in this area takes four paths:</p><p>(1) Projecting plans. As we mentioned, various assumptions we made in this theory tend to be violated regularly when the events encountered comprise a deliberate plan. Relaxing these assumptions involves more reasoning by cases, which tends to be computationally expensive. Additionally, planning involves reasoning about a richer collection of phenomena than can be conveniently modeled using our restriction of rule consequents to binary random variables. We report on this work in <ref type="bibr" target="#b15">[ 16 ]</ref>.</p><p>(2) Relaxing structural assumptions. Relaxing the "projection" assumptionmthat all evidence bearing on ¢'s state at t occurs prior to t--complicates the probability computation, but makes the algorithm amenable to a wider variety of tasks, such as failure diagnosis and replanning. Adding the idea of "induced causality" (rules causing other events to occur, thus generating causal chains) and of "domain constraints" (allowing states of propositions to interact by constraints other than the causal rules) makes the system amenable to reasoning about complex physical systems.</p><p>(3) Incorporating temporal imprecision. We have assumed that the temporal distance between any two points is known precisely, but temporal information can be incomplete and inaccurate in exactly the same sense as can propositional information. We need to integrate probabilistic information about time, e.g. the amount of time an action will take, into our model. Conceptually there is no difficulty; the trick will be to do so in a manner that preserves fast computation.</p><p>(4) Planning, replanning, and learning. We have yet to integrate the model manager into a planning system. Interesting questions arise as to how a planner might use decision-theoretic methods to guide the process of plan selection [10], and how that process affects the interface between planner and world model. Then when the plans are actually executed we confront the problem of how to deal with failure--failure of the world to conform to the agent's model, and thus failure of the plan to achieve its intended effects. Replanning is the problem of how subsequent actions should change to cope with the failure; learning involves how the agent's world model--probabilities, rules, and plan librarymshould be modified to prevent subsequent failures.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The world model imbedded in an agent architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>5 Fig. 2 .</head><label>52</label><figDesc>Fig. 2. Graph representation for observation updates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Furthermore, P(O I R,~o) = P(O I R,-~) = f, the failure-mode parameter we introduced in equation (4). And, of course, P(OIR,-~) = O. Hence, the equations above reduce to e(o[ (0) = r + (1r)f, P(OI~) = (1r)f. Subtracting, we obtain r = e(o I ~o) -e(o I g), which establishes a relationship between r and the components of the likelihood ratio 2o. Considering P(O) yields the corresponding equation, r = e(o[ ~) -e(o[ ¢).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>it was false at ti and changed and ended up true: e(~oi+l [ ~Oi) = P(tPi+l [ ~Ot, ZJi,i+ ~ )P(A~.i+l [ + P(tPi+I I ~0i, A ~ )e(Zi+ll~Oi) i,i + 1 e(~0i+l [ ~ )P(A~.i+llq)i) = ~ot, Ai,i+ 1 , + 1 -P(A~,i+l[cpi), P(~°i+l l-~i) = P(~0,+l I -~0i, A~i,i+l)P(A~.i+, 1 I ~i) + p(tpi+ 1 [ ~i, ~oi,i+l )e(~i,i+ll~i)) = --A ~ )P(A~i+ll-~i ). P(~°i+l l ~°i , i,i+l 12Recall that we have no a priori bound on the amount of time that might elapse between a point t i and the next distinguished time point ti+ 1 . The parameters P(A~,i+ 1 [ ~oi) and P(A~i+a l -~ i) are called the state-change probabilities for ~0. If we assume that ~0's state at ti is irrelevant to its state at t~+~ assuming a state change took place in the interim, we can rewrite P(~i+l l ~Oi, A~,i+ I) = P(foi+i I ~i,A~i+l) which we call the post-change probability for ~. The state-change and postchange probabilities together we call the background probabilities for ~o. Now note that ~,i+1 represents a situation in which no state change, predicted or unpredicted, occurs between ti and ti+ 1 and therefore by definition m~ P(~oi+l [ ~oi, Ai,i+l) = 1, e(~0i+l [~oi, ,,,+1) = 0. The new equation for updating probabilities over intervals then becomes: P(~oi+l) = e(tPi+l I ~ot)P(~oi) + P(~oi+l [ ~i)(1 -e(~oi)) = (e(~0i+l [ A'~. ~oi)e(A~.i+l I ~°i) l,l+l' + P(~0i+l I A ~ ~oi)e(-~i.i+l I qgi))P(~oi) i,l+ l, + (P(~i+l [ ~ --zJi,i+l, ~°i)P('~i,i+l [ ~i) -b e(~0i+l I Ji,i+l,q)i)P(3i,i+l ] ~i) )P(~i) = (e(~0i+l I 3~-1)P(A~i+I [~0i) + (1)(1 -e(A~,i+ll~oi)))P(~o~) a t-(e(q~i+l [J~Pi,i+ 1 )P(J~i+ 1 I~i) + (0)(1 -P(A~,+I I Wi)))(1 -P(~,,))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>to write the persistence update equation once again as a function of an input parameter p = P (~oi):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Summary of cat example's parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Summary of cat example's scenario.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>P</head><label></label><figDesc>(CAT2p M I C--"~t. ) P(CAT2p M I A CAr ~/3[ACAT = --t*,2PM'--',"t*,2PM I C--"~t* ) = (0.6) (1 -e (-°'°°77)d(t''2pM)), and set the left-hand side to the threshold, 0.4 = 0.6 (1 -e (-0"0°77)d(t*,2PM)), 0.667 = 1 -e (-0"0077)d(t*'2PM), ln(0.333) d(t*,2PM) --0.0077 ' d(t*,2PM) = 142, t* ~ 11 : 40AM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>and setting the prior parameter p to the current estimate 0.6 and z to the current threshold 0.4: 0.6 -0.4 F* ~---0.4 + 0.6-2(0.4) (0.6)' r* = 0.38.Similarly for negative causal observations, we set z = 0.4 and p = 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>At*,12N ) P ~t*,12N I CATt ) ,-.-;-CAT ----:-CAT + P(CAT|2N [ CATt, At.,12N)P(At.,12N l, )CATt 0.88 = (0.6) (1 -e (-°'°°578)dCt*,12N)) + ( 1 ) (e (-°'°°578)d(t''~2N)), 0.88 = 0.6 + 0.4e (-°'°°578)d(t*,12N), d(t*, 12N) --ln(0.28/0.4) -0.00578 ' d(t*,12N) = 61.7, t* ~ 11AM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>The agent's world model consists of a set of causal rules that dictate what event types cause what changes to what fact types under what circumstances. One such rule (a variant of which appears in</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>The following causal update equation indicates that ~o's state at t &gt; can be determined by considering its state at t and considering what happens under the (mutually exclusive and exhaustive) circumstances that (a) neither rule fires, (b) one rule fires but the other does not, and (c) both rules fire.</figDesc><table><row><cell>P(~ot&gt; I E) = P(~ot&gt;</cell><cell>Ft 1 , Ft 2, ~o t ) e (51 , Ft 2, ~O t</cell></row><row><cell>P(~ot&gt;</cell><cell>Ft 1, Ft 2, ~ t ) P ( Ft 1, Ft 2, ~O t</cell></row><row><cell>P(~ot&gt;</cell><cell>Ft 1, Ft 2, ~o t ) P ( Ft 1, Ft 2, ~o t</cell></row><row><cell>e ( ~ot&gt;</cell><cell>Ft 1 , Ft 2, ~ t ) P</cell></row><row><cell>P(~ot&gt;</cell><cell></cell></row><row><cell>e ( ~ot&gt;</cell><cell></cell></row><row><cell>P(~ot&gt;</cell><cell></cell></row><row><cell>P((at&gt;</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Table 2 indicates the amount of work that would need to be done in order to process a number of different thresholds. From Table 1 note that</figDesc><table><row><cell>Table 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Probabilities for different collections of evidence</cell><cell></cell></row><row><cell>0</cell><cell>10.6</cell><cell></cell><cell>kl, k2</cell><cell>10.530</cell><cell></cell></row><row><cell>kl k2</cell><cell>10.671 0.498</cell><cell cols="2">[kl, k3 Ik2, k3</cell><cell>10.511 0.354</cell><cell></cell></row><row><cell>k3</cell><cell>0.442</cell><cell></cell><cell cols="2">kl, k2, k3 0.376</cell><cell></cell></row><row><cell>Table 2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Process time as a function of threshold</cell><cell></cell><cell></cell></row><row><cell cols="2">Threshold No. pieces</cell><cell cols="4">Time Threshold No. pieces Time</cell></row><row><cell></cell><cell>evidence</cell><cell>(sees)</cell><cell></cell><cell cols="2">evidence (sees)</cell></row><row><cell>0.0</cell><cell>0</cell><cell>0.00</cell><cell>0.5</cell><cell>2</cell><cell>2.50</cell></row><row><cell>0.1</cell><cell>0</cell><cell>0.75</cell><cell>0.61</cell><cell>3</cell><cell>5.90</cell></row><row><cell>0.2</cell><cell>0</cell><cell>0.85</cell><cell>0.7</cell><cell>0</cell><cell>1.55</cell></row><row><cell>0.3</cell><cell>0</cell><cell>0.95</cell><cell>0.8</cell><cell>0</cell><cell>1.05</cell></row><row><cell>0.36</cell><cell>3</cell><cell>5.50</cell><cell>0.9</cell><cell>0</cell><cell>0.80</cell></row><row><cell>0.4</cell><cell>2</cell><cell>3.05</cell><cell>1.0</cell><cell>0</cell><cell>0.00</cell></row><row><cell cols="6">we were justified in ignoring observation k1: adding it to the set {k2,k3}</cell></row><row><cell cols="6">does not change the estimate's position with respect to the threshold 0.4.</cell></row><row><cell>From Table</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>u This is the actual LISP form evaluated by the implementation.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>Thanks to Denise Draper, Oren Etzioni, and Dan Weld, who commented on several drafts. David Madigan improved the presentation as well. The referees made careful and helpful comments on an earlier version. This work was supported in part by DARPA grant DAAA15-87-K-0001 (Yale University) and by NSF grant IRI-9008670 (University of Washington).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>involves integrating a theory of change with a theory of uncertainty. In this paper we developed an algorithm for computing the projected probability of a proposition (O at a point in time t based on a body of evidence known as a probabilistic causal theory. A causal theory consists of:</p><p>(1) sensory observations---reports from some external sensor about (o's state at some point in time, (2) rules governing change--statements that certain conditions in the world, namely the occurrence of a trigger event in the presence of a certain precondition, will change (o's state, (3) probabilities quantifying confidence in the model----expressing the likelihood that the changes predicted by the rules above are sufficient to predict all changes in q's state.</p><p>This system represents an advance over deterministic theories of change <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26]</ref> both due to its greater expressive power and due to its ability to limit its inference according to the demands of the task at hand.</p><p>The gain in expressive power has to do with enriching the notion of "persistence" or "inertia" captured by the deterministic systems, to include the following notions:</p><p>(</p><p>(2) that if we reason about the state of (O over a long interval of time, we may be reasonably sure that (o's state changes during the interval, even though we have no explicit evidence that it has changed, that our causal model of (O may be incomplete, failing to predict the occurrence of causally relevant events, failing to express correctly the rules that govern (o's changes over time, or failing to capture the exact circumstances under which a particular known rule will actually effect a change.</p><p>Our system for probabilistic projection captures all these concepts, using the following information:</p><p>(1) the known ways in which (o's state can change (the causal rules), (2) the extent to which each rule correctly captures a particular causal relationship (the rule's effectiveness parameter), (3) the extent to which we believe that our causal model is complete (the /1¢ parameter).</p><p>In addition to this increased expressive power--the ability to quantify the perceived accuracy of our causal modelwthe probabilistic approach affords us the advantage of limiting our inferential effort in those cases in which approximate answers will suffice. Instead of computing (o's exact probability at t, we instead decide only whether that probability is less than, or greater</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Temporal reasoning with probabilities</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bellazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Berzuini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Quaglini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Fifth Workshop on Uncertainty in Artificial Intelligence</title>
		<meeting>Fifth Workshop on Uncertainty in Artificial Intelligence<address><addrLine>Windsor, Ont</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<author>
			<persName><forename type="first">C</forename><surname>Berzuini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Representing time in causal probabilistic networks</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Henrion</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Shackle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">N</forename><surname>Kanal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Lemmer</surname></persName>
		</editor>
		<meeting><address><addrLine>Amsterdam, Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="15" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A method for temporal prohabilistic reasoning</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Horvitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Heckerman</surname></persName>
		</author>
		<idno>KSL 88-30</idno>
		<imprint>
			<date type="published" when="1988">1988</date>
			<pubPlace>Stanford, CA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Knowledge Systems Laboratory, Stanford University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Report</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Temporal imagery: an approach to reasoning about time for planning and problem solving</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Dean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985">1985</date>
			<biblScope unit="volume">433</biblScope>
			<pubPlace>New Haven, CT</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, Yale University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. Thesis</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Reasoning about partially ordered events, Artifi Intell</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Boddy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="375" to="399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Probabilistic temporal reasoning</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kanazawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings AAAI-88</title>
		<meeting>AAAI-88<address><addrLine>St. Paul, MN</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A model for reasoning about persistence and causation</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kanazawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Intell</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="142" to="150" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Diagnosing multiple faults</title>
		<author>
			<persName><forename type="first">J</forename><surname>De Kleer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. lntell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="97" to="130" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Issues in decision-theoretic planning: symbolic goals and numeric utilities</title>
		<author>
			<persName><forename type="first">P</forename><surname>Haddawy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hanks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Innovative Approaches to Planning, Scheduling, and Control</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Representations for decision-theoretic planning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Haddawy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hanks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Third International Conference on Knowledge Representation and Reasoning</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
	<note>utility functions for deadline goals</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Hanks</surname></persName>
		</author>
		<title level="m">Proceedings Third Workshop on Uncertainty in Artificial Intelligence</title>
		<meeting>Third Workshop on Uncertainty in Artificial Intelligence<address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
	<note>Temporal reasoning about uncertain worlds</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Practical temporal projection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hanks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings AAAI-90</title>
		<meeting>AAAI-90<address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Projecting plans for uncertain worlds</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hanks</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<pubPlace>New Haven, CT</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, Yale University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. Thesis</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Issues and architectures for planning and execution</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hanks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Firby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Innovative Approaches to Planning, Scheduling, and Control</title>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Nonmonotonic logic and temporal projection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hanks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcdermott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="379" to="412" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Modeling a dynamic and uncertain world II: action representation and plan evaluation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hanks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcdermott</surname></persName>
		</author>
		<idno>93-09-07</idno>
		<imprint>
			<date type="published" when="1993">1993</date>
			<pubPlace>Seattle, WA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of Washington</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Report</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dynamic Probabilistic Systems</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Howard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Markov Models</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1971">1971</date>
			<biblScope unit="volume">I</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Causality and maximum entropy updating</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hunter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Approximate Reasoning</title>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Where do we stand on maximum entropy?</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">T</forename><surname>Jaynes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Maximum Entropy Formalism</title>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Levine</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Tribus</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A logic and time nets for probabilistic inference</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kanazawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings A4AI-91</title>
		<meeting>A4AI-91<address><addrLine>Anaheim, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Formal theories of action</title>
		<author>
			<persName><forename type="first">V</forename><surname>Lifschitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Frame Problem in Artificial Intelligence: Proceedings of the 1987 Workshop</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Brown</surname></persName>
		</editor>
		<meeting><address><addrLine>Los Altos, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Applications of circumscription to formalizing common sense knowledge</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mccarthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Readings in Nonmonotonic Reasoning</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Ginsberg</surname></persName>
		</editor>
		<meeting><address><addrLine>Los Altos, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1986">1986. 1987</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="153" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A temporal logic for reasoning about processes and plans</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mcdermott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cogn. Sci</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="101" to="155" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Relationship between temporal Bayes networks and Markov random process transition tables</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">I</forename><surname>Mensinger</surname></persName>
		</author>
		<author>
			<persName><surname>Nunez</surname></persName>
		</author>
		<idno>CS-89-M2</idno>
		<imprint>
			<date type="published" when="1989">1989</date>
			<pubPlace>Providence, RI</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, Brown University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Report</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<title level="m">Probabilistic Reasoning in Intelligent Systems</title>
		<meeting><address><addrLine>Morgan Kaufmann, Los Altos, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Reasoning about Change and Time from the Standpoint of Aritificial Intelligence</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shoham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Shoham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<title level="m">Temporal reasoning in artificial intelligence</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Shrobe</surname></persName>
		</editor>
		<meeting><address><addrLine>Los Altos, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
	<note>Exploring Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dynamic programming and influence diagrams</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Tatman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Shachter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst. Man Cybern</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="365" to="379" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A parallel algorithm for statistical belief refinement and its use in causal reasoning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Weber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings IJCAI-89</title>
		<meeting>IJCAI-89<address><addrLine>Detroit, MI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Principles and algorithms for causal reasoning under uncertainty</title>
		<author>
			<persName><forename type="first">J</forename><surname>Weber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of Rochester</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Report 287</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Wellman</surname></persName>
		</author>
		<idno>MIT/LCS/TR-427</idno>
		<title level="m">Formulation of tradeoffs in planning under uncertainty</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1988">1988</date>
		</imprint>
		<respStmt>
			<orgName>MIT Laboratory for Computer Science</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
