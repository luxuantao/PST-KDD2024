<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Zero-Shot Hashing via Transferring Supervised Knowledge</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yang</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
								<address>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yadan</forename><surname>Luo</surname></persName>
							<email>lyadanluol@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
								<address>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Weilun</forename><surname>Chen</surname></persName>
							<email>chenweilunster@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
								<address>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fumin</forename><surname>Shen</surname></persName>
							<email>fumin.shen@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
								<address>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jie</forename><surname>Shao</surname></persName>
							<email>shaojie@uestc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
								<address>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tao</forename><surname>Heng</surname></persName>
						</author>
						<author>
							<persName><surname>Shen</surname></persName>
							<email>shenht@itee.uq.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
								<address>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">The University of Queensland</orgName>
								<address>
									<settlement>Brisbane</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Zero-Shot Hashing via Transferring Supervised Knowledge</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7FCF99B75A4263D07CFF8BA7D32F07B9</idno>
					<idno type="DOI">10.1145/2964284.2964319</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T04:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>zero-shot hashing</term>
					<term>discrete hashing</term>
					<term>supervised knowledge transfer</term>
					<term>semantic alignment</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Hashing has shown its efficiency and effectiveness in facilitating large-scale multimedia applications. Supervised knowledge (e.g., semantic labels or pair-wise relationship) associated to data is capable of significantly improving the quality of hash codes and hash functions. However, confronted with the rapid growth of newlyemerging concepts and multimedia data on the Web, existing supervised hashing approaches may easily suffer from the scarcity and validity of supervised information due to the expensive cost of manual labelling. In this paper, we propose a novel hashing scheme, termed zero-shot hashing (ZSH), which compresses images of "unseen" categories to binary codes with hash functions learned from limited training data of "seen" categories. Specifically, we project independent data labels (i.e., 0/1-form label vectors) into semantic embedding space, where semantic relationships among all the labels can be precisely characterized and thus seen supervised knowledge can be transferred to unseen classes. Moreover, in order to cope with the semantic shift problem, we rotate the embedded space to more suitably align the embedded semantics with the low-level visual feature space, thereby alleviating the influence of semantic gap. In the meantime, to exert positive effects on learning high-quality hash functions, we further propose to preserve local structural property and discrete nature in binary codes. Besides, we develop an efficient alternating algorithm to solve the ZSH model. Extensive experiments conducted on various real-life datasets show the superior zero-shot image retrieval performance of ZSH as compared to several state-of-the-art hashing methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Hashing is a powerful indexing technique for enabling efficient retrieval on large-scale multimedia data, such as image <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b33">33]</ref> and video <ref type="bibr" target="#b1">[1]</ref>. Specifically, in order to achieve shorter response time and less computational cost, hashing encodes high-dimensional data into compact binary codes (i.e., 0 or 1) substantially. In this way, Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.   data can be compactly stored and Hamming distances can be efficiently calculated with bit-wise XOR operations. Because of its impressive capacity of dealing with "curse of dimensionality" problem, hashing has been extensively employed in various real-world applications, ranging from multimedia indexing <ref type="bibr" target="#b8">[8]</ref> to multimedia event detection <ref type="bibr" target="#b20">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Do o o ol l lp p p ph h h hi i i i in n n n n D D D D Do o o o o o o ol l l l l l l l l lp p p p p ph h h h h h h h h h h h h hi i i i i i i i i in n n n</head><note type="other">Cannon</note><p>There are mainly two branches of hashing, i.e., data-independent hashing and data-dependent hashing. For data-independent hashing, such as Locality Sensitive Hashing <ref type="bibr" target="#b6">[6]</ref>, no prior knowledge (e.g., supervised information) about data is available, and hash functions are randomly generated. Nonetheless, huge storage and computational overhead might be cost since more than 1, 000 bits are usually required to achieve acceptable performance. To address this problem, research directions turn to data-dependent hashing, which leverages information inside data itself. Roughly, data-dependant hashing can be divided into two categories: unsupervised hashing (e.g., Iterative Quantization <ref type="bibr">[7]</ref>, Order Preserving Hashing <ref type="bibr" target="#b26">[26]</ref> and Robust Discrete Spectral Hashing <ref type="bibr" target="#b32">[32]</ref>), and supervised hashing (e.g., Supervised Hashing with Kernels <ref type="bibr" target="#b16">[16]</ref>, Weakly-supervised Hashing <ref type="bibr" target="#b34">[34]</ref> and Supervised Discrete Hashing <ref type="bibr" target="#b22">[22]</ref>). In general, supervised hashing usually achieves better performance than unsupervised ones because supervised information (e.g., semantic labels and/or pair-wise data relationship) can help to better explore intrinsic data property, thereby generating superior hash codes and hash functions.</p><p>Along with the explosive growth of Web data, traditional supervised hashing methods have been facing an enormous challenge, i.e., the generation of reliable supervised knowledge cannot catch up with the rapid increasing speed of newly-emerging semantic concepts and multimedia data. In other words, due to the expensive cost of manual labelling (time-consuming and labor-intensive), sufficient labelled training data is usually not timely available for learning new hash functions that can accurately encode data of new concepts. As illustrated in Figure <ref type="figure" target="#fig_2">1</ref>, within the "seen" zone, where images are attached with known categories, existing supervised hashing algorithms may perform well because they are fed with correct guidance. However, outside the seen area, supervised hashing algorithms may easily fail to generalize to data of new categories that they never observe, e.g., segway, a two-wheeled, self-balancing, battery-powered electric vehicle. Moreover, most of current approaches use supervised information in the form of either 0/1 semantic labels or pair-wise data relationship for guiding the learning process, which implies that precious correlation among label semantics are inevitably ignored. One straightforward consequence of the semantic independency is that each category can neither learn from other relevant categories nor distribute its own supervised knowledge to other seen classes and/or even those unseen ones.</p><p>The aforementioned disadvantages motivate us to consider if we can encode images of "unseen" categories into binary codes with hash functions learned from limited training samples of "seen" categories? The key challenge of achieving this goal is how to set up a tunnel to transfer supervised knowledge between "seen" and "unseen" categories. In recent years, zero-shot learning (ZSL) <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b24">24]</ref> has been widely recognized as a way to deal with this problem. The ZSL paradigm aims to learn a general mapping from the feature space to a high-level semantic space, which helps to avoid rebuilding models for unseen categories with extra manually labelled data. ZSL is mostly achieved by using class-attribute descriptors to bridge the semantic gap between low-level features and high-level semantics, where new categories are thus learned using only the relationship between attributes and categories. However, most of existing attribute based ZSL methods still suffer from: (1) erroneous guidance derived from imprecise or incomplete humanlabelled attributes <ref type="bibr" target="#b10">[10]</ref>, which is usually due to the lack of expertise or mislabeling by annotators, etc.; (2) diminishing of discrimination for pre-defined attributes when facing domain shift <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b19">19]</ref>.</p><p>Recently, mining other auxiliary datasets has been shown to be helpful to tackle the zero-shot learning problem. For instance, with a huge corpus such as Wikipedia, one can obtain word embeddings that capture distributional similarity in the text corpus <ref type="bibr" target="#b25">[25]</ref>, such that similar words can be located in similar place. During the learning phase, visual modality can be grounded by the word vectors, and such knowledge can thus be transferred into the learned model. Inspired by this, many approaches choose to utilize auxiliary modalities to address the zero-shot problem. Socher et al. <ref type="bibr" target="#b24">[24]</ref> used word embedding as supervision in order to detect novel categories and perform classification accordingly. Frome et al. <ref type="bibr" target="#b5">[5]</ref> adopted a similar manner, which connects raw features and word embedding space using the dot-product similarity and hinge rank loss.</p><p>As aforementioned, with the explosion of the newly-emerging concepts and multimedia data, we are in urgent demand of a reliable and flexible hash function that can be adopted to hash images of unseen categories. However, in the hashing domain, the zeroshot problem has been rarely studied. In this work, we propose a novel hashing scheme, termed zero-shot hashing (ZSH). Inspired by the superior capacity of the word embedding for capturing the semantic correlations among concepts, we map mutually independent labels into a semantic-rich space, where supervised knowledge of both seen and unseen labels can be completely shared. This strategy helps to encode images of unseen categories without any assis-tance of visual observation in those unknown classes. Besides, even though we cannot retrieve images of exactly the same category, semantically related objects can be returned. Moreover, we recognize the problem of semantic shift caused by off-the-shelf embedding. The embedded space is then rotated to make the hash functions more generalized to images of unseen categories. To further improve the quality of hash functions, we also preserve local structural property and discrete nature in binary codes. We summarize our main contributions as below:</p><p>• We address the problem of employing limited training data of seen categories to learn reliable hash functions for transforming images of unseen categories into binary codes. We propose a novel zero-shot hashing scheme, which bridges gaps between originally independent labels through a semantic embedding space. To the best of our knowledge, this is one of the first works that study the problem of hashing data from newly-emerging concepts with limited seen supervised knowledge.</p><p>• We devise an effective strategy for transferring available supervised knowledge from seen classes to unseen classes. In particular, we project labels into a word embedding space, where semantic correlations among labels can be quantitatively measured and captured. In this way, unseen labels can leverage the well-established mapping from its semantically close seen categories. For instance, segway may learn from bicycle and automobile.</p><p>• Since the initial semantic embedding is from an off-the-shelf word embedding space, which may bring in potential semantic shift between categories and the original visual feature.</p><p>To alleviate the influence, we propose to further rotate the embedding space to better fit the underlying feature characteristics, thereby narrowing down the semantic gap effectively.</p><p>• In order to generate more reliable hash functions, we propose to improve the intermediate binary codes by exploring underlying data properties. Concretely, we impose discrete constraints on binary codes during the learning process as well as preserve data local structure, i.e., if two datums share similar representations in the original space, they are supposed to be close to each other in the learned Hamming space.</p><p>The rest of this paper is organized as follows. In Section 2, we briefly review some related work on zero-shot learning and hashing. In Section 3, we elaborate our proposed ZSH, together with an efficient optimization method and the corresponding algorithm analysis. Extensive experimental results on various different datasets are reported and analyzed in Section 4, followed by the conclusion of this work in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>In this section, we briefly review some related works on zeroshot learning and hashing, and discuss the relationship between our proposed approach and existing solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Zero-Shot Learning</head><p>Learning with no data, a.k.a., zero-shot learning, has been proved to be an effective direction to tackle the increasing difficulty posed by insufficient training samples. A promising strategy for solving this problem is to utilize an intermediate semantic layer to represent an image. Specifically, with visual attributes or other semantic abundant descriptors, a novel image can thus be defined as the relationship between category and intermediate representation. <ref type="bibr">Farhadi et al. [4]</ref> proposed to classify unseen objects by describing them with visual attributes. The work in <ref type="bibr" target="#b15">[15]</ref> studies the problem of zero-shot learning for facilitating new tasks, which has also proven to be useful when predicting categories not shown in the training dataset. Recently, learning for classifying novel images with auxiliary knowledge (e.g., leveraging textual relationship in a large corpus) has been shown to be powerful for zero-shot tasks. By exploring the correlations among semantic concepts, the label of any unseen data can be reasonably inferred. Socher et al. proposed a cross-modal transfer method for zero-shot learning <ref type="bibr" target="#b24">[24]</ref>, which uses word embedding to detect unseen classes. The work in <ref type="bibr" target="#b5">[5]</ref> also adopts the similar scheme as <ref type="bibr" target="#b24">[24]</ref>; but the difference is that a new language model and a different loss function is employed to connect two modalities. However, all above methods are only evaluated in classification or recognition scenario. To our best knowledge, our proposed ZSH is one of the first works that focus on handling the large-scale high-dimensional visual indexing problem in zero-shot scenario, i.e., hashing novel images that are not observed by existing learning systems. By adopting a natural language model <ref type="bibr" target="#b9">[9]</ref>, we can precisely capture the correlations between different concepts, and thus hash unseen images into correct spots in Hamming space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Hashing</head><p>In this part, we briefly review fast search with binary codes using hashing techniques. Similarity search is a challenge of pursuing data points of smallest distance in a large scale database. One of the easiest hashing schemes is dubbed Local Sensitive Hashing <ref type="bibr" target="#b6">[6]</ref>, which designs hashing function with no prior knowledge of the data distribution. However, such hashing methods require significantly large code length to achieve acceptable performance, which may cause unnecessary overheads in a database. To address this problem, learning-based hashing comes as a trend. Unsupervised hashing methods mine the statistic distributional information in the data, generating an optimized hashing function to preserve certain data properties in the original space. Classical algorithms (e.g., Spectral Hashing (SH) <ref type="bibr" target="#b27">[27]</ref>) learns binary codes which preserve local structural information in the data. Shen et al. proposed Inductive Manifold Hashing (IMH) <ref type="bibr" target="#b23">[23]</ref>, which adopts manifold learning techniques to better model the intrinsic structure embedded in the feature space. In <ref type="bibr">[7]</ref>, Gong et al. devised an effective approach, termed Iterative Quantization, focusing on minimizing quantization error during unsupervised training. Note that a real-world database is commonly described by multiple modalities/resources, such as visual features or textual information. In <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b29">29]</ref>, they utilized information of at least two different resources to achieve promising performance in cross-media search. Since the unsupervised way is guided with little human-level knowledge, supervised hashing has been proposed to use guiding information to learn binary codes. Hashing techniques in this category have been emerging continuously in recent years, and representative methods include Kernel Supervise Hashing (KSH) <ref type="bibr" target="#b16">[16]</ref>, Minimal Loss Hashing (MLH) <ref type="bibr" target="#b18">[18]</ref>, Supervise Discrete Hashing (SDH) <ref type="bibr" target="#b22">[22]</ref>, Latent Factor Hashing (LFH) <ref type="bibr" target="#b35">[35]</ref> and Column Sampling Based Discrete Supervised Hashing (COSDISH) <ref type="bibr" target="#b11">[11]</ref>, etc. Recently, with the success of deep learning, hashing using CNN has also shown the promising performance <ref type="bibr" target="#b31">[31]</ref>.</p><p>Admittedly, hashing algorithms have provided a possible solution for tackling the "curse of dimensionality" problem. When no training samples of certain "unseen" categories are provided, all above hashing methods may easily fail to generalize to the new data, limiting the hashing capability in the "seen" area where each category possesses some training images. Besides, as the database evolves all the time, re-training hashing function frequently is expensive, thereby leading to impractical usage in large dynamic realworld databases. Based on the above analysis, a hashing method that can perform well on unseen data draws a strong need.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">ZERO-SHOT HASHING</head><p>In this section, we elaborate our zero-shot hashing (ZSH). We firstly present a formal definition of hashing in zero-shot scenario, and then depict the details of ZSH, including a brief introduction of overall framework, transferring supervised knowledge, semantic alignment as well as hashing model. Finally, we introduce the optimization process and algorithm analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Definition</head><p>Suppose we are given n training images X = [x1, x2, . . . , xn] ∈ R d×n labeled with a seen concept set C, where xi ∈ R d×1 , i = 1, 2, . . . , n and d is the dimensionality of visual feature space. Denote Y = [y1, y2, . . . , yn] ∈ {0, 1} c×n is the binary label matrix, where yi ∈ {0, 1} c×1 is the label vector of the i-th sample xi and c is the number of seen classes in C. Different from conventional supervised hashing scenario, where both testing data and training data are associated with the same concept set, i.e., C, we intend to cope with the situation where testing data and training data share no common concepts. In other words, testing data (denoted as X (u) ) belongs to an "unseen" category set C (u) , i.e., C (u) ∩ C = ∅. Using only the training images X where no training samples of the "unseen" categories in C (u) are available, we aim to learn a hash function f : R d×1 → {-1, 1} l×1 , which can map images belonging to both C (u) and C from original visual feature space to l-bit binary codes. The learned hash function f not only guarantees that the binary codes of semantically relevant objects have short Hamming distances, but also generalizes well to the testing data belonging to the unseen categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Overall Framework</head><p>The flowchart of our overall framework is illustrated in Figure <ref type="figure" target="#fig_3">2</ref>. As we can see, there are two stages: the offline phase and the online phase. In the offline phase, suppose only images of a limited number of categories are visible to our system. We firstly extract visual features of the training images through a convolutional neural network. At the same time, we use an off-the-shelf NLP model to transform seen labels into a semantic-rich embedding space, where each label is represented by a real-valued vector. With the embedded semantics, the relationships among both seen and unseen categories can be well captured and characterized. Instead of 0/1-form label vector, ZSH supervises the learning of hash functions with the embedded semantic vectors to transfer supervised knowledge. We further rotate the off-the-shelf embedding space to better align with the low-level visual feature space. Meanwhile, ZSH preserves local structural information and discrete nature of the intermediate binary codes to improve hash functions. Finally, we use the learned hash functions to transform all the images in the database into binary codes for subsequent retrieval. In the online phase, when a new query image of any unseen category arrives, we encode the new image into binary code following the same mapping and retrieve images that are close to this query in the Hamming space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Transferring Supervised Knowledge</head><p>In general, most of existing supervised hashing algorithms may retrieve relevant results of queries in the seen categories since there are supervised information for understanding the queries. Nevertheless, when the hashing systems have no knowledge of certain unseen classes, query images from these classes will be probably be misunderstood, thereby leading to inaccurate search. One of the main causes is that the supervised information is in the form of 0/1form label vectors or pair-wise data relationship, which implicitly makes labels independent to each other and omits the inherent correlation among their high-level semantics (e.g., cat is as different from truck as from dog). As illustrated in Figure <ref type="figure" target="#fig_4">3</ref>, using independent labels, each object will be mapped to an independent vertex of a hypercube, and the distance between any two categories will be the same. In order to address such disadvantage, we propose to connect label semantics by taking advantage of the superior ability endowed by neural language processing techniques. Specifically, as illustrated in Figure <ref type="figure" target="#fig_4">3</ref>, we map independent labels into a word embedding space, where semantic correlations among labels can be quantitatively measured and captured. Therefore, unseen labels can leverage the well-established mapping from its semantically close seen categories. For example, in the embedding space, cat and dog will be close to each other. Hence even the hashing systems may never observe any cat images, they can still gain some useful clues from the supervised knowledge of dog. We adopt the language model <ref type="bibr" target="#b9">[9]</ref> pre-trained using free Wikipedia text. This model leverages not only local information but also global document context, thereby achieving superior performance over other competitive ap- proaches. Every category is embedded into a 50-d word vector <ref type="foot" target="#foot_0">1</ref> . In the subsequent part, we consistently represent the label matrix Y with the embedded label matrix instead of the original 0/1 label matrix, and c is the dimensionality of the embedding space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Semantic Alignment</head><p>Note that the transformed supervised knowledge from the offthe-shelf embedding space may potentially deviate from the underlying semantics of the image data due to the problems of domain difference, semantic shift, semasiological variation, etc. This will inevitably jeopardize the whole learning process in our proposed model. In order to prevent this issue, we propose to a semantic alignment strategy, which actively aligns the initial embedding space with the distributional properties of low-level visual feature. In particular, we seek for certain transformation R ∈ R 50×c matrix with orthogonal constraint R T R = Ic to rotate the embedding space to R T Y , where Ic is an identity matrix of size c × c. Recall that our goal is to use the amendatory supervised knowledge to guide the learning of high-quality hash codes and hash functions, therefore, we minimize the following error:</p><formula xml:id="formula_0">R T Y -W T B 2 F ,<label>(1)</label></formula><p>which W ∈ R l×c is the mapping matrix from binary codes to the supervised information. B = [b1, b2, • • • , bn] ∈ {-1, 1} l×n denotes the binary codes of X, where bi ∈ {-1, 1} l×1 is the binary codes of the i-th sample xi. l is the code length. The above transformation of semantics can help to effectively narrow down the semantic gap between binary codes and the supervised knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Hashing Model</head><p>For convenience, we firstly recap some previous settings here. Suppose we have</p><formula xml:id="formula_1">n training samples X = [x1, x2, • • • , xn] ∈ R d×n .</formula><p>For brevity, we denote the corresponding embedded label knowledge as Y = [y1, y2, • • • , yn] ∈ R c×n . Our ultimate target is to learn a set of hash functions from "seen" training data X supervised by Y , enabling generating high-quality binary codes for data of "unseen" categories. Meanwhile, the quality of hash functions may heavily rely on the reliability of the intermediate binary codes of training data. In other words, the model is supposed to simultaneously well control both hash functions and hash codes. To achieve the above goals, we propose the following model:</p><formula xml:id="formula_2">min f,W,B,R R T Y -W T B 2 F + λ W 2 F + α f (X) -B 2 F + βΩ(f ) + γ n i=1 n j=1 Sij f (xi) -f (xj) 2 F s.t. B ∈ {-1, 1} l×n ∧ R T R = Ic,<label>(2)</label></formula><p>where R ∈ R c×c is the semantic rotation matrix. W ∈ R l×c is the mapping matrix. B ∈ {-1, 1} l×n denotes the binary codes. • F denotes the Frobenius norm of a matrix. λ &gt; 0, α &gt; 0, β &gt; 0 and γ &gt; 0 are balancing parameters. f : R m×1 → R l×1 define a hash function from a non-linear embedded feature space to the desired Hamming space:</p><formula xml:id="formula_3">f (x) = P T φ(x),<label>(3)</label></formula><p>where</p><formula xml:id="formula_4">f (X) = [f (x1), f (x2), . . . , f (xn)]. P ∈ R m×l is the transformation matrix. Ω(f ) = P 2</formula><p>F is a regularizer P . Following the successful practice for learning hash functions in <ref type="bibr" target="#b16">[16]</ref>, we employ kernel mapping to handle the potential problem of linear inseparability:</p><formula xml:id="formula_5">φ(x) = exp(- x-a1 2 δ ), . . . , exp(- x-am 2 δ ) T ,<label>(4)</label></formula><p>where {ai}| m i=1 are m anchors randomly sampled from X and δ is the bandwidth parameter.</p><p>Note that we keep the discrete constraint on the variable B to prevent information loss of binary codes to the greatest extent. The term</p><formula xml:id="formula_6">γ n i=1 n j=1 Sij f (xi) -f (xj) 2</formula><p>F in Eq. ( <ref type="formula" target="#formula_2">2</ref>) preserves local structural information of training data, i.e., if two samples are similar in the original feature space, then they are enforced to share similar binary codes in the Hamming space. Here, Sij is the similarity of xi and xj in the original visual feature space.</p><p>In the next part, we introduce an efficient algorithm to optimize our zero-shot hashing model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Optimization</head><p>We first rewrite the model in matrix form as follows:</p><formula xml:id="formula_7">min P,W,B,R R T Y -W T B 2 F + λ W 2 F + α P T φ(X) -B 2 F + β P 2 F + γT r(P T φ(X)Lφ(X) T P ) s.t. B ∈ {-1, 1} l×n ∧ R T R = Ic,<label>(5)</label></formula><p>where φ(X) = [φ(x1), φ(x2), . . . , φ(xn)]. T r(•) is the trace of a matrix. The Laplacian matrix L is computed as:</p><formula xml:id="formula_8">L = D -S,<label>(6)</label></formula><p>where D is a diagonal matrix with its i-th diagonal element computed as Dii = n j=1 Sij and S = {Sij}|i,j=1,2,...,n is the similarity matrix of X.</p><p>Next, we present an algorithm to optimize the model in Eq. ( <ref type="formula" target="#formula_7">5</ref>) by alternatingly updating P, W, B, R.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.1">Update P</head><p>Fixing all variables except for P , we get the quadratic problem as:</p><formula xml:id="formula_9">min P α P T φ(X)-B 2 F +β P 2 F +γT r(P T φ(X)Lφ(X) T P ).<label>(7)</label></formula><p>By setting its derivative with respect to P to 0, we have the following solution:</p><formula xml:id="formula_10">P = φ(X)φ(X) T + β α Im + γ α φ(X)Lφ(X) T -1 φ(X)B T ,<label>(8)</label></formula><p>where Im is an identity matrix of size m × m.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.2">Update B</head><p>In this step, we fix all other variables and learn binary codes B with discrete constraint. The objective function can be reduced to</p><formula xml:id="formula_11">min B R T Y -W T B 2 F + α P T φ(X) -B 2 F s.t. B ∈ {-1, 1} l×n . (9)</formula><p>The above formulation can be further written as</p><formula xml:id="formula_12">min B W T B 2 F -2T r(B T H) s.t. B ∈ {-1, 1} l×n ,<label>(10)</label></formula><p>where</p><formula xml:id="formula_13">H = W R T Y + αP T φ(X).</formula><p>Inspired by <ref type="bibr" target="#b30">[30]</ref>, we apply the discrete coordinate descent (DCC) algorithm to solve the problem <ref type="bibr" target="#b10">(10)</ref>. Let B = q T 1 , q T 2 , . . . , q T l ,</p><formula xml:id="formula_14">H = h T 1 , h T 2 , • • • , h T l and W = u T 1 , u T 2 , • • • , u T l</formula><p>, where q T i , h T i and u T i are the i-th row of B, H and W , respectively. Furthermore, for convenience, we denote</p><formula xml:id="formula_15">           B¬i = q1 T , ..., qi-1 T , qi+1 T , ..., q l T , H¬i = h1 T , ..., hi-1 T , hi+1 T , ..., h l T , W¬i = u1 T , ..., ui-1 T , ui+1 T , ..., u l T .<label>(11)</label></formula><p>Then, we can have</p><formula xml:id="formula_16">W T B 2 F = T r(B T W W T B) = const + qiu T i 2 F + 2u T i W T ¬i B¬iqi = const + 2u T i W T ¬i B¬iqi.<label>(12)</label></formula><p>Here, qiu T i 2 = T r(uiq T i qiw T i ) = const. Following the same rule, we also have the following conclusion</p><formula xml:id="formula_17">T r(B T H) = const + h T i qi.<label>(13)</label></formula><p>The sub-problem can be transformed to</p><formula xml:id="formula_18">min q i (u T i W T ¬i B¬i -hi)qi s.t. hi ∈ {-1, 1} n×1 . (<label>14</label></formula><formula xml:id="formula_19">)</formula><p>The optimal solution of above equation is</p><formula xml:id="formula_20">qi = sgn(hi -B T ¬i W¬iui),<label>(15)</label></formula><p>where sgn(•) is the sign function. We can see that each bit of the desired binary code B can be learned based on other l -1 bits. Thus, we can use cyclic coordinate descent approach to generate the optimal codes until the entire procedure converges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.3">Update R</head><p>With B, W, P fixed, we then have</p><formula xml:id="formula_21">min R R T Y -W T B 2 F s.t. R T R = Ic,<label>(16)</label></formula><p>which can efficiently solved by the algorithm in <ref type="bibr" target="#b28">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.4">Update W</head><p>By fixing P, B, R, we arrive at a classic ridge regression problem:</p><formula xml:id="formula_22">min W R T Y -W T B 2 F + λ W 2 F .<label>(17)</label></formula><p>The above equation has a closed-form solution:</p><formula xml:id="formula_23">W = (BB T + λI l ) -1 BY T R,<label>(18)</label></formula><p>where I l is a diagonal matrix of size l × l. By iteratively updating P, W, B, R until convergence, we can arrive at an optima. The overall algorithm is summarised in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Algorithm for optimizing Zero-Shot Hashing</head><p>Input: Training data X and the embedded label matrix Y ; Output: Binary codes B, rotation matrix R, hash function P and mapping matrix W ; 1: Randomly initialize B, P and W ; 2: Randomly initialize R to be orthogonal; 3: Map X to φ(X) using m anchors randomly selected from X; 4: Construct Laplacian matrix L; 5: repeat 6:</p><p>Update P according to Eq. ( <ref type="formula" target="#formula_10">8</ref>); 7:</p><p>Update B iteratively by using the solution of ( <ref type="formula" target="#formula_20">15</ref>); 8:</p><p>Update R by solving the problem in Eq. ( <ref type="formula" target="#formula_21">16</ref>); 9:</p><p>Update W according to Eq. ( <ref type="formula" target="#formula_23">18</ref>); 10: until there is no change to P, W, B, R 11: return P, W, B, R;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Algorithm Analysis</head><p>In this section, we analyze the convergence and time complexity of our algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.1">Convergence Study</head><p>As shown in Algorithm 1, in each iteration, the updates of all variables make the value of the objective function decreased. We also conducted empirical study on the convergence property using ImageNet <ref type="bibr" target="#b3">[3]</ref>. Specifically, we trained our zero-shot hashing model with 30, 000 seen images randomly sampled from the Im-ageNet dataset, with label embedding as supervised information. We selected 1, 000 anchors and set the code length to 64. As Figure <ref type="figure" target="#fig_6">4</ref> shows, our algorithm starts with cost function value at roughly 30, 000, but descends dramatically within only 10 iterations, and reaches a stable local minima at the 20-th iteration. This phenomenon clearly indicates the efficiency of our algorithm.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.2">Computational Complexity</head><p>In each iteration (line 6-9), the time cost is analyzed as follows. The computation of P in Eq. ( <ref type="formula" target="#formula_10">8</ref>) is O(m<ref type="foot" target="#foot_1">2</ref> n+nml+m<ref type="foot" target="#foot_2">3</ref> ). The DCC algorithm for updating B costs O(cl 2 +l 2 n). As to the optimization of the sub-problem in Eq. ( <ref type="formula" target="#formula_21">16</ref>), the time cost is O(c 3 ). Finally, the computational cost of updating W is O(l 2 n+lnc+lc 2 +l 3 ). Given that m ≪ n, l ≪ n, c ≪ n and our algorithm converges within a few iterations (less than 10), the overall time cost of our algorithm is O(n). It is worth noting that the dominant operation of our algorithm is matrix multiplication, which can be greatly speeded up by using parallel and/or distributed algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>In our experiments, we employ three real-life image datasets, including CIFAR-10 2 , ImageNet 3 and MIRFlickr <ref type="foot" target="#foot_3">4</ref> .</p><p>CIFAR-10 consists of 60, 000 images which are manually labelled with 10 classes including airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck, with 6, 000 samples in each class. The classes are completely mutually exclusive, i.e., no overlap between classes (e.g., automobiles and trucks).</p><p>ImageNet is an image dataset organized according to the Word-Net <ref type="bibr" target="#b17">[17]</ref> hierarchy. The subset of ImageNet for the Large Scale Visual Recognition Challenge 2012 (ILSVRC2012) is used for our experiments, consisting of over 1.2 million Web images, manually labeled with 1, 000 object categories.</p><p>MIRFlickr comprises 25, 000 images collected from the social photography site Flickr through its public API. Firstly introduced in 2008, this dataset is wildly used in multimedia research. MIR-Flickr is a multi-label dataset with every image associating with 24 popular tags such as sky, river, etc.</p><p>For all image data, we adopted the winning model for the 1000class ImageNet Large Scale Visual Recognition Challenge 2012 <ref type="bibr" target="#b13">[13]</ref> to extract the fully connected layer fc-7 as visual feature.</p><p>Various metrics are employed for measuring performance of different evaluation tasks. For image retrieval, we used the two traditional metrics i.e., Precision and Mean Average Precision (MAP). MAP focuses on the ranking of retrieval results and we reported the results over the top 5, 000 retrieved samples. Precision mainly concentrates on the retrieval accuracy and we reported the results with Hamming radius r ≤ 2.</p><p>We compared our proposed ZSH with four state-of-the-art supervised hashing approaches, including COSDISH <ref type="bibr" target="#b11">[11]</ref>, SDH <ref type="bibr" target="#b22">[22]</ref>, KSH <ref type="bibr" target="#b16">[16]</ref> and LFH <ref type="bibr" target="#b35">[35]</ref>. For all anchor-based algorithms, we randomly sampled 1, 000 anchors from the training dataset. Furthermore, we compared to one of the most representative unsupervised hashing method, i.e., Inductive Hashing on Manifolds (IMH) <ref type="bibr" target="#b23">[23]</ref>.</p><p>For all comparing approaches, we followed their suggested parameter settings. For ZSH, we empirically set α to 10 -5 and γ to 10 -6 . For regularization parameters λ and β, we set them to 10 -2 and 10 -4 , respectively. The number of iterations is set to 10. We define the similarity matrix S as follows:</p><formula xml:id="formula_24">Sij =      exp(- x i -x j 2σ 2 ), if xi ∈ N k (xj) or xi ∈ N k (xj) 0, otherwise,</formula><p>where N k (•) is the function of searching k nearest neighbors. In our experiment, we set σ = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results on CIFAR-10</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Overall Comparison of Zero-Shot Image Retrieval</head><p>To evaluate the efficacy of retrieving images in unseen categories, we split CIFAR-10 into a "seen" training set and an "unseen" testing set. In particular, we selected truck as unseen testing category and left the rest 9 categories as seen training set. For all comparing algorithms, we randomly sampled 10, 000 images for learning hash functions. For testing purpose, we randomly selected 1, 000 images from the unseen category as query images, and the remaining 5, 000 test images together with the 54, 000 images of seen categories are combined to form the retrieval database.</p><p>The performance of all comparing approaches w.r.t. different codes lengths (i.e., {16, 32, 64, 96, 128}) is illustrated in Figure <ref type="figure" target="#fig_7">5</ref>. As we can see, the proposed ZSH outperforms all the other hashing algorithms in terms of MAP at all code lengths. As to Precision, ZSH still shows superior image retrieval performance in most cases. The underlying principle is that our method not only utilizes inherent semantic relationship among labels to transfer supervised knowledge, but also preserves discrete and structural properties of data in the learning of hash codes and hash functions. An interesting observation is that the performance of IMH, which is an unsupervised method, gains competitive even better retrieval results in terms of Precision as compared to some supervised methods such as KSH, SDH. While unsupervised methods encode images solely with the distributional properties in the feature space, the supervised ones may be misled by independent semantic labels in the learning processing.</p><p>Besides, MAP increases rapidly for all methods when code length varies from 16 to 64, and then reaches a slow-growth stage from 64 bits to 128 bits. When code length is short, more codes are required to guarantee the descriptive and discriminative power. However, after encoding space is large enough (e.g. 64 bits), with the expression ability saturated, providing more bits cannot significantly improve the performance. As to Precision, hashing performance significantly deteriorates as code length is larger than 64. Recall that our searching radius is empirically set to 2, forming a hyperball of radius 2 in Hamming space. When the code length increases from 16 to 64, significant improvement in retrieval ability counteracts the searching difficulty. However, as Hamming space becomes larger, searching difficulty grows linearly, thereby degrading the Precision performance. Therefore, as a trade-off between efficiency and effectiveness, an eclectic code length should be chosen. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Effect of Different Unseen Category</head><p>In this experiment, we aim to evaluate the performance of zeroshot image retrieval on different unseen categories. The experimen-tal settings are the same as that in the previous subsection. Figure <ref type="figure" target="#fig_8">6</ref> illustrates the MAP and Precision performance of ZSH using each individual label as unseen testing data. We can observe that zero-shot image retrieval performance varies from one class to another, reaching peak at bird and bottom at automobile. Intuitively, if an unseen class is semantically closer to other seen categories, more relevant supervised knowledge can be transferred from word embedding space for boosting the retrieval performance. To dig deeper about the reason behind the fluctuation of performance on different unseen objects, we computed the average cosine similarity between each unseen category and other seen categories, and list the corresponding MAP in Table <ref type="table" target="#tab_0">1</ref>. We observe that the MAP performance is positively related to the average cosine similarity. For instance, those of larger cosine similarity (e.g., dog, cat) performs relatively well, while those of smaller similarity (e.g., airplane, automobile) gain relatively poor performance. This observation implies that in order to achieve satisfactory retrieval results, unseen classes should have sufficient correlation with seen ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Category</head><p>As we can see in Figure <ref type="figure" target="#fig_8">6</ref>, we also compare the effects of embedded labels and binary labels. The performance of embedded labels is obviously better than that of binary labels. The underlying reason is that the embedding space can help to capture the relationship between seen and unseen categories for transferring supervised knowledge. In contrast, binary labels neglect semantic correlations, thereby leading to irregular fluctuations of retrieval performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Effect of Seen Category Ratio</head><p>In this experiment, we evaluate the performance of our proposed ZSH w.r.t. different numbers of seen categories. Specifically, we varied the ratio of seen categories in the training set from 0.1 to 0.9. For each ratio, we randomly sampled 10, 000 images from the As can be seen, our proposed ZSH method returns the largest number of correct retrieved results with query from unseen category, followed by COSDISH which returns four correct samples. seen categories for training. Further, we randomly selected 1, 000 images from the unseen set as queries to search in the remaining 59, 000 images. Note that when the ratio of seen categories decreases to 0.1, we use all 6, 000 datums of that class as training set.</p><p>Figure <ref type="figure">8</ref>: Effects of different ratios of seen categories on CIFAR-10 dataset.</p><p>We report the experimental results in Figure <ref type="figure">8</ref>, from which we have the following observations: (1) The performance of both MAP and Precision boosts as the ratio of the seen categories grows; <ref type="bibr" target="#b2">(2)</ref> As the ratio increases from 0.1 to 0.3, we see a dramatic leap of the retrieval performance, followed by a relatively slight performance improvement from 0.3 to 0.9. We analyze that by observing more "seen" categories, we have higher possibility to find relevant supervision for the unseen class, which guides to learn better intermediate hash codes, thereby simultaneously improving the quality of hash functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Effect of Training Size</head><p>This part of experiment mainly focuses on evaluating the effect of training size on the searching quality of ZSH. For simplicity, we chose Precision as evaluation metric and selected truck as the unseen object and varied the size of training data in the range of {1000, 2000, . . . , 10000, 20000, . . . , 50000}.</p><p>The experimental results are demonstrated in Figure <ref type="figure" target="#fig_10">9</ref>. As we can see, when the size increases from 1, 000 to 10, 000, we observe a rapid rise of the Precision performance. Nonetheless, when fed with more training data, ZSH does not gain noticeable performance boost. For the balance of training efficiency and effectiveness, in the rest experiments, we consistently set the training size to 10, 000. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results on ImageNet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Overall Comparison of Zero-Shot Image Retrieval</head><p>In this part, we evaluate our proposed ZSH on zero-shot image retrieval as compared to other state-of-the-art methods using the Large Scale Visual Recognition Challenge 2012 (ILSVRC2012) dataset. Recall that the ILSVRC2012 dataset contains more than 1.2 million images tagged with 1, 000 synsets without any overlap. For evaluation purpose, we randomly chose 100 categories which have corresponding word embedding learned from Wikipedia text corpus. This gives us a set of roughly 130, 000 images. We split the data into a training set (90 seen categories) and a testing set (10 unseen categories). For all comparing algorithms, we randomly selected 10, 000 images of seen categories for training. As to image queries, we randomly sampled 1, 000 images from the unseen categories. We used the learned hash function to encode all the remaining images to form the retrieval database.</p><p>The performance of our proposed ZSH and other four state-ofthe-art supervised hashing methods with different code lengths are reported in Figure <ref type="figure" target="#fig_11">10</ref>. As we can see, ZSH consistently outperforms all other competitors in most cases. As code length varies from 16 to 128, we can observe the similar variation tendency of performance on ImageNet to that on CIFAR-10. This phenomenon again implies that we should choose a trade-off code length to guarantee the retrieval performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Image Retrieval in Related Categories</head><p>In zero-shot image retrieval scenario, we expect that even though we fail to retrieve relevant images of the same category, we can still obtain semantically related images. For instance, if the query image describes a cat, we may prefer to retrieve images of dog rather than images of car. Our proposed ZSH utilizes semantic embedding to set up connections between semantically similar labels in the embedded space. In this way, the supervised knowledge of seen categories can be transferred into the learning of hash functions, which can effectively encode images of unseen categories. Since we need to search more related categories, all remaining images of both seen and unseen categories are used to form retrieval database. All the other settings are the same as that in Section 4.3.1. In order to evaluate the performance of retrieving related categories, we use two modified metrics, named MAP related and Precision related , which are defined as</p><formula xml:id="formula_25">MAP related = K i=1 AP related @i K ,<label>(19)</label></formula><formula xml:id="formula_26">Precision related = n related n retrieved ,<label>(20)</label></formula><p>where MAP related is calculated based on the top K retrieved results, AP related @i is the average precision based on the related results, calculated by</p><formula xml:id="formula_27">AP related @i = n (i) related i ,<label>(21)</label></formula><p>where n (i) related is the number of related images in top i retrieved results. n related and n retrieved are the related retrieval under Hamming radius 2 and total examples retrieved under Hamming radius 2, respectively. Using WordNet <ref type="bibr" target="#b17">[17]</ref>, which is a lexical database for the English language, we define query A and retrieved object B are related if: 1) A and B are not of the same category; and 2) A can reach B on WordNet within 5 hops.</p><p>In practice, we set K = 5, 000. Figure <ref type="figure" target="#fig_12">11</ref> shows the experimental results. We can see that in terms of MAP related , our method always outperforms other methods at every code length. When we look at Precision related , our proposed ZSH achieves 0.3262, 0.2636, 0.2129 at 32 bits, 64 bits and 96 bits, respectively, which significantly outperforms the second best method. This observation indicates that ZSH is capable of detecting the semantically similar images from the most related categories. In real-world images, especially in user-generated photos, there often exists multiple tags in each individual picture. To further examine the practical efficacy of our proposed ZSH, we conducted an extra experiment on a real-life multi-label dataset, i.e., MIR-Flickr, which contains 25, 000 images downloaded from the social photography site Flickr. Each image is associated with 24 tags. In multi-label image dataset, different categories share overlapping images, which makes it difficult to divide the dataset into training set and testing set. Therefore, we employed ImageNet as an auxiliary dataset to train our hash functions and evaluated the zeroshot image retrieval performance on MIRFlickr. Specifically, from the ILSVRC2012 dataset we selected 100 categories which does not overlap with the 24 tags in MIRFlickr. For fair comparison, all hashing approaches used 10, 000 randomly sampled images for training. After the hash function was learned, we directly applied them to transform the MIRFlickr images into binary codes. We then sampled 1, 000 datums as query images and searched relevant results in the remaining 24, 000 images. We regarded the retrieval images sharing at least two tags with the query as the true neighbors, and computed MAP on the top 5, 000 retrieved results and Precision under Hamming distance 2. Figure <ref type="figure" target="#fig_13">12</ref> illustrates the results of our ZSH and other comparing algorithms on MIRFlickr. In the left sub-figure, we can see that with different code lengths, our ZSH can consistently achieve the best MAP performance among all the comparing algorithms. As the code length increases, the MAP performance of each algorithm keeps increasing, reaching 0.2488 at 128 bits, which outperforms the second best hashing method COSDISH by 19% at the same length. In terms of Precision, ZSH exceeds all other methods in most cases. Similar to that of CIFAR-10 and ImageNet, we can see a variation pattern with an increasing trend from 16 to 64 and a performance drop from 64 to 128. The promising performance on MIRFlickr demonstrates the potential of ZSH in indexing and searching real-life image data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results on MIRFlickr</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>With the explosion of newly-emerging concepts and multimedia data on the Web, it is impossible to supply existing supervised hashing methods with sufficient labeled data in time. In this paper, we studied the problem of how to map images of unseen categories using hash functions learned from limited seen classes. We pro-posed a novel hashing scheme, termed zero-shot hashing (ZSH), which is capable of transmitting supervised knowledge from seen categories to unseen categories. Independent 0/1-form labels were projected into an off-the-shelf embedding space with abundant semantics, where label semantic correlations can be fully characterized and quantified. Considering the issues of domain difference and semantic shift, we further narrowed down the gap between binary codes and high-level semantics by a semantic alignment operation. Specifically, we intentionally rotated the embedding space to adjust the supervised knowledge more suitable for learning highquality hash codes. Besides, we also preserved local structural property and discrete nature of hash codes in the ZSH model. An efficient algorithm was designed to optimize the model in an alternating manner and the empirical study showed the convergency and efficiency. We evaluated our proposed ZSH hashing approach on three real-world image datasets, including CIFAR-10, ImageNet and MIRFlickr. The experimental results demonstrated the superiority of ZSH as compared to several state-of-the-art hashing approaches on zero-shot image retrieval task.</p><p>In the future, we plan to enhance the exploration of label semantic correlations by integrating knowledge from multiple sources, including textual corpus and visual clues. We expect this will compensate the incomplete representation of each individual modality, thereby solving the problem of domain difference and semantic shift fundamentally.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>MM ' 16 ,</head><label>16</label><figDesc>October 15-19, 2016, Amsterdam, Netherlands c 2016 ACM. ISBN 978-1-4503-3603-1/16/10. . . $15.00 DOI: http://dx.doi.org/10.1145/2964284.2964319 Dolphin Dragonfly D Dr r ra a a a a a a a ag y a a a a a a a ag g g g g g g g go o o o o o o o on n n n n n n n n n n n n n nf f f f f f f f f f f f fl l l l l ly y y y</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>…</head><label></label><figDesc>Ca annon n n n n C Ca a a an n n n n n nn n n n n n n n n n n n n no o o o o o o o o on n n n n n n</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An illustration of newly-emerging concepts and images unseen to the existing learning systems.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The overall architecture of the proposed zero-shot hashing framework.</figDesc><graphic coords="3,400.49,137.99,143.52,74.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An illustration of independent 0/1-form labels v.s. word embedding.</figDesc><graphic coords="4,316.69,75.19,111.42,94.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Convergence study on ImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Performance (MAP and Precision) of different comparing methods on zero-shot image retrieval over CIFAR-10 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Performance (Precision and MAP) of zero-shot image retrieval for each individual unseen category on CIFAR-10 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: A demonstration of the zero-shot image retrieval exemplars using different comparing hashing algorithms on CIFAR-10, where top 15 retrieval images are reported. Pictures with green bounding box indicate the correct results while those with red outlines indicate failure results.As can be seen, our proposed ZSH method returns the largest number of correct retrieved results with query from unseen category, followed by COSDISH which returns four correct samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Effects of training size on Precision performance over CIFAR-10 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Performance (MAP and Precision) of different comparing methods on zero-shot image retrieval over ImageNet dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Comparison of ZSH and other hashing approaches on the capability of retrieving semantically similar images from related categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Performance (MAP and Precision) of different comparing methods on zero-shot image retrieval over MIRFlickr dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Average cosine similarity of each category and all other categories, together the corresponding MAP performance.</figDesc><table><row><cell></cell><cell cols="2">Average Cosine Similarity MAP</cell></row><row><cell>airplane</cell><cell>0.2191</cell><cell>0.2791</cell></row><row><cell>automobile</cell><cell>0.1567</cell><cell>0.2603</cell></row><row><cell>bird</cell><cell>0.3565</cell><cell>0.3751</cell></row><row><cell>cat</cell><cell>0.3661</cell><cell>0.3621</cell></row><row><cell>deer</cell><cell>0.2981</cell><cell>0.2912</cell></row><row><cell>dog</cell><cell>0.3826</cell><cell>0.3467</cell></row><row><cell>frog</cell><cell>0.3485</cell><cell>0.2991</cell></row><row><cell>horse</cell><cell>0.3015</cell><cell>0.3125</cell></row><row><cell>ship</cell><cell>0.1663</cell><cell>0.2987</cell></row><row><cell>truck</cell><cell>0.3358</cell><cell>0.3120</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>In practice, we find that by setting word vector to unit length, retrieval performance can be augmented with no distortion of the cosine similarities. Thus, we empirically normalize word vector to be unit length.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://www.cs.toronto.edu/ kriz/cifar.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>http://image-net.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>http://press.liacs.nl/mirflickr/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">ACKNOWLEDGMENTS</head><p>This work was supported in part by the National Natural Science Foundation of China under Project 61572108 and Project 61502081, the National Thousand-Young-Talents Program of China, and the Fundamental Research Funds for the Central Universities under Project ZYGX2014Z007 and Project ZYGX2015J055.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Submodular video hashing: a unified framework towards video pooling and indexing</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Efficient activity retrieval through semantic graph queries</title>
		<author>
			<persName><forename type="first">G</forename><surname>Castañón</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Describing objects by their attributes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Similarity search in high dimensions via hashing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gionis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Iterative quantization: A procrustean approach to learning binary codes</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Iterative multi-view hashing for cross media indexing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improving word representations via global context and multiple word prototypes</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Zero-shot recognition with unreliable attributes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Column sampling based discrete supervised hashing</title>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Online incremental attribute-based zero-shot learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kankuekul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kawewong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tangruamsub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Hasegawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Attribute-based classification for zero-shot visual object categorization</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Zero-data learning of new tasks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Supervised hashing with kernels</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Minimal loss hashing for compact binary codes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Relative attributes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Streaming first story detection with application to twitter</title>
		<author>
			<persName><forename type="first">S</forename><surname>Petrović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Osborne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An embarrassingly simple approach to zero-shot learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Supervised discrete hashing</title>
		<author>
			<persName><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Tao</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Inductive hashing on manifolds</title>
		<author>
			<persName><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hengel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Zero-shot learning through cross-modal transfer</title>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ganjoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Word representations: a simple and general method for semi-supervised learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Order preserving hashing for approximate nearest neighbor search</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<title level="m">Spectral hashing</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A feasible method for optimization with orthogonality constraints</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mathematical Programming</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sparse multi-modal hashing</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMM</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Coordinate descent algorithms for lasso penalized regression</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lange</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Applied Statistics</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Supervised hashing for image retrieval via image representation learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Robust discrete spectral hashing for large-scale image semantic indexing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TBD</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Visual coding in a semantic hierarchy</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Discrete image hashing using large weakly annotated photo collections</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Supervised hashing with latent factor models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Linear cross-modal hashing for efficient multimedia search</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
