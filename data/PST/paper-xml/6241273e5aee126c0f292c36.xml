<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semi-Supervised Formality Style Transfer with Consistency Training</title>
				<funder ref="#_Jhz8afW">
					<orgName type="full">New Energy and Industrial Technology Development Organization</orgName>
					<orgName type="abbreviated">NEDO</orgName>
				</funder>
				<funder>
					<orgName type="full">Advanced Human Resource Development Fellowship for Doctoral Students, Tokyo Institute of Technology</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ao</forename><surname>Liu</surname></persName>
							<email>liu.ao@nlp.c.titech.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">Tokyo Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">An</forename><surname>Wang</surname></persName>
							<email>wang@de.cs.titech.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">Tokyo Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Naoaki</forename><surname>Okazaki</surname></persName>
							<email>okazaki@c.titech.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">Tokyo Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Semi-Supervised Formality Style Transfer with Consistency Training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Formality style transfer (FST) is a task that involves paraphrasing an informal sentence into a formal one without altering its meaning. To address the data-scarcity problem of existing parallel datasets, previous studies tend to adopt a cycle-reconstruction scheme to utilize additional unlabeled data, where the FST model mainly benefits from target-side unlabeled sentences. In this work, we propose a simple yet effective semi-supervised framework to better utilize source-side unlabeled sentences based on consistency training. Specifically, our approach augments pseudoparallel data obtained from a source-side informal sentence by enforcing the model to generate similar outputs for its perturbed version. Moreover, we empirically examined the effects of various data perturbation methods and propose effective data filtering strategies to improve our framework. Experimental results on the GYAFC benchmark demonstrate that our approach can achieve state-of-the-art results, even with less than 40% of the parallel data 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Formality style transfer (FST) <ref type="bibr" target="#b19">(Rao and Tetreault, 2018)</ref> has garnered growing attention in the text style transfer community, which aims to transform an informal-style sentence into a formal one while preserving its meaning. The large amount of usergenerated data from online resources like tweets often contain informal expressions such as slang words (e.g., gonna), wrong capitalization or punctuations, and grammatical or spelling errors. FST can clean and formalize such noisy data, to benefit downstream NLP applications such as sentiment classification <ref type="bibr" target="#b29">(Yao and Yu, 2021)</ref>. Some examples of FST data are presented in Table <ref type="table">1</ref>.</p><p>Informal TITANIC I THINK IT COST ABOUT 300 MILLION Formal I think that Titanic cost around 300 million dollars. Informal being condiderate of her feelings and needs Formal I am being considerate of her personal needs and feelings.</p><p>Table <ref type="table">1</ref>: Examples of informal-formal sentence pairs.</p><p>With the release of the FST benchmark Grammarly Yahoo Answers Corpus (GYAFC) <ref type="bibr" target="#b19">(Rao and Tetreault, 2018)</ref>, previous studies on FST tend to employ neural networks such as sequence-tosequence (seq2seq) models to utilize parallel (informal and formal) sentence pairs. However, GYAFC only contains 100k parallel examples, which limits the performance of neural network models. Several approaches have been developed to address the data-scarcity problem by utilizing unlabeled sentences. In a previous study, <ref type="bibr" target="#b31">Zhang et al. (2020)</ref> proposed several effective data augmentations methods, such as back-translation, to augment parallel data. Another line of research <ref type="bibr" target="#b22">(Shang et al., 2019;</ref><ref type="bibr" target="#b27">Xu et al., 2019;</ref><ref type="bibr" target="#b2">Chawla and Yang, 2020)</ref> conducted semi-supervised learning (SSL) in a cycle-reconstruction manner, where both forward and backward transfer models were jointly trained while benefiting each other by generating pseudo-parallel data from unlabeled sentences. Under this setting, both additional informal and formal sentences are utilized; however, the forward informal?formal model mostly benefits from the target-side (formal) sentences, which are back-translated by the formal?informal model to construct pseudo training pairs. Conversely, the formal?informal model can only acquire extra supervision signals from informal sentences. Because the main objective of FST is the informal?formal transfer, the additional informal sentences were not well utilized in previous studies. In addition, these semi-supervised models incorporate many auxiliary modules such as style discriminators, to achieve state-of-the-art results, which result in rather complicated frameworks and more model As noisy informal sentences are easier to acquire from online resources, we attempt to take a different view from existing approaches, by adopting additional source-side (informal) sentences via SSL. We gain insights from the state-of-the-art approaches for semi-supervised image and text classification <ref type="bibr" target="#b23">(Sohn et al., 2020;</ref><ref type="bibr" target="#b26">Xie et al., 2020;</ref><ref type="bibr" target="#b0">Berthelot et al., 2019;</ref><ref type="bibr" target="#b30">Zhang et al., 2021;</ref><ref type="bibr" target="#b3">Chen et al., 2020)</ref> and propose a simple yet effective SSL framework for FST using purely informal sentences. Our approach employs consistency training to generate pseudo-parallel data from additional informal sentences. Specifically, we enforce the model to generate similar target sentences for an unlabeled source-side sentence and its perturbed version, making the model more robust against the noise in the unlabeled data. In addition, a supervised loss is trained simultaneously to transfer knowledge from the clean parallel data to the unsupervised consistency training.</p><p>Data perturbation is the key component of consistency training and significantly affects its performance. To obtain a successful SSL framework for FST, we first empirically study the effects of various data perturbation approaches. Specifically, we explore easy data augmentation methods, such as random word deletion, and advanced data augmentation methods, such as back-translation. We also handcraft a line of rule-based data perturbation methods to simulate the features of informal sentences, such as spelling error injection. Furthermore, we propose three data filtering approaches in connection with the three evaluation metrics of FST: style strength, content preservation, and fluency. Specifically, we adopt style accuracy, source-BLEU, and perplexity as three metrics to filter out low-quality pseudo-parallel data based on a threshold. We also propose a dynamic threshold algorithm to automatically select and update the thresholds of source-BLEU and perplexity.</p><p>We evaluate our framework on the two domains of the GYAFC benchmark: Entertainment &amp; Music (E&amp;M) and Family &amp; Relationships (F&amp;R). We further collect 200k unpaired informal sentences for each domain to perform semi-supervised training. Experimental results verify that our SSL framework can enhance the performance of the strong supervised baseline, a pretrained T5-large <ref type="bibr" target="#b18">(Raffel et al., 2020)</ref> model, by a substantial margin, and improve the state-of-the-art results by over 2.0 BLEU scores on both GYAFC domains. Empirically, we also deduce that simple word-level data augmentation approaches are better than advanced data augmentation methods that excessively alter the sentences, and spelling error injection is especially effective. In addition, our evaluation-based data filtering approach can further improve the performance of the SSL framework. Furthermore, we also conduct low-resource experiments by reducing the size of parallel data. Surprisingly, our framework could achieve the state-of-the-art results with only less than 40% of parallel data, demonstrating the advantage of our method in low-resource situations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Formality style transfer FST is an important branch of text style transfer. For FST, <ref type="bibr" target="#b19">Rao and Tetreault (2018)</ref> released a high-quality parallel dataset -GYAFC, comprising two sub-domains and approximately 50k parallel data for each domain. Previous studies <ref type="bibr" target="#b19">(Rao and Tetreault, 2018;</ref><ref type="bibr" target="#b15">Niu et al., 2018;</ref><ref type="bibr" target="#b27">Xu et al., 2019;</ref><ref type="bibr" target="#b31">Zhang et al., 2020)</ref> typically train seq2seq encoder-decoder models on this benchmark. Recent studies <ref type="bibr" target="#b24">(Wang et al., 2019;</ref><ref type="bibr" target="#b29">Yao and Yu, 2021;</ref><ref type="bibr" target="#b2">Chawla and Yang, 2020;</ref><ref type="bibr" target="#b10">Lai et al., 2021)</ref> have deduced that fine-tuning large-scale pretrained models such as <ref type="bibr">GPT-2 (Radford et al., 2019)</ref> and <ref type="bibr">BART (Lewis et al., 2020)</ref> on the parallel corpora can improve the performance. To address the data-scarcity problem of parallel datasets, <ref type="bibr" target="#b31">Zhang et al. (2020)</ref> proposed three data augmentation techniques to augment pseudo-parallel data for training. Similar to prior research on text style transfer that adopt back-translation <ref type="bibr" target="#b32">(Zhang et al., 2018;</ref><ref type="bibr" target="#b10">Lample et al., 2018;</ref><ref type="bibr" target="#b17">Prabhumoye et al., 2018;</ref><ref type="bibr" target="#b13">Luo et al., 2019)</ref>, some other approaches on FST <ref type="bibr" target="#b22">(Shang et al., 2019;</ref><ref type="bibr" target="#b27">Xu et al., 2019;</ref><ref type="bibr" target="#b2">Chawla and Yang, 2020)</ref> adopt a cycle-reconstruction scheme, where an additional backward transfer model is jointly trained together with the forward transfer model, and the two models generate pseudo-paired data for each other via iterative back-translation. Although <ref type="bibr" target="#b27">Xu et al. (2019)</ref> and <ref type="bibr" target="#b2">Chawla and Yang (2020)</ref> train a single model to perform bidirectional transfer, the generation of both directions remain disentangled by a control variable, making each direction rely on the unlabeled data of its target side. Therefore, the unlabeled informal sentences exert no direct effects on the informal?formal transfer. In contrast, our work focuses on how to better utilize source-side unlabeled data (i.e., informal sentences) using SSL and does not introduce any extra models.</p><p>SSL with consistency regularization SSL is popular for its advantage in utilizing unlabeled data. Consistency regularization (also known as consistency training) <ref type="bibr" target="#b20">(Sajjadi et al., 2016)</ref> is an important component of recent SSL algorithms on image and text classification <ref type="bibr" target="#b14">(Miyato et al., 2018;</ref><ref type="bibr">Tarvainen and Valpola, 2017;</ref><ref type="bibr" target="#b0">Berthelot et al., 2019;</ref><ref type="bibr" target="#b23">Sohn et al., 2020)</ref>. It enforces a model to produce invariant predictions for an unlabeled data and its perturbed version. These studies developed different data perturbation <ref type="bibr" target="#b26">(Xie et al., 2020;</ref><ref type="bibr" target="#b0">Berthelot et al., 2019)</ref> or data filtering <ref type="bibr" target="#b30">(Zhang et al., 2021;</ref><ref type="bibr" target="#b28">Xu et al., 2021)</ref> approaches to improve the performance. However, few studies have been made on how to apply consistency training in natural language generation (NLG) tasks such as FST because of the different target spaces, i.e., instead of single class labels or probabilities, the output of NLG is the combination of discrete NL tokens. This renders the experiences in classification tasks not applicable to FST. For instance, classification probabilities are typically adopted as the metric to filter high-confidence pseudo-examples for consistency training in classification tasks <ref type="bibr" target="#b23">(Sohn et al., 2020;</ref><ref type="bibr" target="#b26">Xie et al., 2020;</ref><ref type="bibr" target="#b30">Zhang et al., 2021)</ref>, which is implausible in FST. A similar study <ref type="bibr" target="#b7">(He et al., 2019)</ref> improved self-training by injecting noise into unlabeled inputs and proved its effectiveness on machine translation and text summarization; however, self-training involves multiple iterations to collect pseudo-parallel data and retrain the model, hence the training is not end-to-end. In this study, we explore various data perturbation strategies and propose effective data filtering approaches to realize a successful consistency-based framework for FST, which may also provide useful insights for future studies on semi-supervised NLG.</p><p>3 Method</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Base Model</head><p>FST involves rewriting an informal sentence into a formal one. Formally, given a sentence x = (x 1 , x 2 , . . . , x n ) of length n with style S, our objective is to transform it into a target sentence y = (y 1 , y 2 , . . . , y m ) of length m and style T , while preserving its content. Following prior studies <ref type="bibr" target="#b19">(Rao and Tetreault, 2018;</ref><ref type="bibr" target="#b31">Zhang et al., 2020;</ref><ref type="bibr" target="#b2">Chawla and Yang, 2020;</ref><ref type="bibr" target="#b10">Lai et al., 2021)</ref> on FST, we employ the supervised baseline as a seq2seq encoder-decoder model that directly learns the conditional probability P (y|x) from parallel corpus D comprising (x, y) pairs. The objective is the cross-entropy loss between the decoder outputs and the ground-truth target sentences:</p><formula xml:id="formula_0">L sup = E (x,y)?D [-log P (y|x; ?)] = E (x,y)?D [- i log P (y i |y 1:i-1 , x; ?)],<label>(1)</label></formula><p>where ? denotes the model parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Consistency Training</head><p>Our approach leverages the idea of consistency regularization <ref type="bibr" target="#b20">(Sajjadi et al., 2016)</ref> and enforces a model to generate similar target sentences for an original and perturbed unlabeled sentence. Simultaneously, the model is also trained on the supervised data. Accordingly, the knowledge garnered from supervised training can be gradually transferred to unsupervised training. An overview of our framework is presented in Figure <ref type="figure">1</ref>. Typically, the consistency training loss is computed on the divergence between predictions on an unlabeled input u and its perturbed version ? = c(u), where c(?) is the perturbation function and u ? U S represents a source-side unlabeled sentence (in our case, an informal sentence). Formally, consistency training can be defined as minimizing the following unsupervised loss:</p><formula xml:id="formula_1">E u?U S D [P (y|u; ?)||P (y|c(u); ?)] ,<label>(2)</label></formula><p>where D[?||?] denotes a divergence loss. In practice, we adopt pseudo-labeling <ref type="bibr" target="#b11">(Lee et al., 2013)</ref> to train the unsupervised loss, for which we fix the model parameter ? to predict a "hard label" (pseudo target sentence) ? for u and enforce the consistency of model prediction by training ? with (c(u), ?).</p><p>Hence the unsupervised objective can be optimized as a standard cross-entropy loss as follows:</p><formula xml:id="formula_2">L unsup = E u?U S E ??P (y|u; ?) [-log P (?|c(u); ?)],<label>(3)</label></formula><p>where ? denotes a fixed copy of ?. This training process does not introduce additional model parameters. The entire additional training cost to supervised learning is a training pass and a generation pass for each unlabeled sentence.</p><p>As the overall objective, we train a weighted sum of the supervised loss in Equation ( <ref type="formula" target="#formula_0">1</ref>) and the unsupervised loss in Equation ( <ref type="formula" target="#formula_2">3</ref>):</p><formula xml:id="formula_3">L = L sup + ?L unsup ,<label>(4)</label></formula><p>where ? represents a hyper-parameter for balancing the effects of supervised and unsupervised training.</p><p>To achieve a good initial model for consistency training, we first pretrain the model on the supervised loss for several warm-up steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Data Perturbation Strategies</head><p>Data perturbation is the key component of consistency-based SSL algorithms <ref type="bibr" target="#b26">(Xie et al., 2020;</ref><ref type="bibr" target="#b3">Chen et al., 2020)</ref> and significantly affects the performance. In this section, we briefly introduce a collection of different data perturbation methods explored in this research. First, we consider some easy data augmentation methods commonly used for supervised data augmentation, which includes ? word deletion (drop)<ref type="foot" target="#foot_1">2</ref> : to randomly drop a proportion of words in the sentence.</p><p>? word swapping (swap): to randomly swap a proportion of words with their neighbouring words.</p><p>? word masking (mask): to randomly replace words with a mask token "_".</p><p>? word replacing with synonym (synonym): to randomly replace some words with a synonym based on WordNet <ref type="bibr" target="#b6">(Fellbaum, 1998)</ref>.</p><p>In addition, we consider advanced data augmentation methods that have proven effective in semisupervised text classification <ref type="bibr" target="#b26">(Xie et al., 2020)</ref>:</p><p>? back-translation: to translate a sentence into a pivot language, then translate it back to obtain a paraphrase of the original one.</p><p>? TF-IDF based word replacing (tf-idf): to replace uninformative words with low TF-IDF scores while retaining those with high TF-IDF values.</p><p>Furthermore, we handcraft a set of rule-based data perturbation for FST. There are some typical informal expressions in the parallel corpus, such as the use of slang words and abbreviations, capitalized words for emphasis, and spelling errors. Some existing studies <ref type="bibr" target="#b24">(Wang et al., 2019;</ref><ref type="bibr" target="#b29">Yao and Yu, 2021)</ref> adopt editing rules to revise such informal expressions as a preprocessing step. Inspired by these, we propose the adoption of opposite rules to synthesize such noises. We consider the following methods:</p><p>? spelling error injection (spell): to randomly inject spelling errors to a proportion of words by referring to a spelling error dictionary.</p><p>? word replacing with abbreviations (abbr): to replace all the words in the sentence with their abbreviations or slang words (e.g., "are you" ? "r u") by referring to an abbreviation dictionary.</p><p>? word capitalization (capital): to randomly capitalize a proportion of words.</p><p>These rule-based methods can inject noise into the unlabeled informal sentences without changing its informality, but strengthening it instead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Evaluation-Based Data Filtering</head><p>In the consistency training loss, the noisy pseudotarget ? is generated from the decoder model and may exert negative effects on the training. Therefore, we propose three evaluation-based data filters in connection with the evaluation metrics of FST. Specifically, we attempt to measure the quality of pseudo-target sentences by considering the three most important evaluation criteria of text style transfer: style strength, content preservation, and fluency. Next, we comprehensively explain each evaluation metric and the corresponding data filter.</p><p>Style strength measures the formality of generated sentences. Typically, people adopt binary classifiers such as TextCNN <ref type="bibr" target="#b4">(Chen, 2015)</ref> classifiers to judge the formality of a sentence <ref type="bibr" target="#b10">(Lai et al., 2021)</ref>. Inspired by this, we pretrain a TextCNN formality classifier on the parallel training corpus (i.e., GYAFC) to distinguish between informal and formal sentences. For an unlabeled informal sentence u and its pseudo target sentence ?, we maintain (c(u), ?) for unsupervised training only when</p><formula xml:id="formula_4">p + cls (?) -p + cls (u) &gt; ?,<label>(5)</label></formula><p>where p + cls (?) represents the probability of the sentence being formal, predicted by the style classifier and ? is a threshold of the probability. This guarantees that only the sentence pairs with strong style-differences are used for consistency training.</p><p>Content preservation is another important evaluation metric of FST, typically measured with BLEU between the ground-truth target sentence and the model generations. In unsupervised text style transfer where no ground-truth target exists, source-BLEU is adopted as an alternative, i.e., the BLEU scores between the source input sentence and the generated target sentence. Similarly, we propose the adoption of source-BLEU between u and ? as the metric to filter out pseudo targets that present poor content preservation.</p><p>Fluency is also used to evaluate the quality of generated sentences. We follow <ref type="bibr" target="#b8">(Hu et al., 2020)</ref> to pretrain an N-gram language model on the training data to estimate the empirical distributions of formal sentences. Then, the perplexity score is calculated for the pseudo target sentence ? by the language model. The motivation is that the sentences with lower perplexity scores match the empirical distribution of formal sentences better, and are thus considered as more fluent.</p><p>A natural idea is to filter out pseudo-parallel data based on a source-BLEU or a perplexity threshold. However, it is infeasible to determine the optimal threshold for the two metrics beforehand because the pseudo paired data are generated on-the-fly during the training and we cannot know the distribution of the BLEU or perplexity scores. In addition, choosing the BLEU/perplexity threshold is not as easy as tuning the style probability ? because they heavily depend on the data distribution and exhibit varying ranges of values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Dynamic Threshold Selection</head><p>To realize the selection of thresholds for the BLEUand perplexity-based filters, we propose a dynamic threshold strategy based on the distribution of the scores computed for already generated pseudopaired sentences. Specifically, we maintain an ordered list L to store the scores calculated for previously generated pseudo data and update it continuously following the training. At each iteration, a batch of new scores are inserted into L while maintaining the decreasing order of the list. Subsequently, we update the threshold as the value at a certain position L[? ? len(L)] in the score list, where len(L) denotes the length of the current score list and ? ? [0, 1] represents a ratio that determines the threshold's position in the list. We only keep pseudo data with scores higher (or lower for perplexity scores) than the threshold for consistency training. This actually makes ? approximately the proportion of pseudo data we keep for training, making it more convenient to control the trade-off between the qualities and quantities of selected pseudo data. More details are provided in Appendix B, C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We introduce the experimental settings in Section 4.1. To obtain relevant findings on how to build an effective consistency training framework for FST, we first empirically study the effects of multiple data perturbation methods in Section 4.2 and prove the effectiveness of consistency training via comparisons with the base model. Then, we vali-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>Datasets We evaluate our framework on the GYAFC <ref type="bibr" target="#b19">(Rao and Tetreault, 2018)</ref> benchmark for formality style transfer. It comprises crowdsourced informal-formal sentence pairs split into two domains, namely, E&amp;M and F&amp;R. The informal sentences in the dataset were originally selected from the same domains in Yahoo Answers L6 corpus <ref type="foot" target="#foot_2">3</ref> . We focus on the informal-formal style transfer because it is more realistic in applications. We further collected massive amounts of informal sentences from each of the two domains in Yahoo Answers L6 corpus as the unsupervised data. The statistics of the datasets are presented in Table <ref type="table" target="#tab_2">2</ref>.</p><p>Implementation Details We employ PyTorch <ref type="bibr" target="#b16">(Paszke et al., 2019)</ref> for all the experiments. We pretrain a TextCNN style classifier on the supervised data for each domain of GYAFC, following the setting in <ref type="bibr" target="#b10">(Lai et al., 2021)</ref>. The same classifier is adopted for both the style accuracy evaluation and the style strength filter in our SSL framework. We adopt HuggingFace Transformers <ref type="bibr" target="#b25">(Wolf et al., 2020)</ref> library's implementation of pretrained T5-Large <ref type="bibr" target="#b18">(Raffel et al., 2020)</ref> as the base model. We adopt the Adam (Kingma and Ba, 2014) optimizer with the initial learning rate 2 ? 10 -5 to train all the models. More details of hyper-parameters and model configurations are provided in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Metrics</head><p>The main evaluation metric for FST is the BLEU score between the generated sentence and four human references in the test set.</p><p>We adopt the corpus BLEU in NLTK <ref type="bibr" target="#b12">(Loper and Bird, 2002)</ref> following <ref type="bibr" target="#b2">(Chawla and Yang, 2020)</ref>. In addition, we also pretrained a TextCNN formality classifier to predict the formality of transferred sentences and calculate the accuracy (Acc.). Furthermore, we compute the harmonic mean of BLEU and style accuracy as an overall score, following the settings in <ref type="bibr" target="#b10">(Lai et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Effects of Data Perturbation Methods</head><p>In this experiment, we validate the effectiveness of our consistency training framework and compare the effects of different data perturbation methods. Specifically, we adopt the nine data perturbation methods introduced in Section 3.3 and include the no-perturbation variant that indicates directly using an unlabeled sentence and its pseudo target to train the unsupervised loss. We adopted no data filtering strategy in this experiment to simplify the comparison.</p><p>As shown in Table <ref type="table" target="#tab_4">3</ref>, our framework could consistently improve the base model by using different perturbation methods; however, back-translation resulted in mostly lower results than the base model. This contradicts the conclusion in <ref type="bibr" target="#b26">(Xie et al., 2020)</ref> that back-translation is especially powerful for semi-supervised text classification. We attribute this to the fact that back-translation tends to change the entire sentence into a semantically similar but syntactically different sentence. Compared with other word-level perturbation strategies, backtranslation triggers a larger mismatch between the perturbed input and the pseudo-target sentence generated from the unperturbed input, leading to a poor content preservation ability of the model. In contrast, simple word-level noises achieved consistently better results, especially spell error (spell), random word swapping (swap), and abbreviation replacing (abbr). These three methods tend to alter the words but do not lose their information while other methods eliminate the entire word by deleting (drop, mask) or replacing it with another word (synonym, tf-idf ). This may also cause a larger mismatch between the pseudo input and output.</p><p>Hence, we draw the conclusion that simple wordlevel perturbations tend to bring more effects. This differs from the observations in text classification <ref type="bibr" target="#b26">(Xie et al., 2020)</ref> because content preservation is important in FST. In particular, we also found that spell achieved the highest BLEU scores on both datasets. However, adding no perturbation  even resulted in a worse performance than the base model. Moreover, capital is also relatively weaker than the other two rule-based methods because it only changes the case of a chosen word. This suggests that the perturbation should not be too simple either.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Effects of Data Filtering</head><p>In this section, we analyze whether our proposed data filters are beneficial to the performance of our consistency training framework. Specifically, we chose the most effective data perturbation method spell to analyze the effects of adding the three data filters: style strength (style), content preservation (bleu), and fluency (lm) filters. As presented in Table <ref type="table" target="#tab_5">4</ref>, the results for different datasets and different filters have different tendencies. For example, adding the style filter on the E&amp;M dataset caused negative effects while contributing the best results to the F&amp;R domain. Although a filter does not necessarily improve the result, this is reasonable because filters result in less pseudo data for model training and it is difficult to control the trade-off between the quality and the quantity of selected data. Nevertheless, we still observe that the bleu filter contributes to the highest performance of spell for all the metrics on the E&amp;M domain, while style benefits the performance of spell the most on F&amp;R, leading to the best performing models of our approach<ref type="foot" target="#foot_3">4</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison with Previous Works</head><p>We compare our best model with the following previous studies on GYAFC.</p><p>? NMT <ref type="bibr" target="#b19">(Rao and Tetreault, 2018</ref>) is an LSTMbased encoder-decoder model with attention.</p><p>? GPT-CAT <ref type="bibr" target="#b24">(Wang et al., 2019)</ref> adopts GPT-2 and rule-based pre-processing for informal sentences.</p><p>? NMT-Multi-task <ref type="bibr" target="#b15">(Niu et al., 2018)</ref> jointly solves monolingual formality transfer and formality-sensitive machine translation via multi-task learning.</p><p>? Hybrid Annotations <ref type="bibr" target="#b27">(Xu et al., 2019)</ref>   ? Transformers (DA) <ref type="bibr" target="#b31">(Zhang et al., 2020)</ref> uses three data augmentation methods, including back-translation, formality discrimination, and multi-task transfer.</p><p>? CARI <ref type="bibr" target="#b29">(Yao and Yu, 2021)</ref> improves GPT-CAT by using BERT <ref type="bibr" target="#b5">(Devlin et al., 2018)</ref> to select optimal rules to pre-process the informal sentences.</p><p>? Chawla's <ref type="bibr" target="#b2">(Chawla and Yang, 2020)</ref> uses language model discriminators and maximizing mutual information to improve a pretrained BART-Large (Lewis et al., 2020) model, along with a cycle-reconstruction loss to utilize unlabeled data.</p><p>? BART-large+SC+BLEU <ref type="bibr" target="#b10">(Lai et al., 2021)</ref> improves BART-large by incorporating reinforcement learning rewards to enhance style change and content preservation.</p><p>We also report the results of Ours (base), our backbone T5-large model, and Ours (best), our best performing models selected from Table <ref type="table" target="#tab_5">4</ref>. As observed in Table <ref type="table" target="#tab_6">5</ref>, Ours (best) outperforms previous state-of-the-art models by a substantial margin and improves the BLEU scores from 76.17 and 79.92 to 78.75 and 81.37, respectively, on the E&amp;M and F&amp;R domains of the GYAFC benchmark. Although BART-large+SC+BLEU achieved better results on the Acc. of F&amp;R, the only released official outputs of BART-large+SC+BLEU were obtained from a model that was trained on the training data of both domains and adopted rewards to directly optimize style accuracy; hence, it is not directly comparable to our model. Ours (best) improves the fine-tuned T5-large baseline by a large margin as well, demonstrating the effectiveness of our SSL framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human Evaluation</head><p>We also conduct human evaluation to better capture the quality of the models' outputs. Following <ref type="bibr" target="#b31">(Zhang et al., 2020)</ref>, we measure the Formality, Fluency, and Meaning Preservation of generated sentences by asking two human annotators to assign a score ranging from {0, +1, +2} regarding each aspect. We randomly sampled 50 examples from the test set of each domain and compare the generated outputs of Ours (base), Ours (best), and the previous state-of-the-  art Chawla's model trained on the single-domain data. In addition, the annotators were unaware of the corresponding model of each output. As shown in Table <ref type="table" target="#tab_7">6</ref>, the human evaluation results are consistent with the automatic evaluation results: Ours (base) is competitive compared with Chawla's, while Ours (best) improves over the base model and outperforms the previous state-of-the-art on all the metrics, except that it presents lower results on Meaning than Ours (base) on F&amp;R. More details on human evaluation can be found in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative Examples</head><p>We present some of the generated outputs of Ours (base), Ours (best), and Chawla's in Table <ref type="table" target="#tab_10">8</ref>. It can be observed that all the models can produce high-quality outputs with considerable formality, meaning preservation and fluency. Nevertheless, Ours (best) exhibits a stronger capability to modify the original sentence, especially for some informal expressions, leading to the best performance on the Formality metric. For example, it replaced "like" with "similar to" in Example 2 and deleted the informal word "guys" in Example 3. However, it may alter the original sentence so much that the meaning of the sentence is changed to some extent (Example 1). This may explain why Ours (best) achieves a lower Meaning score than Ours (base) on F&amp;R.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Low-Resource Experiments</head><p>We also simulate the low-resource settings by further reducing the size of available parallel data. Specifically, we randomly sample from the original training data with a size in the range of {100, 1000, 5000, 20000} and compare the results of the base model T5-Large with our SSL model. The size of unlabeled data remains 200k for each domain.</p><p>We adopt the spell data perturbation without any data filter and avoid exhaustive hyper-parameter tuning. Table <ref type="table" target="#tab_8">7</ref> demonstrates that our framework is especially effective under few-shot settings when only 100 parallel data are available. By comparing with previous state-of-the-art results on FST, we can observe that our approach can achieve competitive results with only 5000 (&lt; 10%) parallel training data, and even better results with only 20000 (&lt; 40%) parallel examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this study, we proposed a simple yet effective consistency-based semi-supervised learning framework for formality style transfer. Unlike previous studies that adopted cycle-reconstruction to utilize additional target-side sentences for backtranslation, our method offers a different view, to leverage source-side unlabeled sentences. Without introducing additional model parameters, our method can easily outperform the strong supervised baseline and achieve the new state-of-the-art results on formality style transfer datasets. For future work, we will attempt to generalize our approach to other text generation scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Detailed Experimental Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Hyper-Parameters</head><p>We set the max length of input sentences to 50 Byte-Pair Encoding <ref type="bibr" target="#b21">(Sennrich et al., 2016)</ref> tokens. The weight of unsupervised loss ? is set to 1.0 in all our experiments, which is an empirical choice from previous studies <ref type="bibr" target="#b23">(Sohn et al., 2020)</ref>. The batch size is 8 for the supervised objective and 56 for the unsupervised objective, such that the model can leverage more unlabeled data for training. The threshold ? for the style strength filter is set to 0.8 and the threshold ratio ? is set to 0.4 for both the content preservation and fluency filters. We tested ? in the discrete range between 0.5 and 0.9 and for ?, we searched over the values between 0.1 and 0.8. Although the chosen values of ? and ? are not necessarily the best for all the datasets, we fix them in later experiments for their reasonable results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Training Details</head><p>We train two binary style classifiers on each domain of GYAFC. The training data are the formal and informal sentences in the original training sets of the E&amp;M and F&amp;R domain. The classifiers are validated on the formal sentences in the original validation set. The classifier for E&amp;M could achieve 95.69% accuracy on the validation set, while the classifier for F&amp;R achieved 94.70%. We adopt a 4-gram Kneser-Ney language model to compute perplexity scores for the fluency data filter. During semi-supervised training, first pretrain the model solely on the supervised data for 2000 steps to achieve a good initialization of the model parameters. Then, we jointly train the supervised and consistency losses simultaneously. The model checkpoint is validated with an interval of 1000 steps and selected based on the best BLEU score on the validation set. Early stopping is also adopted with patience 10. We employ beam search with beam width 5 for the model's generations and pseudo-target prediction<ref type="foot" target="#foot_4">5</ref> . All our experiments are conducted on NVIDIA A100 (40GB) GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Details of Unlabeled Data Collection</head><p>We collected 200k from each of the E&amp;M and F&amp;R domains of Yahoo Answers L6 corpus. The collection procedure is as follows. ( <ref type="formula" target="#formula_0">1</ref>) We chose the passages labeled "&lt;bestanswer&gt;" in the corpus and tokenized them into separate sentences. (2) We filtered out sentences with formality scores larger than 0.5 (i.e. judged as formal) predicted by the style classifier we built for model evaluation. <ref type="bibr" target="#b34">(3)</ref> We built an N-gram language model by training on the informal sentences in the original training data of GYAFC, and used it to generate perplexity scores for these sentences. We kept 200k sentences with lowest perplexity scores, such that we obtained a collection of the most informal sentences in the corpus. We only observed one overlapping sentence with the test set of each domain, which we considered negligible and kept in the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Details of Data Perturbation</head><p>All our data perturbation methods are implemented based on the nlpaug<ref type="foot" target="#foot_5">6</ref> library. For the abbr perturbation, we adopt the abbreviation dictionary which <ref type="bibr" target="#b1">Briakou et al. (2021)</ref> used for rule-based pre-processing. We set the ratio of perturbed words in a sentence to 0.1 for all word-level perturbation methods and deduced that increasing the ratio could often result in lower results, as that will enhance the difference between the original and perturbed sentences, which is consistent with our conclusion in Section 4.2. We present examples of all data perturbation methods in Table <ref type="table">9</ref>.</p><p>We also attempted mixing different perturbations with spell, but did not obtain better results than single spell. This can also be attributed to the conclusion that simple perturbations are even better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Formal Description of the Algorithm</head><p>Here, we provide a formal algorithmic description of our consistency training framework in Algorithm 1 and assume that we adopt content preservation (BLEU) data filtering or fluency (perplexity) data filtering in this algorithm to include the formal description of our dynamic threshold strategy. We omit the case when we adopt style strength filtering because it does not use the dynamic threshold and is more straightforward to understand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Details of Dynamic Threshold Selection</head><p>Here, we provide more details of the dynamic threshold strategy for the content preservation and fluency filters. In practice, we do not filter any pseudo data in the initial warm-up steps of consistency training, to initialize the score list. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Details of Human Evaluation</head><p>We describe the rating criteria in the human evaluation. We ask two well-educated annotators to rate the formality, fluency, and meaning preservation on a discrete scale from 0 to 2 for the model outputs, following <ref type="bibr" target="#b31">(Zhang et al., 2020)</ref>. During the annotation, we randomly shuffle the sentences from the three models and make the model names invisible to annotators.</p><p>Formality The annotator are asked to rate the formality change level given a source informal sentence and the generated output sentence, regardless of the fluency and meaning preservation. If the output sentence improves the formality of the source sentence significantly, the score will be 2 points. If the output sentence improves the formality but still keeps some informal expressions, or the improve-ment is minimal, it will be rated 1 point. If there is no improvement on the formality, it will be rated 0 points.</p><p>Fluency The fluency is rated 2 points if the output sentence is meaningful and has no grammatical error. If the target sentence is meaningful but contains some minor grammatical errors, it will be rated 1 point. If the sentence is incoherent, it will be rated 0 points.</p><p>Meaning Preservation Given a source sentence and a corresponding output sentence, the raters are asked to ascertain how much information is preserved in the output sentence compared to the input sentence. If the two sentences are exactly equivalent, the output obtains 2 points. If they are mostly equivalent but different in some trivial details, the output will receive 1 point. If the output omits important details that alter the meaning of the input sentence, it is rated 0 points.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>arXiv:2203.13620v1 [cs.CL] 25 Mar 2022 parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Furthermore, after iterating an epoch of the unsupervised data, we keep the current threshold fixed and do not update the score list any more. The score list is implemented as a skiplist to enable O(log N ) insertion into an ordered list. The overall time complexity of the data filtering is O(log1 + log 2 + ? ? ? + log N ) = O(log N !) = O(N log N ),where N is the number of unlabeled data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The statistics of datasets.</figDesc><table><row><cell>date our consistency training model with different</cell></row><row><cell>data filtering methods in Section 4.3 and demon-</cell></row><row><cell>strate their additional effects on the SSL frame-</cell></row><row><cell>work. Based on the findings in these two experi-</cell></row><row><cell>ments, we further compare our best models with</cell></row><row><cell>previous state-of-the-art models in Section 4.4. We</cell></row><row><cell>also include case studies in Section 4.4 to present</cell></row><row><cell>some qualitative examples. Finally, we conduct</cell></row><row><cell>low-resource experiments (Section 4.5) to demon-</cell></row><row><cell>strate our method's advantage when less parallel</cell></row><row><cell>data are available.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Effects of different data perturbations in our approach on the test splits of GYAFC. The best scores among all the model variants are boldfaced.</figDesc><table><row><cell></cell><cell></cell><cell>E&amp;M</cell><cell></cell><cell></cell><cell>F&amp;R</cell><cell></cell></row><row><cell cols="3">Model variants BLEU Acc(%)</cell><cell>HM</cell><cell cols="2">BLEU Acc(%)</cell><cell>HM</cell></row><row><cell>spell (no-filter)</cell><cell>78.37</cell><cell>94.21</cell><cell>85.56</cell><cell>81.09</cell><cell>85.59</cell><cell>83.28</cell></row><row><cell>spell (+style)</cell><cell>78.19</cell><cell>93.79</cell><cell>85.28</cell><cell>81.37</cell><cell>86.41</cell><cell>83.81</cell></row><row><cell>spell (+bleu)</cell><cell>78.75</cell><cell>94.56</cell><cell>85.94</cell><cell>81.11</cell><cell>86.34</cell><cell>83.64</cell></row><row><cell>spell (+lm)</cell><cell>78.24</cell><cell>94.56</cell><cell>85.63</cell><cell>80.93</cell><cell>86.34</cell><cell>83.55</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Effects of different data filtering methods in our approach on the test splits of GYAFC. Scores larger than the no-filter variant are in bold.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Comparison between our approach and existing works on the test splits of GYAFC. ? indicates we recalculate the scores with our evaluation metrics for the output given in the paper. Otherwise, we copy the results from the paper. * indicates that the model used training data from both domains and is not comparable to our model.</figDesc><table><row><cell>trains</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Human evaluation results.</figDesc><table><row><cell></cell><cell cols="2">E&amp;M</cell><cell cols="2">F&amp;R</cell></row><row><cell cols="5">#Parallel data BLEU Acc(%) BLEU Acc(%)</cell></row><row><cell>100 (base)</cell><cell>59.94</cell><cell>61.58</cell><cell>65.13</cell><cell>49.32</cell></row><row><cell>100 (ours)</cell><cell>64.40</cell><cell>82.91</cell><cell>71.11</cell><cell>55.11</cell></row><row><cell>1000 (base)</cell><cell>70.49</cell><cell>83.26</cell><cell>75.36</cell><cell>76.58</cell></row><row><cell>1000 (ours)</cell><cell>72.22</cell><cell>85.81</cell><cell>76.70</cell><cell>76.20</cell></row><row><cell>5000 (base)</cell><cell>75.13</cell><cell>89.55</cell><cell>77.65</cell><cell>78.38</cell></row><row><cell>5000 (ours)</cell><cell>75.67</cell><cell>87.08</cell><cell>78.87</cell><cell>81.01</cell></row><row><cell>20000 (base)</cell><cell>76.55</cell><cell>90.96</cell><cell>79.25</cell><cell>83.33</cell></row><row><cell>20000 (ours)</cell><cell>76.59</cell><cell>92.09</cell><cell>80.61</cell><cell>86.11</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Experimental results on test sets under lowresource settings with varied parallel data size.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>SourceI like natural / real girls, I don't like fake looking prissy drama queens.Ours(best)I like natural looking girls, not pretentious drama queens.Ours(base)I like natural, real girls, I do not like fake looking, prissy drama queens. Chawla's I like natural and real girls , I do not like fake looking prissy drama queens . Human-Annotation I like natural and real girls, not fake-looking, prissy drama queens. That is like Broke Back Mountain for little John Wanye . Human-Annotation That is similar to "Brokeback Mountain" for young John Wayne. You guys do not have any reason to hate each other . Human-Annotation There is no reason for you two to dislike each other.</figDesc><table><row><cell>Example 1</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Source</cell><cell>That's like Broke Back Mountain for little John Wanye.</cell></row><row><cell></cell><cell>Ours(best)</cell><cell>That is similar to "Broke Back Mountain" for John Wayne.</cell></row><row><cell>Example 2</cell><cell>Ours(base)</cell><cell>That is like "Broke Back Mountain" for John Wayne.</cell></row><row><cell></cell><cell>Chawla's</cell><cell></cell></row><row><cell></cell><cell>Source</cell><cell>You guys don't have any reason to hate each other.</cell></row><row><cell></cell><cell>Ours(best)</cell><cell>You do not have any reason to hate each other.</cell></row><row><cell>Example 3</cell><cell>Ours(base)</cell><cell>You guys do not have any reason to hate each other.</cell></row><row><cell></cell><cell>Chawla's</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Examples sampled from the test set outputs.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Code available at https://github.com/Aolius/ semi-fst.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>We abbreviate each method for ease of denotation.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://webscope.sandbox.yahoo.com/catalog.php ?datatype=l</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>Empirically, we also found that mixing up three filters achieved no better results than a single filter, possibly because this filtered out too much pseudo data.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>The pseudo target can also be obtained by sampling methods.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>https://github.com/makcedward/nlpaug</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This paper is based on results obtained from a project, <rs type="grantNumber">JPNP18002</rs>, commissioned by the <rs type="funder">New Energy and Industrial Technology Development Organization (NEDO)</rs>. <rs type="person">Ao Liu</rs> acknowledges financial support from the <rs type="funder">Advanced Human Resource Development Fellowship for Doctoral Students, Tokyo Institute of Technology</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Jhz8afW">
					<idno type="grant-number">JPNP18002</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original sentence</head><p>Well first you have to get lots of hands on experience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word deletion</head><p>Well first you have to get lots of on experience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word swapping</head><p>Well first have you to get lots of hands on experience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word masking</head><p>Well first _ have to get lots of hands on experience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word replacing with synonym</head><p>Well first you have to begin lots of hands on experience. Back-translation well first you have to get lots of years on experience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TF-IDF based word replacing</head><p>Well first you have walmartmusic get lots of hands on experience Spelling error injection</p><p>Well first you have to get lots of hands or experience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word replacing with abbreviations</head><p>Well first u have to get lots of hands on experience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word capitalization</head><p>Well FIRST you have to get lots of hands on experience.</p><p>Table <ref type="table">9</ref>: Examples of data perturbation methods. Different words compared to the original sentence are marked as red.</p><p>Algorithm 1 Training Procedure of our approach using dynamic threshold selection</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mixmatch: A holistic approach to semisupervised learning</title>
		<author>
			<persName><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="5049" to="5059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ol?, bonjour, salve! XFORMAL: A benchmark for multilingual formality style transfer</title>
		<author>
			<persName><forename type="first">Eleftheria</forename><surname>Briakou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3199" to="3216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semi-supervised formality style transfer using language model discriminator and mutual information maximization</title>
		<author>
			<persName><forename type="first">Kunal</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.212</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2340" to="2354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mix-Text: Linguistically-informed interpolation of hidden space for semi-supervised text classification</title>
		<author>
			<persName><forename type="first">Jiaao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.194</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2147" to="2157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Convolutional neural network for sentence classification</title>
		<author>
			<persName><forename type="first">Yahui</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
		<respStmt>
			<orgName>University of Waterloo</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">WordNet: An Electronic Lexical Database</title>
		<author>
			<persName><forename type="first">Christiane</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Bradford Books</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Revisiting self-training for neural sequence generation</title>
		<author>
			<persName><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc'aurelio</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Ka-Wei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charu</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aston</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.12742</idno>
		<title level="m">Text style transfer: A review and experimental evaluation</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Thank you BART! rewarding pre-trained models improves formality style transfer</title>
		<author>
			<persName><forename type="first">Huiyuan</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Toral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malvina</forename><surname>Nissim ; Guillaume Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc'aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y-Lan</forename><surname>Boureau</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-short.62</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018">2021. 2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="484" to="494" />
		</imprint>
	</monogr>
	<note>International Conference on Learning Representations</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Dong-Hyun</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.703</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</editor>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2013">2013. Marjan Ghazvininejad,. 2020</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
	<note>Workshop on challenges in representation learning, ICML</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">NLTK: The natural language toolkit</title>
		<author>
			<persName><forename type="first">Edward</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<idno type="DOI">10.3115/1118108.1118117</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-02 Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics</title>
		<meeting>the ACL-02 Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="63" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A dual reinforcement learning framework for unsupervised text style transfer</title>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Virtual adversarial training: a regularization method for supervised and semisupervised learning</title>
		<author>
			<persName><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shin-Ichi</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shin</forename><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1979" to="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-task neural models for translating between styles within and across languages</title>
		<author>
			<persName><forename type="first">Xing</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudha</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marine</forename><surname>Carpuat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1008" to="1021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Alch?-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Shrimai Prabhumoye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</editor>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018. 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="866" to="876" />
		</imprint>
	</monogr>
	<note>Style transfer through back-translation</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dear sir or madam, may i introduce the gyafc dataset: Corpus, benchmarks and metrics for formality style transfer</title>
		<author>
			<persName><forename type="first">Sudha</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="129" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Regularization with stochastic transformations and perturbations for deep semi-supervised learning</title>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehran</forename><surname>Javanmardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tolga</forename><surname>Tasdizen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1163" to="1171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semi-supervised text style transfer: Cross projection in latent space</title>
		<author>
			<persName><forename type="first">Mingyue</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piji</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenxin</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4937" to="4946" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ekin</forename><surname>Dogus Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Antti Tarvainen and Harri Valpola</publisher>
			<date type="published" when="2017">2020. 2017</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1195" to="1204" />
		</imprint>
	</monogr>
	<note>Fixmatch: Simplifying semi-supervised learning with consistency and confidence</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Harnessing pre-trained neural networks with rules for formality style transfer</title>
		<author>
			<persName><forename type="first">Yunli</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhoujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3573" to="3578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R?mi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unsupervised data augmentation for consistency training</title>
		<author>
			<persName><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6256" to="6268" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Ruochen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.06353</idno>
		<title level="m">Formality style transfer with hybrid textual annotations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dash: Semi-supervised learning with dynamic thresholding</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinxing</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Feng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baigui</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Jin</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="11525" to="11536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Improving formality style transfer with context-aware rule injection</title>
		<author>
			<persName><forename type="first">Zonghai</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.124</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1561" to="1570" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Flexmatch: Boosting semi-supervised learning with curriculum pseudo labeling</title>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yidong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenxin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jindong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manabu</forename><surname>Okumura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takahiro</forename><surname>Shinozaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Parallel data augmentation for formality style transfer</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3221" to="3228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Zhirui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.07894</idno>
		<title level="m">Style transfer as unsupervised machine translation</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Input: Parallel corpus D = {x, y} M , unlabeled corpus of source-side sentences US = {u} N , initialized model parameters ?; perturbation function c(?), supervised batch size B, unsupervised batch size ?B, weight factor ?, filter type f t ? {BLEU, perplexity}, a data filter score function f , an decreasing-ordered score list L, a function len(?) that returns the length of a list</title>
		<imprint/>
	</monogr>
	<note>Warm-up training 2: Initialize ? with pretrained T5</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m">Semi-supervised training 4: repeat 5: Sample a batch BD = {(xi, yi)} B i=1 from D. 6: Sample a batch BU = {ui} ?B i=1 from US</title>
		<imprint>
			<publisher>Finetune ? on D via Equation</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Obtain B U = {?i|?i = c(ui)} ?B i=1</title>
	</analytic>
	<monogr>
		<title level="m">Generate pseudo targets BY = {?i|?i = argmaxP</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
	<note>y|ui</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Compute a batch of data filter scores LB = {bi|bi = f (ui, ?i)} ?B i=1 10: Insert LB into L while maintaining the decreasing order of L. 11: Obtain s = L[? ? len(L)] as the threshold. 12: if f t = BLEU then 13: Obtain a filtered pseudo-parallel batch B f = {(?i, ?i)|bi &gt; s, i = 1, . . . , ?B} 14: else if f t = perplexity then 15: Obtain a filtered pseudo-parallel batch B f = {(?i, ?i)|bi &lt; s, i = 1</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
		</imprint>
	</monogr>
	<note>?B i=1 . 9. Compute consistency loss Lunsup = E (?,?)?B f [-log P (?|?; ?)</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m">Compute supervised loss Lsup = E (x,y)?B D [-log P</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m">L = Lsup + ?Lunsup and update ?. 20: until CONVERGE</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
