<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">X2Face: A network for controlling face generation using images, audio, and pose codes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Olivia</forename><surname>Wiles</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Visual Geometry Group</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">A</forename><forename type="middle">Sophia</forename><surname>Koepke</surname></persName>
							<email>koepke@robots.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Visual Geometry Group</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Visual Geometry Group</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">X2Face: A network for controlling face generation using images, audio, and pose codes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The objective of this paper is a neural network model that controls the pose and expression of a given face, using another face or modality (e.g. audio). This model can then be used for lightweight, sophisticated video and image editing. We make the following three contributions. First, we introduce a network, X2Face, that can control a source face (specified by one or more frames) using another face in a driving frame to produce a generated frame with the identity of the source frame but the pose and expression of the face in the driving frame. Second, we propose a method for training the network fully self-supervised using a large collection of video data. Third, we show that the generation process can be driven by other modalities, such as audio or pose codes, without any further training of the network. The generation results for driving a face with another face are compared to state-of-the-art self-supervised/supervised methods. We show that our approach is more robust than other methods, as it makes fewer assumptions about the input data. We also show examples of using our framework for video face editing. * Denotes equal contribution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Being able to animate a still image of a face in a controllable, lightweight manner has many applications in image editing/enhancement and interactive systems (e.g. animating an on-screen agent with natural human poses/expressions). This is a challenging task, as it requires representing the face (e.g. modelling in 3D) in order to control it and a method of mapping the desired form of control (e.g. expression or pose) back onto the face representation. In this paper we investigate whether it is possible to forgo an explicit face representation and instead implicitly learn this in a self-supervised manner from a large collection of video data. Further, we investigate whether this implicit representation can then be used directly to control a face with another modality, such as audio or pose information. To this end, we introduce X2Face, a novel self-supervised network architecture that can be used for face puppeteering of a source face given a driving vector.</p><p>Fig. <ref type="figure">1</ref>: Overview of X2Face: a model for controlling a source face using a driving frame, audio data, or specifying a pose vector. X2Face is trained without expression or pose labels.</p><p>The source face is instantiated from a single or multiple source frames, which are extracted from the same face track. The driving vector may come from multiple modalities: a driving frame from the same or another video face track, pose information, or audio information; this is illustrated in Fig. <ref type="figure">1</ref>. The generated frame resulting from X2Face has the identity, hairstyle, etc. of the source face but the properties of the driving vector (e.g. the given pose, if pose information is given; or the driving frame's expression/pose, if a driving frame is given). The network is trained in a self-supervised manner using pairs of source and driving frames. These frames are input to two subnetworks: the embedding network and the driving network (see Fig. <ref type="figure">2</ref>). By controlling the information flow in the network architecture, the model learns to factorise the problem. The embedding network learns an embedded face representation for the source face -effectively face frontalisation; the driving network learns how to map from this embedded face representation to the generated frame via an embedding, named the driving vector. The X2Face network architecture is described in Section 3.1, and the self-supervised training framework in Section 3.2. In addition we make two further contributions. First, we propose a method for linearly regressing from a set of labels (e.g. for head pose) or features (e.g. from audio) to the driving vector; this is described in Section 4. The performance is evaluated in Section 5, where we show (i) the robustness of the generated results compared to state-of-the-art self-supervised <ref type="bibr" target="#b44">[45]</ref> and supervised <ref type="bibr" target="#b0">[1]</ref> methods; and (ii) the controllability of the network using other modalities, such as audio or pose. The second contribution, described in Section 6, shows how the embedded face representation can be used for video face editing, e.g. adding facial decorations in the manner of <ref type="bibr" target="#b30">[31]</ref> using multiple or just a single source frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Explicit modelling of faces for image generation. Traditionally facial animation (or puppeteering) given one image was performed by fitting a 3DMM and then modifying the estimated parameters <ref type="bibr" target="#b2">[3]</ref>. Later work has built on the fitting of 3DMMs by including high level details <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b40">41]</ref>, taking into account additional images <ref type="bibr" target="#b32">[33]</ref> or 3D scans <ref type="bibr" target="#b3">[4]</ref>, or by learning 3DMM parameters directly from RGB data without ground truth labels <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b38">39]</ref>. Please refer to Zollhöfer et. al. <ref type="bibr" target="#b45">[46]</ref> for a survey.</p><p>Given a driving and source video sequence, a 3DMM or 3D mesh can be obtained and used to model both the driving and source face <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b42">43]</ref>. The estimated 3D is used to transform the expression of the source face to match that of the driving face. However, this requires additional steps to transfer the hidden regions (e.g. the teeth). As a result, a neural network conditioned on a single driving image can be used to predict higher level details to fill in these hidden regions <ref type="bibr" target="#b24">[25]</ref>.</p><p>Motivated by the fact that a 3DMM approach is limited by the components of the corresponding morphable model, which may not model the full range of required expressions/deformations and the higher level details, <ref type="bibr" target="#b0">[1]</ref> propose a 2D warping method. Given only one source image, <ref type="bibr" target="#b0">[1]</ref> use facial landmarks in order to warp the expression of one face onto another. They additionally allow for fine scale details to be transferred by monitoring changes in the driving video.</p><p>An interesting related set of works consider how to frontalise a face in a still image using a generic reference face <ref type="bibr" target="#b13">[14]</ref>, transferring expressions of an actor to an avatar <ref type="bibr" target="#b34">[35]</ref> and swapping one face with another <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b23">24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning based approaches for image generation.</head><p>There is a wealth of literature on supervised/self-supervised approaches; here we review only the most relevant work. Supervised approaches for controlling a given face learn to model factors of variation (e.g. lighting, pose, etc.) by conditioning the generated image on known ground truth information which may be head pose, expression, or landmarks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b43">44]</ref>. This requires a training dataset with known pose or expression information which may be expensive to obtain or require subjective judgement (e.g. in determining the expression). Consequently, self-supervised and unsupervised approaches attempt to automatically learn the required factors of variation (e.g. optical flow or pose) without labelling. This can be done by maximising mutual information <ref type="bibr" target="#b6">[7]</ref> or by training the network to synthesise future video frames <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b28">29]</ref>.</p><p>Another relevant self-supervised method is CycleGAN <ref type="bibr" target="#b44">[45]</ref> which learns to transform images of one domain into those of another. While not explicitly devised for this task, as CycleGAN learns to be cycle-consistent, the transformed images often bear semantic similarities to the original images. For example, a CycleGAN model trained to transform images of one person's face (domain A) into those of another (domain B), will often learn to map the pose/position/expression of the face in domain A onto the generated face from domain B.</p><p>Using multi-modal setups to control image generation. Other modalities, such as audio, can control image generation by using a neural network that learns the relationship between audio and correlated parts in corresponding images. Examples are controlling the mouth with speech <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b37">38]</ref>, controlling a head with audio and a known emotional state <ref type="bibr" target="#b15">[16]</ref>, and controlling body movement with music <ref type="bibr" target="#b35">[36]</ref>.</p><p>Our method has the benefits of being self-supervised and the ability to control the generation process from other modalities without requiring explicit modelling of the face. Thus it is applicable to other domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>This section introduces the network architecture in Section 3.1, followed by the curriculum strategy used to train the network in Section 3.2. Fig. <ref type="figure">2</ref>: An overview of X2Face during the initial training stage. Given multiple frames of a video (here 4 frames), one frame is designated the source frame and another the driving frame. The source frame is input to the embedding network, which learns a sampler to map pixels from the source frame to the embedded face. The driving frame is input to the driving network, which learns to map pixels from the embedded face to the generated frame. The generated frame should have the identity of the source frame and the pose/expression of the driving frame. In this training stage, as the frames are from the same video, the generated and driving frames should match. However, at test time the identities of the source and driving face can differ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Architecture</head><p>The network takes two inputs: a driving and a source frame. The source frame is input to the embedding network and the driving frame to the driving network. This is illustrated in Fig. <ref type="figure">2</ref>. Precise architectural details are given in the supplementary material. Embedding network. The embedding network learns a bilinear sampler to determine how to map from the source frame to a face representation, the embedded face. The architecture is based on U-Net <ref type="bibr" target="#b31">[32]</ref> and pix2pix <ref type="bibr" target="#b14">[15]</ref>; the output is a 2-channel image (of the same dimensions as the source frame) that encodes the flow δx, δy for each pixel.</p><p>While the embedding network is not explicitly forced to frontalise the source frame, we observe that it learns to do so for the following reason. Because the driving network samples from the embedded face to produce the generated frame without knowing the pose/expression of the source frame, it needs the embedded face to have a common representation (e.g. be frontalised) across source frames with differing poses and expressions. Driving network. The driving network takes a driving frame as input and learns a bilinear sampler to transform pixels from the embedded face to produce the generated frame. It has an encoder-decoder architecture. In order to sample correctly from the embedded face and produce the generated frame, the latent embedding (the driving vector) must encode pose/expression/zoom/other factors of variation. The network is trained with a curriculum strategy using two stages. The first training stage (I) is fully self-supervised. In the second training stage (II), we make use of a CNN pre-trained for face identification to add additional constraints based on the identity of the faces in the source and driving frames to finetune the model following training stage (I). I. The first stage (illustrated in Fig. <ref type="figure">2</ref>) uses only a pixelwise L1 loss between the generated and the driving frames. Whilst this is sufficient to train the network such that the driving frame encodes expression and pose, we observe that some face shape information is leaked through the driving vector (e.g. the generated face becomes fatter/longer depending on the face in the driving frame). Consequently, we introduce additional loss functions -called identity loss functionsin the second stage. II. In the second stage, the identity loss functions are applied to enforce that the identity is the same between the generated and the source frames irrespective of the identity of the driving frame. This loss should mitigate against the face shape leakage discussed in stage I. In practice, one source frame s A of identity A, and two driving frames d A ,d R are used as training inputs; d A is of identity A and d R a random identity. This gives two generated frames g d A , g d R respectively, which should both be of identity A. Two identity loss functions are then imposed: L identity (d A , g d A ) and L identity (s A , g d R ). L identity is implemented using a network pre-trained for identity to measure the similarity of the images in feature space by comparing appropriate layers of the network (i.e. a content loss as in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13]</ref>). The precise layers are chosen based on whether we are considering g d A or g d R :</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training the network</head><p>1. L identity (d A , g d A ). g d A should have the same identity, pose and expression as d A so we use the photometric L1 loss and a L1 content loss on the Conv2-5 and Conv7 layers (i.e. layers that encode both lower/higher level information such as pose/identity) between g d A and d A . 2. L identity (s A , g d R ) (Fig. <ref type="figure" target="#fig_0">3</ref>). g d R should have the identity of s A but the pose and expression of d R . Consequently, we cannot use the photometric loss but only a content loss. We minimise a L1 content loss on the Conv6-7 layers (i.e. layers encoding higher level identity information) between g d A and s A .</p><p>The pre-trained network used for these losses is the 11-layer VGG network (configuration A) <ref type="bibr" target="#b36">[37]</ref> trained on the VGG-Face Dataset <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Controlling the image generation with other modalities</head><p>Given a trained X2Face network, the driving vector can be used to control the source face with other modalities such as audio or pose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Pose</head><p>Instead of controlling the generation with a driving frame, we can control the head pose of the source face using a pose code such that when varying the code's pitch/yaw/roll angles, the generated frame varies accordingly. This is done by learning a forward mapping f p→v from head pose p to the driving vector v such that f p→v (p) can serve as a modified input to the driving network's decoder. However, this is an ill-posed problem; directly using this mapping loses information, as the driving vector encodes more than just pose.</p><p>As a result, we use vector arithmetic. Effectively we drive a source frame with itself but modify the corresponding driving vector v source emb to remove the pose of the source frame p source and incorporate the new driving pose p driving . This gives:</p><formula xml:id="formula_0">v driving emb = v source emb + v ∆pose emb = v source emb + f p→v (p driving − p source ).<label>(1)</label></formula><p>However, VoxCeleb <ref type="bibr" target="#b22">[23]</ref> does not contain ground truth head pose, so an additional mapping f v→p is needed to determine p source = f v→p (v source emb ).</p><p>f v→p . f v→p is trained to regress p from v. It is implemented using a fully connected layer with bias and trained using an L1 loss. Training pairs (v, p) are obtained using an annotated dataset with image to pose labels p; v is obtained by passing the image through the encoder of the driving network.</p><p>f p→v . f p→v is trained to regress v from p. It is implemented using a fullyconnected linear layer with bias followed by batch-norm. When f v→p is known, this function can be learnt directly on VoxCeleb by passing an image through X2Face to get the driving vector v and f v→p (v) gives the pose p.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Audio</head><p>Audio data from the videos in the VoxCeleb dataset can be used to drive a source face in a manner similar to that of pose by driving the source frame with itself but modifying the driving vector using the audio from another frame. The forward mapping f a→v from audio features a to the corresponding driving vector v is trained using pairs of audio features a and driving vectors v. These can be directly extracted from VoxCeleb (so no backward mapping f v→a is required). a is obtained by extracting the 256D audio features from the neural network in <ref type="bibr" target="#b8">[9]</ref> and the 128D v by passing the corresponding frame through the driving network's encoder. Ordinary least squares linear regression is then used to learn f a→v after first normalising the audio features to ∼ N (0, 1). No normalisation is used when employing the mapping to drive the frame generation; this amplifies the signal, visually improving the generated results.</p><p>As learning the function f a→v : R 1×256 → R 1×128 is under-constrained, the embedding learns to encode some pose information. Therefore, we additionally use the mappings f p→v and f v→p described in Section 4.1 to remove this information. Given driving audio features a driving and the corresponding, non-modified driving vector v source emb , the new driving vector v driving emb is then</p><formula xml:id="formula_1">v driving emb = v source emb + f a→v (a driving ) − f a→v (a source ) + f p→v (p audio − p source ),</formula><p>where p source = f v→p (v source emb ) is the head pose of the frame input to the driving network (i.e. the source frame), p audio = f v→p (f a→v (a driving )) is the pose information contained in f a→v (a driving ), and a source is the audio feature vector corresponding to the source frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>This section evaluates X2Face by first performing an ablation study in Section 5.1 on the architecture and losses used for training, followed by results for controlling a face with a driving frame in Section 5.2, pose information in Section 5.3, and audio information in Section 5. <ref type="bibr" target="#b3">4</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.</head><p>Training. X2Face is trained on the VoxCeleb video dataset <ref type="bibr" target="#b22">[23]</ref> using dlib <ref type="bibr" target="#b17">[18]</ref> to crop the faces to 256 × 256. The identities are randomly split into train/val/test identities (with a split of 75/15/10) and frames extracted at one fps to give 900,764 frames for training and 125,131 frames for testing. The model is trained in PyTorch <ref type="bibr" target="#b26">[27]</ref> using SGD with momentum 0.9 and batchsize of 16. First, it is trained just with L1 loss, and a learning rate of 0.001. The learning rate is decreased by a factor of 10 when the loss plateaus. Once the loss converges, the identity losses are incorporated and are weighted as follows: (i) for same identities to be as strong as the photometric L1 loss at each layer; (ii) for different identities to be 1/10 the size of the photometric loss at each layer. This training phase is started with a learning rate of 0.0001. Testing. The model can be tested using either a single or multiple source frames. The reasoning for this is that if the embedded face is stable (e.g. different facial regions always map to the same place on the embedded face), we expect to be able to combine multiple source frames by averaging over the embedded faces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Architecture studies</head><p>To quantify the utility of using additional views at test time and the benefit of the curriculum strategy for training the network (i.e. using the identity losses explained in Section 3.2), we evaluate the results for these different settings on a left-out test set of VoxCeleb. We consider 120K source and driving pairs where the driving frame is from the same video as the source frames; thus, the generated frame should be the same as the driving frame. The results are given in Table <ref type="table">1</ref>.</p><p>Table <ref type="table">1</ref>: L1 reconstruction error on the test set, comparing the generated frame to the ground truth frame (in this case the driving frame) for different training/testing setups. Lower is better for L1 error. Additionally, we give the percentage improvement over the L1 error for the model trained with only training stage I and tested with a single source frame. In this case, higher is better The results in Table <ref type="table">1</ref> confirm that both training with the curriculum strategy and using additional views at test time improve the reconstructed image. The supplementary material includes qualitative results and shows that using additional source frames when testing is especially useful if a face is seen at an extreme pose in the initial source frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Controlling image generation with a driving frame</head><p>The motivation of our architecture is to be able to map the expression and pose of a driving frame onto a source frame without any annotations on expression or pose. This section demonstrates that X2Face does indeed achieve this, as a set of source frames can be controlled with a driving video and generate realistic results. We compare to two methods: CycleGAN <ref type="bibr" target="#b44">[45]</ref> which uses no labels and <ref type="bibr" target="#b0">[1]</ref> which is designed top down and demonstrates impressive results. Additional qualitative results are given in the supplementary material and video. Comparison to CycleGAN <ref type="bibr" target="#b44">[45]</ref>. CycleGAN learns a mapping from a given domain (in this case a given identity A) to another domain (in this case another identity B). To compare to their method for a given pair of identities, we take all images of the given identities (so images may come from different video tracks) to form two sets of images: one set corresponding to identity A and the other to B. We then train their model using these sets. To compare, for a given driving frame of identity A, we visualise their generated frame from identity B which is compared to that of X2Face. The results in Fig. <ref type="figure" target="#fig_1">4</ref> illustrate multiple benefits. First, X2Face generalises to unseen pairs of identities at test time given only a source and driving frame. Cy-cleGAN is trained on pairs of identities, so if there are too few example images, it fails to correctly model the shape and geometry of the source face, producing unrealistic results. Additionally, our results have better temporal coherence (i.e. consistent background/hair style/etc. across generated frames), as X2Face transforms a given frame whereas CycleGAN samples from a latent space.</p><p>Comparison to Averbuch-Elor et. al. <ref type="bibr" target="#b0">[1]</ref>. We compare to <ref type="bibr" target="#b0">[1]</ref> in Fig. <ref type="figure" target="#fig_2">5</ref>. There are two significant advantages of our formulation over theirs: first, we can handle more significant pose changes in the driving video and source frame (Fig. <ref type="figure" target="#fig_2">5b-c</ref>).</p><p>Second, ours has fewer assumptions: (1) <ref type="bibr" target="#b0">[1]</ref> assumes that the first frame of the driving video is in a frontal pose with a neutral expression and that the source frame also has a neutral expression (Fig. <ref type="figure" target="#fig_2">5d</ref>). ( <ref type="formula">2</ref>) X2Face can be used when given a single driving frame whereas their method requires a video so that the face can be tracked and the tracking used to expand the number of correspondences and to obtain high level details. While this is not the focus of this paper, our method can be augmented with the ideas from these methods. For example, as inspired by <ref type="bibr" target="#b0">[1]</ref>, we can perform simple post-processing to add higher level details (Fig. <ref type="figure" target="#fig_3">5a</ref>, X2Face+p.p.) by transferring hidden regions using Poisson editing <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Controlling the image generation with pose</head><p>Before reporting results on controlling the driving vector using pose, we validate our claim that the driving vector does indeed learn about pose. To do this, we evaluate how accurately we can predict the three head pose angles -yaw, pitch and roll -given the 128D driving vector.</p><p>Pose predictor. To train the pose predictor which also serves as f v→p (Section 4.1), the 25, 993 images in the AFLW dataset <ref type="bibr" target="#b18">[19]</ref> are split into train/val set, leaving out the 1, 000 test images from <ref type="bibr" target="#b21">[22]</ref> as test set. The results on the test set are reported in Table <ref type="table">2</ref> confirming that the driving vector learns about head pose without having been trained on pose labels, as the results are comparable to those of a network directly trained for this task.</p><p>We then use f v→p to train f p→v (Section 4.1) and present generated frames for different, unseen test identities using the learnt mappings in Fig. <ref type="figure">6</ref>. The source frame corresponds to p source in Section 4.1 while p driving is used to vary one head pose angle while keeping the others fixed. Table <ref type="table">2</ref>: MAE in degrees using the driving vector for head pose regression (lower is better). Note that the linear pose predictor from the driving vector performs only slightly worse than a supervised method <ref type="bibr" target="#b21">[22]</ref>, which has been trained for this task</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Roll Pitch Yaw MAE X2Face 5.85 7.59 14.62 9.36 KEPLER <ref type="bibr" target="#b21">[22]</ref> (supervised) 8.75 5.85 6.45 7.02</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Controlling the image generation with audio input</head><p>This section presents qualitative results for using audio data from videos in the VoxCeleb dataset to drive the source frames. The VoxCeleb dataset consists of videos of interviews, suggesting that the audio should be especially correlated with the movements of the mouth. <ref type="bibr" target="#b8">[9]</ref>'s model, trained on the BBC-Oxford 'Lip Reading in the Wild' dataset (LRW), is used to extract audio features. We use the 256D vector activations of the last fully connected layer of the audio stream (FC7) for a 0.2s audio signal centred on the driving frame (the frame occurs half way through the 0.2s audio signal). A potential source of error is the domain gap between the LRW dataset and VoxCeleb, as <ref type="bibr" target="#b8">[9]</ref>'s model is not fine-tuned on the VoxCeleb dataset which contains much more background noise than the LRW dataset. Thus, their model has not necessarily learnt to become indifferent to this noise. However, our model is relatively robust to this problem; we observe that the mouth movements in the generated frames are reasonably close to what we would expect from the sounds Fig. <ref type="figure">6</ref>: Controlling image generation with pose code vectors. Results are shown for a single source frame which is controlled using each of the three head pose angles for the same identity (top three rows) and for different identities (bottom three rows). For further results and a video animation, we refer to the supplementary material. Whilst some artefacts are visible, the method allows the head pose angles to be controlled separately.</p><p>of the corresponding audio, as demonstrated in Fig. <ref type="figure">7</ref>. This is true even if the person in the video is not speaking and instead the audio is coming from an interviewer. However, there is some jitter in the generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Using the embedded face for video editing</head><p>We consider how the embedded face can be used for video editing. This idea is inspired by the concept of an unwrapped mosaic <ref type="bibr" target="#b30">[31]</ref>. We expect the embedded face to be pose and expression invariant, as can be seen qualitatively across the example embedded faces shown in the paper. Therefore, the embedded face can be considered as a UV texture map of the face and drawn on directly. This task is executed as follows. A source frame (or set of source frames) is extracted and input to the embedding network to obtain the embedded face. The embedded face can then be drawn on using an image or other interactive tool. A video is reconstructed using the modified embedded face which is driven by a set of driving frames. Because the embedded face is stable across different identities, Fig. <ref type="figure">7</ref>: Controlling image generation with audio information. We show how the same sounds affect various source frames; if our model is working well then the generated mouths should behave similarly. (a) shows the source frames. (b) shows the generated frames for a given audio sound which is visualised in (d) by the coloured portion of the word being spoken. As most of the change is expected to be in the mouth region, the cropped mouth regions are additionally visualised in (c). The audio comes from a native British speaker. As can be seen, in all generated frames, the mouths are more closed at the "ve" and "I" and more open at the "E" and "U". Another interesting point is that for the "Effects" frame, the audio is actually coming from an interviewer, so while the frame corresponding to the audio has a closed mouth, the generated results still open the mouth. a given edit can be applied across different identities. Example edits are shown in Fig. <ref type="figure" target="#fig_4">8</ref> and in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We have presented a self-supervised framework X2Face for driving face generation using another face. This framework makes no assumptions about the pose, expression, or identity of the input images, so it is more robust to unconstrained settings (e.g. an unseen identity). The framework can also be used with minimal alteration post training to drive a face using audio or head pose information. Finally, the trained model can be used as a video editing tool. Our model has achieved all this without requiring annotations for head pose/facial landmarks/depth data. Instead, it is trained self-supervised on a large collection of videos and learns itself to model the different factors of variation. While our method is robust, versatile, and allows for generation to be conditioned on other modalities, the generation quality is not as high as approaches specifically designed for transforming faces (e.g. <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b39">40]</ref>). This opens an interesting avenue of research: how can the approach be modified such that the  versatility, robustness, and self-supervision aspects are retained but with the generation quality of these methods that are specifically designed for faces. Finally, as no assumptions have been made that the videos are of faces, it is interesting to consider applying our approach to other domains.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 :</head><label>3</label><figDesc>Fig.3: The identity loss function when the source and driving frames are of different identities. This loss enforces that the generated frame has the same identity as the source frame.</figDesc><graphic url="image-3.png" coords="5,169.35,316.80,276.66,88.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 :</head><label>4</label><figDesc>Fig.4: Comparison of X2Face's generated frames to those of CycleGAN given a driving video sequence. Each example shows from bottom to top: the driving frame, our generated result and CycleGAN's generated result. To the left, source frames for X2Face are shown (at test time CycleGAN does not require source frames, as it is has been trained to map between the given source and driving identities). These examples demonstrate multiple benefits of our method. First, X2Face is capable of preserving the face shape of the source identity (top row) whilst driving the pose and expression according to the driving frame (bottom row); CycleGAN correctly keeps pose and expression but loses information about face shape and geometry when given too few training images as in example (a) (whereas X2Face requires no training samples for new identities). Second, X2Face has temporal consistency. CycleGAN samples from the latent space, so it sometimes samples from different videos resulting in jarring changes between frames (e.g. in example (c)).</figDesc><graphic url="image-6.png" coords="9,134.77,370.12,345.83,132.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Comparison of X2Face to supervised methods. In comparison to [1]: X2Face matches (b) pitch, and (c) roll and yaw; and X2Face can handle nonneutral expressions in the source frame (d). As with other methods, postprocessing (X2Face + p.-p.) can be applied to add higher level details (a).</figDesc><graphic url="image-7.png" coords="11,139.95,115.84,335.43,178.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>( a )</head><label>a</label><figDesc>Source frames are input to extract the embedded face which is drawn on. The modified embedded face is used to generate the frames below. (b) An example sequence of generated frames (top row) from the modified embedded face controlled using a sequence of driving frames (bottom row).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: Example results of the video editing application. (a) For given source frames, the embedded face is extracted and modified. (b) The modified embedded face is used for a sequence of driving frames (bottom) and the result is shown (top). Note how for the second example, the blue tattoo disappears behind the nose when the person is seen in profile and how, as above, the modified embedded face can be driven using the same or another identity's pose and expression. Best seen in colour. Zoom in for details. Additional examples using the blue tattoo and Harry Potter scar are given in the supplementary video and pdf.</figDesc><graphic url="image-12.png" coords="14,134.77,318.62,345.83,69.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-8.png" coords="12,134.77,115.84,345.83,243.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-9.png" coords="13,134.77,115.83,345.83,159.66" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements The authors are grateful to Hadar Averbuch-Elor for helpfully running their model on our data and to Vicky Kalogeiton for suggestions/comments. This work was funded by an EPSRC studentship and EPSRC Programme Grant Seebibyte EP/M013774/1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bibliography</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bringing portraits to life</title>
		<author>
			<persName><forename type="first">H</forename><surname>Averbuch-Elor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">3D morphable models as spatial transformer networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A P</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Awais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV Workshop on Geometry Meets Deep Learning</title>
				<meeting>ICCV Workshop on Geometry Meets Deep Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A morphable model for the synthesis of 3d faces</title>
		<author>
			<persName><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGGRAPH</title>
				<meeting>ACM SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large scale 3d morphable models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Booth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roussos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ponniah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dunaway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">2-4</biblScope>
			<biblScope unit="page" from="233" to="254" />
			<date type="published" when="2018-04">Apr 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Load balanced gans for multi-view face image synthesis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.07447</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Photographic image synthesis with cascaded refinement networks</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
				<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Infogan: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Lip reading sentences in the wild</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Out of time: automated lip sync in the wild</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Multi-view Lip-reading, ACCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Video face replacement</title>
		<author>
			<persName><forename type="first">K</forename><surname>Dale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vlasic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Matusik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised learning of disentangled representations from video</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Birodkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Exprgan: Facial expression editing with controllable expression intensity</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sricharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
				<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Image style transfer using convolutional neural networks</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Effective face frontalization in unconstrained images</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Paz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Enbar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Audio-driven facial animation by joint end-to-end learning of pose and emotion</title>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Herva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Richardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<title level="m">Deep video portraits. Proc. ACM SIGGRAPH</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dlib-ml: A machine learning toolkit</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1755" to="1758" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Annotated Facial Landmarks in the Wild: A Large-scale, Real-world Database for Facial Landmark Localization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Koestinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. First IEEE International Workshop on Benchmarking Facial Image Analysis Technologies</title>
				<meeting>First IEEE International Workshop on Benchmarking Facial Image Analysis Technologies</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fast face-swap using convolutional neural networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Korshunova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dambre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
				<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep convolutional inverse graphics network</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">F</forename><surname>Whitney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">KEPLER: keypoint and pose estimation of unconstrained faces by learning efficient H-CNN regressors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Autom. Face and Gesture Recog</title>
				<meeting>Int. Conf. Autom. Face and Gesture Recog</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">VoxCeleb: a large-scale speaker identification dataset</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>INTERSPEECH</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On face segmentation, face swapping, and face perception</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Nirkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Autom. Face and Gesture Recog</title>
				<meeting>Int. Conf. Autom. Face and Gesture Recog</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Realistic dynamic facial textures from a single image using gans</title>
		<author>
			<persName><forename type="first">K</forename><surname>Olszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
				<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
				<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<title level="m">Automatic differentiation in PyTorch</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Poisson image editing</title>
		<author>
			<persName><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gangnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on graphics</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Spatio-temporal video autoencoder with differentiable memory</title>
		<author>
			<persName><forename type="first">V</forename><surname>Pȃtrȃucean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Geometry-contrastive generative adversarial network for facial expression synthesis</title>
		<author>
			<persName><forename type="first">F</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.01822</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unwrap mosaics: A new representation for video editing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rav-Acha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. MICCAI</title>
				<meeting>MICCAI</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Adaptive 3D face reconstruction from unconstrained photo collections</title>
		<author>
			<persName><forename type="first">J</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Photorealistic facial texture inference using deep neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nagano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Real-time avatar animation from a single image</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Autom. Face and Gesture Recog</title>
				<meeting>Int. Conf. Autom. Face and Gesture Recog</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Audio to body dynamics</title>
		<author>
			<persName><forename type="first">E</forename><surname>Shlizerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schoen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for largescale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Synthesizing Obama: learning lip sync from audio</title>
		<author>
			<persName><forename type="first">S</forename><surname>Suwajanakorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Mofa: Model-based deep convolutional face autoencoder for unsupervised monocular reconstruction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
				<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Face2Face: Real-time Face Capture and Reenactment of RGB Videos</title>
		<author>
			<persName><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stamminger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Extreme 3D face reconstruction: Seeing through occlusions</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Paz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nirkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Disentangled representation learning gan for pose-invariant face recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Face transfer with multilinear models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Vlasic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Popović</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Interpretable transformations with encoder-decoder networks</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Garbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Turmukhambetov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
				<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
				<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">State of the art on monocular 3D face reconstruction, tracking, and applications</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Beeler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stamminger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eurographics</title>
				<meeting>Eurographics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
