<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Closest Point Search in Lattices</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Erik</forename><surname>Agrell</surname></persName>
							<email>agrell@s2.chalmers.se</email>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Thomas</forename><surname>Eriksson</surname></persName>
							<email>thomase@s2.chalmers.se</email>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Alexander</forename><surname>Vardy</surname></persName>
							<email>vardy@montblanc.ucsd.edu</email>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Kenneth</forename><surname>Zeger</surname></persName>
							<email>zeger@ucsd.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Signals and Systems</orgName>
								<orgName type="institution">Chalmers University of Technology</orgName>
								<address>
									<postCode>S-412 96</postCode>
									<settlement>Göteborg</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>San Diego, La Jolla</addrLine>
									<postCode>92093-0407</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Closest Point Search in Lattices</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">75F4EE3B868DD6E74DAA1FB8B1FEF536</idno>
					<idno type="DOI">10.1109/TIT.2002.800499</idno>
					<note type="submission">received December 4, 2000; revised October 5, 2001. This work was supported in part by the National Science Foundation, the David and Lucile Packard Foundation, and Stiftelsen ISS &apos;90.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T04:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Closest point search</term>
					<term>kissing number</term>
					<term>Korkine-Zolotareff (KZ) reduction</term>
					<term>lattice decoding</term>
					<term>lattice quantization</term>
					<term>nearest neighbor</term>
					<term>shortest vector</term>
					<term>Voronoi diagram</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this semitutorial paper, a comprehensive survey of closest point search methods for lattices without a regular structure is presented. The existing search strategies are described in a unified framework, and differences between them are elucidated. An efficient closest point search algorithm, based on the Schnorr-Euchner variation of the Pohst method, is implemented. Given an arbitrary point and a generator matrix for a lattice 3,the algorithm computes the point of 3 that is closest to . The algorithm is shown to be substantially faster than other known methods, by means of a theoretical comparison with the Kannan algorithm and an experimental comparison with the Pohst algorithm and its variants, such as the recent Viterbo-Boutros decoder. Modifications of the algorithm are developed to solve a number of related search problems for lattices, such as finding a shortest vector, determining the kissing number, computing the Voronoi-relevant vectors, and finding a Korkine-Zolotareff reduced basis.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>I N lattice theory, a generator matrix is any matrix with real entries whose rows are linearly independent over . We let and denote the number of rows and columns of , respectively. Hence . The lattice generated by is</p><p>The rows of are called basis vectors for , and the number of basis vectors is said to be the dimension of .</p><p>The closest point problem is the problem of finding, for a given lattice and a given input point , a vector such that for all where denotes the Euclidean norm. In channel coding, the closest point problem is referred to as decoding, and we adopt this terminology herein. Note, however, that in source coding, this problem is called encoding (see below).</p><p>The Voronoi region of a lattice point is the set of all vectors in that can be decoded to this point, namely where . The Voronoi diagram of a lattice is the set of all its Voronoi regions. It is known <ref type="bibr" target="#b22">[23]</ref> that the Voronoi regions are convex polytopes, that they are symmetrical with respect to reflection in , and that they are translations of , where is the origin of . In communication theory, lattices are used for both modulation and quantization. If a lattice is used as a code for the Gaussian channel, maximum-likelihood decoding in the demodulator is a closest point search. The decoding of space-time codes is one example <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b24">[25]</ref>. Analogously, if a lattice is used as a codebook for vector quantization and the meansquared-error criterion is used, then the encoding of each input vector is also a closest point search. Furthermore, if the lattice is truncated into a so-called Voronoi code <ref type="bibr" target="#b20">[21]</ref>, another instance of the closest point problem arises at the opposite end of the communication system, in the source decoder and in the modulator. Typical for these applications in communications is that the same lattice is decoded numerous times for different input vectors.</p><p>Other applications where the closest point problem arises include lattice design <ref type="bibr" target="#b2">[3]</ref> and Monte Carlo second-moment estimation <ref type="bibr" target="#b21">[22]</ref>. In both cases, random vectors are generated uniformly in a Voronoi region of a lattice using closest point search.</p><p>The closely related shortest vector problem has been used in assessing the quality of noncryptographic random number generators <ref type="bibr">[50, pp. 89-113]</ref> and in decoding of Chinese remainder codes <ref type="bibr" target="#b38">[38]</ref>, <ref type="bibr" target="#b40">[40]</ref>. It also has important applications in cryptography <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b6">[7]</ref>. Another related problem of paramount importance in cryptography <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b71">[70]</ref> is that of lattice basis reduction. These search problems will be discussed in Section VI.</p><p>The choice of method for solving the closest point problem depends on the structure of the lattice. Intuitively, the more structure a lattice has, the faster can the closest point be found. For many classical lattices, efficient search methods are known <ref type="bibr" target="#b22">[23,</ref><ref type="bibr">Ch. 20]</ref>, <ref type="bibr" target="#b76">[75]</ref>. A more general approach is to represent a lattice by a trellis <ref type="bibr" target="#b73">[72]</ref> and use a trellis decoding algorithm such as the Viterbi algorithm <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b33">[33]</ref>, <ref type="bibr" target="#b34">[34]</ref>, <ref type="bibr" target="#b77">[76]</ref>. However, finite-state trellises exist if and only if the lattice contains mutually orthogonal vectors, and even then decoding complexity quickly becomes prohibitive <ref type="bibr" target="#b74">[73]</ref>.</p><p>Herein, we address the problem of finding the closest point in a general lattice: we assume that it has no exploitable structure. One situation where this problem arises is when a generator matrix is continuously adjusted, e.g., in numerical lattice design <ref type="bibr" target="#b2">[3]</ref>. Another important application of this problem is cryptanalysis <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b69">[68]</ref>. Yet another example is frequency estimation and phase unwrapping <ref type="bibr" target="#b18">[19]</ref>.</p><p>The complexity of the general closest point problem as a function of the dimension was analyzed by van Emde 0018-9448/02$17.00 © 2002 IEEE Boas <ref type="bibr" target="#b75">[74]</ref> two decades ago, who showed that this problem is NP-hard. Micciancio gave a simpler proof in <ref type="bibr" target="#b55">[55]</ref>. Thus, all known algorithms for solving the problem optimally have exponential complexity. It is known <ref type="bibr" target="#b8">[9]</ref> that finding an approximate solution, such that the ratio between the distance found and the true distance is upper-bounded by a constant, is also NP-hard. Even finding a suboptimal solution within a factor for some constant is NP-hard <ref type="bibr" target="#b26">[27]</ref>. Nevertheless, algorithms that find a suboptimal solution are faster and can handle higher dimensions <ref type="bibr" target="#b52">[52]</ref>.</p><p>A common approach to the general closest point problem is to identify a certain region in within which the optimal lattice point must lie, and then investigate all lattice points in this region, possibly reducing its size dynamically. The earliest work in the field was done for the shortest vector problem (see Section VI-A) in the context of assessing the quality of certain random number generators (cf. <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b25">[26]</ref> and <ref type="bibr">[50, pp. 89-101, 110]</ref>). The finite region searched in these algorithms is a parallelepiped, with its axes parallel to the basis vectors.</p><p>In general, the development of closest point algorithms follows two main branches, inspired by two seminal papers: Pohst <ref type="bibr" target="#b64">[63]</ref> in 1981 examined lattice points lying inside a hypersphere, whereas Kannan <ref type="bibr" target="#b46">[46]</ref> in 1983 used a rectangular parallelepiped. Both papers later appeared in revised and extended versions, Pohst's as <ref type="bibr" target="#b29">[30]</ref> and Kannan's (following the work of Helfrich <ref type="bibr" target="#b42">[42]</ref>) as <ref type="bibr" target="#b47">[47]</ref>. The Pohst and Kannan strategies are discussed in greater detail in Section III-A.</p><p>A crucial parameter for the performance of these algorithms is the initial size of the search region. Some suggestions to this point were given in <ref type="bibr" target="#b63">[62]</ref>, <ref type="bibr" target="#b79">[78]</ref> for the Pohst strategy and in <ref type="bibr" target="#b11">[12]</ref> for the Kannan strategy. The latter reference also includes an extensive complexity analysis. Applications are discussed in <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b63">[62]</ref>, <ref type="bibr" target="#b79">[78]</ref>, <ref type="bibr" target="#b81">[80]</ref>.</p><p>Another, more subtle, difference between the two strategies is implicit in their presentation. Grossly generalizing, the Pohst method is intended as a practical tool while the method of Kannan is intended as a theoretical tool. Papers dealing with the Pohst strategy typically discuss issues of implementation, whereas papers dealing with the Kannan strategy usually focus on asymptotic complexity. This is probably the reason why the two strategies, despite having so much in common, have never been compared and evaluated against each other in the literature.</p><p>Recently, Schnorr and Euchner <ref type="bibr" target="#b68">[67]</ref> suggested an important improvement of the Pohst strategy, based on examining the points inside the aforementioned hypersphere in a different order. In Sections V and VII-C, the strategies by Pohst, Kannan, and Schnorr-Euchner are compared to each other, and it is shown that the Schnorr-Euchner strategy is substantially faster than the other two.</p><p>While the preceding discussion is distilled from the existing literature, much of this literature is not directly accessible. Often, the results are buried in the context of specific applications. For example, the Schnorr-Euchner algorithm is described in <ref type="bibr" target="#b68">[67]</ref> merely as a subroutine, called ENUM, in a function that computes the so-called block Korkine-Zolotareff (KZ) reduction, which itself serves as a tool for solving a certain type of subset-sum problems <ref type="bibr" target="#b68">[67]</ref> and attacking the Chor-Rivest cryptosystem <ref type="bibr" target="#b69">[68]</ref>. Thus, although the question "What is the best (fastest) algorithm currently available for decoding a general lattice?" frequently arises in communication practice, the answer to this question is not immediately clear.</p><p>In this paper, we first describe the two main decoding strategies, due to Pohst and to Kannan, in a unified framework, which makes it possible to elucidate the similarities and the differences between them. This is done in Section III-A, where we also discuss the Babai nearest plane algorithm <ref type="bibr" target="#b9">[10]</ref> and the Schnorr-Euchner refinement of the Pohst strategy. In Section III-B, we present a stand-alone implementation of what we believe is the fastest closest point search algorithm currently available for general lattices. The algorithm is based on the Schnorr-Euchner <ref type="bibr" target="#b68">[67]</ref> strategy, bootstrapped with the Babai <ref type="bibr" target="#b9">[10]</ref> nearest point. It is described in sufficient detail to allow straightforward implementation, without knowledge of the underlying theory. One of the main contributions of this paper is a theoretical and experimental comparison of the various closest point search algorithms, presented in Sections V and VII, respectively. We also show in Section IV how a carefully selected preprocessing stage can reduce the complexity of the closest point search even further. Finally, we describe in Section VI several modifications to the algorithm of Section III-B designed to solve numerous related lattice-search problems, such as finding a shortest vector, determining the kissing number, computing the Voronoi-relevant vectors, and finding a Korkine-Zolotareff reduced basis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PRELIMINARIES</head><p>We say that two lattices are identical if all lattice points are the same. Two generator matrices and generate identical lattices if and only if <ref type="bibr" target="#b0">(1)</ref> where is a square matrix with integer entries such that . A generator matrix is a rotated and reflected representation of another generator matrix if <ref type="bibr" target="#b1">(2)</ref> where . This transformation can be regarded as a change of the coordinate system. If is square and lower triangular, it is said to be a lower-triangular representation of . Any generator matrix has a lower-triangular representation, which is unique up to column negation. How to find a lower-triangular representation of a given generator matrix is discussed in Section IV.</p><p>Two lattices are congruent, or equivalent, if one can be obtained from the other through scaling, rotation, and reflection. Two generator matrices and generate equivalent lattices if and only if <ref type="bibr" target="#b2">(3)</ref> where is a real constant, while and obey the same conditions as in ( <ref type="formula">1</ref>) and ( <ref type="formula">2</ref>), respectively. The equivalence relation is denoted . The process of selecting a good basis for a given lattice, given some criterion, is called reduction. In many applications, it is advantageous if the basis vectors are as short as possible and "reasonably" orthogonal to each other (for lattice-search problems, this was first noted by Coveyou and MacPherson <ref type="bibr" target="#b23">[24]</ref>). This property of the basis vectors can be formalized in a number of ways, giving rise to several types of reduction. Simply selecting the shortest nonzero vectors in the lattice is, however, not a practicable approach, since these vectors do not in general form a basis.</p><p>The problem was studied by Hermite in 1850, who suggested <ref type="bibr">[44, pp. 301-303</ref>] that a generator matrix with rows is reduced if the following holds for all :</p><p>, for all generator matrices with rows such that and for . In other words, a generator matrix is reduced in this sense if the sequence comes first in a lexicographically ordered list of the corresponding sequences for all generator matrices of the same lattice. The first basis vector is always a shortest nonzero lattice vector. There exists at least one reduced basis in this sense for every lattice, but Hermite gave no algorithm to compute it. Note that this reduction criterion is usually not referred to as the "Hermite reduction" in recent literature (see footnote 2).</p><p>Minkowski made extensive use of the above reduction criterion in his earlier work <ref type="bibr" target="#b56">[56]</ref> [57], <ref type="bibr" target="#b58">[58]</ref>. In 1905, he suggested a subtle but significant modification <ref type="bibr" target="#b61">[61]</ref>, defining the criterion now known as the Minkowski reduction. A generator matrix with rows is Minkowski-reduced if the following holds for all : for all with rows such that and for . 1 This is in essence a "greedy" version of the stricter criterion by Hermite. Suppose that a set of vectors have been found that satisfy Minkowski's criterion up to a certain value of . Then there is always a Minkowski-reduced basis that contains these vectors, and the search can be focused on finding the next vector in the basis. This is not necessarily the case with the aforementioned criterion by Hermite. In particular, if there is more than one inequivalent shortest nonzero vector, it may well be that only one of them can be included in a reduced basis in the sense of Hermite, whereas there is always at least one Minkowski-reduced basis for each of them.</p><p>Minkowski reduction has received much attention, particularly in number theory <ref type="bibr">[18, pp. 27-28]</ref>, <ref type="bibr">[28, pp. 83-84]</ref>. Algorithms to compute a Minkowski-reduced basis of an arbitrary lattice may be found in <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b42">[42]</ref>.</p><p>Two types of reduction that are more widely used in practice are Korkine-Zolotareff (KZ) reduction and Lenstra-Lenstra-Lovász (LLL) reduction. One reason for their popularity is that with both of those criteria, the -dimensional reduction problem can be recursively reduced to an -dimensional reduction problem, which is not feasible with Minkowski reduction.</p><p>The KZ reduction is named after the authors of <ref type="bibr" target="#b51">[51]</ref>, who defined this reduction criterion in 1873. To determine whether a given generator matrix is a KZ-reduced basis, it is convenient 1 We disregard, as is commonly done in recent literature, that Minkowski also required the scalar product between v v v and v v v to be nonnegative for all i = 1; . . . ; n 0 1.</p><p>to study its lower-triangular representation It is known <ref type="bibr" target="#b65">[64]</ref>, that every lattice has at least one KZ-reduced generator matrix.</p><p>The LLL reduction is named after Lenstra, Lenstra, and Lovász, who suggested the corresponding reduction criteria in <ref type="bibr" target="#b53">[53]</ref>. The LLL reduction is often used in situations where the KZ reduction would be too time-consuming. A lower-triangular generator matrix ( <ref type="formula">4</ref>) is LLL-reduced if either , or else each of the following three conditions holds: <ref type="bibr" target="#b7">(8)</ref> for <ref type="bibr" target="#b8">(9)</ref> and the submatrix ( <ref type="formula">7</ref>) is LLL-reduced. As before, an arbitrary generator matrix is LLL-reduced if its lower-triangular representation is LLL-reduced. Any KZ-reduced matrix is clearly also LLL-reduced. The motivation for the latter reduction is that there exists an efficient algorithm <ref type="bibr" target="#b53">[53]</ref> to convert any generator matrix into an LLL-reduced one. This algorithm, which operates in polynomial time in and , has become very popular. It was improved upon in <ref type="bibr" target="#b70">[69]</ref> and <ref type="bibr" target="#b67">[66]</ref>.</p><p>The LLL reduction algorithm has been modified in a number of ways, see <ref type="bibr">[20, pp. 78-104]</ref>. Hybrids between KZ and LLL reductions have also been proposed <ref type="bibr" target="#b66">[65]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. CLOSEST POINT SEARCH ALGORITHMS</head><p>We start with a conceptual description of various lattice search algorithms in Section III-A. In this framework, we introduce the Babai nearest plane algorithm, the Kannan strategy, the Pohst strategy, and the Schnorr-Euchner refinement of the Pohst strategy. In Section III-B, we present a detailed pseudocode implementation of a closest point search algorithm based on the Schnorr-Euchner strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Conceptual Description</head><p>To understand lattice search algorithms, a recursive characterization of lattices is useful. Let be an generator matrix for a lattice , and let us write as where is an matrix consisting of the top rows of . Furthermore, let us write as , with in the row space of and in the null space. If is lower triangular, as in (4), then this decomposition is particularly simple, namely, and . With this terminology, any -dimensional lattice can be decomposed as follows: <ref type="bibr" target="#b9">(10)</ref> which is basically a stack of -dimensional translated sublattices. The -dimensional hyperplanes that contain these sublattices will be called -dimensional layers. Thus, the index denotes which layer a certain lattice point belongs to. The vector is the offset by which one sublattice is translated within its layer, with respect to an adjacent sublattice. The vector is normal to the layers, and the distance between two adjacent layers is . For lower-triangular generator matrices, we have . Recalling that any generator matrix can be rotated into a lower-triangular form with , we let denote the distance between the -dimensional layers, even when the triangular constraint is not explicitly imposed. Now, all search algorithms for an -dimensional lattice will be described recursively as a finite number of -dimensional search operations. Let be a vector to decode in the lattice , which is decomposed into layers according to <ref type="bibr" target="#b9">(10)</ref>. The orthogonal distance from to the layer with index is given by <ref type="bibr" target="#b10">(11)</ref> where <ref type="bibr" target="#b11">(12)</ref> Let denote the closest lattice point to , and suppose that an upper bound on is known. Then, in order to ensure that will be found, it suffices to consider a finite number of layers in <ref type="bibr" target="#b9">(10)</ref>. The indices of these layers are <ref type="bibr" target="#b12">(13)</ref> since layers for which are not relevant. Of these, the layer with has the shortest orthogonal distance to , where denotes the closest integer to . Four types of search methods will now be identified. They each search the layers indexed in ( <ref type="formula">13</ref>), but they differ in the order in which these layers are examined and in the choice of the upper bound to be used, recursively, in the -dimensional search problems.</p><p>If only is considered, the -dimensional search problem is reduced to just one -dimensional problem, and no upper bound is needed. Recursive application of this strategy <ref type="bibr" target="#b9">[10]</ref> yields the Babai nearest plane algorithm, and we call the returned lattice point the Babai point. The Babai nearest plane algorithm is a fast method to find a nearby lattice point, in time polynomial in the number of rows and columns of . In general, the Babai point depends not only on and the lattice, but also on the basis used to represent the lattice. It is not necessarily the closest point, but the error can be bounded. A probabilistic variant of the Babai nearest plane algorithm was proposed by Klein <ref type="bibr" target="#b49">[49]</ref>.</p><p>The other three methods all find the optimal (closest) point. Scanning all the layers in ( <ref type="formula">13</ref>), and supplying each -dimensional search problem with the same value of regardless of , yields the Kannan strategy. <ref type="foot" target="#foot_1">3</ref> Variants of this strategy <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b42">[42]</ref>, <ref type="bibr" target="#b46">[46]</ref>, <ref type="bibr" target="#b47">[47]</ref> differ mainly in how the bounds are chosen for . In this context, a recent improvement by Blömer <ref type="bibr" target="#b13">[14]</ref> seems particularly promising. Geometrically, the Kannan strategy amounts to generating and examining all lattice points within a given rectangular parallelepiped.</p><p>The -dimensional decoding error vector consists, in the given recursive framework, of two orthogonal components: one in the row space of and one parallel to . The former is the -dimensional decoding error while the length of the latter is . Since varies with , the upper bound can be chosen as <ref type="bibr" target="#b13">(14)</ref> which is different for different layers in <ref type="bibr" target="#b12">(13)</ref>. The idea of letting depend on is the Pohst strategy <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b63">[62]</ref>, <ref type="bibr" target="#b64">[63]</ref>, <ref type="bibr" target="#b79">[78]</ref>, <ref type="bibr" target="#b81">[80]</ref>. In geometrical terms, points inside a hypersphere, not a parallelepiped, are investigated. When any lattice point inside the sphere is found, the bound can be immediately updated to , since is an obvious upper bound on and . The Schnorr-Euchner strategy, proposed in <ref type="bibr" target="#b68">[67]</ref>, combines the advantages of the Babai nearest plane algorithm and the Pohst strategy. Assume that . Then the sequence <ref type="bibr" target="#b14">(15)</ref> orders the layers in ( <ref type="formula">13</ref>) according to nondecreasing distance from . A trivial counterpart holds when . The advantages of examining the layers in this order are subtle but significant. Since the volume of a layer decreases with increasing , the chance of finding the correct layer early is maximized. Another advantage of the nondecreasing distance is that the search can safely be terminated as soon as exceeds the distance to the best lattice point found so far. Notice that the very first lattice point generated will, by definition, be the Babai point. Furthermore, since the ordering in <ref type="bibr" target="#b14">(15)</ref> does not depend on , no initial bound is needed. Instead, this bound can be updated dynamically during the search, with the first finite value of being equal to the distance to the Babai point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Detailed Description</head><p>This subsection contains a stand-alone presentation of an efficient closest point search algorithm, based on the Schnorr-Euchner strategy. It is intended to be sufficiently detailed to allow a straightforward implementation, even without knowledge of the underlying theory.</p><p>For efficiency, the recursive operations discussed in the previous subsection have been restructured into a loop. The variables and are used as input and output parameters, instead of the more natural and . As discussed in Section IV, this is motivated by the typical communication application, where numerous input vectors are decoded in the same lattice.</p><p>First, some notation needs to be defined. Matrix and vector elements are named according to the following conventions: Input: an lower-triangular matrix with positive diagonal elements, and an -dimensional vector to decode in the lattice . Output: an -dimensional vector such that is a lattice point that is closest to . In this algorithm, is the dimension of the sublayer structure that is currently being investigated. Each time the algorithm finds a -dimensional layer, the distance to which is less than the currently smallest distance, this layer is expanded into -dimensional sublayers. This is done in Case A. Conversely, as soon as the distance to the examined layer is greater than the lowest distance, the algorithm moves up one step in the hierarchy of layers. This is done in Case C. Case B is invoked when the algorithm has successfully moved down all the way to the zero-dimensional layer (that is, a lattice point) without exceeding the lowest distance. Then this lattice point is stored as a potential output point, the lowest distance is updated, and the algorithm moves back up again, without restarting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. PREPROCESSING AND POSTPROCESSING</head><p>The algorithm DECODE of the previous section requires a representation of the lattice at hand by a lower-triangular generator matrix, whose diagonal elements are all positive. Such a representation exists for any lattice, so this requirement does not impose any constraints on the kind of lattices that can be searched. Moreover, for any given lattice, a representation with the required properties can be found in infinitely many ways, which leaves the user with the freedom of choosing one of them. The algorithm computes a closest vector regardless of the representation choice, but the speed with which it reaches this result varies considerably between different representations. This is the topic of this section: How should a given search problem be preprocessed, in order to make the most efficient use of DECODE?</p><p>To address this question, we now present a general lattice search algorithm. This algorithm can be regarded as a "frontend" to DECODE, where explicit preprocessing and postprocessing is performed to allow generator matrices that are not lower triangular, possibly not even square. As with DECODE, we first describe this algorithm conceptually, and then suggest how to implement it.</p><p>Assume that a generator matrix and an input vector are given. By linear integer row operations, we first transform into another matrix, say , which generates an identical lattice. The purpose of this transformation is to speed up DECODE; see below. Next, we rotate and reflect into a lower-triangular form , so that It is essential to rotate and reflect the input vector in the same way, so that the transformed input vector, say , is in the same relation to as is to . All this can be regarded as a change of the coordinate system. Now the search problem has a form that is suitable for DECODE, which will find the closest lattice point in this coordinate system. Reversing the operations of rotation and reflection produces , the lattice point closest to in . Following these steps, the algorithm is detailed as follows.</p><p>Input: an generator matrix , and an -element vector to decode in . Output: a lattice point that is closest to .</p><p>Step Step 1 is a basis reduction. This step is optional: it is possible to select as the identity matrix, which amounts to no reduction at all. This works well for low-dimensional and not too ill-conditioned generator matrices, as will be shown in Section VII. However, the speed and the numerical stability of the search can be improved significantly by an appropriate reduction, as discussed later in this section.</p><p>Step 2 implies rotation and reflection of into a lower-triangular form, as in <ref type="bibr" target="#b1">(2)</ref>. The standard method to achieve this is by QR decomposition. Given an arbitrary matrix , its QR decomposition is a factorization of of the form , where is an upper-triangular matrix, and is an orthonormal matrix, that is, one satisfying . It is well known that a QR decomposition exists for any matrix; efficient algorithms to compute it may be found in <ref type="bibr">[41, pp. 208-236</ref>] and [71, pp. 166-176], for example. In our context, QR decomposition of gives both and , with being equal to . As an alternative to QR decomposition, can be obtained by Cholesky decomposition of . Given an positive-definite matrix , its Cholesky decomposition is a factorization of the form where is an upper-triangular matrix. In our context, is equal to , and the rotation matrix is given by . Algorithms for computing the Cholesky decomposition may be found in <ref type="bibr">[20, pp. 102-104]</ref>, <ref type="bibr">[41, pp. 84-93]</ref>, and <ref type="bibr">[71, pp. 332-334]</ref>.</p><p>All these transformations can be thought of as a change of the coordinate system. Measure the first coordinate along (the first row of ), the second in the plane spanned by and , and so on. The generator matrix in this coordinate system will be square and lower triangular.</p><p>For DECODE to work, all diagonal elements of must be positive. Some implementations of QR factorization do not do this automatically; if this is the case, we multiply by all columns of that contain a negative diagonal element, as well as the corresponding rows of .</p><p>In Steps 4-6, the input vectors are processed. They are transformed into the coordinate system of , decoded, and transformed back again.</p><p>If a large set of vectors is to be decoded for the same lattice, Steps 1-3 are, of course, carried out only once for the whole set. In this case, the overall execution time may benefit substantially from an effective but time-consuming reduction method applied in Step 1. To understand precisely what kind of preprocessing would improve the performance of the search algorithm, recall the recursive representation of lattices in <ref type="bibr" target="#b9">(10)</ref>. An -dimensional lattice consists of parallel -dimensional sublattices, translated and stacked on top of each other. This decomposition into sublattices is controlled by the reduction method. Two properties of the decomposition are desirable for a given lattice.</p><p>a) The -dimensional layers should be as far apart as possible. This minimizes the number of layers to be investigated, as only the layers within a certain distance range need to be scanned. As an extreme case, suppose that the spacing between -dimensional layers is much larger than any other -dimensional layer spacing in the lattice. Then the closest point will always lie in the closest -dimensional layer, and the dimensionality of the problem is essentially reduced by one. b) The zero-dimensional layers (lattice points) should be as densely spaced as possible in the one-dimensional layers (lines). The denser they are, the higher is the probability that the closest lattice point will belong to the closest lattice line. If the one-dimensional spacing is much smaller than all other interlayer distances, then the closest point will always lie in the closest line, so the dimensionality of the problem is essentially reduced by one.</p><p>Both observations can, of course, be applied recursively. Thus, high-dimensional layer spacing should be large, while low-dimensional spacing should be small. This suggests two greedy algorithms: a) sequentially maximizing the distances between -dimensional layers, starting at , and b) minimizing the same distances, starting at . These two goals are each other's duals in a fairly strict sense. Even though they may appear contradictory, they are, in fact, very similar (cf. <ref type="bibr">[50, pp. 94-98]</ref>). To see this, observe that a reduction algorithm can choose the numbers in many ways for a given lattice, but their product is invariant: it equals the volume of the Voronoi region. Now, a) is solved by maximizing first , then , and so on. Because of the constant product, this procedure forces low values for , , etc. Thus, a good solution of a) is in general good for b) too. Conversely, b) is solved by first minimizing , then , and so on, which automatically produces a good basis in the sense of a) as well.</p><p>The smallest possible value of that can be selected for a given lattice equals the length of the shortest vector in the lattice. (Shortest vector problems can be solved by a variant of the CLOSESTPOINT algorithm, as described in Section VI-A.) On the other hand, the largest possible is the reciprocal of the length of the shortest vector in the dual lattice , since is a generator matrix for , provided that is square. Applying these shortest vector criteria recursively, we conclude that b) is solved optimally by KZ reduction of any basis for the lattice. This follows immediately from the recursive definition of KZ reduction in Section II. Similarly, a) is solved optimally by KZ reduction of a basis for the dual lattice, followed by reversing the order of the rows and transposing the inverse of the resulting matrix (hereafter, we refer to this procedure as KZ reduction of the dual). Finally, the LLL reduction yields an approximate (but faster) solution to both a) and b), because of its inherent sorting mechanism.</p><p>Our recommendation is to use KZ reduction in applications where the same lattice is to be searched many times, otherwise use LLL. This recommendation is supported by the experimental results in Section VII.</p><p>V. COMPLEXITY ANALYSIS Banihashemi and Khandani <ref type="bibr" target="#b11">[12]</ref> observed that the average complexity of a search method for uniformly distributed input vectors <ref type="foot" target="#foot_2">4</ref> is proportional to the volume of the region being searched. They used this observation to assess the complexity of the Kannan algorithm. We adopt the same approach here to analyze the CLOSESTPOINT algorithm and compare it with the Kannan algorithm. A comparison between CLOSESTPOINT and an algorithm based on the Pohst strategy is carried out experimentally in Section VII.</p><p>For a given lattice, let denote the volume searched in a -dimensional layer, when is the given upper bound on the attainable distance. Since the CLOSESTPOINT algorithm does not require an initial value for , the desired complexity measure is .</p><p>Theorem 1: Let for . Then (16) <ref type="bibr" target="#b16">(17)</ref> Proof: As before, we let denote the upper bound used by the CLOSESTPOINT algorithm when searching a -dimensional layer. In view of ( <ref type="formula">14</ref>), we have for <ref type="bibr" target="#b17">(18)</ref> where is the distance accumulated within the -dimensional layer, as in <ref type="bibr" target="#b10">(11)</ref>. Combining ( <ref type="formula">11</ref>) and ( <ref type="formula">13</ref>), we see that varies from at least to at most . Thus, expressing as an integral over , we obtain the following recursive bound: for <ref type="bibr" target="#b18">(19)</ref> The bounds ( <ref type="formula">16</ref>) and ( <ref type="formula">17</ref>) follow from this recursion in conjunction with two different bounds on . In either case, we use the initial condition <ref type="bibr" target="#b19">(20)</ref> which is the volume of a line extending from to . To derive <ref type="bibr" target="#b15">(16)</ref>, we first use <ref type="bibr" target="#b17">(18)</ref> to transform <ref type="bibr" target="#b18">(19)</ref> into the form where the index of has been dropped. Solving this recursion with the initial condition (20) yields for <ref type="bibr" target="#b20">(21)</ref> Notice that the right-hand side of ( <ref type="formula">21</ref>) is the volume of a -dimensional sphere of radius . It is known <ref type="bibr" target="#b9">[10]</ref> that for any input vector , the distance to the Babai point in dimensions is at most , where . Since the Babai point is the first lattice point generated by the CLOSESTPOINT algorithm, we have <ref type="bibr" target="#b21">(22)</ref> and for . Using this bound on in conjunction with the recursion <ref type="bibr" target="#b18">(19)</ref>, we obtain for <ref type="bibr" target="#b22">(23)</ref> regardless of the value of . This proves <ref type="bibr" target="#b15">(16)</ref>. Notice that the right-hand side of ( <ref type="formula">23</ref>) is the volume of a -dimensional parallelepiped with sides . To complete the proof of ( <ref type="formula">17</ref>), we observe that by ( <ref type="formula">21</ref>) and ( <ref type="formula">22</ref>), we have <ref type="bibr" target="#b23">(24)</ref> where the last inequality follows from , which is the well-known Stirling inequality <ref type="bibr">[29, p. 54]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Let</head><p>denote the volume of the region being searched in the Kannan algorithm for an -dimensional lattice. Since Kannan <ref type="bibr" target="#b47">[47]</ref> focused on proving the existence of an algorithm within a certain complexity bound rather than presenting a single immediately implementable algorithm, there is some ambiguity regarding what exactly is to be meant by "Kannan's algorithm." We here adopt the same interpretation as in <ref type="bibr" target="#b11">[12]</ref>. It is shown in <ref type="bibr" target="#b11">[12]</ref> that for every lattice, is in the range <ref type="bibr" target="#b24">(25)</ref> where the lower bound is exact if the sequence is increasing and the upper bound is exact if it is decreasing. For a "good" lattice (say, one of the first 48 laminated lattices <ref type="bibr">[23, p. 158]</ref>), this sequence generally displays a decreasing trend, although the decrease is not necessarily monotonic <ref type="bibr" target="#b48">[48]</ref>. Thus, is often close to the upper bound. On the other hand, the recursive cube search algorithm <ref type="bibr" target="#b11">[12]</ref>, an improved variant of Kannan's algorithm, attains the lower bound in <ref type="bibr" target="#b24">(25)</ref> with equality (cf. [12, eq. ( <ref type="formula">19</ref>)]).</p><p>The CLOSESTPOINT algorithm is faster than the Kannan algorithm for all dimensions and all lattices, since the upper bound ( <ref type="formula">16</ref>) coincides with the lower bound <ref type="bibr" target="#b24">(25)</ref> for the Kannan algorithm. The magnitude of the gain is suggested by <ref type="bibr" target="#b16">(17)</ref>. For lattices such that the upper bound in <ref type="bibr" target="#b24">(25)</ref> is exact, the CLOSESTPOINT algorithm is faster by at least a factor of . Notice that this factor is meant to indicate the asymptotic relation for large . For low and moderate values of , the first inequality in (24) yields a significantly better bound. Also notice that in assessing the volume searched by the CLOSESTPOINT algorithm, the general bound for may be useful. This bound includes ( <ref type="formula">16</ref>) and ( <ref type="formula">17</ref>) as two extreme special cases. It follows straightforwardly from <ref type="bibr" target="#b18">(19)</ref>, <ref type="bibr" target="#b20">(21)</ref>, and the fact that for . Banihashemi and Khandani <ref type="bibr" target="#b11">[12]</ref> point out that the covering radii of the lattice and its sublattices, if known, can be exploited to reduce the complexity of the Kannan algorithm. This option can be incorporated into the CLOSESTPOINT algorithm as well. However, it is difficult to determine the covering radius of a general lattice. The only known method is the "diamond-cutting" algorithm of <ref type="bibr" target="#b80">[79]</ref>, which, as detailed in Section VI-C, is confined by memory limitations to low dimensions. If an upper bound on the covering radius for the particular lattice is known, it can be used as well, as proposed in <ref type="bibr" target="#b79">[78]</ref>. Unfortunately, even though there exist upper bounds on the minimal possible covering radius for packings in a given dimension <ref type="bibr">[23, pp. 39-40]</ref>, <ref type="bibr">[39, p. 241]</ref>, no method to upper-bound the covering radius of an arbitrary given lattice is known.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. MORE LATTICE SEARCH PROBLEMS</head><p>Other search problems involving lattices can be solved using modifications and extensions of the CLOSESTPOINT algorithm. These include computing lattice parameters such as the shortest vector, the kissing number, and the Voronoi-relevant vectors. The CLOSESTPOINT algorithm can be also used to perform the key step in the KZ basis reduction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Shortest Vector</head><p>Given a lattice , the shortest vector problem is to find a vector in that has the smallest Euclidean norm. The history of the shortest vector problem is closely interlinked with that of the closest point problem. It has been conjectured in <ref type="bibr" target="#b75">[74]</ref> that the shortest vector problem ( <ref type="formula">with</ref>) is NP-hard, but, in contrast to the closest point problem, this is still not proved. The conjecture of <ref type="bibr" target="#b75">[74]</ref> is supported by the result of Ajtai <ref type="bibr" target="#b5">[6]</ref>, who showed that the shortest vector problem is NP-hard under randomized reductions. Micciancio <ref type="bibr" target="#b54">[54]</ref> furthermore proved that finding an approximate solution within any constant factor less than is also NP-hard for randomized reductions. It is known <ref type="bibr" target="#b37">[37]</ref>, <ref type="bibr" target="#b43">[43]</ref>, however, that the shortest vector problem is not harder than the closest vector problem.</p><p>The CLOSESTPOINT algorithm can be straightforwardly modified to solve the shortest vector problem. The idea is to submit as the input and exclude as a potential output. Algorithmically, the changes needed to convert CLOSESTPOINT into SHORTESTVECTOR are as follows.</p><p>1. Omit as an input to DECODE and CLOSESTPOINT. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>}</head><p>In any lattice, there is an even number of shortest vectors, because the lattice is symmetrical with respect to reflection in . Hence, if is a shortest vector, then so is . A factor of two in computation time can be gained by exploiting this symmetry. This is achieved by rewriting DECODE to scan only half of the candidates (say, the ones for which the first nonzero component is positive).</p><p>Of course, when a KZ-reduced basis is used for the lattice at hand, a shortest vector is directly available as the first basis element, and the SHORTESTVECTOR algorithm becomes trivial. However, one of the main applications of the SHORTESTVECTOR algorithm, at least in our context, is precisely to compute a KZreduced basis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Kissing Number</head><p>The kissing number of a lattice is defined as the number of shortest nonzero vectors in . If the lattice has no regular structure (say, if the basis vectors are drawn randomly from a continuous distribution), there are typically exactly two shortest nonzero lattice vectors, and the kissing number is . In general, to compute the kissing number (say, for a structured lattice), it is essential to use infinite precision: an arbitrarily small perturbation of a generator matrix has the potential of reducing the kissing number to , regardless of the original value. However, we do not recommend implementing DECODE using exact arithmetic. The same goal can be achieved far more efficiently by implementing the time-consuming operations, as before, using finite-precision real numbers, followed by an infinite-precision postprocessing stage, whereby a finite set of candidates is evaluated.</p><p>The new version of DECODE needs to keep track of a set of potential shortest vectors, not just the single best candidate. A margin of accuracy must be included in the comparisons, to avoid missing some of the shortest vectors due to numerical errors. Thus, the changes needed to convert CLOSESTPOINT into KISSINGNUMBER are as follows.</p><p>1. Apply the changes 1-3 of Section VI-A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">In DECODE, include "</head><p>" among the initial assignments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">In DECODE, replace line 11 with:</head><p>if then { where is a small positive number. 4. In DECODE, replace lines 20 and 21 with: if then { } 5. In DECODE, remove line 22. 6. In DECODE, replace in line 28 with . In CLOSESTPOINT, replace in Step 5 with . 7. In CLOSESTPOINT, replace Step 6 with:</p><p>Step 6. Compute the exact value of for all and return the number of occurrences of the lowest value.</p><p>As for the shortest vector problem, a variant of the closest point problem can be formulated that, in case of a tie, returns all the lattice points that have minimum distance to a given input vector, not just one of them. Specifically, CLOSESTPOINT can be converted into ALLCLOSESTPOINTS through the following modifications.</p><p>Apply the changes 2-6 above. In CLOSESTPOINT, replace Step 6 with:</p><p>Step 6. Compute the exact value of for all and call the lowest value . Return</p><p>The main application of this algorithm lies in the solution of the next problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Voronoi-Relevant Vectors</head><p>A facet is an -dimensional face of an -dimensional polytope. The relevant-vector problem is to find the facets of the Voronoi region or, in other words, to find a minimal set for which</p><p>The vectors in are called Voronoi-relevant, or simply relevant. Our method to solve the relevant-vector problem is based upon the following proposition.</p><p>Proposition 2: The Voronoi regions of any two distinct lattice points and share a facet if and only if <ref type="bibr" target="#b25">(26)</ref> for all , where</p><p>Proof: It follows from ( <ref type="formula">26</ref>) that , and for all . It is known (cf. <ref type="bibr">[23, p. 33]</ref>) that if two Voronoi regions and intersect but do not share a facet, then all points in also belong to some other Voronoi region . Hence, the above property of the point suffices to establish that and share a facet. To prove the "only if" part of the proposition, assume that and have a common facet. Let be any point in the interior of this facet, so that <ref type="bibr" target="#b27">(28)</ref> for all . In addition to (28), we will make use of the following identity: <ref type="bibr" target="#b28">(29)</ref> which holds for any three points . Now, for all we have where the equality follows from <ref type="bibr" target="#b28">(29)</ref>, while the inequality follows by applying <ref type="bibr" target="#b27">(28)</ref> twice. This establishes <ref type="bibr" target="#b25">(26)</ref>. This proposition was proved by Voronoï in a slightly different context <ref type="bibr">[81, vol. 134, pp. 277-278]</ref>, <ref type="bibr">[23, p. 475</ref>], based on a theory by Minkowski [59, pp. 81-85], <ref type="bibr" target="#b60">[60]</ref>. Similar properties have been established for the Voronoi regions of binary linear codes <ref type="bibr" target="#b1">[2]</ref> and of parallelepipeds <ref type="bibr" target="#b3">[4]</ref>.</p><p>In order to compute for a lattice , we now proceed as follows. Consider a vector , and let . It is obvious that any vector in <ref type="bibr" target="#b26">(27)</ref> is of this form. Notice that is symmetric with respect to reflection in . That is, if is a lattice point, then so is . Although there are infinitely many pairs of lattice points that have as their midpoint, Proposition 2 implies that at most one such pair can share a facet. A closest point search in the lattice , with as the input vector, will find the pair, if it exists. Therefore, we evaluate ALLCLOSESTPOINTS , while distinguishing between the following three cases. Case 1. ALLCLOSESTPOINTS returns one point . Since is also a lattice point at the same distance from , we conclude that and is itself a lattice point. Obviously, this happens if and only if , and no pair of lattice points can satisfy (26) with respect to in this case. Case 2. ALLCLOSESTPOINTS returns exactly two lattice points and . Then these points share a facet by Proposition 2. Notice that if share a facet, then so do and for all . This establishes an equivalence class of pairs of points of that share a facet, whose midpoint is of the form for some . We are interested in only two pairs in this class, namely</p><p>In other words, the points and are the only Voronoi-relevant points derived from this equivalence class. Case 3. ALLCLOSESTPOINTS returns four or more lattice points. Then no pair of points can satisfy <ref type="bibr" target="#b25">(26)</ref>.</p><p>The discussion in Cases 1 and 2 shows that in order to determine for a given lattice , it suffices to investigate potential midpoints in the finite set For each such vector , we can use the ALLCLOSESTPOINTS algorithm to check whether condition (26) of Proposition 2 is satisfied. This leads to the following algorithm.</p><p>Input: an generator matrix . Output: the set of the Voronoi-relevant vectors of .</p><p>Step Optional optimization includes moving Steps 1-3 of the ALLCLOSESTPOINTS algorithm out of the loop, since all the calls to ALLCLOSESTPOINTS concern the same lattice. Since for each , the lattice is symmetric with respect to reflection in , a factor of two in complexity can be gained through the same symmetry argument as for SHORTESTVECTOR in Section VI-A.</p><p>It follows from the preceding discussion that the maximum number of facets that a Voronoi region can have in any -dimensional lattice is , which was proved by Minkowski in 1897 <ref type="bibr" target="#b60">[60]</ref>. Voronoï showed that this number is attained with probability by a lattice whose basis is chosen at random from a continuous distribution <ref type="bibr">[81, vol. 134, pp. 198-211 and vol. 136, pp. 67-70]</ref>.</p><p>Relevant vectors have been determined for many classical lattices <ref type="bibr" target="#b22">[23,</ref><ref type="bibr">Chs. 4 and 21]</ref>, but we believe that the RELEVANTVECTORS algorithm proposed here is the fastest known in the general case. The only alternative algorithm known to the authors is the "diamond-cutting" algorithm of Viterbo and Biglieri <ref type="bibr" target="#b80">[79]</ref>, which computes a complete geometrical description of the Voronoi region of any lattice. This description includes all vertices, edges, etc., which evidently includes the information about the relevant vectors. However, using the diamond-cutting algorithm for the sole purpose of determining the relevant vectors is inefficient. Voronoï showed in his classical work <ref type="bibr" target="#b82">[81]</ref> that the number of -dimensional faces of a Voronoi region of an -dimensional lattice is upper-bounded by <ref type="bibr" target="#b29">(30)</ref> and that there exist lattices whose Voronoi regions attain this number for every <ref type="bibr">[81, vol. 136, pp. 74-82, 137-143]</ref>. One example of such a lattice, given by Voronoï, is the lattice usually denoted by , which is the dual of the root lattice <ref type="bibr">[23, p. 115]</ref>. Furthermore, the number of -dimensional faces is lower-bounded by <ref type="bibr" target="#b30">(31)</ref> This can be proved by induction, keeping in mind that the Voronoi region, as well as all its -faces, are symmetric polytopes. The lower bound <ref type="bibr" target="#b30">(31)</ref> is attained for every by the cubic lattice . Evaluating <ref type="bibr" target="#b29">(30)</ref> and <ref type="bibr" target="#b30">(31)</ref> for shows that the number of vertices is between and , inclusively, the number of edges is between and , and so on. This implies that the memory requirements for the diamond-cutting algorithm grow very rapidly with dimension. This property limits the use of the diamond-cutting algorithm to low dimensions, as the authors of <ref type="bibr" target="#b80">[79]</ref> themselves point out.</p><p>The RELEVANTVECTORS algorithm, on the other hand, uses negligible memory but does not fully determine the Voronoi regions. In those cases where a complete description (vertices, edges, etc.) is desired, we suggest preceding the diamond-cutting algorithm with RELEVANTVECTORS, since the complexity (both time and memory) of the diamond-cutting algorithm can be reduced by incorporating knowledge of the relevant vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. KZ Reduction</head><p>The last problem we deal with here is the reduction problem. This is the problem of finding a KZ-reduced basis, which has been already mentioned in Sections II and IV. Theoretical results are available for specific lattices in <ref type="bibr" target="#b48">[48]</ref>. Algorithms for general lattices have been proposed by <ref type="bibr">Kannan [47]</ref> and by Schnorr <ref type="bibr" target="#b66">[65]</ref>. Since KZ reduction essentially consists of solving shortest vector problems, a closest point algorithm can be used in this context too. In our experiments (see the next section), we have computed KZ-reduced bases using this method.</p><p>The general strategy is to find a shortest vector in the lattice, project the lattice onto the hyperplane orthogonal to this vector, and find a KZ-reduced basis of the resulting -dimensional lattice, recursively. In this application of the SHORTESTVECTOR algorithm, Step 1 is performed using the LLL reduction, since a KZ reduction is obviously not a usable prerequisite for KZ reduction. The implementation details, which we omit, follow straightforwardly from the definition of KZ reduction in Section II.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Closest Point in a Lattice Code</head><p>The primary focus of this paper is search problems for lattices viewed as infinite point sets. Under some circumstances, the methods discussed earlier in the paper can be modified to solve search problems for finite subsets of lattices. This has important applications in communications. Specifically, demodulation and quantization both involve finding the closest vector to a given input in a finite point set. One popular method to design such a point set is to form a lattice code, which is the intersection of a lattice and a bounded region in . This bounded region is usually called the support of the lattice code <ref type="bibr">[35, pp. 470-479]</ref>, <ref type="bibr" target="#b36">[36]</ref>.</p><p>If a general closest point algorithm for lattices is applied to such a problem, there is a risk that the returned lattice point lies outside the support and hence does not belong to the lattice code. This typically happens when the input vector lies outside the support, but it may also happen in some cases when it lies slightly inside the support boundary.</p><p>Several ways to handle this problem have been proposed. If a lattice point outside the support is returned by the closest point algorithm, an obvious option is to declare a failure or erasure, if the application permits this. Otherwise, the algorithm can be modified to disregard such points and output the closest point found in the support, or if no such point is found, to increase the size of the initial search region and try again <ref type="bibr" target="#b79">[78]</ref>, <ref type="bibr" target="#b81">[80]</ref>. Increasing the size repeatedly ensures that the closest point in the lattice code will eventually be found.</p><p>Alternatively, the input vector may be projected onto the boundary of the support before the closest point search algorithm is invoked <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b45">[45]</ref>. Quite often, the closest lattice point to the projected input vector belongs to the lattice code and is its closest point to the original input, but this is not always the case. Hence, it might be advantageous to combine this method with increasing the size of the search region, or to project the vector onto a surface slightly inside the support boundary instead. If the input vector is far outside the support region, a much smaller search region needs to be considered around the projected vector in order to find the closest point in the lattice code, compared to the size of the search region without projection.</p><p>The previously described methods are applicable for the Kannan, Pohst, and Schnorr-Euchner strategies alike. It can be argued that increasing the size of the initial search region is useless for the Schnorr-Euchner strategy, because its initial value of is unbounded. However, we recommend giving an explicit finite value in the context of lattice codes, because if for a certain input vector the Babai point lies outside the support (and if the line through the Babai point in the direction of does not pass through any point in the lattice code), then the unmodified version of DECODE will never terminate. To avoid this, line 2 of DECODE should be appropriately modified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. EXPERIMENTS</head><p>In this section, we report on experiments with the CLOSESTPOINT algorithm of Section III-B. We evaluate its performance for both low-and high-dimensional lattices. We also compare it with other similar algorithms, and show how the basis for the lattice at hand should be preprocessed in order to achieve the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The Setup</head><p>To evaluate the performance of the CLOSESTPOINT algorithm, we must decide what class of lattices to investigate. The closest point search methods studied here are general. Thus, they do not compete well with algorithms specially designed for searching a particular lattice; such algorithms can exploit structure in the lattice and are generally faster (see Section I). Here, we concentrate on experiments with random lattices without any apparent structure that can be exploited in their decoding. However, for comparison, we also include several experiments where the algorithms were applied to classical, highly structured, lattices, such as the Leech lattice in 24 dimensions and the cubic lattice .</p><p>Following the discussion above, we use generator matrices with random elements, drawn from independent and identically distributed zero-mean, unit variance Gaussian distributions. For each point in Figs. 1-3, 50 random matrices are generated, and the mean search time for each matrix is computed by averaging over a large number of random vectors. The exact number of input vectors is dependent on dimension: for large dimensions with long search times the average is computed over 200 vectors for each of the 50 matrices, while for small dimensions the number of vectors is much larger.</p><p>Then the median of the average search times for the 50 matrices is computed. Occasionally, a random matrix with very long search times is drawn. Computing the median rather than the mean guarantees that these rare matrices do not totally dominate the average search times. The search times for all the algorithms are averaged using the same matrices and the same set of input vectors. The results are given as average time (in seconds), using a DELL computer based upon a 733-MHz Pentium III processor, with Visual C++ running under Windows XP.</p><p>The random vectors were drawn according to a uniform distribution. Conway and Sloane <ref type="bibr" target="#b21">[22]</ref> report on a method to generate uniform data within a Voronoi region, which is equivalent to generating data uniformly distributed over a infinite-sized region. Uniform data is a reasonable assumption for applications such as source coding and cryptography. In channel coding applications, a more reasonable assumption is a Gaussian distribution around a lattice point, but such experiments have not been performed here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The Preprocessing</head><p>An important question for a closest point algorithm is whether the performance can be improved by preprocessing the generator matrix. Since the preprocessing needs to be performed only once, while the processed basis is typically used many times (in most communication applications), it is usually worthwhile to invoke a good preprocessing procedure. In Section IV, three dif- ferent preprocessing strategies were discussed: LLL reduction, KZ reduction, and KZ reduction of the dual. All of these strategies basically aim to find as short and as orthogonal basis vectors as possible. Here, we present experiments designed to find the best of these reduction methods.</p><p>In Fig. <ref type="figure" target="#fig_2">1</ref>, the simulation results for the three reduction methods are given (the time needed for the reduction itself is not included in these results). We see that performance can be improved significantly by selecting a good preprocessor. The best methods in our study are the ones based on the two KZ reductions; as expected, there is almost no difference between the KZ reduction and the KZ reduction of the dual. For high dimensions <ref type="bibr" target="#b29">(30 )</ref>, the KZ reductions lower the average search times by almost two orders of magnitude, as compared to unreduced bases, and by about one order of magnitude as compared to the LLL reduction. On the other hand, up to about 10-15 dimensions, the polynomial-time LLL reduction gives good results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison With Other Algorithms</head><p>To assess the performance of the CLOSESTPOINT algorithm, we have also implemented an algorithm described by Viterbo and Boutros in <ref type="bibr" target="#b81">[80]</ref>, which is based on the Pohst strategy. The Viterbo-Boutros algorithm requires an initial bound on the attainable distance (see Section III-A). A natural choice is the covering radius of the lattice, but it is not clear how to compute the covering radius for random lattices. Viterbo <ref type="bibr" target="#b78">[77]</ref> suggests to use the length of the shortest basis vector as an initial guess. If no lattice point is found within this distance from the input vector, the distance is multiplied by some factor greater than , and the search is repeated. We have performed some experiments using factors between and . We have also used the distance to the Babai point as an initial distance bound, thereby ensuring that at least one point is found within the distance. The CLOSESTPOINT algorithm needs no initial bound for the distance; the Babai point is by default the first point examined by this algorithm.  In Fig. <ref type="figure" target="#fig_1">2</ref>, the average time for a single closest point search operation is plotted as a function of dimension for the CLOSESTPOINT and the Viterbo-Boutros algorithms (with several values for the initial distance bound). For both algorithms, KZ reduction was first applied to the generator matrices. We see that the CLOSESTPOINT algorithm is faster for all tested dimensions, by a factor of -in our implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparison With Classical Lattices</head><p>To further illustrate the performance of the CLOSESTPOINT algorithm, we evaluate its performance for classical lattices, and compare it with the performance for random matrices (chosen from an independent and identically distributed Gaussian source). In Fig. <ref type="figure" target="#fig_4">3</ref>, the average search times for random lattices and for the cubic lattice are plotted as a function of dimension, together with the search times for the Leech lattice in 24 dimensions, and for the Barnes-Wall lattices in dimensions 8, 16, and 32. For the classical lattices just as for random lattices, KZ reduction leads to faster search times, and is therefore applied before the experiments.</p><p>We see that although the search times for the classical, highly structured, lattices are slightly higher, the general curve is about the same as that for random lattices. This is the strength as well as the weakness of search algorithms of this type: they do not rely on any particular structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Suboptimal Search</head><p>The search algorithms studied here always return a lattice point that is closest to the input point. However, in certain applications (e.g., source coding), it may be necessary to abort the search before the closest point has been found. Therefore, we have included experiments where the CLOSESTPOINT algorithm is aborted after a given time. The measure of performance in these experiments is the mean squared distance to the point produced by an aborted algorithm.</p><p>In Fig. <ref type="figure" target="#fig_5">4</ref>, the ratio between the suboptimal and the optimal mean squared distances is given for a 45-dimensional example, as a function of the time allotted for the search. From this figure, we see that the CLOSESTPOINT algorithm quickly finds lattice points fairly close to the optimal one.</p><p>We see that if a 10% higher mean squared distance than the optimal can be tolerated, then the CLOSESTPOINT algorithm is approximately 40 times faster than if the optimal point is required. We only report results for a single 45-dimensional example, but the general conclusion is the same for all tested dimensions and lattices. If the search is aborted before the optimal point is found, considerable time savings can be achieved at the cost of a slightly increased mean squared distance. Note that the good result relies on the layers being searched according to <ref type="bibr" target="#b12">(13)</ref>; if the layers are searched according to <ref type="bibr" target="#b14">(15)</ref>, the convergence is considerably slower. ACKNOWLEDGMENT</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>deviate from most built-in sign functions). Ties in the rounding operation are broken arbitrarily.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>2 .</head><label>2</label><figDesc>In CLOSESTPOINT, skip Step 4. 3. In DECODE, replace line 5 with " ." 4. In DECODE, replace lines 20-22 with: if then {</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Comparison of average search times among different reduction methods for preprocessing of the generator matrix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Comparison of the average search times for the CLOSESTPOINT algorithm and the Viterbo-Boutros algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Average search times for classical and random lattices.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Fig.4. Normalized mean squared distance as a function of allowed search time, when the search is aborted before the optimal point is found. The Babai point had a normalized mean squared distance of 1:49 for this 45-dimensional example.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>Because the condition<ref type="bibr" target="#b5">(6)</ref> was proposed by Hermite in his first and second letters to Jacobi[44, pp. 269-271, 280-282], KZ reduction is sometimes called "Hermite reduction" (cf.<ref type="bibr" target="#b42">[42]</ref>). The terminology is further complicated by the fact that in some contexts "Hermite reduction" refers to a criterion for so-called indefinite quadratic forms, not immediately applicable to lattices[18, p. 29].</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>In its original form<ref type="bibr" target="#b46">[46]</ref>,<ref type="bibr" target="#b47">[47]</ref>, Kannan's strategy is described recursively as a set of (i 0 1)-dimensional search problems, where i is the index of the largest element in (v ; . . . ; v ). This viewpoint may be useful for a complexity analysis, but because u ; u ; . . . ; u can be selected sequentially, the strategy is computationally equivalent to recursively eliminating just one dimension at a time.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>In this context, a "uniform distribution" is assumed to be uniform over a region large enough to make boundary effects negligible. This is equivalent to a uniform distribution over just one Voronoi region.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The authors gratefully acknowledge helpful comments by Daniele Micciancio, who brought several recent references to their attention. They also thank Emanuele Viterbo and Joseph Boutrous for valuable suggestions, especially regarding optimization of the Viterbo-Boutros algorithm.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Calculation of Minkowski-reduced lattice bases</title>
		<author>
			<persName><forename type="first">L</forename><surname>Afflerbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Grothe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="269" to="276" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On the Voronoi neighbor ratio for binary linear block codes</title>
		<author>
			<persName><forename type="first">E</forename><surname>Agrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="3064" to="3072" />
			<date type="published" when="1998-11">Nov. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Optimization of lattices for quantization</title>
		<author>
			<persName><forename type="first">E</forename><surname>Agrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Eriksson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="1814" to="1828" />
			<date type="published" when="1998-09">Sept. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">ML optimal CDMA multiuser receiver</title>
		<author>
			<persName><forename type="first">E</forename><surname>Agrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ottosson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electron. Lett</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1554" to="1555" />
			<date type="published" when="1995-08">Aug. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generating hard instances of lattice problems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ajtai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 28th Annu. ACM Symp. Theory of Computing</title>
		<meeting>28th Annu. ACM Symp. Theory of Computing<address><addrLine>Philadelphia, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996-05">May 1996</date>
			<biblScope unit="page" from="99" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The shortest vector problem in L is NP-hard for randomized reductions</title>
	</analytic>
	<monogr>
		<title level="m">Proc. 30th Annu. ACM Symp. Theory of Computing</title>
		<meeting>30th Annu. ACM Symp. Theory of Computing<address><addrLine>Dallas, TX</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-05">May 1998</date>
			<biblScope unit="page" from="193" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A public-key cryptosystem with worst-case/average-case equivalence</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ajtai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 29th Annu. ACM Symp. Theory of Computing</title>
		<meeting>29th Annu. ACM Symp. Theory of Computing<address><addrLine>El Paso, TX</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="284" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adaptive entropy constrained lattice vector quantization for multiresolution image coding</title>
		<author>
			<persName><forename type="first">M</forename><surname>Antonini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Barlaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE, pt</title>
		<meeting>SPIE, pt</meeting>
		<imprint>
			<date type="published" when="1992-11">Nov. 1992</date>
			<biblScope unit="volume">1818</biblScope>
			<biblScope unit="page" from="441" to="457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The hardness of approximate optima in lattices, codes, and systems of linear equations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Babai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sweedyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Syst. Sci</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="317" to="331" />
			<date type="published" when="1997-04">Apr. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On Lovász&apos; lattice reduction and the nearest lattice point problem</title>
		<author>
			<persName><forename type="first">L</forename><surname>Babai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Combinatorica</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Trellis complexity and minimal trellis diagrams of lattices</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Banihashemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">F</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="1829" to="1847" />
			<date type="published" when="1998-09">Sept. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On the complexity of decoding lattices using the Korkine-Zolotarev reduced basis</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Banihashemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Khandani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="162" to="171" />
			<date type="published" when="1998-01">Jan. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">in Codes, Graphs and Systems</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">F</forename><surname>Blake</surname></persName>
		</author>
		<editor>R. E. Blahut and R. Kötter</editor>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Kluwer</publisher>
			<biblScope unit="page" from="317" to="332" />
			<pubPlace>Norwell, MA</pubPlace>
		</imprint>
	</monogr>
	<note>Lattices and cryptography</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Closest vectors, successive minima, and dual HKZ-bases of lattices</title>
		<author>
			<persName><forename type="first">J</forename><surname>Blömer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Colloq. Automata, Languages and Programming</title>
		<editor>
			<persName><forename type="first">U</forename><surname>Montanari</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">D P</forename><surname>Rolim</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Welzl</surname></persName>
		</editor>
		<meeting>Int. Colloq. Automata, Languages and Programming<address><addrLine>Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-07">July 2000</date>
			<biblScope unit="page" from="248" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Good lattice constellations for both Rayleigh fading and Gaussian channels</title>
		<author>
			<persName><forename type="first">J</forename><surname>Boutros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Viterbo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rastello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Belfiore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="502" to="518" />
			<date type="published" when="1996-03">Mar. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Euclidean space lattice decoding for joint detection in CDMA systems</title>
		<author>
			<persName><forename type="first">L</forename><surname>Brunel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Boutros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Workshop Information Theory</title>
		<meeting>Int. Workshop Information Theory<address><addrLine>Kruger Park, South Africa</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-06">June 1999</date>
			<biblScope unit="page">129</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Lattice decoding for joint detection in direct sequence CDMA systems</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note>to be published</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">An Introduction to the Geometry of Numbers</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W S</forename><surname>Cassels</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1959">1959</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Frequency estimation, phase unwrapping, and the nearest lattice point problem</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V L</forename><surname>Clarkson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Acoustics, Speech and Signal Processing</title>
		<meeting>Int. Conf. Acoustics, Speech and Signal essing<address><addrLine>Phoenix, AZ</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-03">Mar. 1999</date>
			<biblScope unit="page" from="1609" to="1612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A Course in Computational Algebraic Number Theory</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cohen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>Berlin, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A fast encoding method for lattice codes and quantizers</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Conway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J A</forename><surname>Sloane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="820" to="824" />
			<date type="published" when="1983-11">Nov. 1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On the Voronoi regions of certain lattices</title>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Algebraic Discr. Methods</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="294" to="305" />
			<date type="published" when="1984-09">Sept. 1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m">Sphere Packings, Lattices and Groups</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note>rd ed</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fourier analysis of uniform random number generators</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Coveyou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Macpherson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Assoc. Comput. Mach</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="100" to="119" />
			<date type="published" when="1967-01">Jan. 1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Lattice code decoder for space-time codes</title>
		<author>
			<persName><forename type="first">O</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chkeif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Belfiore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Commun. Lett</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="161" to="163" />
			<date type="published" when="2000-05">May 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">How to calculate shortest vectors in a lattice</title>
		<author>
			<persName><forename type="first">U</forename><surname>Dieter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. of Comput</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="827" to="833" />
			<date type="published" when="1975-07">July 1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">An improved lower bound for approximating CVP</title>
		<author>
			<persName><forename type="first">I</forename><surname>Dinur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Raz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Safra</surname></persName>
		</author>
		<ptr target="http://www.math.ias.edu/~iritd" />
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Lattice Points</title>
		<author>
			<persName><forename type="first">P</forename><surname>Erdös</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Gruber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hammer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Longman/Wiley</publisher>
			<pubPlace>Harlow, U.K./ New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Feller</surname></persName>
		</author>
		<title level="m">An Introduction to Probability Theory and its Applications</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1968">1968</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note>rd ed</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Improved methods for calculating vectors of short length in a lattice, including a complexity analysis</title>
		<author>
			<persName><forename type="first">U</forename><surname>Fincke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pohst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. of Comput</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="463" to="471" />
			<date type="published" when="1985-04">Apr. 1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A pyramid vector quantizer</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="568" to="583" />
			<date type="published" when="1986-07">July 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Geometric source coding and vector quantization</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="137" to="145" />
			<date type="published" when="1989-01">Jan. 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The Viterbi algorithm</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Forney</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1973-03">Mar. 1973</date>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="268" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Coset codes-Part II: Binary lattices and related codes</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1152" to="1187" />
			<date type="published" when="1988-09">Sept. 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Vector Quantization and Signal Compression</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gersho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<publisher>Kluwer</publisher>
			<pubPlace>Boston, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Lattice quantization</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sayood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Electronics and Electron Physics</title>
		<editor>
			<persName><forename type="first">W</forename><surname>Hawkes</surname></persName>
		</editor>
		<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Academic</publisher>
			<date type="published" when="1988">1988</date>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="259" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Approximating shortest lattice vectors is not harder than approximating closest lattice vectors</title>
		<author>
			<persName><forename type="first">O</forename><surname>Goldreich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Micciancio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Safra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-P</forename><surname>Seifert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inform. Processing Lett</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="55" to="61" />
			<date type="published" when="1999-07">July 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Chinese remaindering with errors</title>
		<author>
			<persName><forename type="first">O</forename><surname>Goldreich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sudan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="1330" to="1338" />
			<date type="published" when="2000-07">July 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Geometry of Numbers</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Gruber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Lekkerkerker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
			<publisher>North-Holland</publisher>
			<pubPlace>Amsterdam, The Netherlands</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Soft-decision&apos; decoding of Chinese remainder codes</title>
		<author>
			<persName><forename type="first">V</forename><surname>Guruswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sahai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sudan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 41st Annu. Symp. Found. Computer Science</title>
		<meeting>41st Annu. Symp. Found. Computer Science<address><addrLine>Redondo Beach, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-11">Nov. 2000</date>
			<biblScope unit="page" from="159" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Applied Numerical Linear Algebra</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Hager</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>Prentice-Hall</publisher>
			<pubPlace>Englewood Cliffs, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Algorithms to construct Minkowski reduced and Hermite reduced lattice bases</title>
		<author>
			<persName><forename type="first">B</forename><surname>Helfrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theor. Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="125" to="139" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Note on shortest and nearest lattice vectors</title>
		<author>
			<persName><forename type="first">M</forename><surname>Henk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inform. Processing Lett</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="183" to="188" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Extraits de lettres à M. Jacobi sur différents objets de la théorie des nombres</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hermite</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Reine und Angewandte Math</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="261" to="315" />
			<date type="published" when="1850">1850</date>
		</imprint>
	</monogr>
	<note>in French</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Uniform and piecewise uniform lattice vector quantization for memoryless Gaussian and Laplacian sources</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Gibson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="786" to="804" />
			<date type="published" when="1993-05">May 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Improved algorithms for integer programming and related lattice problems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kannan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Symp. Theory of Computing</title>
		<meeting>ACM Symp. Theory of Computing<address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1983-04">Apr. 1983</date>
			<biblScope unit="page" from="193" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Minkowski&apos;s convex body theorem and integer programming</title>
	</analytic>
	<monogr>
		<title level="j">Math. Oper. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="415" to="440" />
			<date type="published" when="1987-08">Aug. 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Successive minimization of the state complexity of the self-dual lattices using Korkine-Zolotarev reduced basis</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Khandani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Esmaeili</surname></persName>
		</author>
		<idno>UW-E&amp;CE#97-01</idno>
	</analytic>
	<monogr>
		<title level="j">Dept. Elec. Comput. Eng., Univ. of Waterloo</title>
		<imprint>
			<date type="published" when="1997-01">Jan. 1997</date>
			<pubPlace>Waterloo, ON, Canada</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Finding the closest lattice vector when it&apos;s unusually close</title>
		<author>
			<persName><forename type="first">P</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th ACM-SIAM Symp. Discrete Algorithms</title>
		<meeting>11th ACM-SIAM Symp. Discrete Algorithms<address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-01">Jan. 2000</date>
			<biblScope unit="page" from="937" to="941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Knuth</surname></persName>
		</author>
		<title level="m">The Art of Computer Programming</title>
		<meeting><address><addrLine>2nd ed. Reading, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Addison-Wesley</publisher>
			<date type="published" when="1981">1981</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Sur les formes quadratiques</title>
		<author>
			<persName><forename type="first">A</forename><surname>Korkine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zolotareff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Annalen</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="366" to="389" />
			<date type="published" when="1873">1873</date>
		</imprint>
	</monogr>
	<note>in French</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">On random rotations diversity and minimum MSE decoding of lattices</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Boutros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="1584" to="1589" />
			<date type="published" when="2000-07">July 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Factoring polynomials with rational coefficients</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Lenstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Lenstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jr</forename></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lovász</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Annalen</title>
		<imprint>
			<biblScope unit="volume">261</biblScope>
			<biblScope unit="page" from="515" to="534" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">The shortest vector in a lattice is hard to approximate to within some constant</title>
		<author>
			<persName><forename type="first">D</forename><surname>Micciancio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 39th Annu. Symp. Foundations of Computer Science</title>
		<meeting>39th Annu. Symp. Foundations of Computer Science<address><addrLine>Palo Alto, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-11">Nov. 1998</date>
			<biblScope unit="page" from="92" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">The hardness of the closest vector problem with preprocessing</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="1212" to="1215" />
			<date type="published" when="2001-03">Mar. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<author>
			<persName><forename type="first">H</forename><surname>Minkowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sur la réduction des formes quadratiques positives quaternaires</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Hilbert</surname></persName>
		</editor>
		<meeting><address><addrLine>Leipzig, Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Teubner</publisher>
			<date type="published" when="1883">1883. 1911</date>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page" from="145" to="148" />
		</imprint>
	</monogr>
	<note>Gesammelte Abhandlungen von Hermann Minkowski. in French</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
	</analytic>
	<monogr>
		<title level="m">Gesammelte Abhandlungen von Hermann Minkowski</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Hilbert</surname></persName>
		</editor>
		<meeting><address><addrLine>Leipzig, Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Teubner</publisher>
			<date type="published" when="1886">1886. 1911</date>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="149" to="156" />
		</imprint>
	</monogr>
	<note>Über positive quadratische Formen. in German</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Zur Theorie der positiven quadratischen Formen</title>
	</analytic>
	<monogr>
		<title level="m">Gesammelte Abhandlungen von Hermann Minkowski</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Hilbert</surname></persName>
		</editor>
		<meeting><address><addrLine>Leipzig, Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Teubner</publisher>
			<date type="published" when="1887">1887. 1911</date>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="212" to="218" />
		</imprint>
	</monogr>
	<note>in German. in German</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Geometrie</forename><surname>Der</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zahlen</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1896">1896</date>
			<pubPlace>Leipzig, Germany</pubPlace>
		</imprint>
	</monogr>
	<note>in German</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Allgemeine Lehrsätze über die konvexen Polyeder</title>
	</analytic>
	<monogr>
		<title level="m">Gesammelte Abhandlungen von Hermann Minkowski</title>
		<title level="s">Nachrichten der K. Gesellschaft der Wissenschaften zu Göttingen. Mathematisch-physikalische Klasse</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Hilbert</surname></persName>
		</editor>
		<meeting><address><addrLine>Leipzig, Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Teubner</publisher>
			<date type="published" when="1897">1897. 1911</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="103" to="121" />
		</imprint>
	</monogr>
	<note>in German. in German</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Diskontinuitätsbereich für arithmetische Äquivalenz</title>
		<imprint/>
	</monogr>
	<note>in</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">)</forename><surname>German</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Reine und Angewandte Math.</title>
		<editor>
			<persName><forename type="first">Gesammelte</forename><surname>Abhandlungen Von Hermann Minkowski</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">(</forename><forename type="middle">D</forename><surname>Hilbert</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="53" to="100" />
			<date type="published" when="1905">1905. 1911</date>
			<publisher>Teubner</publisher>
			<pubPlace>Leipzig, Berlin, Germany</pubPlace>
		</imprint>
	</monogr>
	<note>in German</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Maximum likelihood sequence estimation from the lattice viewpoint</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Mow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1591" to="1600" />
			<date type="published" when="1994-09">Sept. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">On the computation of lattice vectors of minimal length, successive minima and reduced bases with applications</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pohst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGSAM Bull</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="37" to="44" />
			<date type="published" when="1981-02">Feb. 1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Classical methods in the theory of lattice packings</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Ryshkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Baranovskii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Usp. Mat. Nauk</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="68" />
			<date type="published" when="1979-08">July-Aug. 1979. 1979</date>
		</imprint>
	</monogr>
	<note>Math. Surv.</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">A hierarchy of polynomial time lattice basis reduction algorithms</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Schnorr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theor. Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="201" to="224" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">A more efficient algorithm for lattice basis reduction</title>
	</analytic>
	<monogr>
		<title level="j">J. Algorithms</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="47" to="62" />
			<date type="published" when="1988-03">Mar. 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Lattice basis reduction: Improved practical algorithms and solving subset sum problems</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Schnorr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Euchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Programming</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="181" to="191" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Attacking the Chor-Rivest cryptosystem by improved lattice reduction</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Schnorr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Hörner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">921</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="1995">1995</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Factorization of univariate integer polynomials by diophantine approximation and an improved basis reduction algorithm</title>
		<author>
			<persName><forename type="first">A</forename><surname>Schönhage</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Colloq. Automata, Languages and Programming</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Paredaens</surname></persName>
		</editor>
		<meeting>Colloq. Automata, Languages and Programming<address><addrLine>Antwerp, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1984-07">July 1984</date>
			<biblScope unit="page" from="436" to="447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Lattices and cryptography: An overview</title>
		<author>
			<persName><forename type="first">J</forename><surname>Stern</surname></persName>
		</author>
		<editor>Public Key Cryptography, H. Imai and Y. Zheng</editor>
		<imprint>
			<date type="published" when="1998-02">Feb. 1998</date>
			<biblScope unit="page" from="50" to="54" />
			<pubPlace>Yokohama, Japan</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Strang</surname></persName>
		</author>
		<title level="m">Linear Algebra and Its Applications, 3rd ed</title>
		<meeting><address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Harcourt Brace Jovanovich</publisher>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Trellis complexity versus the coding gain of lattices, Parts I and II</title>
		<author>
			<persName><forename type="first">V</forename><surname>Tarokh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">F</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="1796" to="1816" />
			<date type="published" when="1996-11">Nov. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Upper bounds on trellis complexity of lattices</title>
		<author>
			<persName><forename type="first">V</forename><surname>Tarokh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vardy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="1294" to="1300" />
			<date type="published" when="1997-07">July 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Another NP-complete partition problem and the complexity of computing short vectors in a lattice</title>
		<author>
			<persName><forename type="first">P</forename><surname>Van Emde</surname></persName>
		</author>
		<author>
			<persName><surname>Boas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematisch Instituut</title>
		<imprint>
			<biblScope unit="page" from="81" to="84" />
			<date type="published" when="1981-04">Apr. 1981</date>
			<pubPlace>Amsterdam, The Netherlands, Rep</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Maximum-likelihood decoding of the Leech lattice</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vardy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Be'ery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1435" to="1444" />
			<date type="published" when="1993-07">July 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Error bounds for convolutional codes and an asymptotically optimum decoding algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Viterbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="260" to="269" />
			<date type="published" when="1967-04">Apr. 1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">E</forename><surname>Viterbo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002-01">Jan. 2002</date>
		</imprint>
	</monogr>
	<note>private communication</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">A universal decoding algorithm for lattice codes</title>
		<author>
			<persName><forename type="first">E</forename><surname>Viterbo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Biglieri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. GRETSI</title>
		<meeting>GRETSI<address><addrLine>Juan-les-Pins, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993-09">Sept. 1993</date>
			<biblScope unit="page" from="611" to="614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Computing the Voronoi cell of a lattice: The diamond-cutting algorithm</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="161" to="171" />
			<date type="published" when="1996-01">Jan. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">A universal lattice code decoder for fading channels</title>
		<author>
			<persName><forename type="first">E</forename><surname>Viterbo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Boutros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="1639" to="1642" />
			<date type="published" when="1999-07">July 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Nouvelles applications des paramètres continus à la théorie des formes quadratiques</title>
		<author>
			<persName><forename type="first">G</forename><surname>Voronoï</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Reine und Angewandte Math</title>
		<imprint>
			<biblScope unit="volume">133</biblScope>
			<biblScope unit="page" from="67" to="181" />
			<date type="published" when="1908">1908. 1908. 1909</date>
		</imprint>
	</monogr>
	<note>Also. in French</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
