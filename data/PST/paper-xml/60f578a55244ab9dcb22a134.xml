<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph Kernel Attention Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-07-16">16 Jul 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Krzysztof</forename><surname>Choromanski</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Han</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Haoxian</forename><surname>Chen</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Jack</forename><surname>Parker-Holder</surname></persName>
							<email>jack.parker-holder@spc.ox.ac.uk</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Google Brain Robotics &amp; Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Graph Kernel Attention Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-07-16">16 Jul 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2107.07999v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a new class of graph neural networks (GNNs), by combining several concepts that were so far studied independently -graph kernels, attention-based networks with structural priors and more recently, efficient Transformers architectures applying small memory footprint implicit attention methods via low rank decomposition techniques. The goal of the paper is twofold. Proposed by us Graph Kernel Attention Transformers (or GKATs) are much more expressive than SOTA GNNs as capable of modeling longer-range dependencies within a single layer. Consequently, they can use more shallow architecture design. Furthermore, GKAT attention layers scale linearly rather than quadratically in the number of nodes of the input graphs, even when those graphs are dense, requiring less compute than their regular graph attention counterparts. They achieve it by applying new classes of graph kernels admitting random feature map decomposition via random walks on graphs. As a byproduct of the introduced techniques, we obtain a new class of learnable graph sketches, called graphots, compactly encoding topological graph properties as well as nodes' features. We conducted exhaustive empirical comparison of our method with nine different GNN classes on tasks ranging from motif detection through social network classification to bioinformatics challenges, showing consistent gains coming from GKATs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graphs are prevalent in a variety of real-world datasets. From social networks to bioinformatics (individual proteins, protein-protein interactions and even genomics), they play prominent roles in several high impact settings, while also making it possible to model relative inductive bias <ref type="bibr" target="#b36">[37]</ref> and define non-Euclidean metrics that are relevant in navigation and planning problems in Robotics (e.g. probabilistic roadmaps) <ref type="bibr" target="#b23">[24]</ref>. As such, there has recently been significant interest in Graph Neural Networks (GNNs) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b29">30]</ref>, specifically designed for processing graph-structured data.</p><p>Despite the tremendous success of modern GNNs in understanding graph data, there are still several challenges with processing it efficiently. Classic spectral methods suffer from computational complexity that is cubic in the number of graph vertices (spectral analysis of graphs' Laplacians) which becomes a problem when the graphs under consideration are larger. One of the possible strategies to mitigate this is to apply a rich theory of spectral sketches <ref type="bibr" target="#b14">[15]</ref>, yet in practice the computed sketches need to be very large. Conversely, algorithms working in a spatial domain avoid expensive spectral computations, yet in order to model longer-range dependencies, they rely on deep GNN architectures for the signal to propagate from distant nodes since individual layers model only local interactions. Furthermore, for dense graphs they require quadratic memory just to store adjacency matrices of the input graphs for all graph-related computations (i.e.to compute attention layers as in GATs <ref type="bibr" target="#b34">[35]</ref>).</p><p>Given these contrasting issues, the key question we seek to answer in this paper is as follows:</p><p>Is it possible to design GNNs with densified individual layers modeling explicitly longerrange node-to-node relationships in a graph, that enable more shallow architectures and at the same time scale up to larger (not necessarily sparse) graphs?</p><p>We answer it affirmatively by introducing Graph Kernel Attention Transformers (GKATs). GKATs take inspiration from recent research on dense linear attention Transformers <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b26">27]</ref>, which have demonstrated the advantages of kernel-based approaches over sparse attention layers. GKATs model graph attention within each layer as a Hadamard product of the kernel matrix of the nodes' feature vectors and a graph kernel matrix, where the latter kernel is defined on graph nodes. GKATs' attention matrices combine a continuous signal coming from the vertices of the graph with a discrete graph topology signal, modulating standard graph-agnostic attention on nodes' representations via the more refined (vs. neighborhood relationship) topological signal encoded by a graph kernel.</p><p>GKATs have several advantages over previous methods: (1) they are capable of modeling dependencies between distant nodes within a single attention layer, (2) they can do this in linear time &amp; memory, not requiring explicitly storing graph representations (such as adjacency matrices) to compute attention layers if a graph kernel has finite (at least on expectation) (random) feature representation, (3) they are highly flexible since they can be used with various graph kernels. In the paper, we will show how to design expressive and efficiently computable graph kernels with finite (random) feature representations and how they relate to well-established graph kernels, such as the graph diffusion kernels, that are notoriously difficult to apply due to their cubic computational cost.</p><p>Graphots: Such a re-designing of GNNs has profound consequences. In GKATs, the embedding of every node in any layer can be obtained by a simple linear transformation from a latent graph state in that layer, which we call a graphot. Even more importantly, graphots' sizes do not depend on the number of the nodes of the graph and play the role of learnable general use-case graph sketches. They can be interpreted as compact associative memories <ref type="bibr" target="#b20">[21]</ref> storing compressed representations of all the nodes in the graph as well as their mutual relationships determined by graph's topology.</p><p>We confirm our theoretical results empirically, where GKATs consistently beat their competitors, including nine other GNN classes. As we show in Section 5, this performance is consistent on a wide range of tasks from motif detection, through social neetwork classification to bioinformatics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The number of different GNN architectures is vast <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b29">30]</ref>, yet most of them admit a modular structure of distinct blocks responsible for: <ref type="bibr" target="#b0">(1)</ref> signal propagation between nodes (defining an information aggregation mechanism), (2) sampling nodes for massive graphs that cannot be entirely stored in memory and (3) pooling for hierarchical representations. The propagation modules most often can be categorized as working in spectral or spatial domain. In the former, a graph signal is translated into spectral domain via graph Fourier transform (FT), convolution is applied via the matrix of eigenvectors of the normalized graph Laplacian and the resulting signal is translated back to the spatial domain via inverse FT <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b3">4]</ref>. In the latter, then convolution is applied directly on the graph via weighted adjacency matrices. Examples include: learnable graph convolutional networks (LGCNs) <ref type="bibr" target="#b11">[12]</ref>, diffusion convolutional neural networks <ref type="bibr" target="#b0">[1]</ref>, Neural FPs <ref type="bibr" target="#b9">[10]</ref> and more.</p><p>Another line of work on GNNs incorporates attention mechanisms that were widely popularized by Transformers <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b25">26]</ref> which became SOTA architectures for processing sequential data. Papers on that subject build on an intrinsic connection between Transformers and GNNs. The former can be thought of as special instantiations of GNNs with the corresponding fully-connected graph topologies. Graph attention neural networks (GATs) <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b41">42]</ref> leverage this connection by replacing a regular attention matrix modeling relationships between all the tokens/nodes by the one with sparsity priors determined by the topology of the input graph. Thus in a fixed layer, every node attends only to its neighbors. As Transformers, GATs apply multi-head attention mechanism to robustify training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries</head><p>We consider weighted undirected graphs G = (V, E, W, H), where V is the set of nodes/vertices, E is the edge set, W = {w i,j : {i, j} ∈ E} is the set of edge weights and H = {h i ∈ R d : i ∈ V} is the set of nodes' feature vectors. We will denote by Adj(G) the (weighted) adjacency matrix of G.</p><p>A regular graph attention mechanism (GAT) <ref type="bibr" target="#b34">[35]</ref> processes such a data with standard attention layers <ref type="bibr" target="#b33">[34]</ref>, where the attention matrix A i of the ith layer is given as</p><formula xml:id="formula_0">A i = Adj(G) A i</formula><p>H (for the Hadamard/element-wise product ), A i H is the attention matrix for nodes' features, i.e.</p><formula xml:id="formula_1">A i H (k, l) = exp(q i k (k i l ) )w k,l {k,r}∈E exp(q i k (k i r ) )w k,r ,<label>(1)</label></formula><p>and q i t , k i t ∈ R d for t ∈ V (row-vector representations of queries/keys respectively) are learnable linear projections of nodes' feature vectors h i t in the ith layer. Thus the topology of the graph provides an inductive bias defining explicitly modeled relationships within a layer (those between neighboring tokens) and thus consequently introducing sparse attention patterns. The output of the attention module is computed as: A i V i , where the rows v i t ∈ R d for t ∈ V of V ∈ R N ×d are other learnable linear projections of the feature vectors h i t , and N = |V| is the number of the vertices of the graph. Cost of the regular attention: That cost is O(M d), where M = |E| is the number of edges and the required memory is O(M + N d) (note that depending on the sparsity of the graph, a matrix can be stored explicitly as an adjacency matrix or as a list of N vertex adjacency lists). Thus time and space complexity is quadratic in the number of nodes for dense graphs with M = Ω(N 2 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">GKATS: Towards Expressive and Scalable Graph Processing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Replacing Local with Longer-Range Attention for Graphs</head><p>The fundamental idea underpinning the GKAT architecture is the observation that the attention matrix A i from GAT can be "densified" as follows:</p><formula xml:id="formula_2">A i (k, l) = K(q k , k l )T(k, l) r∈V K(q k , k r )T(k, r) ,<label>(2)</label></formula><p>where K : R d ×R d → R is a kernel defined on feature vectors in nodes of the graph and T : V ×V → R is another kernel defined on the nodes of the graph G (that for simplicity will depend only on the topology of the graph, but not feature vectors in those nodes). In particular, the former kernel can be a softmax kernel given as K smax (x, y) = exp(xy ) that is already used in the regular attention.</p><p>This method replaces the discrete mechanism, modulating attention on features in nodes based on filtering out nonadjacent pairs of nodes, by a smooth one, where filtering is substituted with weighting via a graph kernel T. The smoothing enables the attention to model longer-range dependencies within a single layer since it allows distant nodes in the graph to directly attend to each other.</p><p>Note that a GAT method is a special instantiation of that paradigm with</p><formula xml:id="formula_3">T GAT : V × V → R defined as T GAT (k, l) = φ GAT (k) φ GAT (l), where φ GAT (i), φ GAT (i) ∈ R M are row-vectors indexed by the edges of G, given as: φ GAT (i)({u, v}) = 1[u = i] √ w u,v , φ GAT (i)({u, v}) = 1[v = i] √ w u,v .</formula><p>Graph Diffusion Kernels: A prominent example of kernels defined on graph nodes is the class of the so-called graph diffusion kernels T λ diff (GDKs) <ref type="bibr" target="#b32">[33]</ref> with kernel matrices</p><formula xml:id="formula_4">T λ diff def = [T λ diff (k, l)</formula><p>] k,l∈V given as: T λ diff = exp(λM) for a kernel hyperparameter λ &gt; 0 and, where M is either a (weighted) adjacency matrix of the input graph or (for the spectral domain variant) its Laplacian (here exp stands for matrix exponentiation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Decomposable Longer-Range Attention for Graphs</head><p>Even though densified graph attention from Sec. 4.1 does not need several layers to communicate signal between nonadjacent nodes, at first glance it seems that it is at the price of substantial extra cost per layer. It requires the computation of the graph kernel matrix T = [T(k, l)] k,l∈V which is of cubic in N time complexity for most interesting graph kernels (in particular graph diffusion kernels). It was observed in <ref type="bibr" target="#b5">[6]</ref> that if a regular (non-graph) attention kernel K :</p><formula xml:id="formula_5">R d × R d → R (general- izing softmax-kernel K soft (x, y) = exp(xy ) from Eq. 1) satisfies: K(x, y) = E[φ(x)φ(y) ]</formula><p>for arbitrary row-vectors x, y ∈ R d and some (deterministic or randomized) mapping φ : R d → R m then an attention matrix A leveraging that kernel can be approximated as:</p><formula xml:id="formula_6">A = Diag −1 (Q (K ) 1 N )Q (K ) T</formula><p>, where N stands for the length of the attention input sequence, 1 N ∈ R N is the all-one vector, Diag(r) stands for the diagonal matrix with diagonal r and furthermore the rows of Q ,K ∈ R N ×m , are φ-transformations of queries and keys respectively. Furthermore, Q (K ) is an unbiased approximation of the unnormalized attention matrix</p><formula xml:id="formula_7">A unnorm = [K(q i , k j )] i,j=1,...,N .</formula><p>Then the output of the attention block can be approximated as:</p><formula xml:id="formula_8">Diag −1 (Q ((K ) 1 N ))Q ((K ) T V),</formula><p>where brackets indicate the order of computations. Thus, the attention matrix is never explicitly materialized and space/time complexity is only linear in N . It is also linear in m, but as long as m N , this mechanism brings computational gains. We call this method an implicit attention.</p><p>We observe that a function</p><formula xml:id="formula_9">K prod (k, l) : V × V → R defined as: K prod (k, l) def = K(q k , k l )T(k, l</formula><p>) is a kernel that can be expressed as:</p><formula xml:id="formula_10">K prod (k, l) = E[(φ K (q k ) ⊗ φ T (k))(φ K (k l ) ⊗ φ T (l)) ] (3) if K(x, y) = E[φ K (x)φ K (y)] for φ K : R d → R m1 and T(k, l) = E[φ T (k)φ T (l)] for φ T : V → R m2</formula><p>, φ K and φ T are constructed independently and where u ⊗ v defines row-vectorized cartesian product of u and v. This is a standard finite cartesian features mechanism for the compositional kernels <ref type="bibr" target="#b6">[7]</ref>. We conclude that K prod admits (on expectation) a finite kernel feature decomposition required for the implicit attention.</p><p>Note that the GKAT mechanism presented in Eq. 2 is an attention method using kernel K prod and thus it can leverage computationally efficient implicit attention mechanism (see also: Fig. <ref type="figure" target="#fig_0">1</ref>), as long as one can find finite mappings φ 1 and φ 2 . Efficient mappings φ 1 exist for several kernels used in attention in practice (often defining them), in particular for the most popular softmax kernel <ref type="bibr" target="#b5">[6]</ref>. Next we show how to define expressive kernels on graph nodes that lead to efficient mappings φ 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Random Walks Graph-Nodes Kernels</head><p>Instead of working directly with graph diffusion kernels defined in Sec. 4.1, we will use introduced by us class of the so-called Random Walks Graph-Nodes Kernels or RWGNKs, defined on graph nodes (as opposed to exhaustively explored random walk graph kernels defined on graphs <ref type="bibr" target="#b15">[16]</ref>). Intuitively, the value of the RWGNK for two nodes is given as a dot-product of two frequency vectors that record visits in graph nodes of random walks beginning in the two nodes of interest. More formally, for the hyperparameters λ, α ≥ 0, and two random walks ω(k),ω(l) with stopping probability 0 ≤ p ≤ 1 (or of a fixed length) starting at k and l respectively, the RWGNK is given as:</p><formula xml:id="formula_11">K λ,α,p RWGNK (k, l) = E ω(k) [f ω(k),λ k ] E ω(k) [f ω(k),λ k ] α 2 E ω(l) [f ω(l),λ l ] E ω(l) [f ω(l),λ l ] α 2 .<label>(4)</label></formula><p>The (row) frequency vector f ω(h),λ h for h ∈ V can be defined in various ways. In its base variant</p><formula xml:id="formula_12">f ω(h),λ h ∈ R N is given as f ω(h),λ h (i) def = l∈L ω(h) (i) λ l</formula><p>, where L ω(h) (i) is the set of lengths of those prefix sub-walks of a given random walk ω(h) that end at i (where the prefix sub-walk of the walk (j 1 , .j 2 , ..., j t ) is any walk of the form (j 1 , ..., j r ) for some r ≤ t or an empty walk). Note that Eq. 4 leads to the desired representation of K λ RWGNK as K λ RWGNK (k, l) = Ψ(k)Ψ(l) , where Ψ(h) is the renormalized expected frequency vector. In practice expectations are replaced by Monte Carlo samplings over few random walks and vectors Ψ(h) are not stored explicitly, but in the form of weighted lookup tables. In the anchor-points variant, frequency vectors f ω(h),λ h ∈ R s are indexed by a selected (e.g. randomly) set of s anchor-points in the graph. The advantage of the anchor-point version is that no lookup tables are needed for efficient implementation and decomposable attention can be realized simply via (heavily optimized on TPUs) dense matrix-matrix multiplication operations.  </p><formula xml:id="formula_13">Γ (1 − p)λ d max (G) Adj(G) ≤ K λ,α,p RWGNK (G) ≤ Γ (1 − p)λ d min (G) Adj(G) ,<label>(5)</label></formula><p>where Γ(A) = ∞ i=0 (i + 1)A i . Using the fact that Adj i (G) encodes the number of walks of length i between pairs of vertices in G, we conclude that K λ,0,p</p><formula xml:id="formula_14">RWGNK (k, l) = ∞ i=0 c i k,l r k,l (i)</formula><p>, where: r k,l (i) is the number of walks of length i between nodes: k and l and</p><formula xml:id="formula_15">i √ i+1(1−p)λ dmax(G) ≤ c k,l ≤ i √ i+1(1−p)λ dmin(G) . Note that GDK with parameter λ satisfies: GDK λ (k, l) = ∞ i=0 ci (k, l)r k,l (i), where: c(k, l) = λ i √ i!</formula><p>. In practice, it suffices to have random walks of fixed length (instead of taking p &gt; 0) (see: Sec 5). Furthermore, by taking α &gt; 0 (e.g. α = 1) we can guarantee that kernel values are bounded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">GKAT Algorithm</head><p>We are ready to summarize GKAT's graph attention layers (for the complete algorithmic box, see: Appendix, Sec. 7.2). The algorithm is parameterized by two kernels:</p><formula xml:id="formula_16">K : R d × R d → R, T : V × V → R.</formula><p>Graph kernel T is chosen to be a RWGNK kernel (as in Sec. 4.3), parameterized by discount factor λ &gt; 0, α ≥ 0 and stopping probability 0 &lt; p &lt; 1 (or alternatively fixed walk length τ ). For each random walk, in each node with probability 1 − p its neighbor is chosen uniformly at random for the next step. Furthermore, K is chosen to admit (potentially on expectation) finite (potentially random) kernel feature decomposition via mapping φ K (e.g. regular softmax attention <ref type="bibr" target="#b5">[6]</ref>). For smaller graphs, the attention matrix A can be computed explicitly as in Eq. 2 and the layer outputs AV (in that setting, in principle any graph kernel defined on nodes can be applied, e.g. GDK). For larger graphs, A is not explicitly materialized, but instead the output of the attention module is calculated, as explained in Sec. 4.2, with the use of introduced there cartesian features (for φ T (l) = Ψ(l) as in Sec. 4.3). Full GKAT's architecture consists of several blocks, where each block (as in the regular Transformer <ref type="bibr" target="#b33">[34]</ref>), is built from the attention layer and standard MLP layer. In practice, because of longer-range dependencies modeling within individual attention layers by GKAT, the total number of blocks can be small (see: Sec. 5), resulting in more shallow end-to-end pipelines.</p><p>Space &amp; Time Complexity: Denote by τ the average length of the random walk and by t the number of random walks used per node. Furthermore, let m be: the average number of nodes visited from a given node in a graph by conducting t random walks for the no-anchor-points version and the number of anchor-points for the anchor-points variant. For the former, we clearly have: m ≤ τ t. Computing all random walks takes time O(N τ t) (sampling uniformly next node of a random walk can be done in constant time regardless of a degree of a current node). This is a one-time cost per input graph. For the anchor-points version, there is an additional one-time cost of computing anchor points (O(m) if those are chosen uniformly at random). Vectors Ψ can be efficiently stored as weighted lookup tables of average length m, so the corresponding memory usage is O(N m). Given vectors Ψ, efficient attention computation described in Sec. 4.2 can be conducted in time O(N mrd) and space O(N mr + N d + mrd), where d is the dimensionality of nodes' embeddings and r is the number of features used to linearize kernel K. Thus if τ, t, m, r are small constants, space and time complexity is effectively linear instead of quadratic in N (see: Section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Graphots -New Neural Network Graph Sketches</head><p>Note that our decomposable longer-range attention from Sec. 4.2 can be interpreted as a graph compression technique. The new embeddings of nodes can be directly computed be left-multiplying a matrix (K ) V ∈ R m×d and a vector (K ) 1 N ∈ R m (storing approximate attention renormalization terms) by the rows of Q (corresponding to different tokens), where m stands for the total number of random features used per embedding. Crucially, the sizes of those matrices/vectors do not explicitly depend on the number of graph nodes N . Thus one can think about pair S = ((K ) V, (K ) 1 N ) as a graph sketch. We call it a graphot (see: Fig. <ref type="figure" target="#fig_0">1</ref>). The graphot stores in a compact way all learned nodes-relationships (involving both the topological signal and features in nodes) so that new embeddings of nodes can be obtained simply by interacting with that (graph-size independent) sketch rather than explicitly with other nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We tested GKAT with RWGNK kernels leading to the decomposable attention (see: Sec. 4.2). We conducted an exhaustive set of experiments ranging from purely combinatorial to bioinformatics tasks and benchmarked 10 different methods. For GKAT, we used few random walks per node. All experiments were run on a single Tesla P100 GPU with 16GB memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Combinatorial Classification</head><p>In this section we focus on the problem of detecting local patterns in graphs. A model takes a graph G as an input and decides whether it contain some graph from the given family of graphs H as a subgraph (not necessarily induced) or is H-free. This benchmark tests the abilities of different methods to solve purely combinatorial tasks. For each pattern H, an algorithm is trained to distinguish between graphs G containing H and H-free. A naive brute-force algorithm for conducting this task would have time complexity Ω(N h ), where h is the number of nodes of the motif, prohibitively expensive for all these motifs (even for small graphs G) since we have here: h ≥ 9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Erdős-Rényi Random Graph with Motifs</head><p>Data Generation: Following procedure from <ref type="bibr" target="#b24">[25]</ref>, we used five binary classification datasets consisting of random Erdős-Rényi (ER) graphs connected with motifs (positive example) or other smaller ER graphs with the same average degree as a motif (negative example), see: Fig. <ref type="figure" target="#fig_3">3</ref> (details in the Appendix, Sec. 7.3). For each dataset we constructed: S = 2048 positive and S negative examples.</p><p>Tested Algorithms &amp; Parameter Setting: We tested our GKAT, graph convolution networks (GCNs)( <ref type="bibr" target="#b19">[20]</ref>), spectral graph convolution networks (SGCs)( <ref type="bibr" target="#b7">[8]</ref>) and graph attention networks (GATs)( <ref type="bibr" target="#b34">[35]</ref>). A feature vector in each vertex was of length l = 5 and contained top ordered l degrees of its neighbors (if there are fewer than l neighbors, we padded zeroes). A dataset for each motif was randomly split into 75%/25% training/validation set. We chose: the number of epochs E = 500, batch size B = 128, used Adam optimizer with learning rate η = 0.001 and early-stopped training if neither the validation loss nor validation accuracy improved for c = 80 continuous epochs.</p><p>We applied 2-layer architectures. For GCNs and SGCs, we used h = 32 nodes in the hidden layer. For SGC, we furthermore binded each hidden layer with 2 polynomial localized filters. For GAT and GKAT, we used 2 attention heads, with h = 9 nodes in the hidden layer to make all models of comparable sizes. In GKAT we used random walks of length τ = 3. The results are presented in Fig. <ref type="figure" target="#fig_4">4</ref>. We see that GKAT outperforms all other methods for all the motifs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Detecting Long Induced Cycles &amp; Deep versus Dense Attention Tests</head><p>Next we took as H an infinite family of motifs rather than just a single motif. The algorithm needed to decide decide whether a graph contains an induced cycle of length &gt; T for a given constant T . Thus the motif itself became a global property that cannot be detected by exploring just a close neighborhood of a node. In this experiment we focused also on the "depth versus density" trade-off. Shallow neural networks with dense attention from Eq. 2 are capable of modeling deeper networks relying on sparse layers, yet the price is extra computational cost per layer. The question arises whether architectures that apply RWGNK kernels leveraging efficient decomposable long-range attention from Sec. 4.2 can also replace deeper counterparts or they lose their expressiveness. Dataset Generation: We created S = 2048 random binary trees, each having 50 nodes, with 75%/25% for training/validation. For each tree, we constructed a positive example, by connecting two nodes with the farthest distance from each other (a negative example was obtained by connecting two random vertices, but not farthest from each other). Note that a positive example constructed in such a way has shortest induced cycle of length P + 1, where P is the diameter of the tree.</p><p>Tested Algorithms &amp; Parameter Setting: We used the same algorithms as before. This time we run detailed ablation studies on the depth of the competitors of our GKAT, by comparing two-layer GKAT with GATs, GCNs and SGCs of up to six hidden layers.</p><p>To make fair comparison, we used models with comparable number of parameters. For the two-layer GKAT, we applied 8 heads in the first layer, and 1 head in the second layer. The dimension of each head was d = 4. The last layer was fully-connected with output dimensionality o = 2 for binary classification. We applied random walk length of τ = 6. For GCN, GAT and SGC, we tested number of layers ranging from 2 to 6. We controlled the number of nodes in the hidden layer(s) for GCN, GAT and SGC, and the number of attention heads in each head for GAT so that their total number of trainable parameters was comparable with this of our two-layer GKAT. All other parameters were chosen as in Sec. 5.1.1. More details on parameter settings and additional ablation tests over random walk length of GKAT are given in Table <ref type="table">5</ref> and Fig. <ref type="figure">6</ref> in the Appendix (Sec. 7.3). Our main results are presented in Fig. <ref type="figure" target="#fig_5">5</ref>.</p><p>We see that a shallow two-layer GKAT beats all GCN-variants (also deeper ones) as well as GATs and SGCs with &lt; 4 layers by a wide margin. A two-layer GKAT is asymptotically equivalent to the four-layer GAT and SGC, yet as we show in Sec. 5.3, is faster to train and run inference on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Graph Neural Networks for Bioinformatics Tasks &amp; Social Networks Data</head><p>Datasets: We tested GKAT for graph classification tasks on 9 standard and publicly available bioinformatics and social networks datasets <ref type="bibr" target="#b18">[19]</ref> using a carefully designed model selection and assessment framework for a fair comparison <ref type="bibr" target="#b10">[11]</ref>. The former include: D&amp;D <ref type="bibr" target="#b8">[9]</ref>, PROTEINS <ref type="bibr" target="#b1">[2]</ref>, NCI1 <ref type="bibr" target="#b35">[36]</ref> and ENZYMES <ref type="bibr" target="#b13">[14]</ref>, and the latter: IMDB-BINARY, IMDB-MULTI, REDDIT-BINARY, REDDIT-5K and COLLAB <ref type="bibr" target="#b38">[39]</ref> (for detailed description, see Table <ref type="table" target="#tab_4">6</ref> in the Appendix, Sec. 7.4.1). Table <ref type="table">1</ref>: Performance of different algorithms on the bioinformatics datasets. For each dataset, we highlihgted the best performing method(s) and underlined the second best. GKAT is the best on three out of four tasks.  Tested Algorithms: We compared GKAT with some of the top GNN methods used previously for that data: DCGNN <ref type="bibr" target="#b42">[43]</ref>, DiffPool <ref type="bibr" target="#b39">[40]</ref>, ECC <ref type="bibr" target="#b31">[32]</ref>, GraphSAGE <ref type="bibr" target="#b12">[13]</ref> and RWNN <ref type="bibr" target="#b24">[25]</ref>, which are selected based on their popularity and architectural differences. For bioinformatics datasets, but ENZYMES, we used the Molecular Fingerprint (MF) method <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b22">23]</ref> as a baseline. It first applies global sum pooling and then a single-layer MLP with ReLU activations. For social datasets and ENZYMES, we applied the DeepMultisets (DM) method <ref type="bibr" target="#b40">[41]</ref> as a baseline. It is a single-layer MLP followed by global sum pooling and another single-layer MLP for classification. These two baselines use no topological graph signal, so are good to quantify the benefits coming from GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D&amp;D</head><p>GKAT Setting: We applied a two-layer GKAT followed by the baseline layers: we first applied an attention layer with k heads (a hyperparameter to be tuned), and then another one with one head to aggregate topological information on graphs. Next, we applied either the MF method or the DM method to further process the aggregated information. The random walk length τ in each GKAT layer satisfied: τ ≤ 4 and depended on the evaluated datasets. A long random walk could in principle capture more information, but at the cost of adding contributions also from non-relevant nodes. The average graph diameter (the average value of the longest shortest path for each pair of nodes and over all graphs in a dataset) shown in Table <ref type="table" target="#tab_4">6</ref> in the Appendix helps to calibrate walk length. We chose it to balance the pros of using a shallow architecture and the cons of information loss from such dense layer compression. Our two-layer GKAT with multi-heads and small number of trainable parameters in each head increased the number of the baseline's parameters only by a negligible fraction.</p><p>Training Details: We used a 10-fold CV for model assessment, and an inner holdout with 90%/10% training/validation split for model selection following the same settings <ref type="bibr" target="#b10">[11]</ref>. We then trained the whole training-fold three times, randomly holding out 10% of the data for early stopping after model selection in each fold. The average score of these three runs was reported in Table <ref type="table">1</ref> and Table <ref type="table" target="#tab_1">2</ref>.</p><p>The results from Table <ref type="table">1</ref> and Table <ref type="table" target="#tab_1">2</ref> show that GKAT is the best on three out of four bioinformatics datasets and is among two best methods on four out of five social network datasets. Furthermore, it's the only GNN method that consistently outperforms baseline on all but one bioinformatics dataset (bio-data benefits more than others from efficient longer-range attention modeling as showed in <ref type="bibr" target="#b5">[6]</ref>).</p><p>In the Appendix (Sec. 7.5) we included additional comparisons of GKAT with GAT on citation networks, where GKAT outperforms GAT on two out of three tested datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Space &amp; Time Complexity Gains of GKAT</head><p>Finally, we measured speed and memory improvements coming from GKAT with decomposable attention mechanism turned on (called here shortly: GKAT+) as compared to GAT as well as accuracy loss in comparison to GKAT with decomposability not leveraged. The results are presented in Table <ref type="table" target="#tab_2">3</ref>. We decided to report relative rather than absolute numbers since the former are transferable across different computational setups. We see that the accuracy gaps of the corresponding GKAT and GKAT+ models (obtained after the same number of epochs) are marginal, yet GKAT+ yields consistent speed and memory gains as compared to GAT per attention layer, particularly substantial for very large graphs as those from Citeseer and Pubmed. Below we show that regular GKAT is also faster that its counterparts (GCN, GAT and SGC) in terms of wall clock time needed to reach particular accuracy levels. We illustrate it by comparing accuracy levels reached by different models in a given wall clock time budget (the clock time that GKAT needs to complete first 100 epochs). The results are presented in Table <ref type="table" target="#tab_3">4</ref>. </p><formula xml:id="formula_17">W Q , W K ∈ R d×d QK ,W V ∈ R d×d . Output: new matrix of embeddings H ∈ R N ×d .</formula><p>Preprocessing [one-time computation per sample and all attention layers]: Version I: For every node h, compute graph kernel random feature map Ψ(h) corresponding to graph kernel T = K λ,α,p RWGNK and defined as:</p><formula xml:id="formula_18">Ψ(h) = v v α for v = 1 t t i=1 f ω(i),λ h</formula><p>, where ω(1), ..., ω(t) are t independent random walks from h constructed either with stopping probability p or of fixed length τ (see: Sec. 4.3). Version II: Compute kernel matrix T</p><formula xml:id="formula_19">(G) = [T(k, l)] k,l∈V(G) . Main computation: 1. Compute Q = HW Q , K = HW K , V = HW V .</formula><p>2. Version I: Compute Q and K with rows defined as: φ K (q l ) ⊗ Ψ(l) and φ K (k l ) ⊗ Ψ(l) respectively for l ∈ V(G) and output:</p><formula xml:id="formula_20">H = Diag −1 (Q ((K ) 1 N ))Q ((K ) T V) (see: Sec. 4.2).</formula><p>2. Version II: Compute attention matrix A as in Eq. 2 and output: H = AV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Combinatorial Classification Experiments: Additional Details</head><p>The data for the motif detection task from Section 5.1.1 was generated as follows:</p><p>• Firstly we created five simple motifs as shown in Fig. <ref type="figure" target="#fig_3">3</ref>. Note that each motif has ≥ 9 vertices so a brute-force combinatorial algorithm for motif-detection would take time Ω(N 9 ), prohibitively expensive even for small graphs G.</p><p>• We then generated for each motif S small Erdős-Rényi graphs with the same number of nodes as that motif and the same average degree.</p><p>• For each motif, we also generated S larger Erdős-Rényi random graphs, each of 100 vertices, again of the same average degree.</p><p>• We obtained positive/negative samples by connecting each larger Erdős-Rényi random graph with the motif/previously generated smaller Erdős-Rényi random graph (with certain edge probability).</p><p>In Table <ref type="table">5</ref> we present additional details regarding architectures used in the experiments from Section 5.1.2, in particular the number of parameters and heads / polynomial filters used in different layers.</p><p>Ablation tests over GKAT random walk length for Section 5.1.2 are presented in Fig. <ref type="figure">6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">GNNs for Bioinformatics</head><p>Tasks &amp; Social Networks Data: Additional Details</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.1">Datasets Descriptions</head><p>Detailed profiles of the datasets used in the experiments from Sec. 5.2 are given in Table <ref type="table" target="#tab_4">6</ref>.   <ref type="table" target="#tab_4">6</ref>.</p><p>and dimension of each head in a GKAT layer. We also tuned other options: whether to add a fullyconnected layer after data normalization, but before GKAT layers, and dimension of fully-connected layers (both preceding and coming after GKAT layers). Due to the large amount of tunable parameters, we decided to first conduct a rough search for each parameter using only one random CV fold, select one/several parameter combination(s) with best performance, and then reused on all other folds.</p><p>For all other methods, we reported the best scores conducted via an extensive hyperparameters grid search <ref type="bibr" target="#b10">[11]</ref>. For GKAT, we fixed the number of epochs to E = 1000, early stopping patience as 500 epochs, the criterion for early stopping as validation accuracy, global pooling method as summation, and used Adam optimizer. Then we performed hyperparameter tuning for: batch size B ∈ {32, 128}, learning rate η ∈ {0.01, 0.001, 0.0001}, dropout ratio ∈ {0.0, 0.1, 0.2, 0.4}, L 2 -regularization rate ∈ {0.001, 0.005}, dimension of attention head in the first GKAT layer h ∈ {4, 8, 16, 32}, number of attention heads in the first GKAT layer ∈ {1, 4, 8, 12}, number of nodes in the MLP layer ∈ {32, 64, 128}, GKAT random walk length τ ∈ {1, 2, 3, 4}, whether to use a fully-connected layer before the first GKAT layer, and whether to apply batch normalization to pre-prosess data before feeding the model with it.</p><p>For some of the datasets (e.g. D&amp;D), we selected the best hyperparameter set optimized over one random CV fold, and used it across all cross-validation outer folds. </p><p>We conclude that:</p><formula xml:id="formula_22">∞ i=0 r k,l (i) (1 − p)λ d max (G) i (i + 1) ≤ K λ,0,p RWGNK (k, l) ≤ ∞ i=0 r k,l (i) (1 − p)λ d min (G) i (i + 1)<label>(11)</label></formula><p>To complete the proof, it suffices to notice that matrix Adj i (G) encodes the number of walks of length i between pairs of vertices in G.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Decomposable longer-range attention in GKATs. For the clarifty of the exposition, we skip rownormalization of the attention matrix A. Matrix A is a Hadamard product of two matrices: AH using kernel K defined on query-key pairs of the nodes' emebeddings from H and kernel matrix corresponding to the graph kernel T. If both K and T admit finite kernel feature decomposition (at least on expectation), a features corresponding to T and b corresponding to K can be used to construct a × b cartesian features (the highlighted slices of Q and K ). These, via matrix associativity property, lead to attention computation scheme avoiding explicit calculation of A and instead relying on the graph sketch called by us a graphot.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: From left to right: unweighted graph G, its adjacency matrix Adj(G), its GDK matrix exp(Adj(G))and RWGNK-matrix with walk length of 3 and α = 1. Colored cells measure the relationship among pairs of nodes (darker is stronger). Last two matrices can be thought of as continuous smoothings of Adj(G).</figDesc><graphic url="image-25.png" coords="5,113.41,213.82,382.69,77.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2</head><label>2</label><figDesc>Figure2shows an intuition behind RWGNKs. Attending only to neighbors within a fixed layer via adjacency matrices limits the network's capability of aggregating fast distinct signal. Unlike adjacency matrix, diffusion kernel reveals long-range dense relationships among all the nodes. Equipped with continuous rather than binary values, it can also quantify their strengths. However, it requires cubic computation time O(N 3 ). Our RWGNKs model dense attention but scale only linearly in N .Next we explore properties of RWGNKs and their connections with GDKs (proof in the Appendix). We denote by d max (G), d min (G) the maximum and minimum degree of a vertex in G respectively. Theorem 1 (RWGNKs count discounted numbers of walks). The following is true for the kernel matrix K λ,α,p RWGNK (G) = [K λ,α,p RWGNK (k, l)] k,l∈V(G) of the RWGNK kernel with 0 ≤ λ ≤ 1, α = 0 and 0 &lt; p &lt; 1 for a graph G with vertex set V(G) of size N (element-wise matrix inequality):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Five motifs (patterns) used in the first class of the combinatorial classification experiments. For each</figDesc><graphic url="image-26.png" coords="6,109.98,506.65,392.03,81.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Model accuracy comparison of all four methods: GKAT, GAT, GCN and SGC on the motif-detection task. All tested architectures are 2-layer. GKAT outperforms other algorithms on all the tasks.</figDesc><graphic url="image-27.png" coords="7,108.00,188.14,392.05,78.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Comparison of the two-layer GKAT with different variants of GCNs, GATs and SGCs, varying by the number of hidden layers. Shallow GKAT architecture has expressiveness of deeper version of its counterparts and in fact outperforms many of them (e.g. graph convolution networks.)</figDesc><graphic url="image-28.png" coords="7,109.98,387.98,392.04,118.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Representative plots for bioinformatics datasets. For each bioinformatics dataset, we chose the graph with number of nodes most similar to the average number of nodes shown in Table6.</figDesc><graphic url="image-30.png" coords="15,108.00,214.97,392.04,68.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-29.png" coords="14,167.40,63.50,277.21,190.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance of different algorithms on the social network datasets. As for the previous table, for each dataset we marked the best performing methods. GKAT is among two top methods for four out of five tasks.</figDesc><table><row><cell></cell><cell>IMDB-B</cell><cell>IMDB-M</cell><cell>REDDIT-B</cell><cell>REDDIT-5K</cell><cell>COLLAB</cell></row><row><cell>Baseline</cell><cell cols="2">70.8±5.0% 49.1 ±3.5%</cell><cell>82.2±3.0%</cell><cell>52.2±1.5%</cell><cell>70.2±1.5%</cell></row><row><cell>DGCNN</cell><cell cols="2">69.2±5.0% 45.6±3.4%</cell><cell>87.8±2.5%</cell><cell>49.2±1.2%</cell><cell>71.2±1.9%</cell></row><row><cell>DiffPool</cell><cell cols="2">68.4±3.3% 45.6±3.4%</cell><cell>89.1±1.6%</cell><cell>53.8±1.4%</cell><cell>68.9±2.0%</cell></row><row><cell>ECC</cell><cell cols="5">67.7±2.8% 43.5±3.1% out of memory out of memory out of memory</cell></row><row><cell cols="3">GraphSAGE 68.8±4.5% 47.6±3.5%</cell><cell>84.3±1.9%</cell><cell>50.0±1.3%</cell><cell>73.9±1.7%</cell></row><row><cell>RWNN</cell><cell cols="2">70.8±4.8% 47.8±3.8%</cell><cell>90.4±1.9%</cell><cell>51.7±1.5%</cell><cell>71.7±2.1%</cell></row><row><cell>GKAT</cell><cell cols="2">71.4±2.6% 47.5±4.5%</cell><cell>89.3±2.3%</cell><cell>55.3±1.6%</cell><cell>73.1±2.0%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Speed &amp; Space Complexity gains coming from GKAT with decomposable attention mechanism turned on (GKAT+). First row: memory compression of the graph via graphot (lower better). Second &amp; third row: speedup in training and inference respectively per one attention layer as compared to GAT. Last row: accuracy loss as compared to GKAT not applying decomposable attention mechanism. We used four datasets from Sec. 5.1.1, a dataset from Sec. 5.1.2 (Tree) and two citation network datasets (see: Sec. 7.5): Citeseer and Pubmed with graphs of much larger sizes and on which GKAT also outperforms GAT. We applied r random features to linearize softmax kernel for features in nodes with r = 256 for citation network datasets, r = 16 for datasets from Sec. 5.1.1 and r = 8 for a dataset from Sec. 5.1.2.</figDesc><table><row><cell></cell><cell cols="4">Cavem. Circle Grid Ladder</cell><cell>Tree</cell><cell cols="2">Citeseer Pubmed</cell></row><row><cell>graphot size/graph size</cell><cell>0.54</cell><cell>0.53</cell><cell>0.55</cell><cell>0.52</cell><cell>0.95</cell><cell>0.18</cell><cell>0.07</cell></row><row><cell>train speedup vs GAT</cell><cell>1.40x</cell><cell>1.41x</cell><cell>1.42x</cell><cell>1.40x</cell><cell>1.10x</cell><cell>5.10x</cell><cell>9.50x</cell></row><row><cell>inf speedup vs GAT</cell><cell>1.46x</cell><cell>1.49x</cell><cell>1.49x</cell><cell>1.47x</cell><cell>1.12x</cell><cell>5.21x</cell><cell>9.54x</cell></row><row><cell>GKAT -GKAT+ (accur.)</cell><cell>0.07%</cell><cell cols="4">0.09% 0.08% 0.07% 0.06%</cell><cell>0.05%</cell><cell>0.06%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Running time of training different networks on datasets from Sec. 5.1.1 and Sec. 5.1.2. For GCN, GAT and SGC, we reported the accuracy with 2 layers. For GKAT, we used a 2-layer architecture and reported the accuracy with a fixed walk length of 6 for Induced Cycle Detection, and of 3 for five motifs from Sec. 5.1.1.We presented Graph Kernel Attention Transformers (GKATs), new attention-based graph neural networks leveraging graph kernel methods and recent advances in scalable attention to provide more expressive models for graph data that are also characterized by low time complexity and memory footprint. Furthermore, we demonstrated that they outperform other techhniques on a wide range of tasks from purely combinatorial problems, through social network data to bioinformatics challenges.We include pointers to this part of the code that does not include sensitive/proprietary information. The core GKAT framework is here: https://anonymous.4open.science/r/GKAT-Experiments-BD32. We used (deterministic and random) feature map mechanisms corresponding to the features defined in graph nodes from this repository: https://github.com/google-research/googleresearch/tree/master/performer.7.2 Graph Kernel Attention Transformers: Algorithmic box for the Attention LayerBelow we present main algorithmic box for a fixed attention layer computation in GKAT (and for a single head). Version II corresponds to the base GKAT version not leveraging decomposability of the attention and Version I to the one that uses it.Algorithm 1 Attention Layer in GKAT [computations per head]Hyperparams: 0 &lt; λ ≤ 1, α ≥ 0, 0 &lt; p &lt; 1 (or fixed walk length τ ), number of walks t per node, kernel K defined on features in nodes or its corresponding kernel feature map φ K .</figDesc><table><row><cell></cell><cell cols="3">Induced Cycle Caveman Circle</cell><cell>Grid</cell><cell cols="2">Ladder Circle Ladder</cell></row><row><cell>GCN</cell><cell>63.2%</cell><cell>62.1%</cell><cell cols="2">71.4% 59.3%</cell><cell>66.7%</cell><cell>87.4%</cell></row><row><cell>GAT</cell><cell>77.0%</cell><cell>69, 1%</cell><cell cols="2">80.6% 73.8%</cell><cell>75.9%</cell><cell>93.7%</cell></row><row><cell>SGC</cell><cell>56.6%</cell><cell>55.4%</cell><cell cols="2">64.7% 58.2%</cell><cell>59.1%</cell><cell>66.5%</cell></row><row><cell>GKAT</cell><cell>83.6%</cell><cell>85.1%</cell><cell cols="3">83.3% 77.1% 82.4%</cell><cell>94.6%</cell></row><row><cell>6 Conclusion</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Input: graph G, H ∈ R N ×d : matrix of rows h i ∈ R d encoding nodes' embeddings, trainable:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>Bioinformatics and Social Dataset descriptions. #NODES, #EDGES and Diameter columns containvalues averaged over all graphs in a given dataset.</figDesc><table><row><cell></cell><cell></cell><cell cols="6">#Graphs #Classes #Nodes #Edges Diameter #Features</cell></row><row><cell>BIOINF.</cell><cell>D&amp;D ENZYMES NCI1 PROTEINS</cell><cell>1178 600 4110 1113</cell><cell>2 6 2 2</cell><cell>284.32 32.63 29.87 39.06</cell><cell>715.66 64.14 32.30 72.82</cell><cell>19.90 10.86 13.26 11.48</cell><cell>89 3 37 3</cell></row><row><cell>SOCIAL</cell><cell>COLLAB IMDB-BINARY IMDB-MULTI REDDIT-BINARY REDDIT-5K</cell><cell>5000 1000 1500 2000 4999</cell><cell>3 2 3 2 5</cell><cell>74.49 19.77 13.00 429.63 508.82</cell><cell>2457.78 96.53 65.94 497.75 594.87</cell><cell>1.86 1.86 1.47 9.72 11.96</cell><cell>1 1 1 1 1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>Hyperparameter settings for the bioinformatics and social network datasets from Section 5.2.</figDesc><table><row><cell></cell><cell></cell><cell cols="6">BS #Heads d Head d FC Len rw Drop</cell><cell>L2</cell><cell cols="2">add FC Norm</cell></row><row><cell>CHEM.</cell><cell>D&amp;D NCI1 PROTEINS</cell><cell>32 32 32 128</cell><cell>8 8 4 8</cell><cell>16 32 8 32</cell><cell>128 64 32 128</cell><cell>2 4 3</cell><cell>− 0.1 −</cell><cell>0.005 0.001 0.001</cell><cell>No Yes Yes</cell><cell>BN BN BN</cell></row><row><cell></cell><cell>ENZYMES</cell><cell>32</cell><cell>4 8</cell><cell>16 32</cell><cell>32 64</cell><cell>3</cell><cell>0.1</cell><cell>0.001</cell><cell>Yes</cell><cell>BN</cell></row><row><cell>SOCIAL</cell><cell>COLLAB IMDB-BINARY IMDB-MULTI</cell><cell>32 32 32</cell><cell>12 8 12 8 12</cell><cell>4 4 8 4 8</cell><cell>128 64 128 64 128</cell><cell>2 1 1</cell><cell>− − −</cell><cell>0.005 0.005 0.005</cell><cell>No No No</cell><cell>No No BN No BN</cell></row><row><cell></cell><cell>REDDIT-BINARY</cell><cell>32</cell><cell>4</cell><cell>4</cell><cell>128</cell><cell>2</cell><cell>−</cell><cell>0.005</cell><cell>No</cell><cell>BN</cell></row><row><cell></cell><cell>REDDIT-5K</cell><cell>32</cell><cell>8</cell><cell>8</cell><cell>64</cell><cell>2</cell><cell>−</cell><cell>0.005</cell><cell>No</cell><cell>BN</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Table <ref type="table">5</ref>: Additional details regarding architectures used in Section 5.1.2. For GKAT, we applied 8 heads in the first layer, and 1 head in the second layer, with 4 hidden units in each attention head. The total number of trainable parameters was 242. For GAT, we tested the number of layers from 2 to 6, changing the number of attention heads in each layer, but with the same number of hidden units in each attention head. For GCN, we modified the number of hidden units in each layer. For SGC, we modified the number of polynomial filters and the number of hidden units in each layer. The number of attention heads in GAT, as well as the number of hidden units in each layer in GCN and SGC were chosen to make their total number of trainable parameters comparable with the corresponding number of GKAT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>#Heads</head><p>Dim For each dataset, we chose graphs with the number of nodes close to the average number of nodes shown in Table <ref type="table">6</ref>. Examples of bioinformatics-graphs from these datasets are given in Fig. <ref type="figure">7</ref>.</p><p>Examples of social network graphs from these datasets are given in Fig. <ref type="figure">8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.2">Hyperparameter Selection</head><p>In this section, we present details regarding hyperparameter selection in Section 5.2 (see: Table <ref type="table">7</ref>).</p><p>The tunable parameters included: general parameters like batch size, learning rate, dropout ratio, global pooling methods, regularization rate, data normalization methods, as well as parameters specific to our GKAT layers, which included: number of GKAT layers, number of attention heads Figure <ref type="figure">8</ref>: Representative plots for social datasets. For each social dataset, we chose the graph with number of nodes most similar to the average number of nodes shown in Table <ref type="table">6</ref>.</p><p>7.5 Experiments with Citation Networks Datasets</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5.1">Datasets Descriptions</head><p>Datasets: To directly compare GKAT with GAT, we also tested both algorithms on three publicly available citation networks datasets: Cora, Citeseer and Pubmed ( <ref type="bibr" target="#b28">[29]</ref>) with the same data splits as in <ref type="bibr" target="#b34">[35]</ref>. Datasets descriptions are given in Table <ref type="table">8</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5.2">Comparison with GAT</head><p>Experiment Settings: We used the same model architecture and parameters as in GAT for our GKAT to make the comparison as accurate as possible. The only difference is that we replaced the adjacency matrix masking in GAT by the normalized dot-product based similarity matrix generated from random walks, as described in Section 4.3. Both models used two-layer attention, with 8 attention heads in the first layer, and 1 head in the second layer. We used 8 hidden units in the first layer, and the number of output units in the second layer was the same as number of classes. Each layer was followed by an exponential linear unit (ELU) activation. We applied L 2 -regularization with λ = 0.0005, dropout with p = 0.6 for inputs and normalized attention coefficients in both layers for all three datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results:</head><p>The results are shown in Table <ref type="table">9</ref>. Our GKAT algorithm achieved lower accuracy on Cora dataset, but higher on the remaining two. Dynamic Generator of Random Walks: We also tried the so-called dynamic-GKAT. The dynamic variant generated random walks from scratch in each training epoch, thus requiring additional compute. However, one advantage of the dynamic version is that we could assign different transition probabilities for adjacent nodes (rather than sampling next point of the walk uniformly at random). The transition probability matrix can be an attention matrix from Eq. 2 in Section. 4.1. An intuition behind that particular variant is that we assign higher transition probabilities for neighbors with higher attention coefficients. The dynamic variant enabled us to improve accuracy of GKAT on Citeseer to 73.3% (with reduced 0.6% standard deviation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5.3">Ablation Tests on Random Walk Length for GKAT</head><p>Figure <ref type="figure">9</ref> compares the effect of random walk path length of GKAT algorithms on training for Cora, Citeseer and Pubmed datasets. We run GKAT with multiple random walk lengths up to 7. The results show that a small path length no longer than 4 is enough for GKAT and dynamic-GKAT, which supports our claim that short walks are sufficient for GKAT. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.6">Proof of Theorem 1</head><p>Proof. Note first that since ω(k) and ω(l) are chosen independently, we have:</p><p>Denote:</p><p>) . The key observation is that X can be rewritten as:</p><p>where Ω(k, l) is the multi-set of walks from k to l that are built from some prefix of ω(k) concatenated with some prefix of ω(l). Therefore we can write X as:</p><p>where R(k, l) is the set of walks from k to l, len(r) stands for the length (number of edges) of walk r and E(r, i) is an event that first i edges of the walk r (counting from k) form the prefix sub-walk of ω(k) and the remaining ones form the prefix sub-walk of ω(l). Therefore we have: </p><p>where r y stands for the y th vertex of the walk r starting from k and deg(v) denotes the degree of a vertex v.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Diffusion-convolutional neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Towsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993">2016. December 5-10, 2016. 1993-2001, 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Protein function prediction via graph kernels</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schönauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="56" />
			<date type="published" when="2005-01">Jan. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Geometric deep learning: Going beyond euclidean data</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno>ICLR 2014</idno>
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Learning Representations</title>
		<title level="s">Conference Track Proceedings</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</editor>
		<meeting><address><addrLine>Banff, AB, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">April 14-16, 2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">A note on spectral graph neural network. arXiv: Spectral Theory</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Rethinking attention with performers</title>
		<author>
			<persName><forename type="first">K</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sarlós</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Colwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Weller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2021. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Random features for compositional kernels</title>
		<author>
			<persName><forename type="first">A</forename><surname>Daniely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<idno>CoRR, abs/1703.07872</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<idno>CoRR, abs/1606.09375</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Distinguishing enzyme structures from non-enzymes without alignments</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Dobson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Doig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of molecular biology</title>
		<imprint>
			<biblScope unit="volume">330</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="771" to="783" />
			<date type="published" when="2003-07">July 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Aguilera-Iparraguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gómez-Bombarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07">2015. December 7-12, 2015. 2015</date>
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A fair comparison of graph neural networks for graph classification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Errica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Podda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bacciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Micheli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Large-scale learnable graph convolutional networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<editor>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Farooq</surname></persName>
		</editor>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining<address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018-08-19">2018. August 19-23. 2018. 2018</date>
			<biblScope unit="page" from="1416" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec ; I. Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Brenda, the enzyme database: updates and major new developments</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Antje</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Christian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Marion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Christian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dietmar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="D431" to="D433" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>suppl</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Efficient Õ(n/ ) spectral sketches for the laplacian and its pseudoinverse. SODA &apos;18</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jambulapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sidford</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Society for Industrial and Applied Mathematics</publisher>
			<biblScope unit="page" from="2487" to="2503" />
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fast random walk graph kernel</title>
		<author>
			<persName><forename type="first">U</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth SIAM International Conference on Data Mining</title>
				<meeting>the Twelfth SIAM International Conference on Data Mining<address><addrLine>Anaheim, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>SIAM / Omnipress</publisher>
			<date type="published" when="2012">April 26-28, 2012. 2012</date>
			<biblScope unit="page" from="828" to="838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Finetuning pretrained transformers into rnns</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kasai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno>CoRR, abs/2103.13076</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Transformers are rnns: Fast autoregressive transformers with linear attention</title>
		<author>
			<persName><forename type="first">A</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning, ICML 2020</title>
				<meeting>the 37th International Conference on Machine Learning, ICML 2020</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020-07-18">13-18 July 2020. 2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="5156" to="5165" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Benchmark data sets for graph kernels</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mutzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<ptr target="https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets" />
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="volume">795</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<title level="s">Conference Track Proceedings. OpenReview.net</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24">2017. April 24-26, 2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dense associative memory for pattern recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Krotov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Hopfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-05">2016. December 5-10, 2016. 2016</date>
			<biblScope unit="page" from="1172" to="1180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Sub-linear memory: How to make performers slim</title>
		<author>
			<persName><forename type="first">V</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Weller</surname></persName>
		</author>
		<idno>CoRR, abs/2012.11346</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Luzhnica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Day</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.04682</idno>
		<title level="m">On graph classification networks, datasets and baselines</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Computing spanners of asymptotically optimal probabilistic roadmaps</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Marble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Bekris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2011</title>
				<meeting><address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">September 25-30, 2011. 2011</date>
			<biblScope unit="page" from="4292" to="4298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Random walk graph neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Nikolentzos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vazirgiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="16211" to="16222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Image transformer</title>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning, ICML 2018</title>
				<editor>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Dy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</editor>
		<meeting>the 35th International Conference on Machine Learning, ICML 2018<address><addrLine>Stockholmsmässan, Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">July 10-15. 2018. 2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="4052" to="4061" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Random feature attention</title>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kong</surname></persName>
		</author>
		<idno>CoRR, abs/2103.02143</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Graph kernels for chemical informatics</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ralaivola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Swamidass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Saigo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Baldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1093" to="1110" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
		<title level="m">Collective classification in network data</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Structured sequence modeling with graph convolutional recurrent networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing -25th International Conference, ICONIP 2018</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cheng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Leung</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Ozawa</surname></persName>
		</editor>
		<meeting><address><addrLine>Siem Reap, Cambodia</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">December 13-16. 2018. 2018</date>
			<biblScope unit="volume">11301</biblScope>
			<biblScope unit="page" from="362" to="373" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">I</forename><surname>Shuman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="83" to="98" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dynamic edge-conditioned filters in convolutional neural networks on graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3693" to="3702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Link enrichment for diffusion-based graph node kernels</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tran-Van</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sperduti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Costa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Neural Networks and Machine Learning -ICANN 2017 -26th International Conference on Artificial Neural Networks</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Lintas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Rovetta</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">F M J</forename><surname>Verschure</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">E P</forename><surname>Villa</surname></persName>
		</editor>
		<meeting><address><addrLine>Alghero, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">September 11-14, 2017. 2017</date>
			<biblScope unit="volume">10614</biblScope>
			<biblScope unit="page" from="155" to="162" />
		</imprint>
	</monogr>
	<note>Proceedings, Part II</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin ; Luxburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017</title>
				<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><surname>Von</surname></persName>
		</editor>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">December 4-9, 2017. 2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations, ICLR 2018</title>
		<title level="s">Conference Track Proceedings. OpenReview.net</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-05-03">April 30 -May 3, 2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Comparison of descriptor spaces for chemical compound retrieval and classification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Wale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">A</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="347" to="375" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Videos as space-time region graphs</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018 -15th European Conference</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</editor>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">September 8-14. 2018. 2018</date>
			<biblScope unit="volume">11209</biblScope>
			<biblScope unit="page" from="413" to="431" />
		</imprint>
	</monogr>
	<note>Proceedings, Part V</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Heterogeneous graph attention network</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mantrach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Silvestri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Baeza-Yates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
				<meeting><address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019-05-13">2019. May 13-17, 2019. 2019</date>
			<biblScope unit="page" from="2022" to="2032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Deep graph kernels</title>
		<author>
			<persName><forename type="first">P</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1365" to="1374" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep sets</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Gaan: Gated attention networks for learning on large and spatiotemporal graphs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Fourth Conference on Uncertainty in Artificial Intelligence, UAI 2018</title>
				<editor>
			<persName><forename type="first">A</forename><surname>Globerson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Silva</surname></persName>
		</editor>
		<meeting>the Thirty-Fourth Conference on Uncertainty in Artificial Intelligence, UAI 2018<address><addrLine>Monterey, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2018">August 6-10, 2018. 2018</date>
			<biblScope unit="page" from="339" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">An end-to-end deep learning architecture for graph classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
