<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Inter-operating grids through Delegated MatchMaking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Alexandru</forename><surname>Iosup</surname></persName>
							<email>a.iosup@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Parallel and Distributed Systems Group</orgName>
								<orgName type="institution">Delft University of Technology</orgName>
								<address>
									<settlement>Delft</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Todd</forename><surname>Tannenbaum</surname></persName>
							<email>tannenba@cs.wisc.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">University of Wisconsin-Madison</orgName>
								<address>
									<settlement>Madison</settlement>
									<region>WI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Matthew</forename><surname>Farrellee</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">University of Wisconsin-Madison</orgName>
								<address>
									<settlement>Madison</settlement>
									<region>WI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dick</forename><surname>Epema</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Parallel and Distributed Systems Group</orgName>
								<orgName type="institution">Delft University of Technology</orgName>
								<address>
									<settlement>Delft</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Miron</forename><surname>Livny</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">University of Wisconsin-Madison</orgName>
								<address>
									<settlement>Madison</settlement>
									<region>WI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">Parallel and Distributed Sys-tems Group</orgName>
								<orgName type="institution">Delft University of Technology</orgName>
								<address>
									<addrLine>Mekelweg 4</addrLine>
									<postCode>2628 CD</postCode>
									<settlement>Delft</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Inter-operating grids through Delegated MatchMaking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">18FAF1C327B3EF36AA1E5307DB5014A7</idno>
					<idno type="DOI">10.3233/SPR-2008-0246</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T05:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Delegated MatchMaking</term>
					<term>grid inter-operation</term>
					<term>performance evaluation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The grid vision of a single computing utility has yet to materialize: while many grids with thousands of processors each exist, most work in isolation. An important obstacle for the effective and efficient inter-operation of grids is the problem of resource selection. In this paper we propose a solution to this problem that combines the hierarchical and decentralized approaches for interconnecting grids. In our solution, a hierarchy of grid sites is augmented with peer-to-peer connections between sites under the same administrative control. To operate this architecture, we employ the key concept of delegated matchmaking, which temporarily binds resources from remote sites to the local environment. With trace-based simulations we evaluate our solution under various infrastructural and load conditions, and we show that it outperforms other approaches to inter-operating grids. Specifically, we show that delegated matchmaking achieves up to 60% more goodput and completes 26% more jobs than its best alternative.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In the mid-1990s, the vision of the grid as a computing utility was formulated <ref type="bibr" target="#b16">[17]</ref>. Since then, hundreds of grids have been built -in different countries, for different sciences, and both for production work and for computer-science research -but most of these grids work in isolation. So, the next natural step is to have multiple grids inter-operate in order to serve much larger and more diverse communities of scientists and to put the ensemble of resources of these grids to better use. However, grid inter-operation raises serious challenges in the areas of, among others, resource management and performance. In this paper we address these two challenges with the design and evaluation of a delegated matchmaking protocol for resource selection and load balancing in inter-operating grids.</p><p>Our work was motivated by the ongoing efforts for making two multi-cluster grids, the DAS <ref type="bibr" target="#b12">[13]</ref> and Grid'5000 <ref type="bibr" target="#b6">[7]</ref>, inter-operate. Much like similar grid systems, e.g., CERN's LCG, their resources are in general under-utilized, yet in few occasions the demand exceeds the capacity of the individual systems. In such occasions, two (undesirable) alternatives are to queue the extra demand until it can be served, and to enlarge the individual systems. A third, and potentially more desirable option is to inter-operate grids, so that their collective demand will ideally incur a rather stable, medium-to-high utilization of the combined system.</p><p>The decision to inter-operate grids leads to nontrivial design choices with respect to resource selection and performance. If there is no common resource management system, jobs must be specifically submitted to one of the grids, which may lead to poor load balancing. If a central meta-scheduler is installed, it will quickly become a bottleneck leading to unnecessarily low system utilization, it will be a single point of failure leading to break-downs of the combined system, A. <ref type="bibr">Iosup</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>et al. / Inter-operating grids through Delegated MatchMaking</head><p>and it is unclear who will physically manage the centralized scheduler. Traditional decentralized solutions can also be impractical. Hierarchical mechanisms (still centralized, but arguably with less demand per hierarchy node) can be efficient and controllable, but still have single points of failure, and are administratively impractical (i.e., who administers the root of the hierarchy?). Completely decentralized systems can be scalable and fault-tolerant, but they can be much less efficient than their hierarchical alternatives. While many solutions have already been proposed <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref>, none has so far managed to achieve acceptance in the grid world, in part because they have yet to prove that they can yield significant benefits when managing typical grid workloads.</p><p>In this paper, we investigate a decentralized architecture for grid inter-operation that is based on two key ideas. First, we leverage a hierarchical architecture in which nodes represent computing sites, and in which we allow the nodes at the same hierarchical level and operating under the same authority (parent) to form a completely decentralized network. In this way, we attempt to combine the efficiency and the control of traditional hierarchical architectures with the scalability and the reliability of completely decentralized approaches. Second, we operate this architecture through delegated matchmaking in which requests for resources are delegated up-and-down the hierarchy, and within the completely decentralized networks. When resource request matches are found, the matched resources are delegated from the resource owner to the resource requester. By delegating resources to jobs instead of the traditional migration of jobs to resources, we lower the administrative overhead of managing user/group accounts on each site where they can use resources. Our architecture can be used as an addition to existing (local) resource managers.</p><p>We assess the performance of our architecture, and compare it against five architectural alternatives. Our experiments use a simulated system with 20 clusters and over 3000 resources. The workloads used throughout the experiments are either real long-term grid traces, or synthetic traces that reflect the properties of grid workloads. Our study shows that: 1. Our architecture achieves a good load balance for any system load, and in particular for high system loads. 2. Our architecture achieves a significant increase in goodput <ref type="bibr" target="#b4">[5]</ref> and a reduction of the average job wait time, when compared to centralized and decentralized approaches. Furthermore, when facing severe imbalance between the loads of the system's composing grids, our architecture achieves a much better performance than its alternatives, while keeping most of the traffic in the originating grids. 3. The overhead of our architecture, expressed in number of messages, remains low, even for high system loads.</p><p>The remainder of the paper is structured as follows. In Section 2 we formulate the scenario that motivates this work: inter-operating the DAS and Grid'5000 grids. In Section 3 we survey briefly the architectural and the operational spectra of meta-scheduling systems. We illustrate our survey with a selection of real systems. In Section 4 we introduce our architecture for inter-operating grids. We present the experimental setup used for this work in Section 5. We assess the performance of our architecture, and that of five architectural alternatives, in Section 6. Last but not least, in Section 7 we present our conclusions, and hint towards future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The motivating scenario: Inter-operating the DAS and Grid'5000</head><p>We consider as a motivating scenario the interoperation of two grid environments, the DAS <ref type="bibr" target="#b12">[13]</ref> and Grid'5000 <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">The dual-grid system: Structure and goals</head><p>The DAS environment (see Fig. <ref type="figure" target="#fig_0">1(a)</ref>) is a wide-area distributed system consisting of 400 processors located at five Dutch Universities (the cluster sizes range from 64 to 144). The users, a scientific community sized around 300, are associated with a home cluster, but a grid infrastructure grants DAS users access to any of the clusters. Each cluster is managed by an independent local cluster manager. The cluster owners may decide to partially or to completely take away the cluster resources, for limited periods of time. The DAS workload comprises a large variety of applications, from single-CPU jobs to parallel jobs that may span across clusters. Jobs can arrive directly at the local clusters managers, or to the KOALA meta-scheduler <ref type="bibr" target="#b30">[31]</ref>.</p><p>The Grid'5000 environment (see Fig. <ref type="figure" target="#fig_0">1(b)</ref>) is an experimental grid platform consisting of 9 sites, geographically distributed in France. Each site comprises one or several clusters, for a total of 15 clusters and over 2750 processors inside Grid'5000. The users, a community of over 600 scientists, are associated with a site, and have access to any of the Grid'5000 re- sources through a grid infrastructure. Each individual site is managed by an independent local cluster manager, the OAR <ref type="bibr" target="#b8">[9]</ref>, which has advance reservation capabilities. The other system characteristics, e.g., the cluster ownership and the workload, are similar to those of the DAS.</p><p>The combined environment that is formed by interoperating the DAS and Grid'5000 comprises 20 clusters, and over 3000 processors. The goal of this combined environment is to increase the performancereduce the job slowdown, even in a highly utilized system. The performance should be higher than that of the individual systems, taken separately. However, in achieving this goal we have to ensure that:</p><p>1. The load is kept local as much as possible, that is, jobs submitted in one grid should not burden the other if this can be avoided (the "keep the load local" policy). 2. The inter-connection should not require that each user, or even that each group, should have an account on each cluster they wish to use. 3. The clusters should continue running their existing resource management systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Load imbalance in grids</head><p>A fundamental premise of our delegated matchmaking architecture is that there exists load imbalance between different parts of the dual-grid system. We show in this section that this imbalance actually exists.</p><p>We want to assess the imbalance between the loads of individual clusters. To this end, we analyze two long-term and complete traces of the DAS and of the Grid'5000 systems, taken from the Grid Workloads Archive (GWA) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b23">24]</ref>: traces GWA-T-1 and GWA-T-2, respectively. The traces, sized respectively over 1,000,000 and over 750,000 jobs, contain for each job information about the cluster of arrival, the arrival time, the duration, the number of processors, etc.</p><p>We define the normalized daily load of a cluster as the number of job arrivals over a day divided by the number of processors in the cluster during that period. We define the hourly load of a cluster as the number of job arrivals during hourly intervals. We distinguish two types of imbalance between the cluster loads, overall and temporary. We define the overall imbalance between two clusters over a period of time as the ratio between their normalized daily loads cumulated until the end of the period. We define the temporary imbalance between two clusters over a period of time as the maximum value of the ratio between the hourly loads of the two clusters, computed for each hour in the time period. The overall imbalance characterizes the load imbalance over a large period of time, while accounting for the differences in cluster size. The temporary imbalance characterizes the load imbalance over relatively short periods of time, regardless of the cluster sizes. The imbalance metrics proposed here characterize well the imbalance of a multi-cluster system when the collection of random variables describing the sizes of the arriving jobs for each cluster are independent and identically distributed. For many grids, the average job size is one or very close to one (see Section 5.2 and <ref type="bibr" target="#b23">[24]</ref>); thus, the proposed imbalance metrics characterize well the imbalance of grid clusters.  cluster 5. The maximum temporary load imbalance between the clusters of the DAS system is over 1000:1.</p><p>We have obtained similar results for the Grid'5000 traces, as shown by Fig. <ref type="figure" target="#fig_1">2(b</ref>) and (d). We conclude that there exists a great potential to reduce the delays through load balancing across DAS and Grid'5000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">A brief review of metascheduling systems</head><p>In this section we review several meta-scheduling systems, from an architectural and from an operational point of view, and for each we give a concise description and a reference to a real system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Architectural spectrum</head><p>We consider a multi-cluster grid. Below we briefly present our taxonomy of architectures that can be used as grid resource management systems (GRMS). We illustrate this taxonomy in Fig. <ref type="figure" target="#fig_3">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Independent clusters: (included for completeness)</head><p>each cluster has its local resource management system (LRMS), i.e., there is no meta-scheduler.</p><p>Users have accounts on each of the clusters they want to submit jobs to. For each job, users are faced with the task of selecting the destination cluster, typically a cumbersome and error-prone process. Centralized meta-scheduler: there exists one (central) system queue, where all grid jobs arrive.</p><p>From the central queue, jobs are routed towards the clusters where they are dispatched. The clusters may optionally employ an LRMS, in which case jobs may also arrive locally. It may be pos- sible for the manager of the central queue to migrate load from one LRMS to another. Hierarchical K-level meta-scheduler: there exists a hierarchy of schedulers. Typically, grid jobs arrive either at the root of the hierarchy, or at the clusters' LRMSs. In both cases, jobs are routed (migrated) towards the clusters' LRMSs. The Hierarchical 2-level metascheduler is the most encountered variant <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11]</ref>. Distributed meta-scheduler: similarly to the independent clusters architecture, each cluster has its LRMS, and jobs arrive at the individual clusters' LRMSs. In addition, cluster schedulers can share jobs between each other. This forms in effect a distributed meta-scheduler. We distinguish between two ways of establishing links between clusters (sharing): static (fixed by administrator, e.g., at system start-up), and dynamic (automatically selected). We also call a distributed metascheduler architecture with static link establishment a federated clusters architecture. Hybrid distributed/hierarchical meta-scheduler:</p><p>each grid site, which may contain one or several clusters, is managed by a hierarchical K-level meta-scheduler. In addition, the root metaschedulers can share the load between each other. Other load-sharing links can also be established.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Operational spectrum</head><p>We define an operational model as the mechanism that ensures that jobs entering the system arrive at the place where they can be run. We identify below three operational models employed by today's resource management systems.</p><p>Job routing: jobs are routed by the schedulers from the arrival point to the resources where they can run through a push operation (scheduler-initiated routing). Job pulling: jobs are acquired by (unoccupied) resources from a higher-level scheduler through a pull operation (resource-initiated routing).</p><p>Matchmaking: jobs and resources are connected to each other by the resource manager, which thus acts as a broker responding to requests from both sides (job-and resource-initiated routing).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Real systems</head><p>There exist several resource management systems that can operate a multi-cluster grid. Below we present a selection, which we summarize in Table <ref type="table">1</ref>, including references.</p><p>The Globus GRAM is a well-known middleware for managing independent clusters environments. It is operated through job routing. Globus GRAM is used in research and in industrial grids.</p><p>The Condor-G, Alien, JoSH, Koala, and OARGrid implement centralized architectures. Condor-G operates on top of Globus grids. Alien is used in (a part of) CERN's production grid. JoSH operates on top of the Sun Grid Engine. Koala and OARGrid are used in research grids. Koala can operate on top of grids that use Globus or DRMAA-enabled resource managers <ref type="bibr" target="#b38">[39]</ref>, e.g., PBS/Torque, the Sun Grid Engine, GridWay, Condor, etc. OARGrid operates on top of the OAR cluster manager. Starting with version 2 (released late 2007-early 2008), OAR also supports resource aggregation, which can be used to build hierarchies of resources. Koala and OAR are some of the first meta-Table <ref type="table">1</ref> Currently deployed meta-scheduling systems. This work proposes a hybrid distributed/hierarchical architecture, operated through matchmaking</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head><p>Architecture Operation</p><p>Condor <ref type="bibr" target="#b37">[38]</ref> Independent Matchmaking</p><p>Globus GRAM <ref type="bibr" target="#b11">[12]</ref> Independent Job routing</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Condor-G [19]</head><p>Centralized Job routing Alien <ref type="bibr" target="#b32">[33]</ref> Centralized Job pull JoSH <ref type="bibr" target="#b9">[10]</ref> Centralized Job routing Koala <ref type="bibr" target="#b30">[31]</ref> Centralized Job routing OAR(Grid) <ref type="bibr" target="#b8">[9]</ref> Centralized Job routing OAR(Grid) v. 2 Hierarchical Job routing CCS <ref type="bibr" target="#b7">[8]</ref> Hierarchical 2-level Job routing Moab/Torque <ref type="bibr" target="#b10">[11]</ref> Hierarchical 2/3-level Job routing</p><p>NorduGrid ARC <ref type="bibr" target="#b13">[14]</ref> Indep./Federated Job routing NWIRE <ref type="bibr" target="#b33">[34]</ref> Federated Job routing</p><p>GridWay <ref type="bibr" target="#b20">[21]</ref> Federated Job routing Condor flocking <ref type="bibr" target="#b14">[15]</ref> Federated Matchmaking OurGrid <ref type="bibr" target="#b3">[4]</ref> Distributed, dynamic Job routing Askalon <ref type="bibr" target="#b35">[36]</ref> Distributed, dynamic Job routing schedulers which can co-allocate jobs, that is, they can simultaneously allocate resources located in different clusters for the same job. Koala supports both besteffort and reservation-based co-allocation. OAR supports reservation-based co-allocation. CCS and Moab/Torque are both hierarchical metaschedulers. CCS is one of the first hierarchical metaschedulers that can operate clusters and super-computers together; it was used mainly in research environments. The commercial package Moab/Torque is currently one of the most used resource management systems.</p><p>The NorduGrid ARC implements an independent clusters architecture operated through job routing. However, the job submission process contacts cluster information systems from a fixed list, and routes jobs to the site where they could be started the fastest. This effectively makes NorduGrid a federated clusters architecture.</p><p>NWIRE, OurGrid and Askalon are all distributed clusters architectures operated through job routing. NWIRE and OurGrid implement a federated clusters architecture. NWIRE is the first such architecture to explore economic, negotiation-based interaction between clusters. OurGrid is the first to use a "tit-for-tat" job migration protocol, in which a destination site prioritizes migrated load by the number of jobs that it has migrated in the reverse direction in the past. Finally, Askalon is the first to build a negotiation-based distributed clusters architecture with dynamic link establishment.</p><p>Condor is a cluster management system. As such, it can straightforwardly be used in an independent clusters environment. However, through its flocking mechanism, Condor can be used in a federated clusters environment. Unlike other cluster management systems, which define few job submission queues (e.g., one up to five, as shown by the setup of the traditional cluster systems with traces published in the Parallel Workloads Archive (PWA) <ref type="bibr" target="#b1">[2]</ref>), the typical Condor deployment uses one queue per user. Condor is widely used in research and production clusters.</p><p>GridWay operates on top of grids that use Globus or DRMAA-enabled resource managers. GridWay adopts Condor's one queue per user strategy; as such, it effectively implements a federated architecture with job routing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">The delegated matchmaking architecture</head><p>In this section we present our resource management architecture for inter-operating multi-cluster grids: the delegated matchmaking architecture (DMM). We first build a hybrid distributed/hierarchical meta-scheduler architecture (see Section 3.1). Then, we operate it using (delegated) matchmaking (see Section 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Overview</head><p>We now define how grid clusters and other administrative units, from hereon sites, are connected. We aim to create a network of sites that manage the available resources, on top of and independently of the local cluster resource managers. First, sites are added according to administrative and political agreements, and parent-child (hierarchical) links are established. Thus, a hierarchy of sites is formed, in which the individual grid clusters are leaves of the hierarchical tree. Then, supplementary to the hierarchical links, sibling links can be formed between sites at the same hierarchical level and operating under the same authority (parent site).</p><p>Each site administers directly its local resources and the workloads of its local users. However, a site may have no local resources, or no local users, or both. A site with no local resources can be installed for a research laboratory with no local computing resources. A site without local users can be installed for an ondemand computing center. A site without users or resources serves only administrative purposes.</p><p>For our motivating scenario, described in Section 2, we create the hierarchical links between the sites as in Fig. <ref type="figure" target="#fig_0">1</ref>. Additionally, the sites 0-4, 5-8, 11-12, 13-14, 15-16, 22-30 and 20-21 are also inter-connected with sibling links, respectively. Sites 20-30 have been installed for administrative purposes. To avoid ownership and maintenance problems, there is in fact no root of the hierarchical tree. Instead, sites 20 and 21 serve as roots for each of the two grids, and are connected through a sibling link.</p><p>We operate our architecture through (delegated) matchmaking. The main idea of our delegated matchmaking mechanism is to delegate resources' ownership to the user that requested them through a chain of sites (and of resource leases), and by adding the resource transparently for the user to the local user's site. Binding the resource to the local user's site stands in contrast to the typical practice in today's systems based on either job routing or job pull, where jobs are sent to (or acquired from) the remote resources, where they are executed. This major change can be beneficial in practice: the resources are added to the trusted pool of resources of a neighboring site (simplifies security issues), and current systems already provide adequate mechanisms (e.g., the Condor glide-in <ref type="bibr" target="#b37">[38]</ref>) that allow resources to be dynamically and temporarily added to a site without the need of root access to the resource (simplifies technical issues). On the contrary, when delegating jobs to resources, the resource management system needs to understand the job's semantics, and in particular the file dependencies, the job's structure, and the job-to-resource mapping strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Local operation</head><p>We assume that each of the grid's clusters uses a Condor-like resource management system. This assumption allows us to consider in our architecture only the mechanisms by which the clusters are interoperated, while benefiting from the local resource management features of Condor <ref type="bibr" target="#b37">[38]</ref>: complete administrative control over owned resources (resources can reject jobs), high tolerance to resource failures, the ability to dynamically add/remove computing resources (through matchmaking and glide-in). This also ensures that the administrators of the grid clusters will understand easily our architecture as it uses concepts from the Condor world, such as matchmaking.</p><p>Similarly to Condor, in our architecture each cluster is managed by a site manager (SM), which is responsible for gathering information about the local resources and jobs, informing resources about their match with a job and vice-versa, and maintaining the resource leasing information. According to this definition, our SM is equivalent to Condor's Negotiator, Collector, and Accountant components combined. Each resource is managed by a resource manager (RM), which will mainly be occupied with starting and running user's jobs. Each user has (at least) one permanent job manager (JM), which acts as an application-centric scheduler <ref type="bibr" target="#b5">[6]</ref> that obtains resources from the local SM. Our RM and JM definitions correspond to those of Condor's Start and Sched daemons, respectively. In addition to the Condor-specific functions, in our architecture a site manager is also responsible for communicating with other site managers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">The delegated matchmaking mechanism</head><p>We now present the operational mechanism of our architecture, the delegated matchmaking mechanism, for obtaining remote resources. Job manager JM-1 informs its SM about the need for resources, by sending a resource request (step 1 in Fig. <ref type="figure" target="#fig_4">4</ref>). The resource request includes the type of and number of resources JM-1 requires. At its next delegation cycle, site manage SM-1 establishes that it cannot serve locally this request, and decides to delegate it. SM-1 selects then contacts SM-2 for this delegation (step 2). To make the selection, SM-1 uses its target site ordering policy (see Section 4.4). During its next matchmaking cycle, SM-2 finds enough free resources, and delegates them to SM-1 through its local resource management protocol (step 3). Then, SM-1 claims the resources, and adds them to its local environment (step 4). At this point, a delegation chain has been created, with SM-2 being the delegation source and SM-1 the delegation sink. During its next delegation cycle, SM-1 handles JM-1's request using its own resource management protocol (step 5). Upon receiving the resources, JM-1 starts the user's job(s) on RM-1 (step 6). Finally, after the user job(s) have finished, JM-1 releases the resource, by informing both RM-1 and SM-1 (step 7). The resource release information is transmitted backwards along the delegation chain: SM-1 informs SM-2 of the resource release. If SM-2's local resource management is Condor-based, RM-1 will also inform SM-2 of its release.</p><p>During steps 1-7, several parties (e.g., JM-1, SM-1, SM-2 and RM-1) are involved in a string of negotiations. The main potential failures occurring in these multi-party negotiations are addressed as follows. First, an SM may not find suitable resources, both locally or through delegation. In this case, the SM sends back to the delegation requester (another SM) a DelegationReject message. Upon receiving a Delega-tionReject message, an SM will attempt to select and contact another SM for delegation (restart from step 2 in Fig. <ref type="figure" target="#fig_4">4</ref>). Second, to prevent routing loops, and for efficiency reasons, the delegation chains are limited to a maximum length, which we call the delegation time-to-live (DTTL). Before delegating a resource request, the SM decreases its DTTL by 1. A resource request with a DTTL equal to 0 cannot be delegated. To ensure that routing loops do not occur, SMs retain a list of resource requests they have seen during the past hour. Third, we account for the case when the user changes his intentions, and cancels the resource request. In this case, JM-1 is still accountable for the time during which the resource management was delegated from SM-2 to SM-1, and charges the user for this time. To prevent being charged, the user can select a reduced DTTL, or even a DTTL of 0. However, requests with a DTTL of 0 will possibly wait more for available resources.</p><p>Delegated matchmaking promises to significantly improve the performance of the system, by occupying otherwise unused free resources with waiting jobs (through load sharing or load balancing, depending on the system policy configuration discussed later in Section 4.4). However, it can also worsen the performance of the system, by poor resource selection and by poor delegation routing. The resource selection is mostly influenced by the load management algorithm (discussed in Section 4.4). Figure <ref type="figure" target="#fig_5">5</ref> shows a worst-case performance scenario for delegation routing. Delegation requests in the figure are ordered in time in their lexicographical order (i.e., delegation A occurs before delegation B). Site 9 issues a delegation request to site 23. The site schedulers base their decision only on local information. Due to the lack of information, sites are unable to find the appropriate candidate (here, site 4), and unnecessary delegation requests occur. This leads in turn to messaging overheads, and to increased waiting times, due to waiting for the delegation and matchmaking cycles of the target sites. Additionally, the decision to delegate resource requests can lead to a suboptimal number of delegations, either too few or too many. All these load management decisions influence decisively the way the delegated matchmaking mechanism is used. We dedicate therefore the next section to load management.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">The delegated matchmaking policies</head><p>To manage the load, we use two independent algorithms: the delegation algorithm, and the local requests dispatching algorithm. We describe them and their associated policies below.</p><p>The delegation algorithm selects what part of the load to delegate, and the site manager from which the resources necessary to serve the load can be obtained. This algorithm is executed whenever the current system load is over an administrator-specified delegation threshold, and at fixed intervals of time. First, requests are ordered according to a customizable delegation policy, e.g., FCFS. Then, the algorithm tries to delegate all the requests, in order, until the local load gets below the threshold that triggered the delegation alarm. The algorithm has to select for each request a possible target from which to bring resources locally. By design, the potential targets must be selected from the site's neighborhood. The neighbors are ordered according to a customizable target site ordering policy, which may take into account information about the current status of the target (e.g., its number of free resources), and an administrator selection of the request-to-target fitting (e.g., Best Fit). Upon finding the best target, the delegation protocol is initiated.</p><p>The local requests dispatching policy deals with the ordering of resource requests, both local and delegated. Similarly to the Condor's matchmaking cycle, we call this algorithm periodically, at intervals normally longer than those of the delegation algorithm cycle. The administrator may select the local request dispatching policy.</p><p>We argue that our architecture is operated with a generic load management mechanism. The three policies defined above allow for many traditional scheduling algorithms, and in particular gives our architecture the ability to leverage existing well-established on-line approximation algorithms <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b34">35]</ref>. The target site ordering policy enables the use in our architecture of many of the results in traditional networking/queuing theory <ref type="bibr" target="#b26">[27]</ref>. However, we consider policy exploration and tuning outside the scope of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">The experimental setup</head><p>In this section we present the experimental setup: a simulated environment encompassing both the DAS and Grid'5000 grids, for a total of 20 sites and over 3300 processors. We first present an overview of the simulator. Then, we describe the typical grid workloads, which are significantly different from the workloads of traditional parallel production environments <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b29">30]</ref>. Specifically, a grid workload comprises a high number of single-processor jobs, which are sent to the grid in batches (Section 5.2). We then present the workloads, the simulated architectures, and the assumptions that are part of the experimental setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">The simulator</head><p>We have developed a custom discrete event simulator to simulate the combined DAS-2 and Grid'5000 grid system (see <ref type="bibr">Section 2)</ref>. Each of the 20 clusters of the combined system receives an independent stream of jobs. Depending on each job's parallelism, one or several resources are assigned to it exclusively, from the time when the job starts until the time when the job finishes.</p><p>We attempt to evaluate the steady-state of the simulated system. To this end, unless otherwise specified the last simulated event in each simulation is the arrival of the last job, all job streams considered together. This ensures that our simulation does not include the cooldown phase of the system, in which no more jobs arrive while the system finishes the remaining load. The inclusion of the cool-down phase may bias the performance metrics, especially if the last jobs queue at only few of the clusters. We do not perform a similar elimination for system warm-up, as (a) we cannot distinguish reliably between the warm-up period and the normal system behavior, and (b) given the long duration of the jobs (see the workloads description in Section 5.3), the start-up period is small compared to the remainder of the simulation, especially for high load levels.</p><p>The simulator assesses the following performance metrics:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Utilization, wait and response time, slowdown:</head><p>We consider in this work the average values of the system utilization (U), average job wait time (AWT), average job response time, and average job slowdown (ASD). For a review of these traditional performance metrics, we refer to <ref type="bibr" target="#b15">[16]</ref>. Goodput, expressed as the total processing time of jobs that finish successfully, from the point of view of the grid resource manager (similar to the traditional definition <ref type="bibr" target="#b4">[5]</ref>, but taking into consideration that all grid jobs are executed "remotely" from the user's perspective). For the DMM architecture, we also measure the goodput of jobs running on resources obtained through delegated matchmaking. Furthermore, we account for goodput obtained on resources delegated from the same site (intra-site goodput), from the same grid (intra-grid goodput), and between the grids (inter-grid goodput). Finished jobs (JF%), expressed as the percentage of jobs that finish, from the jobs in the trace. Due to the cool-down period elimination, the maximum value for this metric is lower than 100%. Overhead: We consider the overhead of an architecture as the number of messages it employs to manage the workload. There are five types of messages: Notify Broker, Negotiate, Job Data Exchange, Resource Match-Claim-Release, and DMM (the last specific to our architecture). The Overhead is then expressed as a set of five values, one for the number of messages of each type. Additionally, we consider for our architecture the number of delegations of a job, which is defined as the length of its delegation chain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Intermezzo: Typical grid workloads</head><p>Two main factors contributing to the reduction of the performance of large-scaled shared systems, such as grids, are the overhead imposed by the system architecture (i.e., the messages, the mechanisms for ensuring access to resources etc.), and the queuing effects due to the random nature of the demand. While the former is under the system designer's control, the latter is dependent on the workload. Despite a strong dependency of performance on the system workload, most of the research in grid resource management does not employ realistic workloads (i.e., trace-based, or based on a validated workload model with realistic parameter values). For the few reported research results that attempt to use realistic workloads <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b36">37]</ref>, the traces considered have been taken from or modelled after the Parallel Workloads Archive. However, there exist significant differences between the parallel supercomputers workloads in the PWA and the workloads of real grid environments. In this section we present two important distinctions between them.</p><p>First, the percentage of "serial" (single-processor) jobs is much higher in grid traces than in the PWA traces. There exist 70-100% single-processor jobs in grid traces (the percentage grows to 99-100% in most production grid environments), but only 20-30% in the PWA traces <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b29">30]</ref>. There are two potential consequences: on the one hand, the resource managers be- come much more loaded, due to the higher number of jobs. On the other hand, the resource managers can be much simpler, since individual single-processor jobs raise fewer scheduling issues.</p><p>Second, the grid single-processor jobs typically represent instances of conveniently parallel jobs, or batch submissions. A batch submission is a set of jobs ordered by the time when they arrive in the system, where each job was submitted at most Δ seconds after the first job (Δ = 120 s is considered the most significant). In a recent study, Iosup et al. <ref type="bibr" target="#b22">[23]</ref> show that 70% of the jobs, accounting for 80% of the consumed processor time, are part of batch submissions. The batch submissions are usually managed by batch engines, and the individual jobs arrive in the system independently. Figure <ref type="figure" target="#fig_6">6</ref> shows that the runtime of jobs belonging to the same batch submission varies on average by at least two orders of magnitude, and that the variability increases towards five orders of magnitude as the size of the batch reaches 500 or more jobs. The predominance of submissions and their jobs' high runtime variability have a high impact on the operation of a large number of today's cluster and grid schedulers. Indeed, the user must submit many jobs as a batch submission with a single runtime estimate. Hence, the user cannot estimate the runtime of individual jobs, other than specifying a large value, typically the largest value allowed by the system. As a result, the scheduling schemes relying on user estimates, e.g., all backfilling variants <ref type="bibr" target="#b40">[41]</ref>, are severely affected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">The workloads</head><p>The workloads used in our experiments are either traces collected from the individual grids starting at identical moments in time, or synthetic traces that re-flect the properties of grid workloads. We use two alternatives to obtain the workloads: we either use data from real grid traces or generate synthetic workloads based on the properties of real grid traces. To the best of our knowledge, ours is the first study that takes into account the difference between the parallel supercomputers workloads (comprising mostly parallel jobs), and the workloads in real grid environments (comprising almost exclusively single-node jobs).</p><p>To validate our approach (Section 6.1), we use traces collected for each of the simulated clusters, starting at identical moments in time (the grids from which the traces originate are described in Section 2). However, these traces raise two problems. First, they incur a load below 20% of the combined system <ref type="bibr" target="#b21">[22]</ref>. Second, they represent the workload of research grid environments, which contains many more parallel jobs than in a production grid.</p><p>To address both these issues, we employ a modelbased trace generation for the rest of our experiments. We use the Lublin and Feitelson model (LFM) <ref type="bibr" target="#b29">[30]</ref>, which has deservedly become the de-facto standard for the community that focuses on resource management in large-scale computing environments. Using this model, we generate streams of rigid jobs (that is, whose size is fixed at the job's arrival in the system) for each cluster. Unless otherwise specified, we use the default LFM parameter values. The job arrival times during the peak hours of the day are modelled in LFM using a Gamma distribution. To generate jobs for a longer period of time, the LFM uses daily cycle with a sinusoidal shape, where the highest value of the curve corresponds to the peak hours. The job parallelism is modelled for three classes: single-processor jobs, parallel jobs with a power-of-two number of nodes, and other parallel jobs. We change the default value of the prob-ability of a new job to be single-processor, p, to reflect the values encountered in grid systems: p = 0.95 <ref type="bibr" target="#b21">[22]</ref>. The LFM divides the remaining jobs between the parallel jobs classes, with equal probability. The actual runtime time of a job is modelled with a hyper-Gamma distribution with two stages; for parallel jobs, the parameter that represents the probability of selecting the first hyper-Gamma stage over the second depends linearly on the number of nodes. Thus, the largest jobs have a high probability of also having a long runtime. With these parameters, the average job runtime is around one hour.</p><p>By modifying the parameters of the Lublin-Feitelson model that characterize the inter-arrival time between consecutive jobs during peak hours, we are able to generate a load of a given level (e.g., 70%), for a system of known size (e.g., 128 processors), during a specified period (e.g., 1 month). Using this approach, we generate 10 sets of 20 synthetic job streams (one per simulated cluster) for each of the following load levels: 10, 30, 50-100% in increments of 10, 95, 98, 120, 150 and 200%. We call the default load levels the following nine load levels: 10, 30, 50, 60, 70, 80, 90, 95 and 98%. The results reported in Sections 6.2, 6.3 and 6.4 are for workloads with a duration of 1 day, for a total of 953-39,550 jobs per set (11827 jobs per set, on average). We have repeated some of the experiments in Sections 6.2 and 6.4 for traces with the duration of 1 week and 1 month, with similar results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">The simulated architectures</head><p>For the simulation of the DMM architecture, unless otherwise noted, we use a delegation threshold of 1.0 and a matchmaking cycle of 300 s. Throughout the experiments, we employ a FCFS delegation policy, a target site ordering policy that considers only the directly connected neighbors of a site, and a FCFS local requests dispatching policy. We simulate five alternative architecture models, described below and summarized in Table <ref type="table" target="#tab_0">2</ref>. 1. cern: This is a centralized meta-scheduler architecture with job pull operation, in which users submit all their jobs to a central queue. Whenever they have free resources, sites pull jobs from the central queue. Jobs are pulled in the order they arrive in the system. 2. condor: This is an independent clusters architecture with matchmaking operation that simulates a Condor-like architecture. Unlike the real system, the emulation does not prioritize users through a fair-sharing mechanism <ref type="bibr" target="#b37">[38]</ref>. Instead, at each matchmaking round jobs are considered in the order of arrival in the system. The matchmaking cycle occurs every 300 s, the default value for Condor (see NEGOTIATOR_ INTERVAL in the Condor manual). 3. fcondor: This is a federated clusters architecture with matchmaking operation that simulates a Condor-like architecture with flocking capabilities <ref type="bibr" target="#b14">[15]</ref>. The user's job manager will switch to a new site manager whenever the current site manager cannot solve all of its resource demands. This simulation model also includes the concept of fair-sharing employed by Condor in practice <ref type="bibr" target="#b37">[38]</ref>. At each matchmaking round, users are sorted by their past usage, which is reduced (decayed) with time. Then, users are served in order, and for each user all feasible demands are solved. Similarly to condor, the matchmaking cycle occurs every 300 s. The performance of the fcondor simulator corresponds to an optimistic performance estimation of a real Condor system with flocking, for two reasons. First, in the fcondor simulator, we allow any job manager to connect to any site manager. This potentially reduces the average job wait time, especially when the grids receive imbalanced load, as JMs can use SMs otherwise unavailable. Second, jobs that cannot be temporarily served are bypassed by jobs that can. Given that 95% of the jobs are single-processor, this results in sequential jobs being executed before parallel jobs, when the system is highly loaded. Then, the resource fragmentation and the average wait time decrease, and the utilization increases. 4. central: This is a centralized grid meta-scheduler architecture with job push operation. Users submit all their jobs to a central queue. As soon as jobs arrive, the queue dispatches them on sites with free resources. Jobs stay in the queue until free resources are found. The information about the number of free resources is gathered periodically by a monitoring service. 5. sep-c: This is an independent clusters architecture with job push operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">The assumptions</head><p>In our simulations we make the following assumptions.</p><p>Assumption 1 (No network overhead). We assume a perfect communication network between the simulated systems, with 0-latency. Given the average job runtime of one hour, we argue that this assumption has little effect. However, we do present the number of messages used by our architecture to manage the workload.</p><p>Assumption 2 (Identical processors). To isolate the effects of the resource management solutions, we assume identical processors across all clusters. However, the system is heterogeneous in number of processors per cluster. Assumption 3 (FCFS scheduling policy at cluster level). We assume that each site employs a FCFS policy, without backfilling. Backfilling systems are effective when many parallel jobs exist in the system, and when accurate job runtime predictions are given by the users. This situation is uncommon in grids, and also in many large-scale parallel computing environments <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b39">40]</ref>.</p><p>Assumption 4 (Processors as scheduling unit). In Condor, multi-processor machines can be viewed (and used) as several single-processor machines. We assume that this feature is available regardless of the modelled alternative architectures. Note that this increases the performance of the cern, sep-c and central architectures, and has no effect on the Condor-based condor, fcondor and DMM.</p><p>Assumption 5 (No background load). In many grids, jobs may arrive directly at the local clusters' resource manager, i.e., bypassing the grid. However, there is little information on the load imposed by this additional workload in practice. Therefore, we assume that there exists no background load in the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">The experimental results</head><p>In this section we present the experimental evaluation of our architecture for inter-operating grids. Our experiments are aimed at characterizing:</p><p>• The behavior of the DMM architecture and of its alternatives, for a duration of one year (Section 6.1); • The performance of the DMM architecture, and of its alternatives, under various load levels (Section 6.2); • The effects of an imbalance between the loads of different grids on the performance of the DMM architecture and of its alternatives (Section 6.3); • The influence of the DMM's delegation threshold parameter on the performance of the system (Section 6.4); • The overhead of the DMM messaging (Section 6.5); • The influence of the resource binding duration on the performance of the DMM-operated systems (Section 6.6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Preliminary real trace-based evaluation</head><p>For this experiment, we first assess the behavior of the DMM architecture and of its alternatives when managing a contiguous subset of 4 months from the real grid traces described in Section 2, starting from 01/11/2005. Only for this experiment, we do not stop the simulation upon the arrival of the last job, and we do include the cool-down period. However, no jobs arrive after the 4-months limit.</p><p>We define the workload management messages as the NotifyBroker, the Negotiate, the Job Data Exchange and the Resource Match-Claim-Release messages used by the delegated matchmaking mechanism (see Section 4.3). We further define the resource use messages as the messages used to maintain and control the resource during the execution of the job, e.g., the KeepAlive messages exchanged between the JM and the RM and in all Condor-based systems while jobs are being run by the RM, to ensure that the RM (JM) are continuing their loan (use) agreement.</p><p>Figure <ref type="figure" target="#fig_7">7</ref> shows the system behavior over a selected period of two days. During this period, all architectures are capable of starting all the incoming jobs. However, the DMM and the fcondor reduce the most the jobs' waiting time (their curves follow closely that of the job arrivals, and are only slightly delayed). In this figure, the independent clusters architecture condor introduces big delays. This can be explained by the bottom row of Fig. <ref type="figure" target="#fig_7">7</ref>, depicting the number of messages used by the DMM to manage the workload. The DMM messages, used to delegate work, appear mostly when the condor introduces large delays: around hours 10-13 and 33-39 (since the start of the two days period). It is  in these periods that the number of arriving jobs rises at some cluster, and delegation or a similar mechanism is needed to alleviate the problem. Note that the number of jobs needs not be very high for a delegation to become desirable: around hour 33 and 39 (since start) a small cluster becomes suffocated, while much larger ones are free. Table <ref type="table" target="#tab_1">3</ref> shows the performance results for running the real traces over the whole 4-months period. All architectures successfully manage all the load. However, the ASD and the AWT vary greatly across architectures. The cern has the smallest ASD and AWT, by a large margin. This is because, unlike the alternatives, it is centralized, and operates in a lightly loaded system, where little guidance is needed, but its speed is critical for good performance. The fcondor and the DMM have similar performance, and are both better than the independent clusters architectures (condor and sep-c). For the latter the job response time is dominated by the time spent waiting for resources. Independently of whether we use AWT or ASD, the condor has a poorer performance than sep-c: the time spent waiting for the next matchmaking cycle affects negatively the performance of condor.</p><p>We have repeated the experiments for a one-year sample with the same starting point, 01/11/2005. We have obtained similar results, with the notable exception of fcondor, whose AWT degraded significantly (it became the worst of all architectures!). We attribute this poor performance to the flocking target selection: if the target SM is also very loaded, fcondor wastes significant time, since the JM will have to wait for the target SM's next matchmaking cycle to discover that the latter cannot fulfill any demands. This gives further reasons for the development of a dynamic target selection mechanism such as the DMM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">The influence of the load level</head><p>In this section we assess the performance of the DMM architecture and of its alternatives under various load levels. We report the results for the default load levels (defined in Section 5.3). Figure <ref type="figure">8</ref> shows the perfor-Fig. <ref type="figure">8</ref>. The performance of the DMM architecture compared with that of alternative architectures, for various system loads. mance of the DMM architecture and of its alternatives, under the default load levels. Starting with a medium system utilization (50%) and up, DMM offers better goodput than the alternatives. The largest difference is achieved for a load of 80%. At this load, DMM offers 32% more goodput than the fcondor. We attribute this difference to DMM's better target site selection policy, and quicker rejection of delegations by loaded sites that are incorrectly targeted. The centralized metaschedulers, cern and central, offer in general lower goodput values with the notable exception of cern's 24% improvement over the best architecture (DMM and fcondor, tied) for a load level of 30%.</p><p>The AWT and the ASD of DMM remain similar to that of central meta-scheduler architectures, regardless of the load level. However, DMM incurs lower AWT and ASD than fcondor at loads over 70%, and much better JF% than cern and central. DMM and fcondor manage to finish many more jobs than their alternatives, in the same time: up to 93% more jobs finished, for load levels below 60%, and up to 378% more jobs finished, for loads up to 98%. We explain these large differences to the scheduling mechanism. The centralized architectures (cern and central) operated with FCFS may keep many jobs blocked in the queue when the oldest job cannot find its needed resources. The decentralized architectures (DMM and fcondor) act as natural backfilling algorithms, that is, they delegate the blocked jobs and dispatch the others. The large delegated jobs will not starve, due to the DTTL (see Section 4.3). Finally, there is a difference of 0-4% in JF% between DMM and fcondor, in favor of DMM.</p><p>The independent cluster architectures, sep-c and condor, are outperformed by the other architectures for all load levels and for all performance metrics.</p><p>The additional performance comes at a cost: the additional messages sent by the DMM architecture for its delegations. Figure <ref type="figure">8</ref> also shows the number of delegations per job. Surprisingly, this overhead is relatively constant for all loads below 90%. This suggests that at medium to high load levels, the DMM manages to find suitable delegation targets in linear time, while using a completely decentralized routing algorithm. Above 80% load, the system is overloaded, and DMM struggles to find good delegation targets, which increases the number of delegations per job linearly with the load level increase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">The influence of the inter-grids load imbalance</head><p>In this section we present the effects of an imbalance between the loads of the two grids on the performance of the DMM architecture and of its alternatives. We simulate an imbalance between the load of the DAS, which we keep fixed at 60%, and that of Grid'5000, which we vary from 60 to 200%. Note that at load levels higher than 120% for Grid'5000, the two-grid system is overloaded.</p><p>Figure <ref type="figure">9</ref> shows the performance of the DMM for various imbalanced system loads. The figure uses a logarithmic scale for the average wait time and for the average slowdown. At 60/100% load, the system starts to be overloaded, and all architectures but the DMM "suffocate", i.e., they are unable to start all jobs. At 60/150%, when the system is truly saturated, only DMM can finish more than 80% of the load (it finishes over 95%). The DMM architecture is superior to its alternatives both in goodput and in percentage of finished jobs. Compared to its best alternative, fcondor, DMM achieves up to 60% more goodput, and finishes up to 26% more jobs. The cern architecture achieves lower ASD by not starting most of its incoming workload! Similarly to the case of balanced load, the number of delegations per job is relatively constant for imbalanced loads of up to 60/100%. Afterwards, the number of delegations per job increases linearly with the load level increase, but at a higher rate than for the balanced load case.</p><p>To better understand the cause for the performance of the DMM, we show in Fig. <ref type="figure" target="#fig_0">10</ref> the breakdown of the goodput components for various imbalanced system loads. According to the "keep the load local" policy (defined in Section 2.1), the goodput on resources delegated between sites is low for per-grid loads below 100%. However, as soon as the Grid'5000 grid is overloaded, the inter-grid delegations become frequent, and the inter-grid goodput rises, to up to 37% from the goodput obtained on delegated resources. A similar effect can be observed for the intra-grid goodput, and for the intra-site goodput.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">The influence of the delegation threshold</head><p>The moments when the DMM architecture issues delegations and the number of requested resources depend highly on the delegation threshold. We therefore assess in this section the influence of the delegation threshold on the performance of the system.</p><p>Figure <ref type="figure" target="#fig_8">11</ref> shows the performance of the DMM architecture for values of the delegation threshold ranging from 0.60 to 1.25, and for six load levels ranging from 10 to 98%. A system administrator attempting to tune the system performance while keeping the overhead re-Fig. <ref type="figure">9</ref>. The performance of the DMM compared with that of alternative architectures, for various imbalanced system loads. Fig. <ref type="figure" target="#fig_0">10</ref>. The components of goodput for various imbalanced system loads, with DMM: overall, inter-grid, intra-grid and intra-site goodput.  duced, should select the best delegation threshold for the predicted system load. For a lightly loaded system, with a load of 30%, setting a delegation threshold higher than 1.0 leads to a quick degradation of the system performance. For a system load of 70%, considered high in systems that can run parallel jobs <ref type="bibr" target="#b25">[26]</ref>, the best delegation threshold is 1.0, as it offers both the best goodput, and the lowest AWT and ASD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Iosup et al. / Inter-operating grids through Delegated MatchMaking</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.">The message overhead</head><p>Figure <ref type="figure" target="#fig_9">12</ref>(a) shows the distributions of the number of workload management and of the Resource Use messages, for various system loads (see Section 6.1 for message type descriptions). DMM adds only up to 19% more messages to the workload management messages for load levels below 60%, but up to 97% for higher load levels. However, the majority of messages in Condor-based systems are KeepAlive messages; when taking them into consideration, the messaging overhead incurred by DMM is at most 16%.</p><p>Figure <ref type="figure" target="#fig_9">12</ref>(b) shows the distribution of the messages in the DMM architecture, for various values of the delegation threshold. The system's load level is set to 70%. The number of DMM messages accounts for 7-16% of the total number of messages, and decreases with the growth of the delegation threshold. The workload management overhead grows from 35% (threshold 1.0) to 86% (threshold 0.6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6.">The influence of the resource binding duration</head><p>In the previous experiments we have not taken into account the duration of the resource binding procedure. The ratio between this duration and the average duration of the average job runtime can be used as a rough estimation of the desirability of the delegated matchmaking mechanism. However, this ratio is only valid when all jobs are run on delegated resources. We use simulations to investigate the realistic case where only some of the jobs require delegations.</p><p>We identify three cases of resource binding. In the ideal binding case, this procedure requires no additional setup, and is instantaneous (except for the exchange of setup messages between the JM and the RM, which can take 2-3 round-trips). In the light binding case, a lightweight virtualization mechanism, e.g., the Condor glide-in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b37">38]</ref>, is deployed on the remote resources before they can be added to the user's resource pool. In the heavy binding case, the resource binding process requires the instantiation of a heavy virtualization mechanism, e.g., a full-blown VMware or Xen virtual machine. Table <ref type="table">4</ref> shows the durations for the ideal, the light, and the heavy binding. We have obtained re-Table <ref type="table">4</ref> The resource binding duration for the ideal, the light and the heavy binding alistic values for the duration of light binding by measuring in a real environment the time it takes for a Condor glide-in to be ready for operation. Similarly, the duration of heavy binding has been reported for several of the most-used virtual machines, e.g., VMware and Xen <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28]</ref>. In addition, we have measured the resource binding duration for Amazon Elastic Computing Cloud (EC2) resources to an average of 50 s, with a standard deviation below 5%. Figure <ref type="figure" target="#fig_11">13</ref> depicts the performance of the DMM architecture when the resource binding duration Δ bind ranges from 0 (ideal binding) to 180 s (heavy binding). For the workloads considered in this work, there is no significant increase in the average waiting time due to the use of resource binding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion and future work</head><p>The next step in the evolution of grids is to interoperate several grids into a single computing infrastructure, to serve larger and more diverse communities of scientists. This raises additional challenges, e.g., load management between separate administrative entities. In this paper we have proposed DMM, a novel delegated matchmaking architecture for interoperating grids. Our hybrid hierarchical/distributed architecture allows the interconnection of several grids, without requiring the operation of a central point of the hierarchy. In DMM, when a user's request cannot be satisfied locally, remote resources are transparently added to the user's site through delegated matchmaking.</p><p>We have evaluated with simulations the performance of our proposed architecture, and compared it against that of five alternative architectures. A key aspect of this research is that the workloads used throughout the experiments are either real long-term grid traces, or synthetic traces that reflect the properties of grid workloads. The analysis of system performance under balanced inter-grid load shows that our architecture can accommodate equally well low and high (up to 80%) system loads. In addition, the results show that starting from a system utilization of 50% and up to 98%, the DMM offers a better goodput and a lower average wait time than the considered alternatives. As a result, DMM can finish up to four times more jobs than its alternatives. The difference increases when the interoperated grids experience high and imbalanced loads. Our analysis of performance under imbalanced intergrid load reveals that, compared to its best alternative, DMM achieves up to 60% more goodput, and finishes up to 26% more jobs.</p><p>These results demonstrate that the DMM architecture can result in significant performance and administrative advantages. We expect that this work will simplify the current efforts in inter-operating the DAS and Grid'5000 systems, which are currently under way. To this end, we expect to implement and to deploy our architecture in the following year. From the technical point of view, we also intend to extend our simulations to a more heterogeneous platform, to account for resource and job failures, and to investigate the impact of existing and unmovable load at the cluster level. Finally, we hope that this architecture will become a useful step for sharing resources across grids.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The logical structure of the dual-grid system composed of (a) the DAS and (b) Grid'5000. Leaves in this structure represent actual clusters of resources. Nodes labelled 20-30 are administrative-only.</figDesc><graphic coords="3,85.32,127.31,404.28,108.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 (</head><label>2</label><figDesc>a) shows the cumulative normalized daily load of the DAS system, over a year, from 2005-03-20 to 2006-03-21. The right-most value indicates the average number of jobs served by each single processor during this period. The maximum overall load imbalance between the clusters of the DAS system is above 3:1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 (Fig. 2 .</head><label>22</label><figDesc>Fig. 2. Load imbalance between clusters of the same grid. (a) and (b) the cumulative normalized daily load of the clusters in the DAS and Grid'5000 systems over time; (c) and (d) the hourly load of the clusters in the DAS and Grid'5000 systems over time. Higher imbalance is denoted by more space between curves.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The meta-scheduling architectures: (a) independent clusters, (b) centralized meta-scheduler, (c) hierarchical K-level meta-scheduler, (d) distributed meta-scheduler with static links.</figDesc><graphic coords="5,299.90,285.88,165.60,132.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The delegated matchmaking mechanism, during a successful match.</figDesc><graphic coords="8,147.71,127.33,279.72,322.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. A worst-case performance scenario for delegated matchmaking.</figDesc><graphic coords="9,58.10,127.31,216.36,119.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. The variability of the runtimes of jobs in batch submissions in grids.</figDesc><graphic coords="11,140.13,127.31,294.84,149.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. System behavior over a selected period of two days. Top graph: comparison of the number of jobs started by the DMM and by its alternatives. Bottom graph: number of workload management messages for the DMM.</figDesc><graphic coords="14,118.66,127.33,337.68,253.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. The performance of the DMM architecture for various values of the delegation threshold.</figDesc><graphic coords="18,130.23,127.32,314.40,226.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. The distribution of the messages in the DMM architecture: (a) for various system load levels and a Delegation T'hold of 1.0; (b) for various Delegation T'hold values and a 70% system load level.</figDesc><graphic coords="18,122.62,388.61,329.76,151.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. The performance of the DMM architecture vs. the resource binding duration.</figDesc><graphic coords="19,90.99,575.10,393.12,140.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="15,135.50,147.33,303.84,548.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="17,135.50,127.33,303.84,438.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">Simulated meta-scheduling architectures</cell></row><row><cell>System</cell><cell>Architecture</cell><cell>Operation</cell></row><row><cell>condor</cell><cell>Independent</cell><cell>Matchmaking</cell></row><row><cell>sep-c</cell><cell>Independent</cell><cell>Job routing</cell></row><row><cell>cern</cell><cell>Centralized</cell><cell>Job pull</cell></row><row><cell>central</cell><cell>Centralized</cell><cell>Job routing</cell></row><row><cell>fcondor</cell><cell>Federated</cell><cell>Matchmaking</cell></row><row><cell>DMM</cell><cell>Distributed</cell><cell>Matchmaking</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3</head><label>3</label><figDesc>Performance results when running real long-term traces</figDesc><table><row><cell>Simulator</cell><cell>No. Jobs</cell><cell>AWT</cell><cell>ASD</cell><cell>Goodput</cell><cell>JF</cell></row><row><cell></cell><cell>[kJobs]</cell><cell>[s]</cell><cell></cell><cell>[CPUyr]</cell><cell>[%]</cell></row><row><cell>cern</cell><cell>455.4</cell><cell>44</cell><cell>6</cell><cell>117</cell><cell>100</cell></row><row><cell>condor</cell><cell>455.4</cell><cell>7,681</cell><cell>1,610</cell><cell>117</cell><cell>100</cell></row><row><cell>DMM</cell><cell>455.4</cell><cell>1,283</cell><cell>298</cell><cell>117</cell><cell>100</cell></row><row><cell>fcondor</cell><cell>455.4</cell><cell>2,570</cell><cell>255</cell><cell>117</cell><cell>100</cell></row><row><cell>sep-c</cell><cell>455.4</cell><cell>1,938</cell><cell>590</cell><cell>117</cell><cell>100</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was carried out in the context of the Virtual Laboratory for e-Science project (www.vl-e.nl), which is supported by a BSIK grant from the Dutch Ministry of Education, Culture and Science (OC&amp;W), and which is part of the ICT innovation program of the Dutch Ministry of Economic Affairs (EZ). We would like to further thank to Greg Thain, for his helpful comments.</p><p>We gratefully acknowledge and thank Dr. Franck Cappello and the Grid'5000 team, Kees Verstoep and the DAS team, and The Grid Workloads Archive team, for providing us with the grid traces used in this work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="http://gwa.ewi.tudelft.nl" />
		<title level="m">The Grid Workloads Archive</title>
		<imprint>
			<date type="published" when="2007-07">July 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<ptr target="http://www.cs.huji.ac.il/labs/parallel/workload/" />
		<title level="m">The Parallel Workloads Archive</title>
		<imprint>
			<date type="published" when="2007-07">July 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On-line algorithms</title>
		<author>
			<persName><forename type="first">S</forename><surname>Albers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Leonardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Our-Grid: An approach to easily assemble grids with equitable resource sharing</title>
		<author>
			<persName><forename type="first">N</forename><surname>Andrade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cirne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">V</forename><surname>Brasileiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Roisenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JSSPP</title>
		<imprint>
			<biblScope unit="volume">2862</biblScope>
			<biblScope unit="page" from="61" to="86" />
			<date type="published" when="2003">2003</date>
			<pubPlace>Seattle, WA</pubPlace>
		</imprint>
	</monogr>
	<note>LNCS</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improving goodput by co-scheduling cpu and network capacity</title>
		<author>
			<persName><forename type="first">J</forename><surname>Basney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Livny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. HPCA</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="220" to="230" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wolski</surname></persName>
		</author>
		<title level="m">Scheduling from the perspective of the application, in: HPDC</title>
		<meeting><address><addrLine>Syracuse, NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="100" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Grid&apos;5000: a large scale and highly reconfigurable experimental grid testbed</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bolze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cappello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Daydé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Desprez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Jeannot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lanteri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leduc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Melab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mornet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Namyst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Primet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Quetier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E.-G</forename><surname>Talbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Touche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. HPCA</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="481" to="494" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Managing clusters of geographically distributed high-performance computers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Brune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Reinefeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CP&amp;E</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="887" to="911" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Capit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Georgiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Huard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mounié</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Neyron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Richard</surname></persName>
		</author>
		<title level="m">A batch scheduler with high level components</title>
		<meeting><address><addrLine>Cardiff, Wales, UK</addrLine></address></meeting>
		<imprint>
			<publisher>CCGrid</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="776" to="783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Josh system design</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cawood</surname></persName>
		</author>
		<idno>EPCC- SUNDCG-SD D4.2</idno>
	</analytic>
	<monogr>
		<title level="m">EPCC Sun Data and Compute Grids Project</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Moab workload manager administrator&apos;s guide</title>
		<imprint>
			<date type="published" when="2007-01">Jan 2007</date>
			<publisher>Cluster Resources Inc</publisher>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Doc</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A resource management architecture for metacomputing systems</title>
		<author>
			<persName><forename type="first">K</forename><surname>Czajkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">T</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">T</forename><surname>Karonis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kesselman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tuecke</surname></persName>
		</author>
		<editor>IPPS/SPDP</editor>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="62" to="82" />
			<pubPlace>Orlando, FL</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m">The distributed ASCI supercomputer 2 (DAS-2)</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
		<respStmt>
			<orgName>Dutch University Backbone</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Advanced Resource Connector middleware for lightweight computational grids</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ellert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grønager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Konstantinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kónya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lindemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Livenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Niinimäki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Smirnova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wäänänen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">FGCS</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="219" to="240" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A worldwide flock of Condors: Load sharing among workstation clusters</title>
		<author>
			<persName><forename type="first">D</forename><surname>Epema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Livny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dantzig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Evers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pruyne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">FGCS</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="53" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Metrics and benchmarking for parallel job scheduling</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Feitelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rudolph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IPPS/SPDP</title>
		<meeting><address><addrLine>Orlando, FL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">1459</biblScope>
			<biblScope unit="page" from="1" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m">The Grid: Blueprint for a New Computing Infrastructure</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Foster</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Kesselman</surname></persName>
		</editor>
		<meeting><address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">An evaluation of parallel job scheduling for</title>
		<author>
			<persName><forename type="first">H</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Moreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pattnaik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Jette</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>SC</publisher>
			<pubPlace>Portland, OR</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tannenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Livny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">T</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tuecke</surname></persName>
		</author>
		<title level="m">Condor-g: A computation management agent for multiinstitutional grids</title>
		<imprint>
			<publisher>HPDC</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="55" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Evaluation of job-scheduling strategies for grid computing</title>
		<author>
			<persName><forename type="first">V</forename><surname>Hamscher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Schwiegelshohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Streit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yahyapour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">GRID</title>
		<imprint>
			<biblScope unit="page" from="191" to="202" />
			<date type="published" when="1971">1971, 2000</date>
			<pubPlace>Bangalore, IN</pubPlace>
		</imprint>
	</monogr>
	<note>LNCS</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A framework for adaptive execution in grids</title>
		<author>
			<persName><forename type="first">E</forename><surname>Huedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Montero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Llorente</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Soft. Pract. Exp</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="631" to="651" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">How are real grids used? The analysis of four grid traces and its implications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Iosup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dumitrescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Epema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">GRID</title>
		<imprint>
			<biblScope unit="page" from="262" to="270" />
			<date type="published" when="2006">2006</date>
			<pubPlace>Barcelona, Spain, IEEE CS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">The characteristics and performance of groups of jobs in grids</title>
		<author>
			<persName><forename type="first">A</forename><surname>Iosup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Sonmez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Epema</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Euro-Par</publisher>
			<pubPlace>Rennes, France, LNCS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<author>
			<persName><forename type="first">A</forename><surname>Iosup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Anoep</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dumitrescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Epema</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.future.2008.02.003</idno>
		<ptr target="http://dx.doi.org/10.1016/j.future.2008.02.003" />
	</analytic>
	<monogr>
		<title level="m">The grid workloads archive</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note type="report_type">print</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sharing networked resources with brokered leases</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Irwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Chase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Grit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Yumerefendi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yocum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Annual Technical Conference</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="199" to="212" />
		</imprint>
	</monogr>
	<note>General Track, USENIX</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Scheduling for parallel supercomputing: A historical perspective of achievable utilization</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nitzberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JSSPP</title>
		<imprint>
			<biblScope unit="volume">1659</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="1999">1999</date>
			<pubPlace>San Juan, Portugal</pubPlace>
		</imprint>
	</monogr>
	<note>LNCS</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">L</forename><surname>Kleinrock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Queueing Systems</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="1975">1975</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">VMPlants: Providing and managing machine execution environments for grid computing</title>
		<author>
			<persName><forename type="first">I</forename><surname>Krsul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ganguly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A B</forename><surname>Fortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J O</forename><surname>Figueiredo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>SC</publisher>
			<biblScope unit="page">7</biblScope>
			<pubPlace>Pittsburgh, PA, IEEE CS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Are user runtime estimates inherently inaccurate?</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Schwartzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hardy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JSSPP</title>
		<imprint>
			<biblScope unit="volume">3277</biblScope>
			<biblScope unit="page" from="253" to="263" />
			<date type="published" when="2004">2004</date>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
	<note>LNCS</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The workload on parallel supercomputers: modeling the characteristics of rigid jobs</title>
		<author>
			<persName><forename type="first">U</forename><surname>Lublin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Feitelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. PDC</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1105" to="1122" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Experiences with the koala co-allocating scheduler in multiclusters</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H J</forename><surname>Epema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CS</title>
		<imprint>
			<biblScope unit="page" from="784" to="791" />
			<date type="published" when="2005">2005</date>
			<publisher>CCGrid</publisher>
			<pubPlace>Cardiff, Wales, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">A case for cooperative and incentive-based coupling of distributed clusters</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Buyya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Harwood</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>CLUSTER, IEEE CS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">AliEn resource brokers</title>
		<author>
			<persName><forename type="first">P</forename><surname>Saiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Buncic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computing in High Energy and Nuclear Physics</title>
		<meeting><address><addrLine>La Jolla, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note>Also available as CoRR cs.DC/0306068</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Resource allocation and scheduling in metasystems</title>
		<author>
			<persName><forename type="first">U</forename><surname>Schwiegelshohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yahyapour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">LNCS</title>
		<editor>
			<persName><surname>Hpcn Europe</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">1593</biblScope>
			<biblScope unit="page" from="851" to="860" />
			<date type="published" when="1999">1999</date>
			<pubPlace>Amsterdam</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">On-line scheduling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sgall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Online Algorithms</title>
		<imprint>
			<biblScope unit="volume">1442</biblScope>
			<biblScope unit="page" from="196" to="231" />
			<date type="published" when="1996">1996</date>
			<pubPlace>Dagstuhl, Germany</pubPlace>
		</imprint>
	</monogr>
	<note>LNCS</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Grid allocation and reservation -grid capacity planning with negotiation-based advance reservation for optimized qos</title>
		<author>
			<persName><forename type="first">M</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Villazón</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fahringer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>SC</publisher>
			<biblScope unit="page">103</biblScope>
			<pubPlace>Tampa Bay, FL</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Scheduling with advanced reservations</title>
		<author>
			<persName><forename type="first">W</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">T</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">E</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CS</title>
		<imprint>
			<biblScope unit="page" from="127" to="132" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
	<note>IPDPS</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Distributed computing in practice: the condor experience</title>
		<author>
			<persName><forename type="first">D</forename><surname>Thain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tannenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Livny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CP&amp;E</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2-4</biblScope>
			<biblScope unit="page" from="323" to="356" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Standardization of an api for distributed resource management systems</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tröger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rajic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Haas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Domagalski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CCGrid, Rio de Janeiro</title>
		<meeting><address><addrLine>Brasil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="619" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<author>
			<persName><forename type="first">D</forename><surname>Tsafrir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Etsion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Feitelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Modeling user runtime estimates</title>
		<meeting><address><addrLine>Cardiff, Wales, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">3834</biblScope>
			<biblScope unit="page" from="1" to="35" />
		</imprint>
	</monogr>
	<note>LNCS</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Utilization, predictability, workloads, and user runtime estimates in scheduling the IBM SP2 with backfilling</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Weil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Feitelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Parallel Distrib. Syst</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="529" to="543" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
