<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science and Technology</orgName>
								<orgName type="department" key="dep2">Tsinghua National Laboratory for Information Science and Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Intelligent Technology and Sys-tems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">78AF631D09BF26ECBD5A1AA60848630C</idno>
					<idno type="DOI">10.1109/TIM.2016.2514779</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Object Recognition Using Tactile Measurements: Kernel Sparse Coding Methods Huaping Liu, Di Guo, and Fuchun Sun</p><p>Abstract-Dexterous robots have emerged in the last decade in response to the need for fine-motor-control assistance in applications as diverse as surgery, undersea welding, and mechanical manipulation in space. Crucial to the fine operation and contact environmental perception are tactile sensors that are fixed on the robotic fingertips. These can be used to distinguish material texture, roughness, spatial features, compliance, and friction. In this paper, we regard the investigated tactile data as time sequences, of which dissimilarity can be evaluated by the popular dynamic time warping method. A kernel sparse coding method is therefore developed to address the tactile data representation and classification problem. However, the naive use of sparse coding neglects the intrinsic relation between individual fingers, which simultaneously contact the object. To tackle this problem, we develop a joint kernel sparse coding model to solve the multifinger tactile sequence classification problem. In this model, the intrinsic relations between fingers are explicitly taken into account using the joint sparse coding, which encourages all of the coding vectors to share the same sparsity support pattern. The experimental results show that the joint sparse coding achieves better performance than conventional sparse coding.</p><p>Index Terms-Dexterous robot, joint sparse coding, kernel sparse coding, object recognition, tactile measurement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>T ACTILE sensors in the robotic fingertips can be used to capture multiple object properties such as texture, roughness, spatial features, compliance, or friction and therefore become a very important sense modality for intelligent robot <ref type="bibr" target="#b17">[18]</ref>.</p><p>During the past decades, tactile sensing has been used for various purposes such as determining object pose <ref type="bibr" target="#b29">[30]</ref>, extracting object shape <ref type="bibr" target="#b2">[3]</ref>, recognizing objects <ref type="bibr" target="#b12">[13]</ref>, and mapping of unknown surfaces <ref type="bibr" target="#b26">[27]</ref>. Among them the tactile object recognition attracts a lot of attentions. The object recognition using tactile measurements has some important potential applications. For example, we can distinguish between hard and soft fruits which may be visually similar <ref type="bibr" target="#b9">[10]</ref>. In addition, we can use it to infer the internal state of some visually similar bottles <ref type="bibr" target="#b7">[8]</ref>. Such functions are important for the fine manipulation of intelligent robots because knowing the object property enables the execution of object-specific control or plan strategies. Reference <ref type="bibr" target="#b28">[29]</ref> investigated the model-based method for blind tactile recognition of 3-D objects and developed a neural network for recognizing the geometric symbols that are embossed on the object surfaces. Liu et al. <ref type="bibr" target="#b18">[19]</ref> adopted a set of low-resolution pressure maps to develop an object shape recognition method. Besides the shape, the deformability of object is also an important cue for object recognition. Chitta et al. <ref type="bibr" target="#b7">[8]</ref> dealt with the problem of discriminating between various types of liquid containers (water bottles, soda cans, and fruit juice bottles) and their respective internal states (full, empty, open, and closed). Reference <ref type="bibr" target="#b9">[10]</ref> developed a novel tactile sensor for distinguishing rigid and deformable objects. To address the sophisticated tactile object recognition problem, several scholars proposed to develop machine learning technology for robust recognition. For example, Soh et al. <ref type="bibr" target="#b31">[32]</ref> proposed a spatiotemporal online recursive kernel Gaussian process for object recognition and uncertainty analysis in data collected by the five-fingered iCub dexterous hand. Reference <ref type="bibr" target="#b23">[24]</ref> resorted to the bag-of-systems method to classify the tactile sequences. Very recently, Madry et al. <ref type="bibr" target="#b24">[25]</ref> have developed an unsupervised spatiotemporal feature learning method for tactile sequence recognition. It extracted rich spatiotemporal structures from raw tactile data without the need for predefined discriminative data characteristics. This method performed well in grasp stability analysis and object instance recognition. A grasping system was presented in <ref type="bibr" target="#b38">[39]</ref> that involved fusing multisensory inputs. Nevertheless, most of the previous work on tactile classification has focused on using grippers and industrial-style robotic arms, except <ref type="bibr" target="#b31">[32]</ref> which dealt with five-finger hand. In addition, Goldfeder et al. <ref type="bibr" target="#b11">[12]</ref> produced a simulated tactile data set, which was generated by the application of different types of dexterous hands to grasp amounts of objects.</p><p>On the other hand, sparse signal reconstruction has also gained considerable interest <ref type="bibr" target="#b6">[7]</ref>. Variations and extensions of sparse representation have been employed in many classification tasks, such as face recognition <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b33">[34]</ref>, action recognition <ref type="bibr" target="#b5">[6]</ref>, person reidentification <ref type="bibr" target="#b39">[40]</ref>, target detection <ref type="bibr" target="#b20">[21]</ref>, object tracking <ref type="bibr" target="#b19">[20]</ref>, image fusion <ref type="bibr" target="#b35">[36]</ref>, and power quality disturbance classification <ref type="bibr" target="#b25">[26]</ref>. Although sparse coding is effective in many fields, its usefulness in time series applications is limited because time series do not usually lie in the Euclidean space <ref type="bibr" target="#b37">[38]</ref>, thus the conventional sparse coding for vector spaces is not appropriate. Kernel sparse coding has been proposed to address this problem. By using a suitably designed kernel, the linear sparse coding can be extended to the nonlinear case <ref type="bibr" target="#b27">[28]</ref>. In <ref type="bibr" target="#b4">[5]</ref>, the kernel sparse coding for time series was established. Liu et al. <ref type="bibr" target="#b21">[22]</ref> developed an effective robust dictionary learning method for kernel sparse coding. Notwithstanding, such methods have never been developed for tactile sensing scenarios.</p><p>As we have mentioned, in most of the existing work on tactile sensing, the hand that performs the grasping action usually has several (typically 2-3) fingers. Existing models do not exploit the intrinsic relations between fingers. Instead, the tactile sequences are concatenated into a single matrix. For this paper, we used the BarretHand platform, which has three fingers with tactile sensors. We investigate the utility of the intrinsic relations between fingers for grasp analysis and tactile sequence classification. To exploit these relations, we develop a joint kernel sparse coding (JKSC) method to fuse the tactile sequences from separate fingers. Our experimental results demonstrate good performance. This paper extends <ref type="bibr" target="#b36">[37]</ref> by incorporating more algorithm analysis and experimental validation. The main contributions are listed as follows.</p><p>1) A JKSC method is proposed to explicitly model the intrinsic relations between individual fingers. 2) We collect a set of tactile sequences that explicitly include the individual finger information and use this data to determine the effectiveness of the proposed method. This data set will be released to the public domain to facilitate study. 3) We also perform the proposed JKSC method on some publicly available data sets to compare the performance. The rest of this paper is organized as follows. In Section II, we give a simple review about the tactile sequence representation. Section III presents the kernel sparse coding and the JKSC methods. Section IV gives the experimental results. In Section V, we give some discussions about the proposed methods. Finally, the conclusions are given in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. TACTILE SEQUENCE REPRESENTATION</head><p>We use the BarrettHand (see Fig. <ref type="figure" target="#fig_0">1</ref>) for tactile measurement, which has three fingers, labeled as Fingers 1-3. Fingers 1 and 2 rotate synchronously and symmetrically about the base joint in a spreading action. The spread motion around the palm allows an on-the-fly grasp reconfiguration to adapt to varying target object sizes, shapes, and orientations. Aside from the spreading motion, each of the three fingers on the BarrettHand feature two joints driven by a single dc brushless servomotor. Using the fingers together allows the BarrettHand to grasp a wide variety of objects securely. Each tactile sensor array contains 24 sensor cells arranged in three columns and eight rows. The main specifications of the BarrettHand are summarized in Table <ref type="table" target="#tab_0">I</ref>.</p><p>The detailed object data collection procedure is illustrated in section IV. In the following, we give a brief introduction about the tactile sequence representation and the corresponding metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Tactile Sequence</head><p>At each time step, we collect the tactile data from the 24 (8 × 3) sensor cells in each finger. By reshaping the 8 × 3 matrix as into a d = 24-D vector, we obtain a d-dimensional dynamic tactile sequence. See Fig. <ref type="figure">2</ref> for illustration.</p><p>Formally, the dynamic tactile sequence S i ∈ R d×T i can be represented as</p><formula xml:id="formula_0">S i = [S i,1 , S i,2 , . . . , S i,T i ]<label>(1)</label></formula><p>where S i,t ∈ R d for t = 1, 2, . . . , T i , and T i is the number of the sampled time instants for this sequence. For a different tactile sequence, the time length T i may be different. The problem of object recognition from tactile sequence measurements can be formulated as follows. Given a set of N training sequences {S i } N i=1 and their corresponding label l i ∈ {1, 2, . . . , C}, where C is the class number of the objects, design a classifier that determines the label of a newly arrived tactile sequence S ∈ R d×T . Exploiting the intrinsic temporal relation within the tactile sequences is important in the design of a robust classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Dynamic Time Warping Distance</head><p>A popular and effective method for comparing the time sequence is using the dynamic time warping (DTW) distance, which employs the dynamic programming paradigm to compute the alignment between two time sequences and allows similar shapes to match, even if they are out of phase in time. The DTW determines the alignment that warps one time sequence onto another, so the distance can be used, therefore, as a basis for determining the similarity between the time sequences. The details can be found in <ref type="bibr" target="#b16">[17]</ref>. We now give a brief introduction to DTW.</p><p>Suppose we have two tactile sequences S i and S j , with the respective lengths T i and T j . The first step in the DTW algorithm is to fill a local distance matrix D ∈ R T i ×T j , of which (s, t)th element is</p><formula xml:id="formula_1">D s,t = ||S i,s -S j,t || 2<label>(2)</label></formula><p>where s = 1, 2, . . . , T i and t = 1, 2, . . . , T j . The DTW method seeks the optimal warping path. The warping path is a set of adjacent matrix elements that identify the mapping between S i and S j . It can be represented as</p><formula xml:id="formula_2">P = {ρ 1 , ρ 2 , . . . , ρ K }, max(T i , T j ) ≤ K ≤ T i + T j (3)</formula><p>where ρ k is a two-element pair and k = 1, 2, . . . , K .</p><p>In particular, we have ρ 1 = {1, 1} and ρ K = {T i , T j }. Each warping path must satisfy the monotonicity, continuity, and boundary constraints <ref type="bibr" target="#b30">[31]</ref>. Although there are many warping paths that satisfy all of the above constraints, DTW is designed to find the one that minimizes the warping distance. Formally, the DTW distance between the two tactile sequences S i and S j is calculated as DTW(S i , S j ) = arg min</p><formula xml:id="formula_3">P 1 K K k=1 D ρ k . (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>In the above equation, the value of K in the denominator is used to normalize warping paths of differing lengths. From this equation, we find that DTW represents the path that minimizes the overall distance between S i and S j . This path can be found using dynamic programming, which is based on the following recursive equation:</p><formula xml:id="formula_5">W s,t = D s,t + min{W s-1,t , W s-1,t -1 , W s,t -1 }. (<label>5</label></formula><formula xml:id="formula_6">)</formula><p>Although DTW is an effective metric for comparing and classifying time sequences, it is not a distance measure because it does not satisfy the triangular inequality <ref type="bibr" target="#b16">[17]</ref>. Nevertheless, it has been widely used in time sequence classification tasks <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. JOINT KERNEL SPARSE CODING</head><p>Linear sparse coding methods cannot be used to deal with time sequences because the time sequences do not lie in Euclidean space. To overcome this problem, we employ the kernel trick.</p><p>We consider C classes of objects, which admit N tactile training samples S = {S i } N i=1 ⊂ M, where M represents the manifold in which the tactile sequences lie. If we map the training samples into higher dimensional space by proper mapping functions, then we expect the linear representation to improve. The linearity in feature space corresponds to the nonlinearity in the original space. To this end, we denote (•) : M → H to be the implicit nonlinear mappings from M into higher dimensional (possibly infinite dimensional) dot product space H. The mapping function is associated with the DTW kernel κ(S i , S j ) = T (S i ) (S j ), which can be represented as</p><formula xml:id="formula_7">κ(S i , S j ) = exp(-γ DTW(S i , S j )) (6)</formula><p>where γ is a prescribed adjusting parameter, which we fix at a value of 0.1. The above Gaussian DTW kernel is not a positive semidefinite kernel <ref type="bibr" target="#b14">[15]</ref>. In practice, we should make a small adjustment to guarantee the positive semidefinite property. See <ref type="bibr" target="#b4">[5]</ref> for details.</p><p>We then arrange the given N training samples as columns of a matrix</p><formula xml:id="formula_8">(S) = [ (S 1 ) (S 2 ) • • • (S N )].</formula><p>We call (S) as a dictionary in the higher dimensional space. Given sufficient training samples, any new (test) sample S will lie approximately in the linear span of the training samples associated with that class. When the number of the class is large, the sparsity is naturally encouraged. We therefore reformulate the following kernel sparse coding problem:</p><formula xml:id="formula_9">min x || (S) -(S)x|| 2 F + λ||x|| 1 (<label>7</label></formula><formula xml:id="formula_10">)</formula><p>where x is a coding vector and λ is the penalty parameter. The reconstruction term</p><formula xml:id="formula_11">|| (S) -(S)x|| 2 F can be represented as trace{( (S) -(S)x) T ( (S) -(S)x)} = trace{κ(S, S) -2κ T (S, S)x + x T κ(S, S)x}<label>(8)</label></formula><p>where κ(S, S) = κ(S, S 1 ) κ(S, S 2 ) • • • κ(S, S N ) T , and κ(S, S) is an N × N square matrix, of which the (i, j )th element is κ(S i , S j ). Therefore, the optimization problem ( <ref type="formula" target="#formula_9">7</ref>) is reformulated as</p><formula xml:id="formula_12">min x -2κ T (S, S)x + x T κ(S, S)x + λ||x|| 1 . (<label>9</label></formula><formula xml:id="formula_13">)</formula><p>The kernel trick obviates the mapping function to be of the form (•), or rather, only the kernel functions are required.</p><p>After obtaining the value of x, we classify the test sample S according to the residue</p><formula xml:id="formula_14">r c = -2κ T (S, S c )x c + x T c κ(S c , S c )x c (<label>10</label></formula><formula xml:id="formula_15">)</formula><p>where S c is associated with the cth object class, which is a subset of S, and x c is the coefficient associated with the cth class, which is a subset of x. The test sample is then assigned to the class that gives the smallest reconstruction error. Thus, the label is given by</p><formula xml:id="formula_16">c * = arg min c∈{1,...,C} r c . (<label>11</label></formula><formula xml:id="formula_17">)</formula><p>The above-mentioned kernel sparse coding method uses the DTW kernel to realize the coding of the single-finger tactile sequence. To achieve tactile sequence classification in practice, however, the hand usually has multiple fingers. Each finger represents a different view and provides a different tactile sequence. How the intrinsic relation among those fingers can be exploited is important for object recognition using tactile measurements.</p><p>Formally, if we are given M fingers, we denote the training sample for the mth finger as S (m) = {S (m)  i } N i=1 , where</p><formula xml:id="formula_18">S (m) i ∈ R d×T (m)</formula><p>i . Similarly, we construct the mth dictionary as (S (m) </p><formula xml:id="formula_19">) = [ (S (m) 1 ) (S (m) 2 ) • • • (S (m) N )].</formula><p>Consider the test sequence S = {S (1) , S (2) , . . . , S (M) }, where S (m) ∈ R d×T (m)  is the sequence, which is obtained by the mth finger and m = 1, 2, . . . , M. Finding the sparse representations is equivalent to solving the following problem:</p><formula xml:id="formula_20">⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ min</formula><p>x (1)   || (S (1) ) -(S (1) )x (1) || 2 F + λ||x (1) || 1 min</p><p>x (2)   || (S (2) ) -(S (2) )x (2) || 2 F + λ||x (2) || 1 . . .</p><formula xml:id="formula_21">min x (M) || (S (M) ) -(S (M) )x (M) || 2 F + λ||x (M) || 1 (<label>12</label></formula><formula xml:id="formula_22">)</formula><p>where x (m) ∈ R N for m = 1, 2, . . . , M is the coding coefficient vector for the mth finger. After solving the above problems, we can calculate the residue corresponding to each finger as</p><formula xml:id="formula_23">r (m) c = -2κ T S (m) , S (m) c x (m) c + x (m) c T κ S (m) c , S (m) c x (m) c (<label>13</label></formula><formula xml:id="formula_24">)</formula><p>where S (m) c is associated with the cth object class, which is a subset of S (m) , and x (m) c is the coefficient associated with the cth class, which is a subset of x (m) . The combined residue is calculated as</p><formula xml:id="formula_25">r c = M m=1 r (m) c . (<label>14</label></formula><formula xml:id="formula_26">)</formula><p>Finally, the label of the test sequence is given by <ref type="bibr" target="#b10">(11)</ref>. Since all of the fingers are used to grasp the same object, there exist intrinsic constraint relations on the coding vectors x (1) , x (2) , . . ., and x (M) . In the above optimization procedure, however, we neglect the intrinsic relation between different fingers and allow each finger modality to independently give the optimal solution. We term this approach the separate kernel sparse coding (SKSC) method.</p><p>A naive way of combining multifinger data is to concatenate all vectors into one single vector that acts as input of the framework, ignoring the characteristics of specific modalities. This is equivalent to requiring that all of the coding vectors be equal. That is to say, we require</p><p>x (1) = x (2) </p><formula xml:id="formula_27">= • • • = x (M) . (<label>15</label></formula><formula xml:id="formula_28">)</formula><p>Conceptually, this is equivalent to concatenating all of the fingers into one finger and solving a single sparse optimization problem. Therefore, we term this the concatenation kernel sparse coding (CKSC) method. The optimization problem is then transformed to</p><formula xml:id="formula_29">min x M m=1 || (S (m) ) -(S (m) )x|| 2 F + λ||x|| 1 (<label>16</label></formula><formula xml:id="formula_30">)</formula><p>where x is the common coding vector. This approach is limited because information sources with different statistical properties are mixed in a simplistic manner. The performance is not satisfactory, as it may lead to overfit or fail to learn associations between fingers with different underlying statistics.</p><p>In fact, each of the different modalities must have its own independent coding vector based only on the modality's own data. To address this problem, we need a tradeoff between SKSC and CKSC. Motivated by the success of joint sparse coding <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b32">[33]</ref>, we regard each finger as being an independent sensor, and thus code the fingers separately. The intrinsic relation is expressed by the sparse support pattern. An illustration can be found in Fig. <ref type="figure" target="#fig_1">3</ref>.</p><p>The method is to require the separate coding vectors to share a common-sparsity pattern, but allow them to have different values. That is to say, we require Sp(x (1) </p><formula xml:id="formula_31">) = Sp(x (2) ) = • • • = Sp(x (M) )</formula><p>where Sp(x) represents the sparsity pattern vector of the vector x. The length of Sp(x) is the same as that of x. If the i th element of x is zero, then the i th element of Sp(x) is zero; otherwise, the i th element of Sp(x) is set to 1. From this definition, we see that the sparsity pattern vector stores the information about its pattern in the vector x. This pattern is essential for a sparse coding vector.</p><p>The above problem can be further transformed into the problem requiring the combined matrix X = [x (1) x (2) • • • x (M) ] ∈ R N×M to be row sparse. Consequently, each column vector x (m) is sparse and the sparsity patterns of x (1) , x (2) , . . . , x (M) are shared by them.</p><p>Solving row-sparsity optimization is an NP problem and we resort the L 2,1 norm for its approximate solution. The L 2,1 norm of X calculates the sum of the L 2 norm of each row. By penalizing the sum of L 2 norms of the rows of coefficients associated with each coding vector across different fingers, similar sparse patterns for all fingers are encouraged. We term this approach as JKSC method, which preserves the intrinsic relation between fingers.</p><p>The above optimization problems can be solved effectively using existing software, such as CVX package <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS</head><p>In this section, we introduce the collected data set for validation of the proposed JKSC method. In addition, we present the experimental results on data sets, which are publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data Collection</head><p>The robot used for gathering data and carrying out the experiments is a Schunk manipulator equipped with a BarretHand having tactile sensors. The experiment is performed with the tactile sensors mounted on the fingertips of the robot hand, as shown in Fig. <ref type="figure" target="#fig_2">4</ref>. The objects are placed in a fixed position on a table and the trajectory planning of the manipulator was performed offline. The data collection procedure is similar to the active perception work flow in <ref type="bibr" target="#b9">[10]</ref>, as a static reading of tactile information is not sufficient to classify the object under inspection. To this end, the manipulator explores the objects using a palpation procedure. During this activity, the tactile sensors are stimulated with a sequence of tactile measurements. Next, the sequence is classified to recognize, which object is palpated. As the tactile sensor outputs zero when the sensor is not touching an object, we can determine the start of a palpation event when the tactile sensor input is above a specified threshold. The event ends when the tactile sensors do not report any significant change. As the execution time varies with each operation, the length of each tactile sequence varies. Finally, we normalize all measurements to the sensor's maximum response.</p><p>The hand is used to grasp five differently shaped bottles with water and without water. In Fig. <ref type="figure" target="#fig_3">5</ref>, we list the photos of the bottles. The bottom row shows the five bottles full of water (denoted by Af, Bf, Cf, Df, and Ef ), and the top row shows the corresponding empty bottles (denoted by Ae, Be, Ce, De, and Ee). It is obvious that their visual appearances are very similar. The difference between empty bottle and full bottle partially comes from the fact that the bottles are made of plastic, which is easy to deformation, and the tactile information is useful in determining deformation properties of bottles. For clear illustration, we show the tactile sequences  which are produced by Finger 1 in Fig. <ref type="figure" target="#fig_4">6</ref> for the objects Ae, Ce, Af, and Cf. Although it is difficult to discriminate the bottles by visual appearance, especially between the full state and the empty state of the same bottle, their tactile sequences do differ.</p><p>During data collection, each bottle was randomly placed in the table and grasped 20 times by the three fingers of the dexterous hands. Each grasp action lasted 8.2-18 s. The sampling period was 0.04 s and therefore the length of the obtained sequence was 205-450. Using these settings, we constructed a challenging ten-object data set, which included 200 tactile sequence samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Result Analysis</head><p>For performance evaluation, we compare the following methods.</p><p>1) Nearest Neighborhood (NN) Method: This method uses the concatenation feature and the DTW distance to search for the nearest training sample. Subsequently, the label of this training sample is assigned to the test sample. This approach serves as the baseline for comparison. 2) SKSC Method: This method separately solves the optimization problems in <ref type="bibr" target="#b11">(12)</ref> and uses <ref type="bibr" target="#b13">(14)</ref> to calculate the fusion residue. 3) CKSC Method: This method adopts the constraints <ref type="bibr" target="#b14">(15)</ref> and solves the optimization problem ( <ref type="formula" target="#formula_29">16</ref>) and uses <ref type="bibr" target="#b13">(14)</ref> to calculate the fusion residue. 4) JKSC Method: This method solves the joint sparse optimization problem <ref type="bibr" target="#b16">(17)</ref> and uses ( <ref type="formula" target="#formula_25">14</ref>) to calculate the fusion residue. Of the above methods, NN, CKSC, and JKSC consider the intrinsic relation between fingers. NN and CKSC use the concatenation feature and impose strict requirements on the relation, while JKSC requires only the common-sparsity pattern and has a relaxed requirement. SKSC does not account for the intrinsic relation.</p><p>We make an extensive comparison of five split cases: training/testing = 5/5, 6/4, 7/3, 8/2, and 9/1. For each split case, the data set is randomly split as training set and testing test for ten trials, and the averaged recognition accuracies are reported. The left panel of Fig. <ref type="figure" target="#fig_5">7</ref> shows the accuracy of the five cases. The accuracy is defined as the ratio of the number of the correctly classified testing samples and the number of all testing samples. The proposed JKSC method consistently outperforms the other methods. The SKSC method, which neglects the intrinsic relations between fingers, performs slightly better than NN but worse than JKSC and CKSC methods. From those results, we make the following observations.</p><p>1) The sparse coding methods, SKSC, CKSC, and JKSC, perform better than NN. Thus the kernel sparse coding strategy is indeed an effective method for improving the classification performance. 2) CKSC and JKSC perform better than SKSC. Therefore, the intrinsic relation between fingers plays an important role in the classifier design. 3) JKSC performs consistently better than CKSC. This demonstrates that the joint sparse coding strategy, which encourages the common-sparsity pattern, is more effective than methods that encourage the common coding vector, which is a stricter requirement. Further, we analyze the influence of the parameter λ for the 9/1 split case. The averaged recognition accuracies are reported under the different values of λ. In the right panel of Fig. <ref type="figure" target="#fig_5">7</ref>, we show the classification accuracy versus the value of λ. The best results are achieved for λ = 0.1. This indicates that both the reconstruction error term and the sparsity term play important roles in the classification.</p><p>In Fig. <ref type="figure" target="#fig_6">8</ref>, we present the confusion matrices for the 9/1 split case. Comparing the confusion matrices of the four classifiers, we find that the objects Bf, Df, and Ef can easily be classified using the proposed JKSC method. Furthermore, for the same bottle, the empty state and the full state are also classified successfully.</p><p>In Fig. <ref type="figure" target="#fig_7">9</ref>, we give an example to show the difference between the various sparse coding strategies. The test sample belongs the sixth class object (Cf ). The SKSC method give three different coding vectors x (1) , x (2) , and x (3) . The results are given in the left panel of Fig. <ref type="figure" target="#fig_7">9</ref>; the green bar indicates the correct class. This result shows that the three fingers give different results and the interpretability is weak. In fact, this method classifies this sample as the fourth class, which is indicated as a red line on the horizontal axis. For the CJKS method, the results of which are shown in the middle panel of Fig. <ref type="figure" target="#fig_7">9</ref>, there is only a single coding vector. To facilitate comparison, however, we plot it as three identical coding vectors.</p><p>From the results, we can see that the atoms corresponding to the fourth class (Bf ) are activated and are thus still incorrect. In the JKSC method, the results of which are shown in the right panel of Fig. <ref type="figure" target="#fig_7">9</ref>, the three tactile sequences obtained from individual fingers are separately coded with a common-sparsity pattern and the results are more robust. The result for Finger 1 is slightly incorrect, but the results for Fingers 2 and 3 are correct and therefore the fused results are also correct.</p><p>Before closing this section, we make a brief analysis about the time-costs. Please note that all of the compared methods do not require training phase and therefore we concern the timecost of the test phase only. The main time-cost is dominated by two stages.</p><p>1) The DTW Calculation: The DTW distances or kernels should be calculated between the test sample and all of the training samples.</p><p>2) The Sparse Coding: This stage is not required by NN. We omit the time-costs of other calculations such as sorting and residue calculation, which are negligible. In Table <ref type="table" target="#tab_1">II</ref>, we show the averaged test time using an unoptimized MATLAB program on PC platform (3.4-GHz CPU, 16-GB memory) for the two stages. From those results, we make the following observations. 1) Compared with the DTW calculation, the sparse coding optimization in SKSC, CKSC, and JKSC do not introduce too much time-cost.</p><p>2) The time-cost of DTW in SKSC and JKSC is smaller than that of NN and CKSC. The reason is that NN and CKSC use the concatenated feature with 72 dimensions, while SKSC and JKSC use three 24-D features for DTW calculation. The latter two require less time-cost. 3) CKSC requires solving one single kernel sparse coding problem, while SKSC requires solving three independent kernel sparse coding problems. Therefore, the coding time-cost of SKSC is significantly larger than that of CKSC. In addition, JKSC requires solving three independent sparse coding problems with one extra joint sparsity constraint. Therefore, it is less efficient due to the involvement of joint sparse optimization.</p><p>Due to the MATLAB implementation, such results are far away from the real-time application. In addition to implementing the algorithm using C program, we can also improve the time efficiency from several aspects. On one hand, we can resort the kernel dictionary learning <ref type="bibr" target="#b21">[22]</ref> to reduce the size of dictionary. On the other hand, there are many emerging methods to speed up the sparse optimization, which can be   used to reduce the time-cost. Finally, the DTW calculation, which forms the bottleneck, can be accelerated using some parallel computing devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Results for the Public Data Set</head><p>In Section IV-B, we adopt the three-fingered BarrettHand for experimental validation. However, the proposed method can easily be adapted for use with other types of multifingered hands. To show the flexibility of the proposed method, we adopt some public available data sets for validation. In those data sets, the number of fingers is limited to 2. This does not cause any difficulties when using the proposed method.</p><p>The adopted data sets include Schunk Parallel data set with ten objects (SPr-10) and the Schunk Parallel data set with seven objects (SPr-7) <ref type="bibr" target="#b9">[10]</ref>. The two data sets were generate from the same set of household objects of complex shape presented in Fig. <ref type="figure" target="#fig_0">10</ref>, using the flexible piezoresistive rubber tactile sensors. In Table <ref type="table" target="#tab_1">III</ref>, we list the information about those data sets. For more details of these data sets, please see <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b24">[25]</ref>.</p><p>Unfortunately, SPr-10 and SPr-7 do not explicitly include the separate finger information. This prevents us from testing the proposed method. Instead, we analyze the data collection procedure and assume that the presented data are a concatenation of a two-finger sequence (from Figs. <ref type="figure" target="#fig_8">11</ref> and<ref type="figure" target="#fig_9">12</ref>, we see that the data admit an obvious two-finger effect). From this Fig. <ref type="figure" target="#fig_0">10</ref>. Grasped objects in the public data sets <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b24">[25]</ref>. Left: SPr-10. Right: SPr-7. Please note that although the left figure shows only nine objects, there are ten object instances in SPr-10. The reason is that Drimus et al. <ref type="bibr" target="#b9">[10]</ref> collected the tactile data using the bottle with two statuses: empty and full.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE III</head><p>SUMMARY OF THE TWO PUBLIC DATA SETS assumption, we manually divide each frame of the sequence into two 64-D parts to virtually correspond to different fingers. This setting serves to validate the proposed method. In the left panel of Fig. <ref type="figure" target="#fig_10">13</ref>, we present the average classification results for the four methods over ten random splits. The split ratio of the training and testing sample is 9:1. The results show that JKSC performs better than SKSC and CKSC. The right panel of Fig. <ref type="figure" target="#fig_10">13</ref> shows the recognition accuracies versus the parameter λ. It shows that the best performance is obtained when λ is between [0.01, 0.1].</p><p>It is worth noting that some recently developed more advanced and more complicated classification methods can achieve even better performance on these two data sets <ref type="bibr" target="#b24">[25]</ref>.   Notwithstanding, it is still unclear how to incorporate the intrinsic relation between fingers into the existing work. In this paper, we consider the sparse coding methods only and focus on showing the effectiveness of JKSC in terms of its simplicity and effectiveness. If we adopt a more sophisticated kernel, we may obtain better performance, but this remains the future work. In addition, the information of the joint angles also plays an important role to deal with the pose changing problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. DISCUSSION</head><p>The core contribution of this paper is the kernel sparse coding method for tactile object recognition. Especially, we develop the JKSC method to exploit the intrinsic relation among fingers. Undoubtedly this method also provides an effective strategy for other applications of tactile measurements. A straightforward extension is the grasp stability assessment, which is one of the most important tasks of the tactile sensors on the robot hand <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b15">[16]</ref>. In <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, and <ref type="bibr" target="#b24">[25]</ref>, the grasp stability analysis problem has been thoroughly studied and a popular method is to reformulate it as a two-class classification problem. In this regard, the proposed kernel sparse coding methods can be used for the grasp stability assessment. The role of the intrinsic relation between fingers to the grasp stability is worthy of deep investigation, and the proposed joint coding strategy may provide a feasible method to achieve it. Further, how to simultaneously perform object recognition and grasp stability analysis is more appealing. Tackling such a problem may reveal more intrinsic relation between object and grasp pose. This will be our future work.</p><p>In spite of the success achieved by the proposed kernel sparse coding method, there still exist some limitations of this paper. The presented kernel sparse coding method only addresses the classifier design problem but does not concern the feature design. In this paper, we only consider the simple raw tactile data and the DTW kernel for the tactile sequence. Such a representation cannot deal with more sophisticated cases where the pose of the object is changed significantly. We note that most of the data sets such as those in <ref type="bibr" target="#b7">[8]</ref> and <ref type="bibr" target="#b9">[10]</ref> are collected according to such a protocol. However, if suitable feature descriptor is extracted, the sparse coding method, especially the JKSC method, can still be used to perform the robust object recognition. In the future, we will try to adopt some recently developed features that are translation invariant or rotation invariant <ref type="bibr" target="#b22">[23]</ref> to obtain better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In practical robotic manipulation, the hand usually has multiple fingers. Each finger represents a different view and provides a different tactile sequence. Therefore, we make a hypothesis that there exists some intrinsic relation which helps to improve the tactile object recognition performance. To validate this hypothesis, we develop the recognition framework of JKSC, which exploits the intrinsic relation among fingers and encourages different coding vectors to share the same sparsity support. That is to say, we regard each finger as an independent sensor, and thus code the fingers separately. The intrinsic relation is expressed by the sparse support pattern. The experimental results on three data sets show that the proposed coding method can elegantly take the advantage of having multiple fingers to improve recognition performance and it outperforms the conventional sparse coding methods including SKSC and CKSC. This validates the hypothesis that the intrinsic relation among those fingers plays an important role for object recognition using tactile measurements.</p><p>Since the proposed joint coding strategy provides a general idea to exploit the intrinsic relation among fingers, it can be used for more sophisticated cases by incorporating the carefully designed feature descriptors or kernels. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Tactile sensors of BarrettHand [35].</figDesc><graphic coords="2,66.47,52.85,216.02,126.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Illustration of the tactile sequence classification with joint kernel sparse representation. For a given object, the tactile sequences of the three fingers are recorded. Next, each finger sequence is represented as a linear combination of the corresponding training sequences in a joint kernel sparse way across all fingers.</figDesc><graphic coords="4,329.03,58.85,216.14,149.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Experimental setup.Thus, the optimization problem becomes</figDesc><graphic coords="5,65.99,58.61,216.14,79.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Experimental objects. The first row shows the five empty bottles (which are labeled Ae, Be, Ce, De, and Ee). The second row shows the five bottles full of water (Af, Bf, Cf, Df, and Ef ).</figDesc><graphic coords="5,329.03,58.97,216.14,127.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Visualization of the tactile sequence for objects Ae, Ce, Af, and Cf.</figDesc><graphic coords="5,329.51,228.89,216.02,157.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Left: recognition accuracy of the four methods versus data split ratio on the ten-object data sets. Right: recognition accuracy versus the value of λ for five split ratios.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Confusion matrices. Left to right: NN, SKSC, CKSC, and JKSC.</figDesc><graphic coords="7,177.47,57.41,126.14,94.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Illustration of the sparse coding strategies. In all training sample sets, atoms 1-18 belong to the first class, atoms 19-36 belong to the second class, and so on. The horizontal axis corresponds to the atoms of the training sample set. The overlaid red line indicates the fourth class and the green line indicates the sixth class. Left to right: SKSC, CKSC, and JKSC.</figDesc><graphic coords="7,68.03,177.41,156.38,117.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Visualization of the tactile sequence of SPr-7. The left panel shows the original 128-D tactile sequence and the right panels show the virtual two-finger sequences which are obtained by dividing the original tactile sequence into two 64-D tactile sequences.</figDesc><graphic coords="8,66.47,58.85,216.02,135.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Visualization of the tactile sequence of SPr-10. The left panel shows the original 128-D tactile sequence and the right panels show the virtual two-finger sequences which are obtained by dividing the original tactile sequence into two 64-D tactile sequences.</figDesc><graphic coords="8,66.47,255.77,216.02,145.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Left: recognition accuracies of the four methods using the public data sets SPr-7 and SPr-10. Right: recognition accuracy versus the values of λ for the two data sets.</figDesc><graphic coords="8,65.03,464.21,108.02,81.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Huaping</head><label></label><figDesc>Liu is currently an Associate Professor with the Department of Computer Science and Technology, Tsinghua University, Beijing, China. His current research interests include robot perception and learning. Mr. Liu serves as an Associate Editor of some journals, including the IEEE ROBOTICS AND AUTOMATION LETTERS, Neurocomputing, the International Journal of Control, Automation and Systems, and some conferences, including International Conference on Robotics and Automation and International Conference on Intelligent Robots and Systems.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I BARRETTHAND</head><label>I</label><figDesc>SPECIFICATIONS Fig. 2. Illustration of the tactile sequence representation for one finger.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II AVERAGED</head><label>II</label><figDesc>TIME-COST COMPARISON IN SECONDS</figDesc><table /></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the National Key Project for Basic Research, China, under Grant 2013CB329403, in part by the National Natural Science Foundation of China under Grant 61327809, and in part by the National High-Tech Research and Development Plan under Grant 2015AA042306. The Associate Editor coordinating the review process was Dr. Shervin Shirmohammadi.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Di Guo received the bachelor's degree from the Department of Instrumentation Science and Optoelectronics Engineering, Beihang University, Beijing, China, in 2011. She is currently pursuing the Ph.D. degree with the Department of Computer Science and Technology, Tsinghua University, Beijing.</p><p>Her current research interests include robotic manipulation and sensor fusion.</p><p>Fuchun Sun is currently a Full Professor with the Department of Computer Science and Technology, Tsinghua University, Beijing, China. His current research interests include intelligent control and robotics.</p><p>Mr. Sun was a recipient of the National Science Fund for Distinguished Young Scholars. He serves as an Associate Editor of a series of international journals, including the IEEE TRANS-ACTIONS ON FUZZY SYSTEMS, Mechatronics, Robotics, and Autonomous Systems.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning grasp stability based on tactile data and HMMs</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bekiroglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kragic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kyrki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Robot Human Interact</title>
		<meeting>IEEE Robot Human Interact</meeting>
		<imprint>
			<date type="published" when="2010-09">Sep. 2010</date>
			<biblScope unit="page" from="132" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Assessing grasp stability based on learning and haptic data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bekiroglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Laaksonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Jorgensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kyrki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kragic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Robot</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="616" to="629" />
			<date type="published" when="2011-06">Jun. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A potential field approach to dexterous tactile exploration of unknown objects</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bierbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rambow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Asfour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dillmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE-RAS Int. Conf. Humanoid Robots (Humanoids)</title>
		<meeting>IEEE-RAS Int. Conf. Humanoid Robots (Humanoids)</meeting>
		<imprint>
			<date type="published" when="2008-12">Dec. 2008</date>
			<biblScope unit="page" from="360" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vandenberghe</surname></persName>
		</author>
		<title level="m">Convex Optimization</title>
		<meeting><address><addrLine>Cambridge, U.K.</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge Univ. Press</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Kernel sparse representation for time series classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">292</biblScope>
			<biblScope unit="page" from="15" to="26" />
			<date type="published" when="2015-01">Jan. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sparsity induced similarity measure and its applications</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint/>
	</monogr>
	<note>to be published</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Sparse representation and learning in visual recognition: Theory and applications</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="1408" to="1425" />
		</imprint>
	</monogr>
	<note>Signal Process</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Tactile sensing for mobile manipulation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chitta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Piccoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Robot</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="558" to="568" />
			<date type="published" when="2011-06">Jun. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning grasp stability</title>
		<author>
			<persName><forename type="first">H</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Robot. Autom. (ICRA)</title>
		<meeting>IEEE Int. Conf. Robot. Autom. (ICRA)</meeting>
		<imprint>
			<date type="published" when="2012-05">May 2012</date>
			<biblScope unit="page" from="2392" to="2397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Design of a flexible tactile sensor for classification of rigid and deformable objects</title>
		<author>
			<persName><forename type="first">A</forename><surname>Drimus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kootstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bilberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kragic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robot. Auto. Syst</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="15" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Face recognition by exploiting local Gabor features with multitask adaptive sparse representation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Instrum. Meas</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2605" to="2615" />
			<date type="published" when="2015-10">Oct. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The Columbia grasp database</title>
		<author>
			<persName><forename type="first">C</forename><surname>Goldfeder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ciocarlie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Robot. Autom. (ICRA)</title>
		<meeting>Int. Conf. Robot. Autom. (ICRA)</meeting>
		<imprint>
			<date type="published" when="2009-05">May 2009</date>
			<biblScope unit="page" from="1710" to="1716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Haptic object recognition using passive joints and haptic key features</title>
		<author>
			<persName><forename type="first">N</forename><surname>Gorges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Navarro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Göger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Worn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Robot. Autom. (ICRA)</title>
		<meeting>IEEE Int. Conf. Robot. Autom. (ICRA)</meeting>
		<imprint>
			<date type="published" when="2010-05">May 2010</date>
			<biblScope unit="page" from="2349" to="2355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Single and multiple object tracking using a multi-feature joint sparse representation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="816" to="833" />
			<date type="published" when="2015-04">Apr. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">GDTW-P-SVMs: Variable-length time series analysis using support vector machines</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jalalian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Chalup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="270" to="282" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Leveraging big data for grasp planning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kappler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bohg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Robot. Autom. (ICRA)</title>
		<meeting>IEEE Int. Conf. Robot. Autom. (ICRA)</meeting>
		<imprint>
			<date type="published" when="2015-05">May 2015</date>
			<biblScope unit="page" from="4304" to="4311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Exact indexing of dynamic time warping</title>
		<author>
			<persName><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Very Large Data Bases</title>
		<meeting>Int. Conf. Very Large Data Bases</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="406" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A new multifunctional tactile sensor for detection of material hardness</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Matsue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Instrum. Meas</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1334" to="1339" />
			<date type="published" when="2011-04">Apr. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Tactile image based contact shape recognition using neural network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Greco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bimbo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Seneviratne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Althoefer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Multisensor Fusion Integr. Intell. Syst. (MFI)</title>
		<meeting>IEEE Conf. Multisensor Fusion Integr. Intell. Syst. (MFI)</meeting>
		<imprint>
			<date type="published" when="2012-09">Sep. 2012</date>
			<biblScope unit="page" from="138" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Robust object tracking based on principal component analysis and local sparse representation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Instrum. Meas</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2863" to="2875" />
			<date type="published" when="2015-11">Nov. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Target detection using sparse representation with element and construction combination feature</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Instrum. Meas</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="290" to="298" />
			<date type="published" when="2015-02">Feb. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Robust kernel dictionary learning using a whole sequence convergent algorithm</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Artif. Intell. (IJCAI)</title>
		<meeting>Int. Conf. Artif. Intell. (IJCAI)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3678" to="3684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Rotation and translation invariant object recognition with a tactile sensor</title>
		<author>
			<persName><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Althoefer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Sensors</title>
		<meeting>IEEE Sensors</meeting>
		<imprint>
			<date type="published" when="2014-11">Nov. 2014</date>
			<biblScope unit="page" from="1030" to="1033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Linear dynamic system method for tactile object classification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. China Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">ST-HMP: Unsupervised spatio-temporal feature learning for tactile data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kragic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Robot. Autom. (ICRA)</title>
		<meeting>IEEE Int. Conf. Robot. Autom. (ICRA)</meeting>
		<imprint>
			<date type="published" when="2014-06">May/Jun. 2014</date>
			<biblScope unit="page" from="2262" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Detection and classification of power quality disturbances using sparse signal decomposition on hybrid dictionaries</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Manikandan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Samantaray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kamwa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Instrum. Meas</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="27" to="38" />
			<date type="published" when="2015-01">Jan. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Tactile robotic mapping of unknown surfaces, with application to oil wells</title>
		<author>
			<persName><forename type="first">F</forename><surname>Mazzini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kettler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guerrero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dubowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Instrum. Meas</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="420" to="429" />
			<date type="published" when="2011-02">Feb. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Design of non-linear kernel dictionaries for object recognition</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">V</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Nasrabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5123" to="5135" />
			<date type="published" when="2013-12">Dec. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Robotic tactile recognition of pseudorandom encoded objects</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Petriu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-M</forename><surname>Cretu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J W</forename><surname>Spoelder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Instrum. Meas</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1425" to="1432" />
			<date type="published" when="2004-10">Oct. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bayesian estimation for autonomous object manipulation based on tactile sensors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Petrovskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Khatib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Robot. Autom. (ICRA)</title>
		<meeting>IEEE Int. Conf. Robot. Autom. (ICRA)</meeting>
		<imprint>
			<date type="published" when="2006-05">May 2006</date>
			<biblScope unit="page" from="707" to="714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Toward accurate dynamic time warping in linear time and space</title>
		<author>
			<persName><forename type="first">S</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intell. Data Anal</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="561" to="580" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Online spatio-temporal Gaussian process experts with application to tactile classification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Soh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Demiris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst. (IROS)</title>
		<meeting>IEEE/RSJ Int. Conf. Intell. Robots Syst. (IROS)</meeting>
		<imprint>
			<date type="published" when="2012-10">Oct. 2012</date>
			<biblScope unit="page" from="4489" to="4496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Joint sparse representation for robust multimodal biometrics recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shekhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Nasrabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="113" to="126" />
			<date type="published" when="2014-01">Jan. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sparse representation for face recognition based on constraint sampling and face alignment</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tsinghua Sci. Technol</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="62" to="67" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dexterous robotic-hand grasp learning using piecewise linear dynamic systems model</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Pract. Appl. Congnitive Syst. Inf. Process</title>
		<imprint>
			<biblScope unit="volume">215</biblScope>
			<biblScope unit="page" from="845" to="855" />
			<date type="published" when="2013-09">Sep. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multifocus image fusion and restoration with sparse representation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Instrum. Meas</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="884" to="892" />
			<date type="published" when="2010-04">Apr. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Tactile sequence classification using joint kernel sparse coding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf. Neural Netw. (IJCNN)</title>
		<meeting>Int. Joint Conf. Neural Netw. (IJCNN)</meeting>
		<imprint>
			<date type="published" when="2015-07">Jul. 2015</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Data fusion-based resilient control system under DoS attacks: A game theoretic approach</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Control Autom. Syst</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="513" to="520" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Improving robustness of robotic grasping by fusing multi-sensor</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Multisensor Fusion Integr. Intell. Syst. (MFI)</title>
		<meeting>IEEE Conf. Multisensor Fusion Integr. Intell. Syst. (MFI)</meeting>
		<imprint>
			<date type="published" when="2012-09">Sep. 2012</date>
			<biblScope unit="page" from="126" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Weight-based sparse coding for multi-shot person re-identification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. China Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
