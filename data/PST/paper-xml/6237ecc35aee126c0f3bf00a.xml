<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Prototypical Verbalizer for Prompt-based Few-shot Tuning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-03-18">18 Mar 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ganqu</forename><surname>Cui</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. of Comp. Sci. &amp; Tech</orgName>
								<orgName type="department" key="dep2">Institute for AI</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shengding</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. of Comp. Sci. &amp; Tech</orgName>
								<orgName type="department" key="dep2">Institute for AI</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ning</forename><surname>Ding</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Longtao</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. of Comp. Sci. &amp; Tech</orgName>
								<orgName type="department" key="dep2">Institute for AI</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. of Comp. Sci. &amp; Tech</orgName>
								<orgName type="department" key="dep2">Institute for AI</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute Guo Qiang</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">International Innovation Center</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Beijing Academy of Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Prototypical Verbalizer for Prompt-based Few-shot Tuning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-03-18">18 Mar 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2203.09770v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Tokyo Olympic Daily Preview, July 26th. sports technology Manual Verbalizer Labels</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Prompt-based tuning for pre-trained language models (PLMs) has shown its effectiveness in few-shot learning. Typically, prompt-based tuning wraps the input text into a cloze question. To make predictions, the model maps the output words to labels via a verbalizer, which is either manually designed or automatically built. However, manual verbalizers heavily depend on domain-specific prior knowledge and human efforts, while finding appropriate label words automatically still remains challenging. In this work, we propose the prototypical verbalizer (ProtoVerb) which is built directly from training data. Specifically, ProtoVerb learns prototype vectors as verbalizers by contrastive learning. In this way, the prototypes summarize training instances and are able to enclose rich class-level semantics. We conduct experiments on both topic classification and entity typing tasks, and the results demonstrate that ProtoVerb significantly outperforms current automatic verbalizers, especially when training data is extremely scarce. More surprisingly, ProtoVerb consistently boosts promptbased tuning even on untuned PLMs, indicating an elegant non-tuning way to utilize PLMs. Our codes are avaliable at https: //github.com/thunlp/OpenPrompt.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The massive-scale pre-trained language models (PLMs) <ref type="bibr" target="#b11">(Han et al., 2021a)</ref> have been proven to be backbones for solving a variety of NLP tasks <ref type="bibr" target="#b16">(Kowsari et al., 2019;</ref><ref type="bibr" target="#b26">Rajpurkar et al., 2016)</ref>. To further adapt these PLMs to downstream tasks such as classification, traditional approaches finetune the language models through an extra classifier <ref type="bibr" target="#b13">(Howard and Ruder, 2018)</ref>. However, when task-specific data is limited <ref type="bibr" target="#b0">(Bragg et al., 2021)</ref>, training the extra classifier effectively is challenging due to the gap between pre-training tasks (e.g., masked language modeling) and fine-tuning tasks (e.g., classification and regression). This gap impedes the fast adaptation of PLMs to downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tech Sports</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Soft</head><p>Recently, prompt-based tuning <ref type="bibr" target="#b28">(Schick and Schütze, 2021;</ref><ref type="bibr" target="#b21">Liu et al., 2021)</ref> has risen to be a powerful way for few-shot learning by bridging the gap between the pre-training stage and downstream task stage. In prompt-based tuning, the input texts are wrapped with task-specific templates to re-formalize the original task as a cloze-style task. For example, in topic classification task, we can use template "&lt;text&gt; This topic is about [MASK]", where &lt;text&gt; is the placeholder for input sentences. The PLMs are asked to infer the words to fill in <ref type="bibr">[MASK]</ref> and the words are further mapped to corresponding labels through a verbalizer (e.g. "sports" for label "Sports"). Verbaliz-ers are of great importance in prompt-based tuning <ref type="bibr" target="#b8">(Gao et al., 2021)</ref> since they are the bridges between model outputs and the final predictions. How to build effective verbalizers for prompt-based tuning-especially for many-class classification, is a critical issue in prompt-based tuning.</p><p>Typically, most current works adopt three kinds of verbalizers: manual verbalizers, search-based verbalizers, and soft verbalizers. We show them by an example in Figure <ref type="figure" target="#fig_0">1</ref>. Human-designed manual verbalizers pick some label words (e.g. label names) to depict classes. These verbalizers are powerful across multiple tasks <ref type="bibr" target="#b28">(Schick and Schütze, 2021)</ref>. Despite their success, a major drawback roots in the strong assumption that we own precise understandings of downstream tasks and are able to sum up each class with several words. Without task-specific prior knowledge, selecting appropriate label words is non-trivial. Further, they also need intensive human labors when facing many classes. To mitigate these issues, search-based verbalizers aim at finding suitable label words from vocabulary with algorithms <ref type="bibr" target="#b27">(Schick et al., 2020;</ref><ref type="bibr" target="#b29">Shin et al., 2020;</ref><ref type="bibr" target="#b8">Gao et al., 2021)</ref> and soft verbalizers use trainable tokens which are optimized during tuning <ref type="bibr" target="#b10">(Hambardzumyan et al., 2021;</ref><ref type="bibr" target="#b33">Zhang et al., 2021)</ref>. However, it is challenging to search or optimize adequately in a large vocabulary or embedding space under a low-data regime, making automatic verbalizers suboptimal compared with manual ones.</p><p>Intuitively, class proxies in verbalizers should encapsulate class-level semantic features, which are expressed implicitly by instances. To obtain these semantic representatives with few data, one promising approach is computing central points of class instances, namely prototypes, as approximation. To this end, we manage to estimate prototype vectors for each class to serve as verbalizer. Summarized from instances, prototypes are supposed to establish concepts similar with human-designed labels.</p><p>In this work, we introduce prototypes into this problem and propose prototypical verbalizer (Pro-toVerb), which learns class prototypes from training data to build verbalizers automatically. For prototype learning, inspired by the idea of PCL <ref type="bibr" target="#b19">(Li et al., 2021)</ref>, ProtoVerb trains the prototype vectors by contrastive learning with the InfoNCE estimator <ref type="bibr">(Oord et al., 2018)</ref>. Specifically, our optimization objective includes two components: The first part is an instance-instance loss to cluster intra-class instances and separate inter-class instances; The second part is an instance-prototype loss which enforces the prototypes to be center points of classes. Compared with other verbalizer construction methods, ProtoVerb learns continuous vectors straight from training instances efficiently, which makes it a plug-in-and-play algorithm with high flexibility.</p><p>To verify the effectiveness of ProtoVerb, we conduct extensive experiments on topic classification and entity typing tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Prompt-based Tuning</head><p>Despite the success of PLMs <ref type="bibr" target="#b3">(Devlin et al., 2019;</ref><ref type="bibr" target="#b22">Liu et al., 2019;</ref><ref type="bibr" target="#b25">Raffel et al., 2019)</ref> in massive NLP tasks, few-shot fine-tuning of PLMs was suboptimal due to the gap between pre-training and downstream tasks. Inspired by the "in context learning" proposed by <ref type="bibr">GPT-3 (Brown et al., 2020)</ref>, stimulating model knowledge with a few prompts has recently received much attention. A series of promptbased work on knowledge probing <ref type="bibr" target="#b31">(Trinh and Le, 2018;</ref><ref type="bibr" target="#b24">Petroni et al., 2019;</ref><ref type="bibr" target="#b2">Davison et al., 2019)</ref>, text classification <ref type="bibr" target="#b28">(Schick and Schütze, 2021;</ref><ref type="bibr" target="#b8">Gao et al., 2021)</ref>, relation extraction <ref type="bibr" target="#b12">(Han et al., 2021b)</ref>, and entity typing <ref type="bibr" target="#b4">(Ding et al., 2021a)</ref> emerge and achieve impressive progress. Typically, a piece of prompt contains a template and a verbalizer. Early prompts employ human-picked prompts which demand human knowledge and manual efforts. To alleviate this issue, later works explore automatic designing and optimizing prompts <ref type="bibr" target="#b21">(Liu et al., 2021;</ref><ref type="bibr" target="#b8">Gao et al., 2021;</ref><ref type="bibr" target="#b33">Zhang et al., 2021)</ref>. Recently research works further propose continuous prompts to replace the discrete phrases <ref type="bibr" target="#b18">(Lester et al., 2021;</ref><ref type="bibr" target="#b20">Li and Liang, 2021)</ref>. However, the designation of verbalizers, an important part of prompts, is less ex-plored. In this work, we investigate the automatic verbalizer construction in prompt-based tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Verbalizer Design</head><p>Verbalizers bridge between model outputs and labels and make great impact on prompt-based tuning <ref type="bibr" target="#b8">(Gao et al., 2021)</ref>. With task-specific knowledge, human-picked words are widely used and proved effective <ref type="bibr" target="#b28">(Schick and Schütze, 2021)</ref>. The major drawback of manual verbalizers is the assumption that we possess sufficient knowledge of downstream tasks, which is not always satisfied. To avoid intensive human labor and expert knowledge dependency in manual verbalizers, some works explore search-based verbalizers <ref type="bibr" target="#b27">(Schick et al., 2020;</ref><ref type="bibr" target="#b8">Gao et al., 2021;</ref><ref type="bibr" target="#b29">Shin et al., 2020)</ref> that identify label words automatically with training data. However, with a large vocabulary and few examples, it is non-trivial to find suitable words. Another line of researches focuses on soft verbalizers <ref type="bibr" target="#b10">(Hambardzumyan et al., 2021;</ref><ref type="bibr" target="#b33">Zhang et al., 2021)</ref>, which insert continuous embeddings as soft labels. The label embeddings are optimized along with model tuning. Similarly, soft verbalizers require abundant data for sufficient optimization, which can not be satisfied with the few-shot setting. In contrast, our approach learns prototype vectors from scratch, hence is more effective for few-shot tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Prototype-based Few-shot Learning</head><p>In few-shot learning, prototype-based metriclearning methods have been promising approaches for their simplicity and effectiveness. Prototypical Networks (ProtoNet) <ref type="bibr" target="#b30">(Snell et al., 2017)</ref> is the pioneering work that introduces prototypes into deep learning. Specifically, ProtoNet calculates prototype vectors by taking the average of instance vectors and makes predictions by metric-based comparisons between prototypes and query instances. A set of following works concentrates on the advancement of prototype estimation <ref type="bibr" target="#b19">(Li et al., 2021;</ref><ref type="bibr" target="#b9">Gao et al., 2019;</ref><ref type="bibr" target="#b6">Ding et al., 2021c)</ref>. Among them, PCL <ref type="bibr" target="#b19">(Li et al., 2021)</ref> achieves remarkable results on self-supervised few-shot learning by using prototypes as latent variables and inspires us in designing training objectives. The success of prototypebased models indicates that prototypes, which are representative embeddings of instances from the same classes, encapsulate some class-level semantic features. Inspired by the intrinsic similarity of prototypes and verbalizers, we find it natural and elegant to introduce prototypes into verbalizer construction for prompt-based tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background</head><p>Given a pre-trained language model M, our goal is to tune it for specific downstream tasks. Take N way K shot few-shot text classification as an example, the support set for class n</p><formula xml:id="formula_0">D n = {x n 1 , • • • , x n K } contains K sentences.</formula><p>We aim to predict the label y ∈ Y for each sentence, where Y is the label set with N distinct classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Fine-tuning</head><p>For a sentence concatenated with special to-</p><formula xml:id="formula_1">kens x = {[CLS], t 1 , • • • , t T , [SEP]}, language model M encodes it into hidden representations {h [CLS] , h 1 , • • • , h T , h [SEP] }. Conventional fine- tuning trains an extra classifier F over the [CLS] embedding h [CLS]</formula><p>and output the probability distribution on label set Y.</p><formula xml:id="formula_2">P (•|x) = Softmax(F (h [CLS] )).</formula><p>(1)</p><p>The classifier and PLM are tuned by maximizing</p><formula xml:id="formula_3">1 N N i=1 log P (y i |x i )</formula><p>, where y i is the label of x i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Prompt-based Tuning</head><p>The vanilla prompt-based tuning converts the downstream task to a cloze-style mask language modeling problem. For example, to formulate the text classification task, we can modify the original input x with a template</p><formula xml:id="formula_4">T (•) = A [MASK] news: to get the prompt input T (x) = A [MASK] news: x.</formula><p>With T (x), M produces the hidden vector at the</p><formula xml:id="formula_5">[MASK] position h [MASK] .</formula><p>To calculate the probability distribution over the label set, a manual verbalizer stores a set of label words V and the score for label y is</p><formula xml:id="formula_6">P M (y|x) = g(P M ([MASK] = v|T (x))|v ∈ V y ),<label>(2)</label></formula><p>where V y is the label words of y and g(•) is to aggregate multiple scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Prototypical Verbalizer</head><p>In previous sections, we introduce the general pipeline of prompt-based tuning. As manually defining or automatically searching for appropriate verbalizers can be challenging, here we propose to learn prototypes directly from training instances. Inspired by PCL <ref type="bibr" target="#b19">(Li et al., 2021)</ref>, the prototypes are trained with contrastive learning. As shown in  Figure <ref type="figure" target="#fig_1">2</ref>, we first get the hidden states of [MASK] tokens to represent instances, then project them to another embedding space for prototype learning. The prototypes are used as verbalizers for prediction. Next, we will introduce the learning and inference stages of ProtoVerb in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Instance Representation and Similarity Function</head><p>Given a piece of training text x wrapped with a template, we take the last layer's hidden state of the [MASK] token h [MASK] as the initial representation of the text. With an encoder E φ (•) parameterized by φ, the instance representation of x is</p><formula xml:id="formula_7">v = E φ (x) = Wh [MASK] .<label>(3)</label></formula><p>In practice, we simply adopt a linear encoder with weight W. To measure the similarity between instances, we adopt cosine similarity function S(•), where</p><formula xml:id="formula_8">S(v i , v j ) = v i ||v i || • v j ||v j || . (4)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Loss Function</head><p>With the instance representation and similarity function, we discuss how to define our training objective. Denote C = {c 1 , • • • , c N } as the set of prototype vectors. Intuitively, there are two goals we need to achieve by optimization: (1) For instanceinstance pairs, intra-class pairs should get higher similarity scores than inter-class pairs. (2) For instance-prototype pairs, the similarity scores between prototype c n and instances of class n should be higher than c n and other instances. To realize these two goals, we define the objective function based on the InfoNCE estimator <ref type="bibr">(Oord et al., 2018)</ref>, which is widely adopted in contrastive learning.</p><p>For the instance-instance objective, we minimize the following loss function</p><formula xml:id="formula_9">L ins = −1 N 2 K 2 n i,j log exp S(v n i , v n j ) n ,j exp S(v n i , v n j )</formula><p>,</p><p>(5) where (v n i , v n j ) are instance pairs of the same class. This loss function maximizes intra-class similarity and minimizes inter-class similarity between instances.</p><p>Similarly, the instance-prototype loss function is defined as</p><formula xml:id="formula_10">L proto = −1 N 2 K i,n log exp S(v n i , c n ) n exp S(v n i , c n ) ,<label>(6)</label></formula><p>and v n i is of class n. This objective forces each prototype to lie at the center point of its instances.</p><p>Overall, combining the instance-instance loss and instance-prototype loss, our final training objective is</p><formula xml:id="formula_11">L = L ins + L proto .<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Inference</head><p>During inference, following the same metric, we calculate the similarity scores of query and prototypes. The probability score for class k is</p><formula xml:id="formula_12">P M (y k |x) = exp S(v, c k ) k exp S(v, c k ) . (<label>8</label></formula><formula xml:id="formula_13">)</formula><p>Then we make prediction by arg max function</p><formula xml:id="formula_14">y = arg max k P M (y k |x).<label>(9)</label></formula><p>When there are other verbalizers (e.g. manual verbalizers), we first process the logits from different verbalizers with a standard scaler (minus mean then divide by standard deviation). Then we take the mean value of the scores to get the final score.</p><p>We conduct extensive few-shot learning experiments to illustrate the effectiveness of ProtoVerb. In this section, we first introduce the experimental settings in use. Then we present and discuss the experiment results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets and Templates</head><p>Verbalizers in many-class classification tasks are difficult to get precise definitions. Hence we adopt three topic classification datasets: AG's News, Yahoo <ref type="bibr" target="#b34">(Zhang et al., 2015)</ref>, and DBPedia <ref type="bibr" target="#b17">(Lehmann et al., 2015)</ref> and one entity typing dataset: FewN-ERD <ref type="bibr" target="#b7">(Ding et al., 2021d)</ref> as benchmarks, and their statistics are summarized in Table <ref type="table" target="#tab_2">1</ref>.</p><p>To focus on the verbalizer and alleviate the influence of templates, we adopt multiple fixed manual templates. For topic classification, following <ref type="bibr" target="#b14">(Hu et al., 2021)</ref>, we use four templates on each dataset. For entity typing, we use three templates from <ref type="bibr" target="#b4">(Ding et al., 2021a)</ref>. Details about the templates can be found in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Task </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Settings</head><p>Under the few-shot setting, we randomly sample k = 1, 2, 4, 8, 16 instances in each class from the training set and test the model on the entire test set.</p><p>As for the evaluation metric, we use accuracy in all experiments. For the different usages of ProtoVerb, we consider two specific settings:</p><p>(1) ProtoVerb as a single verbalizer ( § 5.5). When manual verbalizers are not available, we can tune the model with ProtoVerb. Under this setting, we want to evaluate the performance of ProtoVerb compared with other automatic verbalizer construction methods.</p><p>(2) ProtoVerb as an extra verbalizer ( § 5.6). Naturally, we suppose that there exists a manual verbalizer and we append ProtoVerb to strengthen the performance. Under this setting, ProtoVerb is a plug-in-and-play component and does not participate in the tuning process. We compare ProtoVerb with manual verbalizers and other verbalizer ensembles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Implementation Details</head><p>All our models and baselines are implemented with PyTorch (Paszke et al., 2019) framework, Huggingface transformers <ref type="bibr" target="#b32">(Wolf et al., 2020)</ref>, and Open-Prompt toolkit <ref type="bibr" target="#b5">(Ding et al., 2021b)</ref>. We optimize PLMs with AdamW optimizer <ref type="bibr" target="#b23">(Loshchilov and Hutter, 2019)</ref>. For prototype learning, we set the prototype dimension to 128 and optimize the loss function with Adam optimizer <ref type="bibr" target="#b15">(Kingma and Ba, 2015)</ref>. For topic classification, we use RoBERTalarge <ref type="bibr" target="#b22">(Liu et al., 2019)</ref> as our PLM backbone and tune the model for 5 epochs. The batchsize is 2 and the learning rate is 3e-5. For entity typing, we tune a BERT-base <ref type="bibr" target="#b3">(Devlin et al., 2019)</ref> model for 30 epochs and set the batchsize to 16. The learning rate here is 5e-5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Baselines</head><p>The vanilla prompt-based tuning method fuses the input text with a task-specific template and maps the model outputs to labels through a verbalizer. For fair comparisons, all our baselines and proposed models are built on this pipeline and they merely differ from the verbalizers in use.</p><p>Manual verbalizers (ManualVerb) are defined by human with domain knowledge. Here we simply employ the verbalizers provided by Open-Prompt <ref type="bibr" target="#b5">(Ding et al., 2021b)</ref>.</p><p>Search-based verbalizers (SearchVerb) search for suitable words from vocabulary automatically. We adopt the implementation in PETAL <ref type="bibr" target="#b27">(Schick et al., 2020)</ref>, which finds the words that maximize the likelihood of the training data. To combine SearchVerb with ManualVerb, we merge their verbalizer words together.</p><p>Soft verbalizers (SoftVerb) introduce trainable tokens as verbalizers in prompt-based tuning. We follow the approach in WARP <ref type="bibr" target="#b10">(Hambardzumyan et al., 2021)</ref> that applies soft tokens as a linear decoding layer, and the token embeddings are learned along with model tuning. Note that the templates in WARP are also trainable, but here we only use its soft verbalizers. In single verbalizer experiments, we initialize the token embeddings randomly for fairness. And in extra verbalizer experiments, they are initialized with label names. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Single Verbalizer Results</head><p>Table <ref type="table" target="#tab_3">2</ref>  to discriminate between "person-artist/author" and "person-director" with only a few instances. However, ProtoVerb can also catch up with ManualVerb with enough samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Multiple Verbalizer Results</head><p>Table <ref type="table" target="#tab_4">3</ref> shows the experiment results when we ensemble manual verbalizers with automatic verbalizers. The ensembled versions are denoted as SearchVerb+, SoftVerb+, and ProtoVerb+ respectively. From the table, we have the following observations: (1) Basically, prompt-based tuning outperforms fine-tuning by a large margin with few samples (1∼2 per class). When sufficient training data is available, fine-tuning models will produce comparable results. (2) Overall, ProtoVerb+ certainly improves the performance of prompt-based tuning under most cases, which demonstrates the effectiveness of ProtoVerb+. At the same time, SearchVerb+ and SoftVerb+ seldom show enhancement compared with ManualVerb. As ProtoVerb+ does not introduce any external knowledge, this illustrates that ProtoVerb+ provides a better way to utilize training data.</p><p>Finally, we also present the results of applying ProtoVerb+ on untuned PLMs. It is worth noting that even for untuned models, ProtoVerb+ also boosts them considerably on all tasks. For example on DBPedia, showing only one instance per class to PLMs with ProtoVerb+ leads to 11.26% absolute accuracy improvement. On topic classification, when more training samples are given, untuned PLMs achieve competitive scores. This observation indicates a new cost-efficient way to leverage training data, which we highlight as valuable for future study of none-tuning methods for PLMs. Compared to the "in context learning" in GPT-3 <ref type="bibr" target="#b1">(Brown et al., 2020)</ref>, ProtoVerb+ is not limited by input length and can deal with arbitrary number of samples. We further study this "fixed model" scenario in § 6.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Analysis</head><p>In this section, we discuss several analytical topics for further understandings of ProtoVerb. For simplicity, we conduct experiments on AG's News dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Fixed Model Experiments</head><p>In § 5.6, we see ProtoVerb is still powerful with fixed PLMs. For further comparisons, we conduct experiments to quantitatively evaluate verbalizers when PLMs are fixed. Figure <ref type="figure" target="#fig_5">3</ref> gives the results. To clarify, using ManualVerb on fixed PLMs equals the zero-shot setting, which we plot with a dashed line. Meanwhile, different from § 5.6, ProtoVerb here is a single verbalizer. From the figure we can conclude that (1) Similar with § 5.5, ProtoVerb outperforms SoftVerb and SearchVerb by a large margin under low-shot settings. Notably, ProtoVerb exceeds ManualVerb with only 2 shots per class, illustrating the experessive power of prototypes.</p><p>(2) SoftVerb is also better than SearchVerb under this setting, demonstrating that tunable verbalizers </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Ablation Study</head><p>To validate the effect of each part in the loss function, we conduct an ablation study on AG's News dataset. For comparison, we consider two variants of prototype calculation methods: (1) ProtoVerb with L proto only.</p><p>(2) Following ProtoNet <ref type="bibr" target="#b30">(Snell et al., 2017)</ref>, take the average of instance embeddings for prototype embeddings.  vectors directly, optimizing the embedding vectors of prototypes using our loss functions leads to better performances and stability. Adding L ins is also beneficial, meaning that L ins helps ProtoVerb in learning instance embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Robustness on Noisy Samples</head><p>Noisy data are commonly seen as threats in realworld datasets for few-shot learning systems. For automatic verbalizers, noisy data are more harmful because of the effect on both the quality of verbalizers and the training process. In this section, we evaluate the robustness of different automatic verbalizers against noisy samples on AG's News.</p><p>For training stability, we set K = 8, 16. Table <ref type="table" target="#tab_6">5</ref> presents the accuracy drop when there are 1, 2, or 3 samples having wrong labels. It is clearly seen that a limited number of noisy samples will hinder the performance greatly, showing the vulnerability of automatic verbalizers. Meanwhile, we can also find that ProtoVerb is more robust than baseline methods when facing noisy samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Prototype Discretization</head><p>Since ProtoVerb learns continuous prototype vectors, their meanings are implicit. Here we manage to investigate which words are most similar to the learned prototypes. Due to word embeddings and prototype vectors lying in different embedding spaces, we can not directly calculate their similarity. Hence we use the vocabulary as the input texts (one word at a time) to get the top-scored word for each class. On AG's News dataset, we collect some most similar words for each class and list them in Table <ref type="table" target="#tab_8">6</ref>.</p><p>To investigate the property of prototypes learned with different numbers of samples, we present words for K = 1 and K = 16. With the table, we see that: (1) Even when only one example is available, the learned prototypes are meaningful. Most of the similar words are proper nouns and entity names closely related to class topics. For example, "Steelers", "Raptors", "Knicks", and "Dodgers" are all baseball or basketball teams that appear frequently in sports news. We attribute this to prompt mechanism that allows PLMs to extract the most conclusive information and fill the [MASK] with it. Then the relevant words are also included. ( <ref type="formula" target="#formula_6">2</ref>) With more training instances, prototypes show diverse interests. Despite entity names, more "conceptual" words show up on the list, such as "ball" and "Sports" for class Sports. We interpret this as the summarization and abstraction ability of prototypes. Given many instances, prototypes are enforced to capture their common features, hence some abstract concepts are found automatically. In this way, ProtoVerb encapsulates class-level, rather than entity-level, semantics, which leads to better performance on unseen data.  To give further analyses for the inner workings of prototypes, we measure the similarity between Pro-toVerb and ManualVerb to see whether ProtoVerb is able to learn abstract concepts as humans do. On AG's News dataset, we calculate the similarity scores between prototypes and manual verbalizers and normalize the scores using the softmax function across the four classes. In Figure <ref type="figure" target="#fig_3">4</ref> we plot the scores with various shots. It is clearly seen that the similarity of prototypes and corresponding verbal-izers are above average (0.25). As shot increases, the scores also gradually grow, which illustrates that prototypes can capture the conceptual information better from more instances. This observation matches our findings in § 6.4. Among the four classes, Business and Sports get higher scores than World and Tech. A reasonable guess is that World and Tech news includes diverse sub-topics that are hard to summarize.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we propose a novel approach for automatic verbalizer construction in prompt-based tuning. The proposed ProtoVerb learns class prototypes from training instances using contrastive learning. We explore the performance of ProtoVerb on few-shot topic classification and entity typing tasks. As a single verbalizer, ProtoVerb outperforms state-of-the-art automatic verbalizers considerably. Working together with manual verbalizers, ProtoVerb can also consistently improve promptbased tuning with minor effort. The results validate the effectiveness of ProtoVerb. Our analysis further reveals the intrinsic properties of prototypes. For future work, we will focus on extending ProtoVerb for effective non-tuning algorithms of PLMs and prompt-tuning with soft templates. Moreover, we are finding proper ways to combine label words and prototypes for verbalizer construction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Templates</head><p>For topic classification, we use the default templates and verbalizers in OpenPrompt <ref type="bibr" target="#b5">(Ding et al., 2021b)</ref>.</p><p>AG's News is a news' topic classification dataset. There are four categories: World, Sports, Business, and Tech. We use the following templates. Yahoo is a question classification dataset with 10 classes. Each piece of text consists of a question and an answer. We use the templates in AG's News where "news" is replaced with "question" in T 1 (•)</p><formula xml:id="formula_15">T 1 (x) = A [MASK] question: x T 2 (x) = x This topic is about [MASK]. T 3 (x) = [ Category : [MASK] ] x T 4 (x) = [ Topic : [MASK] ] x</formula><p>FewNERD is a large-scale fine-grained entity typing dataset with 66 types and we use the official split of its supervised setting. Following <ref type="bibr" target="#b4">(Ding et al., 2021a)</ref>, we employ 3 templates as below where [ENT] copies the entity mention in the sentence.</p><formula xml:id="formula_16">T 1 (x) = x [ENT] is [MASK]. T 2 (x) = x [ENT] is a [MASK].</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of three verbalizer construction methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of ProtoVerb. Left: We project the hidden states of [MASK] tokens to the embedding space and learn prototypes. Right: The learned prototypes constitute the verbalizer and map the PLM outputs to corresponding labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Similarity scores between ProtoVerb and ManualVerb on AG's News.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>T 1</head><label>1</label><figDesc>(x) = A [MASK] news: x T 2 (x) = x This topic is about [MASK]. T 3 (x) = [ Category : [MASK] ] x T 4 (x) = [ Topic : [MASK] ] xDBPedia is an ontology classification dataset. Each sample contains an article title x and abstract y extracted from Wikipedia, and the task is to classify the subject's ontology class. There are 14 classes in total. We employ four templates shown below:T 1 (x, y) = x y x is a [MASK].T 2 (x, y) = x y In this sentence,xis a [MASK]. T 3 (x, y) = x y The type ofxis [MASK]. T 4 (x, y) = x y The category ofxis [MASK].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>T 3</head><label>3</label><figDesc>(x) = x In this sentence, [ENT] is a [MASK].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>We study two different settings where ProtoVerb can work: (1) When manual verbalizers are available, ProtoVerb can play as an extra verbalizer in the inference stage. Results show that ProtoVerb consistently improves the classification performance with low cost, and even untuned PLMs benefit largely. (2) Consider a realistic setting where only a limited number of samples are provided with no manual verbalizers, ProtoVerb also produces verbalizers of high quality. Experimental results demonstrate that ProtoVerb significantly outperforms existing search-based and soft verbalizers.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Stocks Fall as Oil Hits High. A [MASK] news: Technology as Fashion. A [MASK] news: Arsenal Beats Everton. A [MASK] news: Tokyo Olympic Daily Preview, July 26th.</figDesc><table><row><cell>Verbalizer</cell><cell>Labels</cell></row><row><cell></cell><cell>World</cell></row><row><cell></cell><cell>Tech</cell></row><row><cell></cell><cell>Sports</cell></row><row><cell>Template</cell><cell>Input</cell></row></table><note>A [MASK] news:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Dataset statistics. TC is for topic classification and ET is for entity typing.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">#Class #Test</cell></row><row><cell cols="2">AG's News TC</cell><cell>4</cell><cell>7,600</cell></row><row><cell>DBPedia</cell><cell>TC</cell><cell>14</cell><cell>70,000</cell></row><row><cell>Yahoo</cell><cell>TC</cell><cell>10</cell><cell>60,000</cell></row><row><cell>FewNERD</cell><cell>ET</cell><cell>66</cell><cell>96,901</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell cols="2">K Method</cell><cell>AG</cell><cell>DB</cell><cell>Yahoo Few</cell></row><row><cell cols="4">0 ManualVerb 75.13 67.06</cell><cell>43.11 20.00</cell></row><row><cell></cell><cell cols="4">ManualVerb 76.67 85.47 50.22 41.68</cell></row><row><cell>1</cell><cell cols="4">SearchVerb 41.50 60.06 27.39 20.88 SoftVerb 49.79 65.35 22.72 18.78</cell></row><row><cell></cell><cell>ProtoVerb</cell><cell cols="3">64.19 72.85 36.12 25.00</cell></row><row><cell></cell><cell cols="4">ManualVerb 81.06 93.61 58.65 46.44</cell></row><row><cell>2</cell><cell cols="4">SearchVerb 65.82 78.21 40.71 31.28 SoftVerb 56.37 80.69 30.72 32.80</cell></row><row><cell></cell><cell>ProtoVerb</cell><cell cols="3">77.34 85.49 46.30 35.72</cell></row><row><cell></cell><cell cols="4">ManualVerb 84.73 95.83 61.41 52.54</cell></row><row><cell>4</cell><cell cols="4">SearchVerb 77.43 86.40 51.58 43.10 SoftVerb 74.38 89.12 41.62 48.77</cell></row><row><cell></cell><cell>ProtoVerb</cell><cell cols="3">81.65 90.91 55.08 48.28</cell></row><row><cell></cell><cell cols="4">ManualVerb 85.85 96.46 64.12 56.59</cell></row><row><cell>8</cell><cell cols="4">SearchVerb 82.17 88.41 58.64 50.78 SoftVerb 79.35 93.69 46.82 53.78</cell></row><row><cell></cell><cell>ProtoVerb</cell><cell cols="3">84.03 95.75 61.40 56.06</cell></row><row><cell></cell><cell cols="4">ManualVerb 84.74 96.05 58.77 61.17</cell></row><row><cell>16</cell><cell cols="4">SearchVerb 83.40 92.00 59.66 55.49 SoftVerb 80.57 86.90 58.20 58.87</cell></row><row><cell></cell><cell>ProtoVerb</cell><cell cols="3">84.48 96.30 64.35 61.29</cell></row></table><note>Results for single verbalizer experiments. We report the mean accuracy scores (%) over 3 random seeds. Italic: results with task-specific knowledge. Bold: best results without task-specific knowledge.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Results for multiple verbalizer experiments. We report the mean accuracy scores (%) over 3 random seeds. ProtoVerb+ w/o tuning: apply ProtoVerb to untuned PLMs. Bold: best results.</figDesc><table><row><cell>presents the performance of different ver-</cell></row><row><cell>balizers. Overall, ManualVerb is the most powerful</cell></row><row><cell>verbalizer, which is reasonable because it is picked</cell></row><row><cell>by human with domain knowledge. ProtoVerb out-</cell></row><row><cell>performs SearchVerb and SoftVerb remarkably and</cell></row><row><cell>consistently, especially when only 1 or 2 instances</cell></row><row><cell>per class are given. The poor performances of the</cell></row><row><cell>two baselines under extreme data scarcity corrobo-</cell></row><row><cell>rate the issues we claim in  § 1. As the training data</cell></row><row><cell>become sufficient, ProtoVerb gets comparable or</cell></row><row><cell>even exceeding scores compared with ManualVerb,</cell></row><row><cell>showing that ProtoVerb is able to learn prototypes</cell></row><row><cell>that well represent the classes. At the same time,</cell></row><row><cell>the gaps between ManualVerb and other verbalizers</cell></row><row><cell>narrow, which also indicates that we can summa-</cell></row><row><cell>rize data across various ways.</cell></row><row><cell>Across tasks, ProtoVerb gets better results on</cell></row><row><cell>topic classification than entity typing. A possible</cell></row><row><cell>reason is that FewNERD is a fine-grained entity typ-</cell></row><row><cell>ing dataset, in which the differences across classes</cell></row><row><cell>are subtle. For example, it is hard for ProtoVerb</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>could exploit training data better with PLMs fixed. Ablation study of ProtoVerb on AG's News.</figDesc><table><row><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>70</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Acc</cell><cell>40 50 60</cell><cell></cell><cell></cell><cell></cell><cell>SearchVerb SoftVerb ProtoVerb ManualVerb</cell></row><row><cell></cell><cell>1</cell><cell>2</cell><cell></cell><cell>4 Shot</cell><cell>8</cell><cell>16</cell></row><row><cell cols="6">Figure 3: Experiment results with fixed PLMs. We re-</cell></row><row><cell cols="6">port the mean accuracy (%) with 95% confidence inter-</cell></row><row><cell cols="3">val on AG's News.</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Method</cell><cell cols="3">K = 2 K = 4 K = 8</cell></row><row><cell></cell><cell cols="2">L ins + L proto</cell><cell></cell><cell>77.34</cell><cell>81.65</cell><cell>84.03</cell></row><row><cell></cell><cell>L proto</cell><cell></cell><cell></cell><cell>76.37</cell><cell>81.06</cell><cell>82.91</cell></row><row><cell></cell><cell cols="4">Instance Mean 73.36</cell><cell>77.76</cell><cell>82.57</cell></row><row><cell cols="6">Instance Mean: using the mean embeddings of in-</cell></row><row><cell cols="6">stances as prototype embeddings. Bold: best results</cell></row><row><cell></cell><cell cols="2">K Method</cell><cell></cell><cell cols="2"># Noisy Samples</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell></row><row><cell></cell><cell></cell><cell cols="4">SearchVerb 4.86 5.96 5.19</cell></row><row><cell></cell><cell>8</cell><cell>SoftVerb</cell><cell></cell><cell cols="2">4.84 7.80 11.71</cell></row><row><cell></cell><cell></cell><cell cols="2">ProtoVerb</cell><cell cols="2">2.34 3.11 4.37</cell></row><row><cell></cell><cell></cell><cell cols="4">SearchVerb 0.80 2.93 5.18</cell></row><row><cell></cell><cell>16</cell><cell>SoftVerb</cell><cell></cell><cell cols="2">2.01 4.17 4.58</cell></row><row><cell></cell><cell></cell><cell cols="2">ProtoVerb</cell><cell cols="2">0.04 2.13 3.16</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Accuracy drop (%) with noisy samples. Lower is better. Bold: best results.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>shows the</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Words that are most similar with prototypes of each class on AG's News.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work is supported by the National Key R&amp;D Program of China (No. 2020AAA0106502), Institute for Guo Qiang at Tsinghua University, Beijing Academy of Artificial Intelligence (BAAI), and International Innovation Center of Tsinghua University, Shanghai, China. Ganqu Cui and Shengding Hu conducted the experiments. Ganqu Cui, Shengding Hu, Ning Ding and Zhiyuan Liu wrote the paper. Longtao Huang provided valuable advices to the research.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethical Considerations</head><p>Our work explores how to stimulate large PLMs with few samples. We conduct experiments under the few-shot setting, where requires less training time and fewer resources than normal full-data setting. Also, we open up our codes and hyperparameters to facilitate future reproduction without repeated energy cost.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Bragg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.07170</idno>
		<title level="m">Flex: Unifying evaluation for few-shot nlp</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ilya Sutskever, and Dario Amodei</title>
				<meeting><address><addrLine>Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam Mc-Candlish, Alec Radford</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Proceedings of NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Commonsense knowledge mining from pretrained models</title>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1109</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-IJCNLP</title>
				<meeting>EMNLP-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1173" to="1178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
				<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Prompt-learning for fine-grained entity typing</title>
		<author>
			<persName><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangwei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengjun</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong-Gee</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.10604</idno>
		<imprint>
			<date type="published" when="2021">2021a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Openprompt: An open-source framework for prompt-learning</title>
		<author>
			<persName><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengding</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.01998</idno>
		<imprint>
			<date type="published" when="2021">2021b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Prototypical representation learning for relation extraction</title>
		<author>
			<persName><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaobin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangwei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengjun</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021c</date>
		</imprint>
	</monogr>
	<note>In Proceedings of ICLR</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Few-nerd: A few-shot named entity recognition dataset</title>
		<author>
			<persName><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangwei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaobin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengjun</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haitao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2021">2021d</date>
			<biblScope unit="page" from="3198" to="3213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Making pre-trained language models better few-shot learners</title>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3816" to="3830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hybrid attention-based prototypical networks for noisy few-shot relation classification</title>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v33i01.33016407</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
				<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6407" to="6414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">WARP: word-level adversarial reprogramming</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Hambardzumyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hrant</forename><surname>Khachatrian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2021-05">May. 2021</date>
			<biblScope unit="page" from="4921" to="4933" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Pretrained models: Past, present and future</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxian</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqi</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021a</date>
			<publisher>AI Open</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.11259</idno>
		<title level="m">Ptr: Prompt tuning with rules for text classification</title>
				<imprint>
			<date type="published" when="2021">2021b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Universal language model fine-tuning for text classification</title>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.06146</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Knowledgeable prompt-tuning: Incorporating knowledge into prompt verbalizer for text classification</title>
		<author>
			<persName><forename type="first">Shengding</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huadong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.02035</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
				<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Text classification algorithms: A survey</title>
		<author>
			<persName><forename type="first">Kamran</forename><surname>Kowsari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiana</forename><forename type="middle">Jafari</forename><surname>Meimandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mojtaba</forename><surname>Heidarysafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjana</forename><surname>Mendu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">150</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Isele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Jakob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anja</forename><surname>Jentzsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Kontokostas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><forename type="middle">N</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Hellmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Morsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Van Kleef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sören</forename><surname>Auer</surname></persName>
		</author>
		<title level="m">Dbpedia-a large-scale, multilingual knowledge base extracted from wikipedia. Semantic web</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="167" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The power of scale for parameter-efficient prompt tuning</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Prototypical contrastive learning of unsupervised representations</title>
		<author>
			<persName><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
				<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Prefix-tuning: Optimizing continuous prompts for generation</title>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.353</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL/IJCNLP</title>
				<meeting>ACL/IJCNLP</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4582" to="4597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujie</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10385</idno>
		<title level="m">Gpt understands, too</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter ; Adam Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Köpf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR. Aaron van den Oord, Yazhe Li, and Oriol Vinyals</title>
				<editor>
			<persName><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</editor>
		<meeting>ICLR. Aaron van den Oord, Yazhe Li, and Oriol Vinyals</meeting>
		<imprint>
			<date type="published" when="2018">2019. 2018. 2019</date>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Language models as knowledge bases?</title>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Miller</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1250</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-IJCNLP</title>
				<meeting>EMNLP-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2463" to="2473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Automatically identifying words that can serve as labels for few-shot text classification</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helmut</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
				<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5569" to="5578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Exploiting cloze-questions for few-shot text classification and natural language inference</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
				<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="255" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Autoprompt: Eliciting knowledge from language models with automatically generated prompts</title>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasaman</forename><surname>Razeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4222" to="4235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
				<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Trieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.02847</idno>
		<title level="m">A simple method for commonsense reasoning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><surname>Gugger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<editor>
			<persName><forename type="first">Mariama</forename><surname>Drame</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Quentin</forename><surname>Lhoest</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</editor>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Differentiable prompt makes pre-trained language models better few-shot learners</title>
		<author>
			<persName><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luoqiu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.13161</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
				<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
