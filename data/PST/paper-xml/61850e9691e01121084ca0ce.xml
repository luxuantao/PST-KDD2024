<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-supervised Learning for Large-scale Item Recommendations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-02-25">25 Feb 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tiansheng</forename><surname>Yao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc</orgName>
								<address>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xinyang</forename><surname>Yi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc</orgName>
								<address>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Derek</forename><forename type="middle">Zhiyuan</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc</orgName>
								<address>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Felix</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc</orgName>
								<address>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc</orgName>
								<address>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aditya</forename><surname>Menon</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc</orgName>
								<address>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lichan</forename><surname>Hong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc</orgName>
								<address>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc</orgName>
								<address>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Steve</forename><surname>Tjoa</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc</orgName>
								<address>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jay</forename><surname>Kang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc</orgName>
								<address>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Evan</forename><surname>Ettinger</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc</orgName>
								<address>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Self-supervised Learning for Large-scale Item Recommendations</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-02-25">25 Feb 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2007.12865v4[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large scale recommender models find most relevant items from huge catalogs, and they play a critical role in modern search and recommendation systems. To model the input space with large-vocab categorical features, a typical recommender model learns a joint embedding space through neural networks for both queries and items from user feedback data. However, with millions to billions of items in the corpus, users tend to provide feedback for a very small set of them, causing a power-law distribution. This makes the feedback data for long-tail items extremely sparse.</p><p>Inspired by the recent success in self-supervised representation learning research in both computer vision and natural language understanding, we propose a multi-task self-supervised learning (SSL) framework for large-scale item recommendations. The framework is designed to tackle the label sparsity problem by learning better latent relationship of item features. Specifically, SSL improves item representation learning as well as serving as additional regularization to improve generalization. Furthermore, we propose a novel data augmentation method that utilizes feature correlations within the proposed framework.</p><p>We evaluate our framework using two real-world datasets with 500M and 1B training examples respectively. Our results demonstrate the effectiveness of SSL regularization and show its superior performance over the state-of-the-art regularization techniques. We also have already launched the proposed techniques to a web-scale commercial app-to-app recommendation system, with significant improvements top-tier business metrics demonstrated in A/B experiments on live traffic. Our online results also verify our hypothesis that our framework indeed improves model performance even more on slices that lack supervision.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recently, neural-net models have emerged to the main stage of modern recommendation systems throughout the industry (see, e.g., <ref type="bibr" target="#b25">[18,</ref><ref type="bibr" target="#b43">31,</ref><ref type="bibr" target="#b54">39,</ref><ref type="bibr" target="#b58">42]</ref>), and academia ( <ref type="bibr" target="#b9">[8,</ref><ref type="bibr" target="#b44">32]</ref>). Compared to conventional approaches like matrix factorization <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b30">21,</ref><ref type="bibr" target="#b31">22]</ref>, gradient boosted decision trees <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b40">29]</ref>, and logistic regression based recommenders <ref type="bibr" target="#b27">[19]</ref>, these deep models handle categorical features more effectively. They also enable more complex data representations, and introduce more non-linearity to better fit the complex data for recommenders.</p><p>A particular recommendation task we focus on in this paper is to identify the most relevant items given a query from a huge item catalog. This general problem of large-scale item recommendations has been widely adopted in various applications. Depending on the type of the query, a recommendation task could be: (i) personalized recommendation: when the query is a user; (ii) item to item recommendation: when the query is also an item; and (iii) search: when the query is a piece of free text. To model the interactions between a query and an item, a well-known approach leverages embedding-based neural networks. The recommendation task is typically formulated as an extreme classification problem <ref type="bibr" target="#b13">[10]</ref> where each item is represented as a dense vector in the output space.</p><p>This paper is focused on the two-tower DNNs (see Figure <ref type="figure" target="#fig_0">1</ref>), popular in many real-world recommenders (see e.g. <ref type="bibr" target="#b32">[23,</ref><ref type="bibr" target="#b54">39]</ref>). In this architecture, a neural network encodes a set of item features into an embedding thus making it applicable even for indexing cold-start items. Moreover, the two-tower DNN architecture enables efficient serving for a large corpus of items in real time, by converting the top-k nearest neighbor search problem to Maximum-Inner-Product-Search (MIPS) <ref type="bibr" target="#b11">[9]</ref> that is solvable in sublinear complexity.</p><p>Embedding-based deep models typically have large amount of parameters because they are built with high-dimensional embeddings that represent high cardinality sparse features such as topics or item IDs. In many existing literature, the loss functions for training these models are formulated as a supervised learning problem. The supervision comes from the collected labels (e.g., clicks). Modern recommendation systems collect billions to trillions of footprints from users, providing huge amount of training data for building deep models. However, when it comes to modeling a huge catalogue of items in the order of millions (e.g., songs and apps <ref type="bibr" target="#b39">[28]</ref>) to even billions (e.g., videos on YouTube <ref type="bibr" target="#b13">[10]</ref>), data could still be highly sparse for certain slices due to:</p><p>â€¢ Highly-skewed data distribution: The interaction between queries and items are often highly skewed in a power-law distribution <ref type="bibr" target="#b42">[30]</ref>. So a small set of the popular items gets most of the interactions. This will always leave the training data for long-tail items very sparse.</p><p>â€¢ Lack of explicit user feedback: Users often provide lots of positive feedback implicitly like clicks and thumb-ups. However, they are much less likely to provide explicit feedback like item ratings, feedback for user happiness, and relevance scores.</p><p>Self-supervised learning (SSL) offers a different angle to improve deep representation learning via unlabeled data. The basic idea is to enhance training data with various data augmentations, and supervised tasks to predict or reconstitute the original examples as auxiliary tasks. Self-supervised learning has been widely used in the areas of Compute Vision (CV) <ref type="bibr" target="#b21">[15,</ref><ref type="bibr" target="#b34">25,</ref><ref type="bibr" target="#b46">33]</ref> and Natural Language Understanding (NLU) <ref type="bibr" target="#b17">[12,</ref><ref type="bibr" target="#b33">24]</ref>. An example work <ref type="bibr" target="#b34">[25]</ref> in CV proposed to rotate images at random, and train a model to predict how each augmented input image was rotated. In NLU, masked language task was introduced in the BERT model, to help improve pre-training of language models. Similarly, other pre-training tasks like predicting surrounding sentences and linked sentences in Wikipedia articles have also been used in improving dual-encoder type models in NLU <ref type="bibr" target="#b2">[3]</ref>. Compared to conventional supervised learning, self-supervised learning provides complementary objectives eliminating the prerequisite of collecting labels manually. In addition, SSL enables autonomous discovery of good semantic representations by exploiting the internal relationship of input features.</p><p>Despite the wide adoption in computer vision and natural language understanding, the application of self-supervised learning in the field of recommendation systems is less well studied. The closest line of research studies a set of regularization techniques <ref type="bibr" target="#b24">[17,</ref><ref type="bibr" target="#b32">23,</ref><ref type="bibr" target="#b56">41]</ref>, which are designed to force learned representations (i.e., output layer (embeddings) of a multi-layer perception), of different examples to be farther away from each other, and spread out in the entire latent embedding space. Although sharing similar spirit with SSL, these techniques do not explicitly construct SSL tasks. In contrast to models in CV or NLU applications, recommendation model takes extremely sparse input where high cardinality categorical features are one-hot (or multi-hot) encoded, such as the item IDs or item categories <ref type="bibr" target="#b43">[31]</ref>. These features are typically represented as learnable embedding vectors in deep models. As most models in computer vision and NLU deal with dense input, existing methods for creating SSL tasks are not directly applicable to the sparse models in recommendation systems. More recently, a line of research studies self-supervised learning improving sequential user modeling in recommendations <ref type="bibr" target="#b37">[27,</ref><ref type="bibr" target="#b52">37,</ref><ref type="bibr" target="#b60">43]</ref>. Different from these works, this paper focuses on item representation learning, and shows how SSL can help improve generalization in the context of long-tail item distribution. Moreover, in contrast to using SSL on a certain sequential user feature, we design new SSL tasks and demonstrate their effectiveness for learning with a set of heterogeneous categorical features, which we believe is a more general setup for other types of recommendation models such as multitask ranking models (e.g., <ref type="bibr" target="#b58">[42]</ref>).</p><p>In this paper, we propose to leverage self-supervised learning based auxiliary tasks to improve item representations, especially with long-tail distributions and sparse data. Different from CV or NLU applications, input space of recommendation model is highly sparse and represented by a set of categorical features (e.g. item ids) with large cardinality. For such sparse models, we propose a new SSL framework, where the key idea is to: (i) augment data by masking input information; (ii) encode each pair of augmented examples by a two-tower DNN; and (iii) apply a contrastive loss to learn representations of augmented data. The goal of contrastive learning is to let augmented data from the same example be discriminated against others. Note that the two-tower DNN for contrastive learning and the one for encoding query and item can share a certain amount of model parameters. See more details in Section 3.</p><p>Our contribution is four-fold:</p><p>â€¢ SSL framework: We present a model architecture agnostic self-supervised learning framework for sparse neural models in recommendations. The auxiliary self-supervised loss and the primary supervised loss are jointly optimized via a multi-task learning framework. We focus on using this framework for efficiently scoring a large corpus of items, which is also known as item retrieval in two-stage recommenders <ref type="bibr" target="#b13">[10]</ref>. We believe it would also shed light on designing SSL for other types of models such as ranking models <ref type="bibr" target="#b7">[7]</ref>.</p><p>â€¢ Data augmentation: We propose a novel data augmentation method that exploits feature correlations and are tailored for heterogeneous categorical features that are common in recommender models.</p><p>â€¢ Offline experiments: On one public dataset and one industry scale dataset for recommendation systems, we demonstrate that introducing SSL as an auxiliary task can significantly improve model performance, especially when labels are scarce. Comparing to the state-of-art non-SSL regularization techniques <ref type="bibr" target="#b24">[17,</ref><ref type="bibr" target="#b32">23,</ref><ref type="bibr" target="#b56">41]</ref>, we demonstrate that SSL consistently performs better, and improves model performance even when non-SSL regularization does not bring any additional gains.</p><p>â€¢ Live experiment in a web-scale recommender: We have launched the proposed SSL technique in a fairly strong twotower app-to-app recommendation model in a large-scale realworld system. Live A/B testing shows significantly improved top-tier metrics. We especially see bigger improvements for slices without much supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Self-supervised Learning and Pre-training. Various unsupervised and self-supervised learning tasks have been studied in the computer vision community. The closest line of research is SimCLR <ref type="bibr" target="#b5">[5]</ref> which also utilizes self-supervised learning and contrastive learning for visual representation learning. Different with SimCLR and other works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">6]</ref> in vision, here we propose augmentations that are more suitable and tailed for categorical features for recommendations, instead of relying on image-specific augmentations such as image cropping, rotation and color distortion. In addition, the proposed framework does not require multi-stage training schedules (such as pre-training then fine-tuning). <ref type="bibr" target="#b28">[20]</ref>.</p><p>In NLU, for dual-encoder models, <ref type="bibr" target="#b2">[3]</ref> shows that pre-training tasks better aligned with the final task are more helpful than generic tasks such as next sentence prediction and masked-LM. The pretraining tasks are designed to leverage large-scale public NLU content, such as Wikipedia. In this paper, we also use the dual-encoder model architecture. Different from the above, the proposed selfsupervision tasks do not require the use of a separate data source.</p><p>Spread-out Regularization. Zhang et al. <ref type="bibr" target="#b56">[41]</ref> and Wu et al. <ref type="bibr" target="#b51">[36]</ref> use spread-out regularization for improving generalization of deep models. Specifically, in <ref type="bibr" target="#b56">[41]</ref>, a regularization promoting separation between random instances is shown to improve training stability and generalization. In <ref type="bibr" target="#b51">[36]</ref>, one objective is to train a classifier treating each instance as its own class, therefore promoting large instance separations in the embedding space. Both the above approaches are studied for computer vision applications.</p><p>Neural Recommenders. Deep learning has led to many successes in building industry-scale recommender systems such as video suggestion <ref type="bibr" target="#b13">[10]</ref>, news recommendation <ref type="bibr" target="#b48">[34]</ref>, and visual discovery in social networks <ref type="bibr" target="#b36">[26,</ref><ref type="bibr" target="#b55">40]</ref>. For large-scale item retrieval, two-tower models with separate query and item towers are widely used due to its high efficiency for serving. The recommendations are typically computed by a dot-product between query and item embeddings so that finding top-k items can be converted to MIPS problem <ref type="bibr" target="#b11">[9]</ref> with sublinear time complexity. One popular factorized structure is softmax-based multi-class classification model. The work in <ref type="bibr" target="#b13">[10]</ref> treats the retrieval task as an extreme multi-class classification trained with multi-layer perceptron (MLP) model using sampled softmax as loss function. Such models leverages item ID as the only item feature, suffering from cold-start issue. More recently, a line of research <ref type="bibr" target="#b32">[23,</ref><ref type="bibr" target="#b54">39]</ref> considers applying two-tower DNNs on retrieval problems, which is also known as dual-encoder <ref type="bibr" target="#b23">[16,</ref><ref type="bibr" target="#b53">38]</ref>, where item embeddings are constructed by a MLP from ID and other categorical metadata features. The self-supervised approach proposed is applicable to both ranking and retrieval models. In this paper we focus on using SSL for retrieval models, particularly, on improving item representations in two-tower DNNs.</p><p>Self-supervised Learning in Sequential Recommendations. In recommender systems, a line of research has been recently studied for utilizing self-supervised learning for sequential recommendation. Self-supervised learning tasks are designed to capture information among user history <ref type="bibr" target="#b60">[43]</ref> and learn more robust disentangled user representation <ref type="bibr" target="#b37">[27]</ref> in user sequential recommendation. Moreover, Xin et al. shows combining SSL with reinforcement learning is effective to capture long-term user interest in sequential recommendation. Different from the above, our proposed SSL framework is focusing on improving item representation with long-tail distributions. The proposed SSL tasks do not require modeling sequential information and are generally applicable to deep models with heterogeneous categorical features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>We present our framework of self-supervised learning for deep neural net models for recommenders using large-vocab categorical features. Particularly, a general self-supervised learning framework is introduced in Section 3.1. In Section 3.2, we present a data augmentation method to construct SSL tasks and elaborates on their connections to spread-out regularization. Finally, in Section 3.3, we describe how to use SSL to improve factorized models (i.e., two-tower DNNs as shown in Figure <ref type="figure" target="#fig_0">1</ref>), via a multi-task learning framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Framework</head><p>Inspired by the SimCLR framework <ref type="bibr" target="#b5">[5]</ref> for visual representation learning, we adopt similar contrastive learning algorithms for learning representations of categorical features. The basic idea is two folds: first, we apply different data augmentation for the same training example to learn representations; and then use contrastive loss function to encourage the representations learned for the same training example to be similar. Contrastive loss was also applied in training two-tower DNNs (see e.g., <ref type="bibr" target="#b32">[23,</ref><ref type="bibr" target="#b54">39]</ref>), although the goal there was to make positive item agree with its corresponding queries. ğ‘– while minimizing the similarity between z ğ‘— and z â€² ğ‘— . We consider a batch of ğ‘ item examples ğ‘¥ 1 , ..., ğ‘¥ ğ‘ , where ğ‘¥ ğ‘– âˆˆ X represents a set of features for example ğ‘–. In the context of recommenders, an example indicates a query, an item or a query-item pair. Suppose there are a pair of transform function â„, ğ‘” : X â†’ X that augment ğ‘¥ ğ‘– to be ğ‘¦ ğ‘– , ğ‘¦ â€² ğ‘– respectively,</p><formula xml:id="formula_0">ğ‘¦ ğ‘– â† â„(ğ‘¥ ğ‘– ), ğ‘¦ â€² ğ‘– â† ğ‘”(ğ‘¥ ğ‘– ).<label>(1)</label></formula><p>Given the same input of example ğ‘–, we want to learn different representations ğ‘¦ ğ‘– , ğ‘¦ â€² ğ‘– after augmentation to make sure the model still recognizes that both ğ‘¦ ğ‘– and ğ‘¦ ğ‘– represent the same input ğ‘–. In other words, the contrastive loss learns to minimize the difference between ğ‘¦ ğ‘– , ğ‘¦ â€² ğ‘– . In the mean time, for different example ğ‘– and ğ‘—, the contrastive loss maximizes the difference between the representations learned ğ‘¦ ğ‘– , ğ‘¦ â€² ğ‘— after data different augmentations. Let z ğ‘– , z â€² ğ‘– denote the embeddings of ğ‘¦ ğ‘– , ğ‘¦ â€² ğ‘– after encoded by two neural networks H, G : X â†’ R ğ‘‘ , that is</p><formula xml:id="formula_1">z ğ‘– â† H (ğ‘¦ ğ‘– ), z â€² ğ‘– â† G(ğ‘¦ â€² ğ‘– ).<label>(2)</label></formula><p>We treat (z ğ‘– , z â€² ğ‘– ) as positive pairs, and</p><formula xml:id="formula_2">(z ğ‘– , z â€² ğ‘— ) as negative pairs for ğ‘– â‰  ğ‘—. Let ğ‘  (z ğ‘– , z â€² ğ‘— ) = âŸ¨z ğ‘– , z â€² ğ‘— âŸ©/(âˆ¥z ğ‘– âˆ¥ â€¢ âˆ¥z â€² ğ‘— âˆ¥).</formula><p>To encourage the above properties, we define the SSL loss for a batch of ğ‘ examples {ğ‘¥ ğ‘– } as:</p><formula xml:id="formula_3">L ğ‘ ğ‘’ğ‘™ ğ‘“ ({ğ‘¥ ğ‘– }; H, G) := âˆ’ 1 ğ‘ âˆ‘ï¸ ğ‘– âˆˆ [ğ‘ ] log exp (ğ‘  (z ğ‘– , z â€² ğ‘– )/ğœ) ğ‘— âˆˆ [ğ‘ ] exp (ğ‘  (z ğ‘– , z â€² ğ‘— )/ğœ) . (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>where ğœ is a tunable hyper-parameter for the softmax temperature. The above loss function learns a robust embedding space such that similar items are close to each other after data augmentation, and random examples are pushed farther away. The overall framework is illustrated in Figure <ref type="figure" target="#fig_1">2</ref>.</p><p>Encoder Architecture. For input examples with categorical features, H, G are typically constructed with an input layer and a multi-layer perceptron (MLP) on top. The input layer is often a concatenation of normalized dense features and multiple sparse feature embeddings, where the sparse feature embeddings are learnt representations stored in embedding tables (In contrast, the input layers for computer vision and language models directly work on raw inputs). In order to make SSL facilitate the supervised learning task, we share the embedding table of sparse features for both neural networks H, G. Depending on the technique for data augmentation (â„, ğ‘”), the MLPs of H and G could also be fully or partially shared.</p><p>Connection with Spread-out Regularization <ref type="bibr" target="#b56">[41]</ref>. In the special case where (â„, ğ‘”) are identical map and H, G are the same neural network, loss function in equation ( <ref type="formula" target="#formula_3">3</ref>) is then reduced to</p><formula xml:id="formula_5">âˆ’ğ‘ âˆ’1 âˆ‘ï¸ ğ‘– log ğ‘’ğ‘¥ğ‘ (1/ğœ) ğ‘’ğ‘¥ğ‘ (1/ğœ) + ğ‘—â‰ ğ‘– ğ‘’ğ‘¥ğ‘ (ğ‘  (z ğ‘– , z ğ‘— )/ğœ)</formula><p>which encourages learned representations of different examples to have small cosine similarity. The loss is similar to the spread-out regularization introduced in <ref type="bibr" target="#b56">[41]</ref>, except that the original proposal uses square loss, i.e., ğ‘ âˆ’1 ğ‘– ğ‘—â‰ ğ‘– âŸ¨z ğ‘– , z ğ‘— âŸ© 2 , instead of softmax. Spreadout regularization has been proven to improve generalization of large-scale retrieval models. In Section 4, we show that by introducing specific data augmentations, using SSL-based regularization can further improve model performance compared to spread-out.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">A Two-stage Data Augmentation</head><p>We introduce the data augmentation, i.e., â„ and ğ‘” in Figure <ref type="figure" target="#fig_1">2</ref>. Given a set of item features, the key idea is to create two augmented examples by masking part of the information. A good transformation and data augmentation should make minimal amount of assumptions on the data such that it can be generally applicable to a large variety of tasks and models. The idea of masking is inspired by the Masked Language Modeling in BERT <ref type="bibr" target="#b17">[12]</ref>. Different from sequential tokens, a general set of features does not have sequential order, and leaves the choice of masking pattern as an open question. We seek to design the masking pattern by exploring feature correlation. We propose Correlated Feature Masking (CFM), tailored to categorical features with awareness of feature correlations.</p><p>Before diving into the details of masking, we first present a twostage augmentation algorithm. Note that without augmentation, the input layer is created by concatenating the embeddings of all categorical features. The two-stage augmentation includes:</p><p>â€¢ Masking. Apply a masking pattern on the set of item features.</p><p>We use a default embedding in the input layer to represent the features that are masked.</p><p>â€¢ Dropout. For categorical features with multiple values, we drop out each value with a certain probability. It further reduces input information and increase the hardness of SSL task.</p><p>The masking step can be interpreted as a special case of dropout with a 100% dropout rate. One strategy is the complementary masking pattern, that we split the feature set into two exclusive features sets into the two augmented examples. Specifically, we could randomly split the feature set into two disjoint subsets. We call this method Random Feature Masking (RFM), and will use it as one of our baselines. We now introduce Correlated Feature Masking (CFM) where we further explore the feature correlations when creating masking patterns.</p><p>Mutual Information of Categorical Features. If the set of masked features are chosen at random, (â„, ğ‘”) are essentially sampled from 2 ğ‘˜ different masking patterns over the whole feature set with ğ‘˜ features. Different masking patterns would naturally lead to different effects for the SSL task. For instance, the SSL contrastive learning task may exploit the shortcut of highly correlated features between the two augmented examples, making the SSL task too easy. To address this issue, we propose to split features according to feature correlation measured by mutual information. The mutual information of two categorical features is given by</p><formula xml:id="formula_6">ğ‘€ğ¼ (ğ‘‰ ğ‘– , ğ‘‰ ğ‘— ) = âˆ‘ï¸ ğ‘£ ğ‘– âˆˆğ‘‰ ğ‘– ,ğ‘£ ğ‘— âˆˆğ‘‰ ğ‘— ğ‘ƒ (ğ‘£ ğ‘– , ğ‘£ ğ‘— ) log ğ‘ƒ (ğ‘£ ğ‘– , ğ‘£ ğ‘— ) ğ‘ƒ (ğ‘£ ğ‘– )ğ‘ƒ (ğ‘£ ğ‘— ) ,<label>(4)</label></formula><p>where ğ‘‰ ğ‘– , ğ‘‰ ğ‘— denote their vocab sets. The mutual information for all pairs of features can be pre-computed.</p><p>Correlated Feature Masking. With the pre-computed mutual information, we propose Correlated Feature Masking (CFM) that exploits the feature-dependency patterns for more meaningful SSL tasks. For the set of masked features, ğ¹ ğ‘š , we seek to mask highly correlated features together. We do so by first uniformly sample a seed feature ğ‘“ ğ‘ ğ‘’ğ‘’ğ‘‘ from all the available features ğ¹ = {ğ‘“ 1 , ..., ğ‘“ ğ‘˜ }, and then select the top-n most correlated features ğ¹ ğ‘ = {ğ‘“ ğ‘,1 , ..., ğ‘“ ğ‘,ğ‘› } according to their mutual information with ğ‘“ ğ‘ ğ‘’ğ‘’ğ‘‘ . The final ğ¹ ğ‘š will be the union of the seed and set of correlated features, i.e., ğ¹ ğ‘š = {ğ‘“ ğ‘ ğ‘’ğ‘’ğ‘‘ , ğ‘“ ğ‘,1 , ..., ğ‘“ ğ‘,ğ‘› }. We choose ğ‘› = âŒŠğ‘˜/2âŒ‹ so that the masked and retained set of features have roughly the same size. We change the seed feature per batch so that the SSL task will learn on various kinds of masking patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multi-task Training</head><p>To enable SSL learned representations to help improve the learning for the main supervised task such as regression or classification, we leverage a multi-task training strategy where the main supervised task and the auxiliary SSL task are jointly optimized. Precisely, let {(ğ‘ ğ‘– , ğ‘¥ ğ‘– )} be a batch of query-item pairs sampled from the training data distribution D ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› , and let {ğ‘¥ ğ‘– } be a batch of items sampled from an item distribution D ğ‘–ğ‘¡ğ‘’ğ‘š . Then the joint loss is:</p><formula xml:id="formula_7">L = L ğ‘šğ‘ğ‘–ğ‘› {(ğ‘ ğ‘– , ğ‘¥ ğ‘– )} + ğ›¼ â€¢ L ğ‘ ğ‘’ğ‘™ ğ‘“ {ğ‘¥ ğ‘– } ,<label>(5)</label></formula><p>where L ğ‘šğ‘ğ‘–ğ‘› is the loss function for the main task capturing interaction between query and item, and ğ›¼ is the regularization strength.</p><p>Heterogeneous Sample Distributions. The marginal item distribution from D ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› typically follows a power-law. Therefore, using the training item distribution for L ğ‘ ğ‘’ğ‘™ ğ‘“ would cause the learned feature relationship to be biased towards head items. Instead, we sample items uniformly from the corpus for L ğ‘ ğ‘’ğ‘™ ğ‘“ . In other words, D ğ‘–ğ‘¡ğ‘’ğ‘š is the uniform item distribution. In practice, we find using the heterogeneous distributions for main and ssl tasks is critical for SSL to achieve superior performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss for Main</head><p>Task. There could be many choices for the main loss depending on the objectives. In this paper, we consider the batch softmax loss used in both recommenders <ref type="bibr" target="#b54">[39]</ref> and NLP <ref type="bibr" target="#b23">[16]</ref> for optimizing top-k accuracy. In detail, let q ğ‘– , x ğ‘– be the embeddings of query and item examples (ğ‘ ğ‘– , ğ‘¥ ğ‘– ) after being encoded by two neural networks, then for a batch of pairs {(ğ‘ ğ‘– , ğ‘¥ ğ‘– )} ğ‘ ğ‘–=1 and temperature ğœ, the batch softmax cross entropy loss is In the SSL task, we apply feature masking and dropout on the item features to learn item embeddings. The whole item tower (in red) is shared with the supervised task.</p><formula xml:id="formula_8">L ğ‘šğ‘ğ‘–ğ‘› = âˆ’ 1 ğ‘ âˆ‘ï¸ ğ‘– âˆˆ [ğ‘ ] log exp (ğ‘  (q ğ‘– , x ğ‘– )/ğœ) ğ‘— âˆˆ [ğ‘ ] exp (ğ‘  (q ğ‘– , x ğ‘— )/ğœ) .<label>(6)</label></formula><p>Other Baselines. As mentioned in Section 2, we use two-tower DNNs as the baseline model for main task. Two-tower model has the unique property of encoding item features compared to classic matrix factorization (MF) and classification models. While the latter two methods are also applicable to large-scale item retrieval, they only learn item embeddings based on IDs, and thus do not fit in our proposal of using SSL for exploiting item feature relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OFFLINE EXPERIMENTS</head><p>We provide empirical results to demonstrate the effectiveness of our proposed self-supervised framework both in academic public dataset and in actual large-scale recommendation products. The experiments are designed to answer the following research questions.</p><p>â€¢ RQ1: Does the proposed SSL Framework improve deep models for recommendations?</p><p>â€¢ RQ2: SSL is designed to improve primary supervised task through introduced SSL task on unlabeled examples. What is the impact of the amount of training data on the improvement from SSL?</p><p>â€¢ RQ3: How do the SSL parameters, i.e., loss multiplier ğ›¼ and dropout rate in data augmentation, affect model quality?</p><p>â€¢ RQ4: How does RFM perform compared to CFM? What is the benefit of leveraging feature correlations in data augmentation?</p><p>The above questions are addressed in order from Section 4.3 -4.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We conduct experiments on two large-scale datasets that both come with a rich set of item metadata features. We formulate their primary supervised task as an item-to-item recommendation problem to study the effects of SSL on training recommender (in this case, retrieval) models. See Appendix .1 for details about the statistics of these two datasets.</p><p>Wikipedia <ref type="bibr" target="#b19">[14]</ref>: The first dataset focuses on the problem of link prediction between Wikipedia pages. It consists of pairs of pages (ğ‘¥, ğ‘¦) âˆˆ ğœ’ Ã— ğœ’, where ğ‘¥ indicates a source page, and ğ‘¦ is a destination page linked from ğ‘¥. The goal is to predict the set of pages that are likely to be linked to a given source page from the whole corpus of web pages. Each page is represented by a feature vector ğ‘¥ = (ğ‘¥ ğ‘–ğ‘‘ , ğ‘¥ ğ‘›ğ‘”ğ‘Ÿğ‘ğ‘šğ‘  , ğ‘¥ ğ‘ğ‘ğ‘¡ğ‘  ), where all the features are categorical. Here, ğ‘¥ ğ‘–ğ‘‘ denotes the one-hot encoding of the page URL, ğ‘¥ ğ‘›ğ‘”ğ‘Ÿğ‘ğ‘šğ‘  denotes a bag-of-words representation of the set of n-grams of the page's title, and ğ‘¥ ğ‘ğ‘ğ‘¡ğ‘  denotes a bag-of-words representation of the categories that the page belongs to. We partitioned the dataset into training and evaluation using a (90%, 10%) split, following the same treatment in <ref type="bibr" target="#b32">[23]</ref> and <ref type="bibr" target="#b54">[39]</ref>.</p><p>App-to-App Install (AAI): The AAI dataset was collected on the app landing pages from a commercial mobile app store. On a particular app's (seed app) landing page, the app installs (candidate apps) from the section of recommended apps were collected. Each training example represents a pair of seed-candidate pairs denoted as (ğ‘¥ ğ‘ ğ‘’ğ‘’ğ‘‘ , ğ‘¥ ğ‘ğ‘ğ‘›ğ‘‘ğ‘–ğ‘‘ğ‘ğ‘¡ğ‘’ ) and their metadata features. The goal is to recommend highly similar apps given a seed app. This is also formulated as an item-to-item recommendation problem via a multi-class classification loss. Note that we only collect positive examples, i.e., ğ‘¥ ğ‘ğ‘ğ‘›ğ‘‘ğ‘–ğ‘‘ğ‘ğ‘¡ğ‘’ is an installed app from the landing page of ğ‘¥ ğ‘ ğ‘’ğ‘’ğ‘‘ . All the impressed recommended apps with no installs are all ignored since we consider them more like weak positives instead of negatives for building retrieval models. Each item (app) is represented by a feature vector x with the following features:</p><p>â€¢ id: Application id as a one-hot categorical feature.</p><p>â€¢ developer_name: Name of the app developer as a one-hot categorical feature.</p><p>â€¢ categories: Semantic categories of the app as a multi-hot categorical feature.</p><p>â€¢ title_unigram: Uni-grams of the app title as a multi-hot categorical feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiment Setup</head><p>Backbone Network. For the main task that predicts relevant items given the query, we use the two-tower DNN to encode query and items features (see Figure <ref type="figure" target="#fig_0">1</ref>) as the backbone network. The itemto-item recommendation problem is formalized as a multi-class classification problem, using the batch softmax loss presented in Equation ( <ref type="formula" target="#formula_8">6</ref>) as the loss function. For discussions of the choice of backbone network, we refer the readers to related sections in Section 2 and Section 3.3.</p><p>Hyper-parameters. For the backbone two-tower DNN, we search the set of hyper-parameters such as the learning rate, softmax temperature (ğœ) and model architecture that gives the highest Recall@50 on the validation set. Note that the training batch size in batch softmax is critical for model quality as it determines the number of negatives used for each positive item. Throughout this section, we use batch sizes 1024 and 4096 for Wikipedia and AAI respectively. We also tuned the number of hidden layers, hidden layer sizes and softmax temperature ğœ for the baseline models. For Wikipedia dataset, we use softmax temperature ğœ = 0.07, and â„ğ‘–ğ‘‘ğ‘‘ğ‘’ğ‘›_ğ‘™ğ‘ğ‘¦ğ‘’ğ‘Ÿğ‘  with sizes <ref type="bibr">[1024,</ref><ref type="bibr">128]</ref>. For AAI, we use ğœ = 0.06 and â„ğ‘–ğ‘‘ğ‘‘ğ‘’ğ‘›_ğ‘™ğ‘ğ‘¦ğ‘’ğ‘Ÿğ‘  <ref type="bibr">[1024,</ref><ref type="bibr">256]</ref>. Note that the dimension of last hidden layer is also the dimension of final query and item embeddings. All models are trained with Adagrad <ref type="bibr" target="#b18">[13]</ref> optimizer with learning rate 0.01.</p><p>We consider two SSL parameters: 1) the SSL loss multiplier ğ›¼ in equation ( <ref type="formula" target="#formula_7">5</ref>), and 2) the feature dropout rate, denoted as ğ‘‘ğ‘Ÿ , in the second phase of data augmentation (see Section 3.2). For each augmentation method (e.g., CFM, RFM), we conduct grid search of the two parameters by ranges ğ›¼ = [0.1, 0.3, 1.0, 3.0], ğ‘‘ğ‘Ÿ = [0.1, 0.2, ..., 0.9], and report the best result.</p><p>Evaluation. To evaluate the recommendation performance given a seed item, we compute and find the top ğ¾ items with the highest cosine similarity from the whole corpus and evaluate the quality based on the ğ¾ retrieved items. Note this is a relatively challenging task, given the sparsity of the dataset and large number of items in the corpus. We adopt popular standard metrics ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™@ğ¾ and mean average precision (ğ‘€ğ´ğ‘ƒ@ğ¾) to evaluate recommendation performance <ref type="bibr" target="#b25">[18]</ref>. For each configuration of experiment results, we ran the experiment 5 times and report the average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Effectiveness of SSL with Correlated</head><p>Feature Masking</p><p>To answer RQ1, we first evaluate the impact of SSL on model quality. We focus on using CFM followed by dropout as the data augmentation technique. We will show the superior performance of CFM over other variants in Section 4.5.</p><p>We consider three baseline methods:</p><p>â€¢ Baseline: Vanilla backbone network with the two-tower DNN architecture.</p><p>â€¢ Feature Dropout (FD) <ref type="bibr" target="#b50">[35]</ref>: Backbone model with random feature dropout on the item tower in the supervised learning task. The feature dropout on item features could be treated as data augmentation. FD does not have the additional SSL regularization compared to our approach.</p><p>â€¢ Spread-out Regularization (SO) <ref type="bibr" target="#b56">[41]</ref>: Backbone model with spreadout regularization on the item tower as a regularization. The SO regularization shares similar contrastive loss as that in our SSL framework. However, it applies contrastive learning on original examples without any data augmentation, and is thus different from our approach.</p><p>The latter two methods are chosen since they are (1) model-agnostic and scalable for industrial-size recommendation systems; (2) compatible with categorical sparse features for improving generalization. In addition, FD can be viewed as an ablation study to isolate the potential improvement from contrastive learning. Similarly, SO is included to isolate the improvement from feature augmentation. We observe that with full datasets (see Table <ref type="table" target="#tab_0">1</ref>), CFM consistently performs the best compared with non-SSL regularization techniques. On AAI, CFM out-performs the next best method by 8.69% relatively and on AAI by 3.98%. This helps answer RQ1 that the proposed SSL framework and tasks indeed improves model performance for recommenders. By comparing CFM with SO, it shows that the data augmentation is critical for the SSL regularization to have better performance. Without any data augmentation, the Wikipedia Method MAP@10 MAP@50 Recall@10 Recall@50 Baseline 0.0171 0.0229 0.0537 0.1930 FD <ref type="bibr" target="#b50">[35]</ref> 0.0172 0.0229 0.0535 0.1912 SO <ref type="bibr" target="#b56">[41]</ref> 0 proposed SSL method is reduced to SO. By comparing CFM and FD, we find the feature augmentation is more effective when applied to the SSL task than to the supervised task as a standard regularization technique. Note that FD, as a well known approach for improving generalization in some cases, applies feature augmentation together with supervised training.</p><p>Head-tail Analysis. To understand the gain from SSL, we further break down the overall performance by looking at different item slices by item popularity. The splitting of the head and tail test set is described in the appendix .2. Our hypothesis is that SSL generally helps improve the performance for slices without much supervision (e.g., tail items). The results evaluated on the tail and head test sets are reported the results in Table <ref type="table" target="#tab_2">3</ref>. We observe that the proposed SSL methods improve the performance for both head and tail item recommendations, with larger gains from the tail items. For instance, in AAI, the CFM improves over 51.5% of the Recall@10 on tail items, while the improvement is 8.57% on head.</p><p>Effects of SSL Parameters (RQ3). Figure <ref type="figure" target="#fig_5">5</ref> summarizes the Recall@50 evaluated on the Wikipedia and AAI dataset w.r.t. the regularization strength ğ›¼. It also shows the results of SO which shares the same regularization parameter. We observe that with increasing ğ›¼, the model performance is worse than the baseline model (shown in dash line) after certain threshold. This is expected, since large SSL weight ğ›¼ leads to the multitask loss L dominated by ğ›¼ â€¢ L ğ‘ ğ‘’ğ‘™ ğ‘“ in equation <ref type="bibr" target="#b5">(5)</ref>. By further comparing our approach with SO, we show that the SSL based regularization outperforms SO in a wide range of ğ›¼. Figure <ref type="figure" target="#fig_6">6</ref> shows the model performance across different dropout rates ğ‘‘ğ‘Ÿ . It also shows ğ·ğ‘‚ which shares the same parameter. As ğ‘‘ğ‘Ÿ increases, the model performance of ğ·ğ‘‚ continues to deteriorate. For most choices of ğ›¼ (except ğ›¼ = 0.1), ğ·ğ‘‚ is worse than the baseline. For the SSL task with feature dropout, the model performance peaks when ğ‘‘ğ‘Ÿ = 0.3 and then deteriorates when we further improve dropout rate. The model starts to underperform the baseline when ğ‘‘ğ‘Ÿ is too large. This observation aligns with our expectation in the sense that when the rate is too large, the input information becomes too little for to learn meaningful representations through SSL.</p><p>Visualization of Item Representations. We visualize the learned app embeddings from the AAI dataset in t-SNE plot. We postpone  the detailed setup to Appendix .3. As shown in in Figure <ref type="figure" target="#fig_4">4</ref>, we clearly see that apps embeddings learned with our SSL regularization are better clustered according to their own categories, compared to the counter parts of our baseline, which demonstrates that representations learned through SSL have stronger semantic structures. This partially explains the gain from SSL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Data Sparsity</head><p>We study the effectiveness of CFM in presence of sparse data to address RQ2. We uniformly down-sampled 10% of training data and evaluate on the same (full) test dataset. Experiment results are reported in Table <ref type="table" target="#tab_1">2</ref>. With increased data sparsity, CFM demonstrates larger improvement for Wikipedia and AAI respectively. In particular, the CFM on the full Wikipedia dataset improves Recall@10 by 6.1% compared to the baseline, while the relative improvement is 20.6% on the 10% dataset. Similar trend is observed for the AAI dataset (10.2% vs 25.7% on the down-sampled dataset). It's worth noting that, CFM consistently outperforms FD and the gap is larger as data becomes sparser. This demonstrates that having dropout for data augmentation in SSL tasks is more effective than directly applying dropout in supervised task.</p><p>As a summary, these findings answer research questions raised in RQ2 that the proposed SSL framework improves model performance more with even less supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Comparison of Different Data Augmentations</head><p>In this section, we compare several feature augmentation alternatives with CFM to answer RQ4 by studying: 1) the benefit of 10% Wikipedia Dataset Method MAP@10 MAP@50 Recall@10 Recall@50 Baseline 0.0077 0.0105 0.0237 0.0924 FD <ref type="bibr" target="#b50">[35]</ref> 0.0089 0.0120 0.0272 0.1046 SO <ref type="bibr" target="#b56">[41]</ref> 0  exploiting feature correlation in masking and 2) benefit of using dropout as part of augmentation. In specific, we consider the following alternatives:</p><p>â€¢ ğ‘…ğ¹ ğ‘€: Random Feature Masking. In this method, random set of features are masked, instead of guided by mutual information in CFM.</p><p>â€¢ ğ‘…ğ¹ ğ‘€ ğ‘›ğ‘œ_ğ‘ğ‘œğ‘šğ‘ğ‘™ : Random Feature Masking with no Complementary sets of features applied. In this method, 2 independent sets of features are masked at random, instead of a complementary pair of masks in CFM.  â€¢ ğ¶ğ¹ ğ‘€ ğ‘›ğ‘œ_ğ‘‘ğ‘Ÿğ‘œğ‘ğ‘œğ‘¢ğ‘¡ : Correlated Feature Masking with No Dropout applied. In other words, only apply correlated masking as the augmentation in the SSL task.</p><p>â€¢ ğ‘ğ‘œğ‘€ğ‘ğ‘ ğ‘˜ğ‘–ğ‘›ğ‘”: Correlated Feature Masking but skipping the masking phrase in augmentation. In other words, we only apply dropout to features as the augmentation.</p><p>We apply these feature augmentation functions in the SSL framework and report the results on AAI dataset in Table <ref type="table" target="#tab_3">4</ref>.</p><p>First, we observe that all the variants are worse than CFM, but still out-perform the baseline model. In particular, we see having mutual information in selecting the masking set is critical to model improvement, since we see the biggest performance drop is from RFM where masking set is selected at random. By comparing CFM with results from the two methods (ğ‘…ğ¹ ğ‘€ ğ‘›ğ‘œ_ğ‘ğ‘œğ‘šğ‘ğ‘™ and ğ‘ğ‘œğ‘€ğ‘ğ‘ ğ‘˜ğ‘–ğ‘›ğ‘”) that allow feature overlap between the two augmented examples via independent dropout, we see the contrastive learning task is more helpful with complementary information that potentially avoids shortcut in learning. Finally, by comparing ğ¶ğ¹ ğ‘€ ğ‘›ğ‘œ_ğ‘‘ğ‘Ÿğ‘œğ‘ğ‘œğ‘¢ğ‘¡ and CFM, we see the second phrase of randomly dropping out feature values also helps, which could be potentially explained by introducing more feature variants in the SSL task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">LIVE EXPERIMENT</head><p>In this section, we describe how we apply our proposed SSL framework to a web-scale commercial app recommender system. Specifically, given an app as the query, the system identifies similar apps  given the query. One of the models surfacing this recommendation is trained on the AAI dataset as described in Section 4.1, with the same backbone network structure as the two-tower DNN structure in Figure <ref type="figure" target="#fig_0">1</ref> (with modifications). For a natural extension of the offline experiments conducted on Section 4.3 for the AAI experiments, we conducted an A/B experiment for investigating the synergy of deploying the best SSL-based model online. While we already presented improved offline metrics on this dataset, in many real-world systems, offline studies might not align with live impact due to 1) lack of implicit feedback, since the offline evaluation data is collected via user engagement history based on the production system; 2) failing to capture product's multiple objectives optimization goal, where it's very likely that recommending more engaging apps hurts other business goals. Therefore, this experiment is critical in demonstrating the effectiveness of the proposed framework in real-world settings.</p><p>In our live A/B testing, we add the best performing SSL task with the same set of hyper-parameters on top of the existing well-tuned two-tower DNN model used in production. In a time frame of 14 days, the model improved the overall business metrics significantly, with +0.67% increase in key user engagement (Figure <ref type="figure" target="#fig_10">8a</ref>) and +1.5% increase in top business metric (Figure <ref type="figure" target="#fig_8">7a</ref>). To echo the study on the Head-tail Analysis in Section 4.3 and the data sparsity analysis in Section 4.4, we see significant improvements on two slices: 1) cold-starting for fresh apps: the model improves +4.5% on user engagement for fresh apps (Figure <ref type="figure" target="#fig_10">8b</ref>); and (2) international countries that have sparser training data compared to major markets: we see significant +5.47% top business metric gains (Figure <ref type="figure" target="#fig_8">7b</ref> right). Again, both of these results verify our hypothesis that our SSL framework indeed significantly improves model performance for slices without much supervision. Given the results, the SSL empowered model was successfully launched in the current production system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we proposed a model architecture agnostic self-supervised learning (SSL) framework for large-scale neural recommender models. Within the SSL framework, we also introduced a novel data augmentation applicable to heterogeneous categorical features and showed its superior performance over other variants.</p><p>For future works, we plan to investigate how different training schemes impact the model quality. One direction is to first pre-train    <ref type="table">5</ref> shows some basic stats for the Wikipedia and AAI datasets. Figure <ref type="figure">9</ref> shows the CDF of most frequent items for the two datasets, indicating a highly skewed data distribution. For example, the top 50 items in the AAI dataset collectively appeared roughly 10% in the training data. If we consider a naive baseline (i.e., TopPopular recommender <ref type="bibr" target="#b15">[11]</ref>) that recommends the most frequent top-K items for every query, the ğ¶ğ·ğ¹ of the ğ¾-th frequent item essentially represents the ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™@ğ¾ metric of such baseline. This suggests a naive TopPopular recommender achieves ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™@50 â‰ˆ 0.1 for AAI and ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™@50 â‰ˆ 0.05 for Wikipedia. We present that all the proposed methods outperform this baseline by a large margin in Section 4. .3 Visualization of Learned Embeddings. Besides better model performance, we expect the representations learned with SSL to have better quality than the counterparts without SSL. To verify our hypothesis, we take the app embeddings learned in the models trained on AAI dataset, and plot them using t-SNE plot in Figure <ref type="figure" target="#fig_4">4</ref>. Apps from different categories are plotted in different colors, as illustrated in the legends in Figure <ref type="figure" target="#fig_4">4</ref>. Compared to the apps in (Figure <ref type="figure" target="#fig_4">4a</ref>), the apps in the best SSL model (Figure <ref type="figure" target="#fig_4">4b</ref>) tend to group much better with similar apps in the same category, and the separation of different category looks much more clear. For example, we could see that the "Sports &amp; Recreation" apps (in red) are mixed together with "Law &amp; Government" and "Travel" apps in Figure <ref type="figure" target="#fig_4">4a</ref>. While in Figure <ref type="figure" target="#fig_4">4b</ref>, we clearly see the 4 categories of apps grouped together among themselves. This indicates that the representations learned with SSL carry more semantic information, and is also why SSL leads to better model performance in our experiments.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Model architecture: Two-tower DNN with query and item representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Self-supervised learning framework illustration. Two data augmentations â„ and ğ‘” are applied to the input; Encoders H and G are applied to the augmented examples y ğ‘– and y â€² ğ‘– to generate embeddings z ğ‘– and z â€² ğ‘– . The SSL loss L ğ‘ ğ‘’ğ‘™ ğ‘“ w.r.t. z ğ‘– is optimized towards maximizing the similarity with z â€²ğ‘– while minimizing the similarity between z ğ‘— and z â€² ğ‘— . We consider a batch of ğ‘ item examples ğ‘¥ 1 , ..., ğ‘¥ ğ‘ , where ğ‘¥ ğ‘– âˆˆ X represents a set of features for example ğ‘–. In the context of recommenders, an example indicates a query, an item or a query-item pair. Suppose there are a pair of transform function â„, ğ‘” : X â†’ X that augment ğ‘¥ ğ‘– to be ğ‘¦ ğ‘– , ğ‘¦ â€² ğ‘– respectively, ğ‘¦ ğ‘– â† â„(ğ‘¥ ğ‘– ), ğ‘¦ â€² ğ‘– â† ğ‘”(ğ‘¥ ğ‘– ).(1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Model architecture: Two-tower model with SSL. In the SSL task, we apply feature masking and dropout on the item features to learn item embeddings. The whole item tower (in red) is shared with the supervised task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Comparison of t-SNE plots for app embeddings for baseline, and our best SSL model.</figDesc><graphic url="image-7.png" coords="7,95.84,218.78,156.16,102.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Impact of regularization strength ğ›¼ for SSL-based and spread-out regularization methods. The vertical dash line indicates the baseline model's metric.</figDesc><graphic url="image-8.png" coords="7,320.77,468.33,103.30,57.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Impact of dropout rate ğ‘‘ğ‘Ÿ for our method and the standard dropout training on the Wikipedia dataset.</figDesc><graphic url="image-13.png" coords="8,57.61,162.27,115.32,75.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Top Business Metric Improvement Percentage (yaxis) over Days (x-axis) in Online Experiments: (a) improvement globally; (b) improvement on a localized market.</figDesc><graphic url="image-15.png" coords="8,320.20,95.47,115.31,101.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Top User Engagement Improvement Percentage (yaxis) over Days (x-axis) in Online Experiments: (a) improvement on all apps; (b) improvement on fresh apps.on SSL task to learn query and item representations and fine-tune on primary supervised tasks. Alternatively, it would be interesting to extend the technique for deep models in application domains such as search ranking or pCTR prediction.</figDesc><graphic url="image-18.png" coords="9,176.49,93.65,115.31,99.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 : 5 :</head><label>95</label><figDesc>Figure 9: CDF of most frequent items in the Wikipedia and AAI datasets. The distribution is extremely dominated by popular items. For example, the top 50 items out of the 2.4M items already constitute 10% of data in the AAI dataset. Dataset # queries # items # examples Wikipedia 5.3M 5.3M 490M AAI 2.4M 2.4M 1B Table 5: Corpus sizes of the Wikipedia and the AAI datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>. 2</head><label>2</label><figDesc>Head and Tail Item Evaluation Dataset. We partition the full test dataset based on popularity of the groundtruth item. For the AAI test dataset, the Head dataset consists examples where the groundtruth items are in the top 10% most frequent items, and the rest of the test examples are treated as tail. For Wikipedia, we follow the data partitions in [23], where test examples containing items not included in the training set are treated as Tail, and the rest test examples are treated as Head.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results on the full Wikipedia and AAI dataset.</figDesc><table><row><cell></cell><cell>.0176</cell><cell>0.0235</cell><cell>0.0549</cell><cell>0.1956</cell></row><row><cell>Our method</cell><cell>0.0183</cell><cell>0.0243</cell><cell>0.057</cell><cell>0.2009</cell></row><row><cell></cell><cell></cell><cell>AAI</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="4">MAP@10 MAP@50 Recall@10 Recall@50</cell></row><row><cell>Baseline</cell><cell>0.1257</cell><cell>0.1363</cell><cell>0.2793</cell><cell>0.4983</cell></row><row><cell>FD [35]</cell><cell>0.1278</cell><cell>0.1384</cell><cell>0.2840</cell><cell>0.5058</cell></row><row><cell>SO [41]</cell><cell>0.1300</cell><cell>0.1406</cell><cell>0.2870</cell><cell>0.5076</cell></row><row><cell>Our method</cell><cell>0.1413</cell><cell>0.1522</cell><cell>0.3078</cell><cell>0.5355</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Experiment results trained on the sparse (10% downsampled) Wikipedia and AAI datasets.</figDesc><table><row><cell></cell><cell>.0083</cell><cell>0.0112</cell><cell>0.0254</cell><cell>0.0978</cell></row><row><cell>Our method</cell><cell>0.0093</cell><cell>0.0126</cell><cell>0.0286</cell><cell>0.1093</cell></row><row><cell></cell><cell cols="2">10% AAI Dataset</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="4">MAP@10 MAP@50 Recall@10 Recall@50</cell></row><row><cell>Baseline</cell><cell>0.1112</cell><cell>0.1194</cell><cell>0.2406</cell><cell>0.4068</cell></row><row><cell>FD [35]</cell><cell>0.1217</cell><cell>0.1302</cell><cell>0.2611</cell><cell>0.4324</cell></row><row><cell>SO [41]</cell><cell>0.1220</cell><cell>0.1308</cell><cell>0.2632</cell><cell>0.4400</cell></row><row><cell>Our method</cell><cell>0.1409</cell><cell>0.1507</cell><cell>0.3024</cell><cell>0.5014</cell></row><row><cell></cell><cell></cell><cell>Wikipedia</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell></cell><cell>Tail</cell><cell></cell><cell>Head</cell></row><row><cell></cell><cell cols="4">Recall@10 Recall@50 Recall@10 Recall@50</cell></row><row><cell>Baseline</cell><cell>0.0472</cell><cell>0.1621</cell><cell>0.0610</cell><cell>0.2273</cell></row><row><cell>FD</cell><cell>0.0474</cell><cell>0.1638</cell><cell>0.0593</cell><cell>0.2212</cell></row><row><cell>SO</cell><cell>0.0481</cell><cell>0.1644</cell><cell>0.0606</cell><cell>0.2268</cell></row><row><cell>Our method</cell><cell>0.0524</cell><cell>0.1749</cell><cell>0.0619</cell><cell>0.2283</cell></row><row><cell></cell><cell></cell><cell>AAI</cell><cell></cell><cell></cell></row><row><cell>Baseline</cell><cell>0.0475</cell><cell>0.2333</cell><cell>0.2846</cell><cell>0.4993</cell></row><row><cell>FD</cell><cell>0.0727</cell><cell>0.2743</cell><cell>0.2849</cell><cell>0.5069</cell></row><row><cell>SO</cell><cell>0.0661</cell><cell>0.2602</cell><cell>0.2879</cell><cell>0.5086</cell></row><row><cell>Our method</cell><cell>0.0720</cell><cell>0.2906</cell><cell>0.309</cell><cell>0.537</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results of Wikipedia and AAI on tail and head item slices.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Results of CFM and other data augmentation techniques on the AAI dataset.</figDesc><table><row><cell cols="4">Comparison of Multiple Data Augmentations</cell><cell></cell></row><row><cell>Method</cell><cell cols="4">MAP@10 MAP@50 Recall@10 Recall@50</cell></row><row><cell>Baseline</cell><cell>0.1257</cell><cell>0.1363</cell><cell>0.2793</cell><cell>0.4983</cell></row><row><cell>CFM</cell><cell>0.1413</cell><cell>0.1522</cell><cell>0.3078</cell><cell>0.5355</cell></row><row><cell>RFM</cell><cell>0.1281</cell><cell>0.1389</cell><cell>0.2851</cell><cell>0.5104</cell></row><row><cell>ğ‘…ğ¹ ğ‘€ ğ‘›ğ‘œ_ğ‘ğ‘œğ‘šğ‘ğ‘™</cell><cell>0.1363</cell><cell>0.1472</cell><cell>0.3007</cell><cell>0.5290</cell></row><row><cell>ğ¶ğ¹ ğ‘€ ğ‘›ğ‘œ_ğ‘‘ğ‘Ÿğ‘œğ‘ğ‘œğ‘¢ğ‘¡</cell><cell>0.1309</cell><cell>0.1417</cell><cell>0.2898</cell><cell>0.5150</cell></row><row><cell>ğ‘ ğ‘œğ‘€ğ‘ğ‘ ğ‘˜ğ‘–ğ‘›ğ‘”</cell><cell>0.1303</cell><cell>0.1408</cell><cell>0.2868</cell><cell>0.5053</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Beyond Globally Optimal: Focused Learning for Improved Recommendations</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hubert</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Anderson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">S4L: Self-Supervised Semi-Supervised Learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Wei-Cheng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin-Wen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>n.d.</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Pre-training Tasks for Embedding-based Large-scale Retrieval</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">XGBoost: A Scalable Tree Boosting System</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>n.d.</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2002.05709" />
		<title level="m">A Simple Framework for Contrastive Learning of Visual Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10029</idno>
		<title level="m">Big Self-Supervised Models are Strong Semi-Supervised Learners</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Heng-Tze</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Levent</forename><surname>Koc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremiah</forename><surname>Harmsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hrishi</forename><surname>Aradhye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Glen</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Ispir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zakaria</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vihan</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaobing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hemal</forename><surname>Shah</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>n.d.</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Wide &amp; Deep Learning for Recommender Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Evangelia</forename><surname>Christakopoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>n.d.</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m">Local Latent Space Models for Top-N Recommendation</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Edith</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>n.d.</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Approximating Matrix Multiplication for Pattern Recognition Tasks</title>
	</analytic>
	<monogr>
		<title level="m">SODA 1997</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Paul</forename><surname>Covington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emre</forename><surname>Sargin</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>n.d.</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep Neural Networks for YouTube Recommendations</title>
	</analytic>
	<monogr>
		<title level="m">RecSys</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Maurizio</forename><surname>Ferrari Dacrema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Cremonesi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dietmar</forename><surname>Jannach</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>n.d.</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Are We Really Making Much Progress? A Worrying Analysis of Recent Neural Recommendation Approaches</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</title>
		<author>
			<persName><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07">2011. July 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m">Wikimedia Foundation</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<ptr target="https://dumps.wikimedia.org/" />
		<title level="m">Wikimedia</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>n.d.</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Unsupervised Representation Learning by Predicting Image Rotations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">End-to-End Retrieval in Continuous Space</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Presta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Singh Tomar</surname></persName>
		</author>
		<idno>CoRR abs/1811.08008</idno>
		<ptr target="http://arxiv.org/abs/1811.08008" />
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Breaking the Glass Ceiling for Embedding-Based Classifiers for Large Output Spaces</title>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Mousavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satyen</forename><surname>Daniel N Holtmann-Rice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sashank</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjiv</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><surname>Kumar</surname></persName>
		</author>
		<editor>Neurips, H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;AlchÃ©-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>n.d.</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Neural Collaborative Filtering</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Practical Lessons from Predicting Clicks on Ads at Facebook</title>
		<author>
			<persName><forename type="first">Xinran</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junfeng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ou</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianbing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Atallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralf</forename><surname>Herbrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stuart</forename><surname>Bowers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joaquin</forename><forename type="middle">QuiÃ±onero</forename><surname>Candela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Workshop on Data Mining for Online Advertising</title>
				<meeting>the Eighth International Workshop on Data Mining for Online Advertising</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>n.d.</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Revisiting Self-Supervised Visual Representation Learning</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Matrix Factorization Techniques for Recommender Systems</title>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009-08">2009. Aug. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">M</forename><surname>Bell</surname></persName>
		</author>
		<title level="m">Advances in Collaborative Filtering</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="77" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Efficient Training on Very Large Corpora via Gramian Estimation</title>
		<author>
			<persName><forename type="first">Walid</forename><surname>Krichene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Mayoraz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyang</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Anderson</surname></persName>
		</author>
		<idno>ICLR 2019</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</title>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Gustav</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maire</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>n.d.</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning Representations for Automatic Colorization</title>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Related Pins at Pinterest: The Evolution of a Real-World Recommender System</title>
		<author>
			<persName><forename type="first">C</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Shiau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">C</forename><surname>Kislyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhigang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenny</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yushi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Jing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>n.d.</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Disentangled Self-Supervision in Sequential Recommenders</title>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Music Recommendation and the Long Tail</title>
	</analytic>
	<monogr>
		<title level="m">1st Workshop On Music Recommendation And Discovery (WOMRAD), ACM RecSys</title>
				<imprint>
			<publisher>Klaas Bosteels Mark Levy</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Rishabh</forename><surname>Mehrotra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mounia</forename><surname>Lalmas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Kenney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Lim-Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Golli</forename><surname>Hashemian</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>n.d.</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Jointly Leveraging Intent and Interaction Signals to Predict User Satisfaction with Slate Recommendations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Power Law Distributions in Information Science: Making the Case for Logarithmic Binning</title>
		<author>
			<persName><forename type="first">StaÅ¡a</forename><surname>MilojeviÄ‡</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Soc. Inf. Sci. Technol</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="2417" to="2425" />
			<date type="published" when="2010-12">2010. Dec. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Deep Learning Recommendation Model for Personalization and Recommendation Systems</title>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Naumov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dheevatsa</forename><surname>Mudigere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Hao-Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Narayanan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jongsoo</forename><surname>Sundaraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Udit</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carole-Jean</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alisson</forename><forename type="middle">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmytro</forename><surname>Azzolini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Dzhulgakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilia</forename><surname>Mallevich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinghai</forename><surname>Cherniavskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raghuraman</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ansha</forename><surname>Krishnamoorthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Volodymyr</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Kondratenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianjie</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenlin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Misha</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><surname>Smelyanskiy</surname></persName>
		</author>
		<idno>CoRR abs/1906.00091</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Wei</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Caverlee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haokai</forename><surname>Lu</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>n.d.</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Neural Personalized Ranking for Image Recommendation</title>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>n.d.</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles</title>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Shumpei</forename><surname>Okura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukihiro</forename><surname>Tagami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shingo</forename><surname>Ono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akira</forename><surname>Tajima</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>n.d.</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Embedding-Based News Recommendation for Millions of Users</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">DropoutNet: Addressing Cold Start in Recommender Systems</title>
		<author>
			<persName><forename type="first">Maksims</forename><surname>Volkovs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangwei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomi</forename><surname>Poutanen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Unsupervised Feature Learning via Non-Parametric Instance-level Discrimination</title>
		<author>
			<persName><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<idno>CoRR abs/1805.01978</idno>
		<ptr target="http://arxiv.org/abs/1805.01978" />
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Self-Supervised Reinforcement Learning for Recommender Systems</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Karatzoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Arapakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR 2020</title>
				<imprint/>
	</monogr>
	<note>n. d.</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning Semantic Textual Similarity from Conversations</title>
		<author>
			<persName><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng-Yi</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petr</forename><surname>Pilar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heming</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun-Hsuan</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Strope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ray</forename><surname>Kurzweil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Third Workshop on Representation Learning for NLP. ACL</title>
				<meeting>The Third Workshop on Representation Learning for NLP. ACL</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="164" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Sampling-Bias-Corrected Neural Modeling for Large Corpus Item Recommendations</title>
		<author>
			<persName><forename type="first">Xinyang</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><forename type="middle">Zhiyuan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Heldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditee</forename><surname>Kumthekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RecSys</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>n.d.</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Visual Discovery at Pinterest</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Kislyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yushi</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Li Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Xu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>n.d.</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Learning Spread-Out Local Feature Descriptors</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aniruddh</forename><surname>Nath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shawn</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditee</forename><surname>Kumthekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maheswaran</forename><surname>Sathiamoorthy</surname></persName>
		</author>
		<editor>Xinyang Yi, and Ed Chi</editor>
		<imprint/>
	</monogr>
	<note>n.d.</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Recommending What Video to Watch next: A Multitask Ranking System</title>
	</analytic>
	<monogr>
		<title level="m">RecSys</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">S3-Rec: Self-Supervised Learning for Sequential Recommendation with Mutual Information Maximization</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jirong</forename><surname>Zhong Yuan Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM 2020</title>
				<imprint/>
	</monogr>
	<note>n.d.. n. d.</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
