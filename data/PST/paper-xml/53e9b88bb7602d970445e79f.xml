<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Combining Instance-Based Learning and Logistic Regression for Multilabel Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Weiwei</forename><surname>Cheng</surname></persName>
							<email>cheng@mathematik.uni-marburg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Mathematics and Computer Science</orgName>
								<orgName type="institution">University of Marburg</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Eyke</forename><surname>Hüllermeier</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Mathematics and Computer Science</orgName>
								<orgName type="institution">University of Marburg</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Combining Instance-Based Learning and Logistic Regression for Multilabel Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C4E620A93C4268D1FB3BCA08F76783A4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multilabel classification is an extension of conventional classification in which a single instance can be associated with multiple labels. Recent research has shown that, just like for standard classification, instance-based learning algorithms relying on the nearest neighbor estimation principle can be used quite successfully in this context. However, since hitherto existing algorithms do not take correlations and interdependencies between labels into account, their potential has not yet been fully exploited. In this paper, we propose a new approach to multilabel classification, which is based on a framework that unifies instancebased learning and logistic regression, comprising both methods as special cases. This approach allows one to capture interdependencies between labels and, moreover, to combine model-based and similarity-based inference for multilabel classification. As will be shown by experimental studies, our approach is able to improve predictive accuracy in terms of several evaluation criteria for multilabel prediction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In conventional classification, each instance is assumed to belong to exactly one among a finite set of candidate classes. As opposed to this, the setting of multilabel classification allows an instance to belong to several classes simultaneously or, say, to attach more than one label to a single instance. Problems of this type are ubiquitous in everyday life: At IMDb, a movie can be categorized as action, crime, and thriller ; a CNN news report can be tagged as people and political at the same time; in biology, a typical multilabel learning example is the gene functional prediction problem, where a gene can be associated with multiple functional classes, such as metabolism, transcription, and protein synthesis.</p><p>Multilabel classification has received increasing attention in machine learning in recent years, not only due to its practical relevance, but also as it is interesting from a theoretical point of view. In fact, even though it is possible to reduce the problem of multilabel classification to conventional classification in one way or the other and, hence, to apply existing methods for the latter to solve the former, straightforward solutions of this type are usually not optimal. In particular, since the presence or absence of the different class labels has to be predicted simultaneously, it is obviously important to exploit correlations and interdependencies between these labels. This is usually not accomplished by simple transformations to standard classification.</p><p>Even though quite a number of more sophisticated methods for multilabel classification has been proposed in the literature, the application of instancebased learning (IBL) has not been studied very deeply in this context so far. This is a bit surprising, given that IBL algorithms based on the nearest neighbor estimation principle have been applied quite successfully in classification and pattern recognition for a long time <ref type="bibr" target="#b0">[1]</ref>. A notable exception is the multilabel knearest neighbor (MLKNN) method that was recently proposed in <ref type="bibr" target="#b1">[2]</ref>, where it was shown to be competitive to state-of-the-art machine learning methods.</p><p>In this paper, we propose a novel approach to multilabel classification, which is based on a framework that unifies instance-based learning and logistic regression, comprising both methods as special cases. This approach overcomes some limitations of existing instance-based multilabel classification methods, including MLKNN. In particular, it allows one to capture interdependencies between the class labels in a proper way.</p><p>The rest of this paper is organized as follows: The problem of multilabel classification is introduced in a more formal way in Section 2, and related work is discussed in Section 3. Our novel method is then described in Section 4. Section 5 is devoted to experiments with several benchmark data sets. The paper ends with a summary and some concluding remarks in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Multilabel Classification</head><p>Let X denote an instance space and let L = {λ 1 , λ 2 . . . λ m } be a finite set of class labels. Moreover, suppose that each instance x ∈ X can be associated with a subset of labels L ∈ 2 L ; this subset is often called the set of relevant labels, while the complement L \ L is considered as irrelevant for x. Given training data in the form of a finite set T of observations in the form of tuples (x, L x ) ∈ X×2 L , typically assumed to be drawn independently from an (unknown) probability distribution on X × 2 L , the goal in multilabel classification is to learn a classifier h : X → 2 L that generalizes well beyond these observations in the sense of minimizing the expected prediction loss with respect to a specific loss function; commonly used loss functions will be reviewed in Section 5.3.</p><p>Note that multilabel classification can be reduced to a conventional classification problem in a straightforward way, namely by considering each label subset L ∈ 2 L as a distinct (meta-)class. This approach is referred to as label powerset (LP) in the literature. An obvious drawback of this approach is the potentially large number of classes that one has to deal with in the newly generated problem; obviously, this number is 2 |L| (or 2 |L| -1 if the empty set is excluded as a prediction). This is the reason why LP typically works well if the original label set L is small but quickly deteriorates for larger label sets. Nevertheless, LP is often used as a benchmark, and we shall also include it in our experiments later on (cf. Section 5).</p><p>Another way of reducing multilabel to conventional classification is offered by the binary relevance approach. Here, a separate binary classifier h i is trained for each label λ i ∈ L, reducing the supervision to information about the presence or absence of this label while ignoring the other ones. For a query instance x, this classifier is supposed to predict whether λ i is relevant for x (h i (x) = 1) or not (h i (x) = 0). A multilabel prediction for x is then given by h(x) = {λ i ∈ L | h i (x) = 1}. Since binary relevance learning treats every label independently of all other labels, an obvious disadvantage of this approach is that it ignores correlations and interdependencies between labels.</p><p>Some of the more sophisticated approaches learn a multilabel classifier h in an indirect way via a scoring function f : X × L → R that assigns a real number to each instance/label combination. The idea is that a score f (x, λ) is in direct correspondence with the probability that λ is relevant for x. Given a scoring function of this type, multilabel prediction can be realized via thresholding:</p><formula xml:id="formula_0">h(x) = {λ ∈ L | f (x, λ) ≥ t } ,</formula><p>where t ∈ R is a threshold. As a byproduct, a scoring function offers the possibility to produce a ranking of the class labels, simply by ordering them according to their score. Sometimes, this ranking is even more desirable as a prediction, and indeed, there are several evaluation metrics that compare a true label subset with a predicted ranking instead of a predicted label subset (cf. Section 5.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related Work</head><p>Multilabel classification has received a great deal of attention in machine learning in recent years, and a number of methods has been developed, often motivated by specific types of applications such as text categorization <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref>, computer vision <ref type="bibr" target="#b6">[7]</ref>, and bioinformatics <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b5">6]</ref>. Besides, several well-established methods for conventional classification have been extended to the multi-label case, including support vector machines <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b6">7]</ref>, neural networks <ref type="bibr" target="#b5">[6]</ref>, and decision trees <ref type="bibr" target="#b10">[11]</ref>.</p><p>In this paper, we are especially interested in instance-based approaches to multilabel classification, i.e., methods based on the nearest neighbor estimation principle <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b0">1]</ref>. This interest is largely motivated by the multilabel k-nearest neighbor (MLKNN) method that has recently been proposed in <ref type="bibr" target="#b1">[2]</ref>. In that paper, the authors show that MLKNN performs quite well in practice. In the concrete experiments presented, MLKNN even outperformed some state-of-theart model-based approaches to multilabel classification, including RankSVM and AdaBoost.MH <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13]</ref>.</p><p>MLKNN is a binary relevance learner, i.e., it learns a single classifier h i for each label λ i ∈ L. However, instead of using the standard k-nearest neighbor (KNN) classifier as a base learner, it implements the h i by means of a combination of KNN and Bayesian inference: Given a query instance x with unknown multilabel classification L ⊆ L, it finds the k nearest neighbors of x in the training data and counts the number of occurrences of λ i among these neighbors.</p><p>Considering this number, y, as information in the form of a realization of a random variable Y , the posterior probability of λ i ∈ L is given by</p><formula xml:id="formula_1">P(λ i ∈ L | Y = y) = P(Y = y | λ i ∈ L) • P(λ i ∈ L) P(Y = y) ,<label>(1)</label></formula><p>which leads to the decision rule</p><formula xml:id="formula_2">h i (x) = 1 if P(Y = y | λ i ∈ L)P(λ i ∈ L) ≥ P(Y = y | λ i ∈ L)P(λ i ∈ L) 0 otherwise</formula><p>The prior probabilities P(λ i ∈ L) and P(λ i ∈ L) as well as the conditional probabilities P(Y = y | λ i ∈ L) and P(Y = y | λ i ∈ L) are estimated from the training data in terms of corresponding relative frequencies. As an aside, we note that these estimations come with a relatively high computational complexity, since they involve the consideration of all k-neighborhoods of all training instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Combining IBL and Logistic Regression</head><p>In this section, we introduce a machine learning method whose basic idea is to consider the information that derives from examples similar to a query instance as a feature of that instance, thereby blurring the distinction between instancebased and model-based learning to some extent. This idea is put into practice by means of a learning algorithm that realizes instance-based classification as logistic regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">KNN Classification</head><p>Suppose an instance x to be described in terms of features φ i , i = 1, 2 . . . n, where φ i (x) denotes the value of the i-th feature for instance x. The instance space X is endowed with a distance measure: ∆(x, x ) is the distance between instances x and x . We shall first focus on the case of binary classification and hence define the set of class labels by</p><formula xml:id="formula_3">Y = {-1, +1}. A tuple (x, y) ∈ X × Y is called a labeled instance or example. D denotes a sample that consists of N labeled instances (x i , y i ), 1 ≤ i ≤ N . Finally, a new instance x 0 ∈ X (a query) is given, whose label y 0 ∈ {-1, +1} is to be estimated.</formula><p>The nearest neighbor (NN) principle prescribes to estimate the label of the yet unclassified query x 0 by the label of the nearest (least distant) sample instance. The KNN approach is a slight generalization, which takes the k ≥ 1 nearest neighbors of x 0 into account. That is, an estimation ŷ0 of y 0 is derived from the set N k (x 0 ) of the k nearest neighbors of x 0 , usually by means of a majority vote:</p><formula xml:id="formula_4">ŷ0 = arg max y∈Y #{x i ∈ N k (x 0 ) | y i = y}.<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">IBL as Logistic Regression</head><p>A key idea of our approach is to consider the labels of neighbored instances as "features" of the query x 0 whose label is to be estimated. It is worth mentioning that similar ideas have recently been exploited in relational learning <ref type="bibr" target="#b13">[14]</ref> and collective classification <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>. Denote by p 0 the prior probability of y 0 = +1 and by π 0 the corresponding posterior probability. Moreover, let δ i df = ∆(x 0 , x i ) be the distance between x 0 and x i . Taking the known label y i as information about the unknown label y 0 , we can consider the posterior probability</p><formula xml:id="formula_5">π 0 df = P(y 0 = +1 | y i ).</formula><p>More specifically, Bayes' rule yields</p><formula xml:id="formula_6">π 0 1 -π 0 = P(y i | y 0 = +1) P(y i | y 0 = -1) • p 0 1 -p 0 = ρ • p 0 1 -p 0 ,</formula><p>where ρ is the likelihood ratio. Taking logarithms on both sides, we get</p><formula xml:id="formula_7">log π 0 1 -π 0 = log(ρ) + ω 0<label>(3)</label></formula><p>with ω 0 = log(p 0 ) -log(1 -p 0 ). Model (3) still requires the specification of the likelihood ratio ρ. In order to obey the basic principle underlying IBL, the latter should be a function of the distance δ i . In fact, ρ should become large for δ i → 0 if y i = +1 and small if y i = -1: Observing a very close instance x i with label y i = +1 (y i = -1) makes y 0 = +1 more (un)likely in comparison to y i = -1. Moreover, ρ should tend to 1 as δ i → ∞: If x i is too far away, its label does not provide any evidence, neither in favor of y 0 = +1 nor in favor of y 0 = -1. A parameterized function satisfying these properties is</p><formula xml:id="formula_8">ρ = ρ(δ) df = exp y i • α δ ,</formula><p>where α &gt; 0 is a constant. Note that the choice of a special functional form for ρ is quite comparable to the specification of the kernel function used in (non-parametric) kernel-based density estimation, as well as to the choice of the weight function in weighted NN estimation. ρ(δ) actually determines the probability that two instances whose distance is given by δ = ∆(x 0 , x i ) do have the same label. Now, taking the complete sample neighborhood N (x 0 ) of x 0 into account and -as in the naive Bayes approach-making the simplifying assumption of conditional independence, we obtain</p><formula xml:id="formula_9">log π 0 1 -π 0 = ω 0 + α xi∈N (x0) y i δ i (4) = ω 0 + α • ω + (x 0 ),</formula><p>where ω + (x 0 ) can be seen as a summary of the evidence in favor of label +1. As can be seen, the latter is simply given by the sum of neighbors with label +1, weighted by their distance, minus the weighted sum of neighbors with label -1.</p><p>As concerns the classification of the query x 0 , the decision is determined by the sign of the right-hand side in <ref type="bibr" target="#b3">(4)</ref>. From this point of view, (4) does basically realize a weighted NN estimation, or, stated differently, it is a "model-based" version of instance-based learning. Still, it differs from the simple NN scheme in that it includes a bias term ω 0 , which plays the same role as the prior probability in Bayesian inference.</p><p>From a statistical point of view, ( <ref type="formula">4</ref>) is nothing else than a logistic regression equation. In other words, taking a "feature-based" view of instance-based learning and applying a Bayesian approach to inference comes down to realizing IBL as logistic regression.</p><p>By introducing a similarity measure κ, inversely related to the distance function ∆, (4) can be written in the form</p><formula xml:id="formula_10">log π 0 1 -π 0 = ω 0 + α xi∈N (x0) κ(x 0 , x i ) • y i .<label>(5)</label></formula><p>Note that, as a special case, this approach can mimic the standard KNN classifier (2), namely by setting ω 0 = 0 and defining κ in terms of the (data-dependent) "KNN kernel"</p><formula xml:id="formula_11">κ(x 0 , x i ) = 1 if x i ∈ N k (x 0 ) 0 otherwise .<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Estimation and Classification</head><p>The parameter α in (4) determines the weight of the evidence</p><formula xml:id="formula_12">ω + (x 0 ) = xi∈N (x0) κ(x 0 , x i ) • y i<label>(7)</label></formula><p>and, hence, its influence on the posterior probability estimation π 0 . In fact, α plays the role of a smoothing (regularization) parameter. The smaller α is chosen, the smoother an estimated probability function ( obtained by applying (5) to all points x 0 ∈ X ) will be. In the extreme case where α = 0, one obtains a constant function (equal to ω 0 ). An optimal specification of α can be accomplished by adapting this parameter to the data D, using the method of maximum likelihood (ML). For each sample point x j denote by</p><formula xml:id="formula_13">ω + (x j ) df = xj =xi∈N (xj ) κ(x i , x j ) • y i</formula><p>the sample evidence in favor of y j = +1. The log-likelihood function is then given by the mapping</p><formula xml:id="formula_14">α → j : yj =+1 w 0 + α ω + (x j ) - N j=1 log 1 + exp(w 0 + α ω + (x j ) ,<label>(8)</label></formula><p>and the optimal parameter α * is the maximizer of <ref type="bibr" target="#b7">(8)</ref>. The latter can be computed by means of standard methods from logistic regression. The posterior probability π 0 for the query is then given by</p><formula xml:id="formula_15">π 0 = exp(ω 0 + α * ω + (x 0 )) 1 + exp(ω 0 + α * ω + (x 0 ))</formula><p>.</p><p>To classify x 0 , one applies the decision rule</p><formula xml:id="formula_16">ŷ0 df = +1 if π 0 ≥ 1/2 -1 if π 0 &lt; 1/2 .</formula><p>Subsequently, we shall refer to the method outlined above as IBLR (Instance-Based Learning by Logistic Regression).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Including Additional Features</head><p>In the previous section, instance-based learning has been embedded into logistic regression, using the information coming from the neighbors of a query x 0 as a "feature" of that query. In this section, we consider a possible generalization of this approach, namely the idea to extend the model ( <ref type="formula" target="#formula_10">5</ref>) by taking further features of x 0 into account:</p><formula xml:id="formula_17">log π 0 1 -π 0 = α ω + (x 0 ) + ϕs∈F β s ϕ s (x 0 ),<label>(9)</label></formula><p>where F = {ϕ 0 , ϕ 1 . . . ϕ r } is a subset of the available features {φ 0 , φ 1 . . . φ n } and ϕ 0 = φ 0 ≡ 1, which means that β 0 plays the role of ω 0 . Equation ( <ref type="formula" target="#formula_17">9</ref>) is a common logistic regression model, except that ω + (x 0 ) is a "non-standard" feature. The approach (9), that we shall call IBLR+, integrates instance-based and model-based (attribute-based) learning and, by estimating the regression coefficients in <ref type="bibr" target="#b8">(9)</ref>, achieves an optimal balance between both approaches. The extended model ( <ref type="formula" target="#formula_17">9</ref>) can be interpreted as a logistic regression model of IBL, as outlined in Section 4.2, where the bias ω 0 is no longer constant:</p><formula xml:id="formula_18">log π 0 1 -π 0 = ω 0 (x 0 ) + α ω + (x 0 ) ,<label>(10)</label></formula><p>with ω 0 (x 0 ) df = β s ϕ s (x 0 ) being an instance-specific bias determined by the model-based part of (9).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Extension to Multilabel Classification</head><p>So far, we only considered the case of binary classification. To extend the approach to multilabel classification with a label set L = {λ 1 , λ 2 . . . λ m }, the idea is to train one classifier h i for each label. For the i-th label λ i , this classifier is derived from the model log π</p><formula xml:id="formula_19">(i) 0 1 -π (i) 0 = ω (i) 0 + m j=1 α (i) j • ω (i) +j (x 0 ) ,<label>(11)</label></formula><p>where π (i) 0 denotes the (posterior) probability that λ i is relevant for x 0 , and</p><formula xml:id="formula_20">ω (i) +j (x 0 ) = x∈N (x0) κ(x 0 , x) • y j (x)<label>(12)</label></formula><p>is a summary of the presence of the j-th label λ j in the neighborhood of x 0 ; here, y j (x) = +1 if λ j is present (relevant) for the neighbor x, and y j (x) = -1 in case it is absent (non-relevant).</p><p>Obviously, the approach ( <ref type="formula" target="#formula_19">11</ref>) is able to take interdependencies between class labels into consideration. More specifically, the estimated coefficient α (i) j indicates to what extent the relevance of label λ i is influenced by the relevance of λ j . A value α (i) j 0 means that the presence of λ j makes the relevance of λ i more likely, i.e., there is a positive correlation. Correspondingly, a negative coefficient would indicate a negative correlation.</p><p>Note that the estimated probabilities π (i) 0</p><p>can naturally be considered as scores for the labels λ i . Therefore, a ranking of the labels is simply obtained by sorting them in decreasing order according to their probabilities. Moreover, a pure multilabel prediction for x 0 is derived from this ranking via thresholding at t = 0.5.</p><p>Of course, it is also possible to combine the model <ref type="bibr" target="#b10">(11)</ref> with the extension proposed in Section 4.4. This leads to a model log π</p><formula xml:id="formula_21">(i) 0 1 -π (i) 0 = m j=1 α (i) j • ω (i) +j (x 0 ) + ϕs∈F β (i) s ϕ r (x 0 ) .<label>(13)</label></formula><p>We shall refer to the extensions ( <ref type="formula" target="#formula_19">11</ref>) and ( <ref type="formula" target="#formula_21">13</ref>) of IBLR to multilabel classification as IBLR-ML and IBLR-ML+, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>This section is devoted to experimental studies that we conducted to get a concrete idea of the performance of our method. Before presenting the results of our experiments, we give some information about the learning algorithms and data sets included in the study, as well as the criteria used for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Learning Algorithms</head><p>For the reasons mentioned previously, our main interest is focused on MLKNN, which is arguably the state-of-the-art in instance-based multilabel ranking. This method is parameterized by the size of the neighborhood, for which we adopted the value k = 10. This value is recommended in <ref type="bibr" target="#b1">[2]</ref>, where it was found to yield the best performance. For the sake of fairness, we use the same neighborhood size for our method, in conjunction with the KNN kernel <ref type="bibr" target="#b5">(6)</ref>. In both cases, the simple Euclidean metric (on the complete attribute space) was used as a distance function. For our method, we tried both variants, the pure instancebased version <ref type="bibr" target="#b10">(11)</ref>, and the extended model <ref type="bibr" target="#b12">(13)</ref> with F including all available features. Intuitively, one may expect the latter, IBLR-ML+, to be advantageous to the former, IBLR-ML, as it can use features in a more flexible way. Yet, one should note that, since we simply included all attributes in F, each attribute will essentially be used twice in IBLR-ML+, thus producing a kind of redundancy. Besides, model induction will of course become more difficult, since a larger number of parameters needs to be estimated.</p><p>As an additional baseline we used binary relevance learning (BR) with three different base learners: logistic regression, C4.5 (the WEKA <ref type="bibr" target="#b16">[17]</ref> implementation J48 in its default setting), and KNN (again with k = 10). Finally, we also included label powerset (LP) with C4.5 as a base learner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Data Sets</head><p>Benchmark data for multi-label classification is not as abundant as for conventional classification, and indeed, experiments in this field are often restricted to a very few or even only a single data set. For our experimental study, we have collected a comparatively large number of seven data sets from different domains; an overview is given in Table <ref type="table" target="#tab_0">1</ref>. <ref type="foot" target="#foot_0">1</ref>The emotions data was created from a selection of songs from 233 musical albums <ref type="bibr" target="#b17">[18]</ref>. From each song, a sequence of 30 seconds after the initial 30 seconds was extracted. The resulting sound clips were stored and converted into wave files of 22050 Hz sampling rate, 16-bit per sample and mono. From each wave file, 72 features have been extracted, falling into two categories: rhythmic and timbre. Then, in the emotion labeling process, 6 main emotional clusters are retained corresponding to the Tellegen-Watson-Clark model of mood: amazed-surprised, happy-pleased, relaxing-clam, quiet-still, sad-lonely and angry-aggressive.</p><p>Image and scene are semantic scene classification data sets proposed, respectively, by <ref type="bibr" target="#b18">[19]</ref> and <ref type="bibr" target="#b6">[7]</ref>, in which a picture can be categorized into one or more classes. In the scene data, for example, pictures can have the following classes: beach, sunset, foliage, field, mountain, and urban. Features of this data set correspond to spatial color moments in the LUV space. Color as well as spatial information have been shown to be fairly effective in distinguishing between certain types of outdoor scenes: bright and warm colors at the top of a picture may correspond to a sunset, while those at the bottom may correspond to a desert rock. Features of the image data set are generated by the SBN method <ref type="bibr" target="#b19">[20]</ref> and essentially correspond to attributes in an RGB color space.</p><p>From the biological field, we have chosen the two data sets yeast and genbase. The yeast data set is about predicting the functional classes of genes in the Yeast Saccharomyces cerevisiae. Each gene is described by the concatenation of microarray expression data and a phylogenetic profile, and is associated with a set of 14 functional classes. The data set contains 2417 genes in total, and each gene is represented by a 103-dimensional feature vector. In the genbase data, 27 important protein families are considered, including, for example, PDOC00064 (a class of oxydoreductases) and PDOC00154 (a class of isomerases). During the preprocessing, a training set was exported, consisting of 662 proteins that belong to one or more of these 27 classes.</p><p>From the text processing field, we have chosen a subset of the widely studied Reuters-21578 collection <ref type="bibr" target="#b20">[21]</ref>. The seven most frequent categories are considered. After removing documents whose label sets or main texts are empty, 8866 documents are retained where only 3.37% of them are associated with more than one class label. After randomly removing documents with only one label, a text categorization data set containing 2,000 documents is obtained. Each document is represented as a bag of instances using the standard sliding window techniques, where each instance corresponds to a text segment enclosed in one sliding window of size 50 (overlapped with 25 words). "Function words" are removed from the vocabulary and the remaining words are stemmed. Instances in the bags adopt the "bag-of-words" representation based on term frequency. Without loss of effectiveness, dimensionality reduction is performed by retaining the top 2% words with highest document frequency. Thereafter, each instance is represented as a 243-dimensional feature vector.</p><p>The mediamill data set is from the field of multimedia indexing and originates from the well-known TREC Video Retrieval Evaluation data <ref type="bibr">(TRECVID 2005</ref><ref type="bibr">(TRECVID /2006</ref>) initiated by American National Institute of Standards and Technology (NIST), which contains 85 hours of international broadcast news data. The task in this data set is the automated detection of a lexicon of 101 semantic concepts in videos. Every instance of this data set has 120 numeric features including visual, textual, as well as fusion information. The trained classifier should be able to categorize an unseen instance to some of these 101 labels, e.g., face, car, male, soccer, and so on. More details about this data set can be found at <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Evaluation Measures</head><p>To evaluate the performance of multilabel classification methods, a number of criteria and metrics have been proposed in the literature. For a classifier h, let h(x) ⊆ L denote its multilabel prediction for an instance x, and let L x denote the true set of relevant labels. Moreover, in case a related scoring function f is also defined, let f (x, λ) denote the score assigned to label λ for instance x. The most commonly used evaluation measures are defined as follows:</p><p>-Hamming loss computes the percentage of labels whose relevance is predicted incorrectly:</p><formula xml:id="formula_22">HamLoss(h) = 1 |L| h(x) ∆ L x ,<label>(14)</label></formula><p>where ∆ is the symmetric difference between two sets. -One error computes how many times the top-ranked label is not relevant:</p><formula xml:id="formula_23">OneError(f ) = 1 if arg max λ∈L f (x, λ) / ∈ L x 0 otherwise<label>(15)</label></formula><p>-Coverage determines how far one needs to go in the list of labels to cover all the relevant labels of an instance. This measure is loosely related to the precision at the level of perfect recall:</p><formula xml:id="formula_24">Coverage(f ) = max λ∈Lx rank f (x, λ) -1 ,<label>(16)</label></formula><p>where rank f (x, λ) denotes the position of label x in the ordering induced by f . -Rank loss computes the average fraction of label pairs that are not correctly ordered:</p><formula xml:id="formula_25">RankLoss(f ) = #{(λ, λ ) | f (x, λ) ≤ f (x, λ ), (λ, λ ) ∈ L x × L x } |L x ||L x | ,<label>(17)</label></formula><p>where L x = L \ L x is the set of irrelevant labels. -Average precision determines for each relevant label λ ∈ L x the percentage of relevant labels among all labels that are ranked above it, and averages these percentages over all relevant labels:</p><formula xml:id="formula_26">AvePrec(f ) = 1 |L x | λ∈Lx |{λ | rank f (x, λ ) ≤ rank f (x, λ), λ ∈ L x }| rank f (x, λ) .<label>(18)</label></formula><p>Notice that only Hamming loss evaluates mere multilabel predictions (i.e., the multilabel classifier h), while the others metrics evaluate the underlying ranking function f . Moreover, smaller values indicate better performance for all measures except average precision. Finally, except for coverage, all measures are normalized and assume values between 0 and 1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Results and Discussion</head><p>The results of a cross validation study (10-fold, 5 repeats) are summarized in Table <ref type="table">3</ref> at the end of the paper. As can be seen, the baseline methods BR and LP are in general not competitive. Looking at the average ranks, IBLR-ML consistently outperforms all other methods, regardless of the evaluation metric, indicating that it is the strongest method overall. The ranking among the three instance-based methods is IBLR-ML IBLR-ML+ MLKNN for all measures except OneError, for which the latter two change the position.</p><p>To analyze the results more thoroughly, we followed the two-step statistical test procedure recommended in <ref type="bibr" target="#b22">[23]</ref>, consisting of a Friedman test of the null hypothesis that all learners have equal performance and, in case this hypothesis is rejected, a Nemenyi test to compare learners in a pairwise way. Both tests are based on the average ranks as shown in the bottom line in Table <ref type="table">3</ref>. Even though the Friedman test suggests that there are significant differences between the methods, most of the pairwise comparisons remain statistically non-significant (at a significance level of 5%); see Fig. <ref type="figure" target="#fig_0">1</ref>. This is not surprising, however, given that the number of data sets included in the experiments, despite being much higher than usual, is still quite limited from a statistical point of view. Nevertheless, the overall picture taken from the experiments is clearly in favor of IBLR-ML.</p><p>As to MLKNN, it is interesting to compare this method with the BR-version of KNN. In fact, since MLKNN is a binary relevance learner, too, the only differ-Table <ref type="table">2</ref>. Classification error on binary classification problems. The number in brackets behind the performance value is the rank of the method on the corresponding data set (for each data set, the methods are ranked in decreasing order of performance). The average rank is the average of the ranks across all data sets. data set iblr-ml+ iblr-ml mlknn br-knn breast-cancer .280 <ref type="bibr" target="#b3">(4)</ref> .252 <ref type="bibr" target="#b0">(1)</ref> .259 <ref type="bibr" target="#b1">(2)</ref>  ence between these two methods concerns the incorporation of global information in MLKNN, which is accomplished through the Bayesian updating (1) of local information about the relevance of labels. From Table <ref type="table">3</ref>, it can be seen that MLKNN is better than BR-KNN in terms of all ranking measures, but not in terms of the Hamming loss, for which it is even a bit worse. Thus, in terms of mere relevance prediction, MLKNN does not seem to offer special advantages. Our explanation for this finding is that the incorporation of global information is indeed not useful for a simple 0/1 prediction. In a sense, this is perhaps not very surprising, given that the use of global information is somehow in conflict with the basic principle of local estimation underlying nearest neighbor prediction. Exploiting such information does, however, offer a reasonable way to break ties between class labels, which in turn explains the positive effect on ranking performance. In fact, one should note that, when simply scoring labels by the number of occurrences among the k neighbors of a query, such ties are quite likely; in particular, all non-relevant labels that never occur will have a score of 0 and will hence be tied. Resorting to global information about their relevance is then clearly more reasonable than breaking ties at random. To validate our conjecture that the incorporation of global information in MLKNN is actually not very useful for mere relevance prediction, we have conducted an additional experiments using 16 binary classification problems from the UCI repository. Using this type of data makes sense, since, for a binary relevance learner, minimizing Hamming loss is equivalent to minimizing 0/1 loss for m binary classification problems that are solved independently of each other.</p><p>The results of a 5 times 10-fold cross validation, summarized in Table <ref type="table">2</ref>, are completely in agreement with our previous study. MLKNN does indeed show the worst performance and is even outperformed by the simple BR-KNN. Interestingly, IBLR-ML+ is now a bit better than IBLR-ML. A reasonable explanation for this finding is that, compared to the multi-label case, the relevance information that comes from the neighbors of a query in binary classification only concerns a single label and, therefore, is rather sparse. Correspondingly, information about additional features is revaluated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Summary and Conclusions</head><p>We have presented a novel approach to instance-based learning, called IBLR, that can be used for classification in general and for multilabel classification in particular. Considering label information of neighbored examples as features of a query instance, the idea of IBLR is to reduce instance-based learning formally to logistic regression. An optimal balance between global and local inference, and in the extended version IBLR+ also between instance-based and model-based (attribute-oriented) learning, can then be achieved by the estimation of optimal regression coefficients.</p><p>For multilabel classification, this idea is especially appealing, as it allows one to take interdependencies between different labels into consideration. These dependencies are directly reflected by the sign and magnitude of related regression coefficients. This ability distinguishes IBLR from hitherto existing instancebased methods for multilabel classification, and is probably one of the main factors for its excellent performance. In fact, our extensive empirical study has clearly shown that IBLR improves upon existing methods, in particular the MLKNN method that can be considered as the state-of-the-art in instance-based multilabel classification.</p><p>Interestingly, our results also suggest that the basic idea underlying MLKNN, namely to combine instance-based learning and Bayesian inference, is beneficial for the ranking performance but not in terms of mere relevance prediction. Investigating the influence on specific performance measures in more detail, and elaborating on (instance-based) methods for minimizing specific loss functions, is an interesting topic of future work. Besides, for IBLR+, we plan to exploit the possibility to combine instance-based and model-based inference in a more sophisticated way, for example by selecting optimal feature subsets for both parts instead of simply using all features twice.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Comparison of all classifiers against each other with the Nemenyi test. Groups of classifiers that are not significantly different (at p = 0.05) are connected.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Statistics for the multilabel data sets used in the experiments. The symbol * indicates that the data set contains binary features; cardinality is the average number of labels per instance.</figDesc><table><row><cell cols="6">data set domain #instances #attributes #labels cardinality</cell></row><row><cell>emotions</cell><cell>music</cell><cell>593</cell><cell>72</cell><cell>6</cell><cell>1.87</cell></row><row><cell>image</cell><cell>vision</cell><cell>2000</cell><cell>135</cell><cell>5</cell><cell>1.24</cell></row><row><cell>genbase</cell><cell>biology</cell><cell>662</cell><cell>1186  *</cell><cell>27</cell><cell>1.25</cell></row><row><cell cols="2">mediamill multimedia</cell><cell>5000</cell><cell>120</cell><cell>101</cell><cell>4.27</cell></row><row><cell>reuters</cell><cell>text</cell><cell>7119</cell><cell>243</cell><cell>7</cell><cell>1.24</cell></row><row><cell>scene</cell><cell>vision</cell><cell>2407</cell><cell>294</cell><cell>6</cell><cell>1.07</cell></row><row><cell>yeast</cell><cell>biology</cell><cell>2417</cell><cell>103</cell><cell>14</cell><cell>4.24</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>All data sets are public available at http://mlkd.csd.auth.gr/multilabel.html and http://lamda.nju.edu.cn/data.htm.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Table <ref type="table">3</ref>. Experimental results in terms of different evaluation measures. The number in brackets behind the performance value is the rank of the method on the corresponding data set (for each data set, the methods are ranked in decreasing order of performance). The average rank is the average of the ranks across all data sets. iblr-ml+ iblr-ml mlknn lp br-lr br-c4. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Instance-based learning algorithms</title>
		<author>
			<persName><forename type="first">D</forename><surname>Aha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kibler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="66" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">ML-kNN: A lazy learning approach to multi-label learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2038" to="2048" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Boostexter: a boosting-based system for text categorization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="135" to="168" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Parametric mixture models for multi-label text</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ueda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Becker</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Obermayer</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="721" to="728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Maximal margin labeling for multi-topic text categorization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kazawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Izumitani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Taira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Maeda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Inf</title>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-label neural networks with applications to functional genomics and text categorization</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1338" to="1351" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning multi-label scene classiffication</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Boutell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1757" to="1771" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Knowledge discovery in multi-label phenotype data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Clare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Raedt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Siebes</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">2168</biblScope>
			<biblScope unit="page" from="42" to="53" />
			<date type="published" when="2001">2001</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A kernel method for multi-labelled classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Elisseeff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Becker</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="681" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Discriminative methods for multi-labeled classiffication</title>
		<author>
			<persName><forename type="first">S</forename><surname>Godbole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sarawagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Knowledge Discovery and Data Mining</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">3056</biblScope>
			<biblScope unit="page" from="20" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Decision trees for hierarchical multi-label classification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Vens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Struyf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schietgat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dzeroski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Blockeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="185" to="214" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Nearest Neighbor (NN) Norms: NN Pattern Classification Techniques</title>
		<author>
			<persName><forename type="first">B</forename><surname>Dasarathy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>IEEE Computer Society Press</publisher>
			<pubPlace>Los Alamitos, California</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning multi-label alternating decision tree from texts and data</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">D</forename><surname>Comite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gilleron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tommasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Perner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">2734</biblScope>
			<biblScope unit="page" from="35" to="49" />
			<date type="published" when="2003">2003</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
		<title level="m">Introduction to Statistical Relational Learning</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Link-based classification</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML-03</title>
		<meeting>ICML-03<address><addrLine>Washington</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="496" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Collective multi-label classification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ghamrawi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CIKM-05</title>
		<meeting>CIKM-05<address><addrLine>Bremen, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Data Mining: Practical Machine Learning Tools and Techniques. 2nd edn</title>
		<author>
			<persName><forename type="first">I</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Morgan Kaufmann</publisher>
			<pubPlace>San Francisco, CA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multilabel classification of music into emotions</title>
		<author>
			<persName><forename type="first">K</forename><surname>Trohidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tsoumakas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kalliris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Vlahavas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Music Information Retrieval</title>
		<meeting>Int. Conf. Music Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-instance multi-label learning with application to scene classification</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Inf</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Platt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1609" to="1616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multiple-instance learning for natural scene classification</title>
		<author>
			<persName><forename type="first">O</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Ratan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML, Madison WI</title>
		<meeting>ICML, Madison WI</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="341" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Machine learning in automated text categorization</title>
		<author>
			<persName><forename type="first">F</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The challenge problem for automated detection of 101 semantic concepts in multimedia</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Geusebroek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Multimedia</title>
		<meeting>ACM Multimedia<address><addrLine>Santa Barbara, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="421" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Statistical comparisons of classifiers over multiple data sets</title>
		<author>
			<persName><forename type="first">J</forename><surname>Demsar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
