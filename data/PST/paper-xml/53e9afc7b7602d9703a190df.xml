<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Online Robust PCA via Stochastic Optimization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
							<email>jiashi@nus.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">ECE Department</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huan</forename><surname>Xu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">ME Department</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">ECE Department</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Online Robust PCA via Stochastic Optimization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4A7A3079109C7ADEE11E894E64D6E863</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Robust PCA methods are typically based on batch optimization and have to load all the samples into memory during optimization. This prevents them from efficiently processing big data. In this paper, we develop an Online Robust PCA (OR-PCA) that processes one sample per time instance and hence its memory cost is independent of the number of samples, significantly enhancing the computation and storage efficiency. The proposed OR-PCA is based on stochastic optimization of an equivalent reformulation of the batch RPCA. Indeed, we show that OR-PCA provides a sequence of subspace estimations converging to the optimum of its batch counterpart and hence is provably robust to sparse corruption. Moreover, OR-PCA can naturally be applied for tracking dynamic subspace. Comprehensive simulations on subspace recovering and tracking demonstrate the robustness and efficiency advantages of the OR-PCA over online PCA and batch RPCA methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Principal Component Analysis (PCA) <ref type="bibr" target="#b18">[19]</ref> is arguably the most widely used method for dimensionality reduction in data analysis. However, standard PCA is brittle in the presence of outliers and corruptions <ref type="bibr" target="#b10">[11]</ref>. Thus many techniques have been developed towards robustifying it <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b6">7]</ref>. One prominent example is the Principal Component Pursuit (PCP) method proposed in <ref type="bibr" target="#b3">[4]</ref> that robustly finds the low-dimensional subspace through decomposing the sample matrix into a low-rank component and an overall sparse component. It is proved that both components can be recovered exactly through minimizing a weighted combination of the nuclear norm of the first term and 1 norm of the second one. Thus the subspace estimation is robust to sparse corruptions. However, PCP and other robust PCA methods are all implemented in a batch manner. They need to access every sample in each iteration of the optimization. Thus, robust PCA methods require memorizing all samples, in sharp contrast to standard PCA where only the covariance matrix is needed. This pitfall severely limits their scalability to big data, which are becoming ubiquitous now. Moreover, for an incremental samples set, when a new sample is added, the optimization procedure has to be re-implemented on all available samples. This is quite inefficient in dealing with incremental sample sets such as network detection, video analysis and abnormal events tracking.</p><p>Another pitfall of batch robust PCA methods is that they cannot handle the case where the underlying subspaces are changing gradually. For example, in the video background modeling, the background is assumed to be static across different frames for applying robust PCA <ref type="bibr" target="#b3">[4]</ref>. Such assumption is too restrictive in practice. A more realistic situation is that the background is changed gradually along with the camera moving, corresponding to a gradually changing subspace. Unfortunately, traditional batch RPCA methods may fail in this case.</p><p>In order to efficiently and robustly estimate the subspace of a large-scale or dynamic samples set, we propose an Online Robust PCA (OR-PCA) method. OR-PCA processes only one sample per time instance and thus is able to efficiently handle big data and dynamic sample sets, saving the memory cost and dynamically estimating the subspace of evolutional samples. We briefly explain our intuition here. The major difficulty of implementing the previous RPCA methods, such as PCP, in an online fashion is that the adopted nuclear norm tightly couples the samples and thus the samples have to be processed simultaneously. To tackle this, OR-PCA pursues the low-rank component in a different manner: using an equivalent form of the nuclear norm, OR-PCA explicitly decomposes the sample matrix into the multiplication of the subspace basis and coefficients plus a sparse noise component. Through such decomposition, the samples are decoupled in the optimization and can be processed separately. In particular, the optimization consists of two iterative updating components. The first one is to project the sample onto the current basis and isolate the sparse noise (explaining the outlier contamination), and the second one is to update the basis given the new sample.</p><p>Our main technical contribution is to show the above mentioned iterative optimization sheme converges to the global optimal solution of the original PCP formulation, thus we establish the validity of our online method. Our proof is inspired by recent results from <ref type="bibr" target="#b15">[16]</ref>, who proposed an online dictionary learning method and provided the convergence guarantee of the proposed online dictionary learning method. However, <ref type="bibr" target="#b15">[16]</ref> can only guarantee that the solution converges to a stationary point of the optimization problem.</p><p>Besides the nice behavior on single subspace recovering, OR-PCA can also be applied for tracking time-variant subspace naturally, since it updates the subspace estimation timely after revealing one new sample. We conduct comprehensive simulations to demonstrate the advantages of OR-PCA for both subspace recovering and tracking in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The robust PCA algorithms based on nuclear norm minimization to recover low-rank matrices are now standard, since the seminal works <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b5">6]</ref>. Recent works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> have taken the nuclear norm minimization approach to the decomposition of a low-rank matrix and an overall sparse matrix. Different from the setting of samples being corrupted by sparse noise, <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b23">24]</ref> and <ref type="bibr" target="#b6">[7]</ref> solve robust PCA in the case that a few samples are completely corrupted. However, all of these RPCA methods are implemented in batch manner and cannot be directly adapted to the online setup.</p><p>There are only a few pieces of work on online robust PCA <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b9">10]</ref>, which we discuss below. In <ref type="bibr" target="#b12">[13]</ref>, an incremental and robust subspace learning method is proposed. The method proposes to integrate the M -estimation into the standard incremental PCA calculation. Specifically, each newly coming data point is re-weighted by a pre-defined influence function <ref type="bibr" target="#b10">[11]</ref> of its residual to the current estimated subspace. However, no performance guarantee is provided in this work. In <ref type="bibr" target="#b19">[20]</ref>, a compressive sensing based recursive robust PCA algorithm is proposed. The proposed method essentially solves compressive sensing optimization over a small batch of data to update the principal components estimation instead of using a single sample, and it is not clear how to extend the method to the latter case. Recently, He et al. propose an incremental gradient descent method on Grassmannian manifold for solving the robust PCA problem, named GRASTA <ref type="bibr" target="#b9">[10]</ref>. In each iteration, GRASTA uses the gradient of the updated augmented Lagrangian function after revealing a new sample to perform the gradient descent. However, no theoretic guarantee of the algorithmic convergence for GRASTA is provided in this work. Moreover, in the experiments in this work, we show that our proposed method is more robust than GRASTA to the sparse corruption and achieves higher breakdown point.</p><p>The most closely related work to ours in technique is <ref type="bibr" target="#b15">[16]</ref>, which proposes an online learning method for dictionary learning and sparse coding. Based on that work, <ref type="bibr" target="#b8">[9]</ref> proposes an online nonnegative matrix factorization method. Both works can be seen as solving online matrix factorization problems with specific constraints (sparse or non-negative). Though OR-PCA can also be seen as a kind of matrix factorization, it is essentially different from those two works. In OR-PCA, an additive sparse noise matrix is considered along with the matrix factorization. Thus the optimization and analysis are different from the ones in those works. In addition, benefitting from explicitly considering the noise, OR-PCA is robust to sparse contamination, which is absent in either the dictionary learning or nonnegative matrix factorization works. Most importantly, in sharp contrast to <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b8">9]</ref> which shows their methods converge to a stationary point, our method is solving essentially a re-formulation of a convex optimization, and hence we can prove that the method converges to the global optimum.</p><p>After this paper was accepted, we found similar works which apply the same main idea of combining the online learning framework in <ref type="bibr" target="#b15">[16]</ref> with the factorization formulation of nuclear norm was published in <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">23]</ref> before. However, in this work, we use different optimization from them. More specifically, our proposed algorithm needs not determine the step size or solve a Lasso subproblem.</p><p>3 Problem Formulation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Notation</head><p>We use bold letters to denote vectors. In particular, x ∈ R p denotes an authentic sample without corruption, e ∈ R p is for the noise, and z ∈ R p is for the corrupted observation z = x + e. Here p denotes the ambient dimension of the observed samples. Let r denote the intrinsic dimension of the subspace underlying {x i } n i=1 . Let n denote the number of observed samples, t denote the index of the sample/time instance. We use capital letters to denote matrices, e.g., Z ∈ R p×n is the matrix of observed samples. Each column z i of Z corresponds to one sample. For an arbitrary real matrix E, Let E F denote its Frobenius norm, E 1 = i,j |E ij | denote the 1 -norm of E seen as a long vector in R p×n , and E * = i σ i (E) denote its nuclear norm, i.e., the sum of its singular values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Objective Function Formulation</head><p>Robust PCA (RPCA) aims to accurately estimate the subspace underlying the observed samples, even though the samples are corrupted by gross but sparse noise. As one of the most popular RPCA methods, the Principal Component Pursuit (PCP) method <ref type="bibr" target="#b3">[4]</ref> proposes to solve RPCA by decomposing the observed sample matrix Z into a low-rank component X accounting for the low-dimensional subspace plus an overall sparse component E incorporating the sparse corruption. Under mild conditions, PCP guarantees that the two components X and E can be exactly recovered through solving:</p><formula xml:id="formula_0">min X,E 1 2 Z -X -E 2 F + λ 1 X * + λ 2 E 1 .<label>(1)</label></formula><p>To solve the problem in (1), iterative optimization methods such as Accelerated Proximal Gradient (APG) <ref type="bibr" target="#b14">[15]</ref> or Augmented Lagrangian Multiplier (ALM) <ref type="bibr" target="#b13">[14]</ref> methods are often used. However, these optimization methods are implemented in a batch manner. In each iteration of the optimization, they need to access all samples to perform SVD. Hence a huge storage cost is incurred when solving RPCA for big data (e.g., web data, large image set).</p><p>In this paper, we consider online implementation of PCP. The main difficulty is that the nuclear norm couples all the samples tightly and thus the samples cannot be considered separately as in typical online optimization problems. To overcome this difficulty, we use an equivalent form of the nuclear norm for the matrix X whose rank is upper bounded by r, as follows <ref type="bibr" target="#b20">[21]</ref>,</p><formula xml:id="formula_1">X * = inf L∈R p×r ,R∈R n×r 1 2 L 2 F + 1 2 R 2 F : X = LR T .</formula><p>Namely, the nuclear norm is re-formulated as an explicit low-rank factorization of X. Such nuclear norm factorization is developed in <ref type="bibr" target="#b2">[3]</ref> and well established in recent works <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b20">21]</ref>. In this decomposition, L ∈ R p×r can be seen as the basis of the low-dimensional subspace and R ∈ R n×r denotes the coefficients of the samples w.r.t. the basis. Thus, the RPCA problem (1) can be re-formulated as</p><formula xml:id="formula_2">min X,L∈R p×r ,R∈R n×r ,E 1 2 Z -X -E 2 F + λ 1 2 ( L 2 F + R 2 F ) + λ 2 E 1 , s.t. X = LR T .</formula><p>Substituting X by LR T and removing the constraint, the above problem is equivalent to:</p><formula xml:id="formula_3">min L∈R p×r ,R∈R n×r ,E 1 2 Z -LR T -E 2 F + λ 1 2 ( L 2 F + R 2 F ) + λ 2 E 1 .<label>(2)</label></formula><p>Though the reformulated objective function is not jointly convex w.r.t. the variables L and R, we prove below that the local minima of ( <ref type="formula" target="#formula_3">2</ref>) are global optimal solutions to original problem in <ref type="bibr" target="#b0">(1)</ref>. The details are given in the next section.</p><p>Given a finite set of samples Z = [z 1 , . . . , z n ] ∈ R p×n , solving problem (2) indeed minimizes the following empirical cost function,</p><formula xml:id="formula_4">f n (L) 1 n n i=1 (z i , L) + λ 1 2n L 2 F ,<label>(3)</label></formula><p>where the loss function for each sample is defined as</p><formula xml:id="formula_5">(z i , L) min r,e 1 2 z i -Lr -e 2 2 + λ 1 2 r 2 2 + λ 2 e 1 .<label>(4)</label></formula><p>The loss function measures the representation error for the sample z on a fixed basis L, where the coefficients on the basis r and the sparse noise e associated with each sample are optimized to minimize the loss. In the stochastic optimization, one is usually interested in the minimization of the expected cost overall all the samples <ref type="bibr" target="#b15">[16]</ref>,</p><formula xml:id="formula_6">f (L) E z [ (z, L)] = lim n→∞ f n (L),<label>(5)</label></formula><p>where the expectation is taken w.r.t. the distribution of the samples z. In this work, we first establish a surrogate function for this expected cost and then optimize the surrogate function for obtaining the subspace estimation in an online fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Stochastic Optimization Algorithm for OR-PCA</head><p>We now present our Online Robust PCA (OR-PCA) algorithm. The main idea is to develop a stochastic optimization algorithm to minimize the empirical cost function <ref type="bibr" target="#b2">(3)</ref>, which processes one sample per time instance in an online manner. The coefficients r, noise e and basis L are optimized in an alternative manner. In the t-th time instance, we obtain the estimation of the basis L t through minimizing the cumulative loss w.r.t. the previously estimated coefficients {r i } t i=1 and sparse noise {e i } t i=1 . The objective function for updating the basis L t is defined as,</p><formula xml:id="formula_7">g t (L) 1 t t i=1 1 2 z i -Lr i -e i 2 2 + λ 1 2 r i 2 2 + λ 2 e i 1 + λ 1 2t L 2 F .<label>(6)</label></formula><p>This is a surrogate function of the empirical cost function f t (L) defined in (3), i.e., it provides an upper bound for f t (L):</p><formula xml:id="formula_8">g t (L) ≥ f t (L).</formula><p>The proposed algorithm is summarized in Algorithm 1. Here, the subproblem in <ref type="bibr" target="#b6">(7)</ref> involves solving a small-size convex optimization problem, which can be solved efficiently by the off-the-shelf solver (see the supplementary material). To update the basis matrix L, we adopt the block-coordinate descent with warm restarts <ref type="bibr" target="#b1">[2]</ref>. In particular, each column of the basis L is updated individually while fixing the other columns.</p><p>The following theorem is the main theoretic result of the paper, which states that the solution from Algorithm 1 will converge to the optimal solution of the batch optimization. Thus, the proposed OR-PCA converges to the correct low-dimensional subspace even in the presence of sparse noise, as long as the batch version -PCP -works.</p><p>Theorem 1. Assume the observations are always bounded. Given the rank of the optimal solution to (5) is provided as r, and the solution L t ∈ R p×r provided by Algorithm 1 is full rank, then L t converges to the optimal solution of (5) asymptotically.</p><p>Note that the assumption that observations are bounded is quite natural for the realistic data (such as images, videos). We find in the experiments that the final solution L t is always full rank. A standard stochastic gradient descent method may further enhance the computational efficiency, compared with the used method here. We leave the investigation for future research.</p><p>Algorithm 1 Stochastic Optimization for OR-PCA Input: {z 1 , . . . , z T } (observed data which are revealed sequentially), λ 1 , λ 2 ∈ R (regularization parameters), L 0 ∈ R p×r , r 0 ∈ R r , e 0 ∈ R p (initial solutions), T (number of iterations). for t = 1 to T do 1) Reveal the sample z t .</p><p>2) Project the new sample:</p><formula xml:id="formula_9">{r t , e t } = arg min 1 2 z t -L t-1 r -e 2 2 + λ 1 2 r 2 2 + λ 2 e 1 .<label>(7)</label></formula><p>3) A t ← A t-1 + r t r T t , B t ← B t-1 + (z t -e t )r T t . 4) Compute L t with L t-1 as warm restart using Algorithm 2:</p><formula xml:id="formula_10">L t arg min 1 2 Tr L T (A t + λ 1 I) L -Tr(L T B t ).<label>(8)</label></formula><p>end for Return X T = L T R T T (low-rank data matrix), E T (sparse noise matrix).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 The Basis Update</head><formula xml:id="formula_11">Input: L = [l 1 , . . . , l r ] ∈ R p×r , A = [a 1 , . . . , a r ] ∈ R r×r , and B = [b 1 , . . . , b r ] ∈ R p×r . Ã ← A + λ 1 I. for j = 1 to r do l j ← 1 Ãj,j (b j -Lã j ) + l j .<label>(9)</label></formula><p>end for Return L.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Proof Sketch</head><p>In this section we sketch the proof of Theorem 1. The details are deferred to the supplementary material due to space limit.</p><p>The proof of Theorem 1 proceeds in the following four steps: (I) we first prove that the surrogate function g t (L t ) converges almost surely; (II) we then prove that the solution difference behaves as L t -L t-1 F = O(1/t); (III) based on (II) we show that f (L t ) -g t (L t ) → 0 almost surely, and the gradient of f vanishes at the solution L t when t → ∞; (IV) finally we prove that L t actually converges to the optimum solution of the problem (5).</p><p>Theorem 2 (Convergence of the surrogate function g t ). Let g t denote the surrogate function defined in <ref type="bibr" target="#b5">(6)</ref>. Then, g t (L t ) converges almost surely when the solution L t is given by Algorithm 1.</p><p>We prove Theorem 2, i.e., the convergence of the stochastic positive process g t (L t ) &gt; 0, by showing that it is a quasi-martingale. We first show that the summation of the positive difference of g t (L t ) is bounded utilizing the fact that g t (L t ) upper bounds the empirical cost f t (L t ) and the loss function (z t , L t ) is Lipschitz. These imply that g t (L t ) is a quasi-martingale. Applying the lemma from <ref type="bibr" target="#b7">[8]</ref> about the convergence of quasi-martingale, we conclude that g t (L t ) converges.</p><p>Next, we show the difference of the two successive solutions converges to 0 as t goes to infinity.</p><p>Theorem 3 (Difference of the solution L t ). For the two successive solutions obtained from Algorithm 1, we have L t+1 -L t F = O(1/t) a.s.</p><p>To prove the above result, we first show that the function g t (L) is strictly convex. This holds since the regularization component λ 1 L 2 F naturally guarantees that the eigenvalues of the Hessian matrix are bounded away from zero. Notice that this is essentially different from <ref type="bibr" target="#b15">[16]</ref>, where one has to assume that the smallest eigenvalue of the Hessian matrix is lower bounded. Then we further show that variation of the function g t (L), g t (L t ) -g t+1 (L t ), is Lipschitz if using the updating rule shown in Algorithm 2. Combining these two properties establishes Theorem 3.</p><p>In the third step, we show that the expected cost function f (L t ) is a smooth one, and the difference f (L t ) -g t (L t ) goes to zero when t → ∞. In order for showing the regularity of the function f (L t ), we first provide the following optimality condition of the loss function (L t ). Lemma 1 (Optimality conditions of Problem ( <ref type="formula" target="#formula_5">4</ref>)). r ∈ R r and e ∈ R p is a solution of Problem (4) if and only if</p><formula xml:id="formula_12">C Λ (z Λ -e Λ ) = λ 2 sign(e Λ ), |C Λ c (z Λ c -e Λ c )| ≤ λ 2 , otherwise, r = (L T L + λ 1 I) -1 L T (z -e ),</formula><p>where C = I -L(L T L + λ 1 I) -1 L T and C Λ denotes the columns of matrix C indexed by Λ = {j|e [j] = 0} and Λ c denotes the complementary set of Λ. Moreover, the optimal solution is unique.</p><p>Based on the above lemma, we can prove that the solution r and e are Lipschitz w.r.t. the basis L. Then, we can obtain the following results about the regularity of the expected cost function f . Lemma 2. Assume the observations z are always bounded. Define</p><formula xml:id="formula_13">{r , e } = arg min r,e 1 2 z -Lr -e 2 2 + λ 1 2 r 2 2 + λ 2 e 1 .</formula><p>Then, 1) the function defined in (4) is continuously differentiable and</p><formula xml:id="formula_14">∇ L (z, L) = (Lr + e -z)r T ; 2) ∇f (L) = E z [∇ L (z, L)]; and 3)∇f (L) is Lipschitz.</formula><p>Equipped with the above regularities of the expected cost function f , we can prove the convergence of f , as stated in the following theorem. Theorem 4 (Convergence of f ). Let g t denote the surrogate function defined in (2). Then, 1) f (L t ) -g t (L t ) converges almost surely to 0; and 2) f (L t ) converges almost surely, when the solution L t is given by Algorithm 1.</p><p>Following the techniques developed in <ref type="bibr" target="#b15">[16]</ref>, we can show the solution obtained from Algorithm 1, L ∞ , satisfies the first order optimality condition for minimizing the expected cost f (L). Thus the OR-PCA algorithm provides a solution converging to a stationary point of the expected loss. Theorem 5. The first order optimal condition for minimizing the objective function in (5) is satisfied by L t , the solution provided by Algorithm 1, when t tends to infinity.</p><p>Finally, to complete the proof, we establish the following result stating that any full-rank L that satisfies the first order condition is the global optimal solution. Theorem 6. When the solution L satisfies the first order condition for minimizing the objective function in (5) , the obtained solution L is the optimal solution of the problem (5) if L is full rank.</p><p>Combining Theorem 5 and Theorem 6 directly yields Theorem 1 -the solution from Algorithm 1 converges to the optimal solution of Problem (5) asymptotically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Empirical Evaluation</head><p>We report some numerical results in this section. Due to space constraints, more results, including those of subspace tracking, are deferred in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Medium-scale Robust PCA</head><p>We here evaluate the ability of the proposed OR-PCA of correctly recovering the subspace of corrupted observations, under various settings of the intrinsic subspace dimension and error density. In particular, we adopt the batch robust PCA method, Principal Component Pursuit <ref type="bibr" target="#b3">[4]</ref>, as the batch counterpart of the proposed OR-PCA method for reference. PCP estimates the subspace in a batch manner through solving the problem in (1) and outputs the low-rank data matrix. For fair comparison, we follow the data generation scheme of PCP as in <ref type="bibr" target="#b3">[4]</ref>: we generate a set of n clean data points as a product of X = U V T , where the sizes of U and V are p × r and n × r respectively. The elements of both U and V are i.i.d. sampled from the N (0, 1/n) distribution. Here U is the basis of the subspace and the intrinsic dimension of the subspace spanned by U is r. The observations are generated through Z = X + E, where E is a sparse matrix with a fraction of ρ s non-zero elements. The elements in E are from a uniform distribution over the interval of [-1000, 1000]. Namely, the matrix E contains gross but sparse errors.</p><p>We run the OR-PCA and the PCP algorithms 10 times under the following settings: the ambient dimension and number of samples are set as p = 400 and n = 1, 000; the intrinsic rank r of the subspace varies from 4 to 200; the value of error fraction, ρ s , varies from very sparse 0.01 to relatively dense 0.5. The trade-off parameters of OR-PCA are fixed as λ 1 = λ 2 = 1/ √ p. The performance is evaluated by the similarity between the subspace obtained from the algorithms and the groundtruth. In particular, the similarity is measured by the Expressed Variance (E.V.) (see definition in <ref type="bibr" target="#b23">[24]</ref>). A larger value of E.V. means better subspace recovery.</p><p>We plot the averaged E.V. values of PCP and OR-PCA under different settings in a matrix form, as shown in Figure <ref type="figure" target="#fig_0">1</ref>(a) and Figure <ref type="figure" target="#fig_0">1</ref>(b) respectively. The results demonstrate that under relatively low intrinsic dimension (small rank/n) and sparse corruption (small ρ s ), OR-PCA is able to recover the subspace nearly perfectly (E.V.= 1). We also observe that the performance of OR-PCA is close to that of the PCP. This demonstrates that the proposed OR-PCA method achieves comparable performance with the batch method and verifies our convergence guarantee on the OR-PCA. In the relatively difficult setting (high intrinsic dimension and dense error, shown in the top-right of the matrix), OR-PCA performs slightly worse than the PCP, possibly because the number of streaming samples is not enough to achieve convergence.</p><p>To better demonstrate the robustness of OR-PCA to corruptions and illustrate how the performance of OR-PCA is improved when more samples are revealed, we plot the performance curve of OR-PCA against the number of samples in Figure <ref type="figure" target="#fig_0">1</ref>(c), under the setting of p = 400, n = 1, 000, ρ s = 0.1, r = 80, and the results are averaged from 10 repetitions. We also apply GRASTA <ref type="bibr" target="#b9">[10]</ref> to solve this RPCA problem in an online fashion as a baseline. The parameters of GRASTA are set as the values provided in the implementation package provided by the authors. We observe that when more samples are revealed, both OR-PCA and GRASTA steadily improve the subspace recovery. However, our proposed OR-PCA converges much faster than GRASTA, possibly because in each iteration OR-PCA obtains the optimal closed-form solution to the basis updating subproblem while GRASTA only takes one gradient descent step. Observe from the figure that after 200 samples are revealed, the performance of OR-PCA is already satisfactory (E.V.&gt; 0.8). However, for GRASTA, it needs about 400 samples to achieve the same performance. To show the robustness of the proposed OR-PCA, we also plot the performance of the standard online (or incremental) PCA <ref type="bibr" target="#b0">[1]</ref> for comparison. This work focuses on developing online robust PCA. The non-robustness of (online) PCA is independent of used optimization method. Thus, we only compare with the basic online PCA method <ref type="bibr" target="#b0">[1]</ref>, which is enough for comparing robustness. The comparison results are given in Figure <ref type="figure" target="#fig_0">1</ref>(c). We observe that as expected, the online PCA cannot recover the subspace correctly (E.V.≈ 0.1), since standard PCA is fragile to gross corruptions. We then increase the corruption level to ρ s = 0.3, and plot the performance curve of the above methods in Figure <ref type="figure" target="#fig_0">1(d)</ref>. From the plot, it can be observed that the performance of GRASTA decreases severely (E.V.≈ 0.3) while OR-PCA still achieves E.V. ≈ 0.8. The performance of PCP is around 0.88. This result clearly demonstrates the robustness advantage of OR-PCA over GRASTA. In fact, from other simulation results under different settings of intrinsic rank and corruption level (see supplementary material), we observe that the GRASTA breaks down at 25% corruption (the value of E.V. is zero). However, OR-PCA achieves a performance of E.V.≈ 0.5, even in presence of 50% outlier corruption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Large-scale Robust PCA</head><p>We now investigate the computational efficiency of OR-PCA and the performance for large scale data. The samples are generated following the same model as explained in the above subsection.</p><p>The results are provided in Table <ref type="table" target="#tab_0">1</ref>. All of the experiments are implemented in a PC with 2.83GHz Quad CPU and 8GB RAM. Note that batch RPCA cannot process these data due to out of memory. From the above results, we observe that OR-PCA is much more efficient and performs better than GRASTA. In fact, the computational time of OR-PCA is linear in the sample size and nearly linear in the ambient dimension. When the ambient dimension is large (p = 1 × 10 4 ), OR-PCA is more efficient than GRASTA with an order magnitude efficiency enhancement. We then compare OR-PCA with batch PCP. In each iteration, batch PCP needs to perform an SVD plus a thresholding operation, whose complexity is O(np 2 ). In contrast, for OR-PCA, in each iteration, the computational cost is O(pr 2 ), which is independent of the sample size and linear in the ambient dimension. To see this, note that in step 2) of Algorithm 1, the computation complexity is O(r 2 + pr + r 3 ). Here O(r 3 ) is for computing L T L. The complexity of step 3) is O(r 2 + pr). For step 4) (i.e., Algorithm 2), the cost is O(pr 2 ) (updating each column of L requires O(pr) and there are r columns in total). Thus the total complexity is O(r 2 + pr + r 3 + pr 2 ). Since p r, the overall complexity is O(pr 2 ).</p><p>The memory cost is significantly reduced too. The memory required for OR-PCA is O(pr), which is independent of the sample size. This is much smaller than the memory cost of the batch PCP algorithm (O(pn)), where n p for large scale dataset. This is quite important for processing big data. The proposed OR-PCA algorithm can be easily parallelized to further enhance its efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>In this work, we develop an online robust PCA (OR-PCA) method. Different from previous batch based methods, the OR-PCA need not "remember" all the past samples and achieves much higher storage efficiency. The main idea of OR-PCA is to reformulate the objective function of PCP (a widely applied batch RPCA algorithm) by decomposing the nuclear norm to an explicit product of two low-rank matrices, which can be solved by a stochastic optimization algorithm. We provide the convergence analysis of the OR-PCA method and show that OR-PCA converges to the solution of batch RPCA asymptotically. Comprehensive simulations demonstrate the effectiveness of OR-PCA.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (a) and (b): subspace recovery performance under different corruption fraction ρ s (vertical axis) and rank/n (horizontal axis). Brighter color means better performance; (c) and (d): the performance comparison of the OR-PCA, Grasta, and online PCA methods against the number of revealed samples under two different corruption levels ρ s with PCP as reference.</figDesc><graphic coords="7,123.51,92.19,72.54,56.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The comparison of OR-PCA and GRASTA under different settings of sample size (n) and ambient dimensions (p). Here ρ s = 0.3, r = 0.1p. The corresponding computational time (in ×10 3 seconds) is shown in the top row and the E.V. values are shown in the bottom row correspondingly. The results are based on the average of 5 repetitions and the variance is shown in the parentheses.</figDesc><table><row><cell>p</cell><cell></cell><cell>1 × 10 3</cell><cell></cell><cell cols="2">1 × 10 4</cell></row><row><cell>n</cell><cell>1 × 10 6</cell><cell>1 × 10 8</cell><cell>1 × 10 10</cell><cell>1 × 10 6</cell><cell>1 × 10 8</cell></row><row><cell>OR-PCA</cell><cell cols="4">0.013(0.0004) 1.312(0.082) 139.233(7.747) 0.633(0.047) 0.99(0.01) 0.99(0.00) 0.99(0.00) 0.82(0.09)</cell><cell>15.910(2.646) 0.82(0.01)</cell></row><row><cell>GRASTA</cell><cell cols="5">0.023(0.0008) 2.137(0.016) 240.271(7.564) 2.514(0.011) 252.630(2.096) 0.54(0.08) 0.55(0.02) 0.57(0.03) 0.45(0.02) 0.46(0.03)</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>J. Feng and S. Yan are supported by the Singapore National Research Foundation under its International Research Centre @Singapore Funding Initiative and administered by the IDM Programme Office. H. Xu is partially supported by the Ministry of Education of Singapore through AcRF Tier Two grant R-265-000-443-112 and NUS startup grant R-265-000-384-133.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Incremental pca for on-line visual learning and recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Artac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 16th International Conference on</title>
		<meeting>16th International Conference on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002">2002. 2002</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="781" to="784" />
		</imprint>
	</monogr>
	<note>Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Nonlinear programming</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Athena Scientific</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A nonlinear programming algorithm for solving semidefinite programs via low-rank factorization</title>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Burer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renato</forename><surname>Monteiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Progam</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Robust principal component analysis?</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Candes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<idno>ArXiv:0912.3599</idno>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rank-sparsity incoherence for matrix decomposition</title>
		<author>
			<persName><forename type="first">V</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sanghavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Parrilo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Willsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="572" to="596" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Matrix rank minimization with applications</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fazel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Robust PCA in high-dimension: A deterministic approach</title>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Quasi-martingales</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Fisk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the American Mathematical Society</title>
		<imprint>
			<date type="published" when="1965">1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Online nonnegative matrix factorization with robust stochastic approximation. Neural Networks and Learning Systems</title>
		<author>
			<persName><forename type="first">N</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1087" to="1099" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Online robust subspace tracking from partial information</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Balzano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Lui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1109.3827</idno>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ronchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myilibrary</forename></persName>
		</author>
		<title level="m">Robust statistics</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Robpca: a new approach to robust principal component analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Rousseeuw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">V</forename><surname>Branden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On incremental and robust subspace learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The augmented lagrange multiplier method for exact recovery of corrupted low-rank matrices</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1009.5055</idno>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fast convex optimization algorithms for exact recovery of a corrupted low-rank matrix</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Computational Advances in Multi-Sensor Adaptive Processing</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Online learning for matrix factorization and sparse coding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Dynamic anomalography: Tracking network anomalies via sparsity and low rank</title>
		<author>
			<persName><forename type="first">Morteza</forename><surname>Mardani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gonzalo</forename><surname>Mateos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Giannakis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rank minimization for subspace tracking from incomplete data</title>
		<author>
			<persName><forename type="first">Morteza</forename><surname>Mardani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gonzalo</forename><surname>Mateos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgios</forename><forename type="middle">B</forename><surname>Giannakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On lines and planes of closest fit to systems of points in space</title>
		<author>
			<persName><forename type="first">K</forename><surname>Pearson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Magazine</title>
		<imprint>
			<date type="published" when="1901">1901</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hogben</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1211.3754</idno>
		<title level="m">Recursive robust pca or recursive sparse recovery in large but structured noise</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fazel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Parrilo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM review</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="471" to="501" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fast maximum margin matrix factorization for collaborative prediction</title>
		<author>
			<persName><forename type="first">Jasson</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning efficient sparse and low rank models</title>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Sprechmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.3631</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Principal component analysis with contaminated data: The high dimensional case</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Caramanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Robust pca via outlier pursuit. Information Theory</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Caramanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sanghavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="3047" to="3064" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
