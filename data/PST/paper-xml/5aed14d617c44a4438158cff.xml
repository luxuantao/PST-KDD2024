<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Billion-scale Commodity Embedding for E-commerce Recommendation in Alibaba</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jizhe</forename><surname>Wang</surname></persName>
							<email>jizhe.wjz@alibaba-inc.com</email>
						</author>
						<author>
							<persName><forename type="first">Pipei</forename><surname>Huang</surname></persName>
							<email>pipei.hpp@alibaba-inc.com</email>
						</author>
						<author>
							<persName><forename type="first">Huan</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhibo</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Binqiang</forename><surname>Zhao</surname></persName>
							<email>binqiang.zhao@alibaba-inc.com</email>
						</author>
						<author>
							<persName><forename type="first">Dik</forename><forename type="middle">Lun</forename><surname>Lee</surname></persName>
							<email>dlee@cse.ust.hk</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Alibaba Group Hangzhou</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Hong Kong University of Science and Technology Kowloon</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">Alibaba Group Beijing</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Hong Kong University of Science and Technology Kowloon</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Billion-scale Commodity Embedding for E-commerce Recommendation in Alibaba</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3219819.3219869</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Recommendation system</term>
					<term>Collaborative filtering</term>
					<term>Graph Embedding</term>
					<term>E-commerce Recommendation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recommender systems (RSs) have been the most important technology for increasing the business in Taobao, the largest online consumer-to-consumer (C2C) platform in China. There are three major challenges facing RS in Taobao: scalability, sparsity and cold start. In this paper, we present our technical solutions to address these three challenges. The methods are based on a wellknown graph embedding framework. We first construct an item graph from users' behavior history, and learn the embeddings of all items in the graph. The item embeddings are employed to compute pairwise similarities between all items, which are then used in the recommendation process. To alleviate the sparsity and cold start problems, side information is incorporated into the graph embedding framework. We propose two aggregation methods to integrate the embeddings of items and the corresponding side information. Experimental results from offline experiments show that methods incorporating side information are superior to those that do not. Further, we describe the platform upon which the embedding methods are deployed and the workflow to process the billion-scale data in Taobao. Using A/B test, we show that the online Click-Through-Rates (CTRs) are improved comparing to the previous collaborative filtering based methods widely used in Taobao, further demonstrating the effectiveness and feasibility of our proposed methods in Taobao's live production environment.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Internet technology has been continuously reshaping the business landscape, and online businesses are everywhere nowadays. Alibaba, the largest provider of online business in China, makes it possible for people or companies all over the world to do business online. With one billion users, the Gross Merchandise Volume (GMV) of Alibaba in 2017 is 3,767 billion Yuan and the revenue in 2017 is 158 billion Yuan. In the famous Double-Eleven Day, the largest online shopping festival in China, in 2017, the total amount of transactions was around 168 billion Yuan. Among all kinds of online platforms in Alibaba, Taobao 1 , the largest online consumerto-consumer (C2C) platform, stands out by contributing 75% of the total traffic in Alibaba E-commerce.</p><p>With one billion users and two billion items, i.e., commodities, in Taobao, the most critical problem is how to help users find the needed and interesting items quickly. To achieve this goal, recommendation, which aims at providing users with interesting items based on their preferences, becomes the key technology in Taobao. For example, the homepage on Mobile Taobao App (see Figure <ref type="figure" target="#fig_0">1</ref>), which are generated based on users' past behaviors with recommendation techniques, contributes 40% of the total recommending traffic. Furthermore, recommendation contributes the majority of both revenues and traffic in Taobao. In short, recommendation has become the vital engine of GMV and revenues of Taobao and Alibaba. Despite the success of various recommendation methods in academia and industry, e.g., collaborative filtering (CF) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16]</ref>, content-based methods <ref type="bibr" target="#b1">[2]</ref>, and deep learning based methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b21">22]</ref>, the problems facing these methods become more severe in Taobao because of the billion-scale of users and items.</p><p>There are three major technical challenges facing RS in Taobao: • Scalability: Despite the fact that many existing recommendation approaches work well on smaller scale datasets, i.e., millions of users and items, they fail on the much larger scale dataset in Taobao, i.e., one billion users and two billion items. • Sparsity: Due to the fact that users tend to interact with only a small number of items, it is extremely difficult to train an accurate recommending model, especially for users or items with quite a small number of interactions. It is usually referred to as the "sparsity" problem. • Cold Start: In Taobao, millions of new items are continuously uploaded each hour. There are no user behaviors for these items. It is challenging to process these items or predict the preferences of users for these items, which is the so-called "cold start" problem.</p><p>To address these challenges in Taobao, we design a two-stage recommending framework in Taobao's technology platform. The first stage is matching, and the second is ranking. In the matching stage, we generate a candidate set of similar items for each item users have interacted with, and then in the ranking stage, we train a deep neural net model, which ranks the candidate items for each user according to his or her preferences. Due to the aforementioned challenges, in both stages we have to face different unique problems. Besides, the goal of each stage is different, leading to separate technical solutions.</p><p>In this paper, we focus on how to address the challenges in the matching stage, where the core task is the computation of pairwise similarities between all items based on users' behaviors. After the pairwise similarities of items are obtained, we can generate a candidate set of items for further personalization in the ranking stage. To achieve this, we propose to construct an item graph from users' behavior history and then apply the state-of-art graph embedding methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17]</ref> to learn the embedding of each item, dubbed Base Graph Embedding (BGE). In this way, we can generate the candidate set of items based on the similarities computed from the dot product of the embedding vectors of items. Note that in previous works, CF based methods are used to compute these similarities. However, CF based methods only consider the co-occurrence of items in users' behavior history <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16]</ref>. In our work, using random walk in the item graph, we can capture higher-order similarities between items. Thus, it is superior to CF based methods. However, it's still a challenge to learn accurate embeddings of items with few or even no interactions. To alleviate this problem, we propose to use side information to enhance the embedding procedure, dubbed Graph Embedding with Side information (GES). For example, items belong to the same category or brand should be closer in the embedding space. In this way, we can obtain accurate embeddings of items with few or even no interactions. However, in Taobao, there are hundreds of types of side information, like category, brand, or price, etc., and it is intuitive that different side information should contribute differently to learning the embeddings of items. Thus, we further propose a weighting mechanism when learning the embedding with side information, dubbed Enhanced Graph Embedding with Side information (EGES).</p><p>In summary, there are three important parts in the matching stage:</p><p>(1) Based on years of practical experience in Taobao, we design an effective heuristic method to construct the item graph from the behavior history of one billion users in Taobao.</p><p>(2) We propose three embedding methods, BGE, GES, and EGES, to learn embeddings of two billion items in Taobao. We conduct offline experiments to demonstrate the effectiveness of GES and EGES comparing to BGE and other embedding methods.</p><p>(3) To deploy the proposed methods for billion-scale users and items in Taobao, we build the graph embedding systems on the XTensorflow (XTF) platform constructed by our team. We show that the proposed framework significantly improves recommending performance on the Mobile Taobao App, while satisfying the demand of training efficiency and instant response of service even on the Double-Eleven Day. The rest of the paper is organized as follows. In Section 2, we elaborate on the three proposed embedding methods. Offline and online experimental results are presented in Section 3. We introduce the deployment of the system in Taobao in Section 4, and review the related work in Section 5. We conclude our work in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">FRAMEWORK</head><p>In this section, we first introduce the basics of graph embedding, and then elaborate on how we construct the item graph from users' behavior history. Finally, we study the proposed methods to learn the embeddings of items in Taobao.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Preliminaries</head><p>In this section, we give an overview of graph embedding and one of the most popular methods, DeepWalk <ref type="bibr" target="#b14">[15]</ref>, based on which we propose our graph embedding methods in the matching stage. Given a graph G = (V, E), where V and E represent the node set and the edge set, respectively. Graph embedding is to learn a low-dimensional representation for each node v ∈ V in the space R d , where d ≪ |V |. In other words, our goal is to learn a mapping function Φ : V → R d , i.e., representing each node in V as a d-dimensional vector.</p><p>In <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>, word2vec was proposed to learn the embedding of each word in a corpus. Inspired by word2vec, Perozzi et al. proposed DeepWalk to learn the embedding of each node in a graph <ref type="bibr" target="#b14">[15]</ref>. They first generate sequences of nodes by running random walk in the graph, and then apply the Skip-Gram algorithm to learn the representation of each node in the graph. To preserve the topological structure of the graph, they need to solve the following optimization problem: minimize</p><formula xml:id="formula_0">Φ v ∈V c ∈N (v ) − log Pr (c |Φ(v)) ,<label>(1)</label></formula><p>where N (v) is the neighborhood of node v, which can be defined as nodes within one or two hops from v. Pr (c |Φ(v)) defines the conditional probability of having a context node c given a node v.</p><p>In the rest of this section, we first present how we construct the item graph from users' behaviors, and then propose the graph embedding methods based on DeepWalk for generating lowdimensional representation for two billion items in Taobao.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Construction of Item Graph from Users' Behaviors</head><p>In this section, we elaborate on the construction of the item graph from users' behaviors. In reality, a user's behaviors in Taobao tend to be sequential as shown in Figure <ref type="figure" target="#fig_2">2</ref> (a). Previous CF based methods only consider the co-occurrence of items, but ignore the sequential information, which can reflect users' preferences more precisely. However, it is not possible to use the whole history of a user because 1) the computational and space cost will be too expensive with so many entries; 2) a user's interests tend to drift with time. Therefore, in practice, we set a time window and only choose users' behaviors within the window. This is called session-based users' behaviors.</p><p>Empirically, the duration of the time window is one hour.</p><p>After we obtain the session-based users' behaviors, two items are connected by a directed edge if they occur consecutively, e.g., in Figure <ref type="figure" target="#fig_2">2</ref> (b) item D and item A are connected because user u 1 accessed item D and A consecutively as shown in Figure <ref type="figure" target="#fig_2">2 (a)</ref>. By utilizing the collaborative behaviors of all users in Taobao, we assign a weight to each edge e i j based on the total number of occurrences of the two connected items in all users' behaviors. Specifically, the weight of the edge is equal to the frequency of item i transiting to item j in the whole users' behavior history. In this way, the constructed item graph can represent the similarity between different items based on all of the users' behaviors in Taobao.</p><p>In practice, before we extract users' behavior sequences, we need to filter out invalid data and abnormal behaviors to eliminate noise for our proposed methods. Currently, the following behaviors are regarded as noise in our system:</p><p>• If the duration of the stay after a click is less than one second, the click may be unintentional and needs to be removed. • There are some "over-active" users in Taobao, who are actually spam users. According to our long-term observations in Taobao, if a single user bought 1,000 items or his/her total number of clicks is larger than 3,500 in less than three months, it is very likely that the user is a spam user. We need to filter out the behaviors of these users. • Retailers in Taobao keep updating the details of a commodity.</p><p>In the extreme case, a commodity can become a totally different item for the same identifier in Taobao after a long sequence of updates. Thus, we remove the item related to the identifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Base Graph Embedding</head><p>After we obtain the weighted directed item graph, denoted as G = (V, E), we adopt DeepWalk to learn the embedding of each node in G. Let M denote the adjacency matrix of G and M i j the weight of the edge from node i pointing to node j. We first generate node sequences based on random walk and then run the Skip-Gram algorithm on the sequences. The transition probability of random walk is defined as</p><formula xml:id="formula_1">P (v j |v i ) =          M i j j ∈N+ (v i ) M i j , v j ∈ N + (v i ) , 0 , e i j E ,<label>(2)</label></formula><p>where N + (v i ) represents the set of outlink neighbors, i.e. there are edges from v i pointing to all of the nodes in N + (v i ). By running random walk, we can generate a number of sequences as shown in Figure <ref type="figure" target="#fig_2">2 (c</ref>).</p><p>Then we apply the Skip-Gram algorithm <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> to learn the embeddings, which maximizes the co-occurrence probability of two nodes in the obtained sequences. This leads to the following optimization problem:  where w is the window size of the context nodes in the sequences.</p><formula xml:id="formula_2">minimize Φ − log Pr {v i−w , • • • , v i+w }\v i |Φ(v i ) ,<label>(3)</label></formula><p>Using the independence assumption, we have</p><formula xml:id="formula_3">Pr {v i−w , • • • , v i+w }\v i |Φ(v i ) = i+w j=i−w, j i Pr v j |Φ(v i ) . (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>Applying negative sampling <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>, Eq. ( <ref type="formula" target="#formula_2">3</ref>) can be transformed into minimize</p><formula xml:id="formula_5">Φ log σ Φ(v j ) T Φ(v i ) + t ∈N (v i ) ′ log σ − Φ(v t ) T Φ(v i ) .</formula><p>(5) where N (v i ) ′ is the negative samples for v i , and σ () is the sigmoid function σ (x ) = 1 1+e −x . Empirically, the larger |N (v i ) ′ | is, the better the obtained results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Graph Embedding with Side Information</head><p>By applying the embedding method in Section 2.3, we can learn embeddings of all items in Taobao to capture higher-order similarities in users' behavior sequences, which are ignored by previous CF based methods. However, it is still challenging to learn accurate embeddings for "cold-start" items, i.e., those with no interactions of users.</p><p>To address the cold-start problem, we propose to enhance BGE using side information attached to cold-start items. In the context of RSs in e-commerce, side information refers to the category, shop, price, etc., of an item, which are widely used as key features in the ranking stage but rarely applied in the matching stage. We can alleviate the cold-start problem by incorporating side information in graph embedding. For example, two hoodies (same category) from UNIQLO (same shop) may look alike, and a person who likes Nikon lens may also has an interest in Canon Camera (similar category and similar brand). It means that items with similar side information should be closer in the embedding space. Based on this assumption, we propose the GES method as illustrated in Figure <ref type="figure" target="#fig_3">3</ref>.</p><p>For the sake of clarity, we modify slightly the notations. We use W to denote the embedding matrix of items or side information. Specifically, W 0 v denotes the embedding of item v, and W s v denotes the embedding of the s-th type of side information attached to item v. Then, for item v with n types of side information, we have</p><formula xml:id="formula_6">n + 1 vectors W 0 v , • • • , ...W n v ∈ R d</formula><p>, where d is the embedding dimension. Note that the dimensions of the embeddings of items and side information are empirically set to the same value.</p><p>As shown in Figure <ref type="figure" target="#fig_3">3</ref>, to incorporate side information, we concatenate the n + 1 embedding vectors for item v and add a layer with average-pooling operation to aggregate all of the embeddings related to item v, which is</p><formula xml:id="formula_7">H v = 1 n + 1 n s=0 W s v ,<label>(6)</label></formula><p>where H v is the aggregated embeddings of item v. In this way, we incorporate side information in such a way that items with similar side information will be closer in the embedding space. This results in more accurate embeddings of cold-start items and improves the offline and online performance (see Section 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Enhanced Graph Embedding with Side Information</head><p>Despite the performance gain of GES, a problem remains when integrating different kinds of side information in the embedding procedure. In Eq. ( <ref type="formula" target="#formula_7">6</ref>), the assumption is that different kinds of side information contribute equally to the final embedding, which does not reflect the reality. For example, a user who has bought an iPhone tends to view Macbook or iPad because of the brand "Apple", while a user may buy clothes of different brands in the same shop in Taobao for convenience and lower price. Therefore, different kinds of side information contribute differently to the co-occurrence of items in users' behaviors.</p><p>To address this problem, we propose the EGES method to aggregate different types of side information. The framework is the same to GES (see Figure <ref type="figure" target="#fig_3">3</ref>). The idea is that different types of side information have different contributions when their embeddings are aggregated. Hence, we propose a weighted average layer to aggregate the embeddings of the side information related to the items. Given an item v, let A ∈ R |V |×(n+1) be the weight matrix and the entry A i j the weight of the j-th type of side information of the i-th item. Note that A * 0 , i.e., the first column of A, denotes the weight of item v itself. For simplicity, we use a s v to denote the weight of the s-th type of side information of item v with a 0 v denoting the weight of item v itself. The weighted average layer combining different side information is defined in the following:</p><formula xml:id="formula_8">H v = n j=0 e a j v W j v n j=0 e a j v ,<label>(7)</label></formula><p>where we use e a j v instead of a j v to ensure that the contribution of each side information is greater than 0, and n j=0 e a j v is used to normalize the weights related to the embeddings of different side information.</p><p>For node v and its context node u in the training data, we use Z u ∈ R d to represent its embedding and y to denote the label. Then, the objective function of EGES becomes</p><formula xml:id="formula_9">L(v, u, y) = −[yloд(σ (H T v Z u )) + (1 − y)loд(1 − σ (H T v Z u ))] .<label>(8)</label></formula><p>To solve it, the gradients are derived in the following:</p><formula xml:id="formula_10">∂L ∂Z u = (σ (H T v Z u ) − y)H v .<label>(9)</label></formula><p>For s-th side information</p><formula xml:id="formula_11">∂L ∂a s v = ∂L ∂H v ∂H v ∂a s v = (σ (H T v Z u ) − y)Z u ( n j=0 e a j v )e a s v W s v − e a s v n j=0 e a j v W j v ( n j=0 e a j v ) 2 , (<label>10</label></formula><formula xml:id="formula_12">) ∂L ∂W s v = ∂L ∂H v ∂H v ∂W s v = e a s v n j=0 e a j v (σ (H T v Z u ) − y)Z u . (<label>11</label></formula><formula xml:id="formula_13">)</formula><p>The pseudo code of EGES is listed in Algorithm 1, and the pseudo code of the weighted Skip-Gram updater is shown in Algorithm 2. The final hidden representation of each item is computed by Eq. ( <ref type="formula" target="#formula_8">7</ref>).</p><p>Algorithm 1 Framework of EGES.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INPUT:</head><p>The item graph G = (V, E), side information S, number of walks per node w, walk length l, Skip-Gram window size k, number of negatives samples #ns, embedding dimension d; OUTPUT:</p><p>The item &amp; side-information embeddings W 0 , . . . , W n Weight matrix A; 1: Initialize W 0 , . . . , W n , A; 2: for i = 1 → w do WeightedSkipGram(W 0 , . . . , W n , A, k, #ns, l, SEQ);  </p><formula xml:id="formula_14">: function WeightedSkipGram(W 0 , • • • , W n , A, k, #ns, l, SEQ) 2: for i = 1 → l do 3: v = SEQ[i]; 4: for j = max (0, i − k ) → min(i + k, l ) &amp; j i do 5: u = SEQ[j] 6:</formula><p>Update(v, u, 1) 7:</p><p>for t = 0 → #ns do </p><formula xml:id="formula_15">Z new u = Z old u − η • ∂ ∂Z u L; (Eq. (<label>9)) 16:</label></formula><p>for s = 0 → n do 17:</p><formula xml:id="formula_16">a s new v = a s v old − η • ∂ L ∂a s v ; (Eq. (<label>10</label></formula><formula xml:id="formula_17">)) 18: W s new v = W s ol d v − η • ∂ L ∂W s v ; (Eq. (<label>11)) 19:</label></formula><p>end for 20: end function</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTS</head><p>In this section, we conduct extensive experiments to demonstrate the effectiveness of our proposed methods. First, we evaluate the methods by the link prediction task, and then report the online experimental results on Mobile Taobao App. Finally, we present some real-world cases to give insight into the proposed methods in Taobao. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Offline Evaluation</head><p>Link Prediction. The link prediction task is used in the offline experiments because it is a fundamental problem in networks. Given a network with some edges removed, the link prediction task is to predict the occurrence of links. Following similar experimental settings in <ref type="bibr" target="#b29">[30]</ref>, 1/3 of the edges are randomly chosen and removed as ground truth in the test set, and the remaining graph is taken as the training set. The same number of node pairs in the test data with no edges connecting them are randomly chosen as negative samples in the test set. To evaluate the performance of link prediction, the Area Under Curve (AUC) score is adopted as the performance metric.</p><p>Dataset. We use two datasets for the link prediction task. The first is Amazon Electronics<ref type="foot" target="#foot_0">2</ref> provided by <ref type="bibr" target="#b11">[12]</ref>, denoted as Amazon. The second is extracted from Mobile Taobao App, denoted as Taobao. Both of these two datasets include different types of side information. For the Amazon dataset, the item graph is constructed from "co-purchasing" relations (denoted as also_bought in the provided data), and three types of side information are used, i.e., category, sub-category and brand. For the Taobao dataset, the item graph is constructed according to Section 2.2. Note that, for the sake of efficiency and effectiveness, twelve types of side information are used in Taobao's live production, including retailer, brand, purchase level, age, gender, style, etc. These types of side information have been demonstrated to be useful according to years of practical experience in Taobao. The statistics of the two datasets are shown in Table <ref type="table" target="#tab_0">1</ref>. We can see that the sparsity of the two datasets are greater than 99%.</p><p>Comparing Methods. Experiments are conducted to compare four methods: BGE, LINE, GES, and EGES. LINE was proposed in <ref type="bibr" target="#b16">[17]</ref>, which captures the first-order and second-order proximity in graph embedding. We use the implementation provided by the authors <ref type="foot" target="#foot_1">3</ref> , and run it using first-order and second-order proximity, which are denoted, respectively, as LINE(1st) and LINE(2nd). We implement the other three methods. The emdedding dimension of all the methods is set to 160. For our BGE, GES and EGES, the length of random walk is 10, the number of walks per node is 20, and the context window is 5.</p><p>Results Analysis. The results are shown in Table <ref type="table" target="#tab_1">2</ref>. We can see that GES and EGES outperform BGE, LINE(1st) and LINE(2st) in terms of AUC on both datasets. This demonstrates the effectiveness of the proposed methods. In other words, the sparsity problem is alleviated by incorporating side information. When comparing the improvements on Amazon and Taobao, we can see that the performance gain is more significant on Taobao dataset. We attribute this to the larger number of types of effective and informative side information used on Taobao dataset. When comparing GES and EGES, we can see that the performance gain on Amazon is lager than that on Taobao. It may be due to the fact that the performance on Taobao is already very good, i.e., 0.97. Thus, the improvement of EGES is not prominent. On Amazon dataset, EGES outperforms GES significantly in terms of AUC. Based on these results, we can observe that incorporating side information can be very useful for graph embedding, and the accuracy can be further improved by weighted aggregation of the embeddings of various side information. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Online A/B Test</head><p>We conduct online experiments in an A/B testing framework. The experimental goal is Click-Through-Rate (CTR) on the homepage of Mobile Taobao App. We implement the above graph embedding methods and then generate a number of similar items for each item as recommendation candidates. The final recommending results on the homepage in Taobao (see Figure <ref type="figure" target="#fig_0">1</ref>) is generated by the ranking engine, which is implemented based on a deep neural network model. We use the same method to rank the candidate items in the experiment. As mentioned above, the quality of the similar items directly affects the recommending results. Therefore, the recommending performance, i.e., CTR, can represent the effectiveness of different methods in the matching stage. We deploy the four methods in an A/B test framework and the results of seven days in November 2017 are shown in Figure <ref type="figure" target="#fig_4">4</ref>. Note that "Base" represents an item-based CF method which has been widely used in Taobao before graph embedding methods was deployed. It calculates the similarity between two items according to item co-occurrence and user voting weight. The similarity measurement is well-tuned and suitable for Taobao's business.</p><p>From Figure <ref type="figure" target="#fig_4">4</ref>, we can see that EGES and GES outperform BGE and Base consistently in terms of CTR, which demonstrates the effectiveness of the incorporation of side information in graph embedding. Further, the CTR of Base is larger than that of BGE. It means that well-tuned CF based methods can beat simple embedding method because a large number of hand-crafted heuristic strategies have been exploited in practice. On the other hand, EGES outperforms GES consistently, which aligns with the results in the offline experimental results in Section 3.1. It further demonstrates that weighted aggregation of side information is better than average aggregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Case Study</head><p>In this section, we present some real-world cases in Taobao to illustrate the effectiveness of the proposed methods. The cases are examined in three aspects: 1) visualization of the embeddings by EGES, 2) cold start items, and 3) weights in EGES.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Visualization.</head><p>In this part, we visualize the embeddings of items learned by EGES. We use the visualization tool provided by tensorflow 4 . The results are shown in Figure <ref type="figure" target="#fig_12">7</ref>. From Figure <ref type="figure" target="#fig_12">7</ref> (a), we can see that shoes of different categories are in separate clusters. Here one color represents one category of shoes, like badminton, table tennis, or football shoes. It demonstrates the effectiveness of the learned embeddings with incorporation of side information, i.e., items with similar side information should be closer in the embedding space. From Figure <ref type="figure" target="#fig_12">7</ref> (b), we further analyze the embeddings of three kinds of shoes: badminton, table tennis, and football. It is very interesting to observe that badminton and table tennis shoes are closer to each other while football shoes are farther in the embedding space. This can be explained by a phenomenon that people in China who like table tennis have much overlapping with those who like badminton. However, those who like football are quite different from those who like indoor sports, i.e., table tennis and badminton. In this sense, recommending badminton shoes to those who have viewed table tennis shoes is much better than recommending football shoes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Cold Start Items.</head><p>In this part, we show the quality of the embeddings of cold start items. For a newly updated item in Taobao, no embedding can be learned from the item graph, and previous CF based methods also fail in handling cold start items. Thus, we represent a cold start item with the average embeddings of its side information. Then, we retrieve the most similar items from the existing items based on the dot product of the embeddings of two items. The results are shown in Figure <ref type="figure" target="#fig_9">5</ref>. We can see that despite the missing of users' behaviors for the two cold start items, different side information can be utilized to learn their embeddings effectively 4 http://projector.tensorflow.org/ in terms of the quality of the top similar items. In the figure, we annotate for each similar item the types of side information connected to the cold start item. We can see that the shops of the items are very informative for measuring the similarity of two items, which also aligns with the weight of each side information in the following part. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Weights in EGES.</head><p>In this part, we visualize the weights of different types of side information for various items. Eight items in different categories are selected and the weights of all side information related to these items are extracted from the learned weight matrix A. The results are shown in Figure <ref type="figure" target="#fig_10">6</ref>, where each row records the results of one item. Several observations are worth noting: 1) The weight distributions of different items are different, which aligns with our assumption that different side information contribute differently to the final representation. 2) Among all the items, the weights of "Item", representing the embeddings of the item itself, are consistently larger than those of all the other side information. It confirms the intuition that the embedding of an item itself remains to be the primary source of users' behaviors whereas side information provides additional hints for inferring users' behaviors. 3) Besides "Item", the weights of "Shop" are consistently larger than those of the other side information. It aligns with users' behaviors in Taobao, that is, users tend to purchase items in the same shop for convenience and lower price.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SYSTEM DEPLOYMENT AND OPERATION</head><p>In this section, we introduce the implementation and deployment of the proposed graph embedding methods in Taobao. We first give a high-level introduction of the whole recommending platform powering Taobao and then elaborate on the modules relevant to our embedding methods. In Figure <ref type="figure" target="#fig_13">8</ref>, we show the architecture of the recommending platform in Taobao. The platform consists of two subsystems: online and offline. For the online subsystem, the main components are Taobao Personality Platform (TPP) and Ranking Service Platform (RSP). A typical workflow is illustrated in the following:</p><p>• When a user launches Mobile Taobao App, TPP extracts the user's latest information and retrieves a candidate set of items from the offline subsystem, which is then fed to RSP. RSP ranks the candidate set of items with a fine-tuned deep neural net model and returns the ranked results to TPP.</p><p>• Users' behaviors during their visits in Taobao are collected and saved as log data for the offline subsystem.</p><p>The workflow of the offline subsystem, where graph embedding methods are implemented and deployed, is described in the following:</p><p>• The logs including users' behaviors are retrieved. The item graph is constructed based on the users' behaviors. In practice, we choose the logs in the recent three months. Before generating session-based users' behavior sequences, anti-spam processing is applied to the data. The remaining logs contains about 600 billion entries. Then, the item graph is constructed according to the method described in Section 2.2. • To run our graph embedding methods, two practical solutions are adopted: GPUs are used in our XTF platform. On the deployed platform, with 150 billion samples, all modules in the offline subsystem, including log retrieval, anti-spam processing, item graph construction, sequence generation by random walk, embedding, item-to-item similarity computation and map generation, can be executed in less than six hours. Thus, our recommending service can respond to users' latest behaviors in a very short time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head><p>In this section, we briefly review the related work of graph embedding, graph embedding with side information, and graph embedding for RS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Graph Embedding</head><p>Graph Embedding algorithms have been proposed as a general network representation method. They have been applied to many real-world applications. In the past few years, there has been a lot of research in the field focusing on designing new embedding algorithms. These methods could be categorized into three broad categories: 1) Factorization methods such as LINE <ref type="bibr" target="#b0">[1]</ref> try to approximately factorize the adjacency matrix and preserve both first order and second proximities; 2) Deep learning methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref> enhance the model's ability of capturing non-linearity in graph; 3) Random walk based techniques <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b14">15]</ref> use random walks on graphs to obtain node representations which are extraordinary efficient and thus could be used in extremely large-scale networks.</p><p>In this paper, our embedding framework is based on random walk.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Graph Embedding with Side Information</head><p>The above graph embedding methods only use the topological structure of the network, which suffer from the sparsity and cold start problems. In recent years, a lot of works tried to incorporate side information to enhance graph embedding methods. Most works build their tasks based on the assumption that nodes with similar side information should be closer in the embedding space.</p><p>To achieve this, a joint framework was proposed to optimize the embedding objective function with a classifier function <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b18">19]</ref>.</p><p>In <ref type="bibr" target="#b23">[24]</ref>, Xie et al. further embedded a complicated knowledge graph with the nodes in a hierarchical structure, like sub-categories, etc. Besides, textual information related to the nodes is incorporated into graph embedding <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>. Moreover, in <ref type="bibr" target="#b3">[4]</ref>, Chang et al.</p><p>proposed a deep learning framework to simultaneously deal with the text and image features for heterogeneous graph embedding.</p><p>In this paper, we mainly process discrete side information related to items in Taobao, such as category, brand, price, etc., and design a hidden layer to aggregate different types of side information in the embedding framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Graph Embedding for RS</head><p>RSs have been one of the most popular downstream tasks of graph embedding. With the representation in hand, various prediction models can be used to recommend. In <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b28">29]</ref>, embeddings of users and items are learned under the supervision of meta-path and meta-graphs, respectively, in heterogeneous information networks. Yu et al. <ref type="bibr" target="#b26">[27]</ref> proposed a linear model to aggregate the embeddings for recommendation while Zhao et al. <ref type="bibr" target="#b28">[29]</ref> proposed to apply factorization machine to the embeddings for recommendation. In <ref type="bibr" target="#b27">[28]</ref>, Zhang et al. proposed a joint embedding framework to learn the embeddings of graph, text and images, which are used for recommendation. In <ref type="bibr" target="#b29">[30]</ref>, Zhou et al. proposed graph embedding to capture asymmetric similarities for node recommendation. In this paper, our graph embedding methods are integrated in a two-stage recommending platform. Thus, the performance of the embeddings directly affects the final recommending results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION AND FUTURE WORK</head><p>Taobao's billion-scale data (one billion users and two billion items) is putting tremendous stress on its RS in terms of scalability, sparsity and cold start. In this paper, we present graph embedding based methods to address these challenges. To cope with the sparsity and cold-start problems, we propose to incorporate side information into graph embedding. Offline experiments are conducted to demonstrate the effectiveness of side information in improving recommending accuracy. Online CTRs are also reported to demonstrate the effectiveness and feasibility of our proposed methods in Taobao's live production. Real-world cases are analyzed to highlight the strength of our proposed graph embedding methods in clustering related items using users' behavior history and dealing with cold start items using side information. Finally, to address the scalability and deployment issues of our proposed solutions in Taobao, we elaborate on the the platforms for training our graph embedding methods and the overall workflow of the recommendation platform in Taobao. For future work, we will pursue two directions. The first is to utilize attention mechanism in our graph embedding methods, which can provide more flexibility to learn the weights of different side information. The second direction is to incorporate textual information into our methods to exploit the large number of reviews attached to the items in Taobao.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The areas highlighted with dashed rectangles are personalized for one billion users in Taobao. Attractive images and textual descriptions are also generated for better user experience. Note they are on Mobile Taobao App homepage, which contributes 40% of the total recommending traffic.</figDesc><graphic url="image-1.png" coords="2,134.20,83.68,343.60,190.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) Users' behavior sequences. (b) Item graph construction. (c) Random walk generation. (d) Embedding with Skip-Gram.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of graph embedding in Taobao: (a) Users' behavior sequences: One session for user u1, two sessions for user u2 and u3; these sequences are used to construct the item graph; (b) The weighted directed item graph G = (V, E); (c) The sequences generated by random walk in the item graph; (d) Embedding with Skip-Gram.</figDesc><graphic url="image-2.png" coords="4,64.78,93.65,123.20,114.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: The general framework of GES and EGES. SI denotes the side information, and "SI 0" represents the item itself. In practice, 1) Sparse features tend to be onehot-encoder vectors for items and different SIs. 2) Dense embeddings are the representation of items and the corresponding SI. 3) The hidden representation is the aggregation embedding of an item and its corresponding SI.</figDesc><graphic url="image-6.png" coords="5,59.81,83.69,228.23,163.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>4 :</head><label>4</label><figDesc>SEQ = RandomWalk(G,v,l); (Eq. (2)) 5:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>end for 8: return W 0 , . . . , W n , A; Algorithm 2 Weighted Skip-Gram.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>1</head><label>1</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>14 :</head><label>14</label><figDesc>function Update(v, u, y) 15:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Online CTRs of different methods in seven days in November 2017.</figDesc><graphic url="image-7.png" coords="6,323.96,345.06,228.23,171.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Similar items for cold start items. Top 4 similar items are shown. Note that "cat" means category.</figDesc><graphic url="image-8.png" coords="7,323.96,83.68,228.24,139.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Weights for different side information of various items. Here "Item" means the embedding of an item itself.</figDesc><graphic url="image-9.png" coords="7,323.96,347.74,228.23,114.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>(a) Visualization of sports shoes of all categories.(b) Visualization of badminton, table tennis and football shoes. Items in gray do not belong to any of the three categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Visualization of the learned embeddings of a set of randomly chosen shoes. Item embeddings are projected into a 2-D plane via principal component analysis (PCA). Different colors represent different categories. Items in the same category are grouped together.</figDesc><graphic url="image-10.png" coords="8,79.02,93.65,226.98,164.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Architecture of the recommending platform in Taobao.</figDesc><graphic url="image-12.png" coords="8,59.81,423.97,228.22,122.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the two datasets. #SI denotes the number of types of side information. Sparsity is computed according to 1 −</figDesc><table><row><cell cols="2">#Edдes #N odes×(#N odes−1) .</cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell>#Nodes</cell><cell>#Edges</cell><cell cols="2">#SI Sparsity(%)</cell></row><row><cell cols="2">Amazon 300,150</cell><cell>3,740,196</cell><cell>3</cell><cell>0.9958</cell></row><row><cell cols="4">Taobao 2,632,379 44,997,887 12</cell><cell>0.9994</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>AUCs of different methods on the two datasets. Percentages in the brackets are the improvements of AUC comparing to BGE.</figDesc><table><row><cell>Dataset</cell><cell>Amazon</cell><cell>Taobao</cell></row><row><cell>BGE</cell><cell>0.9327</cell><cell>0.8797</cell></row><row><cell cols="3">LINE(1st) 0.9554(+2.43%) 0.9100(+3.44%)</cell></row><row><cell cols="3">LINE(2nd) 0.8664(-7.65%) 0.9411(+6.98%)</cell></row><row><cell>GES</cell><cell cols="2">0.9575(+2.66%) 0.9704(+10.1%)</cell></row><row><cell>EGES</cell><cell cols="2">0.9700(+4.00%) 0.9746(+10.8%)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">http://jmcauley.ucsd.edu/data/amazon/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">https://github.com/tangjianpku/LINE</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2">KDD 2018, August 19-23, 2018, London, United Kingdom</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">ACKNOWLEDGMENTS</head><p>We would like to thank colleagues of our team -Wei Li, Qiang Liu, Yuchi Xu, Chao Li, Zhiyuan Liu, Jiaming Xu, Wen Chen and Lifeng Wang for useful discussions and supports on this work. We are grateful to our cooperative team -search engineering team. We also thank the anonymous reviewers for their valuable comments and suggestions that help improve the quality of this manuscript.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Distributed large-scale natural graph factorization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narayanamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Josifovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="37" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fab: Content-based, collaborative recommendation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Balabanović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shoham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="66" to="72" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep neural networks for learning graph representations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1145" to="1152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Heterogeneous network embedding via deep architectures</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="119" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Wide &amp; deep learning for recommender systems</title>
		<author>
			<persName><forename type="first">H.-T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Koc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Harmsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Aradhye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ispir</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep neural networks for youtube recommendations</title>
		<author>
			<persName><forename type="first">P</forename><surname>Covington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sargin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RecSys</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="191" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">metapath2vec: Scalable representation learning for heterogeneous networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="135" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An algorithmic framework for performing collaborative filtering</title>
		<author>
			<persName><forename type="first">J</forename><surname>Herlocker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Konstan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Borchers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="230" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Discriminative deep random walk for network classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1004" to="1013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Amazon. com recommendations: Item-to-item collaborative filtering</title>
		<author>
			<persName><forename type="first">G</forename><surname>Linden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>York</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet computing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="76" to="80" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Image-based recommendations on styles and substitutes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Targett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<title level="m">Efficient estimation of word representations in vector space</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Item-based collaborative filtering recommendation algorithms</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sarwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Konstan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="285" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cane: Context-aware network embedding for relation modeling</title>
		<author>
			<persName><forename type="first">C</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1722" to="1731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Max-margin deepwalk: Discriminative learning of network representation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3889" to="3895" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Transnet: Translation-based network representation learning for social relation extraction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="19" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Structural deep network embedding</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1225" to="1234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Collaborative deep learning for recommender systems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1235" to="1244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Text-enhanced representation learning for knowledge graph</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1293" to="1299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Representation learning of knowledge graphs with hierarchical types</title>
		<author>
			<persName><forename type="first">R</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2965" to="2971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Network representation learning with rich text information</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2111" to="2117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Incorporating knowledge graph embeddings into topic modeling</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3119" to="3126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Personalized entity recommendation: A heterogeneous information network approach</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sturt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Norick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="283" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Collaborative knowledge base embedding for recommender systems</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="353" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Meta-graph based recommendation fusion over heterogeneous information networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="635" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Scalable graph embedding for asymmetric proximity</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2942" to="2948" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
