<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Structured Pruning Learns Compact and Accurate Models</title>
				<funder>
					<orgName type="full">Google Research, Ameet Deshpande</orgName>
				</funder>
				<funder ref="#_Tk8U2Kc">
					<orgName type="full">Princeton University</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-04-01">1 Apr 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mengzhou</forename><surname>Xia</surname></persName>
							<email>mengzhou@cs.princeton.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zexuan</forename><surname>Zhong</surname></persName>
							<email>zzhong@cs.princeton.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
							<email>danqic@cs.princeton.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Structured Pruning Learns Compact and Accurate Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-04-01">1 Apr 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2204.00408v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The growing size of neural language models has led to increased attention in model compression.</p><p>The two predominant approaches are pruning, which gradually removes weights from a pre-trained model, and distillation, which trains a smaller compact model to match a larger one. Pruning methods can significantly reduce the model size but hardly achieve large speedups as distillation. However, distillation methods require large amounts of unlabeled data and are expensive to train. In this work, we propose a task-specific structured pruning method CoFi 1 (Coarse-and Fine-grained Pruning), which delivers highly parallelizable subnetworks and matches the distillation methods in both accuracy and latency, without resorting to any unlabeled data. Our key insight is to jointly prune coarse-grained (e.g., layers) and fine-grained (e.g., heads and hidden units) modules, which controls the pruning decision of each parameter with masks of different granularity. We also devise a layerwise distillation strategy to transfer knowledge from unpruned to pruned models during optimization. Our experiments on GLUE and SQuAD datasets show that CoFi yields models with over 10? speedups with a small accuracy drop, showing its effectiveness and efficiency compared to previous pruning and distillation approaches. 2  </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Pre-trained language models <ref type="bibr" target="#b3">(Devlin et al., 2019;</ref><ref type="bibr">Liu et al., 2019a;</ref><ref type="bibr">Raffel et al., 2020, inter alia)</ref> have become the mainstay in natural language processing. These models have high costs in terms of storage, memory, and computation time and it has motivated a large body of work on model compression to make them smaller and faster to use in real-world applications <ref type="bibr" target="#b9">(Ganesh et al., 2021</ref> Table <ref type="table">1</ref>: A comparison of state-of-the-art distillation and pruning methods. U and T denote whether Unlabeled and Task-specific are used for distillation or pruning. The inference speedups ( ) are reported against a BERT base model and we evaluate all the models on an NVIDIA V100 GPU ( ?4.1). The models labeled as ? use a different teacher model and are not a direct comparison. Models are one order of magnitude faster. <ref type="foot" target="#foot_2">3</ref>The two predominant approaches to model compression are pruning and distillation (Table <ref type="table">1</ref>).</p><p>Pruning methods search for an accurate subnetwork in a larger pre-trained model. Recent work has investigated how to structurally prune Transformer networks <ref type="bibr" target="#b42">(Vaswani et al., 2017)</ref>, from removing entire layers <ref type="bibr" target="#b5">(Fan et al., 2020;</ref><ref type="bibr" target="#b33">Sajjad et al., 2020)</ref>, to pruning heads <ref type="bibr" target="#b25">(Michel et al., 2019;</ref><ref type="bibr" target="#b43">Voita et al., 2019)</ref>, intermediate dimensions <ref type="bibr" target="#b24">(McCarley et al., 2019;</ref><ref type="bibr">Wang et al., 2020b)</ref> and blocks in weight matrices <ref type="bibr" target="#b16">(Lagunas et al., 2021)</ref>. The trend of structured pruning leans towards removing finegrained units to allow for flexible final structures. However, thus far, pruned models rarely achieve large speedups (2-3? improvement at most).</p><p>By contrast, distillation methods usually first specify a fixed model architecture and perform a general distillation step on an unlabeled corpus, before further fine-tuning or distillation on task-specific data <ref type="bibr" target="#b34">(Sanh et al., 2019;</ref><ref type="bibr" target="#b41">Turc et al., 2019;</ref><ref type="bibr" target="#b38">Sun et al., 2019;</ref><ref type="bibr" target="#b15">Jiao et al., 2020)</ref>. Welldesigned student architectures achieve compelling speedup-performance tradeoffs, yet distillation to these randomly-initialized student networks on large unlabeled data is prohibitively slow. <ref type="foot" target="#foot_3">4</ref> For instance, TinyBERT <ref type="bibr" target="#b15">(Jiao et al., 2020)</ref> is first trained on 2,500M tokens for 3 epochs, which requires training 3.5 days on 4 GPUs (Figure <ref type="figure">1</ref>). <ref type="foot" target="#foot_4">5</ref>In this work, we propose a task-specific, structured pruning approach called CoFi (Coarse and Fine-grained Pruning) and show that structured pruning can achieve highly compact subnetworks and obtain large speedups and competitive accuracy as distillation approaches, while requiring much less computation. Our key insight is to jointly prune coarse-grained units (e.g., self-attention or feed-forward layers) and fine-grained units (e.g., heads, hidden dimensions) simultaneously. Different from existing works, our approach controls the pruning decision of every single parameter by multiple masks of different granularity. This is the key to large compression, as it allows the greatest flexibility of pruned structures and eases the optimization compared to only pruning small units.</p><p>It is known that pruning with a distillation objective can substantially improve performance <ref type="bibr" target="#b35">(Sanh et al., 2020;</ref><ref type="bibr" target="#b16">Lagunas et al., 2021)</ref>. Unlike a fixed student architecture, pruned structures are unkown prior to training and it is challenging to distill between intermediate layers of the unpruned and pruned models <ref type="bibr" target="#b15">(Jiao et al., 2020)</ref>. Hence, we propose a layerwise distillation method, which dynamically learns the layer mapping between the two structures. We show that this strategy can better lead to performance gains beyond simple prediction-layer distillation.</p><p>Our experiments show that CoFi delivers more accurate models at all levels of speedups and model sizes on the GLUE <ref type="bibr" target="#b44">(Wang et al., 2019)</ref> and SQuAD v1.1 <ref type="bibr" target="#b31">(Rajpurkar et al., 2016)</ref> datasets, compared to strong pruning and distillation baselines. Concretely, it achieves over 10? speedups and a 95% sparsity across all the datasets while preserving more than 90% of accuracy. Our results suggest that task-specific structured pruning is an appealing solution in practice, yielding smaller and faster models without requiring additional unlabeled data for general distillation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Transformers</head><p>A Transformer network <ref type="bibr" target="#b42">(Vaswani et al., 2017)</ref> is composed of L blocks and each block consists of a multi-head self-attention (MHA) layer, and a feed-forward (FFN) layer. An MHA layer with N h heads takes an input X and outputs:</p><formula xml:id="formula_0">MHA(X) = N h i=1 Att(W (i) Q , W (i) K , W (i) V , W (i) O , X),</formula><p>where</p><formula xml:id="formula_1">W (i) Q , W (i) K , W (i) V , W<label>(i)</label></formula><p>O ? R d?d h denote the query, key, value and output matrices respectively and Att(?) is an attention function. Here d denotes the hidden size (e.g., 768) and d h = d/N h denotes the output dimension of each head (e.g., 64).</p><p>Next comes a feed-forward layer, which consists of an up-projection and a down-projection layer, parameterized by</p><formula xml:id="formula_2">W U ? R d?d f and W D ? R d f ?d : FFN(X) = gelu(XW U ) ? W D .</formula><p>Typically, d f = 4d. There is also a residual connection and a layer normalization operation after each MHA and FFN layer.</p><p>MHAs, FFNs account for 1/3 and 2/3 of the model parameters in Transformers (embeddings excluded). According to <ref type="bibr" target="#b9">Ganesh et al. (2021)</ref>, both MHAs and FFNs take similar time on GPUs while FFNs become the bottleneck on CPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Distillation</head><p>Knowledge distillation <ref type="bibr" target="#b12">(Hinton et al., 2015)</ref> is a model compression approach that transfers knowledge from a larger teacher model to a smaller student model. General distillation <ref type="bibr" target="#b34">(Sanh et al., 2019;</ref><ref type="bibr" target="#b39">Sun et al., 2020;</ref><ref type="bibr">Wang et al., 2020a)</ref> and task-specific distillation <ref type="bibr" target="#b38">(Sun et al., 2019)</ref> exploit unlabeled data and task-specific data respectively for knowledge transfer. A combination of the two leads to increased performance <ref type="bibr" target="#b15">(Jiao et al., 2020)</ref>. General distillation or pre-training the student network on unlabeled corpus is essential for retaining performance while being computationally expensive <ref type="bibr" target="#b41">(Turc et al., 2019;</ref><ref type="bibr" target="#b15">Jiao et al., 2020)</ref>. </p><formula xml:id="formula_3">Z head Z MHA Z int Z FFN Z hidn ?L Figure 1:</formula><p>Comparison of (a) TinyBERT <ref type="bibr" target="#b15">(Jiao et al., 2020)</ref> and (b) our pruning approach CoFi. TinyBERT trains a randomly-initialized network through two-step distillation: (1) general distillation on a large unlabeled corpus, which takes 3.5 days to finish on 4 GPUs, and (2) task-specific distillation on the task dataset. CoFi directly prunes the fine-tuned BERT model and jointly learn five types of mask variables (i.e., z FFN , z int , z MHA , z head , z hidn ) to prune different types of units ( ?3.1). CoFi takes at most 20 hours to finish on 1 GPU on all the GLUE datasets (smaller datasets need &lt; 3 hour). 6</p><p>Different distillation objectives have been also explored. Besides standard distillation from the prediction layer <ref type="bibr" target="#b12">(Hinton et al., 2015)</ref>, transferring knowledge layer-by-layer from representations <ref type="bibr" target="#b15">(Jiao et al., 2020;</ref><ref type="bibr" target="#b39">Sun et al., 2020)</ref> and multi-head attention matrices <ref type="bibr">(Wang et al., 2020a;</ref><ref type="bibr" target="#b15">Jiao et al., 2020;</ref><ref type="bibr" target="#b39">Sun et al., 2020)</ref> lead to significant improvements. Most distillation approaches assume a fixed student structure prior to training. <ref type="bibr" target="#b13">Hou et al. (2020)</ref> attempt to distill to a dynamic structure with specified widths and heights. <ref type="bibr" target="#b52">Yin et al. (2021)</ref> adopt a one-shot Neural Architecture Search solution to search architectures of student networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Pruning</head><p>Pruning gradually removes redundant parameters from a teacher model, mostly producing taskspecific models. Previous works focus on pruning different components in Transformer models, from coarse-grained to fine-grained units.</p><p>Layer pruning <ref type="bibr" target="#b5">Fan et al. (2020)</ref> and <ref type="bibr" target="#b33">Sajjad et al. (2020)</ref>  (2019) show that only a small subset of heads are important and the majority can be pruned. We follow these works to mask heads by introducing variables z (i) head ? {0, 1} to multi-head attention:</p><formula xml:id="formula_4">MHA(X) = N h i=1 z (i) head Att(W (i) Q , W (i) K , W<label>(i)</label></formula><p>V , W</p><p>O , X).</p><p>Only removing heads does not lead to large latency improvement- <ref type="bibr" target="#b17">Li et al. (2021)</ref> demonstrate a 1.4? speedup with only one remaining head per layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FFN pruning</head><p>The other major part-feed-forward layers (FFNs)-are also known to be overparameterized. Strategies to prune an FFN layer for an inference speedup include pruning an entire FFN layer <ref type="bibr" target="#b28">(Prasanna et al., 2020;</ref><ref type="bibr">Chen et al., 2020b)</ref> and at a more fine-grained level, pruning intermediate dimensions <ref type="bibr" target="#b24">(McCarley et al., 2019;</ref><ref type="bibr" target="#b13">Hou et al., 2020)</ref> by introducing</p><formula xml:id="formula_6">z int ? {0, 1} d f : FFN(X) = gelu(XW U ) ? diag(z int ) ? W D .</formula><p>Block and unstructured pruning More recently, pruning on a smaller unit, blocks, from MHAs and FFNs have been explored <ref type="bibr" target="#b16">(Lagunas et al., 2021)</ref>. However, it is hard to optimize models with blocks pruned thus far: <ref type="bibr" target="#b51">Yao et al. (2021)</ref> attempt to optimize block-pruned models with the block sparse MatMul kernel provided by Triton <ref type="bibr" target="#b40">(Tillet et al., 2019)</ref>, but the reported results are not competitive.</p><p>Similarly, unstructured pruning aims to remove individual weights and has been extensively studied in the literature <ref type="bibr">(Chen et al., 2020a;</ref><ref type="bibr">Huang et al., 2021)</ref>. Though the sparsity reaches up to 97% <ref type="bibr" target="#b35">(Sanh et al., 2020)</ref>, it is hard to obtain inference speedups on the current hardware. Combination with distillation Pruning is commonly combined with a prediction-layer distillation objective <ref type="bibr" target="#b35">(Sanh et al., 2020;</ref><ref type="bibr" target="#b16">Lagunas et al., 2021</ref>). Yet it is not clear how to apply layerwise distillation strategies as the pruned student model's architecture evolves during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>We propose a structured pruning approach CoFi, which jointly prunes Coarse-grained and Finegrained units ( ?3.1) with a layerwise distillation objective transferring knowledge from unpruned to pruned models ( ?3.2). A combination of the two leads to highly compressed models with large inference speedups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Coarse-and Fine-Grained Pruning</head><p>Recent trends in structured pruning move towards pruning smaller units for model flexibility. Pruning fine-grained units naturally entails pruning coarsegrained units-for example, pruning N h (e.g., 12) heads is equivalent to pruning one entire MHA layer. However, we observe that this rarely happens in practice and poses difficulty to optimization especially at a high sparsity regime.</p><p>To remedy the problem, we present a simple solution: we allow pruning MHA and FFN layers explicitly along with fine-grained units (as shown in ?2.3) by introducing two additional masks z MHA and z FFN for each layer. Now the multi-head selfattention and feed-forward layer become:</p><formula xml:id="formula_7">MHA(X) = z MHA ? N h i=1 (z (i) head ? Att(W (i) Q , W (i) K , W (i) V , W (i) O , X), FFN(X) = z FFN ? gelu(XW U ) ? diag(z int ) ? W D .</formula><p>With these layer masks, we explicitly prune an entire layer, instead of pruning all the heads in one MHA layer (or all the intermediate dimensions in one FFN layer). Different from the layer dropping strategies in <ref type="bibr" target="#b5">Fan et al. (2020)</ref>; <ref type="bibr" target="#b33">Sajjad et al. (2020)</ref>, we drop MHA and FFN layers separately, instead of pruning them as a whole. Furthermore, we also consider pruning the output dimensions of MHA(X) and FFN(X), referred to as 'hidden dimensions' in this paper, to allow for more flexibility in the final model structure. We define a set of masks z hidn ? {0, 1} d , shared across layers because each dimension in a hidden representation is connected to the same dimension in the next layer through a residual connection. These mask variables are applied to all the weight matrices in the model, e.g., diag(z hidn )W Q . Empirically, we find that only a small number of dimensions are pruned (e.g., 768 ? 760), but it still helps improve performance significantly ( ?4.3).</p><p>CoFi differs from previous pruning approaches in that multiple mask variables jointly control the pruning decision of one single parameter. For example, a weight in an FFN layer is pruned when the entire FFN layer, or its corresponding intermediate dimension, or the hidden dimension is pruned. As a comparison, a recent work Block Pruning (Lagunas et al., 2021) adopts a hybrid approach which applies a pruning pruning strategy on MHAs and FFNs separately.</p><p>To learn these mask variables, we use l 0 regularization modeled with hard concrete distributions following <ref type="bibr" target="#b22">Louizos et al. (2018)</ref>. We also follow <ref type="bibr">Wang et al. (2020b)</ref> to replace the vanilla l 0 objective with a Lagrangian multiplier to better control the desired sparsity of pruned models. <ref type="foot" target="#foot_5">7</ref> We adapt the sparsity function accordingly to accommodate pruning masks of different granularity:</p><formula xml:id="formula_8">? = 1 M ? 4 ? d h ? L i N h j d k z (i) MHA ? z (i,j) head ? z (k) hidden + 1 M ? 2 ? L i d f j d k z (i) FFN ? z (i,j) int ? z (k) hidden ,</formula><p>where ? is the expected sparsity and M denotes the full model size. All masking variables are learned as real numbers in [0, 1] during training and we map the masking variables below a threshold to 0 during inference and get a final pruned structure where the threshold is determined by the expected sparsity of each weight matrix (see Appendix B for more details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Distillation to Pruned Models</head><p>Previous work has shown that combining distillation with pruning improves performance, where the distillation objective only involves a cross-entropy loss between the pruned student's and the teacher's output probability distributions p s and p t <ref type="bibr" target="#b35">(Sanh et al., 2020;</ref><ref type="bibr" target="#b16">Lagunas et al., 2021)</ref>:</p><formula xml:id="formula_9">L pred = D KL (p s p t ).</formula><p>In addition to prediction-layer distillation, recent works show great benefits in distillation of intermediate layers <ref type="bibr" target="#b38">(Sun et al., 2019;</ref><ref type="bibr" target="#b15">Jiao et al., 2020)</ref>.</p><p>In the context of distillation approaches, the architecture of the student model is pre-specified, and it is straightforward to define a layer mapping between the student and teacher model. For example, the 4-layer TinyBERT 4 model distills from the 3, 6, 9 and 12-th layer of a 12-layer teacher model. However, distilling intermediate layers during the pruning process is challenging as the model structure changes throughout training.</p><p>We propose a layerwise distillation approach for pruning to best utilize the signals from the teacher model. Instead of pre-defining a fixed layer mapping, we dynamically search a layer mapping between the full teacher model and the pruned student model. Specifically, let T denote a set of teacher layers that we use to distill knowledge to the student model. We define a layer mapping function m(?), i.e., m(i) represents the student layer that distills from the teacher layer i. The hidden layer distillation loss is defined as</p><formula xml:id="formula_10">L layer = i?T MSE(W layer H m(i) s , H i t ),</formula><p>where W layer ? R d?d is a linear transformation matrix, initialized as an identity matrix. H m(i) s , H i t are hidden representations from m(i)-th student FFN layer and i-th teacher FFN layer. The layer mapping function m(?) is dynamically determined during the training process to match a teacher layer to its closest layer in the student model:</p><formula xml:id="formula_11">m(i) = arg min j:z (j) FFN &gt;0 MSE(W layer H j s , H i t ).</formula><p>Calculating the distance between two sets of layers is highly parallelizable and introduces a minimal training overhead. To address the issue of layer mismatch, which mostly happens for small-sized datasets, e.g., RTE, MRPC, we add a constraint to only allow matching a teacher layer to a lower student layer than the previously matched student layer. When pruning with larger sized datasets, layer mismatch rarely happens, showing the superiority of dynamic matching-layers between student and teacher models match in a way that benefits the pruning process the most. Finally, we combine layer distillation with the prediction-layer distillation:</p><formula xml:id="formula_12">L distil = ?L pred + (1 -?)L layer ,</formula><p>where ? controls the contribution of each loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>Datasets We evaluate our approach on eight GLUE tasks <ref type="bibr" target="#b44">(Wang et al., 2019)</ref> and SQuAD v1.1 <ref type="bibr" target="#b31">(Rajpurkar et al., 2016)</ref>. GLUE tasks include SST-2 <ref type="bibr" target="#b37">(Socher et al., 2013)</ref>, MNLI <ref type="bibr" target="#b48">(Williams et al., 2018)</ref>, QQP, QNLI, MRPC (Dolan and Brockett, 2005), CoLA <ref type="bibr" target="#b47">(Warstadt et al., 2019)</ref>, STS-B <ref type="bibr" target="#b0">(Cer et al., 2017)</ref> and RTE (see Appendix D for dataset sizes and metrics).</p><p>Training setup In our experiments, sparsity is computed as the number of pruned parameters divided by the full model size (embeddings excluded). Following <ref type="bibr">Wang et al. (2020b)</ref>; <ref type="bibr" target="#b16">Lagunas et al. (2021)</ref>, we first finetune the model with the distillation objective, then we continue training the model with the pruning objective with a scheduler to linearly increase the sparsity to the target value. We finetune the pruned model until convergence (see Appendix A for more training details).</p><p>We train models with target sparsities of {60%, 70%, 75%, 80%, 85%, 90%, 95%} on each dataset. For all the experiments, we start from the BERT base model<ref type="foot" target="#foot_6">8</ref> and freeze embedding weights following <ref type="bibr" target="#b35">Sanh et al. (2020)</ref>. We report results on development sets of all datasets.</p><p>Baselines We compare CoFi against several baselines: DistillBERT 6 <ref type="bibr" target="#b34">(Sanh et al., 2019)</ref>, TinyBERT 6 and TinyBERT 4 <ref type="bibr" target="#b15">(Jiao et al., 2020)</ref>, DynaBERT <ref type="bibr" target="#b13">(Hou et al., 2020)</ref>, and Block Pruning <ref type="bibr" target="#b16">(Lagunas et al., 2021)</ref> (see Appendix C for details). We also compare to other pruning methods such as FLOP <ref type="bibr">(Wang et al., 2020b)</ref>, Layer-Drop <ref type="bibr" target="#b5">(Fan et al., 2020)</ref>, Movement Pruning <ref type="bibr" target="#b35">(Sanh et al., 2020)</ref> and distillation methods such as  MobileBERT <ref type="bibr" target="#b39">(Sun et al., 2020)</ref> and AutoTiny-BERT <ref type="bibr" target="#b52">(Yin et al., 2021)</ref> in Appendix F. 9  For TinyBERT and DynaBERT, the released models are trained with task-specific augmented 9 We show these results in Appendix F as they are not directly comparable to CoFi. data. For a fair comparison, we train these two models with the released code without data augmentation. <ref type="foot" target="#foot_7">10</ref> For Block Pruning, we train models with their released checkpoints on GLUE tasks and use SQuAD results from the paper.</p><p>Speedup evaluation Speedup rate is a primary measurement we use throughout the paper as the compression rate does not necessarily reflect the actual improvement in inference latency 11 . We use an unpruned BERT base as the baseline and evaluate all the models with the same hardware setup on a single NVIDIA V100 GPU to measure inference speedup. The input size is 128 for GLUE tasks and 384 for SQuAD, and we use a batch size of 128. Note that the results might be different from the original papers as the environment for each platform is different.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Main Results</head><p>Overall performance In Figure <ref type="figure">2</ref>, we compare the accuracy of CoFi models to other methods in terms of both inference speedup and model size. CoFi delivers more accurate models than distillation and pruning baselines at every speedup level and model size. Block Pruning <ref type="bibr" target="#b16">(Lagunas et al., 2021)</ref>, a recent work that shows strong performance against TinyBERT 6 , is unable to achieve comparable speedups as TinyBERT 4 . Instead, CoFi has the option to prune both layers and heads &amp; intermediate units and can achieve a model with a comparable or higher performance compared to TinyBERT 4 and all the other models. Additionally, DynaBERT performs much worse speed-wise because it is restricted to remove at most half of the MHA and FFN layers.</p><p>Comparison with TinyBERT 4 In Table <ref type="table" target="#tab_2">2</ref>, we show that CoFi produces models with over 10? inference speedup and achieves comparable or even better performance than TinyBERT 4 . General distillation (GD), which distills information from a large corpus, is essential for training distillation models, especially for small-sized datasets (e.g., TinyBERT 4 w/o GD performs poorly on CoLA, RTE and STS-B). While general distillation could take up to hundreds of GPU hours for training, CoFi trains for a maximum of 20 hours on a taskspecific dataset with a single GPU. We argue that pruning approaches-trained with distillation objectives like CoFi-are more economical and efficient in achieving compressed models.</p><p>We further compare CoFi with TinyBERT 4 under the data augmentation setting in Table <ref type="table" target="#tab_3">3</ref>. As the augmented dataset is not publicly released, we follow its GitHub repository to create our own augmented data. We train CoFi with the same set of augmented data and find that it still outperforms TinyBERT 4 on most datasets. 12</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>Pruning units We first conduct an ablation study to investigate how additional pruning units such as 12 We only conduct experiments with data augmentation on four datasets because training on augmented data is very expensive. For example, training on the augmented dataset for MNLI takes more than 200 GPU hours in total. See more details in Appendix E.</p><p>MHA layers, FFN layers and hidden units in CoFi affect model performance and inference speedup beyond the standard practice of pruning heads and FFN dimensions. We show results in Table <ref type="table" target="#tab_4">4</ref> for models of similar sizes. Removing the option to prune hidden dimensions (z hidn ) leads to a slightly faster model with a performance drop across the board and we find that it removes more layers than CoFi and does not lead to optimal performance under a specific sparsity constraint. In addition, removing the layer masks (z MHA , z FFN ) brings a significant drop in speedup on highly compressed models (95%, 5M). This result shows that even with the same amount of parameters, different configurations for a model could lead to drastically different speedups. However, it does not affect the lower sparsity regime (60%, 34M). In short, by placing masking variables at different levels, the optimization procedure is incentivized to prune units accordingly under the sparsity constraint while maximizing the model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distillation objectives</head><p>We also ablate on distillation objectives to see how each part contributes to the performance of CoFi in Table <ref type="table" target="#tab_5">5</ref>. We first observe that removing distillation entirely leads to a performance drop up to 1.9-6.8 points across various datasets, showing the necessity to combine pruning and distillation for maintaining performance. The proposed hidden layer distillation objective dynamically matches the layers from the teacher model to the student model. We also experiment with a simple alternative i.e., "fixed Hidden Distillation", which matches each layers from the teacher model to the corresponding layer in the student model -if a layer is already pruned, the distillation objective will not be added. We find that fixed hidden distillation underperforms the dynamic layer matching objective used for CoFi. Interestingly, the proposed dynamic layer matching objective consistently converges to a specific alignment between the layers of the teacher model and student model. For example, we find that on QNLI the training process dynamically matches the 3, 6, 9, 12 layers in the teacher model to 1, 2, 4, 9 layers in the student model. 13 Moreover, as shown in the table, removing it hurts the performance for all the datasets except SST-2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Structures of Pruned Models</head><p>Finally, we study the pruned structures produced by CoFi. We characterize the pruned models of sparsities {60%, 70%, 80%, 90%, 95%} on five datasets.</p><p>For each setting, we run CoFi three times. Figure <ref type="figure" target="#fig_2">3</ref> demonstrates the number of remaining heads and intermediate dimensions of the pruned models for different sparsities. 14 Interestingly, we discover common structural patterns in the pruned models:</p><p>(1) Feed-forward layers are significantly pruned across all sparsities. For example, at the 60% sparsity level, the average number of intermediate dimensions in FFN layers after pruning is reduced by 71% (3, 072 ? 884), and the average number of heads in MHA is reduced by 39% (12 ? 7.3). This suggests FFN layers are more redundant than MHA layers.</p><p>(2) CoFi tends to prune submodules more from upper layers than lower layers. For example, upper MHA layers have fewer remaining heads than lower layers on average. Furthermore, we study the number of remaining FFN and MHA layers and visualize the results in Table <ref type="table">6</ref> for highly compressed models (sparsity = 95%). Although all the models are roughly of 14 We show more layer analysis in Appendix H.  We study different sparsities {60%, 70%, 80%, 90%, 95%}.</p><p>the same size, they present different patterns across datasets, which suggests that there exist different optimal sub-networks for each dataset. We find that on SST-2 and QNLI, the first MHA layer is preserved but can be removed on QQP and SQuAD. We also observe that some layers are particularly important across all datasets. For example, the first MHA layer and the second MHA layer are preserved most of the time, while the middle layers are often removed. Generally, the pruned models contain more MHA layers than FFN layers (see Appendix H), which suggests that MHA layers are more important for solving downstream tasks. Similar to <ref type="bibr" target="#b29">Press et al. (2020)</ref>, we find that although standard Transformer networks have interleaving FFN layers and MHA layers, in our pruned models, adjacent FFN/MHA layers could possibly lead to a better performance.</p><formula xml:id="formula_13">F M M M F M F F M F F M F M F M F M F F M M F M M F M F M M SQuAD F M F M F M M F M F F M M F M F M F M F F M F M M F M F M F</formula><p>Table <ref type="table">6</ref>: Remaining layers in the models pruned by CoFi on different datasets. All models are pruned at a sparsity of 95%. For each setting, we run the experiments three times to obtain three different pruned models. M represents a remaining MHA layer and F represents a remaining FFN layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Structured pruning has been widely explored in computer vision, where channel pruning <ref type="bibr" target="#b11">(He et al., 2017;</ref><ref type="bibr" target="#b23">Luo et al., 2017;</ref><ref type="bibr" target="#b20">Liu et al., 2017</ref><ref type="bibr">Liu et al., , 2019c,b;,b;</ref><ref type="bibr" target="#b26">Molchanov et al., 2019;</ref><ref type="bibr" target="#b10">Guo et al., 2020</ref>) is a standard approach for convolution neural networks.</p><p>The techniques can be adapted to Transformerbased models as introduced in ?2.3. Unstructured pruning is another major research direction, especially gaining popularity in the theory of Lottery Ticket Hypothesis <ref type="bibr" target="#b7">(Frankle and Carbin, 2019;</ref><ref type="bibr" target="#b54">Zhou et al., 2019;</ref><ref type="bibr" target="#b32">Renda et al., 2020;</ref><ref type="bibr" target="#b8">Frankle et al., 2020;</ref><ref type="bibr">Chen et al., 2020a)</ref>. Unstructured pruning produces models with high sparsities <ref type="bibr" target="#b35">(Sanh et al., 2020;</ref><ref type="bibr" target="#b50">Xu et al., 2021;</ref><ref type="bibr">Huang et al., 2021)</ref> yet hardly bring actual inference speedups. Developing computing platform for efficient sparse tensor operations is an active research area. DeepSparse 15 is CPU inference engine that leverages unstructured sparsity for speedup. <ref type="bibr">Huang et al. (2021)</ref> measure the real inference speedup induced by unstructured pruning on Moffett AI's latest hardware platform ANTOM.</p><p>We do not directly compare to these methods because the evaluation environments are different. While all the aforementioned methods produce task-specific models through pruning, several works explore upstream pruning where they prune a large pre-trained model with the masked 15 https://github.com/neuralmagic/deepsparse language modeling task. <ref type="bibr">Chen et al. (2020a)</ref> show a 70%-sparsity model retains the MLM accuracy produced by iterative magnitude pruning. <ref type="bibr" target="#b53">Zafrir et al. (2021)</ref> show the potential advantage of upstream unstructured pruning against downstream pruning. We consider applying CoFi for upstream pruning as a promising future direction to produce task-agnostic models with flexible structures.</p><p>Besides pruning, many other techniques have been explored to gain inference speedups for Transformer models, including distillation as introduced in ?2.2, quantization <ref type="bibr" target="#b36">(Shen et al., 2020;</ref><ref type="bibr" target="#b6">Fan et al., 2021)</ref>, dynamic inference acceleration <ref type="bibr" target="#b49">(Xin et al., 2020)</ref> and matrix decomposition (Noach and Goldberg, 2020). We refer the readers to <ref type="bibr" target="#b9">Ganesh et al. (2021)</ref> for a comprehensive survey.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We propose CoFi, a structured pruning approach that incorporates all levels of pruning, including MHA/FFN layers, individual heads, and hidden dimensions for Transformer-based models. Coupled with a distillation objective tailored to structured pruning, we show that CoFi compresses models into a rather different structure from standard distillation models but still achieves competitive results with more than 10? speedup. We conclude that task-specific structured pruning from large-sized models could be an appealing replacement for distillation to achieve extreme model compression, without resorting to expensive pre-training or data augmentation. Though CoFi can be directly applied to structured pruning for task-agnostic models, we frame the scope of this work to task-specific pruning due to the complexity of the design choices for upstream pruning. We hope that future research continues this line of work, given that pruning from a large pre-trained model could possibly incur less computation compared to general distillation and leads to more flexible model structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Reproducibility &amp; Hyperparameters</head><p>We report the hyperparameters that we use in our experiments in Table <ref type="table" target="#tab_8">7</ref>.  For four relatively larger GLUE datasets, MNLI, QNLI, SST-2 and QQP, and SQuAD, we train the model for 20 epochs in total and finetune the finalized sub-network for another 20 epochs. In the first 20 epochs, following <ref type="bibr" target="#b16">Lagunas et al. (2021)</ref> and <ref type="bibr">Wang et al. (2020b)</ref>, we first finetune the model with the distillation objective for 1 epoch, and then start pruning with a linear schedule to achieve the target sparsity within 2 epochs. For the four small GLUE datasets, we train the model for 100 epochs in total and finetune for 20 epochs. We finetune the model with the distillation objective for 4 epochs and prune till the target sparsity within the next 20 epochs. Note that even if the final sparsity is achieved, the pruning process keeps searching better performing structures in the rest of the training epochs. In addition, we find that finetuning the final subnetwork is essential for high sparsity models. Hyperparameters like ?, batch size, and learning rate do not generally affect performance much. <ref type="bibr" target="#b22">Louizos et al. (2018)</ref> propose l 0 optimization for model compression where the masks are modelled with hard concrete distributions as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Optimization Details</head><formula xml:id="formula_14">u ? U (0, 1) s = sigmoid 1 ? log u 1 -u + log ? s = s ? (r -l) + l z = min(1, max(0, s)).</formula><p>U (0, 1) is a uniform distribution in the interval [0, 1]; l &lt; 0 and r &gt; 0 are two constants that stretch the sigmoid output into the interval (l, r). ? is a hyperparameter that controls the steepness of the sigmoid function and log ? is the main learnable parameter. We learn the masks through updating the learnable parameters of the distributions from which the masks are sampled in the forward pass.</p><p>In our preliminary experiments, we find that optimizing ? z 0 with different learning rates and pruning schedules may converge to models of drastically different sizes. Hence, we follow <ref type="bibr">Wang et al. (2020b)</ref> to add a Lagrangian term, which imposes an equality constraint ? = t by introducing a violation penalty:</p><formula xml:id="formula_15">L c = ? 1 ? (? -t) + ? 2 ? (? -t) 2 ,</formula><p>where ? is the expected model sparsity calculated from z and t is the target sparsity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Details of Baseline Methods</head><p>We compare against several strong pruning and distillation models, including 1) DistillBERT 6 <ref type="bibr" target="#b34">(Sanh et al., 2019)</ref>; 2) TinyBERT 6 and TinyBERT 4 <ref type="bibr" target="#b15">(Jiao et al., 2020)</ref> both include general distillation for pretraining and task-specific distillation; 3) DynaBERT <ref type="bibr" target="#b13">(Hou et al., 2020)</ref>: a method that provides dynamic-sized models by specifying width and depth; 4) Block Pruning <ref type="bibr" target="#b16">(Lagunas et al., 2021)</ref>: a pruning method coupled with predictionlayer distillation. We choose their strongest approach "Hybrid Filled" as our baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Data Statistics</head><p>We show train sizes and metrics for each dataset we use in Table <ref type="table" target="#tab_9">8</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E TinyBERT 4 w/ Data Augmentation</head><p>We conduct task-specific distillation with the script provided by the TinyBERT repository. 16 However, our reproduced results are slightly lower than the reported results in <ref type="bibr" target="#b15">(Jiao et al., 2020)</ref>. The difference between these two sets of scores may stem from augmented data or teacher models. Note that the authors of TinyBERT did not release the augmented dataset. We run their codes to obtain augmented datasets. We compare CoFi and TinyBERT under the same setting where we use the same teacher model and the same set of augmented data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Additional Comparisons</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1 Comparison to Movement Pruning</head><p>We compare CoFi with a state-of-the-art unstructured pruning method, Movement Pruning <ref type="bibr" target="#b35">(Sanh et al., 2020)</ref> in Figure <ref type="figure" target="#fig_3">4</ref>. As Movement Pruning is trained with prediction-layer (logit) distillation only, we also show results of CoFi trained with the same distillation objective. We observe that CoFi largely outperforms Movement Pruning even without layerwise distillation on MNLI and is comparable to SQuAD on models with a size over 10M parameters. CoFi, as a structured pruning method, is less performant on models of a sparsity up to 95%, as pruning flexibility is largely restricted by the smallest pruning unit. However, pruned models from CoFi achieve 2 -11? inference speedups while no speedup gains are achieved from Movement Pruning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 Comparison to Block Pruning</head><p>In Figure <ref type="figure">6</ref>, we compare CoFi with Block Pruning while unifying the distillation objective. Without the layer distillation objective, CoFi still outperforms or is on par with Block Pruning. Block Pruning never achieves a speedup of 10 even the pruned model is of a similar size as CoFi (SST-2), backing up our argument that pruning layers for high sparsity models is the key to high speedups.</p><p>16 https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/TinyBERT </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3 More Baselines</head><p>We show additional pruning and distillation methods that are not directly comparable to CoFi in Table <ref type="table">10</ref>. CoFi still largely outperforms these baselines even though these methods hold an inherent advantage due to a stronger teacher or base model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G More Analyses on Layer Distillation G.1 Layer Alignment</head><p>We find that the alignment between the layers of the student model and the teacher model shifts during the course of training. To take SST-2 for an example, as the training goes on, the model learns the alignment to match the 7, 9, 10, 11 layers of the student model to the 3, 6, 9, 12 layers of the teacher model. For QQP, the model eventually learns to map 2, 5, 8, 11 layers to the four layers of the teacher model. The final alignment shows that our dynamic layer matching distillation objective can find task-specific alignment and improve performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2 Ablation on Distillation Objectives</head><p>In Table <ref type="table" target="#tab_13">11</ref>, we show ablation studies on adding the dynamic layer distillation onto prediction distillation across all sparsities. Using the layer distillation loss clearly helps improve the performance on all sparsity rates and different tasks.</p><p>H FFN/MHA Layers in Pruned Models</p><p>Figure <ref type="figure" target="#fig_4">5</ref> shows the average number of FFN layers and MHA layers in the pruned models by CoFi. We study different sparsities {60%, 70%, 80%, 90%, 95%}. It is clear that when the sparsity increases, the pruned models become shallower (i.e., the number of layers becomes fewer). Furthermore, we find that the pruned models usually have more MHA layers than FFN layers. This may indicate that MHA layers are more important for solving these downstream tasks than FFN layers. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I RoBERTa Pruning</head><p>We show CoFi results with RoBERTa in Figure <ref type="figure" target="#fig_6">7</ref> across sparsities from 60% to 95%. Similar to BERT, models with 60% weights pruned are able to maintain the performance of a full model. Pruning from RoBERTa outperforms BERT on sparsities lower than 90% but as the sparsity further increases, BERT surpasses RoBERTa. Similar patterns are observed from DynaBERT <ref type="bibr" target="#b13">(Hou et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J Training Time Measurement</head><p>We use NVIDIA RTX 2080Ti GPUs to measure the training time of TinyBERT. For the general distillation step of TinyBERT, we measure the training time on a small corpus (containing 10.6M tokens) on 4 GPUs and estimate the training time on the original corpus (containing 2500M tokens) by scaling the time with the corpus size difference. Specifically, it takes 430s to finish one epoch on 10.6M tokens with 4 GPUs, and we estimate that it will take 338 GPU hours (or 3.5 days with 4 GPUs) to finish three epochs on 2500M tokens.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The average intermediate dimensions at each FFN layer and the average number of heads at each MHA layer in the pruned models across five datasets (SST-2, MNLI, QQP, QNLI, and SQuAD 1.1).We study different sparsities {60%, 70%, 80%, 90%, 95%}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: CoFi v.s. Movement Pruning (unstructured pruning). CoFi Logit denotes that we run CoFi with prediction-layer distillation only as Movement Pruning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The average number of FFN layers and MHA layers in the pruned models at different sparsities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: CoFi with BERT and RoBERTa on SST-2 and MNLI.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>).</figDesc><table><row><cell>U T</cell><cell></cell><cell cols="2">Params MNLI</cell></row><row><cell>BERT base (teacher)</cell><cell>1.0?</cell><cell>85M</cell><cell>84.8</cell></row><row><cell>Distillation</cell><cell></cell><cell></cell><cell></cell></row><row><cell>DistillBERT 6</cell><cell>2.0?</cell><cell>43M</cell><cell>82.2</cell></row><row><cell>TinyBERT 6</cell><cell>2.0?</cell><cell>43M</cell><cell>84.0</cell></row><row><cell>MobileBERT  ?</cell><cell>2.3?</cell><cell>20M</cell><cell>83.9</cell></row><row><cell>DynaBERT</cell><cell>6.3?</cell><cell>11M</cell><cell>76.3</cell></row><row><cell>AutoTinyBERT  ?</cell><cell>9.1?</cell><cell>3.3M</cell><cell>78.2</cell></row><row><cell>TinyBERT 4</cell><cell>11.4?</cell><cell>4.7M</cell><cell>78.8</cell></row><row><cell>Pruning</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Movement Pruning</cell><cell>1.0?</cell><cell>9M</cell><cell>81.2</cell></row><row><cell>Block Pruning</cell><cell>2.7?</cell><cell>25M</cell><cell>83.7</cell></row><row><cell>CoFi Pruning (ours)</cell><cell>2.7?</cell><cell>26M</cell><cell>84.9</cell></row><row><cell cols="2">CoFi Pruning (ours) 12.1?</cell><cell>4.4M</cell><cell>80.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Accuracy v.s. speedup (top)  or model size (bottom). We compare CoFi against state-of-the-art distillation and pruning baselines. Note that we exclude embedding size when calculating model size following previous work, as forwarding through the embedding layer has little effect on inference time. CoFi v.s. TinyBERT 4<ref type="bibr" target="#b15">(Jiao et al., 2020</ref>) models with a ?10? speedup. GD: general distillation, which distills the student model on a large unlabeled corpus. Train time is measured in GPU hours (see Appendix J for details). The number of parameters for both models are around 5M (around 95% sparsity). CoFi closes the gap between distillation and pruning with significantly less computation. Note that we remove data augmentation from TinyBERT for a fair comparison, see Table3for experiments with augmented data.</figDesc><table><row><cell>BERT</cell><cell cols="2">95% BERT</cell><cell cols="2">DistilBERT6</cell><cell>TinyBERT6</cell><cell></cell><cell>TinyBERT4</cell><cell cols="2">Block Pruning</cell><cell cols="2">DynaBERT</cell><cell>CoFi</cell></row><row><cell>Figure 2: Task</cell><cell></cell><cell cols="10">SST-2 QNLI MNLI QQP CoLA RTE STS-B MRPC SQuAD Train Time</cell></row><row><cell></cell><cell></cell><cell cols="6">(67k) (105k) (393k) (364k) (8.5k) (2.5k)</cell><cell>(7k)</cell><cell>(3.7k)</cell><cell>(88k)</cell></row><row><cell cols="2">BERT base (teacher)</cell><cell>93.1</cell><cell>91.5</cell><cell>84.8</cell><cell>91.2</cell><cell>61.2</cell><cell>70.0</cell><cell>88.7</cell><cell>85.0</cell><cell>88.4</cell><cell>-</cell></row><row><cell cols="3">TinyBERT 4 w/o GD 87.7</cell><cell>81.8</cell><cell>78.7</cell><cell>89.5</cell><cell>16.6</cell><cell>47.3</cell><cell>17.8</cell><cell>68.9</cell><cell>-</cell><cell>? 10</cell></row><row><cell>TinyBERT 4</cell><cell></cell><cell>89.7</cell><cell>86.7</cell><cell>78.8</cell><cell>90.0</cell><cell>32.5</cell><cell>63.2</cell><cell>85.0</cell><cell>81.4</cell><cell>82.1</cell><cell>? 350</cell></row><row><cell>Speedup</cell><cell></cell><cell cols="8">11.4? 11.4? 11.4? 11.4? 11.4? 11.4? 11.4? 11.4?</cell><cell>8.7?</cell><cell>-</cell></row><row><cell cols="2">CoFi Pruning (ours)</cell><cell>90.6</cell><cell>86.1</cell><cell>80.6</cell><cell>90.1</cell><cell>35.6</cell><cell>64.7</cell><cell>83.1</cell><cell>82.6</cell><cell>82.6</cell><cell>? 20</cell></row><row><cell>Speedup</cell><cell></cell><cell cols="8">12.0? 12.1? 12.1? 11.0? 11.5? 11.9? 12.9? 11.9?</cell><cell>8.7?</cell><cell>-</cell></row><row><cell>Task</cell><cell cols="2">TinyBERT 4</cell><cell cols="2">CoFi (ours)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SST-2</cell><cell cols="5">89.7 ? 91.6 90.6 ? 92.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">QNLI 86.7 ? 87.6 86.1 ? 86.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RTE</cell><cell cols="5">63.2 ? 62.5 64.7 ? 67.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">MRPC 81.4 ? 83.6 82.6 ? 84.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>CoFi v.s. TinyBERT 4 trained with taskspecific data augmentation introduced in Jiao et al. (2020). All models have around 5M parameters (95% sparsity) and achieve similar speedups (11-12?). The numbers before ? are without data augmentation.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Ablation studies on pruning units on QNLI, MNLI and SQuAD. : speedup. The pruned models of a sparsity 60% and 95% have a model size of 34M and 5M respectively. -layer: When we do not prune entire layers (no z MHA or z FFN ), the speed-ups are greatly reduced for a high sparsity e.g., 95%; -hidden: when we remove the mask variables corresponding to hidden units (z hidn ), we observe a significant drop in accuracy.</figDesc><table><row><cell></cell><cell cols="6">QNLI (60%) QNLI (95%) MNLI (60%) MNLI (95%) SQuAD (60%) SQuAD (95%)</cell></row><row><cell></cell><cell></cell><cell>acc</cell><cell>acc</cell><cell>acc</cell><cell>acc</cell><cell>F1</cell><cell>F1</cell></row><row><cell>CoFi</cell><cell cols="5">2.1? 91.8 12.1? 86.1 2.1? 85.1 12.1? 80.6 2.0?</cell><cell>89.1</cell><cell>8.7?</cell><cell>82.6</cell></row><row><cell>-hidden</cell><cell cols="5">2.2? 91.3 13.3? 85.6 2.1? 85.2 13.7? 79.8 2.0?</cell><cell>88.7</cell><cell>9.7?</cell><cell>80.8</cell></row><row><cell cols="5">-layer &amp; hidden 2.2? 91.3 7.2? 84.6 2.1? 84.8</cell><cell>7.0? 78.4 2.1?</cell><cell>88.5</cell><cell>6.4?</cell><cell>74.1</cell></row><row><cell>CoFi</cell><cell cols="5">2.1? 91.8 12.1? 86.1 2.1? 85.1 12.1? 80.6 2.0?</cell><cell>89.1</cell><cell>8.7?</cell><cell>82.6</cell></row><row><cell>-layer</cell><cell cols="4">2.1? 91.5 8.3? 86.7 2.1? 85.4</cell><cell>8.4? 80.6 2.0?</cell><cell>89.1</cell><cell>7.9?</cell><cell>80.5</cell></row><row><cell></cell><cell cols="4">SST-2 QNLI MNLI SQuAD</cell><cell></cell></row><row><cell>CoFi</cell><cell>90.6</cell><cell>86.1</cell><cell>80.6</cell><cell>82.6</cell><cell></cell></row><row><cell>-L layer</cell><cell>91.1</cell><cell>85.1</cell><cell>79.7</cell><cell>82.5</cell><cell></cell></row><row><cell>-L pred , L layer</cell><cell>86.6</cell><cell>84.2</cell><cell>78.2</cell><cell>75.8</cell><cell></cell></row><row><cell cols="2">Fixed Hidn Distil. 90.0</cell><cell>85.8</cell><cell>80.5</cell><cell>80.9</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Ablation study of different distillation objectives on pruned models with sparsity = 95%. Fixed hidden distillation: simply matching each layer of the student and the teacher model, see ?4.3 for more details. In ?G.2, we show that the dynamic layer distillation objective improves model performance more significantly on lower sparsity rates.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Hyperparemeters in the experiments.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Data statistics of GLUE and SQuAD datasets.</figDesc><table><row><cell>Task</cell><cell>Train Size</cell><cell>Metric</cell></row><row><cell>SST-2</cell><cell>67k</cell><cell>accuracy</cell></row><row><cell>QNLI</cell><cell>105k</cell><cell>accuracy</cell></row><row><cell>MNLI</cell><cell>393k</cell><cell>accuracy</cell></row><row><cell>QQP</cell><cell>364k</cell><cell>accuracy</cell></row><row><cell>CoLA</cell><cell cols="2">8.5k Matthews corr.</cell></row><row><cell>RTE</cell><cell>2.5k</cell><cell>accuracy</cell></row><row><cell>STS-B</cell><cell cols="2">7k Spearman corr.</cell></row><row><cell>MRPC</cell><cell>3.7k</cell><cell>accuracy</cell></row><row><cell>SQuAD</cell><cell>88k</cell><cell>F1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Re-implemented (TinyBERT 4 reimpl.) results and the results reported in<ref type="bibr" target="#b15">Jiao et al. (2020)</ref>.</figDesc><table><row><cell></cell><cell cols="3">SST-2 QNLI RTE MRPC</cell></row><row><cell cols="2">TinyBERT 4 reimpl. 91.6</cell><cell>87.6 62.5</cell><cell>83.6</cell></row><row><cell>Jiao et al. (2020)</cell><cell>92.7</cell><cell>88.0 65.7</cell><cell>85.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>Figure 6: CoFi v.s. Block Pruning with the same distillation objective -prediction-layer distillation (Logit Distill). CoFi still outperforms or is on par with Block Pruning. +0.34) 90.66 91.84 (+1.18) 85.16 85.31 (+0.15) 88.84 89.13 (+0.29) 70% 91.74 93.00 (+1.26) 89.93 91.29 (+1.36) 84.57 84.89 (+0.32) 88.11 88.56 (+0.45) 75% 91.40 92.89 (+1.49) 88.96 91.31 (+2.35) 84.19 84.75 (+0.56) 87.54 87.99 (+0.45) 80% 91.06 92.89 (+1.83) 88.76 90.43 (+0.67) 83.36 84.26 (+0.90) 86.52 87.26 (+0.74) 85% 90.48 92.55 (+2.07) 86.84 89.69 (+2.85) 82.69 83.44 (+0.75) 85.76 86.40 (+0.64) 90% 90.25 91.51 (+1.26) 85.80 88.89 (+3.19) 81.09 82.61 (+1.52) 83.28 84.08 (+0.80) 95% 91.06 90.37 (-0.69) 85.08 86.14 (+1.06) 79.66 80.55 (+0.89) 82.52 82.59 (+0.07)</figDesc><table><row><cell></cell><cell></cell><cell>BERT</cell><cell cols="2">95% BERT</cell><cell cols="2">Block Pruning</cell><cell>CoFi Logit Distill</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MNLI</cell><cell>SQuAD</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Accuracy</cell><cell>82 84</cell><cell></cell><cell>F1</cell><cell>88 84 86</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>80</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>10</cell><cell>20</cell><cell>30</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Model Size (M)</cell></row><row><cell></cell><cell></cell><cell>SST-2</cell><cell></cell><cell>QNLI</cell><cell></cell><cell></cell><cell>MNLI</cell><cell>SQuAD</cell></row><row><cell cols="2">sparsity L pred</cell><cell>+L layer</cell><cell>L pred</cell><cell cols="2">+L layer</cell><cell>L pred</cell><cell>+L layer</cell><cell>L pred</cell><cell>+L layer</cell></row><row><cell>60%</cell><cell cols="2">92.66 93.00 (</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 11 :</head><label>11</label><figDesc>Ablation study on the proposed layer distillation objective across all sparsities.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>CoFi is pronounced as .</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Our code and models are publicly available at https: //github.com/princeton-nlp/CoFiPruning.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Following previous work, we exclude embedding matrices in calculating the number of parameters. We exclude task-specific data augmentation for a fair comparison. More results with data augmentation can be found in Table3.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>There are exceptions like DistillBERT<ref type="bibr" target="#b35">(Sanh et al., 2020)</ref>, which initializes the student from the teacher by taking one layer out of two, yet it is unclear how to generalize this initialization scheme to other compact structures.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>  5  See training time measurement details in Appendix J.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5"><p>We also tried a straight-through estimator as proposed in<ref type="bibr" target="#b35">Sanh et al. (2020)</ref> and found the performance comparable. We choose l0 regularization because it is easier to control the sparsity precisely.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6"><p>We also experiments CoFi on RoBERTa models(Liu  et al., 2019a). Please refer to Appendix I for details.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_7"><p>For TinyBERT, the augmented data is 20? larger than the original data, making the training process significantly slower.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_8"><p>Models with the same compression rate could have considerably different speedups.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The authors thank <rs type="person">Tao Lei</rs> from <rs type="funder">Google Research, Ameet Deshpande</rs>, <rs type="person">Dan Friedman</rs>, <rs type="person">Sadhika Malladi</rs> from <rs type="funder">Princeton University</rs> and the anonymous reviewers for their valuable feedback on our paper. This research is supported by a Hisashi and Masae Kobayashi *67 Fellowship and a <rs type="grantName">Google Research Scholar Award</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Tk8U2Kc">
					<orgName type="grant-name">Google Research Scholar Award</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I?igo</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S17-2001</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation</title>
		<meeting>the 11th International Workshop on Semantic Evaluation<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
	<note>SemEval-2017</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The lottery ticket hypothesis for pre-trained BERT networks</title>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Carbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="15834" to="15846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Xiaohan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00063</idno>
		<title level="m">Earlybert: Efficient bert training via early-bird lottery tickets</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automatically constructing a corpus of sentential paraphrases</title>
		<author>
			<persName><forename type="first">B</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><surname>Brockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Workshop on Paraphrasing</title>
		<meeting>the Third International Workshop on Paraphrasing</meeting>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Reducing Transformer depth on demand with structured dropout</title>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Training with quantization noise for extreme model compression</title>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R?mi</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herve</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The lottery ticket hypothesis: Finding sparse, trainable neural networks</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Carbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Linear mode connectivity and the lottery ticket hypothesis</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karolina</forename><surname>Gintare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Dziugaite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><surname>Carbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3259" to="3269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Compressing large-scale Transformer-based models: A case study on BERT</title>
		<author>
			<persName><forename type="first">Prakhar</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Ali Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hassan</forename><surname>Sajjad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marianne</forename><surname>Winslett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association of Computational Linguistics (TACL)</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1061" to="1080" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dmcp: Differentiable markov channel pruning for neural networks</title>
		<author>
			<persName><forename type="first">Shaopeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanquan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Channel pruning for accelerating very deep neural networks</title>
		<author>
			<persName><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1389" to="1397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">DynaBERT: Dynamic bert with adaptive width and depth</title>
		<author>
			<persName><forename type="first">Lu</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Hang Liu, and Caiwen Ding. 2021. Sparse progressive distillation: Resolving overfitting under pretrain-and-finetune paradigm</title>
		<author>
			<persName><forename type="first">Shaoyi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongkuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">Eh</forename><surname>Yen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingbing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mimi</forename><surname>Xie</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2110</biblScope>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">TinyBERT: Distilling BERT for natural language understanding</title>
		<author>
			<persName><forename type="first">Xiaoqi</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linlin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4163" to="4174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Fran?ois</forename><surname>Lagunas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ella</forename><surname>Charlaix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.04838</idno>
		<title level="m">Block pruning for faster transformers</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Differentiable subset pruning of Transformer heads</title>
		<author>
			<persName><forename type="first">Jiaoda</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mrinmaya</forename><surname>Sachan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association of Computational Linguistics</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>TACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized BERT pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Metapruning: Meta learning for automatic neural network channel pruning</title>
		<author>
			<persName><forename type="first">Zechun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyuan</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zichao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kwang-Ting</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3296" to="3305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning efficient convolutional networks through network slimming</title>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shoumeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changshui</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2736" to="2744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rethinking the value of network pruning</title>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingjie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning sparse neural networks through l0 regularization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Thinet: A filter level pruning method for deep neural network compression</title>
		<author>
			<persName><forename type="first">Jian-Hao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiyao</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5058" to="5066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Structured pruning of a BERT-based question answering model</title>
		<author>
			<persName><forename type="first">Rishav</forename><surname>Js Mccarley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avirup</forename><surname>Chakravarti</surname></persName>
		</author>
		<author>
			<persName><surname>Sil</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.06360</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Are sixteen heads really better than one?</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Importance estimation for neural network pruning</title>
		<author>
			<persName><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iuri</forename><surname>Frosio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="11264" to="11272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Compressing pre-trained language models by matrix decomposition</title>
		<author>
			<persName><forename type="first">Matan</forename><surname>Ben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noach</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="884" to="889" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">When BERT plays the lottery, all tickets are winning</title>
		<author>
			<persName><forename type="first">Sai</forename><surname>Prasanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Rumshisky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3208" to="3229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Improving transformer models by reordering their sublayers</title>
		<author>
			<persName><forename type="first">Ofir</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2996" to="3005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text Transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research (JMLR)</title>
		<imprint>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page">21</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Comparing rewinding and fine-tuning in neural network pruning</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Renda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Carbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Hassan</forename><surname>Sajjad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fahim</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadir</forename><surname>Durrani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.03844</idno>
		<title level="m">Poor man&apos;s BERT: Smaller and faster transformer models</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">DistilBERT, a distilled version of bert: smaller, faster, cheaper and lighter</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01108</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Movement pruning: Adaptive sparsity by finetuning</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Q-BERT: Hessian based ultra low precision quantization of BERT</title>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayu</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8815" to="8821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Patient knowledge distillation for BERT model compression</title>
		<author>
			<persName><forename type="first">Siqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4314" to="4323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">MobileBERT: a compact task-agnostic bert for resource-limited devices</title>
		<author>
			<persName><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renjie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2158" to="2170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Triton: an intermediate language and compiler for tiled neural network computations</title>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Tillet</surname></persName>
		</author>
		<author>
			<persName><surname>Hsiang-Tsung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Kung</surname></persName>
		</author>
		<author>
			<persName><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages</title>
		<meeting>the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Well-read students learn better: On the importance of pre-training compact models</title>
		<author>
			<persName><forename type="first">Iulia</forename><surname>Turc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08962</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned</title>
		<author>
			<persName><forename type="first">Elena</forename><surname>Voita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Talbot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fedor</forename><surname>Moiseev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5797" to="5808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><surname>Samuel R Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">MiniLM: Deep selfattention distillation for task-agnostic compression of pre-trained Transformers</title>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Structured pruning of large language models</title>
		<author>
			<persName><forename type="first">Ziheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Wohlwend</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6151" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Neural network acceptability judgments</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Warstadt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><surname>Samuel R Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association of Computational Linguistics (TACL)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="625" to="641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">DeeBERT: Dynamic early exiting for accelerating BERT inference</title>
		<author>
			<persName><forename type="first">Ji</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raphael</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaejun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaoliang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2246" to="2251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Rethinking network pruning-under the pre-train and fine-tune paradigm</title>
		<author>
			<persName><forename type="first">Dongkuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>En-Hsu Yen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinxi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhibin</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter</title>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2376" to="2382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">Zhewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.14636</idno>
		<title level="m">MLPruning: A multilevel structured pruning framework for transformer-based models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">AutoTinyBERT: Automatic hyper-parameter optimization for efficient pre-trained language models</title>
		<author>
			<persName><forename type="first">Yichun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5146" to="5157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">Ofir</forename><surname>Zafrir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Larey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Boudoukh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haihao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moshe</forename><surname>Wasserblat</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.05754</idno>
		<title level="m">Prune once for all: Sparse pre-trained language models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Deconstructing lottery tickets: Zeros, signs, and the supermask</title>
		<author>
			<persName><forename type="first">Hattie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janice</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rosanne</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
