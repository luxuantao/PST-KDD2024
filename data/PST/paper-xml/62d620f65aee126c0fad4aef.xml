<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Label2Label: A Language Modeling Framework for Multi-Attribute Learning</title>
				<funder>
					<orgName type="full">Beijing Academy of Artificial Intelligence</orgName>
					<orgName type="abbreviated">BAAI</orgName>
				</funder>
				<funder ref="#_spf95FR #_u2bJ3f2">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_7PQ5FNJ">
					<orgName type="full">National Key Research and Development Program of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-07-18">18 Jul 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wanhua</forename><surname>Li</surname></persName>
							<email>wanhua016@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Zhexuan</forename><surname>Cao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jianjiang</forename><surname>Feng</surname></persName>
							<email>jfeng@tsinghua.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
							<email>jzhou@tsinghua.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
							<email>lujiwen@tsinghua.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Automation</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Label2Label: A Language Modeling Framework for Multi-Attribute Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-07-18">18 Jul 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2207.08677v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>multi-attribute</term>
					<term>language modeling</term>
					<term>attribute relations</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Objects are usually associated with multiple attributes, and these attributes often exhibit high correlations. Modeling complex relationships between attributes poses a great challenge for multi-attribute learning. This paper proposes a simple yet generic framework named Label2Label to exploit the complex attribute correlations. Label2Label is the first attempt for multi-attribute prediction from the perspective of language modeling. Specifically, it treats each attribute label as a "word" describing the sample. As each sample is annotated with multiple attribute labels, these "words" will naturally form an unordered but meaningful "sentence", which depicts the semantic information of the corresponding sample. Inspired by the remarkable success of pre-training language models in NLP, Label2Label introduces an image-conditioned masked language model, which randomly masks some of the "word" tokens from the label "sentence" and aims to recover them based on the masked "sentence" and the context conveyed by image features. Our intuition is that the instance-wise attribute relations are well grasped if the neural net can infer the missing attributes based on the context and the remaining attribute hints. Label2Label is conceptually simple and empirically powerful. Without incorporating task-specific prior knowledge and highly specialized network designs, our approach achieves stateof-the-art results on three different multi-attribute learning tasks, compared to highly customized domain-specific methods. Code is available at https://github.com/Li-Wanhua/Label2Label.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Attributes are mid-level semantic properties for objects which are shared across categories <ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b32">33]</ref>. We can describe objects with a wide variety of attributes. For example, human beings easily perceive gender, hairstyle, expression, and so on from a facial image <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b33">34]</ref>. Multi-attribute learning, which aims to predict the attributes of an object accurately, is essentially a multi-label classification task <ref type="bibr" target="#b51">[52]</ref>. As multi-attribute learning involves many important tasks, including facial attribute recognition <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b39">40]</ref>, pedestrian attribute recognition <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b53">54]</ref>, and cloth attribute prediction <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b62">63]</ref>, it plays a central role in a wide range of applications, such as face identification <ref type="bibr" target="#b4">[5]</ref>, scene understanding <ref type="bibr" target="#b50">[51]</ref>, person retrieval <ref type="bibr" target="#b29">[30]</ref>, and fashion search <ref type="bibr" target="#b1">[2]</ref>.</p><p>For a given sample, many of its attributes are correlated. For example, if we observe that a person has blond hair and heavy makeup, the probability of that person being attractive is high. Another example is that the attributes of beard and woman are almost impossible to appear on a person at the same time. Modeling complex inter-attribute associations is an important challenge for multi-attribute learning. To address this challenge, most existing approaches <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b53">54]</ref> adopt a multi-task learning framework, which formulates multi-attribute recognition as a multi-label classification task and simultaneously learns multiple binary classifiers. To boost the performance, many methods further incorporate domain-specific prior knowledge. For example, PS-MCNN <ref type="bibr" target="#b4">[5]</ref> divides all attributes into four groups and presents highly customized network architectures to learn shared and group-specific representations for face attributes. In addition, some methods attempt to introduce additional domain-specific guidance <ref type="bibr" target="#b25">[26]</ref> or annotations <ref type="bibr" target="#b38">[39]</ref>. However, these methods struggle to model samplewise attribute relationships with a simple multi-task learning framework.</p><p>Recent years have witnessed great progress in the large-scale pre-training language models <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b45">46]</ref>. As a representative work, BERT <ref type="bibr" target="#b10">[11]</ref> utilizes a masked language model (MLM) <ref type="bibr" target="#b54">[55]</ref> to capture the word co-occurrence and language structure. Inspired by these methods, we propose a language modeling framework named Label2Label to model the complex instance-wise attribute relations. Specifically, we regard an attribute label as a "word", which describes the current state of the sample from a certain point of view. For example, we treat the labels "attractive" and "no eyeglasses" as two "words", which give us a sketch of the sample from different perspectives. As multiple attribute labels of each sample are used to depict the same object, these "words" can be organized as an unordered yet meaningful "sentence". For example, we can describe the human face in Fig. <ref type="figure" target="#fig_0">1</ref> with the sentence "attractive, not bald, brown hair, no eyeglasses, not male, wearing lipstick, ...". Although this "sentence" has no grammatical structure, it can convey some contextual semantic information. By treating multiple attribute labels as a "sentence", we exploit the correlation between attributes with a language modeling framework.</p><p>Our proposed Label2Label consists of an attribute query network (AQN) and an image-conditioned masked language model (IC-MLM). The attribute query network first generates the initial attribute predictions. Then these predictions are treated as pseudo label "sentences" and sent to the IC-MLM. Instead of simply adopting the masked language modeling framework, our IC-MLM randomly masks some "word" tokens from the pseudo label "sentence" and predicts the masked "words" conditioned on the masked "sentence" and image features. The proposed image-conditioned masked language model provides partial attribute prompts during the precise mapping from images to attribute categories, thereby facilitating the model to learn complex sample-level attribute correlations. We take facial attribute recognition as an example and show the key differences between our method and existing methods in Fig. <ref type="figure" target="#fig_0">1</ref>.</p><p>We summarize the contributions of this paper as follows:</p><p>-We propose Label2Label to model the complex attribute relations from the perspective of language modeling. As far as we know, Label2Label is the first language modeling framework for multi-attribute learning. -Our Label2Label proposes an image-conditioned masked language model to learn complex sample-level attribute correlations, which recovers a "sentence" from the masked one conditioned on image features. -As a simple and generic framework, Label2Label achieves very competitive results across three multi-attribute learning tasks, compared to highly tailored task-specific approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Multi-Attribute Recognition: Multi-attribute learning has attracted increasing interest due to its broad applications <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b29">30]</ref>. It involves many different visual tasks <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b38">39]</ref> according to the object of interest. Many works focus on domain-specific network architectures. Cao et al. <ref type="bibr" target="#b4">[5]</ref> proposed a partially shared multi-task convolutional neural network (PS-MCNN) for face attribute recognition. The PS-MCNN consists of four task-specific networks and one shared network to learn shared and task-specific representations. Zhang et al. <ref type="bibr" target="#b62">[63]</ref> proposed Two-Stream Networks for clothing classification and attribute recognition. Since some attributes are located in the local area of the image, many methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b53">54]</ref> resort to the attention mechanism. Guo et al. <ref type="bibr" target="#b16">[17]</ref> presented a two-branch network and constrained the consistency between two attention heatmaps. A multi-scale visual attention and aggregation method was introduced in <ref type="bibr" target="#b48">[49]</ref>, which extracted visual attention masks with only attribute-level supervision. Tang et al. <ref type="bibr" target="#b53">[54]</ref> proposed a flexible attribute localization module to learn attribute-specific regional features. Some other methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b38">39]</ref> further attempt to use additional domain-specific guidance. Semantic segmentation was employed in <ref type="bibr" target="#b25">[26]</ref> to guide the attention of the attribute prediction. Liu et al. <ref type="bibr" target="#b38">[39]</ref> learned clothing attributes with additional landmark labels. There are also some methods <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b63">64]</ref> to study multi-attribute recognition with insufficient data, but this is beyond the scope of this paper.</p><p>Language Modeling: Pre-training language models is a foundational problem for NLP. ELMo <ref type="bibr" target="#b45">[46]</ref> was proposed to learn deep contextualized word representations. It was trained with a bidirectional language model objective, which combined both a forward and backward language model. ELMo representations significantly improve the performance across six NLP tasks. GPT <ref type="bibr" target="#b46">[47]</ref> employed a standard language model objective to pre-train a language model on large unlabeled text corpora. The Transformer was used as the model architecture. The pre-trained model was fine-tuned on downstream tasks and achieved excellent results in 9 of 12 tasks. BERT <ref type="bibr" target="#b10">[11]</ref> used a masked language model pretraining objective, which enabled BERT to learn bidirectional representations conditioned on the left and right context. BERT employed a multi-layer bidirectional Transformer encoder and advanced the state-of-the-art performance. Our work is inspired by the recent success of these methods and is the first attempt to model multi-attribute learning from the perspective of language modeling.</p><p>Transformer for Computer Vision: Transformer <ref type="bibr" target="#b55">[56]</ref> was first proposed for sequence modeling in NLP. Recently, Transformer-based methods have been deployed in many computer vision tasks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b59">60]</ref>. ViT <ref type="bibr" target="#b12">[13]</ref> demonstrated that a pure transformer architecture achieved very competitive results on image classification tasks. DETR <ref type="bibr" target="#b5">[6]</ref> formulated the object detection as a set prediction problem and employed a transformer encoder-decoder architecture. Pix2Seq <ref type="bibr" target="#b7">[8]</ref> regarded object detection as a language modeling task and obtained competitive results. Zheng et al. <ref type="bibr" target="#b64">[65]</ref> replaced the encoder of FCN with a pure transformer for semantic segmentation. Liu et al. <ref type="bibr" target="#b35">[36]</ref> utilized the Transformer decoder architecture for multi-label classification. Temporal query networks were introduced in <ref type="bibr" target="#b59">[60]</ref> for fine-grained video understanding with a query-response mechanism. There are also some efforts <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b43">44]</ref> to apply Transformer to the task of multi-label image classification. Note that the main contribution of this paper is not the use of Transformer, but modeling multi-attribute recognition from the perspective of language modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>In this section, we first give an overview of our framework. Then we present the details of the proposed attribute query network and image-conditioned masked Fig. <ref type="figure">2</ref>. The pipeline of our framework. We recover the entire label "sentence" with a Transformer decoder module, which is conditioned on the token embeddings and image features. Although there are some wrong "words" in the pseudo labels, which are shown in orange, we can treat them as another form of masks. Here E1 or E0 indicates the presence or absence of an attribute.</p><p>language model. Lastly, we introduce the training objective function and inference process of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>Given a sample x from a dataset D with M attribute types, we aim to predict the multiple attributes y to the image x. We let A = {a 1 , a 2 , ..., a M } denote the attribute set, where a j (1 ? j ? M ) represents the j-th attribute type.</p><p>For simplicity, we assume that the values of all attribute types are binary. In other words, the value of a j is 0 or 1, where 1 means that the sample has this attribute and 0 means not. However, our method can be easily extended to the case where each attribute type is multi-valued. With this assumption, we have y ? {0, 1} M . Existing methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b24">25]</ref> usually employ a multi-tasking learning framework, which uses M binary classifiers to predict M attributes respectively. Binary cross-entropy loss is used as the objective. This paper proposes a language modeling framework. We show the pipeline of our framework in Fig. <ref type="figure">2</ref>. The key idea of this paper is to treat attribute labels as unordered "sentences" and use an image-conditioned masked language model to exploit the relationships between attributes. Although we can directly use the real attribute labels as the input of the IC-MLM during training, we cannot access these labels for inference. To address this issue, our Label2Label introduces an attribute query network to generate the initial attribute predictions. These predictions are then treated as pseudo-labels and used as input to the IC-MLM in the training and testing phases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Attribute Query Network</head><p>Given an input image x ? R H0?W0?3 and its corresponding label y = {y j |1 ? j ? M }, we send the image to a feature extractor to obtain the image features, where H 0 and W 0 denote the height and width of the input image respectively, y j denotes the value of j-th attribute a j for the sample x. As our framework is agnostic to the feature extractor, we can use any popular backbones such as ResNet-50 <ref type="bibr" target="#b21">[22]</ref> and ViT <ref type="bibr" target="#b12">[13]</ref>. A naive way to generate initial attribute predictions is to directly feed the extracted image features to a linear layer and learn M binary classifiers. As recent progress <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b59">60]</ref> shows the superiority of Transformer, we consider using the Transformer decoder to implement our attribute query network to generate initial predictions with higher quality.</p><p>Our attribute query network learns a set of permutation-invariant query vectors Q = {q 1 , q 2 , ..., q M }, where each query q j corresponds to an attribute type a j . Then each query vector q j pools the attribute-related features from the image features with Transformer decoder layers and generates the corresponding response vector r j . Finally, we learn a binary classifier for each response vector to generate the initial attribute predictions.</p><p>Since many attributes are only located in some local areas of the image, using global image features is not an excellent choice. Therefore, we preserve the spatial dimensions of image features following <ref type="bibr" target="#b35">[36]</ref>. For ResNet-50, we simply abandon the global pooling layer and employ the output of the last convolution block as the extracted features. We denote the extracted features as X ? R H?W ?d , where H, W , and d represent the height, width, and channel of the image features respectively. To fit with the Transformer decoder, we reshape the feature to be X ? ? R HW ?d . Following common practices <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13]</ref>, we add 2D-aware position embeddings X pos ? R HW ?d to the feature vectors X ? to retain positional information. In this way, we obtain the visual feature vectors X = X ? + X pos .</p><p>With the local visual contexts X, the query features Q = {q j ? R d |1 ? j ? M } are updated using multi-layer Transformer decoders. Formally, we update the query features Q i-1 in the i-th Transformer decoder layer as follows:</p><formula xml:id="formula_0">Q sa i-1 = MultiHead(Q i-1 , Q i-1 , Q i-1 ), Q ca i-1 = MultiHead(Q sa i-1 , X, X ? ), Q i = FFN(Q ca i-1 ),<label>(1)</label></formula><p>where the MultiHead() and FFN() denote the multi-head attention layer and feed-forward layer respectively. Here we set Q as Q 0 . The design philosophy is that for each attribute query vector, it can give high attention scores to the interested local visual features to produce attribute-related features. This design is compatible with the locality of some attributes. Assuming that the attribute query network consists of L layers of Transformer decoders, then we denote Q L as R = {r 1 , r 2 , ..., r M }, where each response vector r j ? R d corresponds to a query vector q j . With the response vectors, we use M independent binary classifiers to predict the attribute values l j = ?(W T j r j + b j ), where W j ? R d and b j ? R 1 are learnable parameters of the j-th attribute classifier, ?(?) is the sigmoid function and l j is the predicted probability for attribute a j of image x.</p><p>In the end, we read out the pseudo label "sentence" s = {s 1 , s 2 , ..., s M } from the predictions {l j } with s j = I(l j &gt; 0.5), where I(?) is an indicator function.</p><p>It is worth noting that the predictions from the attribute query network are not 100% correct, resulting in some wrong "words" in the generated label "sentence". However, we can treat the wrong "words" as another form of masks, because the wrong predictions account for only a small proportion. In fact, the masking strategy of the wrong word is artificially performed in some language models, such as BERT <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Image-Conditioned Masked Language Model</head><p>In existing multi-attribute databases, images are annotated with a variety of attribute labels. This paper is dedicated to modeling sample-wise complex attribute correlations. Instead of treating attribute labels as numbers, we regard them as "words". Since different attribute labels describe the object in an image from different perspectives, we can group them as a sequence of "words". Although the sequence is essentially an unordered "sentence" without any grammatical structure, it still conveys meaningful contextual information. In this way, we treat y as an unordered yet meaningful "sentence", where y j is a "word".</p><p>By treating the labels as sentences, we resort to language modeling methods to mine the instance-level attribute relations effectively. In recent years, pretraining large-scale task-agnostic language models have substantially advanced the development of NLP, among which representative works include ELMo <ref type="bibr" target="#b45">[46]</ref>, GPT-3 <ref type="bibr" target="#b3">[4]</ref>, BERT <ref type="bibr" target="#b10">[11]</ref>, and so on. Inspired by the success of these methods, we consider a masked language model to learn the relationship between "words". We mask some percentage of the attribute label "sentence" y at random, and then reconstruct the entire label "sentence". Specifically, for a binary label sequence, we replace those masked "words" with a special work token [mask] to obtain the masked sentence. Then we input the masked sentence to a masked language model, which aims to recover the entire label sequence. While the MLM has proven to be an effective tool in NLP, directly using it for multi-attribute learning is not feasible. Therefore, we propose several important improvements.</p><p>Instance-wise Attribute Relations: MLM essentially constructs the task P (y 1 , y 2 , ..., y M |M(y 1 ), M(y 2 ), ..., M(y M )) to capture the "word" co-occurrence and learn the joint probability of "word" sequences P (y 1 , y 2 , ..., y M ), where M() denotes the random masking operation. Such a naive approach leads to two problems. The first problem is that MLM only captures statistical attribute correlations. A diverse dataset means that the mapping {M(y 1 ), M(y 2 ), ..., M(y M )} ? {y 1 , y 2 , ..., y M } is a one-to-many mapping. Therefore MLM only learns how different attributes are statistically related to each other. Meanwhile, our experiments find that this prior can be easily modeled by the attribute query network P (y 1 , y 2 , ..., y M |x). The second problem is that MLM and attribute query network cannot be jointly trained. Since MLM uses only the hard prediction of the attribute query network, the gradient from MLM cannot influence the training of the attribute query network. In this way, the method becomes a two-stage label refinement process, which significantly reduces the optimization efficiency.</p><p>To address these issues, we propose an image-conditioned masked language model to learn instance-wise attribute relations. Our IC-MLM captures the relations by constructing a task P (y 1 , y 2 , ..., y M |x, M(y 1 ), M(y 2 ), ..., M(y M )). Introducing an extra image condition is not trivial, as this fundamentally changes the behavior of MLM. With the conditions of image x, the transformation {x, M(y 1 ), M(y 2 ), ..., M(y M )} ? {y 1 , y 2 , ..., y M } is an accurate one-to-one mapping. Our IC-MLM infers other attribute values by combining some attribute label prompts and image contexts in the precise image-to-label mapping, which facilitates the model to learn sample-level attribute relations. In addition, IC-MLM and the attribute query network can use shared image features, which enables them to be jointly optimized with a one-stage framework.</p><p>Word Embeddings: It is known that the word id is not a good word representation in NLP. Therefore, we need to map the word id to a token embedding. Instead of utilizing existing word embeddings with a large token vocabulary like BERT <ref type="bibr" target="#b10">[11]</ref>, we directly learn attribute-related word embeddings E from scratch. We use the word embedding module to map the "word" in the masked sentence to the corresponding token embedding. Since all attributes are binary, we need to build a token vocabulary with a size of 2M to model all possible attribute words. Also, we need to include the token embedding for the special word [mask]. This paper considers three different strategies for the [mask] token embedding. The first strategy believes the [mask] words for different attributes have different meanings, so M attribute-specific learnable token embeddings are learned, where one [mask] token embedding corresponds to one attribute. The second strategy treats the [mask] words for different attributes as the same word. Only one attribute-agnostic learnable token embedding is learned and shared by all attributes. The third strategy is based on the second strategy, which simply replaces the learnable token embedding with a fixed 0 vector. Our experiments find all three strategies work well while the first strategy performs best.</p><p>As mentioned earlier, we use pseudo labels s = {s 1 , s 2 , ..., s M } as input to IC-MLM, so we actually construct P (y 1 , y 2 , ..., y M |x, M(s 1 ), M(s 2 ), ..., M(s M )) as the task. We randomly mask out some "words" in the pseudo-label sequence with a probability of ? to generate masked label "sentences". The "word" M(s j ) in the masked label "sentences" may have three values: 0, 1, and [mask]. We use the word embedding module to map the masked labels "sentences" to a sequence of token embeddings E = {E 1 , E 2 , ..., E M } according to the word value, where E j ? R d denotes the embedding for "word" M(s j ).</p><p>Positional Embeddings: In BERT, the positional embedding of each word is added to its corresponding token embeddings to obtain the position information. Since our "sentences" are unordered, there is no need to introduce positional embeddings to "word" representations. We conducted experiments with positional embeddings by randomly defining some word order and found no improvement. Therefore we do not use positional embeddings for "word" representations and the learned model is permutation invariant for "words".</p><p>Architecture: In NLP, Transformer encoder layers are usually used to implement MLM, while we use multi-layer Transformer decoders to implement IC-MLM due to additional image input conditions. Following the design philosophy similar to the attribute query network, token embeddings E pool features from the local visual features X ? with a cross-attention mechanism. We update the token features E i-1 in the i-th Transformer decoder layer as follows:</p><formula xml:id="formula_1">E sa i-1 = MultiHead(E i-1 , E i-1 , E i-1 ), E ca i-1 = MultiHead(E sa i-1 , X, X ? ), E i = FFN(E ca i-1</formula><p>).</p><p>(</p><formula xml:id="formula_2">)<label>2</label></formula><p>We set E to E 0 and the number of Transformer decoder layers in IC-MLM to D. Then we denote</p><formula xml:id="formula_3">E D as R ? = {r ? 1 , r ? 2 , ..., r ? M }</formula><p>, where r ? j corresponds to the updated feature of token E j . In the end, we perform the final multi-attribute classification with linear projection layers. Formally, we have:</p><formula xml:id="formula_4">p j = ?(W ? j T r ? j + b ? j ), 1 ? j ? M,<label>(3)</label></formula><p>where W ? j ? R d and b ? j ? R 1 are the learnable parameters of the j-th attribute classifier, and p j is the final predicted probability for attribute a j of image x. Note that we are committed to recovering the entire label "sentence" and not just the masked part. In this reconstruction process, we expect our model to grasp the instance-level attribute relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Objective and Inference</head><p>As commonly used in most existing methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b48">49]</ref>, we adopt the binary cross-entropy loss to train the IC-MLM. On the other hand, since most of the datasets for multi-attribute recognition are highly imbalanced, different tasks usually use different weighting strategies. The loss function for the IC-MLM is formulated as L mlm (x) = M j=1 w j (y j log(p j ) + (1 -y j )log(1 -p j )), where w j is the weighting coefficient. According to different tasks, we choose different weighting strategies and always follow the most commonly used strategy for a fair comparison. Meanwhile, to ensure the quality of the generated pseudo label sequences, we also supervise the attribute query network with the same loss function L aqn (x)= M j=1 w j (y j log(l j )+(1-y j )log(1-l j )). The final loss function L total is a combination of the two loss functions above:</p><formula xml:id="formula_5">L total (x) = L aqn (x) + ?L mlm (x), (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>where ? is used to balance these two losses. At inference time, we ignore the masking step and directly input the pseudo label "sentence" to the IC-MLM. Then the output of the IC-MLM is used as the final attribute prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we conducted extensive experiments on three multi-attribute learning tasks to validate the effectiveness of the proposed framework.  <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b39">40]</ref>, we partition the LFWA dataset into two sets, with 6,263 images for training and 6,880 for testing. All images are pre-cropped to a size of 250 ? 250. We adopt the classification error for evaluation following <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b52">53]</ref>.</p><p>Experimental Settings: We trained our model for 57 epochs with a batch size of 16. For optimization, we used an SGD optimizer with a base learning rate of 0.01 and cosine learning rate decay. The weight decay was set to 0.001. To augment the dataset, Rand-Augment <ref type="bibr" target="#b9">[10]</ref> and Random horizontal flipping were performed. We also adopted Mixup <ref type="bibr" target="#b60">[61]</ref> for regularization.</p><p>Parameters Analysis: We first analyze the influence of the number of Transformer decoder layers in the attribute query network and IC-MLM. The results are shown in Tables <ref type="table" target="#tab_1">1</ref> and<ref type="table" target="#tab_2">2</ref>. We see that the best performance is achieved when L = 1 and D = 2. We further conduct experiments with different mask ratios ? and list the results in Table <ref type="table" target="#tab_3">3</ref>. As we mentioned above, the wrong "words" in the pseudo label sequences also provide some form of masks. Therefore, our method performs well when ? = 0. We observe that our method attains the best performance when ? = 0.1. Table <ref type="table" target="#tab_4">4</ref> shows the results with different ?, and we see that ? = 1 gives the best trade-off in (4). We consider three different strategies for [MASK] token embedding and list the results in Table <ref type="table">6</ref>. We see that the attribute-specific strategy achieves the best performance among them, as it better models the differences between the attributes. Unless explicitly mentioned, we adopt these optimal parameters in all subsequent experiments.</p><p>Ablation Study: To validate the effectiveness of our Label2Label, we also conduct experiments on the LFWA dataset with two baseline methods. We first consider the Attribute Query Network (AQN) method, which ignores the IC-MLM and treats the outputs of AQN in Fig. <ref type="figure">2</ref> as final predictions. FC Head method further replaces the Transformer decoder layers in AQN with a linear classification layer. To further verify the generalization of our method, we use dif- ferent feature extraction backbone networks for ablation experiments. To better demonstrate the significance of the results, we also report the standard deviation. The results are presented in Table <ref type="table" target="#tab_5">5</ref>. In addition, we report the computation cost (MACs) of each method in Table <ref type="table" target="#tab_5">5</ref>. We observe that our method significantly outperforms FC Head and AQN across various backbones with marginal computational overhead, which illustrates the effectiveness of our method. 14.72 2018 AFFAIR <ref type="bibr" target="#b30">[31]</ref> 13.87 2018 GNAS <ref type="bibr" target="#b23">[24]</ref> 13.63 2018 PS-MCNN <ref type="bibr" target="#b4">[5]</ref>* 12.64 2018 DMM-CNN <ref type="bibr" target="#b41">[42]</ref> 13.44 2020 SSPL <ref type="bibr" target="#b52">[53]</ref> 13.47 2021 Label2Label 12.49?0.02 -</p><p>We then conducted experiments to show how image-conditioned MLM improves performance. The results are listed in Table <ref type="table" target="#tab_6">7</ref>. As we analyzed above, MLM leads to a two-stage label refinement process. We consider two network architectures to implement MLM: Transformer encoder and multilayer perceptron (MLP). The results show that none of them improve the performance of AQN (13.36%). The reason is that MLM only learns statistical attribute relations, and this prior is easily captured by AQN. Meanwhile, our IC-MLM learns instancewise attribute relations. To see the benefits of the additional image conditions, we still adopt the two-stage label refinement process, and train Transformer decoder layers with fixed image features. We see that performance is boosted to 13.01%, which demonstrates the effectiveness of modeling instance- wise attribute relations. We further jointly train the IC-MLM and attribute query network, which achieves significant performance improvement. These results illustrate the superiority of the proposed IC-MLM.</p><p>Comparison with State-of-the-art Methods: Following <ref type="bibr" target="#b52">[53]</ref>, we employ ResNet50 as the backbone. We present the performance comparison on the LFWA dataset in Table <ref type="table" target="#tab_7">8</ref>. We observe that our method attains the best performance with a simple framework compared to highly tailored domain-specific methods. Label2Label even exceeds the methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b25">26]</ref> of using additional annotations, which further illustrates the effectiveness of our framework.</p><p>Visualization: As the Transformer decoder architecture is used to model the instance-level relations, our method can give better interpretable predictions. We visualize the attention scores in the IC-MLM with DODRIO <ref type="bibr" target="#b57">[58]</ref>. As shown in Fig. <ref type="figure" target="#fig_1">3</ref>, we see that related attributes tend to have higher attention scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Pedestrian Attribute Prediction</head><p>Dataset: The PA-100K <ref type="bibr" target="#b36">[37]</ref> dataset is the largest pedestrian attribute dataset so far <ref type="bibr" target="#b53">[54]</ref>. It contains 100,000 pedestrian images from 598 scenes, which are collected from real outdoor surveillance videos. All pedestrians in each image are annotated with 26 attributes including gender, handbag, and upper clothing. The dataset is randomly split into three subsets: 80% for training, 10% for validation, and 10% for testing. Following SSC <ref type="bibr" target="#b24">[25]</ref>, we merge the training set and the validation set for model training. We use five metrics: one label-based and four instance-based. For the label-based metric, we adopt the mean accuracy (mA) metric. For instance-based metrics, we employ accuracy, precision, recall, and F1 score. As mentioned in <ref type="bibr" target="#b53">[54]</ref>, mA and F1 score are more appropriate and convincing criteria for class-imbalanced pedestrian attribute datasets.</p><p>Experimental Settings: Following the state-of-the-art methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b24">25]</ref>, we adopted ResNet50 as the backbone network to extract image features. We first resize all images into 256?192 pixels. Then random flipping and random cropping were used for data augmentation. SGD optimizer was utilized with the weight decay of 0.0005. We set the initial learning rate of the backbone to 0.01. For fast convergence, we set the initial learning rate of the attribute query network and IC-MLM to 0.1. The batch size was equal to 64. We trained our model for 25 epochs using a plateau learning rate scheduler. We reduced the learning rate by a factor of 10 once learning stagnates and the patience was 4.</p><p>Results and Analysis: We report the results in Table <ref type="table" target="#tab_8">9</ref>. We observe that Label2Label achieves the best performance in mA, Accuracy, and F1 score. Compared to the previous state-of-the-art method SSC <ref type="bibr" target="#b24">[25]</ref>, which designs complex SPAC and SEMC modules to extract discriminative semantic features, our method achieves 0.37% performance improvements in mA. In addition, we report the re-implemented results of the MsVAA, VAC, and ALM methods in the same setting as did in <ref type="bibr" target="#b24">[25]</ref>. Our method consistently outperforms these methods. We further show the results of the FC Head and Attribute Query Network. We see that the performance is improved by replacing the FC head with Transformer decoder layers, which shows the superiority of our attribute query network. Our Label2Label outperforms the attribute query network method by 1.35% for mA, which shows the effectiveness of the language modeling framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Clothing Attribute Recognition</head><p>Dataset: Clothing Attributes Dataset <ref type="bibr" target="#b6">[7]</ref> consists of 1,856 images that contain clothed people. Each image is annotated with 26 clothing attributes, such as For a fair comparison, we only use 23 binary attributes and ignore the remaining three multi-class value attributes as in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b42">43]</ref>. We adopt accuracy as the metric and also report the accuracy of four clothing attribute groups following <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b42">43]</ref>.</p><p>Experimental Settings: For a fair comparison, we utilized AlexNet to extract image features following <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b42">43]</ref>. We trained our model for 22 epochs using a cosine decay learning rate scheduler. We utilized an SGD optimizer with an initial learning rate of 0.05. The batch size was set to 32. For the attribute query network, we employed a 2-layer Transformer decoder (L = 2).</p><p>Results and Analysis: Table <ref type="table" target="#tab_9">10</ref> shows the results. We observe that our La-bel2Label attains a total accuracy of 92.87%, which outperforms other methods with a simple framework. MG-CNN learns one CNN for each attribute, resulting in more training parameters and longer training time. Compared with the attribute query network method, our method achieves better performance on all attribute groups, which illustrates the superiority of our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we have presented Label2Label, which is a simple and generic framework for multi-attribute learning. Different from the existing multi-task learning framework, we proposed a language modeling framework, which regards each attribute label as a "word". Our model learns instance-level attribute relations by the proposed image-conditioned masked language model, which randomly masks some "words" and restores them based on the remaining "sentence" and image context. Compared to well-optimized domain-specific methods, La-bel2Label attains competitive results on three multi-attribute learning tasks. For pedestrian attribute recognition, we follow the widely used weighted binary-entropy strategy in <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b53">54]</ref>. In this way, we have:</p><formula xml:id="formula_7">L mlm (x)= M j=1 w j (y j log(p j )+(1-y j )log(1-p j )), L aqn (x)= M j=1</formula><p>w j (y j log(l j )+(1-y j )log(1-l j )),</p><formula xml:id="formula_8">w j = y j e 1-?j + (1 -y j )e ?j ,<label>(8)</label></formula><p>where ? j is the positive example ratio of the j-th attribute. We conducted ablation experiments on the position embeddings of word representations. Since we are dealing with unordered "sentences", we randomly define three different label sequences and use the corresponding position embeddings respectively. We report the average performance of three different label sequences on the LFWA database in Table <ref type="table" target="#tab_10">11</ref>. We found no additional performance gain from the position embeddings of the word representations. The reason is that our "sentences" are essentially made up of unordered "words".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Position Embeddings for Visual Features</head><p>In our paper, we add 2D-aware position embeddings to visual feature vectors to retain positional information. We conduct experiments to verify their effectiveness and show the results on the LFWA database in Table <ref type="table" target="#tab_11">12</ref>. We observe that introducing position embeddings in visual features is beneficial for performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Comparisons with Transformer-based Multi-label Classification Methods</head><p>Many Transformer-based multi-label classification methods <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b35">36]</ref> have been proposed in recent years. To further verify the effectiveness of the proposed method, we conducted experiments on the three datasets used in our paper. Table <ref type="table" target="#tab_12">13</ref> shows the results. We see our method consistently outperforms C-Tran <ref type="bibr" target="#b26">[27]</ref> and Q2L <ref type="bibr" target="#b35">[36]</ref>, which shows the superiority of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 The Need of Masking</head><p>To verify the effectiveness of masking, we construct three pure reconstruction (without masking) baselines. 1) Feature Reconstruction: direct reconstruction of the word features r 1 , r 2 , ..., r M . 2) Score Reconstruction: direct reconstruction of the predicted scores l 1 , l 2 , ..., l M . 3) Label Reconstruction: direct reconstruction of the labels: y 1 , y 2 , ...y M . Table <ref type="table" target="#tab_13">14</ref> shows the results on the LFWA dataset. Although the Label Reconstruction works competitively, it is still inferior to our method with masking. Just as found in <ref type="bibr" target="#b19">[20]</ref>, although Autoencoder (reconstruction) works well, the Masked Autoencoder (masking) is the key factor to learning better features. In BERT, the masked word is replaced with the [mask] token or a random word. So the MLM has two tasks: mask-recovering and error-correcting. Both increase the training difficulty. In our method, the wrong predictions are like random words in BERT. See Table <ref type="table" target="#tab_13">14</ref>, Ours (? = 0) outperforms Label Reconstruction (12.55 vs 12.70). The only difference is that the input of our IC-MLM contains wrong predictions while Label Reconstruction does not, which proves that our proposed IC-MLM also benefits from handling this special "mask".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Network Structure Configuration</head><p>We show the default hyper-parameters for the Transformer decoder layer of our method in Table <ref type="table" target="#tab_14">15</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E More Visualization Results</head><p>We provide more visualization results of the attention scores in Figure <ref type="figure" target="#fig_2">4</ref>. We conducted the experiments on the LFWA database. We read out the attention from the self-attention layer of our label decoder. The DODRIO <ref type="bibr" target="#b57">[58]</ref> is used for visualization. We show the attention scores of the first head at layer 1 with four examples.</p><p>For the first example, the attribute "Wearing Earrings" is strongly related to the existence of "Wearing Lipstick", "No Beard", and "Female" and the absence of "5 o'clock shadow". For the second sample, the attributes "Oval Face", "Pointy Nose", "Sideburns", "Wearing Necktie" and "Male" imply the existence of "Attractive". For the third sample, the attributes "Wearing Earrings" and "Wearing Lipstick" indicate the gender "Female". For the last example, the attribute "Wearing Lipstick" assigns more attention to the existence of "Wearing Earrings", "Wearing Necklace", "Heavy Makeup" and the absence of "Mustache", "Male". We see our method can learn the instance-level attribute relations even if a sample has some wrong labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Detailed Results</head><p>For facial attribute recognition, some methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b41">42]</ref> report the pre-class recognition accuracy. We report the pre-attribute classification error on the LFWA database in Table <ref type="table" target="#tab_15">16</ref> for a comprehensive comparison.</p><p>We observe our method attains very competitive results with a simple framework compared to highly tailored domain-specific methods, which demonstrates the effectiveness of our method. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Comparisons of the existing multi-task learning framework and our proposed language modeling framework.</figDesc><graphic url="image-4.png" coords="2,148.01,230.86,54.30,54.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Visualization of attention scores among attributes in the self-attention layer. We show the attention of the first head at layer 1 with two samples. The positive attributes of each sample are listed in the corresponding bottom-left corner.</figDesc><graphic url="image-8.png" coords="12,188.09,127.59,114.87,115.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. More visualization results of attention scores in the self-attention layer. We show the attention of the first head at layer 1 with four samples. The positive ground truth attribute labels of each sample are listed in the corresponding bottom-left corner.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Attribute Set Pseudo Labels 0 [M] 1 0 [M] 1 ? 1 Masked Sentence Learnable Word Embeddings</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>0</cell><cell>1</cell><cell>1</cell><cell>0</cell><cell>1</cell><cell></cell><cell></cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Transformer Decoder</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Feature Extractor</cell><cell>? ?</cell><cell>? [?]</cell><cell>? ?</cell><cell>? ?</cell><cell>? [?]</cell><cell>? ?</cell><cell>?</cell><cell>? ?</cell><cell>Token Embeddings</cell></row><row><cell cols="2">Image Features</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>? ?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>? ?</cell><cell>Transformer</cell><cell>0</cell><cell>1</cell><cell>1</cell><cell>0</cell><cell>1</cell><cell>1</cell><cell>?</cell><cell>1</cell></row><row><cell>?</cell><cell>Decoder</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>? ?</cell><cell></cell><cell>? ?</cell><cell>? ?</cell><cell>? ?</cell><cell>? ?</cell><cell>? ?</cell><cell>? ?</cell><cell>?</cell><cell>? ?</cell></row><row><cell cols="2">Attribute Query Network</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Results with different Transformer decoder layers D for IC-MLM. We fix L as 1.</figDesc><table><row><cell>D</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell></row><row><cell cols="5">Error(%) 12.58 12.49 12.54 12.52</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Results with different Transformer decoder layers L for attribute query network. We fix D as 2.</figDesc><table><row><cell>L</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell></row><row><cell cols="5">Error(%) 12.49 12.52 12.50 12.58</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Results on the LFWA dataset with different mask ratios ?.</figDesc><table><row><cell>?</cell><cell>0</cell><cell>0.1 0.15 0.2 0.3</cell></row><row><cell cols="3">Error(%) 12.55 12.49 12.55 12.54 12.57</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Results on the LFWA dataset with different coefficients ?.</figDesc><table><row><cell>?</cell><cell>0.5 0.8</cell><cell>1</cell><cell>1.2 1.5</cell></row><row><cell cols="4">Error(%) 12.64 12.56 12.49 12.60 12.63</cell></row></table><note><p><p><p>Dataset: LFWA</p><ref type="bibr" target="#b39">[40]</ref> </p>is a popular unconstrained facial attribute dataset, which consists of 13,143 facial images of 5,749 identities. Each facial image has 40 attribute annotations. Following the same evaluation protocol in</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Ablation experiments with different backbones.</figDesc><table><row><cell>Backbone</cell><cell cols="2">ResNet-50</cell><cell cols="2">ResNet-101</cell><cell>ViT-B</cell><cell></cell></row><row><cell>Metric</cell><cell cols="6">Error(%) MACs(G) Error(%) MACs(G) Error(%) MACs(G)</cell></row><row><cell>FC Head</cell><cell>13.63?0.02</cell><cell>5.30</cell><cell>13.05?0.03</cell><cell cols="3">10.15 13.73? 0.02 16.85</cell></row><row><cell>AQN</cell><cell>13.36?0.04</cell><cell>5.63</cell><cell>12.70?0.02</cell><cell>10.48</cell><cell>13.32?0.04</cell><cell>16.97</cell></row><row><cell cols="2">Label2Label 12.49?0.02</cell><cell>6.30</cell><cell cols="4">12.44?0.04 11.16 12.79?0.01 17.23</cell></row><row><cell>Table 6.</cell><cell cols="2">Results of differ-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">ent strategies for [Mask] embed-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>dings.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Strategy</cell><cell>Error(%)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0 Vector</cell><cell>12.60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Attribute-Agnostic 12.57</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Attribute-Specific 12.49</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>Comparisons of MLM and IC-MLM.</figDesc><table><row><cell cols="2">Method Architecture</cell><cell>Co-training Error(%) with AQN</cell></row><row><cell>MLM</cell><cell>MLP TransEncoder</cell><cell>13.34 13.32</cell></row><row><cell>IC-MLM</cell><cell>TransDecoder TransDecoder</cell><cell>13.01 12.49</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 .</head><label>8</label><figDesc>Performance</figDesc><table><row><cell></cell><cell cols="2">comparison with</cell></row><row><cell cols="3">state-of-the-art methods on the LFWA</cell></row><row><cell cols="3">dataset. We report the average classifica-</cell></row><row><cell cols="3">tion error results. * indicates that addi-</cell></row><row><cell cols="3">tional labels are used for training, such as</cell></row><row><cell cols="3">identity labels or segment annotations.</cell></row><row><cell>Method</cell><cell cols="2">Error(%) Year</cell></row><row><cell>SSP + SSG [26]*</cell><cell>12.87</cell><cell>2017</cell></row><row><cell>He et al. [23]</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 .</head><label>9</label><figDesc>Comparisons on the PA100K dataset. * represents the reimplementation performance using the same setting. We also report the standard deviations.</figDesc><table><row><cell>Method</cell><cell>mA</cell><cell>Accuracy</cell><cell>Precision</cell><cell>Recall</cell><cell>F1</cell></row><row><cell>DeepMAR [28]</cell><cell>72.70</cell><cell>70.39</cell><cell>82.24</cell><cell>80.42</cell><cell>81.32</cell></row><row><cell>HPNet [37]</cell><cell>74.21</cell><cell>72.19</cell><cell>82.97</cell><cell>82.09</cell><cell>82.53</cell></row><row><cell>VeSPA [50]</cell><cell>76.32</cell><cell>73.00</cell><cell>84.99</cell><cell>81.49</cell><cell>83.20</cell></row><row><cell>LGNet [35]</cell><cell>76.96</cell><cell>75.55</cell><cell>86.99</cell><cell>83.17</cell><cell>85.04</cell></row><row><cell>PGDM [29]</cell><cell>74.95</cell><cell>73.08</cell><cell>84.36</cell><cell>82.24</cell><cell>83.29</cell></row><row><cell>MsVAA [49]*</cell><cell>80.10</cell><cell>76.98</cell><cell>86.26</cell><cell>85.62</cell><cell>85.50</cell></row><row><cell>VAC [17]*</cell><cell>79.04</cell><cell>78.95</cell><cell>88.41</cell><cell>86.07</cell><cell>86.83</cell></row><row><cell>ALM [54]*</cell><cell>79.26</cell><cell>78.64</cell><cell>87.33</cell><cell>86.73</cell><cell>86.64</cell></row><row><cell>SSC [25]</cell><cell>81.87</cell><cell>78.89</cell><cell>85.98</cell><cell>89.10</cell><cell>86.87</cell></row><row><cell>FC Head</cell><cell cols="5">77.96?0.06 75.86?0.79 86.27?0.13 84.16?1.02 84.72?0.55</cell></row><row><cell>AQN</cell><cell cols="5">80.89?0.08 78.51?0.08 86.15?0.40 87.85?0.43 86.58?0.03</cell></row><row><cell>Label2Label</cell><cell cols="5">82.24?0.13 79.23?0.13 86.39?0.32 88.57?0.20 87.08?0.08</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 .</head><label>10</label><figDesc>The comparisons between our method and other state-of-the-art methods on the Clothing Attributes Dataset. We report accuracy and standard deviation.</figDesc><table><row><cell>Method</cell><cell>Colors</cell><cell>Patterns</cell><cell>Parts</cell><cell>Appearance</cell><cell>Total</cell></row><row><cell>S-CNN [1]</cell><cell>90.50</cell><cell>92.90</cell><cell>87.00</cell><cell>89.57</cell><cell>90.43</cell></row><row><cell>M-CNN [1]</cell><cell>91.72</cell><cell>94.26</cell><cell>87.96</cell><cell>91.51</cell><cell>91.70</cell></row><row><cell>MG-CNN [1]</cell><cell>93.12</cell><cell>95.37</cell><cell>88.65</cell><cell>91.93</cell><cell>92.82</cell></row><row><cell>Meng et al. [43]</cell><cell>91.64</cell><cell>96.81</cell><cell>89.25</cell><cell>89.53</cell><cell>92.39</cell></row><row><cell>FC Head</cell><cell cols="5">91.39?0.23 96.07?0.05 87.00?0.27 88.21?0.36 91.57?0.12</cell></row><row><cell>AQN</cell><cell cols="5">91.98?0.25 96.37?0.23 88.19?0.47 89.89?0.33 92.29?0.05</cell></row><row><cell>Label2Label</cell><cell cols="5">92.73?0.07 96.82?0.02 88.20?0.09 90.88?0.18 92.87?0.03</cell></row><row><cell cols="6">colors and patterns. We use 1,500 images for training and the rest for testing.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 11 .</head><label>11</label><figDesc></figDesc><table><row><cell></cell><cell>Ablation experi-</cell></row><row><cell cols="2">ments on the position embed-</cell></row><row><cell cols="2">dings of word representations.</cell></row><row><cell>Method</cell><cell>Pos Error(%)</cell></row><row><cell>Label2Label</cell><cell>12.49 12.51</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 12 .</head><label>12</label><figDesc>Ablation experiments on the position embeddings of visual features.</figDesc><table><row><cell>Method</cell><cell>Pos Error(%)</cell></row><row><cell>AQN</cell><cell>13.51 13.36</cell></row><row><cell>Label2Label</cell><cell>12.98 12.49</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 13 .</head><label>13</label><figDesc>Comparisons of our method with other Transformer-based methods.</figDesc><table><row><cell>Dataset</cell><cell>LFWA</cell><cell></cell><cell>PA100K</cell><cell></cell><cell>Clothing</cell></row><row><cell>Meteic</cell><cell>Error</cell><cell>mA</cell><cell>Accuracy</cell><cell>F1</cell><cell>Accuracy</cell></row><row><cell>C-Tran [27]</cell><cell>14.66</cell><cell>81.53</cell><cell>78.97</cell><cell>86.86</cell><cell>90.00</cell></row><row><cell>Q2L [36]</cell><cell>13.28</cell><cell>80.72</cell><cell>78.78</cell><cell>86.73</cell><cell>91.81</cell></row><row><cell>Ours</cell><cell>12.49</cell><cell>82.37</cell><cell>79.03</cell><cell>86.96</cell><cell>92.87</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 14 .</head><label>14</label><figDesc>Comparisons with three pure reconstruction baselines.</figDesc><table><row><cell>Method</cell><cell cols="2">Reconstruction Feature Score</cell><cell>Label</cell><cell cols="2">Ours (Masking) ? = 0 ? = 0.1</cell></row><row><cell>Error(%)</cell><cell>13.45</cell><cell>13.63</cell><cell>12.70</cell><cell>12.55</cell><cell>12.49</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 15 .</head><label>15</label><figDesc>Hyperparameters for the Transformer decoder layer of our method.</figDesc><table><row><cell>Component</cell><cell>Hyperparameters</cell></row><row><cell>Activation</cell><cell>GELU</cell></row><row><cell>Hidden dim</cell><cell>2048</cell></row><row><cell>FFN hidden size</cell><cell>2048</cell></row><row><cell>Attention heads</cell><cell>4</cell></row><row><cell>Attention head szie</cell><cell>512</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 16 .</head><label>16</label><figDesc>The classification error (%) obtained by all the competing methods on the LFWA datasets. The accuracy for each attribute obtained by the proposed method is highlighted in bold. .00  19.00 20.00 16.00 16.00 27.00 21.00 13.00 6.00 26.00 26.00 21.00 31.00 LNets+ANet [40] 16.00 18.00 17.00 17.00 12.00 12.00 25.00 19.00 10.00 3.00 26.00 23.00 18.00 27.00 NSA [41] 22.41 18.28 19.84 17.38 8.12 9.29 21.03 16.87 7.51 2.53 13.58 19.07 15.74 23.94 MCNN-AUX [19] 22.94 18.22 19.69 16.52 8.06 9.92 20.76 15.02 7.37 2.59 14.77 19.15 15.03 23.14 MCFA [66] 25.00 21.00 23.00 21.00 9.00 11.00 25.00 19.00 9.00 3.00 14.00 23.00 24.00 26.00 PS-MCNN-LC [5] 21.83 16.47 18.16 13.26 7.40 8.55 17.30 13.52 7.04 1.49 12.80 18.13 14.28 21.89 DMTL [18] 20.00 14.00 18.00 16.00 8.00 7.00 23.00 17.00 8.00 3.00 11.00 19.00 20.00 25.00 DMM-CNN [42] 20.82 17.30 18.90 17.30 8.04 8.70 20.18 16.33 8.45 2.83 12.42 18.44 14.67 22.34 Label2Label 20.7616.6718.2816.10 6.93 8.06 19.4015.00 6.96 2.25 13.0016.8812.8921.61 .00 25.00 19.00 7.00 14.00 8.00 22.00 13.00 27.00 25.00 28.00 16.00 24.00 LNets+ANet [40] 22.00 5.00 22.00 16.00 5.00 12.00 6.00 18.00 8.00 19.00 21.00 26.00 16.00 20.00 NSA [41] 19.51 8.50 16.99 11.54 4.61 11.66 7.40 17.50 7.03 17.25 19.23 23.20 9.03 15.80 MCNN-AUX [19] 18.48 8.70 17.03 11.07 4.15 11.62 5.98 6.49 6.57 17.14 17.85 22.61 6.68 15.86 MCFA [66] 23.00 9.00 20.00 12.00 6.00 15.00 7.00 22.00 9.00 22.00 21.00 26.00 18.00 20.00 PS-MCNN-LC [5] 13.30 7.22 15.89 8.96 3.40 11.23 4.82 15.40 5.53 16.49 17.99 22.10 5.03 12.48 DMTL [18] 22.00 8.00 14.00 12.00 5.00 11.00 7.00 14.00 5.00 18.00 19.00 25.00 9.00 16.00 DMM-CNN [42] 19.02 7.17 17.18 10.62 4.32 11.87 5.86 15.55 5.54 16.33 17.52 23.06 8.14 15.49 Label2Label 16.24 7.38 15.3410.13 3.88 10.36 5.78 16.38 5.89 15.5216.4520.26 8.47 15.09 .00 24.00 11.00 27.00 25.00 8.00 18.00 7.00 14.00 21.00 18.00 18.97 LNets+ANet [40] 15.00 22.00 23.00 9.00 24.00 24.00 6.00 12.00 5.00 12.00 21.00 14.00 16.15 NSA [41] 15.10 12.92 18.24 9.20 21.09 21.72 5.25 9.77 5.93 10.41 18.60 14.32 14.18 MCNN-AUX [19] 13.75 12.08 16.87 8.17 21.47 18.39 5.05 9.93 4.96 10.06 19.34 14.16 13.69 MCFA [66] 15.00 15.00 22.00 12.00 23.00 21.00 7.00 9.00 6.00 11.00 18.00 13.00 16.37 PS-MCNN-LC [5] 12.50 11.19 15.58 7.30 20.35 16.65 4.46 8.79 4.30 9.08 17.82 13.12 12.64 DMTL [18] 15.00 14.00 20.00 8.00 21.00 20.00 6.00 8.00 7.00 9.00 19.00 13.00 13.85 DMM-CNN [42] 13.70 13.56 17.01 7.76 20.80 20.13 5.86 9.16 4.89 10.53 18.72 11.06 13.44 Label2Label 12.4610.6514.90 7.85 16.8317.17 5.12 8.02 4.96 9.74 16.0313.82 12.49</figDesc><table><row><cell></cell><cell>5 o'clock Shadow</cell><cell>Arched Eyebrows</cell><cell>Attractive</cell><cell>Bags Under Eyes</cell><cell>Bald</cell><cell>Bangs</cell><cell>Big Lips</cell><cell>Big Nose</cell><cell>Black Hair</cell><cell>Blond Hair</cell><cell>Blurry</cell><cell>Brown Hair</cell><cell>Bushy Eyebrows</cell><cell>Chubby</cell></row><row><cell>PANDA [62]</cell><cell cols="2">16.00 21Double Chin Eyeglasses</cell><cell>Goatee</cell><cell>GrayHair</cell><cell>Heavy Makeup</cell><cell>High Cheekbones</cell><cell>Male</cell><cell>MouthOpen</cell><cell>Mustache</cell><cell>NarrowEyes</cell><cell>NoBeard</cell><cell>OvalFace</cell><cell>PaleSkin</cell><cell>PointyNose</cell></row><row><cell>PANDA [62]</cell><cell cols="2">25.00 11RecedingHairline RosyCheeks</cell><cell>Sideburns</cell><cell>Smiling</cell><cell>Straight Hair</cell><cell>WavyHair</cell><cell>WearingEarrings</cell><cell>WearingHat</cell><cell>WearingLipstick</cell><cell>WearingNecklace</cell><cell>WearingNecktie</cell><cell>Young</cell><cell></cell><cell>Average</cell></row><row><cell>PANDA [62]</cell><cell cols="2">16.00 27</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. This work was supported in part by the <rs type="funder">National Key Research and Development Program of China</rs> under Grant <rs type="grantNumber">2017YFA0700802</rs>, in part by the <rs type="funder">National Natural Science Foundation of China</rs> under Grant <rs type="grantNumber">62125603</rs> and Grant <rs type="grantNumber">U1813218</rs>, in part by a grant from the <rs type="funder">Beijing Academy of Artificial Intelligence (BAAI)</rs>. The authors would sincerely thank <rs type="person">Yongming Rao</rs> and <rs type="person">Zhiheng Li</rs> for their generous helps.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_7PQ5FNJ">
					<idno type="grant-number">2017YFA0700802</idno>
				</org>
				<org type="funding" xml:id="_spf95FR">
					<idno type="grant-number">62125603</idno>
				</org>
				<org type="funding" xml:id="_u2bJ3f2">
					<idno type="grant-number">U1813218</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Materials</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Evaluation Metrics</head><p>For pedestrian attribute prediction, we adopted five evaluation metrics. We present the details of these metrics. The only label-based metric is the mean accuracy (mA) metric, which is the mean of positive accuracy and negative accuracy for each attribute. Mathematically, the mA is calculated by:</p><p>where M is the number of attributes, P j and T P j represent the numbers of positive samples and correctly predicted positive samples of the j-th attribute respectively, N and T N j are the numbers of negative samples and correctly predicted negative samples of the j-th attribute respectively. We also consider four example-based metrics: accuracy, precision, recall, and F1 score:</p><p>where N denotes the number of samples, Y i is the positive labels of the i-th sample and Y ? i is the predicted positive values for the i-th sample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Weighting Strategy</head><p>For facial attribute recognition and clothing attribute recognition, we follow the common practice which does not utilize the weighting strategy for loss functions. Therefore, we have: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Earrings</head><p>Wear.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hat</head><p>Wear.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lipstick</head><p>Wear.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Necklace</head><p>Wear.</p><p>Necktie Young</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multi-task cnn model for attribute prediction</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Abdulnabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TMM</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1949" to="1959" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning attribute representations with localization for flexible fashion search</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Ak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Kassim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Tham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7708" to="7717" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Beit: Bert pre-training of image transformers</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08254</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Partially shared multi-task convolutional neural network with local constraint for face attribute learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4290" to="4299" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="213" to="229" />
		</imprint>
		<respStmt>
			<orgName>ECCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Describing clothing by semantic attributes</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Girod</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="609" to="623" />
		</imprint>
		<respStmt>
			<orgName>ECCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Pix2seq: A language modeling framework for object detection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.10852</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.06195</idno>
		<title level="m">Mltr: Multi-label classification with transformer</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>NeurIPS</publisher>
			<biblScope unit="page" from="18613" to="18624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NAACL</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Crosstransformers: spatially-aware few-shot transfer</title>
		<author>
			<persName><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Discovering localized attributes for fine-grained recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="3474" to="3481" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Describing objects by their attributes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1778" to="1785" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Visual attention consistency under image transforms for multi-label image classification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="729" to="739" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Heterogeneous face attribute estimation: A deep multi-task learning approach</title>
		<author>
			<persName><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2597" to="2609" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Attributes for improved attributes: A multi-task network utilizing implicit and explicit relationships for facial attribute classification</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Hand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="16000" to="16009" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xinlei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08254</idno>
		<title level="m">Masked autoencoders are scalable vision learners</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Harnessing synthesized abstraction images to improve facial attribute recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="page" from="733" to="740" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Gnas: A greedy neural architecture search method for multi-attribute learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Q</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>ACM MM</publisher>
			<biblScope unit="page" from="2049" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Spatial and semantic consistency regularizations for pedestrian attribute recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="962" to="971" />
		</imprint>
		<respStmt>
			<orgName>ICCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Improving facial attribute prediction using semantic segmentation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Kalayeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6942" to="6950" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">General multi-label image classification with transformers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lanchantin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="16478" to="16488" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Multi-attribute learning for pedestrian attribute recognition in surveillance scenarios</title>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="111" to="115" />
		</imprint>
		<respStmt>
			<orgName>ACPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Pose guided deep model for pedestrian attribute recognition in surveillance scenarios</title>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
		<respStmt>
			<orgName>ICME</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A richly annotated pedestrian dataset for person retrieval in real surveillance scenarios</title>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1575" to="1590" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Landmark free face attribute prediction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4651" to="4662" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Graph-based social relation reasoning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="18" to="34" />
		</imprint>
		<respStmt>
			<orgName>ECCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Learning probabilistic ordinal embeddings for uncertainty-aware regression</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="13896" to="13905" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Bridgenet: A continuity-aware probabilistic network for age estimation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1145" to="1154" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Localization guided learning for pedestrian attribute recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.10834</idno>
		<title level="m">Query2label: A simple transformer way to multi-label classification</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Hydraplus-net: Attentive deep features for pedestrian analysis</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="350" to="359" />
		</imprint>
		<respStmt>
			<orgName>ICCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Deepfashion: Powering robust clothes recognition and retrieval with rich annotations</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1096" to="1104" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3730" to="3738" />
		</imprint>
		<respStmt>
			<orgName>ICCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Segment-based methods for facial attribute detection from partial faces</title>
		<author>
			<persName><forename type="first">U</forename><surname>Mahbub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TAC</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="601" to="613" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Deep multi-task multi-label cnn for effective facial attribute classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>TAC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Efficient relative attribute learning using graph neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Adluru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Singh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="552" to="567" />
		</imprint>
		<respStmt>
			<orgName>ECCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Modular graph transformer networks for multilabel image classification</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">S</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9092" to="9100" />
		</imprint>
		<respStmt>
			<orgName>AAAI</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Temporalrelational crosstransformers for few-shot action recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Masullo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Burghardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirmehdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Damen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="475" to="484" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep contextualized word representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Moon: A mixed objective optimization network for the recognition of facial attributes</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Rudd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>G?nther</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Boult</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="19" to="35" />
		</imprint>
		<respStmt>
			<orgName>ECCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Deep imbalanced attribute classification using visual attention aggregation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Sarafianos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">A</forename><surname>Kakadiaris</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="680" to="697" />
		</imprint>
		<respStmt>
			<orgName>ECCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Deep view-sensitive pedestrian attribute inference in an end-to-end model</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Sarfraz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Deeply learned attributes for crowded scene understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Change Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4657" to="4666" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Semi-supervised learning with a teacher-student network for generalized attribute prediction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="509" to="525" />
		</imprint>
		<respStmt>
			<orgName>ECCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Learning spatialsemantic relationship for facial attribute recognition with limited labeled data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="11916" to="11925" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Improving pedestrian attribute recognition with weakly-supervised multi-scale attribute-specific localization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4997" to="5006" />
		</imprint>
		<respStmt>
			<orgName>ICCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">cloze procedure&quot;: A new tool for measuring readability</title>
		<imprint>
			<date type="published" when="1953">1953</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="415" to="433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">End-to-end video instance segmentation with transformers</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8741" to="8750" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Dodrio: Exploring transformer models with interactive visualization</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Turko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Chau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Frequency-aware spatiotemporal transformers for video inpainting detection</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8188" to="8197" />
		</imprint>
		<respStmt>
			<orgName>ICCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Temporal query networks for fine-grained video understanding</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4486" to="4496" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Panda: Pose aligned networks for deep attribute modeling</title>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1637" to="1644" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Texture and shape biased two-stream networks for clothing classification and attribute recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="13538" to="13547" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Recognizing part attributes with insufficient data</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="350" to="360" />
		</imprint>
		<respStmt>
			<orgName>ICCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6881" to="6890" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Multi-task learning of cascaded cnn for facial attribute classification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2069" to="2074" />
		</imprint>
		<respStmt>
			<orgName>ICPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
