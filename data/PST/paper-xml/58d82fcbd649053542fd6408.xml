<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Person Re-Identification by Unsupervised Video Matching</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiaolong</forename><surname>Ma</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">China Academy of Electronics and Information Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
							<email>xiatian.zhu@qmul.ac.uk</email>
							<affiliation key="aff2">
								<orgName type="institution">Queen Mary University of London</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
							<email>s.gong@qmul.ac.uk</email>
							<affiliation key="aff2">
								<orgName type="institution">Queen Mary University of London</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xudong</forename><surname>Xie</surname></persName>
							<email>xdxie@mail.tsinghua.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jianming</forename><surname>Hu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kin-Man</forename><surname>Lam</surname></persName>
							<email>kin.man.lam@polyu.edu.hk</email>
							<affiliation key="aff3">
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yisheng</forename><surname>Zhong</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Shaogang Gong</orgName>
								<orgName type="institution">Xiatian Zhu</orgName>
								<address>
									<addrLine>Xudong Xie, Jianming Hu, Kin-Man Lam</addrLine>
									<settlement>Yisheng Zhong</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Person Re-Identification by Unsupervised Video Matching</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8E89FE99AADFB4D2020CAF99B060ADAE</idno>
					<idno type="DOI">10.1016/j.patcog.2016.11.018</idno>
					<note type="submission">Received date: 31 March 2016 Revised date: 21 November 2016 Accepted date: 21 November 2016 Preprint submitted to Pattern Recognition November 19, 2016</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T05:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Pattern Recognition Person re-identification</term>
					<term>action recognition</term>
					<term>gait recognition</term>
					<term>video matching</term>
					<term>temporal sequence matching</term>
					<term>spatio-temporal pyramids</term>
					<term>time shift</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most existing person re-identification (ReID) methods rely only on the spatial appearance information from either one or multiple person images, whilst ignore the space-time cues readily available in video or image-sequence data.</p><p>Moreover, they often assume the availability of exhaustively labelled cross-view pairwise data for every camera pair, making them non-scalable to ReID applications in real-world large scale camera networks. In this work, we introduce a novel video based person ReID method capable of accurately matching people across views from arbitrary unaligned image-sequences without any labelled pairwise data. Specifically, we introduce a new space-time person representation by encoding multiple granularities of spatio-temporal dynamics in form of time series. Moreover, a Time Shift Dynamic Time Warping (TS-DTW) model is derived for performing automatically alignment whilst achieving data selection and matching between inherently inaccurate and incomplete sequences in a unified way. We further extend the TS-DTW model for accommodating multiple feature-sequences of an image-sequence in order to fuse information from different descriptions. Crucially, this model does not require pairwise labelled training data (i.e. unsupervised) therefore readily scalable to large scale cam-</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In visual surveillance, associating automatically individual people across disjoint camera views is essential. This task is known as person re-identification (ReID). Cross-view person ReID enables automated discovery and analysis of person-specific long-term structural activities over widely expanded areas and is fundamental to many important surveillance applications such as multi-camera people tracking and forensic search. Specifically, for performing person ReID, one matches a probe (or query) person observed in one camera view against a set of gallery people captured in another disjoint view for generating a ranked list according to their matching distance or similarity <ref type="bibr" target="#b0">[1]</ref>. This is an inherently challenging problem <ref type="bibr" target="#b1">[2]</ref>. Most existing approaches <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref> perform ReID by modelling spatial visual appearance (shape, texture and colour) of one or multiple person images. However, people appearance is intrinsically limited due to the inevitable visual ambiguity and unreliability caused by appearance similarity among different people and appearance variations of the same person from unknown significant cross-view changes in human pose, viewpoint, illumination, occlusion, and dynamic background clutter. This motivates the need of seeking additional visual information sources for person ReID.</p><p>On the other hand, video (or image-sequence) data are often available from visual surveillance cameras. Videos have been extensively exploited for performing action and activity recognition by extracting and modelling a variety of dynamic space-time visual features <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>. However, action recognition differs fundamentally from person ReID. First, it often aims to discriminate between Apart from action recognition, another closely related problem is gait recognition <ref type="bibr" target="#b12">[13]</ref>. Similar to ReID, gait recognition aims for differentiating between distinct people by characterising people's walking dynamics. Further, an advantage of gait recognition is no assumption being made on either subject cooperation or person distinctive actions. These characteristics are analogous in spirit to person ReID. Nonetheless, existing gait recognition methods are heavily subject to stringent requirements on person foreground segmentation and accurate temporal alignment throughout a gait image sequence (a walking cycle). Additionally, most gait recognition methods do not deal well with cluttered background and/or random occlusions with unknown covariate conditions <ref type="bibr" target="#b13">[14]</ref> (Figures <ref type="figure" target="#fig_6">1</ref> and<ref type="figure" target="#fig_2">2</ref>). Hence, person ReID in public spaces is inherently challenging for existing gait recognition techniques.</p><p>This work aims to develop a video based person ReID approach, without the need for exhaustively labelling people pairs across camera views. To that end, one needs to extract and model reliably person-specific space-time infor-mation from videos. This is non-trivial, especially when the videos are captured from uncontrolled and crowded public scenes. The specific challenges include:</p><p>(1) The starting/ending frames of individual videos may correspond to arbitrary walking phases. Thus, any two compared videos are mostly unaligned.</p><p>This misalignment leads to inaccuracy in people matching, especially when the useful space-time information in person videos can be very subtle. <ref type="bibr" target="#b1">(2)</ref> Person videos have varying numbers of walking cycles and a holistic matching between videos may yield suboptimal recognition. While pose estimation and walking cycle detection may help in theory, contemporary techniques <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> are still rather unreliable for video data with distracting background and low imaging quality.</p><p>(3) Person image-sequences captured from public places can consist of corrupted frames due to background clutter and random inter-object occlusions (see Figure <ref type="figure" target="#fig_0">1</ref>). A blind trust and utilisation of all visual data may degrade the person matching accuracy. Following <ref type="bibr" target="#b16">[17]</ref>, we call this unregulated imagesequences. We wish to develop an accurate person ReID method that does not require performing explicit walking phase detection for videos neither occlusion estimation for image frames. The main contributions of this study are:</p><p>1. We propose an unsupervised approach to person ReID based on typical surveillance image-sequences. Our model differs significantly from most conventional static image based methods (e.g. leveraging dynamic spacetime information versus static appearance information), and also the recent DVR video ReID model <ref type="bibr" target="#b17">[18]</ref> (e.g. unsupervised versus supervised).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">We present a new video representation particularly tailored for person</head><p>ReID. Specifically, this representation is built up on existing action spacetime features (e.g. histograms of oriented 3D spatio-temporal gradient <ref type="bibr" target="#b18">[19]</ref>) and spatio-temporal pyramids <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. In contrast to most visual features for action recognition which are vectorial, our video representation is in form of sequence or time series. This is specially designed for reliable selection based person matching between cross-view unregulated video pairs with possibly ambiguous, incomplete and noisy observation. We show the effectiveness of the proposed approach on two benchmarking image-sequence ReID datasets (PRID2011 <ref type="bibr" target="#b21">[22]</ref> and iLIDS-VID <ref type="bibr" target="#b16">[17]</ref>) under both the closed-world and more realistic open-world scenarios <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b8">9]</ref>. Extensive comparative evaluations were conducted by comparing alternative sequencematching person recognition models including gait recognition <ref type="bibr" target="#b23">[24]</ref> and dynamic time warping <ref type="bibr" target="#b24">[25]</ref>, and the state-of-the-art person ReID methods including SDALF <ref type="bibr" target="#b2">[3]</ref>, eSDC <ref type="bibr" target="#b5">[6]</ref>, DVR <ref type="bibr" target="#b17">[18]</ref>, RDL <ref type="bibr" target="#b25">[26]</ref>, and XQDA <ref type="bibr" target="#b26">[27]</ref>.</p><p>The remainder of this paper is organised as follows. In Section 2, we discuss broadly the related studies. In Section 3, we present an overview of our approach, followed by video representation in Section 4, video matching in Section 5, and person re-identification application in Section 6. Then, we depict the experimental settings in Section 7 and provide comparative evaluations of our proposed approach in Section 8. Finally, we conclude this study in Section 9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Gait recognition. Gait recognition <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref> has been extensively exploited for people identification using video space-time features, e.g. correlation based motion feature <ref type="bibr" target="#b31">[32]</ref>, and Gait Energy Image (GEI) templates <ref type="bibr" target="#b32">[33]</ref>.</p><p>To improve gait representations, Veres et al. <ref type="bibr" target="#b33">[34]</ref> and Matovski et al. <ref type="bibr" target="#b34">[35]</ref> suggest feature selection and quality measure. These methods assume that imagesequences are aligned and captured in controlled environments with uncluttered background, as well as having complete gait cycles, little occlusion, and accurate gait phase estimation. However, these constraints are often invalid in person ReID context as shown in Figures <ref type="figure" target="#fig_2">2</ref> and<ref type="figure" target="#fig_10">7</ref>.</p><p>To handle often-occurring occlusion, Hofmann et al. <ref type="bibr" target="#b35">[36]</ref> propose a specific dataset for evaluating their negative influence on gait recognition performance.</p><p>Meanwhile, a number of part-based methods <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref> are developed by assuming that matched people share common observed parts (COPs). For relaxing this assumption, Muramatsu et al. <ref type="bibr" target="#b39">[40]</ref> reconstruct complete gait features from partially observed body parts without sharing COPs. These methods rely on accurate body part segmentation and occlusion detection, which is however overdemanding for contemporary segmentation methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b40">41]</ref> given typical ReID video data captured against uncooperative people and dynamic scenes.</p><p>Main challenges for gait recognition arise from various covariate conditions, e.g. carrying, clothing, walking surface, footwear, and viewpoint. Beyond the attempts of designing and investigating gait features invariable to specific covariates <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b13">14]</ref>, more powerful learning based methods have also been presented for explicitly and accurately modelling the complex variances of gait structures. For example, Martín-Félez and Xiang <ref type="bibr" target="#b44">[45]</ref> exploit the learningto-rank strategy for jointly characterising a variety of covariate conditions in a unified model. Whilst a learning process may help improve the gait recognition accuracy, this strategy is heavily affected by the goodness of gait features. On person ReID videos however, gait features are likely to be extremely unreliable, as demonstrated in Figure <ref type="figure" target="#fig_2">2</ref>.</p><p>Temporal sequence matching. Temporal sequence matching is another alternative strategy. The Dynamic Time Warping (DTW) model <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47]</ref> and its variants including derivative DTW <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b48">49]</ref>, weighted DTW <ref type="bibr" target="#b49">[50]</ref>, are common sequence matching algorithms widely used in data mining and pattern recognition. Given two temporal sequences, it searches for the optimal non-linear warp path between the sequences that minimises the matching distance. However, the conventional DTW models assume that the two sequences have the same number of temporal cycles (phases) and are aligned at the starting and end- ing points/elements. These conditions are difficult to be met in person videos from typical surveillance scenes. Hence, directly using DTW variants to holistically match these unregulated videos may be suboptimal. To further compound the problem, there are often unknown occlusions and background clutters that can lead to corrupted video frames with missing and/or noisy observation thus potentially inaccurate distance measurement.</p><p>In case of cyclic sequences, e.g. closed curves, the starting element is often unknown and may be located by a greedy search or some heuristic method <ref type="bibr" target="#b50">[51]</ref>. However, there can exist more than one starting elements for periodic sequences like people walking videos. Whilst continuous dynamic programming or spotting <ref type="bibr" target="#b51">[52]</ref> identifies both starting/ending elements, it requires a good predefined threshold, which however is not available in our person ReID problem.</p><p>Single/multi-shot and video based person ReID. Most existing ReID methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b56">57]</ref> only consider one-shot image per person per view. This is inherently weak when multi-shot are available, due to the intrinsically ambiguous and noisy people appearance and large cross-view appearance variations (Figure <ref type="figure" target="#fig_0">1</ref>). There are efforts on multi-shot ReID. For example, Hamdoun et al. <ref type="bibr" target="#b57">[58]</ref> propose to employ the interest points cumulated across a number of images; Cong et al. <ref type="bibr" target="#b58">[59]</ref> utilise the data manifold geometric structures of multiple images for constructing more compact spatial appearance description. Other attempts include training a robust appearance model using image sets <ref type="bibr" target="#b59">[60]</ref> and enhancing local image region/patch spatial feature representation <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b62">63]</ref>. In contrast to all these methods focusing on exploiting spatial appearance information, this work explores space-time information from available videos for person ReID. Other video based ReID methods <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b68">69]</ref> are also supervised and thus subject to the similar scalability limitation as DVR.</p><p>Space-time visual features. Our person video representation is inspired by existing successful action features and the DVR model <ref type="bibr" target="#b17">[18]</ref>, e.g. histograms of oriented 3D spatio-temporal gradient (HOG3D) <ref type="bibr" target="#b18">[19]</ref>. In contrast to most feature vector based action representations <ref type="bibr" target="#b69">[70,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b75">76]</ref>, we represent person videos with temporal sequences based representations. This design is capable of (1) not only encoding the dynamic temporal structures of motion, <ref type="bibr" target="#b1">(2)</ref> but also selectively matching unregulated person videos (see Section 5). While some action recognition models also regard videos as sequences of observation <ref type="bibr" target="#b76">[77,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr" target="#b80">81]</ref>, their focus is coarse temporal structure modelling alone.</p><p>To extract different granularities of localised temporal ordering dynamics, we adopt the notion of temporal pyramids (see Figure <ref type="figure">4</ref>(b)). Instead of using temporal sub-sampling to construct a temporal pyramid <ref type="bibr" target="#b81">[82,</ref><ref type="bibr" target="#b82">83]</ref>, we segment videos with different sequence-element lengths for preserving all possible dynamic information at all levels as in <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b83">84]</ref>. However, our representation is different significantly from the latter two because: (1) They use vector based representations whilst ours are sequential or temporal series; (2) They assume well segmented videos as input (e.g. one action per video) whilst our person videos can contain a varying numbers of walking action periods without any temporal segmentation; (3) We additionally consider spatial pyramid <ref type="bibr" target="#b19">[20]</ref> at each temporal granularity and importantly data selection in video matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach Overview</head><p>Unlike most action recognition methods that represent each video with a feature vector <ref type="bibr" target="#b10">[11]</ref> or the image-sequence based person re-identification (ReID) approach that describes each video with a set of independent vectors <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>,</p><p>we consider person videos as sequences of localised space-time dynamics for performing ReID. This allows to: ( </p><formula xml:id="formula_0">Q p Q g A probe video</formula><p>A gallery video the-art DVR re-id model, our method is able to extract and employ much richer space-time cues from videos. Moreover, the proposed method is unsupervised, as opposite to DVR which needs a large number of cross-view matching pairs for every camera pair. Therefore, our proposed method is more scalable to the real-world applications involving large surveillance camera networks. Additionally, we further consider information fusion from multiple feature-sequences each capturing some different aspects of person video data. An overview diagram of the proposed approach is presented in Figure <ref type="figure" target="#fig_1">3</ref>.</p><formula xml:id="formula_1">Q p Q g , ,<label>…</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Structured Video Representation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Video Sequentialisation</head><p>Suppose we have a collection of video (or image-sequence) pairs</p><formula xml:id="formula_2">{(Q p i , Q g i )} n i=1</formula><p>, where Q p i and Q g i denote the videos of person i captured by two disjoint cameras p and g, and n the number of people. Each video is defined as a set of consecutive frames I (e.g. obtained by an independent person tracking process <ref type="bibr" target="#b84">[85]</ref> with simple post-processing or not): Q = {I 1 , I 2 , ...}, where the video length |Q| is varying as in typical surveillance settings, independently extracted person videos do not guarantee to have a uniform duration (arbitrary frame number), nor the number of walking cycles and starting/ending phases.</p><p>Given varying-long videos with unknown and random noise, it is ineffective to perform matching between two image-sequences holistically. A possible strategy <ref type="bibr" target="#b17">[18]</ref> is: (1) Segmenting each video into multiple independent fragments; <ref type="bibr" target="#b1">(2)</ref> Selecting the optimal/best fragment pairs for matching. This method, however, may lose potentially useful information encoded in the discarded fragments. In this work, we instead consider a richer representation for exploiting as much space-time information from inherently noisy videos as possible.</p><p>Specifically, we divide uniformly each individual video Q into multiple temporally localised slices with a small number l of image frames. Different slice lengths l correspond to different temporal granularities. Each slice encodes localised space-time information about the walking characteristics of the corresponding person. As a result, a video can be converted into a space-time slice-sequence S = {s 1 , s 2 , . . . } (Figure <ref type="figure">4</ref>). This localised slice-based sequence representation has three advantages over the bag-of-fragments model <ref type="bibr" target="#b16">[17]</ref>: <ref type="bibr" target="#b0">(1)</ref> It keeps the original sequential data form, whilst DVR only considers each fragment of a sequence as an isolated instance without temporal ordering among fragments. This allows us to enjoy the merits of existing sequence matching algorithms, e.g. non-linear dynamic time warping for handling the misalignment problem. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Temporal Pyramid</head><p>Since variations in walking styles may exist over various local temporal extends, it is suboptimal to utilise video slices of a uniform length. Also, fineto-coarse localised temporal information is possible to complement each other in expressing temporal structure dynamics, as demonstrated in existing action recognition studies <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b83">84]</ref>. In light of these considerations, we enrich our representation of person videos by imposing a temporal pyramid structure, motivated by pyramid match kernel <ref type="bibr" target="#b85">[86]</ref> and its spatial extension <ref type="bibr" target="#b19">[20]</ref>.</p><p>Specifically, we use a set of video slice length for video sequentialisation as:</p><formula xml:id="formula_3">L = {2 0 l, . . . , 2 (ht-1) l} (1)</formula><p>which corresponds to a temporal pyramid with h t levels/layers. Given a video Q i , we generate a separate slice-sequence at each temporal pyramid level. Thus, a total of h t slice-sequences {S l i } ht-1 l=0 can be produced for each video Q i after</p><formula xml:id="formula_4">Level-0 Level-1</formula><p>Extracting localised space-time descriptor applying this temporal pyramid (Figure <ref type="figure">4</ref>(c)). During sequentialising a video, at any temporal pyramid level, we discard the last few image frames of person videos if they are not sufficient to form a slice. For example, suppose there is 56 frames in a person video and the slice length is 10, we drop/ignore the last 6 frames as they are not enough for a complete slice of 10 frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Spatial Pyramid</head><p>After obtaining slice-sequences S = {s 1 , . . . , s i , . . . } of person video, we need to consider how to represent their localised video slices s i . This is the same as deriving video representation for action recognition <ref type="bibr" target="#b10">[11]</ref> in that each slice can be considered as a tiny action video. We want to capture localised spatio-temporal dynamic structures of people's walking. Apparently, the style or characteristics of walking motion is closely related to the action of different body parts, e.g. head, torso, arms, legs. Hence, we spatially decompose every slice into a grid of 2×5 uniform cells which approximately correspond to the layout of all body parts (Figure <ref type="figure" target="#fig_5">5</ref>(right)). This division allows to encode roughly detailed spatial cues of individual parts into video slices.</p><p>Additionally, accurate ReID may need more fine-grained and subtle spatially structured cues of people's walking behaviour. This is because finer spatial decomposition provides more detailed information and potentially complements coarse divisions. To that end, we adopt the spatial pyramid match kernel <ref type="bibr" target="#b19">[20]</ref>,</p><p>due to its superior expressive capability shown in action recognition <ref type="bibr" target="#b71">[72]</ref>. In particular, we further split each cell into 2×2 smaller ones, resulting in a grid of 40 cells on each slice (Figure <ref type="figure" target="#fig_5">5</ref>(left)). By repeating this process, we can obtain a h s -level spatial pyramid. Together with temporal pyramid, we call our video representation as "Spatio-Temporal Pyramidal Sequence" (STPS). Next, we describe the dynamic feature descriptor for numerically representing localised space-time cells below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Localised Space-Time Descriptor</head><p>We consider the HOG3D feature <ref type="bibr" target="#b18">[19]</ref> for representing video slices due to its strong expressiveness for recognising different activities <ref type="bibr" target="#b86">[87]</ref> and importantly for distinguishing between distinct people <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>. Particularly, given a specific spatial division on any video slice s, we first extract the space-time gradient histogram from each cell where 3D gradient orientations are quantised using regular polyhedrons <ref type="bibr" target="#b18">[19]</ref>, then concatenate them to form a HOG3D feature vector x for the slice s. Note that there is 50% overlap between any two adjacent cells for increasing robustness against tracking/annotation errors. As such, we obtain a HOG3D feature-sequence X = {x 1 , x 2 , . . . } for a slice-sequence S = {s 1 , s 2 , . . . }. Finally, we apply histogram equalisation for reducing the effect of uneven illuminations. While other space-time descriptors, such as motion boundary histograms (MBH) <ref type="bibr" target="#b87">[88]</ref>, are considerable, it is beyond our scope to exhaustively discuss and evaluate a variety of different space-time descriptors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Unsupervised Video Matching</head><p>In this section, we describe the details of the proposed sequence/video matching model for person ReID. We aim to formulate an unsupervised model. As a result, the expensive cross-camera pairwise labelling process for every camera pair can be eliminated for realising good deployment scalability in reality. To that end, we select the well-known Dynamic Time Warping (DTW) algorithm <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b88">89]</ref> as the basis of our model due to: delay based studies <ref type="bibr" target="#b89">[90,</ref><ref type="bibr" target="#b90">91]</ref>, multi-dimension fusion <ref type="bibr" target="#b91">[92]</ref>, and neural networks (or deep learning) <ref type="bibr" target="#b92">[93]</ref>. This proposed model is characterised with alignment free, data selection, and information fusion. Before detailing our method, let us first briefly describe the conventional DTW model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Conventional DTW</head><p>In general, the DTW model <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b88">89]</ref> aims at measuring the distance or similarity between two temporal-sequences by searching for the optimal nonlinear warp path. Formally, given two feature-sequences X p = {s p 1 , . . . , s p i , . . . } and X g = {s g 1 , . . . , s g j , . . . }, we define a warp path as:</p><formula xml:id="formula_5">W = {w 1 , . . . , w d } (2)</formula><p>where the k-th entry w k = (w p k , w g k ) indicates that the w p k -th element from X p and w g k -th element from X g are matched. The warp path length holds as:</p><formula xml:id="formula_6">max(|X p |, |X g |) ≤ d &lt; |X p | + |X g |. The symbol | • |</formula><p>denotes the set size. We then define the sequence matching distance dist dtw (X p , X g ) between X p and X g as:</p><formula xml:id="formula_7">dist dtw (X p , X g ) = 1 d d k=1 dist el (x p w p k , x g w g k )<label>( 3 )</label></formula><p>with dist el (•, •) as the distance metric between two elements (or slices), e.g. L 1</p><p>or L 2 norm, and d = |W | the warp path length. The objective of DTW is to find the optimal warp path W * such that</p><formula xml:id="formula_8">W * = argmin W ∈Ω dist dtw (X p , X g ) (<label>4</label></formula><formula xml:id="formula_9">)</formula><p>where Ω is the set of all possible warp paths. This optimisation can be realised using dynamic programming <ref type="bibr" target="#b93">[94]</ref> subject to three constraints: space-time slice</p><formula xml:id="formula_10">X g X p Δt = -2 X g (Δt = 3)</formula><p>X p (Δt = 3)</p><formula xml:id="formula_11">Δt = 3 Δt = 9 DTW Time Shift</formula><p>Matching Distance As indicated in the above bounding constraint, DTW assumes that the starting and ending data elements of the two sequences are aligned. However, this is mostly invalid in videos available for person ReID as aforementioned. Moreover, DTW utilises all sequence element data for distance computation, regardless the 345 quality of individual elements. This is likely to make the obtained distance sensitive to data noise often present in typical ReID videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Time Shift Driven Alignment and Selective Matching</head><p>To overcome the above limitations of DTW, we develop a new model, Time Shift Dynamic Time Warping (TS-DTW), by introducing additionally the notions of time shift and max-pooling into sequence matching. Instead of matching two sequences (X p , X g ) holistically at one time as DTW, we perform iterative and partial matching. An illustration of this time shift driven sequence alignment and matching is depicted in Figure <ref type="figure" target="#fig_8">6</ref>. Specifically, given two featuresequences X p (probe) and X g (gallery), we temporally shift one sequence (say X p ) against the other (X g ) from the beginning position (where only the rightmost slice of X p is utilised in matching with the leftmost slice of X g , e.g. Δt = -2 as in Figure <ref type="figure" target="#fig_8">6</ref>), to the ending position (where the rightmost slice of X g is matched with the leftmost slice of X p , e.g. Δt = 9 as in Figure <ref type="figure" target="#fig_8">6</ref>, and black dotted vertical lines indicate several (not all) shift positions attempted during the entire shifting process). At any shift Δt, the alignment between partial segments X p (Δt) and X g (Δt) (highlighted by the corresponding blue and red bounding box in Figure <ref type="figure" target="#fig_8">6</ref>) is performed by the conventional DTW algorithm <ref type="bibr" target="#b88">[89]</ref>. As such, a set of local matching distances D = {dist dtw (X p , X g , Δt)} Δt∈T (indicated as the black hollow circles in Figure <ref type="figure" target="#fig_8">6</ref>) can be generated over all time shifts T . Finally, we obtain the person video matching distance by taking together all local ones as</p><formula xml:id="formula_12">dist ts (X p , X g ) = min Δt∈T {dist dtw (X p , X g , Δt)}<label>(5)</label></formula><p>i.e. selecting the best-matched result. This time shift ensemble model is inspired by the max-pooling layer in neural networks which aim at summarising the responses of neighbouring groups of neurons <ref type="bibr" target="#b92">[93]</ref>. We cope with a similar situation if sequence-element is thought of as neuron and sequence-segment as group of neurons. Critically, the max-pooling operation has data selection capability for guiding the supervised learning of neurons in neural network learning. Whereas our objective is to achieve data selective sequence matching or recognition in an unsupervised way, enjoying similar spirit but with a different learning strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>The data selection capability in our proposed matching algorithm above is significant to accurately matching sequences, especially for unregulated ReID videos from uncontrolled camera viewing conditions. We summarise the key points for data selection below. First, we automatically select the starting/ending walking poses, in contrast to DTW which enforces the first and last elements of compared sequences to be aligned so potentially introduces weak or noisy alignments into distance computation. Moreover, we attempt many different partial segments of X p and X g , and select the best-aligned parts for distance estimation, different from DTW that uses all observed data regardless of how good the constituent elements are. Thus, noisy elements can be possibly suppressed in distance computation. These two abilities are achieved by successively varying Δt, since the element data of X p (Δt) and X g (Δt) changes over time shifts. Apparently, the two benefits are complementary to each other and their combination allows us to more accurately match incomplete and noisy surveillance videos for person ReID in an unsupervised manner, as demonstrated by our experimental evaluations in Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Generalisation to the Multi-Dimensional Setting</head><p>The TS-DTW model presented in Section 5.2 assumes one feature-sequence per person video. This is the single-dimensional setting, a special case of the multi-dimensional setting, e.g. ≥2 feature-sequences per video <ref type="bibr" target="#b91">[92]</ref>. The term "dimension" here can be understood as a specific way of extracting featuresequence from videos. Our setting is multi-dimensional (Figure <ref type="figure">4</ref>). Specifically, defining a dimension in our context is related to one of the two aspects: (i) temporal pyramid (h t levels); and (ii) spatial pyramid (h s levels); Thus, we have a total of h t ×h s dimensions (feature extraction ways). Note that, two feature-sequences at different dimensions for the same video may have different lengths, e.g. those extracted at different temporal pyramid levels (Section 4.2).</p><p>Generally, there are two strategies to combine information from multiple dimensions of sequences: (1) dependent, and (2) independent. We will generalise our TS-DTW model to the multi-dimensional setting using both strategies as detailed below.</p><p>(I) Dependent fusion. The dependent fusion strategy assumes that: <ref type="bibr" target="#b0">(1)</ref> feature-sequences of a given video at different dimensions have the same length;</p><p>(2) different dimensions are strongly correlated one another, i.e. their warping paths should be identical. Due to condition (1), we can not perform fusion of multiple dimensions across different temporal pyramid levels with this strategy.</p><p>Consequently, we can only combine the h s dimensions from different spatial divisions within each individual temporal pyramid level, those extracted from the same slice-sequence.</p><p>Formally, when matching two slice-sequences of the same temporal pyramid level: S p = {s p 1 , . . . , s p i , . . . } from video Q p , and S g = {s g 1 , . . . , s g j , . . . } from video Q g , we perform a joint sequence alignment by using the feature data of all dimensions to compute the distance between two elements s p i and s g j as</p><formula xml:id="formula_13">dist D el (s p i , s g j ) = κ k=1 α k × dist el (x p (i,k) , x g (j,k) )<label>( 6 )</label></formula><p>where x p (i,k) and x g (j,k) are the feature data in the k-th dimension for s p i and s g j , respectively, κ is the total number of dimensions to be fused, and α k defines the weight of the k-th dimension. To incorporate the fine-to-coarse spatial information encoded in walking motion, we relate the value of α k to the structure of spatial pyramid by setting</p><formula xml:id="formula_14">α k = 2 ε k (7)</formula><p>where ε k ∈ [0, 1, . . . , h s -1] denotes the spatial pyramid level of the k-th dimension (see Figure <ref type="figure" target="#fig_5">5</ref>). This design is similar in spirit to pyramid kernel matching <ref type="bibr" target="#b85">[86]</ref>. All fused dimensions are at the same level of the temporal pyramid whose structure is thus not considered here.</p><p>By replacing the single-dimensional distance dist el (•, •) of DTW with Eqn. (II) Independent fusion. In contrast to the dependent fusion policy, the independent counterpart assumes independent alignment behaviours among individual dimensions by performing information combination in the distance level.</p><p>Importantly, this strategy is more flexible than the former as it allows each dimension having their respective sequence structure, e.g. the sequence length.</p><p>Therefore, sequences across different temporal pyramid levels can be combined in this fusion way. Similarly, we further take into account temporal fine-tocoarse structures and combine all dimensions to generate the final matching sequence distance between two videos Q p and</p><formula xml:id="formula_15">Q g via dist I (Q p , Q g ) = κ k=1 β k × α k × dist k (Q p , Q g ) (<label>8</label></formula><formula xml:id="formula_16">)</formula><p>where</p><formula xml:id="formula_17">β k = 2 τ k , τ k ∈ {0, 1, . . . , h t -1}</formula><p>is the temporal pyramid level of the k-th dimension (see Figure <ref type="figure">4</ref>), and dist k (Q p , Q g ) the corresponding matching distance using our TS-DTW model, i.e. Eqn. <ref type="bibr" target="#b4">(5)</ref>. The parameters κ and α k are same as in Eqns. ( <ref type="formula" target="#formula_13">6</ref>) and <ref type="bibr" target="#b6">(7)</ref>. We call this model "MDTS-DTW I "</p><p>Usually, the two fusion strategies yield different matching results over the same dimensions. This is because each dimension may capture different aspects of video data and produce non-identical alignment solutions, and thus result in different distance values. We will evaluate and discuss their performances for person ReID in Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Model Complexity</head><p>We analyse the video matching complexity of our TS-DTW model. Formally, given two feature-sequences X p and X g , we need to compute the matching distance between X p (Δt) and X g (Δt) with the time shift Δt ∈ T =</p><formula xml:id="formula_18">{-|X p | + 1, . . . , |X p | + |X g | -1}. |X p (Δt)| (or |X g (Δt)|) lies in the range of [1, min(|X p |, |X g |)]</formula><p>(see Figure <ref type="figure" target="#fig_8">6</ref>). Therefore, the total matching complexity ψ tsdtw of our TS-DTW model is</p><formula xml:id="formula_19">ψ tsdtw = Δt∈T ψ dtw (|X p (Δt)|)<label>( 9 )</label></formula><p>where ψ dtw (|X p (Δt)|) refers to the matching complexity of DTW, which is</p><formula xml:id="formula_20">O(|X p (Δt)| 2</formula><p>) by the standard DTW model <ref type="bibr" target="#b88">[89]</ref> and O(|X p (Δt)|) by fast variants <ref type="bibr" target="#b94">[95]</ref>. As person feature-sequences are typically short (e.g. &lt;25 on PRID2011 and &lt;40 on iLIDS-VID), the entire matching process is still efficient. Moreover, we can parallelise easily the matching process over individual time shifts for further reducing the running time, as they are independent against each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Person Re-Identification</head><p>Given a probe person video Q p ∈ P and a gallery set G = {Q g i } captured from two non-overlapping cameras, person ReID aims to find the true identity match of Q p in G. To achieve this, we first compute the space-time feature based distance dist st (Q p , Q g i ) between Q p and every gallery video Q g i with our TS-DTW (Eqn. ( <ref type="formula" target="#formula_12">5</ref>)) or MDTS-DTW D (Eqn. ( <ref type="formula" target="#formula_13">6</ref>)) or MDTS-DTW I (Eqn. ( <ref type="formula" target="#formula_15">8</ref>)) model. In this way, we can obtain all cross-camera pairwise video matching</p><formula xml:id="formula_21">distances {dist st (Q p , Q g i )} |G| i=1</formula><p>. Finally, we generate a ranked list of all the gallery people in ascendant order of their matching distances, where the rank-1 gallery video is considered to be the most likely true match of Q p .</p><p>Combination with the spatial appearance methods. The ReID matching distances computed by the proposed model can be readily fused with those by other spatial appearance models. In particular, we incorporate our results</p><formula xml:id="formula_22">dist st (Q p , Q g i ) into other appearance based distance measures {dist sp k } as dist fused (Q p , Q g i ) = dist st (Q p , Q g i ) + k c k × dist sp k (Q p , Q g i ) (<label>10</label></formula><formula xml:id="formula_23">)</formula><p>where c i is a weighting assigned to the k-th method. Instead of cross-validation, we simply set c k = 1 for generality consideration since in practice it is not always valid to assume the availability of pairwise labelled data which is required by cross-validation. As matching distances by distinct methods may lie in different ranges, we normalise all per-probe pairwise distances dist </p><formula xml:id="formula_24">st (Q p , Q g i )/dist sp k (Q p , Q g i ) to [0,</formula><formula xml:id="formula_25">(Q p , Q g i )} |G| i=1 with respect to a probe Q p as dist * (Q p , Q g i ) = dist * (Q p , Q g i ) max({dist * (Q p , Q g i )} |G| i=1 )<label>(11)</label></formula><p>where max(•) returns the maximal value of a set. Then, the final fused distance can be expressed as</p><formula xml:id="formula_26">dist fused (Q p , Q g i ) = dist st (Q p , Q g i ) + k dist sp k (Q p , Q g i )<label>(12)</label></formula><p>We will evaluate the complementary effect between space-time and appearance 430 features based person ReID methods in Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Experimental Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Datasets</head><p>Two benchmark image sequence based person ReID datasets (PRID2011 <ref type="bibr" target="#b21">[22]</ref> and iLIDS-VID <ref type="bibr" target="#b16">[17]</ref>) were utilised for evaluating the performance of the pro- 1. PRID2011. The PRID2011 dataset <ref type="bibr" target="#b21">[22]</ref> includes 400 image sequences captured from 200 different people under two disjoint outdoor camera views. Each image sequence contains 5 to 675 image frames<ref type="foot" target="#foot_0">5</ref> (Figure <ref type="figure" target="#fig_10">7a</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">iLIDS-VID.</head><p>The iLIDS-VID dataset <ref type="bibr" target="#b16">[17]</ref> contains a total of 600 image sequences from 300 randomly sampled people, each with one pair of image sequences from two indoor camera views. Every image sequence has a variable length, e.g. consisting of 22 to 192 image frames (Figure <ref type="figure" target="#fig_10">7b</ref>).</p><p>Compared with PRID2011, this dataset has more complex occlusion and background clutter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Baseline Methods</head><p>We compared our method with related state-of-the-art methods as follows:</p><p>1. GEI-RSVM <ref type="bibr" target="#b23">[24]</ref>: A state-of-the-art gait recognition model using Gait Energy Image (GEI) feature <ref type="bibr" target="#b32">[33]</ref> and the ranking SVM <ref type="bibr" target="#b95">[96]</ref> model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">DTW [89]:</head><p>The widely used sequence matching algorithm -Dynamic Time Warping. DTW measures the distance between two sequences based on the optimal non-linear warping of elements across sequences. 4. WDTW <ref type="bibr" target="#b49">[50]</ref>: The weighted form of DTW model that also takes into account the shape similarity between two sequences. Specifically, WDTW introduces a multiplicative weight penalty on the warping distance between elements during distance estimation. This may suppress the negative influence of some outlier elements that are far away in element index but happen to be well matched. This model usually prefers close warping. We utilised a logistic weight function of the warping index-difference abs(w p kw q k ) as:</p><formula xml:id="formula_27">f (w p k , w q k ) = 1 1+exp(-(abs(w p k -w q k )-μ)/2)</formula><p>, where μ is the half average-length of two sequences Q p and Q g ; w p k and w q k are the corresponding aligned element index of the k-th warp path entry (Eqn. ( <ref type="formula">2</ref>)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">SDALF [3]:</head><p>A classic hand-crafted visual appearance ReID feature. Both single and multiple shot cases are considered.</p><p>6. eSDC <ref type="bibr" target="#b5">[6]</ref>: A state-of-the-art unsupervised spatial appearance based ReID method, which is able to learn localised appearance saliency statistics for measuring local patch importance.</p><p>7. Iterative Sparse Ranking (ISR) <ref type="bibr" target="#b96">[97]</ref>: A contemporary weighted dictionary learning based algorithm that iteratively extends sparse discriminative classifiers in a transductive learning manner.</p><p>8. Regularised Dictionary Learning (RDL) <ref type="bibr" target="#b25">[26]</ref>: The most recent dictionary learning based unsupervised ReID model. It iteratively learns the dictionary with the regularisation term updated in each iteration so that the cross-view noisy correspondence can be improved gradually. <ref type="bibr" target="#b4">[5]</ref>: A ranking SVM model <ref type="bibr" target="#b95">[96]</ref> based ReID method with one of the most effective features Colour&amp;LBP <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">SS-ColLBP</head><p>10. MS-ColLBP <ref type="bibr" target="#b16">[17]</ref>: A multi-shot extension of SS-ColLBP. Specifically, the averaged Colour&amp;LBP feature <ref type="bibr" target="#b4">[5]</ref> over all image frames of a video is used to represent the spatial appearance of the person.</p><p>11. L 1 /L 2 -norm: The basic common distance metrics that can be very competitive with other complex metrics in many cases <ref type="bibr" target="#b97">[98]</ref>. For matching two sequences, we remove the tail part of the longer one to make the two sequences have an equal duration.</p><p>12. Kernelised Cross-View Discriminant Component Analysis (KCVDCA) <ref type="bibr" target="#b98">[99]</ref>:</p><p>A competitive asymmetric distance learning method capable of inducing camera-specific projections for transforming unmatched visual features from different camera views to a shared subspace wherein discriminative features can be then learned and extracted.</p><p>13. Cross-View Quadratic Discriminant Analysis (XQDA) <ref type="bibr" target="#b26">[27]</ref>: A state-ofthe-art static appearance feature based supervised person ReID approach.</p><p>Specifically, the XQDA algorithm learns simultaneously a discriminant low dimensional subspace and a QDA metric on the derived subspace.</p><p>14. DVR <ref type="bibr" target="#b17">[18]</ref>: The state-of-the-art image-sequence based person ReID model which achieves the most competitive performance. In particular, this supervised model is characterised by discriminative fragment selection and exploitation for learning an effective space-time ranking function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Person ReID Scenarios</head><p>We evaluated two person ReID scenarios, closed-world and open-world:</p><p>1. Closed-World ReID: In this setting, all probe people are assumed to exist in the gallery. In evaluations, we followed the data partition setting as <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>. Specifically, for either PRID2011 or iLIDS-VID, we split the entire dataset into two partitions: one half for training, and the other half for testing. Note that our model does not utilise the training partition since it is unsupervised.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Open-World ReID:</head><p>In addition, we evaluated a more realistic scenario called open-world ReID <ref type="bibr" target="#b22">[23]</ref>. Specifically, its key difference from the closed-world case is that a probe person i ∈ P is not assumed to appear necessarily in the gallery G under the open-world setting. This situation is more plausible to real-world ReID applications since we generally have no prior knowledge about whether one person (in gallery) re-appears in certain (probe) camera views in most applications, e.g. due to the complex topology structure of camera networks. That is, P and G may be just partially overlapped in different camera views. Similar data partitions as the closed-world case were utilised, with the only difference that the gallery set of the testing partition is reduced by one third ( <ref type="formula">1</ref>3 ) of randomly selected people (they are considered as imposters, only appearing in the probe set), i.e. 60 gallery people on PRID2011 and 100 on iLIDS-VID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.">Evaluation Metrics</head><p>For closed-world ReID, the conventional Cumulated Matching Characteristics (CMC) curves were utilised for a quantitative performance comparison between different methods <ref type="bibr" target="#b0">[1]</ref>. For open-world ReID, two separate steps are involved in performance evaluation under the open-world setting <ref type="bibr" target="#b22">[23]</ref>: (1) Detection -decide if a probe person Q p ∈ P exists in the gallery or not; For convenience, we define P = P \ G, the probe people that are not included in the gallery G. (2) Identification -compute the truly matched rates over only accepted target people. Specifically, we utilised detection and identification rate (DIR) and false accept rate (FAR) defined as:</p><formula xml:id="formula_28">DIR(τ, k) = |{Q p | Qg ∈ G, rank(Q p ) ≤ k, dist( Qg , Q p ) ≤ τ }| |G| (13) FAR(τ ) = |{Q p |Q p ∈ P , minQg∈G dist(Q g , Q p ) ≤ τ }| | P | (<label>14</label></formula><formula xml:id="formula_29">)</formula><p>where dist(•, •) refers to the cross-view distance score induced by some person ReID model, Qg the gallery person having the same identity (i.e. true match)</p><p>as the probe person Q p , and τ the decision threshold. rank( Qg ) = k means that the true match Qg is ranked at k in the ranking list. Thus, given a rank k, a</p><p>Receiver Operating Characteristic (ROC) curve can be obtained by varying τ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5.">Implementation Details</head><p>Since video slices are localised over time, the value of l (the shortest slice length) should be small and related to the walking cycle length. We fixed l = 5 in that the process of a walking step takes around 2l = 10 frames. Whilst the size h t of the temporal pyramid largely depends on video length, e.g. an over-large h t may lead to discarding many frames during sequentialisation (thus causing potentially much information loss), or very few slices produced for videos (with little temporal ordering dynamics). Thus, h t is set to 2 accordingly. We utilised a 2-level spatial pyramid, i.e. h s = 2. This is because, our empirical experiments suggest that the addition of one more spatial pyramid level slightly degrades the model performance possibly due to the local patch misalignment problem in over fine-grained spatial decomposition. The distance metric between sequence</p><formula xml:id="formula_30">elements dist el (•, •) is set as L 1 .</formula><p>For obtaining stable statistics, we evaluated both person ReID scenarios with 10 folds of experiments with different random training/testing partitions on each dataset, and reported the averaged results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.">Evaluation on Our Proposed Approach</head><p>We evaluated the detailed aspects of the proposed video representation and sequence matching models for person ReID in the common closed-world scenario, i.e. the ReID accuracies of our TS-DTW and MDTS-DTW models using different parts of the proposed STPS features. The results are reported in Table <ref type="table" target="#tab_3">1</ref>. It is evident that both temporal and spatial pyramids are effective for person ReID and their fusion with the proposed method can improve significantly the matching accuracy. This is consistent with the finding in scene and action recognition <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. Specifically, given either of the two temporal pyramid levels, when comparing with the coarse spatial pyramid level (SPL-1), the fine-grained spatial division  That is, the average matching time for two person sequences is around 0.06 second. Note that, the whole process above can be conducted in parallel over a cluster of machines to further speed up model deployment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.">Evaluation on Closed-World Person ReID</head><p>In this conventional setting, we performed comparative evaluations with gait recognition, temporal sequence matching, and person ReID approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.1.">Comparing Gait Recognition and Temporal Sequence Matching Methods</head><p>In Table <ref type="table" target="#tab_5">2</ref>, we compared our MDTS-DTW I model with a number of stateof-the-art gait recognition and dynamic programming based sequence matching methods. It is evident that the proposed model outperforms both alternative strategies by a large margin on each dataset. Specifically, the gait recognition method produces much better ReID accuracy on PRID2011 than on iLIDS-VID. This is because, the image sequences from the latter contain more background noise such as clutter and occlusion which can contaminate the gait feature heavily (see Figure <ref type="figure" target="#fig_2">2</ref>). By automatically aligning starting/ending walking phases and selecting best-matched sequence parts, our TS-DTW model allows to better overcome this challenge. On the other hand, conventional temporal sequence matching algorithms, e.g. DTW and its variants, can only provide much weaker results than the proposed MDTS-DTW. This is largely owing to: (1) ReID image sequences have different lengths with arbitrary starting/ending phases, and incomplete/noisy frames. Hence, attempts to match and utilise entire sequences inevitably suffer from mismatching with erroneous similarity measurement; <ref type="bibr" target="#b1">(2)</ref> there is no explicit mechanism to avoid incomplete/missing data, typical in crowded surveillance scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.2.">Comparing Person ReID Methods</head><p>We compared our MDTS-DTW I method with contemporary unsupervised and supervised ReID methods, and further evaluated the complementary effect between appearance and space-time feature based approaches. Comparing unsupervised methods. Table <ref type="table" target="#tab_7">3</ref> shows the comparison among unsupervised ReID approaches. The proposed MDTS-DTW I outperforms significantly all competitors on PRID2011 and iLIDS-VID. Specifically, space-time feature based methods (e.g. ours and L 1 /L 2 -norm) produce better ReID accuracies than the remaining spatial appearance based methods, particularly on the more challenging iLIDS-VID dataset. This suggests the inherent challenge caused by the ambiguous and unreliable nature of people's appearance in person ReID applications, and simultaneously the exceptional effectiveness of space-time cues for people matching when expressed and exploited effectively.</p><p>In addition, the weak performance by SDALF is largely because of the intrinsic difficulty in designing general identity-discriminative hand-crafted appearance feature given unknown cross-camera covariates. Through iteratively learning and extending discriminative classifiers in ISR or modelling localised saliency statistics in eSDC or exploiting iteratively cross-view soft-correspondence in RDL, person ReID performance is greatly improved. However, due to relying on static appearance information alone, they are inherently sensitive to cross-camera viewing conditions, e.g. with a severe perform degradation from PRID2011 to iLIDS-VID. In contrast, our method mitigates this challenge by properly designing and effectively exploiting dynamic space-time features, another information source which presents better stability than the widely-used appearance features. Comparing supervised methods. We present the comparison between our unsupervised MDTS-DTW I and previous supervised methods in Table <ref type="table" target="#tab_9">4</ref>. It is found that space-time feature based methods (i.e. DVR &amp; ours) are less sensitive to crowded background than other appearance feature based models particularly XQDA and KCVDCA, when comparing the ReID performance on PRID and iLIDS-VID (more busy and crowded, see Figure <ref type="figure" target="#fig_10">7</ref>). This is partially attributed to the selective matching strategy in the former models for extracting more reliable space-time representations. Moreover, it is observed that our method surpasses appearance based SS-/MS-ColLBP on two datasets and XQDA/KCVDCA on iLIDS-VID, and produces competitive results as video based DVR. Note that the DVR model exploits both space-time and colour information in the price of exhaustive pairwise labelling whilst our MDTS-DTW I method only utilises dynamic space-time cues without the need for cross-view pairwise labelling. These comparisons demonstrate the advantage and capability of our STPS video representation and selective matching model in extracting and exploiting identity-discriminative space-time information from noisy person videos for relaxing the label availability assumption and making better use of unregulated video data. Evaluating complementary effect. We further evaluated how well spatial appearance and space-time feature based ReID methods complement each other. To this end, we integrated contemporary unsupervised (eSDC, ISR and RDL) and supervised (MS-ColLBP, KCVDCA and XQDA) appearance based approaches with DVR and our MDTS-DTW I model (Eqn. ( <ref type="formula" target="#formula_22">10</ref>)), respectively.</p><p>The results are presented in Table <ref type="table" target="#tab_11">5</ref>. It is observed that by fusing space-time feature based ReID results of either DVR or ours, the matching accuracies of existing appearance based methods can be significantly boosted. This confirms the similar finding by <ref type="bibr" target="#b17">[18]</ref> that, the combination of appearance and space-time motion information sources can be very effective for person ReID as they are largely independent in nature. Overall, XQDA+DVR achieves the best performance on PRID2011 whilst KCVDCA+Ours and KCVDCA+DVR perform similarly best on iLIDS-VID. This is as expected because the combination with DVR doubly benefits much from effective modelling on labelled data which contain strong discriminative information but very expensive to acquire for every camera pair in reality. Once removing the label availability assumption, the best results are obtained by eSDC+Ours on iLIDS-VID and RDL+Ours on PRID2011. Under the unsupervised setting, we observed a similar complementary effect as XQDA/KCVDCA+DVR/Ours. This validates the efficacy of our ReID method in deriving dynamic identity information from unregulated videos, independent of and completing well the commonly used spatial appearance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3.">Evaluation on Open-World Person ReID</head><p>In this section, we evaluated the open-world ReID problem, a more practical scenario compared to the above closed-world setting. Different single ReID methods and their combinations were assessed and reported in Table <ref type="table" target="#tab_12">6</ref>. The performance evaluation metric is Detection and Identification Rate (DIR, Eqn.</p><p>(13)) with k = 1 (e.g. Rank-1) at given False Accept Rates (FAR, Eqn. ( <ref type="formula" target="#formula_28">14</ref>)).</p><p>For the performance of single models, largely similar situations are found as in the closed-world case. Particularly, for iLIDS-VID, the supervised spacetime ReID method DVR obtains the best results followed by our approach and KCVDCA but ours is unsupervised. On PRID2011, our method has the best DIR scores given low (≤ 10%) FAR rates (corresponding to small τ in Eqn. ( <ref type="formula" target="#formula_28">14</ref>)). That means, our method can recognise more accurately the true match at rank-1 when the false accept rate is required to be small. This situation is mostly ignored in the current ReID literature but very important in real-world applications, particularly when a large number of probe people are given and high FARs are not acceptable.</p><p>When fusing appearance and space-time feature based ReID methods, the recognition scores across all FARs are greatly improved, similar to the early observations. In particular, the best ReID accuracies are obtained by the combination of XQDA/KCVDCA and DVR/Ours, assuming truth match labels are accessible. In the unsupervised setting, RDL+Ours is the best on both </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The challenges of person re-identification in visual surveillance [2]. (a) The appearance of the same person may change significantly across disjoint camera views due to great cross-camera variations in illumination, viewpoint, random inter-object occlusion and complex background clutter in typically-crowded public spaces. Each blue bounding box corresponds to a specific person. (b) Different people may present largely similar visual appearance. different action categories but tolerate the variance of the same action performed by different people. In contrast, the objective of ReID is to discriminate among different person identities regardless of actions by the person. Moreover, action recognition methods often consider a pre-defined set of action categories during both training/testing phases, whereas person ReID models are required to generalise from the training categories (identities) to previously unseen ones.</figDesc><graphic coords="4,159.23,124.49,75.37,74.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>3 .</head><label>3</label><figDesc>We introduce an effective video matching algorithm, Time Shift Dynamic Time Warping (TS-DTW) and its Multi-Dimensional variant MDTS-DTW, for data selective based sequence matching. Particularly, the proposed model computes the distance between two videos by iteratively (1) altering their mutual time shift relation and (2) then matching two partial segments of them. Importantly, our method is capable of simultaneously performing sequence alignment, selecting best-matched segments, and fusing diverse information for person ReID in a unified manner.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example GEI features of PRID2011 [22] (top) and iLIDS-VID [17] (bottom) videos.</figDesc><graphic coords="8,137.39,207.05,337.46,76.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Overview of the proposed unsupervised video matching approach for person ReID. (a) An input pair of person videos; (b) Construct video representation by video sequentialisation (Section 4.1), temporal pyramid (Section 4.2), spatial pyramid (Section 4.3), and localised space-time descriptor computation (Section 4.4); (c) Obtained feature-sequences; (d) Video matching by the proposed TS-DTW (Section 5.2) and MDTS-DTW (Section 5.3) models. novel unsupervised person re-identification method capable of extracting multiscale spatio-temporal structure information (Section 4), automatically aligning sequence pairs and adaptively selecting/employing informative visual data (Section 5) from noisy person videos captured in non-overlapping camera views. This allows to relax the stringent assumptions of existing gait recognition methods and overcome the limitations of previous temporal sequence matching models, and result in more accurate person recognition, particularly with incomplete and noisy person videos captured in public spaces. Compared with the state-of-</figDesc><graphic coords="11,191.27,124.49,87.50,106.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>( 2 )Figure 4 :</head><label>24</label><figDesc>Figure 4: Illustration of temporal pyramid and video sequentialisation. Note the colour-coded correspondence between (b) the temporal pyramid level and (c) the slice-sequence. number of short localised slices corresponding to various walking phases. In contrast, the bag-of-fragments strategy may suffer from fragilely aligned fragment pairs at times when only a small number of fragments are available from a video and the starting/ending phases of fragments are not sufficiently diverse to match. (3) It provides more flexible opportunities for selecting and exploring informative localised space-time information irregularly distributed across the original image-sequences, e.g. not only in the form of isolated fragments. This is difficult for the bag-of-fragments representation in DVR due to its hard video fragmentation and coarse fragment selection mechanism.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Spatial pyramid structures on a temporally-localised video slice.</figDesc><graphic coords="14,208.19,125.45,61.38,112.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>( 1 )</head><label>1</label><figDesc>Its great success and popularity in sequence based data analysis; (2) Its simple but elegant modelling. Specifically, we derive a new sequence matching algorithm based on the DTW model, called Time Shift Dynamic Time Warping (TS-DTW), and further generalise TS-DTW to the multi-dimensional setting, i.e. with multiple featuresequences per person video. This formulation is motivated by works in time</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(1) bounding constraint: w 1 = (1, 1) and w d = (|X p |, |X g |); (2) monotonicity constraint: w p 1 ≤ w p 2 ≤ ... ≤ w p d and w g 1 ≤ w g 2 ≤ ... ≤ w g d ; and (3) step-size constraint: w k+1w k ∈ (1, 0), (0, 1), (1, 1) for k ∈ [1 : d -1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Overview of our proposed time shift driven sequence alignment and matching.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>400( 6 )</head><label>6</label><figDesc>, our TS-DTW model can be readily generalised to the multi-dimensional scenario and performs dimension fusion dependently. We call this dependently generalised model "MDTS-DTW D ".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7:Example person videos from the (a) PRID2011<ref type="bibr" target="#b21">[22]</ref> and (b) iLIDS-VID<ref type="bibr" target="#b16">[17]</ref> datasets. In each dataset, every blue bounding box contains two videos from the same person captured by two non-overlapping camera views.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>3 .</head><label>3</label><figDesc>DDTW<ref type="bibr" target="#b48">[49]</ref>: In contrast to DTW directly comparing feature values of elements that can be sensitive to diverse variations, DDTW considers the global shape of sequences by matching the first derivative of the original sequences. Besides, DDTW allows to avoid singularities, i.e. a single element of one sequence may map with a large partition of another sequence, which may lead to pathological measures<ref type="bibr" target="#b47">[48]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>(</head><label></label><figDesc>SPL-0) produces similar result on PRID2011, but significantly better accuracy on the more challenging iLIDS-VID. In contrast, with the same SPL, two temporal pyramid levels (TPL-0 and TPL-1) produce similar results. The plausible reason is that larger spatial regions are more likely to be contaminated by random noise in a crowded public space. When combining the matching results from different dimensions/feature-sequences of the same temporal pyramid level by either MDTS-DTW D or MDTS-DTW I , the ReID accuracy can be improved similarly on both datasets. This suggests largely the independence property among distinct sequence dimensions, i.e. modelling their dependence does not bring any benefit in enhancing ReID. Moreover, after the results from different temporal granularities are fused by MDTS-DTW I , ReID accuracies are further increased (note, MDTS-DTW D is not able to fuse image sequences of different lengths, see Section 5.3). These evidences show good complementary effect of different spatio-temporal pyramid levels and effectiveness of our model in fusing information from multiple localised motion patterns with different space-time extends. In the remaining evaluations, we utilised our MDTS-DTW I model and the full STPS video representation for comparison with the baseline methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>PRID201 9 .</head><label>9</label><figDesc>and iLIDS-VID. Clearly, most findings in the closed-world scenario can be reflected in the open-world setting, whilst some new different observations emerge especially under strict false accept rate conditions. In general, all comparisons above extensively validate the advantages and effectiveness of the proposed video representation and selective matching models for person ReID. Conclusion and Future Work Conclusion. In this work, we presented a video matching based person ReID framework. This is achieved by (1) developing an effective spatio-temporal pyramids based video representation, called Spatio-Temporal Pyramid Sequence (STPS), for encoding more effective and complete space-time information available in person video data; and (2) formulating a novel Time Shift Dynamic Time Warping (TS-DTW) model and its Multi-Dimensional extension named MDTS-DTW for selective matching between pairs of inherently incomplete and noisy image sequences from two disjoint camera views. Our method also shows significant complementary effect on previous spatial appearance based ReID approaches for obtaining favourable ReID accuracies. Importantly, our model is unsupervised and does not require exhaustive cross-view pairwise data annotation for every camera pair in model building. Under both the closed-world and open-world ReID scenarios, extensive comparative evaluations have demonstrated clearly the advantages of the proposed approach over a wide range of contemporary state-of-the-art gait recognition, temporal sequence matching, supervised and unsupervised ReID methods. Future work. Our future work for the unsolved person ReID problem includes: (1) How to introduce other complementary schemes beyond time shift based data selection for further suppressing noisy observations caused by background distractions; (2) How to exploit effectively extra types of information (e.g. semantic text from human or correlated sources) as computing constraints for improving the matching performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>1] per method separately before performing fusion. Specifically, given any matching distance dist</figDesc><table /><note><p>* ∈ {dist st , dist sp 1 , . . . , dist sp k , . . . }, we rescale all distances {dist *</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>The closed-world person ReID performance of the proposed TS-DTW (single-</figDesc><table><row><cell cols="9">dimensional) and MDTS-DTW (multi-dimensional) model with different parts of our STPS</cell></row><row><cell cols="9">video representation. (TPL: Temporal Pyramid Level; SPL: Spatial Pyramid Level)</cell></row><row><cell>Dataset</cell><cell></cell><cell cols="2">PRID2011 [22]</cell><cell></cell><cell></cell><cell cols="2">iLIDS-VID [17]</cell><cell></cell></row><row><cell>Rank R (%)</cell><cell>1</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>1</cell><cell>5</cell><cell>10</cell><cell>20</cell></row><row><cell cols="9">TS-DTW(TPL 0 ,SPL 0 ) 36.7 59.1 73.5 84.7 23.3 51.5 65.2 79.6</cell></row><row><cell cols="9">TS-DTW(TPL 0 ,SPL 1 ) 32.5 63.8 75.4 84.9 12.3 37.0 53.2 68.5</cell></row><row><cell cols="9">MDTS-DTW D (TPL 0 ) 37.1 60.2 73.7 85.7 25.1 51.9 66.5 79.9</cell></row><row><cell>MDTS-DTW I (TPL 0 )</cell><cell cols="8">39.2 60.8 75.3 86.6 25.9 52.7 67.1 79.1</cell></row><row><cell cols="9">TS-DTW(TPL 1 ,SPL 0 ) 34.2 58.9 74.4 86.1 23.8 49.5 62.7 78.4</cell></row><row><cell cols="9">TS-DTW(TPL 1 ,SPL 1 ) 32.4 61.7 77.0 87.2 16.5 40.7 53.4 68.7</cell></row><row><cell cols="9">MDTS-DTW D (TPL 1 ) 36.2 60.3 74.8 86.3 23.8 50.0 62.5 78.6</cell></row><row><cell>MDTS-DTW I (TPL 1 )</cell><cell cols="8">37.2 61.7 75.2 87.0 24.3 50.1 62.4 78.5</cell></row><row><cell>MDTS-DTW I (full)</cell><cell>41.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>67.1 79.4 90.1 31.5 62.1 72.8 82.4</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Comparing gait recognition and sequence matching methods (closed-world scenario).</figDesc><table><row><cell>Dataset</cell><cell></cell><cell cols="2">PRID2011 [22]</cell><cell></cell><cell></cell><cell cols="2">iLIDS-VID [17]</cell><cell></cell></row><row><cell>Rank R (%)</cell><cell>1</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>1</cell><cell>5</cell><cell>10</cell><cell>20</cell></row><row><cell cols="5">GEI-RSVM [24] 20.9 45.5 58.3 70.9</cell><cell>2.8</cell><cell cols="3">13.1 21.3 34.5</cell></row><row><cell>DTW [89]</cell><cell cols="8">19.9 41.2 53.6 65.8 15.9 32.1 41.5 55.5</cell></row><row><cell>DDTW [49]</cell><cell>5.4</cell><cell cols="3">18.2 27.5 38.5</cell><cell>2.9</cell><cell cols="3">10.1 18.1 31.5</cell></row><row><cell>WDTW [50]</cell><cell>4.2</cell><cell cols="3">13.7 20.9 29.4</cell><cell>5.1</cell><cell cols="3">11.5 16.0 23.9</cell></row><row><cell>MDTS-DTW</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>I 41.7 67.1 79.4 90.1 31.5 62.1 72.8 82.4</head><label></label><figDesc>Computational cost: Apart from person re-id accuracy, we also evaluated the computational cost of our MDTS-DTW I model on matching cross-view person videos for ReID. Time was measured on a work station (Intel i7-4770K CPU at 3.50 GHz and memory of 16 GB) with Matlab implementation in Windows OS. Time analysis was conducted under the same experimental setting as above. On average, matching each probe video against the gallery set takes 5.26 seconds on PRID (89 gallery people) and 9.50 seconds on iLIDS-VID (150 gallery people).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Comparing unsupervised person ReID methods (closed-world scenario).</figDesc><table><row><cell>Dataset</cell><cell></cell><cell cols="2">PRID2011 [22]</cell><cell></cell><cell></cell><cell cols="2">iLIDS-VID [17]</cell><cell></cell></row><row><cell>Rank R (%)</cell><cell>1</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>1</cell><cell>5</cell><cell>10</cell><cell>20</cell></row><row><cell>L 1 -norm</cell><cell cols="8">26.4 47.5 57.8 73.7 19.3 39.2 51.9 66.5</cell></row><row><cell>L 2 -norm</cell><cell cols="8">23.3 46.7 57.5 73.6 15.6 37.7 49.0 63.1</cell></row><row><cell>SS-SDALF [3]</cell><cell>4.9</cell><cell cols="3">21.5 30.9 45.2</cell><cell>5.1</cell><cell cols="3">14.9 20.7 31.3</cell></row><row><cell>MS-SDALF [3]</cell><cell>5.2</cell><cell cols="3">20.7 32.0 47.9</cell><cell>6.3</cell><cell cols="3">18.8 27.1 37.3</cell></row><row><cell>ISR [97]</cell><cell cols="4">17.3 38.2 53.4 64.5</cell><cell>7.9</cell><cell cols="3">22.8 30.3 41.8</cell></row><row><cell>eSDC [6]</cell><cell cols="8">25.8 43.6 52.6 62.0 10.2 24.8 35.5 52.9</cell></row><row><cell>RDL [26]</cell><cell cols="8">29.1 53.6 66.2 76.1 11.5 26.2 34.3 46.3</cell></row><row><cell>MDTS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>-DTW I 41.7 67.1 79.4 90.1 31.5 62.1 72.8 82.4</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>Comparing supervised person ReID methods (closed-world scenario).</figDesc><table><row><cell>Dataset</cell><cell></cell><cell cols="2">PRID2011 [22]</cell><cell></cell><cell></cell><cell cols="2">iLIDS-VID [17]</cell><cell></cell></row><row><cell>Rank R (%)</cell><cell>1</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>1</cell><cell>5</cell><cell>10</cell><cell>20</cell></row><row><cell>SS-ColLBP [5]</cell><cell cols="4">22.4 41.8 51.0 64.7</cell><cell>9.1</cell><cell cols="3">22.6 33.2 45.5</cell></row><row><cell>MS-ColLBP [5]</cell><cell cols="8">34.3 56.0 65.5 77.3 23.2 44.2 54.1 68.8</cell></row><row><cell>DVR [18]</cell><cell cols="8">40.0 71.7 84.5 92.2 39.5 61.1 71.7 81.0</cell></row><row><cell>KCVDCA [99]</cell><cell cols="8">43.8 69.7 76.4 87.6 16.7 43.3 54.0 70.7</cell></row><row><cell>XQDA [27]</cell><cell cols="8">46.3 78.2 89.1 96.3 16.7 39.1 52.3 66.8</cell></row><row><cell cols="6">MDTS-DTW I 41.7 67.1 79.4 90.1 31.5</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>62.1 72.8 82.4</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 5 :</head><label>5</label><figDesc>Evaluating the complementary effect between space-time and appearance feature based person ReID methods (closed-world scenario).</figDesc><table><row><cell>Dataset</cell><cell></cell><cell cols="2">PRID2011 [22]</cell><cell></cell><cell></cell><cell cols="2">iLIDS-VID [17]</cell><cell></cell></row><row><cell>Rank R (%)</cell><cell>1</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>1</cell><cell>5</cell><cell>10</cell><cell>20</cell></row><row><cell>DVR [18]</cell><cell cols="8">40.0 71.7 84.5 92.2 39.5 61.1 71.7 81.0</cell></row><row><cell>MDTS-DTW I</cell><cell cols="8">41.7 67.1 79.4 90.1 31.5 62.1 72.8 82.4</cell></row><row><cell>eSDC[6]</cell><cell cols="8">25.8 43.6 52.6 62.0 10.2 24.8 35.5 52.9</cell></row><row><cell>eSDC+DVR[18]</cell><cell cols="8">44.3 68.4 78.2 91.1 29.5 54.0 66.4 78.4</cell></row><row><cell>eSDC+MDTS-DTW I</cell><cell cols="8">48.0 69.9 82.0 91.8 33.5 64.1 74.2 83.5</cell></row><row><cell>ISR [97]</cell><cell cols="4">17.3 38.2 53.4 64.5</cell><cell>7.9</cell><cell cols="3">22.8 30.3 41.8</cell></row><row><cell>ISR+DVR</cell><cell cols="8">43.8 63.3 72.5 81.3 30.0 46.0 55.1 63.6</cell></row><row><cell>ISR+MDTS-DTW I</cell><cell cols="8">46.2 66.7 72.6 83.3 33.1 51.5 58.7 69.7</cell></row><row><cell>RDL [26]</cell><cell cols="8">29.1 53.6 66.2 76.1 11.5 26.2 34.3 46.3</cell></row><row><cell>RDL+DVR</cell><cell cols="8">58.9 79.7 87.5 93.6 31.7 56.9 67.7 80.5</cell></row><row><cell>RDL+MDTS-DTW I</cell><cell cols="8">59.2 82.7 88.4 94.9 35.3 63.4 73.9 83.3</cell></row><row><cell>MS-ColLBP [5]</cell><cell cols="8">34.3 56.0 65.5 77.3 23.2 44.2 54.1 68.8</cell></row><row><cell>MS-ColLBP+DVR</cell><cell cols="8">44.8 66.9 77.1 89.9 39.5 61.0 72.7 82.8</cell></row><row><cell cols="9">MS-ColLBP+MDTS-DTW I 47.8 67.5 79.9 91.0 44.1 69.9 79.1 88.8</cell></row><row><cell>KCVDCA [99]</cell><cell cols="8">43.8 69.7 76.4 87.6 16.7 43.3 54.0 70.7</cell></row><row><cell>KCVDCA+DVR</cell><cell cols="8">65.7 88.1 93.4 97.3 54.9 76.8 83.7 91.3</cell></row><row><cell>KCVDCA+MDTS-DTW I</cell><cell cols="8">71.0 89.0 93.8 97.5 50.6 77.0 85.6 92.6</cell></row><row><cell>XQDA [27]</cell><cell cols="8">46.3 78.2 89.1 96.3 16.7 39.1 52.3 66.8</cell></row><row><cell>XQDA+DVR</cell><cell cols="8">77.4 93.9 97.0 99.4 51.1 75.7 83.9 90.5</cell></row><row><cell>XQDA+MDTS-DTW I</cell><cell cols="8">69.6 89.4 94.3 97.9 49.5 75.7 84.5 91.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6 :</head><label>6</label><figDesc>Comparing the open-world ReID performance. Metric: Detection and Identification Rate (DIR, Eqn. (13) with k = 1) over four False Accept Rates (FAR, Eqn. (14)). .8 42.7 47.7 10.7 20.3 29.3 32.9 ISR+MDTS-DTW I 25.2 36.0 46.8 49.7 11.3 17.6 32.6 35.7 RDL+DVR 26.7 39.3 58.8 62.7 8.5 15.4 30.1 37.3 RDL+MDTS-DTW I 21.8 38.5 59.7 63.7 9.2 18.7 33.7 41.7 MS-ColLBP+DVR 25.5 29.2 45.8 50.0 16.3 22.6 38.5 43.3 MS-ColLBP+MDTS-DTW I 27.7 33.2 49.7 51.2 11.6 21.3 43.8 50.0 KCVDCA+DVR 31.7 55.2 72.5 75.3 17.0 29.7 50.9 56.4 KCVDCA+MDTS-DTW I 42.7 52.8 72.5 73.5 16.8 30.2 51.5 56.4 XQDA+DVR 46.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell cols="2">PRID2011 [22]</cell><cell></cell><cell></cell><cell cols="2">iLIDS-VID [17]</cell><cell></cell></row><row><cell>FAR (%)</cell><cell>1</cell><cell>10</cell><cell>50</cell><cell>100</cell><cell>1</cell><cell>10</cell><cell>50</cell><cell>100</cell></row><row><cell>L 1 -norm</cell><cell>4.3</cell><cell>8.7</cell><cell cols="2">18.5 28.3</cell><cell>1.0</cell><cell>5.2</cell><cell cols="2">15.6 22.9</cell></row><row><cell>MS-SDALF [3]</cell><cell>0.5</cell><cell>1.0</cell><cell>4.5</cell><cell>6.3</cell><cell>0.2</cell><cell>0.5</cell><cell>3.3</cell><cell>8.4</cell></row><row><cell>ISR [97]</cell><cell>0.0</cell><cell cols="3">18.0 18.2 18.8</cell><cell>0.0</cell><cell>8.9</cell><cell>8.9</cell><cell>10.6</cell></row><row><cell>eSDC [6]</cell><cell>5.2</cell><cell>9.7</cell><cell cols="2">20.8 28.3</cell><cell>1.4</cell><cell>4.2</cell><cell>8.3</cell><cell>12.4</cell></row><row><cell>RDL [26]</cell><cell>9.3</cell><cell cols="3">13.3 27.5 33.0</cell><cell>2.1</cell><cell>4.9</cell><cell cols="2">10.4 13.9</cell></row><row><cell>MS-ColLBP [5]</cell><cell>4.3</cell><cell>6.7</cell><cell cols="2">24.3 39.8</cell><cell>1.1</cell><cell>4.8</cell><cell cols="2">15.6 25.9</cell></row><row><cell>DVR [18]</cell><cell>4.0</cell><cell cols="3">12.3 34.7 46.8</cell><cell cols="4">4.2 14.1 31.8 43.7</cell></row><row><cell>KCVDCA [99]</cell><cell cols="4">14.5 20.2 43.0 49.5</cell><cell>7.1</cell><cell cols="3">12.1 20.8 24.8</cell></row><row><cell>XQDA [27]</cell><cell cols="5">11.5 19.8 40.3 51.7 1.3</cell><cell>4.3</cell><cell cols="2">11.5 21.2</cell></row><row><cell>MDTS-DTW I</cell><cell cols="4">17.5 25.5 38.2 46.5</cell><cell>3.4</cell><cell>8.7</cell><cell cols="2">26.4 37.0</cell></row><row><cell>eSDC+DVR</cell><cell cols="4">13.3 25.2 43.3 48.5</cell><cell>7.2</cell><cell cols="3">14.4 27.7 34.6</cell></row><row><cell>eSDC+MDTS-DTW I</cell><cell cols="4">16.8 28.2 44.7 51.3</cell><cell>6.3</cell><cell cols="3">12.0 31.5 39.6</cell></row><row><cell>ISR+DVR</cell><cell cols="2">15.0 27</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>8 58.3 78.3 79.7 17.3 29</head><label></label><figDesc>.1 49.9 57.8 XQDA+MDTS-DTW I 42.7 55.2 70.5 72.8 12.7 32.6 51.8 57.3</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_0"><p>For a fair comparison with existing methods, we followed the setting in<ref type="bibr" target="#b16">[17]</ref>, i.e. sequences of more than 21 frames from 178 people were selected and utilised in our evaluations.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was partially supported by National Basic Research Program of China (973 Project) 2012CB725405, the national science and technology support program(2014BAG03B01), National Natural Science Foundation China 61273238, Beijing Municipal Science and Technology Project (D15110900280000), Tsinghua University Project (20131089307) and the Foundation of Beijing Key Laboratory for Cooperative Vehicle Infrastructure Systems and Safety Control. Xiatian Zhu and Xiaolong Ma equally contributed to this work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<title level="m">Person re-identification</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The re-identification challenge</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Person reidentification by symmetry-driven accumulation of local features</title>
		<author>
			<persName><forename type="first">M</forename><surname>Farenzena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Perina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2360" to="2367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Person re-identification by support vector ranking</title>
		<author>
			<persName><forename type="first">B</forename><surname>Prosser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Relaxed pairwise learned metric for person re-identification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Köstinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="780" to="793" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised salience learning for person re-identification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3586" to="3593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Person re-identification by discriminatively selecting parts and features</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bhuiyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Perina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop of European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="147" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On-the-fly feature importance mining for person re-identification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="1602" to="1615" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Towards open-world person reidentification by one-shot group-based verification</title>
		<author>
			<persName><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="591" to="606" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning a discriminative null space for person re-identification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Evaluation of local spatio-temporal features for action recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Ullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Klaser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A survey on vision-based human action recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Poppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="976" to="990" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The humanid gait challenge problem: Data sets, performance, and analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">R</forename><surname>Vega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Grother</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="162" to="177" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Gait recognition without subject cooperation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Bashir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="2052" to="2060" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Articulated human detection with flexible mixtures of parts</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2878" to="2890" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-source deep learning for human pose estimation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2337" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Person re-identification by video ranking</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="688" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Person re-identification by discriminative selection in video ranking</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2501" to="2514" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A spatio-temporal descriptor based on 3dgradients</title>
		<author>
			<persName><forename type="first">A</forename><surname>Klaser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2169" to="2178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Detecting activities of daily living in firstperson camera views</title>
		<author>
			<persName><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2847" to="2854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Person re-identification by descriptive and discriminative classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Beleznai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Scandinavian Conference on Image Analysis</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<title level="m">Open-set person re-identification</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Gait recognition by ranking</title>
		<author>
			<persName><forename type="first">R</forename><surname>Martín-Félez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="328" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Rabiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-H</forename><surname>Juang</surname></persName>
		</author>
		<title level="m">Fundamentals of speech recognition</title>
		<imprint>
			<publisher>Prentice Hall Englewood Cliffs</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dictionary learning with iterative laplacian regularisation for unsupervised person re-identification</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Person re-identification by local maximal occurrence representation and metric learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2197" to="2206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Human gait recognition using patch distribution feature and locality-constrained group sparse representation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="316" to="326" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The tum gait from audio, image and depth (gaid) database: Multimodal recognition of subjects and traits</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bachmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rigoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="195" to="206" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pose depth volume extraction from rgb-d streams for frontal gait recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Chattopadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sural</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mukhopadhyay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="63" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Robust view-invariant multiscale gait recognition</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tjahjadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="798" to="811" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Action and simultaneous multiple-person identification using cubic higher-order local auto-correlation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Otsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="741" to="744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Individual recognition using gait energy image</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bhanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="316" to="322" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">What image information is important in silhouette-based gait recognition?</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">V</forename><surname>Veres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Nixon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">776</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">On including quality in applied automatic gait recognition</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Matovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Nixon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mahmoodi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mansfield</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="3272" to="3275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Gait recognition in the presence of occlusion: A new dataset and baseline algorithms</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sural</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rigoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Graphics, Visualization and Computer Vision</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="99" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Human gait recognition based on matching of body components</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Boulgouris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">X</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1763" to="1770" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Clothing-invariant gait identification using part-based clothing categorization and adaptive weight control</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Makihara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2281" to="2291" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Gait recognition using partial silhouettebased approach</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Shaikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saeed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Signal Processing and Integrated Networks</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="101" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Gait regeneration for recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Muramatsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Makihara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IAPR International Conference on Biometrics</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Bilateral filtering-based optical flow estimation with occlusion detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sawhney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isnardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="211" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Modelling the effect of view angle variation on appearance-based gait recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="807" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Gait recognition based on dynamic region analysis</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2350" to="2356" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Biometric gait recognition with carrying and clothing variants</title>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Biswas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="446" to="451" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Uncooperative gait recognition by learning to rank</title>
		<author>
			<persName><forename type="first">R</forename><surname>Martín-Félez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3793" to="3806" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Dynamic time warping algorithm review</title>
		<author>
			<persName><forename type="first">P</forename><surname>Senin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="23" />
			<pubPlace>USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Information and Computer Science Department University of Hawaii at Manoa Honolulu</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Searching and mining trillions of time series subsequences under dynamic time warping</title>
		<author>
			<persName><forename type="first">T</forename><surname>Rakthanmanon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Campana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mueen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Westover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zakaria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="262" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Derivative dynamic time warping</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Keogh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Pazzani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIAM International Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A time series representation model for accurate and fast similarity detection</title>
		<author>
			<persName><forename type="first">F</forename><surname>Gullo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ponti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tagarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Greco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2998" to="3014" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Weighted dynamic time warping for time series classification</title>
		<author>
			<persName><forename type="first">Y.-S</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">A</forename><surname>Omitaomu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2231" to="2240" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">An automatic and efficient dynamic programming algorithm for polygonal approximation of digital curves</title>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Horng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="171" to="182" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Spotting method for classification of real world data</title>
		<author>
			<persName><forename type="first">R</forename><surname>Oka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Computer Journal</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="559" to="565" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Viewpoint invariant human re-identification in camera networks using pose priors and subject-discriminative features</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Radke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1095" to="1108" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Mirror representation for modeling viewspecific transform in person re-identification</title>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference of Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3402" to="3408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Human-in-the-loop person reidentification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="405" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Towards unsupervised open-set person re-identification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Person re-identification by unsupervised l1 graph learning</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Person reidentification in multi-camera system by signature based on interest point descriptors collected on short video sequences</title>
		<author>
			<persName><forename type="first">O</forename><surname>Hamdoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Moutarde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Stanciulescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steux</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>ACM International Conference on Distributed Smart Cameras</publisher>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Video sequences association for people re-identification across multiple non-overlapping cameras</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N T</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Achard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Khoudour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Douadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Analysis and Processing</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="179" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Full-body person recognition system</title>
		<author>
			<persName><forename type="first">C</forename><surname>Nakajima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Heisele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1997" to="2006" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Person reidentification using spatiotemporal appearance</title>
		<author>
			<persName><forename type="first">N</forename><surname>Gheissari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1528" to="1535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Custom pictorial structures for re-identification</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stoppa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Human re-identification by matching compositional template with cluster sampling</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A hierarchical method combining gait and phase of motion with spatiotemporal model for person re-identification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sural</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mukherjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="1891" to="1901" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Person re-identification using view-dependent score-level fusion of gait and color features</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kawai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Makihara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Iwama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2694" to="2697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Gait-assisted person re-identification in wide area surveillance</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bedagkar-Gala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop of Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="633" to="649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Enhancing person re-identification by integrating gait biometric</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">168</biblScope>
			<biblScope unit="issue">30</biblScope>
			<biblScope unit="page" from="1144" to="1156" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Top-push video-based person reidentification</title>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Recurrent convolutional network for video-based person re-identification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mclaughlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Martinez Del Rincon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Recognizing human actions: a local svm approach</title>
		<author>
			<persName><forename type="first">C</forename><surname>Schüldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="32" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Behavior recognition via sparse spatio-temporal features</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Learning realistic human actions from movies</title>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rozenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Canonical correlation analysis of video volume tensors for action categorization and detection</title>
		<author>
			<persName><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1415" to="1428" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">A 3-dimensional sift descriptor and its application to action recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Scovanner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="357" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">An efficient dense and scaleinvariant spatio-temporal interest point detector</title>
		<author>
			<persName><forename type="first">G</forename><surname>Willems</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="650" to="663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Convolutional sparse coding for trajectory reconstruction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="529" to="540" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Discriminative subsequence mining for action classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bakir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tsuda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Action snippets: How many frames does human action recognition require?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Modeling temporal structure of decomposable motion segments for activity classification</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="392" to="405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Actom sequence models for efficient action detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="3201" to="3208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">A time series kernel for action recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Event-based analysis of video</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Efficient representations of video sequences and their applications</title>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Anandan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bergen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing: Image Communication</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="327" to="351" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Lee</surname></persName>
		</author>
		<title level="m">ACM International Conference on Multimedia Information Retrieval</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="291" to="297" />
		</imprint>
	</monogr>
	<note>Spatio-temporal pyramid matching for sports videos</note>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Visual tracking: An experimental survey</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1442" to="1468" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">The pyramid match kernel: Efficient learning with sets of features</title>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="725" to="760" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Lpm for fast action recognition with large number of classes</title>
		<author>
			<persName><forename type="first">F</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Laganiere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Petriu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">A robust and efficient video representation for action recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Oneata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Using dynamic time warping to find patterns in time series</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Berndt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clifford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop of ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="359" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Independent coordinates for strange attractors from mutual information</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Swinney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review A</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">1134</biblScope>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Time-delayed correlation analysis for multicamera activity understanding</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="106" to="129" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">On the non-trivial generalization of dynamic time warping to the multi-dimensional case</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shokoohi-Yekta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIAM International Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">Dynamic time warping, Information retrieval for music and motion</title>
		<author>
			<persName><forename type="first">M</forename><surname>Müller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="69" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Fastdtw: Toward accurate dynamic time warping in linear time and space</title>
		<author>
			<persName><forename type="first">S</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop of ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Efficient algorithms for ranking with svms</title>
		<author>
			<persName><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Keerthi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="201" to="215" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Person re-identification by iterative re-weighted sparse ranking</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lisanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Del</surname></persName>
		</author>
		<author>
			<persName><surname>Bimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1629" to="1642" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Querying and mining of time series data: experimental comparison of representations and distance measures</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Trajcevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Scheuermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the Very Large Data Bases Endowment</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1542" to="1552" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">An asymmetric distance model for cross-view feature mapping in person re-identification</title>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
