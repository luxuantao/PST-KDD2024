<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A de-identifier for medical discharge summaries</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">O</forename><surname>¨zlem Uzuner</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University at Albany</orgName>
								<orgName type="institution" key="instit2">State University of New York</orgName>
								<address>
									<addrLine>Draper 114, 135 Western Avenue</addrLine>
									<postCode>12222</postCode>
									<settlement>Albany</settlement>
									<region>NY</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tawanda</forename><forename type="middle">C</forename><surname>Sibanda</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
								<address>
									<addrLine>32 Vassar Street</addrLine>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuan</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University at Albany</orgName>
								<orgName type="institution" key="instit2">State University of New York</orgName>
								<address>
									<addrLine>Draper 114, 135 Western Avenue</addrLine>
									<postCode>12222</postCode>
									<settlement>Albany</settlement>
									<region>NY</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peter</forename><surname>Szolovits</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
								<address>
									<addrLine>32 Vassar Street</addrLine>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">University at Albany</orgName>
								<orgName type="institution" key="instit2">State University of New York</orgName>
								<address>
									<addrLine>Draper 114A, 135 Western Avenue</addrLine>
									<postCode>12222</postCode>
									<settlement>Albany</settlement>
									<region>NY</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A de-identifier for medical discharge summaries</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E90145F059BA4E282026BB8FAE583546</idno>
					<idno type="DOI">10.1016/j.artmed.2007.10.001</idno>
					<note type="submission">Received 23 November 2006; received in revised form 8 October 2007; accepted 9 October 2007</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Automatic de-</keywords>
			</textClass>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Medical discharge summaries can be a major source of information for many studies. However, like all other clinical records, discharge summaries contain explicit personal health information (PHI) which, if released, would jeopardize patient privacy. In the United States, the Health Information Portability and Accountability Act (HIPAA) provides guidelines for protecting the confidentiality of patient records. Paragraph 164.514 of the Administrative Simplification Regulations promulgated under the HIPAA states that for data to be treated as de-identified, it must clear one of two hurdles:</p><p>1. An expert must determine and document ''that the risk is very small that the information could be used, alone or in combination with other reasonably available information, by an anticipated recipient to identify an individual who is a subject of the information.'' 2. Or, the data must be purged of a specified list of seventeen categories of possible identifiers, i.e., PHI, relating to the patient or relatives, household members and employers, and any other information that may make it possible to identify the individual <ref type="bibr" target="#b0">[1]</ref>. Many institutions consider the clinicians caring for a patient and the names of hospitals, clinics, and wards to fall under this final category because of the heightened risk of identifying patients from such information <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>.</p><p>Of the 17 categories of PHI listed by HIPAA, the following appear in medical discharge summaries: first and last names of patients, of their health proxies, and of their family members; identification numbers; telephone, fax, and pager numbers; geographic locations; and dates. In addition, names of doctors and hospitals are frequently mentioned in discharge summaries; for this study, we add them to the list of PHI. Given discharge summaries, our goal is to find the above listed PHI and to replace them with either anonymous tags or realistic surrogates.</p><p>Medical discharge summaries are characterized by fragmented, incomplete utterances and domainspecific language. As such, they cannot be effectively processed by tools designed for lay language text such as news articles <ref type="bibr" target="#b3">[4]</ref>. In addition, discharge summaries contain some words that can appear both as PHI and non-PHI within the same corpus, e.g., the word Huntington can be both the name of a person, ''Dr. Huntington'', and the name of a disease, ''Huntington's disease''. They also contain foreign and misspelled words as PHI, e.g., John misspelled as Jhn and foreign variants such as Ioannes. These complexities pose challenges to de-identification.</p><p>An ideal de-identification system needs to identify PHI perfectly. However, while anonymizing the PHI, such a system needs to also protect the integrity of the data by maintaining all of the non-PHI, so that medical records can later be processed and retrieved based on their inclusion of these terms. Almost all methods that determine whether a target word, <ref type="foot" target="#foot_0">1</ref> i.e., the word to be classified as PHI or non-PHI, is PHI base their decision on a combination of features related to the target itself, to words that surround the target, and to discourse segments containing the target. We call the features extracted from the words surrounding the target and from the discourse segment containing the target the context of the target. In this paper, we are particularly interested in comparing methods that rely on what we call local context, by which we mean the words that immediately surround the target (local lexical context) or that are linked to it by some immediate syntactic relationship (local syntactic context), and global context, which refers to the relationships of the target with the contents of the discourse segment containing the target. For example, the surrounding k-tuples of words to the left and right of a target are common components of local context, whereas a model that selects the highest probability interpretation of an entire sentence by a Markov model employs sentential global context (where the discourse segment is a sentence).</p><p>In this paper, we present a de-identifier, Stat Deid, which uses local context to de-identify medical discharge summaries. We treat de-identification as a multi-class classification task; the goal is to consider each word in isolation and to decide whether it represents a patient, doctor, hospital, location, date, telephone, ID, or non-PHI. We use support vector machines (SVMs), as implemented by LIBSVM <ref type="bibr" target="#b4">[5]</ref>, trained on human-annotated data as a means to this end. Our representation of local context benefits from orthographic, syntactic, and semantic characteristics of each target word and the words within a AE2 context window of the target. Other models of local context have used the features of words immediately adjacent to the target word; our representation is more thorough as it includes (for a AE2 context) local syntactic context, i.e., the features of words that are linked to the target by syntactic relations identified by a parse of the sentence. This novel representation of local syntactic context uses the Link Grammar Parser <ref type="bibr" target="#b5">[6]</ref>, which can provide at least a partial syntactic parse even for incomplete and fragmented sentences <ref type="bibr" target="#b6">[7]</ref>. Note that syntactic parses can be generally regarded as sentential features. However, in our corpora, more than 40% of the sentences only partially parse. The features extracted from such partial parses represent phrases rather than sentences and contribute to local context. For sentences that completely parse, our representation benefits from syntactic parses only to the extent that they help us relate the target to its immediate neighbors (within two links), again extracting local context.</p><p>On five separate corpora obtained from Partners Healthcare and Beth Israel Deaconess Medical Center, we show that despite the fragmented and incomplete utterances and the domain-specific language that dominate the text of discharge summaries, we can capture the patterns in the language of these documents by focusing on local context; we can use these patterns for de-identification. Stat De-id, presented in this paper, is built on this hypothesis. It finds more than 90% of the PHI even in the face of ambiguity between PHI and non-PHI, and even in the presence of foreign words and spelling errors in PHI.</p><p>We compare Stat De-id with a rule-based heuristic + dictionary approach <ref type="bibr" target="#b7">[8]</ref>, two named entity recognizers, SNoW <ref type="bibr" target="#b8">[9]</ref> and IdentiFinder <ref type="bibr" target="#b9">[10]</ref>, and a Conditional Random Field De-identifier (CRFD). SNoW and IdentiFinder also use local context; however, their representation of local context is relatively simple and, for named entity recognition (NER), is complemented with information from sentential global context, i.e., the dependencies of entities with each other and with non-entity tokens in a single sentence. CRFD, developed by us for the studies presented in this paper, employs the exact same local context used by Stat De-id and reinforces this local context with sentential global context. In this manuscript, we refer to sentential global context simply as global context. Because medical dis-charge summaries contain many short, fragmented sentences, we hypothesize that global context will add limited value to local context for de-identification, and that strengthening the representation of local context will be more effective for improving de-identification. We present experimental results to support this hypothesis: on our corpora, Stat Deid significantly outperforms all of SNoW, IdentiFinder, CRFD, and the heuristic + dictionary approach.</p><p>The performance of Stat De-id is encouraging and can guide research in identification of entities in corpora with fragmented, incomplete utterances and even domain-specific language. Our results show that even on such corpora, it is possible to create a useful representation of local context and to identify the entities indicated by this context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background and related work</head><p>A number of investigators have developed methods for de-identifying medical corpora or for recognizing named entities in non-clinical text (which can be directly applied to at least part of the de-identification problem). The two main approaches taken have been either (a) use of dictionaries, pattern matching, and local rules or (b) statistical methods trained on features of the word(s) in question and their local or global context. Our work on Stat De-id falls into the second of these traditions and differs from others mainly in its use of novel local context features determined from a (perhaps partial) syntactic parse of the text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">De-identification</head><p>Most de-identification systems use dictionaries and simple contextual rules to recognize PHI <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11]</ref>. Gupta et al. <ref type="bibr" target="#b10">[11]</ref>, for example, describe the DeID system which uses the US Census dictionaries to find proper names, employs patterns to detect phone numbers and zip codes, and takes advantage of contextual clues (such as section headings) to mark doctor and patient names. Gupta et al. report that, after scrubbing with DeID, of the 300 reports scrubbed, two reports still contained accession numbers, two reports contained clinical trial names, three reports retained doctors' names, and three reports contained hospital or lab names.</p><p>Beckwith et al. <ref type="bibr" target="#b11">[12]</ref> present a rule-based de-identifier for pathology reports. Unlike our discharge summaries, pathology reports contain significant header information. Beckwith et al. identify PHI that appear in the headers (e.g., medical record number and patient name) and remove the instances of these PHI from the narratives. They use pattern-matchers to find dates, IDs, and addresses; they utilize wellknown markers such as Mr., MD, and PhD to find patient, institution, and physician names. They conclude their scrubbing by comparing the narrative text with a database of proper names. Beckwith et al. report that they remove 98.3% of unique identifiers in pathology reports from three institutions. They also report that on average 2.6 non-PHI phrases per record are removed.</p><p>The de-identifier of Berman <ref type="bibr" target="#b12">[13]</ref> takes advantage of standard nomenclature available in UMLS. This system assumes that words that do not correspond to nomenclature and that are not in a standard list of stop words are PHI and need to be removed. As a result, this system produces a large number of false positives.</p><p>Sweeney's Scrub system <ref type="bibr" target="#b2">[3]</ref> employs numerous experts each of which specializes in recognizing a single class of personally identifying information, e.g., person names. Each expert uses lexicons and morphological patterns to compute the probability that a given word belongs to the personally identifying information class it specializes in. The expert with the highest probability determines the class of the word. On a test corpus of patient records and letters, Scrub identified 99-100% of personally identifying information. Unfortunately, Scrub is a proprietary system and is not readily available for use.</p><p>To identify patient names, Taira et al. <ref type="bibr" target="#b13">[14]</ref> use a lexical analyzer that collects name candidates from a database and filters out the candidates that match medical concepts. They refine the list of name candidates by applying a maximum entropy model based on semantic selectional restrictions--the hypothesis that certain word classes impose semantic constraints on their arguments, e.g., the verb vomited implies that its subject is a patient. They achieve a precision of 99.2% and recall of 93.9% on identification of patient names in a clinical corpus.</p><p>De-identification resembles NER. NER is the task of identifying entities such as people, places, and organizations in narrative text. Most NER tasks are performed on news and journal articles. However, given the similar kinds of entities targeted by deidentification and NER, NER approaches can be relevant to de-identification. Technologies developed for ACE-2007, for example, have been designed for and evaluated on several individual corpora: a 65000-word Broadcast News corpus, a 47500-word Broadcast Conversations corpus, a 60000-word Newswire corpus, a 47500-word Weblog corpus, a 47500-word Usenet corpus, and a 47500-word Conversational Telephone Speech corpus <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Named entity recognition</head><p>One of the most successful named entity recognizers, among the NER systems developed for and outside of MUC and ACE, is IdentiFinder <ref type="bibr" target="#b9">[10]</ref>. Iden-tiFinder uses a hidden Markov model (HMM) to learn the characteristics of names that represent entities such as people, locations, geographic jurisdictions, organizations, and dates. For each entity class, IdentiFinder learns a bigram language model, where a word is defined as a combination of the actual lexical unit and various orthographic features. To find the names and classes of all entities, IdentiFinder computes the most likely sequence of entity classes in a sentence given the observed words and their features. The information obtained from the entire sentence constitutes IdentiFinder's global context.</p><p>Isozaki and Kazawa <ref type="bibr" target="#b15">[16]</ref> use SVMs to recognize named entities in Japanese text. They determine the entity type of each target word by employing features of the words within two words of the target (a AE2 word window). The features they use include the part of speech and the structure of the word, as well as the word itself.</p><p>Roth and Yih's SNoW system <ref type="bibr" target="#b8">[9]</ref> labels the entities and their relationships in a sentence. The relationships expressed in the sentence constitute SNoW's global context and aid it in creating a final hypothesis about the entity type of each word. SNoW recognizes names of people, locations, and organizations.</p><p>Our de-identification solution combines the strengths of some of the abovementioned systems. Like Isozaki et al., we use SVMs to identify the class of individual words (where the class is one of seven categories of PHI or the class non-PHI); we use orthographic information as well as part of speech and local context as features. Like Taira et al., we hypothesize that PHI categories are characterized by their local lexical and syntactic context. However, our approach to de-identification differs from prior NER and de-identification approaches in its use of deep syntactic information obtained from the output of the Link Grammar Parser <ref type="bibr" target="#b5">[6]</ref>. We benefit from this information to capture local syntactic context even when parses are partial, i.e., input text contains fragmented and incomplete utterances. We enrich local lexical context with local syntactic context and thus create a more thorough representation of local context. We use our newly defined representation of local context to identify PHI in clinical text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>x3. Definitions</head><p>We define the PHI found in medical discharge summaries as follows:</p><p>Patients: include the first and last names of patients, their health proxies, and family members. Titles, such as Mr., are excluded, e.g., ''Mrs.</p><p>[Lunia Smith] patient was . . .''. Doctors: include medical doctors and other practitioners. Again titles, such as Dr., are not considered part of PHI, e.g., ''He met with Dr. [John Doe] doctor ''. Hospitals: include names of medical organizations. We categorize the entire institution name as PHI including common words such as hospital, e.g., ''She was admitted to [Brigham and Women's Hospital] hospital ''. IDs: refer to any combination of numbers and letters identifying medical records, patients, doctors, or hospitals, e.g., ''Provider Number:</p><p>[12344] ID ''. Dates: HIPAA specifies that years are not considered PHI, but all other elements of a date are. We label a year appearing in a date as PHI if the date appears as a single lexical unit, e.g., 12/02/99, and as non-PHI if the year exists as a separate token, e.g., <ref type="bibr">23 March, 2006</ref>. This decision was motivated by the fact that many solutions to deidentification and NER classify entire tokens as opposed to segments of a token. Also, once identified, dates such as 12/02/99 can be easily post-processed to separate the year from the rest. Locations: include geographic locations such as cities, states, street names, zip codes, and building names and numbers, e.g., ''He lives in [Newton] location ''. Phone numbers: include telephone, pager, and fax numbers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Hypotheses</head><p>We hypothesize that we can de-identify medical discharge summaries even when the documents contain many fragmented and incomplete utterances, even when many words are ambiguous between PHI and non-PHI, and even in the presence of foreign words and spelling errors in PHI. Given the nature of the domain-specific language of discharge summaries, we hypothesize that a thorough representation of local context will be more effective for de-identification than (relatively simpler) local context enhanced with global context; in this manuscript, local context refers to the characteristics of the target and of the words within a AE2 context window of the target whereas global context refers to the dependencies of entities with each other and with non-entity tokens in a sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Corpora</head><p>We tested our methods on five different corpora, three of which were developed from a corpus of 48 discharge summaries from various medical departments at the Beth Israel Deaconess Medical Center (BIDMC), the fourth of which consisted of authentic data including actual PHI from 90 discharge summaries of deceased patients from Partners Health-Care, and the fifth of which came from a corpus of 889 de-identified discharge summaries, also from Partners. The sizes of these corpora and the distribution of PHI within them are shown in Table <ref type="table" target="#tab_1">1</ref>. The collection and use of these data were approved by the Institutional Review Boards of Partners, BIDMC, State University of New York at Albany, and Massachusetts Institute of Technology.</p><p>A successful de-identification scheme must achieve two competing objectives: it must anonymize all PHI in the text; however, it must leave Word counts depend on the number and format of inserted surrogates.</p><p>intact the non-PHI. Two of the major challenges to achieving these objectives in medical discharge summaries are the existence of ambiguous PHI and the existence of out-of-vocabulary PHI. Four of our corpora were specifically created to test our system in the presence of these challenges. Three of these artificial corpora were based on the corpus of 48 already de-identified discharge summaries from BIDMC. In this corpus, the PHI had been replaced by [REMOVED] tags (see excerpt below). This replacement had been performed semi-automatically. In other words, the PHI had been removed by an automatic system <ref type="bibr" target="#b7">[8]</ref> and the output had been manually scrubbed. Before studying this corpus, our team confirmed its correctness. We used the definitions of PHI classes in conjunction with local contextual clues to identify the PHI category corresponding to each of the <ref type="bibr">[REMOVED]</ref> phrases in this corpus. We used dictionaries of common names from the US Census Bureau, dictionaries of hospitals and locations from online sources, and lists of diseases, treatments, and diagnostic tests from the UMLS Metathesaurus to generate surrogate PHI for three corpora: a corpus populated with random surrogate PHI <ref type="bibr" target="#b7">[8]</ref>, a corpus populated with ambiguous surrogate PHI, and a corpus populated with out-of-vocabulary surrogate PHI. The surrogate PHI inserted into each of the corpora represent the common patterns associated with each PHI class.</p><p>The name John K. Smith, for example, can appear as John K. Smith, J.K. Smith, J. Smith, Smith, John, etc. The date 5 July 1982 can be expressed as July 5, 5th of July, 07/05/82, 07-05-82, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Corpus populated with random PHI</head><p>We randomly selected names of people from a dictionary of common names from the US Census Bureau, and names of hospitals and locations from online dictionaries in order to generate surrogate PHI for the corpus with random PHI (details of these dictionaries can be found in Section 6.5.2). After manually tagging the PHI category of each [REMOVED] phrase, we replaced each [REMOVED] with a random surrogate from the correct PHI category and dictionary. In the rest of this paper, we refer to this corpus as the random corpus. The first column of Table <ref type="table" target="#tab_1">1</ref> shows the breakdown of PHI in the random corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Corpus populated with ambiguous PHI</head><p>To generate a corpus containing ambiguous PHI, two graduate students marked medical concepts corresponding to diseases, tests, and treatments in the deidentified corpus. Agreement, as measured by Kappa, on marking these concepts was 93%. The annotators discussed and resolved their differences, generating a single gold standard for medical concepts.</p><p>We used the marked medical concepts to generate ambiguous surrogate PHI with which to populate the de-identified corpus. In addition to the people, hospital, and location dictionaries employed in generating the random corpus, we also used lists of diseases, treatments, and diagnostic tests from the UMLS Metathesaurus in order to locate examples of medical terms that occur in the narratives of our records and to deliberately inject these terms into the surrogate patients, doctors, hospitals, and locations (with appropriate formatting). This artificially enhanced the occurrence of challenging examples such as ''Mr. Huntington suffers from Huntington's disease'' where the first occurrence of ''Huntington'' is a PHI and the second is not. The ambiguous terms we have injected into the corpus were guaranteed to appear both as PHI and as non-PHI in this corpus.</p><p>In addition to the ambiguities resulting from injection of medical terms into patients, doctors, hospitals, and locations, this corpus also already contained ambiguities between dates and non-PHI. Many dates appear in the format ##/##, a common format for reporting medical measurements. In our corpus, 14% of dates are ambiguous with non-PHI. After injection of ambiguous medical terms, 49% of patients, 79% of doctors, 100% of locations, and 14% of hospitals are ambiguous with non-PHI. In return, 20% of non-PHI are O ¨. Uzuner et al.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>History of present illness:</head><p>The patient is a 77year-old woman with long standing hypertension who presented as a walk-in to me at the [REMOVED] Health Center on [REMOVED]. Recently had been started q.o.d. on clonidine since [REMOVED] to taper off of the drug. Was told to start zestril 20 mg q.d. again. The patient was sent to the [REMOVED] Unit for direct admission for cardioversion and anticoagulation, with the Cardiologist, Dr.</p><p>[REMOVED] to follow. Social history: Lives alone, has one daughter living in <ref type="bibr">[REMOVED]</ref>. Is a non-smoker, and does not drink alcohol. Hospital course and treatment: During admission, the patient was seen by Cardiology, Dr.</p><p>[REMOVED], was started on IV heparin, sotalol 40 mg PO b.i.d. increased to 80 mg b.i.d., and had an echocardiogram. By [REMOVED] the patient had better rate control and blood pressure control but remained in atrial fibrillation. On [REMOVED], the patient was felt to be medically stable. . . ambiguous with PHI. In the rest of this paper, we refer to this corpus as the ambiguous corpus. The second column of Table <ref type="table" target="#tab_1">1</ref> shows the distribution of PHI in the ambiguous corpus. Table <ref type="table" target="#tab_2">2</ref> shows the distribution of tokens that are ambiguous between PHI and non-PHI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Corpus populated with out-ofvocabulary PHI</head><p>The corpus containing out-of-vocabulary PHI was created by the same process used to generate the random corpus. However, instead of using dictionaries, we generated surrogates by randomly selecting word lengths and letters, e.g., ''O. Ymfgi was admitted . . .''. Almost all generated patient, doctor, location, and hospital names were consequently absent from common dictionaries. In the rest of this paper, we refer to this corpus as the out-of-vocabulary (OoV) corpus. The third column of Table <ref type="table" target="#tab_1">1</ref> shows the distribution of PHI in the OoV corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Authentic discharge summary corpus</head><p>In addition to the artificial corpora, we obtained and used a corpus of authentic discharge summaries with genuine PHI about deceased patients. In the rest of this paper, we refer to this corpus as the authentic corpus.</p><p>The authentic corpus contained approximately 90 discharge summaries of various lengths from various medical departments from Partners HealthCare. This corpus differed from the artificial corpora obtained from BIDMC in both the writing style and in the distribution and frequency of use of PHI. However, it did contain the same basic categories of PHI. Three annotators manually marked the PHI in this corpus so that each record was marked three times. Agreement among the annotators, as measured by Kappa, was 100%. As with artificial corpora, we automatically deidentified the narrative portions of these records.</p><p>The fourth column of Table <ref type="table" target="#tab_1">1</ref> shows the breakdown of PHI in the authentic discharge summary corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Challenge corpus</head><p>Finally, we obtained and used a separate, larger corpus of 889 discharge summaries, again from Partners HealthCare. This corpus, which had formed the basis for a workshop and shared-task on de-identification organized at the 2006 AMIA fall symposium, had been manually de-identified and all authentic PHI in it had been replaced with realistic out-of-vocabulary or ambiguous surrogates <ref type="bibr" target="#b16">[17]</ref>. Of the surrogate PHI tokens in this corpus, 73% of patients, 67% of doctors, 56% of locations, and 49% of hospitals were out-of-vocabulary. Ten percent of patients, 15% of doctors, 10% of locations, and 37% of hospitals were ambiguous with non-PHI. This corpus thus combined the challenges of out-of-vocabulary and ambiguous PHI de-identification. In the rest of this paper, we refer to this corpus as the challenge corpus. The fifth column of Table <ref type="table" target="#tab_1">1</ref> shows the breakdown of PHI in the challenge discharge summary corpus.</p><p>In general, our corpora include non-uniform representation of various PHI categories. What is more, in terms of overall number of tokens, although our authentic and challenge corpora are larger than the standard corpora used for NER shared-tasks organized by NIST, our random, ambiguous, and out-ofvocabulary corpora contain very few examples of some of the PHI categories, e.g., 24 examples of locations. Therefore, in this manuscript, while we maintain the distinction among the PHI categories for classification, we report results on the aggregate set of PHI consisting of patients, doctors, locations, hospitals, dates, IDs, and phone numbers. We measure the performance of systems in differentiating this aggregate set of PHI from non-PHI. We report significance test results on the aggregate set of PHI and on non-PHI separately. Finally, we analyze the performance of our system on individual PHI categories only to understand its strengths and weaknesses on our data so as to identify potential courses of action for future work.</p><p>While access to more and larger corpora is desirable, freely available corpora for training de-identifiers are not common, and until de-identification research becomes more successful and accepted, it will require large investments in human reviewers to create them. Even then, multiple rounds of human review of the records may not be satisfactory for the Institutional Review Boards to allow widespread and unhindered use of clinical records for de-identification research. Even for those who can obtain the data, the use of the records may be limited to a particular task <ref type="bibr" target="#b16">[17]</ref>. While titles such as Dr. provide easy context markers, other clues may not be as straightforward, especially when the language of documents is dominated by fragmented and incomplete utterances. We created a representation of local context that is useful for recognizing PHI even in fragmented, incomplete utterances. We devised Stat De-id, a de-identifier that uses SVMs, to classify each word in the sentence as belonging to one of eight categories: doctor, location, phone, date, patient, ID, hospital, or non-PHI. Stat De-id uses features of the target, as well as features of the words surrounding the target in order to capture the contextual clues human annotators found useful in de-identification. We refer to the features of the target and its close neighbors as local context.</p><p>Stat De-id is distinguished from similar approaches in its use of syntactic information extracted from the Link Grammar Parser <ref type="bibr" target="#b5">[6]</ref>. Despite the fragmented nature of the language of discharge summaries, we can obtain (partial) syntactic parses from the Link Grammar Parser <ref type="bibr" target="#b6">[7]</ref> and we can use this information in creating a representation of local context. We augment the syntactic information with semantic information from medical dictionaries, such as the medical subject headings (MeSH) of the unified medical language system (UMLS) <ref type="bibr" target="#b17">[18]</ref>. Stat De-id will be freely available through the i2b2 Hive, https:// www.i2b2.org/resrcs/hive.html, a common tools distribution mechanism of the National Centers for Biomedical Computing project on Informatics for Integrating Biology and the Bedside (i2b2) that partially funded its development.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Support vector machines</head><p>Given a collection of data points represented by multi-dimensional vectors and class labels, (x i , y i ), i = 1, . . ., l where x i 2 R n and y i 2 {1,À1} l , SVMs <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">19]</ref> optimize:</p><formula xml:id="formula_0">min w;b;j 1 2 w T w þ C X l i¼1 j i<label>(1)</label></formula><p>Subject to</p><formula xml:id="formula_1">y i ðw T fðx i Þ þ bÞ ! 1 À j i ; j i ! 0: (2)</formula><p>where C &gt; 0 is the penalty parameter, j i is a measure of misclassification error, and w is the normal to the plane. This optimization maps input training vectors, x i , to a higher dimensional space given by the function, f. SVMs find a hyperplane that in this space best separates the data points according to their class. To prevent over-fitting, the hyperplane is chosen so as to maximize the distance between the hyperplane and the closest data point in each class. The data points that are closest to the discovered hyperplane are called the support vectors. Given a data point whose class is unknown, an SVM determines on which side of the hyperplane the point lies and labels it with the corresponding class <ref type="bibr" target="#b18">[19]</ref>. The kernel function:</p><formula xml:id="formula_2">Kðx i ; x j Þ fðx i Þ T fðx j Þ<label>(3)</label></formula><p>plays a role in determining the optimal hyperplane and encodes a ''similarity measure between two data points'' <ref type="bibr" target="#b19">[20]</ref>. In this paper, we explore a high dimensional feature space which can be prone to over-fitting. To minimize this risk, we employ the linear kernel</p><formula xml:id="formula_3">Kðx i ; x j Þ x T i x j<label>(4)</label></formula><p>and investigate the impact of various features on de-identification.</p><p>The choice of SVMs over other classifiers is motivated by their capability to robustly handle large feature sets; in our case, the number of features in the set is on the order of thousands. SVMs tend to be robust to the noise that is frequently present in such high dimensional feature sets <ref type="bibr" target="#b18">[19]</ref>. In addition, while ''classical learning systems like neural networks suffer from their theoretical weakness, e.g., back-propagation usually converges only to locally optimal solutions.'' <ref type="bibr" target="#b20">[21]</ref>, in comparison to other neural classifiers, SVMs are often more successful in finding globally optimum solutions <ref type="bibr" target="#b21">[22]</ref>.</p><p>Conditional Random Fields (CRF) <ref type="bibr" target="#b22">[23]</ref> provide a viable alternative to SVMs. Just like SVMs, CRFs can ''handle many dependent features''; however, unlike SVMs, they also can ''make joint inference over entire sequences'' <ref type="bibr" target="#b23">[24]</ref>. In our case, predictions over entire sequences correspond to global context. Given our interest in using local context, for the purposes of this manuscript, we focus primarily on SVMs. We use CRFs as a basis for comparison, in order to gauge the contribution of sentential global context to local context. We employ the multi-class SVM implementation of LIBSVM <ref type="bibr" target="#b4">[5]</ref>. This implementation builds a multi-class classifier from several binary classifiers using one-against-one voting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Knowledge representation</head><p>We use a vector to represent our features for use with an SVM. In this vector, each row corresponds to a single target and each column represents the possible values of all features of all targets in the training corpus. For example, suppose the first feature under consideration is dictionary information, i.e., the dictionaries that the target appears in, and the second is the part of speech of the target. Let w be the number of unique dictionaries relevant to the training corpus, and p be the number of unique parts of speech in the training corpus. Then, the first w columns of the feature vector represent the possible dictionaries. We mark the dictionaries that contain the target by setting the value of their entry(ies) (where an entry is the intersection of a row and a column) to one; all other dictionary entries for that target will be zero. Similarly, let's assume that the next p columns of the vector represent the possible values of parts of speech extracted from the training corpus; we mark the part of speech of the target by setting that entry to one and leaving the other part of speech entries at zero.</p><p>The vector that is fed into the SVM is concatenation of individual feature vectors that capture the target itself, the lexical bigrams of the target, use of capitalization, punctuation, or numbers in the target, and the length of the target, part of speech of the target, as well as syntactic bigrams, MeSH IDs, dictionary information, and section headings of the target. We evaluate our system using cross-validation. At each round of cross-validation, we re-create the feature vector based on the training corpus used for that round, i.e., the feature vector does not overfit to the validation set.</p><p>6.3. Lexical and orthographic features 6.3.1. The target itself Some words consistently occur as non-PHI. Taking the word itself into consideration allows the classifier to learn that certain words, such as and and they, are never PHI.</p><p>We incorporate the target word feature into our knowledge representation using a vector of all unique words in the training corpus. We mark unique words after normalization using UMLS's Norm <ref type="bibr" target="#b17">[18]</ref>. We mark each target word feature by setting the value of the entry corresponding to the target to one and leaving all other entries at zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.2.">Lexical bigrams</head><p>The context of the target can reveal its identity. For example, in a majority of the cases, the bigram admitted to is followed by the name of a hospital. Similarly, the bigram was admitted is preceded by the patient. To capture such indicators of PHI, we consider uninterrupted strings of two words occurring before and after the target. We refer to these strings as lexical bigrams.</p><p>We keep track of lexical bigrams using a vector that contains entries for both left and right lexical bigrams of the target. The columns of the vector correspond to all lexical bigrams that are observed in the training corpus. We mark the left and right lexical bigrams of a target by setting their entries to one and leaving the rest of the lexical bigram entries at zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.3.">Capitalization</head><p>Orthographic features such as capitalization can aid identification of PHI. Most names, i.e., names of locations as well as people, usually begin with a capital letter. We represent capitalization information in the form of a single column vector which for each target (row) contains an entry of one if the target is capitalized and zero if it is not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.4.">Punctuation</head><p>Dates, phone numbers, and IDs tend to contain punctuation. Including information about the presence or absence of ''À'' or ''/'' in the target helps us recognize these categories of PHI. Punctuation information is incorporated into the knowledge representation in a similar manner to capitalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.5.">Numbers</head><p>Dates, phone numbers, and IDs consist of numbers. Information about the presence or absence of numbers in the target can help us assess the probability that the target belongs to one of these PHI categories. Presence of numbers in a target is incorporated into the knowledge representation in a similar manner to capitalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.6.">Word length</head><p>Certain entities are characterized by their length, e.g., phone numbers. For each target, we mark its length in terms of characters by setting the vector entry corresponding to its length to one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Syntactic features 6.4.1. Part of speech</head><p>Most PHI instances are more likely to be nouns than adjectives or verbs. We obtain information about the part of speech of words using the Brill tagger <ref type="bibr" target="#b24">[25]</ref>. Brill first uses lexical lookup to assign to each word its most likely part of speech tag; it then refines each tag, as necessary, based on the tags immediately surrounding it.</p><p>In addition to the part of speech of the target, we also consider the parts of speech of the words within a AE2 context window of the target. This information helps us capture some syntactic patterns without fully parsing the text. We include part of speech information in our knowledge representation via a vector that contains entries for all parts of speech present in the training corpus. We mark the part of speech of a target by setting its entry to one and leaving the rest of the part of speech entries in the vector at zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.2.">Syntactic bigrams</head><p>Syntactic bigrams capture the local syntactic dependencies of the target, and we hypothesize that particular types of PHI in discharge summaries occur within similar syntactic structures. For example, patients are often the subject of the passive construction was admitted, e.g., ''John was admitted yesterday''. The same syntactic dependency exists in the sentence ''John, who had hernia, was admitted yesterday'', despite the differences in the immediate lexical context of John and was admitted.</p><p>6.4.2.1. Link Grammar Parser. To extract syntactic dependencies between words, we use the Link Grammar Parser. This parser's computational efficiency, robustness, and explicit representation of syntactic dependencies make it appealing for use even on our fragmented text <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>The Link Grammar Parser models words as blocks with left and right links. This parser imposes local restrictions on the type of links, out of 107 main link types, that a word can have with surrounding words. A successful parse of a sentence satisfies the link requirements of each word in the sentence.</p><p>The Link Grammar Parser has several features that increase robustness in the face of ungrammatical, incomplete, fragmented, or complex sentences. In particular, the lexicon contains generic definitions ''for each of the major parts of speech: noun, verb, adjective, and adverb.'' When the parser encounters a word that does not appear in the lexicon, it replaces the word with each of the generic definitions and attempts to find a valid parse.</p><p>This parser can also be set to enter a less scrupulous ''panic mode'' if a valid parse is not found within a given time limit. The panic mode comes in very handy when text includes fragmented or incomplete utterances; this mode allows the parser to suspend some of the link requirements so that it can output partial parses <ref type="bibr" target="#b6">[7]</ref>. As we are concerned with local context, these partial parses are often sufficient. This structure shows that the verb lives has an Ss connection to its singular subject John on the left and an MVp connection to its modifying preposition with on the right. To use such syntactic dependency information obtained from the Link Grammar Parser, we created a novel representation that captures the syntactic context, i.e., the immediate left and right dependencies, of each word. We refer to this novel representation as ''syntactic n-grams''; syntactic ngrams capture all the words and links within n connections of the target. For example, for the word lives in the parsed sentence, we extract all of its immediate right connections (where a connection is a pair consisting of the link name and the word linked to)--in this case the set {(with, MVp)}. We represent the right syntactic unigrams of the word with this set of connections. For each element of the right unigram set thus extracted, we find all of its immediate right connections--in this case {(brother, Js)}. The right syntactic bigram of the word lives is then {{(with, MVp)}, {(brother, Js)}}. The left syntactic bigram of lives, obtained through a similar process, is {{(LEFT-WALL, Wd)}, {(John, Ss)}}. For words with no left or right links, we create their syntactic bigrams using the two words immediately surrounding them with a link value of NONE. Note that when words have no links, this representation implicitly reverts back to uninterrupted strings of words (which we refer to as lexical ngrams).</p><p>To summarize, the syntactic bigram representation consists of: the right-hand links originating from the target; the words linked to the target through single right-hand links (call this set R 1 ); the righthand links originating from the words in R 1 ; the words connected to the target through two righthand links (call this set R 2 ); the left-hand links originating from the target; the words linked to the target though single left-hand links (call this set L 1 ); the left-hand links originating from the words in L 1 ; and the words linked to the target through two left-hand links (call this set L 2 ). The vector representation of syntactic bigrams sets the entries corresponding to L 1 , R 1 , L 2 , R 2 , and their links to target to one; the rest of the entries are set to zero.</p><p>In our corpus, syntactic bigrams provide stable, meaningful local context. We find that they are particularly useful in eliminating the (sometimes irrelevant) local lexical context often introduced by relative clauses, (modifier) prepositional phrases, and adverbials. Even when local lexical context shows much variation, syntactic bigrams remain stable. For example, consider the sentences ''She lives in Hatfield'', ''She lives by herself in Hatfield'', and ''She lives alone in Hatfield'', which we adopted from actual examples in our data. In these sentences, the lexical bigrams of Hatfield differ; however, in all of them, Hatfield has the left syntactic bigram {{(lives, MVp), {(in, Js)}}. <ref type="foot" target="#foot_3">3</ref>Similarly, in the sentences ''She was taken to Deaconess Hospital'', ''She was taken by car to Deaconess Hospital'', and ''She was taken by his brother to Deaconess Hospital'', lexical local context of Deaconess varies but local syntactic context remains stable. <ref type="foot" target="#foot_4">4</ref>In our corpus, we find that some verbs, e.g., live, admit, discharge, transfer, follow up, etc., have stable local syntactic context which can be relied on even in the presence of much variation in local lexical context. For example, a word that has the left syntactic bigram of {{(follow, MVp), {(on, ON)}} is usually a date; a word that has the left syntactic bigram of {{(follow, MVp), {(with, Js)}} is usually a doctor; and a word that has the left syntactic bigram of {{(follow, MVp)}, {(at, Js)}} is usually a hospital. 5,6  6.5. Semantic features 6.5.1. MeSH ID We use the MeSH ID of the noun phrase containing the target as a feature representing the word. MeSH maps biological terms to descriptors, which are arranged in a hierarchy. There are 15 highlevel categories in MeSH: e.g., A for Anatomy, B for Organism, etc. Each category is divided up to a depth of 11. MeSH descriptors have unique tree numbers which represent their position in this hierarchy. We find the MeSH ID of phrases by shallow parsing the text to identify noun phrases and exhaustively searching each phrase in the UMLS Metathesaurus. We conjecture that this feature will be useful in distinguishing medical non-PHI from PHI: unlike most PHI, medical terms such as diseases, treatments, and tests have MeSH ID's.</p><p>We include the MeSH ID's in our knowledge representation via a vector that contains entries for all MeSH ID's in the training corpus. We mark the MeSH ID of a target by setting its entry to one and leaving the rest of the MeSH ID entries at zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.2.">Dictionary information</head><p>Dictionaries are useful in detecting common PHI. We use information about the presence of the target and of words within a AE2 word window of the target in location, hospital, and name dictionaries. The dictionaries used for this purpose include:</p><p>A dictionary of names, from US Census Bureau <ref type="bibr" target="#b27">[29]</ref>, consisting of:</p><p>One thousand three hundred and fifty-three male first names, including the 100 most com-mon male first names in the US, covering approximately 90% of the US population. Four thousand four hundred and one female first names, including the 100 most common female first names in the US, covering approximately 90% of the US population. Ninety thousand last names, including the 100 most common last names in the US, covering 90% of the US population. A dictionary of locations, from US Census <ref type="bibr" target="#b28">[30]</ref> and from WorldAtlas <ref type="bibr" target="#b29">[31]</ref>, consisting of names of 3606 major towns and cities in New England (the location of the hospital from which the corpora were obtained), in the US, and around the world. And, a dictionary of hospitals, from Douglass <ref type="bibr" target="#b7">[8]</ref>, consisting of names of 369 hospitals in New England.</p><p>We added to these a dictionary of dates, consisting of names and abbreviations of months, e.g., January, Jan, and names of the days of the week. The overlap of these dictionaries with each of our corpora is shown in Table <ref type="table" target="#tab_3">3</ref>. The incorporation of dictionary information into the vector representation has been discussed in Section 6.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.3.">Section headings</head><p>Discharge summaries have a repeating structure that can be exploited by taking into consideration the heading of the section in which the target appears, e.g., HISTORY OF PRESENT ILLNESS. In particular, the headings help determine the types of PHI that appear in the templated parts of the text. For example, dates follow the DISCHARGE DATE heading. The section headings have been incorporated into the feature vector by setting the entry corresponding to the relevant section heading to one and leaving the entries corresponding to the rest of section headings at zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Baseline approaches</head><p>We compared Stat De-id with a scheme that relies heavily on dictionaries and hand-built heuristics <ref type="bibr" target="#b7">[8]</ref>, with Roth and Yih's SNoW <ref type="bibr" target="#b8">[9]</ref>, with BBN's IdentiFinder <ref type="bibr" target="#b9">[10]</ref>, and with our in-house Conditional Random Field De-identifier (CRFD). SNoW, IdentiFinder, and CRFD take into account dependencies of entities with each other and with non-entity tokens in a sentence, i.e., sentential global context, while Stat De-id focuses on each word in the sentence in isolation, using only local context provided by a few surrounding words and the words linked by close syntactic relationships. We chose these baseline schemes to explore the contributions of local and global context to de-identification in clinical narrative text.</p><p>While we cross-validated SNoW and Stat De-id on our corpora, we did not have the trainable version of IdentiFinder available for our use. Thus, we were unable to train this system on the training data used for Stat De-id and SNoW, but had to use it as trained on news corpora. Clearly, this puts IdentiFinder at a relative disadvantage, so our analysis intends not so much to draw conclusions about the relative strengths of these systems but to study the contributions of different features. In order to strengthen our conclusions about contributions of global and local context to de-identification, we compare Stat De-id with CRFD, which adds sentential global context to the features employed by Stat De-id and, like Stat Deid and SNoW, is cross-validated on our corpora.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Heuristic + dictionary scheme</head><p>Most traditional de-identification approaches use dictionaries and hand-tailored heuristics. We obtained one such system that identifies PHI by checking to see if the target words occur in hospital, location, and name dictionaries, but not in a list of common words <ref type="bibr" target="#b7">[8]</ref>. Simple contextual clues, such as titles, e.g., Mr., and manually determined bigrams, e.g., lives in, are also used to identify PHI not occurring in dictionaries. We ran this rule-based system on each of the artificial and authentic corpora. Note that the discharge summaries obtained from the BIDMC had been automatically de-identified by this approach prior to manual scrubbing. The dictionaries used by Stat De-id were identical to the dictionaries of this system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">SNoW</head><p>Roth and Yih's SNoW system <ref type="bibr" target="#b8">[9]</ref> recognizes people, locations, and organizations. This system takes advantage of words in a phrase, surrounding bigrams and trigrams of words, the number of words in the phrase, and information about the presence of the phrase or constituent words in people and location dictionaries to determine the probability distribution of entity types and relationships between the entities in a sentence. This system uses the probability distributions and constraints imposed by relationships on the entity types to compute the most likely assignment of relationships and entities in the sentence. In other words, SNoW uses its beliefs about relationships between entities, i.e., the global context of the sentence, to strengthen or weaken its hypothesis about each entity's type.</p><p>We cross-validated SNoW on each of the artificial and authentic corpora, but only on the entity types it was designed to recognize, i.e., people, locations, and organizations. For each corpus, 10-fold crossvalidation trained SNoW on 90% of the corpus and validated it on the remaining 10%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">IdentiFinder</head><p>IdentiFinder, described in more detail in the Section 2, uses HMMs to find the most likely sequence of entity types in a sentence given a sequence of words. Thus, it uses the global context of the entities in a sentence. IdentiFinder is distributed pre-trained on news corpora. We obtained and used this system out-of-the-box.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.">Conditional Random Field De-identifier (CRFD)</head><p>We built CRFD, a de-identifier based on Conditional Random Fields (CRF) <ref type="bibr" target="#b22">[23]</ref>, which, like SVMs, can handle a very large number of features, but which makes joint inferences over entire sequences. For our purposes, following the example of IdentiFinder, sequences are set to be sentences. CRFD employs exactly the same local context features used by Stat De-id. However, the use of Conditional Random Fields allows this de-identifier to also take into consideration sentential global context while predicting PHI; CRFD finds the optimal sequence of PHI tags over the complete sentence. We use the CRF implementation provided by IIT Bombay <ref type="bibr" target="#b30">[32]</ref> and cross-validate (10fold) CRFD on each of our corpora.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Evaluation methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.">Precision, recall, and F-measure</head><p>We evaluated the de-identification and NER systems on four artificial and one authentic corpora. We evaluated Stat De-id using 10-fold cross-validation; in each round of cross-validation we extracted features only from the training corpus, trained the SVM only on these features, and evaluated performance on a held-out validation set. To compare with the performance of baseline systems, we computed precision, recall, and F-measures for each system. Precision for class x is defined as b/B where b is the number of correctly classified instances of class x and B is the total number of instances classified as class x. Recall for class x is defined as v=V where v is the number of correctly classified instances of x and V is the total number of instances of x in the corpus. The metric that is of most interest in de-identification is recall for PHI. Recall measures the percentage of PHI that is correctly identified and should ideally be very high. We are also interested in maintaining the integrity of the data, i.e., avoiding the classification of non-PHI as PHI. This is captured by precision. In this paper, we also compute Fmeasure, which is the harmonic mean of precision and recall, given by:</p><formula xml:id="formula_4">F ¼ 2 Â precision Â recall precision þ recall (5)</formula><p>and provides a single number that can be used to compare systems. In the biomedical informatics literature, precision is referred to as positive predictive value and recall is referred to as sensitivity.</p><p>In this paper, the purpose of de-identification is to find all PHI and not to distinguish between types of PHI. Therefore, we group the seven PHI classes into a single PHI category, and compute precision and recall for PHI versus non-PHI. In order to study the performance on each PHI type, in Section 10, we present the precision, recall and F-measure for each individual PHI class. More details can be found in Sibanda <ref type="bibr" target="#b31">[33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.">Statistical significance</head><p>Precision, recall, and F-measure represent proportions of populations. In trying to determine the difference in performance of two systems, we therefore employ the z-test on two proportions. We test the significance of the differences in Fmeasures on PHI and the differences in F-measures on non-PHI <ref type="bibr" target="#b32">[34]</ref><ref type="bibr" target="#b33">[35]</ref><ref type="bibr">[36]</ref>.</p><p>Given two system outputs, the null hypothesis is that there is no difference between the two proportions, i.e., H 0 :p 1 = p 2 . The alternate hypothesis states that there is a difference between the two proportions, i.e., H 0 :p 1 6 ¼ p 2 . At the significance level a, the z-statistic is given by:</p><formula xml:id="formula_5">z ¼ p 1 À p 2 ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi pð1 À pÞð1=n 1 þ 1=n 2 Þ p<label>(6)</label></formula><p>where</p><formula xml:id="formula_6">p ¼ n 1 p 1 þ n 2 p 2 n 1 þ n 2<label>(7)</label></formula><p>n 1 and n 2 refer to sample sizes. A z-statistic of AE1.96 means that the difference between the two proportions is significant at a = 0.05. All significance tests in this paper are run at this a.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Results and discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1.">De-identifying random and authentic corpora</head><p>We first de-identified the random and authentic corpora. On the random corpus, Stat De-id significantly outperformed all of IdentiFinder, CRFD, and the heuristic + dictionary baseline. Its F-measure on PHI was 97.63% compared to IdentiFinder's 68.35%, CRFD's 81.55%, and the heuristic + dictionary scheme's 77.82% (see Table <ref type="table" target="#tab_7">4</ref>). <ref type="foot" target="#foot_7">7</ref> We evaluated SNoW only on the three kinds of entities it is designed to recognize. We found that it recognized PHI with an F-measure of 96.39% (see Table <ref type="table" target="#tab_4">5</ref>). In comparison, when evaluated only on the entity types SNoW could recognize, Stat De-id achieved a comparable Fmeasure of 97.46%. On the authentic corpus, Stat De-id significantly outperformed all other systems (see Tables <ref type="table" target="#tab_5">6</ref> and<ref type="table" target="#tab_6">7</ref>). F-measure differences from SNoW were also significant. The superiority of Stat De-id over the heuristic + dictionary approach suggests that using dictionaries with only simple, incomplete contextual clues is not as effective for recognizing PHI. The superiority of Stat De-id over IdentiFinder, CRFD, and SNoW suggest that, on our corpora, a system using (a more complete representation of) local context performs as well as (and sometimes better than) systems using (weaker representations of) local context combined with global context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2.">De-identifying the ambiguous corpus</head><p>Ambiguity of PHI with non-PHI complicates the de-identification process. In particular, a greedy de-identifier that removes all keyword matches to possible PHI would remove Huntington from both the doctor's name, e.g., ''Dr. Huntington'', and the disease name, e.g., ''Huntington's disease''. Conversely, use of common words as PHI, e.g., ''Consult Dr. Test'', may result in inadequate anonymization of some PHI.</p><p>When evaluated on such a challenging data set where some PHI were ambiguous with non-PHI, Stat De-id accurately recognized 94.27% of all PHI: its performance measured in terms of F-measure was significantly better than that of IdentiFinder, SNoW, CRFD, and the heuristic + dictionary scheme on both the complete corpus (see Tables <ref type="table" target="#tab_11">8</ref> and<ref type="table" target="#tab_12">9</ref>) and on only the ambiguous entries in the corpus (see Table <ref type="table" target="#tab_13">10</ref>) for both PHI and non-PHI at a = 0.05. For example, the patient name Camera in ''Camera underwent relaxation to remove mucous plugs.'' is missed by all baseline schemes but is recognized correctly by Stat De-id.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.3.">De-identifying the out-of-vocabulary corpus</head><p>In many cases, discharge summaries contain foreign or misspelled words, i.e., out-of-vocabulary words, as PHI. An approach that simply looks up words in a dictionary of proper nouns may fail to anonymize such PHI. We hypothesized that on the data set Note that these are the only entity types SNoW was built to recognize. The difference in PHI F-measures between SNoW and Stat De-id is not significant at a = 0.05. The difference in non-PHI F-measures is significant at the same a and marked as such with an *.   containing out-of-vocabulary PHI, context would be the key contributor to de-identification. As expected, the heuristic + dictionary method recognized PHI with the lowest F-measures on this data set (see Tables <ref type="table" target="#tab_8">11</ref> and<ref type="table" target="#tab_9">12</ref>). Again, Stat De-id outperformed all other approaches significantly (a = 0.05), obtaining an F-measure of 97.44% for recognizing out-of-vocabulary PHI in our corpus, while IdentiFinder, CRFD, and the heuristic + dic-tionary scheme had F-measures of 53.51%, 80.32%, and 38.71%, respectively (see Table <ref type="table" target="#tab_8">11</ref>).</p><p>Of only the out-of-vocabulary PHI, 96.49% were accurately identified by Stat De-id. In comparison, the heuristic + dictionary approach accurately identified those PHI that could not be found in dictionaries 11.15% of the time, IdentiFinder recognized these PHI 57.33% of the time, CRFD recognized them 84.75% of the time, and SNoW gave an accuracy of 95.08% (see Table <ref type="table" target="#tab_10">13</ref>). For example, the fictitious doctor name Znw was recognized by Stat De-id but missed by all other systems in the sentence ''Labs showed hyperkalemia (increased potassium), . . ., discussed with primary physicians (Znw) and cardiologist (P. Nwnrgo).''      The F-measure differences from Stat De-id in PHI and in non-PHI are significant at a = 0.05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.4.">De-identifying the challenge corpus</head><p>The challenge corpus combines the difficulties posed by out-of-vocabulary and ambiguous PHI.</p><p>Being the largest of our corpora, we expect the results on this corpus to be most reliable. Consistent with our observations on the rest of the corpora, Stat De-id outperformed all other systems significantly (a = 0.05) on the challenge corpus, obtaining an F-measure of 98.03% (see Tables <ref type="table" target="#tab_15">14</ref> and<ref type="table" target="#tab_16">15</ref>). The performance of IdentiFinder on this corpus is the worst (F-measure = 33.20%), followed by the heuristic + dictionary approach (F-measure = 43.95%) and CRFD (F-measure = 85.57%). When evaluated only on the entity types SNoW could recognize, Stat De-id achieved an F-measure of 97.96% in recognizing PHI, significantly outperforming SNoW with Fmeasure = 96.21%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.5.">Feature importance</head><p>To understand the gains of Stat De-id, we determined the relative importance of each feature by running Stat De-id with the following restricted feature sets on the random, authentic, and challenge corpora:</p><p>1. The target words alone.</p><p>2. The syntactic bigrams alone.</p><p>3. The lexical bigrams alone. 4. The part of speech (POS) information alone. 5. The dictionary-based features alone. 6. The MeSH features alone. 7. The orthographic features alone.</p><p>Table <ref type="table" target="#tab_14">16</ref> shows that running Stat De-id only with the target word, i.e., a linear SVM with keywords as A de-identifier for medical discharge summaries 29 For all pairs of features, the differences between F-measures for PHI and the differences between F-measures for non-PHI are significant at a = 0.05. The only exceptions are the difference of F-measures in non-PHI of lexical bigrams and POS information (marked by y ), the difference in F-measures in PHI of MeSH and orthographic features (marked by z ), and the difference in F-measures in non-PHI of MeSH and orthographic features (marked by ).  feature, would give an F-measure of 57% on the random corpus. In comparison, Table <ref type="table" target="#tab_7">4</ref> shows that Stat De-id with the complete feature set gives an Fmeasure of 97%. Similarly, Stat De-id with only the target word gives an F-measure of 80% on the authentic corpus (Table <ref type="table" target="#tab_17">17</ref>). When employed with all of the features, the F-measure rises to 97% (Table <ref type="table" target="#tab_5">6</ref>). Finally, the target word by itself can recognize 65% of PHI in the challenge corpus whereas Stat De-id with the complete feature set gives an F-measure of 98%. The observed improvements on each of the corpora suggest that the features that contribute to a more thorough representation of local context also contribute to more accurate de-identification. Note that keywords are much more useful on the authentic corpus than on the random and challenge corpora. This is because there are more and varied PHI in the random and challenge corpora. In contrast, in the authentic corpus, many person and hospital names repeat, making keywords informative. Regardless of this difference, on both corpora, as the overall feature set improves, so does the performance of Stat De-id.</p><p>The results in Table <ref type="table" target="#tab_14">16</ref> also show that, when used alone, lexical and syntactic bigrams are two of the most useful features for de-identification of the random corpus. The same two features constitute the most useful features for de-identification of the challenge corpus (Table <ref type="table" target="#tab_20">18</ref>). In the authentic corpus (Table <ref type="table" target="#tab_17">17</ref>), target word and syntactic bigrams are the most useful features. All of random, authentic, and challenge corpora highlight the relative importance of local context features; in all three corpora, context is more useful than dictionaries, reflecting the repetitive structure and language of discharge summaries.</p><p>On all three corpora, syntactic bigrams outperform lexical bigrams in recognizing PHI. The F-measure difference between the syntactic and lexical bigrams is significant for PHI on all of the random, challenge, and authentic corpora. Most prior approaches to de-identification/NER have used only lexical bigrams, ignoring syntactic dependencies. Our experiments suggest that syntactic context is more informative than lexical context for the identification of PHI in discharge summaries, even though these records contain fragmented and incomplete utterances.</p><p>We conjecture that the lexical context of PHI is more variable than their syntactic context because many English sentences are filled with clauses, adverbs, etc., that separate the subject from its main verb. The Link Grammar Parser can recognize these interjections so that the words break up lexical context but not syntactic context. For example, the word supposedly gets misclassified by lexical bigrams as PHI when encountered in the sentence ''Trantham, Faye supposedly lives at home with home health aide and uses a motorized wheelchair''. This is because the verb lives which appears on the right-hand-side of supposedly is a strong lexical indicator for PHI. If we parse this sentence with the Link Grammar Parser, we find that the righthand link for the word supposedly is (lives, E) where E is the link for ''verb-modifying adverbs which precede the verb'' <ref type="bibr" target="#b5">[6]</ref>. This link is not an indicator  For all pairs of features, the differences between F-measures for PHI and the differences between F-measures for non-PHI are significant at a = 0.05. The only exceptions are the difference of F-measures in non-PHI of lexical bigrams and syntactic bigrams (marked by y ) and the difference of F-measures in non-PHI of MeSH and orthographic features (marked by z ).</p><p>of patient names and helps mark supposedly as non-PHI.   <ref type="table" target="#tab_22">20</ref> and<ref type="table" target="#tab_23">21</ref> show that Stat Deid performs relatively poorly in recognizing phone and location PHI classes. Of the 88 location words in the authentic corpus, 22% are classified as non-PHI and 16% are classified as hospital names. For example, the location Hollist in the sentence ''The patient lives at home in Hollist with his parents.'' is missed. The errors in the location class arise because there are too few positive examples in the training set to learn the context distinguishing locations. Furthermore, the context for locations often overlaps with the context for hospitals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.6.">Local versus global context</head><p>Of the 32 phone numbers in the authentic corpus, 34% are misclassified as non-PHI. For example, the number 234-907-1924 is missed in the sentence ''DR. JANE DOE (234-907-1924)''. Again, these errors arise because there are too few phone numbers in the training set.</p><p>Despite being person names, patients and doctors are rarely misclassified as each other, although they sometimes do get misclassified as non-PHI. This is because the honorifics used with the patients' and doctors' names help differentiate between the two. However, the honorifics are absent from some of the names. When these names also lack local context, this leaves Stat De-id to its best guess, i.e., non-PHI. For example, the name ''Jn Smth'' which consists of rare tokens and appears on a line all by itself, with no context, gets misclassified as non-PHI. Four percent of patients and 3% of doctors in the authentic corpus are misclassified as non-PHI. These observations generalize to the challenge corpus also.</p><p>Misclassifying patient and doctor names as non-PHI is detrimental for de-identification. Similarly,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>32</head><p>O ¨. Uzuner et al.  Note that, in general, even after de-identification, it may be possible to deduce the identity of individuals mentioned in the records, for example, by combining the de-identified data with other publicly available information or by studying some indirect identifiers that may be mentioned in the records but that do not fall into one of the PHI categories defined by HIPAA <ref type="bibr" target="#b34">[37]</ref>. To further reduce the risk of re-identification, such indirect identifiers may also need to be removed or generalized. Unfortunately, the problem of minimizing data loss by generalizing or removing data is computationally intractable <ref type="bibr" target="#b35">[38]</ref>, so only heuristic methods are typically employed <ref type="bibr" target="#b36">[39]</ref>. The category of doctors is one indirect identifier that is not included in PHI defined by HIPAA but is included in the PHI marked by Stat De-id.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">Conclusions</head><p>In this paper, we have shown that we can de-identify clinical text, characterized by fragmented and incomplete utterances, using local context 94-97% of the time. Our representation of local context is novel; it includes novel syntactic features which provide us with useful linguistic information even when the language of documents is fragmented. The results presented imply that de-identification can be performed even when corpora are dominated by fragmented and incomplete utterances, even when many words in the corpora are ambiguous between PHI and non-PHI, and even when many PHI include out-of-vocabulary terms. Structure and repetitions in the language of documents can be exploited for this purpose.</p><p>Experiments on our corpora suggest that local context plays an important role in de-identification of narratives characterized by fragmented and incomplete utterances. This fact remains true even when the narratives contain uncommon PHI instances that are not present in easily obtainable dictionaries and even when PHI are ambiguous with non-PHI. The more thorough the local context, the better the performance; and strengthening the representation of local context may be more beneficial for de-identification than complementing local with global context. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Much</head><label></label><figDesc>NER work has been inspired by the Message Understanding Conference (MUC) and by the Entity Detection and Tracking task of Automatic Content Extraction (ACE) conference organized by the National Institute of Standards and Technology.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>6. 4 . 2 . 2 .</head><label>422</label><figDesc>Using Link Grammar Parser output as an SVM input. The Link Grammar Parser produces the following structure for the sentence 2 :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>global context. Comparison with a Conditional Random Field De-identifier (CRFD), which utilizes global context in addition to the local context of Stat Deid, confirms this finding (F-measure = 88%) and establishes that strengthening the representation of local context may be more beneficial for de-identification than complementing local with global context. # 2007 Elsevier B.V. All rights reserved.</figDesc><table /><note><p><p><p>14</p>O ¨. Uzuner et al.</p>with</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>Number of words in each PHI category in the corpora</figDesc><table><row><cell>Category</cell><cell>Number of tokens</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Random</cell><cell>Ambiguous</cell><cell>Out-of-vocabulary</cell><cell>Authentic</cell><cell>Challenge</cell></row><row><cell></cell><cell>corpus</cell><cell>corpus</cell><cell>corpus</cell><cell>corpus</cell><cell>corpus</cell></row><row><cell>Non-PHI</cell><cell>17,874</cell><cell>19,275</cell><cell>17,875</cell><cell>112,669</cell><cell>444,127</cell></row><row><cell>Patient</cell><cell>1,048</cell><cell>1,047</cell><cell>1,037</cell><cell>294</cell><cell>1,737</cell></row><row><cell>Doctor</cell><cell>311</cell><cell>311</cell><cell>302</cell><cell>738</cell><cell>7,697</cell></row><row><cell>Location</cell><cell>24</cell><cell>24</cell><cell>24</cell><cell>88</cell><cell>518</cell></row><row><cell>Hospital</cell><cell>600</cell><cell>600</cell><cell>404</cell><cell>656</cell><cell>5,204</cell></row><row><cell>Date</cell><cell>735</cell><cell>736</cell><cell>735</cell><cell>1,953</cell><cell>7,651</cell></row><row><cell>ID</cell><cell>36</cell><cell>36</cell><cell>36</cell><cell>482</cell><cell>5,110</cell></row><row><cell>Phone</cell><cell>39</cell><cell>39</cell><cell>39</cell><cell>32</cell><cell>271</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>Distribution of words, i.e., tokens, that are ambiguous between PHI and non-PHI Categories of PHI are often characterized by local context. For example, the word Dr. before a name invariably suggests that the name is that of a doctor.</figDesc><table><row><cell>Category</cell><cell>Number of</cell><cell>Number of</cell></row><row><cell></cell><cell>ambiguous</cell><cell>ambiguous</cell></row><row><cell></cell><cell>tokens in the</cell><cell>tokens in the</cell></row><row><cell></cell><cell>ambiguous</cell><cell>challenge</cell></row><row><cell></cell><cell>corpus</cell><cell>corpus</cell></row><row><cell>Non-PHI</cell><cell>3,787</cell><cell>39,374</cell></row><row><cell>Patient</cell><cell>514</cell><cell>158</cell></row><row><cell>Doctor</cell><cell>247</cell><cell>1,083</cell></row><row><cell>Location</cell><cell>24</cell><cell>44</cell></row><row><cell>Hospital</cell><cell>86</cell><cell>1,910</cell></row><row><cell>Date</cell><cell>201</cell><cell>81</cell></row><row><cell>ID</cell><cell>0</cell><cell>4</cell></row><row><cell>Phone</cell><cell>0</cell><cell>1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc>Percentage of words that appear in name, location, hospital, and month dictionaries used by Stat De-id and by the heuristic + dictionary approach</figDesc><table><row><cell>Corpus</cell><cell>Patients</cell><cell>Doctors</cell><cell>Locations</cell><cell>Hospitals</cell><cell>Dates</cell><cell>Non-PHI</cell><cell>Non-PHI in</cell><cell>Non-PHI in</cell><cell>Non-PHI</cell></row><row><cell></cell><cell>in names</cell><cell>in names</cell><cell>in location</cell><cell>in hospital</cell><cell>in month</cell><cell>in names</cell><cell>location</cell><cell>hospitals</cell><cell>in month</cell></row><row><cell></cell><cell>dict. (%)</cell><cell>dict. (%)</cell><cell>dict. (%)</cell><cell>dict. (%)</cell><cell>dict. (%)</cell><cell>dict. (%)</cell><cell>dict. (%)</cell><cell>dict. (%)</cell><cell>dict. (%)</cell></row><row><cell>Random</cell><cell>86.45</cell><cell>86.50</cell><cell>87.5</cell><cell>87.5</cell><cell>12.65</cell><cell>15.87</cell><cell>9.19</cell><cell>14.10</cell><cell>0.07</cell></row><row><cell cols="2">Authentic 78.57</cell><cell>70.33</cell><cell>54.55</cell><cell>80.18</cell><cell>21.97</cell><cell>16.12</cell><cell>10.19</cell><cell>12.74</cell><cell>0.02</cell></row><row><cell cols="2">Ambiguous 86.53</cell><cell>86.50</cell><cell>100</cell><cell>87.5</cell><cell>12.64</cell><cell>19.53</cell><cell>10.50</cell><cell>14.03</cell><cell>0.08</cell></row><row><cell>OoV</cell><cell>2.51</cell><cell>1.99</cell><cell>0</cell><cell>19.56</cell><cell>12.65</cell><cell>15.87</cell><cell>9.19</cell><cell>14.10</cell><cell>0.07</cell></row><row><cell cols="2">Challenge 14.10</cell><cell>17.20</cell><cell>11.40</cell><cell>26.59</cell><cell>5.15</cell><cell>15.36</cell><cell>11.32</cell><cell>8.61</cell><cell>0.06</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5</head><label>5</label><figDesc>Evaluation of SNoW and Stat De-id on recognizing people, locations, and organizations found in the random corpus</figDesc><table><row><cell>Method</cell><cell>Class</cell><cell>Precision</cell><cell>Recall</cell><cell>F-measure</cell></row><row><cell></cell><cell></cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell></row><row><cell cols="2">Stat De-id PHI</cell><cell>98.31</cell><cell cols="2">96.62 97.46</cell></row><row><cell>SNoW</cell><cell>PHI</cell><cell>95.18</cell><cell>97.63</cell><cell>96.39</cell></row><row><cell cols="3">Stat De-id Non-PHI 99.64</cell><cell cols="2">99.82 99.73</cell></row><row><cell>SNoW</cell><cell cols="2">Non-PHI 99.75</cell><cell>99.48</cell><cell>99.61 *</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6</head><label>6</label><figDesc>Evaluation on authentic discharge summaries</figDesc><table><row><cell>Method</cell><cell>Class</cell><cell>Precision</cell><cell>Recall</cell><cell>F-measure</cell></row><row><cell></cell><cell></cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell></row><row><cell cols="2">Stat De-id PHI</cell><cell>98.46</cell><cell cols="2">95.24 96.82</cell></row><row><cell>IFinder</cell><cell>PHI</cell><cell>26.17</cell><cell>61.98</cell><cell>36.80 *</cell></row><row><cell>H + D</cell><cell>PHI</cell><cell>82.67</cell><cell>87.30</cell><cell>84.92 *</cell></row><row><cell>CRFD</cell><cell>PHI</cell><cell>91.16</cell><cell>84.75</cell><cell>87.83 *</cell></row><row><cell cols="3">Stat De-id Non-PHI 99.84</cell><cell cols="2">99.95 99.90</cell></row><row><cell>IFinder</cell><cell cols="2">Non-PHI 98.68</cell><cell>94.19</cell><cell>96.38 *</cell></row><row><cell>H + D</cell><cell cols="2">Non-PHI 99.58</cell><cell>99.39</cell><cell>99.48 *</cell></row><row><cell>CRFD</cell><cell cols="2">Non-PHI 99.62</cell><cell>99.86</cell><cell>99.74 *</cell></row><row><cell cols="5">The F-measure differences from Stat De-id in PHI and in non-</cell></row><row><cell cols="3">PHI are significant at a = 0.05.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7</head><label>7</label><figDesc>Evaluation of SNoW and Stat De-id on authentic discharge summaries</figDesc><table><row><cell>Method</cell><cell>Class</cell><cell>Precision</cell><cell>Recall</cell><cell>F-measure</cell></row><row><cell></cell><cell></cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell></row><row><cell cols="2">Stat De-id PHI</cell><cell>98.40</cell><cell cols="2">93.75 96.02</cell></row><row><cell>SNoW</cell><cell>PHI</cell><cell>96.36</cell><cell>91.03</cell><cell>93.62 *</cell></row><row><cell cols="3">Stat De-id Non-PHI 99.90</cell><cell cols="2">99.98 99.94</cell></row><row><cell>SNoW</cell><cell cols="2">Non-PHI 99.86</cell><cell>99.95</cell><cell>99.90 *</cell></row></table><note><p>The F-measure differences from Stat De-id in PHI and in non-PHI are significant at a = 0.05.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4</head><label>4</label><figDesc>Precision, recall, and F-measure on random corpus IFinder refers to IdentiFinder and H + D refers to heuristic + dictionary approach. Highest F-measures are in bold. The Fmeasure differences from Stat De-id, in PHI and in non-PHI, that are significant at a = 0.05 are marked with an *.</figDesc><table><row><cell>Method</cell><cell>Class</cell><cell>Precision</cell><cell>Recall</cell><cell>F-measure</cell></row><row><cell></cell><cell></cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell></row><row><cell cols="2">Stat De-id PHI</cell><cell>98.34</cell><cell cols="2">96.92 97.63</cell></row><row><cell>IFinder</cell><cell>PHI</cell><cell>62.21</cell><cell>75.83</cell><cell>68.35 *</cell></row><row><cell>H + D</cell><cell>PHI</cell><cell>93.67</cell><cell>66.56</cell><cell>77.82 *</cell></row><row><cell>CRFD</cell><cell>PHI</cell><cell>81.94</cell><cell>81.17</cell><cell>81.55 *</cell></row><row><cell cols="3">Stat De-id Non-PHI 99.53</cell><cell cols="2">99.75 99.64</cell></row><row><cell>IFinder</cell><cell cols="2">Non-PHI 96.15</cell><cell>92.92</cell><cell>94.51 *</cell></row><row><cell>H + D</cell><cell cols="2">Non-PHI 95.07</cell><cell>99.31</cell><cell>97.14 *</cell></row><row><cell>CRFD</cell><cell cols="2">Non-PHI 98.91</cell><cell>99.05</cell><cell>98.98 *</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 11</head><label>11</label><figDesc>Evaluation on the out-of-vocabulary corpus</figDesc><table><row><cell>Method</cell><cell>Class</cell><cell>Precision</cell><cell>Recall</cell><cell>F-measure</cell></row><row><cell></cell><cell></cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell></row><row><cell cols="2">Stat De-id PHI</cell><cell>98.12</cell><cell cols="2">96.77 97.44</cell></row><row><cell>IFinder</cell><cell>PHI</cell><cell>52.44</cell><cell>54.62</cell><cell>53.51 *</cell></row><row><cell>H + D</cell><cell>PHI</cell><cell>88.24</cell><cell>24.79</cell><cell>38.71 *</cell></row><row><cell>CRFD</cell><cell>PHI</cell><cell>82.01</cell><cell>78.71</cell><cell>80.32 *</cell></row><row><cell cols="3">Stat De-id Non-PHI 99.54</cell><cell cols="2">99.74 99.64</cell></row><row><cell>IFinder</cell><cell cols="2">Non-PHI 93.52</cell><cell>92.97</cell><cell>93.25 *</cell></row><row><cell>H + D</cell><cell cols="2">Non-PHI 90.32</cell><cell>99.53</cell><cell>94.70 *</cell></row><row><cell>CRFD</cell><cell cols="2">Non-PHI 98.43</cell><cell>99.01</cell><cell>98.72 *</cell></row><row><cell cols="5">The F-measure differences from Stat De-id in PHI and in non-</cell></row><row><cell cols="3">PHI are significant at a = 0.05.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 12</head><label>12</label><figDesc>Evaluation of SNoW and Stat De-id on the people, locations, and organizations found in the outof-vocabulary corpus</figDesc><table><row><cell>Method</cell><cell>Class</cell><cell>Precision</cell><cell>Recall</cell><cell>F-measure</cell></row><row><cell></cell><cell></cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell></row><row><cell cols="2">Stat De-id PHI</cell><cell>98.04</cell><cell cols="2">96.49 97.26</cell></row><row><cell>SNoW</cell><cell>PHI</cell><cell>96.50</cell><cell>95.08</cell><cell>95.78 *</cell></row><row><cell cols="3">Stat De-id Non-PHI 99.67</cell><cell cols="2">99.82 99.74</cell></row><row><cell>SNoW</cell><cell cols="2">Non-PHI 99.53</cell><cell>99.67</cell><cell>99.60 *</cell></row><row><cell cols="5">The F-measure differences from Stat De-id in PHI and in non-</cell></row><row><cell cols="3">PHI are significant at a = 0.05.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 13</head><label>13</label><figDesc>Recall on only the out-of-vocabulary PHI</figDesc><table><row><cell>Method</cell><cell>Recall (%)</cell></row><row><cell>Stat De-id</cell><cell>96.49</cell></row><row><cell>IFinder</cell><cell>57.33 *</cell></row><row><cell>SNoW</cell><cell>95.08 *</cell></row><row><cell>H + D</cell><cell>11.15 *</cell></row><row><cell>CRFD</cell><cell>84.75 *</cell></row><row><cell cols="2">Highest recall is in bold. The differences from Stat De-id are</cell></row><row><cell>significant at a = 0.05.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8</head><label>8</label><figDesc>Evaluation on the corpus containing ambiguous data</figDesc><table><row><cell>Method</cell><cell>Class</cell><cell>Precision</cell><cell>Recall</cell><cell>F-measure</cell></row><row><cell></cell><cell></cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell></row><row><cell cols="2">Stat De-id PHI</cell><cell>96.37</cell><cell cols="2">94.27 95.31</cell></row><row><cell>IFinder</cell><cell>PHI</cell><cell>45.52</cell><cell>69.04</cell><cell>54.87 *</cell></row><row><cell>H + D</cell><cell>PHI</cell><cell>79.69</cell><cell>44.25</cell><cell>56.90 *</cell></row><row><cell>CRFD</cell><cell>PHI</cell><cell>81.84</cell><cell>78.08</cell><cell>79.92 *</cell></row><row><cell cols="3">Stat De-id Non-PHI 99.18</cell><cell cols="2">99.49 99.34</cell></row><row><cell>IFinder</cell><cell cols="2">Non-PHI 95.23</cell><cell>88.22</cell><cell>91.59 *</cell></row><row><cell>H + D</cell><cell cols="2">Non-PHI 92.52</cell><cell>98.39</cell><cell>95.36 *</cell></row><row><cell>CRFD</cell><cell cols="2">Non-PHI 98.12</cell><cell>98.78</cell><cell>98.45 *</cell></row><row><cell cols="5">The F-measure differences from Stat De-id in PHI and in non-</cell></row><row><cell cols="3">PHI are significant at a = 0.05.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9</head><label>9</label><figDesc>Evaluation of SNoW and Stat De-id on ambiguous data</figDesc><table><row><cell>Method</cell><cell>Class</cell><cell>Precision</cell><cell>Recall</cell><cell>F-measure</cell></row><row><cell></cell><cell></cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell></row><row><cell cols="2">Stat De-id PHI</cell><cell>95.75</cell><cell cols="2">93.24 94.48</cell></row><row><cell>SNoW</cell><cell>PHI</cell><cell>92.93</cell><cell>91.57</cell><cell>92.24 *</cell></row><row><cell cols="3">Stat De-id Non-PHI 99.33</cell><cell cols="2">99.59 99.46</cell></row><row><cell>SNoW</cell><cell cols="2">Non-PHI 99.17</cell><cell>99.31</cell><cell>99.24 *</cell></row><row><cell cols="5">The F-measure differences from Stat De-id in PHI and in non-</cell></row><row><cell cols="3">PHI are significant at a = 0.05.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10</head><label>10</label><figDesc>Evaluation only on ambiguous people, locations, and organizations found in ambiguous data</figDesc><table><row><cell>Method</cell><cell>Class</cell><cell>Precision</cell><cell>Recall</cell><cell>F-measure</cell></row><row><cell></cell><cell></cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell></row><row><cell cols="2">Stat De-id PHI</cell><cell>94.02</cell><cell cols="2">92.08 93.04</cell></row><row><cell>IFinder</cell><cell>PHI</cell><cell>50.26</cell><cell>67.16</cell><cell>57.49 *</cell></row><row><cell>H + D</cell><cell>PHI</cell><cell>58.35</cell><cell>30.08</cell><cell>39.70 *</cell></row><row><cell>SNoW</cell><cell>PHI</cell><cell>91.80</cell><cell>87.83</cell><cell>89.77 *</cell></row><row><cell>CRFD</cell><cell>PHI</cell><cell>74.15</cell><cell>71.15</cell><cell>72.62 *</cell></row><row><cell cols="3">Stat De-id Non-PHI 98.28</cell><cell cols="2">98.72 98.50</cell></row><row><cell>IFinder</cell><cell cols="2">Non-PHI 92.26</cell><cell>85.48</cell><cell>88.74 *</cell></row><row><cell>H + D</cell><cell cols="2">Non-PHI 86.19</cell><cell>95.31</cell><cell>90.52 *</cell></row><row><cell>SNoW</cell><cell cols="2">Non-PHI 97.34</cell><cell>98.27</cell><cell>97.80 *</cell></row><row><cell>CRFD</cell><cell cols="2">Non-PHI 95.84</cell><cell>96.89</cell><cell>96.37 *</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 16</head><label>16</label><figDesc>Comparison of features for random corpus</figDesc><table><row><cell>Feature</cell><cell>Class</cell><cell>Precision (%)</cell><cell>Recall (%)</cell><cell>F-measure (%)</cell></row><row><cell>Target words</cell><cell>Non-PHI</cell><cell>91.61</cell><cell>98.95</cell><cell>95.14</cell></row><row><cell></cell><cell>PHI</cell><cell>86.26</cell><cell>42.03</cell><cell>56.52</cell></row><row><cell>Lexical bigrams</cell><cell>Non-PHI</cell><cell>95.61</cell><cell>98.10</cell><cell>96.84 y</cell></row><row><cell></cell><cell>PHI</cell><cell>85.43</cell><cell>71.14</cell><cell>77.63</cell></row><row><cell>Syntactic bigrams</cell><cell>Non-PHI</cell><cell>96.96</cell><cell>98.72</cell><cell>97.83</cell></row><row><cell></cell><cell>PHI</cell><cell>90.76</cell><cell>80.20</cell><cell>85.15</cell></row><row><cell>POS information</cell><cell>Non-PHI</cell><cell>94.85</cell><cell>98.38</cell><cell>96.58 y</cell></row><row><cell></cell><cell>PHI</cell><cell>86.38</cell><cell>65.84</cell><cell>74.73</cell></row><row><cell>Dictionary</cell><cell>Non-PHI</cell><cell>88.99</cell><cell>99.26</cell><cell>93.85</cell></row><row><cell></cell><cell>PHI</cell><cell>81.92</cell><cell>21.41</cell><cell>33.95</cell></row><row><cell>MeSH</cell><cell>Non-PHI</cell><cell>86.49</cell><cell>100</cell><cell>92.75</cell></row><row><cell></cell><cell>PHI</cell><cell>0</cell><cell>0</cell><cell>0z</cell></row><row><cell>Orthographic</cell><cell>Non-PHI</cell><cell>86.49</cell><cell>100</cell><cell>92.75</cell></row><row><cell></cell><cell>PHI</cell><cell>0</cell><cell>0</cell><cell>0z</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 14</head><label>14</label><figDesc>Evaluation on the challenge corpus</figDesc><table><row><cell>Method</cell><cell>Class</cell><cell>Precision</cell><cell>Recall</cell><cell>F-measure</cell></row><row><cell></cell><cell></cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell></row><row><cell>Stat De-id</cell><cell>PHI</cell><cell>98.69</cell><cell>97.37</cell><cell>98.03</cell></row><row><cell>IFinder</cell><cell>PHI</cell><cell>25.10</cell><cell>49.10</cell><cell>33.20 *</cell></row><row><cell>H + D</cell><cell>PHI</cell><cell>36.24</cell><cell>55.84</cell><cell>43.95 *</cell></row><row><cell>CRFD</cell><cell>PHI</cell><cell>86.37</cell><cell>84.79</cell><cell>85.57 *</cell></row><row><cell>Stat De-id</cell><cell>Non-PHI</cell><cell>99.83</cell><cell>99.92</cell><cell>99.86</cell></row><row><cell>IFinder</cell><cell>Non-PHI</cell><cell>97.25</cell><cell>92.47</cell><cell>94.80 *</cell></row><row><cell>H + D</cell><cell>Non-PHI</cell><cell>97.67</cell><cell>94.95</cell><cell>96.29 *</cell></row><row><cell>CRFD</cell><cell>Non-PHI</cell><cell>99.55</cell><cell>99.65</cell><cell>99.60 *</cell></row><row><cell cols="5">The F-measure differences from Stat De-id in PHI and in non-PHI</cell></row><row><cell cols="2">are significant at a = 0.05.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 15</head><label>15</label><figDesc>Evaluation of SNoW and Stat De-id on the people, locations, and organizations found in the challenge corpus</figDesc><table><row><cell>Method</cell><cell>Class</cell><cell>Precision</cell><cell>Recall</cell><cell>F-measure</cell></row><row><cell></cell><cell></cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell></row><row><cell>Stat De-id</cell><cell>PHI</cell><cell>98.98</cell><cell>96.96</cell><cell>97.96</cell></row><row><cell>SNoW</cell><cell>PHI</cell><cell>98.73</cell><cell>93.81</cell><cell>96.21 *</cell></row><row><cell>Stat De-id</cell><cell>Non-PHI</cell><cell>99.90</cell><cell>99.97</cell><cell>99.93</cell></row><row><cell>SNoW</cell><cell>Non-PHI</cell><cell>99.80</cell><cell>99.96</cell><cell>99.88 *</cell></row><row><cell cols="5">The F-measure differences from Stat De-id in PHI and in non-PHI</cell></row><row><cell cols="2">are significant at a = 0.05.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 17</head><label>17</label><figDesc>Comparison of features for authentic corpus</figDesc><table><row><cell>Feature</cell><cell>Class</cell><cell>Precision (%)</cell><cell>Recall (%)</cell><cell>F-measure (%)</cell></row><row><cell>Target words</cell><cell>Non-PHI</cell><cell>98.79</cell><cell>99.94</cell><cell>99.36</cell></row><row><cell></cell><cell>PHI</cell><cell>97.64</cell><cell>67.38</cell><cell>79.74</cell></row><row><cell>Lexical bigrams</cell><cell>Non-PHI</cell><cell>98.46</cell><cell>99.83</cell><cell>99.14 y</cell></row><row><cell></cell><cell>PHI</cell><cell>92.75</cell><cell>58.47</cell><cell>71.73</cell></row><row><cell>Syntactic bigrams</cell><cell>Non-PHI</cell><cell>98.55</cell><cell>99.87</cell><cell>99.21 y</cell></row><row><cell></cell><cell>PHI</cell><cell>94.66</cell><cell>60.97</cell><cell>74.17</cell></row><row><cell>POS information</cell><cell>Non-PHI</cell><cell>97.95</cell><cell>99.63</cell><cell>98.78</cell></row><row><cell></cell><cell>PHI</cell><cell>81.99</cell><cell>44.64</cell><cell>57.81</cell></row><row><cell>Dictionary</cell><cell>Non-PHI</cell><cell>97.11</cell><cell>99.89</cell><cell>98.48</cell></row><row><cell></cell><cell>PHI</cell><cell>88.11</cell><cell>21.14</cell><cell>34.10</cell></row><row><cell>MeSH</cell><cell>Non-PHI</cell><cell>96.37</cell><cell>100</cell><cell>98.15 z</cell></row><row><cell></cell><cell>PHI</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>Orthographic</cell><cell>Non-PHI</cell><cell>96.39</cell><cell>99.92</cell><cell>98.12 z</cell></row><row><cell></cell><cell>PHI</cell><cell>22.03</cell><cell>0.61</cell><cell>1.19</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 19</head><label>19</label><figDesc></figDesc><table><row><cell>shows that the local context features of</cell></row><row><cell>SNoW and IdentiFinder are also quite powerful.</cell></row><row><cell>When employed with the same learning algorithm</cell></row><row><cell>utilized by Stat De-id, the individual local feature</cell></row><row><cell>sets of SNoW and IdentiFinder give performance F-</cell></row><row><cell>measures above 86%. Note that Stat De-id's local</cell></row><row><cell>context outperforms the local context of SNoW and</cell></row><row><cell>IdentiFinder on all corpora. This result supports the</cell></row><row><cell>hypothesis that developing a more thorough repre-</cell></row><row><cell>sentation of local context can benefit de-identifica-</cell></row><row><cell>tion. Our experiments with CRFD were designed</cell></row><row><cell>specifically to address the relative value of</cell></row><row><cell>improved local versus global context features.</cell></row><row><cell>Our results show that a CRF-based system using</cell></row><row><cell>exactly the same local context feature set as Stat</cell></row><row><cell>De-id performs significantly worse than Stat De-id on</cell></row><row><cell>all corpora (Tables 4, 6, 8, 10, 11, 13 and 14). Thus,</cell></row><row><cell>on our corpora and for CRFD, attending to global</cell></row><row><cell>consistency in addition to a rich set of local features</cell></row><row><cell>actually hurts performance. This strongly supports</cell></row><row><cell>our hypothesis that improved local features are</cell></row><row><cell>dominant for de-identification.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 18</head><label>18</label><figDesc>Comparison of features for challenge corpus</figDesc><table><row><cell>Feature</cell><cell>Class</cell><cell>Precision (%)</cell><cell>Recall (%)</cell><cell>F-measure (%)</cell></row><row><cell>Target words</cell><cell>Non-PHI</cell><cell>96.90</cell><cell>99.87</cell><cell>98.36</cell></row><row><cell></cell><cell>PHI</cell><cell>96.05</cell><cell>49.56</cell><cell>65.38</cell></row><row><cell>Lexical bigrams</cell><cell>Non-PHI</cell><cell>97.34</cell><cell>99.69</cell><cell>98.50</cell></row><row><cell></cell><cell>PHI</cell><cell>91.99</cell><cell>56.87</cell><cell>70.29</cell></row><row><cell>Syntactic bigrams</cell><cell>Non-PHI</cell><cell>97.50</cell><cell>99.74</cell><cell>98.61</cell></row><row><cell></cell><cell>PHI</cell><cell>93.44</cell><cell>59.61</cell><cell>72.79</cell></row><row><cell>POS information</cell><cell>Non-PHI</cell><cell>96.04</cell><cell>99.42</cell><cell>97.70</cell></row><row><cell></cell><cell>PHI</cell><cell>79.33</cell><cell>35.24</cell><cell>48.80</cell></row><row><cell>Dictionary</cell><cell>Non-PHI</cell><cell>94.26</cell><cell>99.90</cell><cell>96.99</cell></row><row><cell></cell><cell>PHI</cell><cell>69.70</cell><cell>3.79</cell><cell>7.19</cell></row><row><cell>MeSH</cell><cell>Non-PHI</cell><cell>94.05</cell><cell>100</cell><cell>96.93</cell></row><row><cell></cell><cell>PHI</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>Orthographic</cell><cell>Non-PHI</cell><cell>96.05</cell><cell>99.60</cell><cell>97.79</cell></row><row><cell></cell><cell>PHI</cell><cell>84.67</cell><cell>35.30</cell><cell>49.83</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 19</head><label>19</label><figDesc>Comparison of local context feature sets of Stat De-id, SNoW, and IdentiFinder, evaluated individually, with SVMs, on each of the corpora All differences from the corresponding All Stat De-id F-measures are significant at a = 0.05 and marked as such with an *.</figDesc><table><row><cell>Corpus</cell><cell>Feature</cell><cell>Class</cell><cell>Precision</cell><cell>Recall</cell><cell>F-measure</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell></row><row><cell>Random</cell><cell>All Stat De-id/All CRFD</cell><cell>Non-PHI</cell><cell>99.53</cell><cell>99.75</cell><cell>99.64</cell></row><row><cell></cell><cell></cell><cell>PHI</cell><cell>98.34</cell><cell>96.92</cell><cell>97.63</cell></row><row><cell></cell><cell>All local context features of SNoW</cell><cell>Non-PHI</cell><cell>98.79</cell><cell>99.36</cell><cell>99.08 *</cell></row><row><cell></cell><cell></cell><cell>PHI</cell><cell>95.76</cell><cell>92.23</cell><cell>93.96 *</cell></row><row><cell></cell><cell>All local context features of IdentiFinder</cell><cell>Non-PHI</cell><cell>99.35</cell><cell>99.18</cell><cell>99.27 *</cell></row><row><cell></cell><cell></cell><cell>PHI</cell><cell>94.33</cell><cell>95.85</cell><cell>95.33 *</cell></row><row><cell>Authentic</cell><cell>All Stat De-id/All CRFD</cell><cell>Non-PHI</cell><cell>99.84</cell><cell>99.95</cell><cell>99.90</cell></row><row><cell></cell><cell></cell><cell>PHI</cell><cell>98.46</cell><cell>95.24</cell><cell>96.82</cell></row><row><cell></cell><cell>All local context features of SNoW</cell><cell>Non-PHI</cell><cell>99.72</cell><cell>99.94</cell><cell>99.83 *</cell></row><row><cell></cell><cell></cell><cell>PHI</cell><cell>98.42</cell><cell>92.67</cell><cell>95.46 *</cell></row><row><cell></cell><cell>All local context features of IdentiFinder</cell><cell>Non-PHI</cell><cell>99.66</cell><cell>99.92</cell><cell>99.79 *</cell></row><row><cell></cell><cell></cell><cell>PHI</cell><cell>97.75</cell><cell>91.04</cell><cell>94.28 *</cell></row><row><cell>Ambiguous</cell><cell>All Stat De-id/All CRFD</cell><cell>Non-PHI</cell><cell>99.18</cell><cell>99.49</cell><cell>99.34</cell></row><row><cell></cell><cell></cell><cell>PHI</cell><cell>96.37</cell><cell>94.27</cell><cell>95.31</cell></row><row><cell></cell><cell>All local context features of SNoW</cell><cell>Non-PHI</cell><cell>98.15</cell><cell>98.98</cell><cell>98.56 *</cell></row><row><cell></cell><cell></cell><cell>PHI</cell><cell>92.51</cell><cell>87.11</cell><cell>89.73 *</cell></row><row><cell></cell><cell>All local context features of IdentiFinder</cell><cell>Non-PHI</cell><cell>97.62</cell><cell>98.49</cell><cell>98.05 *</cell></row><row><cell></cell><cell></cell><cell>PHI</cell><cell>88.86</cell><cell>83.42</cell><cell>86.06 *</cell></row><row><cell>OoV</cell><cell>All Stat De-id/All CRFD</cell><cell>Non-PHI</cell><cell>99.54</cell><cell>99.74</cell><cell>99.64</cell></row><row><cell></cell><cell></cell><cell>PHI</cell><cell>98.12</cell><cell>96.77</cell><cell>97.44</cell></row><row><cell></cell><cell>All local context features of SNoW</cell><cell>Non-PHI</cell><cell>98.95</cell><cell>99.45</cell><cell>99.20 *</cell></row><row><cell></cell><cell></cell><cell>PHI</cell><cell>96.10</cell><cell>92.67</cell><cell>94.33 *</cell></row><row><cell></cell><cell>All local context features of IdentiFinder</cell><cell>Non-PHI</cell><cell>99.12</cell><cell>99.17</cell><cell>99.14 *</cell></row><row><cell></cell><cell></cell><cell>PHI</cell><cell>94.23</cell><cell>93.87</cell><cell>94.05 *</cell></row><row><cell>Challenge</cell><cell>All Stat De-id/All CRFD</cell><cell>Non-PHI</cell><cell>99.83</cell><cell>99.92</cell><cell>99.86</cell></row><row><cell></cell><cell></cell><cell>PHI</cell><cell>98.69</cell><cell>97.37</cell><cell>98.03</cell></row><row><cell></cell><cell>All local context features of SNoW</cell><cell>Non-PHI</cell><cell>99.50</cell><cell>99.86</cell><cell>99.68 *</cell></row><row><cell></cell><cell></cell><cell>PHI</cell><cell>97.72</cell><cell>92.14</cell><cell>94.85 *</cell></row><row><cell></cell><cell>All local context features of IdentiFinder</cell><cell>Non-PHI</cell><cell>99.50</cell><cell>99.89</cell><cell>99.70 *</cell></row><row><cell></cell><cell></cell><cell>PHI</cell><cell>98.21</cell><cell>92.15</cell><cell>95.08 *</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 20</head><label>20</label><figDesc>Multi-class classification results for Stat De-id on authentic and challenge corpora phone numbers as non-PHI can cause serious privacy concerns as today's technology allows us to cross-reference a single phone number with a search engine, e.g., Google, and link it directly with individuals. To minimize the risk of revealing PHI and of easy re-identification, we plan to improve performance on PHI with stereotypical formats, such as names, phone numbers, social security numbers, medical record numbers, dates, and addresses, by enhancing Stat De-id with the patterns employed by rule-based systems, not to make a final determination of whether something matching the pattern is PHI, but as an additional input feature. Such features can be drawn from available dictionaries of names, places, etc., to augment what can be learned automatically from labeled corpora.</figDesc><table><row><cell>Class</cell><cell>Precision (%)</cell><cell></cell><cell>Recall (%)</cell><cell></cell><cell>F-measure (%)</cell><cell></cell></row><row><cell></cell><cell>Authentic</cell><cell>Challenge</cell><cell>Authentic</cell><cell>Challenge</cell><cell>Authentic</cell><cell>Challenge</cell></row><row><cell>Non-PHI</cell><cell>99.84</cell><cell>99.83</cell><cell>99.94</cell><cell>99.92</cell><cell>99.89</cell><cell>99.88</cell></row><row><cell>Patient</cell><cell>98.94</cell><cell>97.17</cell><cell>95.24</cell><cell>96.72</cell><cell>97.05</cell><cell>96.94</cell></row><row><cell>Doctor</cell><cell>98.48</cell><cell>98.64</cell><cell>96.34</cell><cell>97.37</cell><cell>97.40</cell><cell>98.00</cell></row><row><cell>Location</cell><cell>92.73</cell><cell>91.98</cell><cell>57.95</cell><cell>75.29</cell><cell>71.33</cell><cell>82.80</cell></row><row><cell>Hospital</cell><cell>94.15</cell><cell>98.63</cell><cell>90.70</cell><cell>96.58</cell><cell>92.39</cell><cell>97.59</cell></row><row><cell>Date</cell><cell>98.23</cell><cell>97.23</cell><cell>96.83</cell><cell>96.86</cell><cell>97.52</cell><cell>97.04</cell></row><row><cell>ID</cell><cell>98.16</cell><cell>98.51</cell><cell>99.38</cell><cell>98.53</cell><cell>98.76</cell><cell>98.52</cell></row><row><cell>Phone</cell><cell>90.48</cell><cell>97.84</cell><cell>59.38</cell><cell>83.76</cell><cell>71.70</cell><cell>90.26</cell></row></table><note><p>misclassifying</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 21</head><label>21</label><figDesc>Multi-class confusion matrix for Stat De-id on authentic corpus</figDesc><table><row><cell>Actual</cell><cell>Predicted</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Non-PHI</cell><cell>Patient</cell><cell>Doctor</cell><cell>Location</cell><cell>Hospital</cell><cell>Date</cell><cell>ID</cell><cell>Phone</cell></row><row><cell>Non-PHI</cell><cell>112,605</cell><cell>2</cell><cell>4</cell><cell>0</cell><cell>17</cell><cell>33</cell><cell>8</cell><cell>0</cell></row><row><cell>Patient</cell><cell>12</cell><cell>280</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1</cell></row><row><cell>Doctor</cell><cell>24</cell><cell>1</cell><cell>711</cell><cell>0</cell><cell>2</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>Location</cell><cell>19</cell><cell>0</cell><cell>3</cell><cell>51</cell><cell>14</cell><cell>0</cell><cell>0</cell><cell>1</cell></row><row><cell>Hospital</cell><cell>54</cell><cell>0</cell><cell>3</cell><cell>4</cell><cell>595</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>Date</cell><cell>58</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>4</cell><cell>1,891</cell><cell>0</cell><cell>0</cell></row><row><cell>ID</cell><cell>3</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>479</cell><cell>0</cell></row><row><cell>Phone</cell><cell>11</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>1</cell><cell>19</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Or a target phrase, though we will refer here only to target words.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>O ¨. Uzuner et al.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>Wd links the main clause back to the LEFT-WALL; Ss links singular nouns to singular verb forms; MVp connects verbs to their (prepositional) modifying phrases; Js links prepositions to their objects; Ds links determiners to nouns; and Xp links periods to words[28].</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3"><p>connects prepositions to their objects''[28].</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4"><p>Pv links verb be to the following passive participle; MVa connects verbs to adverbs; ''ID marks the idiomatic strings found in the link grammar dictionary''[28].</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_5"><p>''I connects verbs with infinitives''; ''K connects verbs with particles like in, out,'' etc.; ''ON connects the preposition on to time expressions''; ''TM connects month names to day numbers''[28].</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_6"><p>''TO connects verbs and adjectives which take infinitival complements to the word to''; ''G connects proper nouns together in series''; Xi connects punctuation symbols to abbreviations[28].</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_7"><p>Throughout this paper, the baseline F-measures that are significantly different from the corresponding F-measure of Stat De-id at a = 0.05 are marked with * in the tables. Highest Fmeasures are in bold.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_8"><p>For all pairs of features, the differences between F-measures for PHI and the differences between F-measures for non-PHI are significant at a = 0.05.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported in part by the National Institutes of Health through research grants 1 RO1 EB001659 from the National Institute of Biomedical Imaging and Bioengineering and through the NIH Roadmap for Medical Research, Grant U54LM008748. IRB approval has been granted for the studies presented in this manuscript. We thank the anonymous reviewers for their insightful comments and constructive feedback.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>§ This is a thoroughly revised and extended version of the preliminary draft ''Role of Local Context in De-identification of Ungrammatical, Fragmented Text'' which was presented at the conference of the North American Chapter of Association for Computational Linguistics/Human Language Technology (NAACL-HLT 2006) in June 2006.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<idno>accessed 9.05.07</idno>
		<ptr target="&lt;http://www.hhs.gov/ocr/AdminSimpRegText.pdf&gt;" />
	</analytic>
	<monogr>
		<title level="j">Health Information Portability and Accountability Act, Section</title>
		<imprint>
			<biblScope unit="volume">164</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The effects of location access behavior on re-identification risk in a distributed environment</title>
		<author>
			<persName><forename type="first">B</forename><surname>Malin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Airoldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th international workshop on privacy enhancing technologies, privacy enhancing technologies</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Danezis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Golle</surname></persName>
		</editor>
		<meeting>the 6th international workshop on privacy enhancing technologies, privacy enhancing technologies<address><addrLine>Berlin/Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer/Heidelberg</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">4258</biblScope>
			<biblScope unit="page" from="413" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Replacing personally-identifying information in medical records, the Scrub system</title>
		<author>
			<persName><forename type="first">L</forename><surname>Sweeney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the American Medical Informatics Association</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Cimino</surname></persName>
		</editor>
		<meeting>the American Medical Informatics Association<address><addrLine>Philadelphia, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Hanley &amp; Belfus, Inc</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="333" to="337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast exact string pattern matching algorithms adapted to the characteristics of the medical language</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lovis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Baud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Am Med Inform Assoc</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="378" to="391" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">LIBSVM: a library for support vector machines. Manual</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<idno>6.10.07</idno>
		<ptr target="&lt;http://www.csie.ntu.edu.tw/$cjlin/libsvm/&gt;" />
		<imprint>
			<date type="published" when="2001">2001</date>
			<pubPlace>Taipei, Taiwan</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science and Information Engineering, National Taiwan University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Parsing English with a link grammar</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sleator</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Temperley</surname></persName>
		</author>
		<idno>CMU-CS-91-196</idno>
		<imprint>
			<date type="published" when="1991">1991</date>
			<pubPlace>Pittsburgh, PA, USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Computer Science Department, Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A robust parsing algorithm for link grammars</title>
		<author>
			<persName><forename type="first">D</forename><surname>Grinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sleator</surname></persName>
		</author>
		<idno>CMU-CS-95-125</idno>
		<imprint>
			<date type="published" when="1995">1995</date>
			<pubPlace>Pittsburgh, PA, USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Computer Science Department, Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Computer assisted de-identification of free text nursing notes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Douglass</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005-02">February 2005</date>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Probabilistic reasoning for entity and relation recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th international conference on computational linguistics. Morristown</title>
		<editor>
			<persName><forename type="first">W</forename><surname>Lenders</surname></persName>
		</editor>
		<meeting>the 19th international conference on computational linguistics. Morristown<address><addrLine>NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>COLING, Association for Computational Linguistics</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An algorithm that learns what&apos;s in a name</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bikel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach Learn J Spec Issue Nat Lang Learn</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1/3</biblScope>
			<biblScope unit="page" from="211" to="231" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Evaluation of a deidentification (De-Id) software engine to share pathology reports and clinical documents for research</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gilbertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Am J Clin Pathol</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="176" to="186" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Development and evaluation of an open source software tool for de-identification of pathology reports</title>
		<author>
			<persName><forename type="first">B</forename><surname>Beckwith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mahaadevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Balis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Med Inform Decis Making</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Concept-match medical data scrubbing: how pathology text can be used in research</title>
		<author>
			<persName><forename type="first">J</forename><surname>Berman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Arch Pathol Lab Med</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="680" to="686" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Identification of patient name references within medical documents using semantic selectional restrictions</title>
		<author>
			<persName><forename type="first">R</forename><surname>Taira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kangarloo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the American Medical Informatics Association</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Kohane</surname></persName>
		</editor>
		<meeting>the American Medical Informatics Association<address><addrLine>Philadelphia, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Hanley &amp; Belfus, Inc</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="757" to="761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<idno>28.08.07</idno>
		<ptr target="&lt;http://www.nist.gov/speech/tests/ace/ace07/doc/ace07-evalplan" />
		<title level="m">The ACE 2007 evaluation plan</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficient support vector classifiers for named entity recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Isozaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kazawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th international conference on computational linguistics</title>
		<editor>
			<persName><forename type="first">W</forename><surname>Lenders</surname></persName>
		</editor>
		<meeting>the 19th international conference on computational linguistics<address><addrLine>Morristown, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>COLING, Association for Computational Linguistics</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="390" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Evaluating the state-of-the-art in automatic de-identification</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">¨</forename><surname>Uzuner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Szolovits</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Am Med Inform Assoc</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="550" to="563" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title/>
		<idno>2.10.07</idno>
		<ptr target="&lt;http://www.nlm.nih.gov/pubs/factsheets/umls.html&gt;" />
	</analytic>
	<monogr>
		<title level="j">Unified Medical Language System</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>web page</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Support-vector networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach Learn</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Support vector machines</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sewell</surname></persName>
		</author>
		<idno>9.07.07</idno>
		<ptr target="&lt;http://www.svm.org&gt;" />
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Algorithms and architectures for machine learning based on regularized neural networks and support vector approaches</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rychetsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Shaker Verlag GmBH</publisher>
			<pubPlace>Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A tutorial on support vector machines for pattern recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Burges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl Disc Data Min</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="121" to="167" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Conditional random fields: probabilistic models for segmenting and sequence data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the international conference on machine learning</title>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Brodley</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Danyluk</surname></persName>
		</editor>
		<meeting>the international conference on machine learning<address><addrLine>San Francisco</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Accurate information extraction from research papers using conditional random fields</title>
		<author>
			<persName><forename type="first">F</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the human language technology conference/North American chapter of the association for computational linguistics annual meeting</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Hirschberg</surname></persName>
		</editor>
		<meeting>the human language technology conference/North American chapter of the association for computational linguistics annual meeting<address><addrLine>Morristown, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="329" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A simple rule-based part of speech tagger</title>
		<author>
			<persName><forename type="first">E</forename><surname>Brill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on applied natural language processing</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Bates</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">O</forename><surname>Stock</surname></persName>
		</editor>
		<meeting>the conference on applied natural language processing<address><addrLine>Morristown, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="152" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">The grammatical analysis of technical texts using a link parser</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sutcliffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brehony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mcelligott</surname></persName>
		</author>
		<idno>UL- CSIS-94-13</idno>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science and Information Systems, University of Limerick</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical note</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Evaluation of two dependency parsers on biomedical corpus targeted at protein-protein interactions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pyysalo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pahikkala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Boberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jarvinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salakoski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int J Med Inform</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="430" to="442" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<idno>07.07</idno>
		<ptr target="&lt;http://www.census.gov/genealogy/names/&gt;" />
		<title level="m">US Census Bureau. 1990 census name files</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Census 2000 urbanized area and urban cluster information</title>
		<idno>9.07.07</idno>
		<ptr target="&lt;http://www.census.gov/geo/www/ua/uaucinfo.html#lists&gt;" />
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>US Census Bureau</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName><surname>Worldatlas</surname></persName>
		</author>
		<author>
			<persName><surname>Com</surname></persName>
		</author>
		<idno>accessed 9.07.07</idno>
		<ptr target="&lt;http://worldatlas.com&gt;" />
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Sarawagi</surname></persName>
		</author>
		<idno>21.09.07</idno>
		<ptr target="&lt;http://crf.sourceforge.net/introduction/&gt;;2007" />
		<title level="m">Conditional random fields implementation</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Was the patient cured? Understanding semantic categories and their relationships in patient records. Master&apos;s thesis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sibanda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006-06">June 2006</date>
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology: Department of Electrical Engineering and Computer Science</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Statistics made simple for healthcare and social science professionals and students</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Phua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>University Putra Malaysia Press</publisher>
			<pubPlace>Serdang, Selangor, Malaysia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Osborn</surname></persName>
		</author>
		<title level="m">Statistical applications for health information management</title>
		<meeting><address><addrLine>Boston</addrLine></address></meeting>
		<imprint>
			<publisher>Jones &amp; Bartlett Publishers</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>nd ed.</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Computational disclosure control: a primer on data privacy protection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Sweeney</surname></persName>
		</author>
		<idno>27.09.07</idno>
		<ptr target="&lt;http://www.swiss.ai.mit.edu/classes/6.805/articles/privacy/sweeney-thesis-draft.pdf&gt;" />
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Privacy: a machine learning view</title>
		<author>
			<persName><forename type="first">S</forename><surname>Vinterbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Knowl Data Eng</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="939" to="948" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Lasko</surname></persName>
		</author>
		<title level="m">Spectral anonymization of data</title>
		<imprint>
			<date type="published" when="2007-08">August 2007</date>
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology: Department of Electrical Engineering and Computer Science</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
