<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Saliency based Ulcer Detection for Wireless Capsule Endoscopy Diagnosis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yixuan</forename><surname>Yuan</surname></persName>
						</author>
						<author>
							<persName><roleName>Student Member, IEEE</roleName><forename type="first">Jiaole</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><roleName>Student Member, IEEE</roleName><forename type="first">Baopu</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">-H</forename><surname>Meng</surname></persName>
						</author>
						<title level="a" type="main">Saliency based Ulcer Detection for Wireless Capsule Endoscopy Diagnosis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E5F4D370A705D9494CA7DB06F827E8A7</idno>
					<idno type="DOI">10.1109/TMI.2015.2418534</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMI.2015.2418534, IEEE Transactions on Medical Imaging This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMI.2015.2418534, IEEE Transactions on Medical Imaging</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Multi-level superpixel representation</term>
					<term>Saliency</term>
					<term>Saliency based max-pooling method</term>
					<term>LLC</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Ulcer is one of the most common symptoms of many serious diseases in the human digestive tract. Especially for the ulcers in the small bowel where other procedures cannot adequately visualize, wireless capsule endoscopy (WCE) is increasingly being used in the diagnosis and clinical management. Because WCE generates large amount of images from the whole process of inspection, computer-aided detection of ulcer is considered an indispensable relief to clinicians. In this paper, a two-staged fully automated computer-aided detection system is proposed to detect ulcer from WCE images. In the first stage, we propose an effective saliency detection method based on multi-level superpixel representation to outline the ulcer candidates. To find the perceptually and semantically meaningful salient regions, we first segment the image into multi-level superpixel segmentations. Each level corresponds to different initial region sizes of the superpixels. Then we evaluate the corresponding saliency according to the color and texture features in superpixel region of each level. In the end, we fuse the saliency maps from all levels together to obtain the final saliency map. In the second stage, we apply the obtained saliency map to better encode the image features for the ulcer image recognition tasks. Because the ulcer mainly corresponds to the saliency region, we propose a saliency max-pooling method integrated with the Locality-constrained Linear Coding (LLC) method to characterize the images. Experiment results achieve promising 92.65% accuracy and 94.12% sensitivity, validating the effectiveness of the proposed method. Moreover, the comparison results show that our detection system outperforms the state-ofthe-art methods on the ulcer classification task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>U LCER is one of the most common lesions of the gas- trointestinal (GI) tract that affects approximately 10% of the people in the world <ref type="bibr" target="#b0">[1]</ref>. It is a chronic inflammatory sore or erosion on the internal mucous membranes. Helicobacter pylori bacteria and non-steroidal anti-inflammatory drugs (NSAIDs) are considered to be two important causes of ulcer in the digestive tract. Ulcer itself is not lethal, however, it is the symptom of some serious diseases, such as Crohn's disease and ulcerative colitis whose complications may cause death <ref type="bibr" target="#b1">[2]</ref>. This work is supported by RGC GRF # 415613 awarded to Max Q.-H. Meng and partially by National Natural Science Foundation of China (61305099) awarded to Baopu Li. 1 Yixuan Yuan, Jiaole Wang and Max Q.-H. Meng are with the Department of Electronic Engineering, The Chinese University of Hong Kong, N.T., Hong Kong SAR, China {yxyuan,jlwang,max}@ee.cuhk.edu.hk 2 Baopu Li is with the Department of Biomedical Engineering in Shenzhen University, Shenzhen, China bpli@szu.edu.cn * Corresponding author. Traditional imaging technique for the ulcer is push and sonde enteroscopy. In the inspection procedure, it will be inserted to patient's mouth or anus by experienced physicians to view the GI tract <ref type="bibr" target="#b2">[3]</ref>. Although the traditional techniques play an important role to view the upper and lower ends of the GI tract, they are highly invasive to the patients. Moreover, it is technically difficult for the traditional procedures to get full access to the small intestines <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>.</p><p>Iddan et al. <ref type="bibr" target="#b5">[6]</ref> introduced the revolutionary wireless capsule endoscopy (WCE) as an alternative way to provide direct, painless and noninvasive inspection of the small bowel. The commercially available WCE usually consists of an optical dome, an illumination part, an imaging sensor, batteries, and a radio frequency transmitter as shown in Fig. <ref type="figure" target="#fig_0">1(a)</ref>. After a WCE is swallowed by a patient, it is then pushed by peristalsis to slowly travel through the small intestine. During about 8 hours inside patient's GI tract, the WCE takes 2-4 images per second. These images are compressed and transmitted wirelessly to a data-recording device attached to the patient's waist. All the images can be downloaded and examined off-line by doctors to make diagnostic decisions <ref type="bibr" target="#b6">[7]</ref>. An example image of ulcer that captured by a WCE during inspection is shown in Fig. <ref type="figure" target="#fig_0">1(b)</ref>.</p><p>Although the WCE has shown significant advantages over the traditional endoscopies to inspect the ulcer nidus in the small intestine, there are new challenges associated with this technology. A WCE creates 55, 000 images for each patient, and the captured abnormal images occupy only 5% of the whole WCE images collected, it is tedious for clinicians to go through all these images manually frame by frame to locate the abnormal images. Therefore, it is crucial to design an automatic computer-aided system to assist the clinicians to analyze the ulcer images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Related Work</head><p>Many efforts have been devoted to develop automated computer-aided detection (CAD) for ulcer screening using WCE. Li and Meng <ref type="bibr" target="#b7">[8]</ref> put forward a new texture feature combining merits of discrete curvelet transform (DCT) and local binary pattern (LBP), which leads to better descriptions of textures with multi-directional characteristic. Charisis et al. <ref type="bibr" target="#b8">[9]</ref> presented a novel method which first processes the whole WCE image with a color rotation operation (CR), then extracts the uniform rotation invariant LBP (ULBP) feature.</p><p>Eid et al. <ref type="bibr" target="#b9">[10]</ref> proposed a curvelet-based lacunarity texture extraction method (DCT-LAC). The textural information is acquired by calculating the lacunarity index of DCT sub-bands of the WCE images. Yu et al. <ref type="bibr" target="#b10">[11]</ref> introduced an improved bag of word model to detect the ulcer region using spatial pyramid kernel and feature fusion techniques. Karagyris et al. <ref type="bibr" target="#b11">[12]</ref> proposed to apply log-Gabor filter bank to segment the Hue component in the HSV color space of the WCE images to obtain the ulcer candidates. Then the RGB values and texture information are extracted to classify the ulcer images from the normal ones.</p><p>There are some limitations in the conventional ulcer classification methods. The image features in <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b9">[10]</ref> were all extracted from the whole WCE image, which may be ineffective to describe the specific ulcer information. The problem of using the whole WCE image is that the features extracted from the non-ulcer region may bring noisy and redundant information to the classification. In order to handle these problems, Karagyris et al. <ref type="bibr" target="#b11">[12]</ref> outlined the ulcer candidates and extracted the features directly from the ulcer. However, they used traditional segmentation methods which are not able to segment the ulcer region correctly and compactly because the ulcer shows vague boundary.</p><p>The saliency method, which extracts the most important region in the image, recently attracts more and more attentions and achieves promising segmentation results <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b19">[20]</ref>. Therefore, we are motivated to outline the ulcer region in WCE images through the saliency detection method first and then apply the achieved saliency map to better encode the image features for ulcer image classification task.</p><p>Abnormal image classification is a fundamental problem in the field of automatic medical image diagnosis and it greatly relies on image representation. The bag of feature (BoF) model <ref type="bibr" target="#b20">[21]</ref>, which demonstrates promising performance in many natural images applications, has been adopted in this area <ref type="bibr" target="#b21">[22]</ref>- <ref type="bibr" target="#b24">[25]</ref>. The BoF method regards a single image as an order-less collection of local key point features, and represents images as the histograms of the visual words. In the BoF model, each key point feature from an image is assigned to the nearest codeword by hard assignment, which will cause severe information loss when the feature locates around the boundary of several codewords.</p><p>As an improvement of the BoF method, Wang et al. <ref type="bibr" target="#b25">[26]</ref> introduced the Locality-constrained Linear Coding (LLC) method to code the image features to preserve the locality and sparsity. The LLC method utilizes the locality constraints to project each descriptor into its local-coordinate system and the projected descriptors are integrated by max pooling to generate the final representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The Proposed Methods and Original Contributions</head><p>In this paper, we propose a two-staged fully automated computer-aided detection system to detect ulcers from WCE images. In the first stage, owing to the drawbacks of traditional segmentation methods <ref type="bibr" target="#b11">[12]</ref>, we focus on the automatic estimation of salient regions across the WCE images. The classical saliency extraction approaches often calculate a pixel-based saliency <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b17">[18]</ref>- <ref type="bibr" target="#b19">[20]</ref>, ignoring the neighbor information and boundary information of the object. In addition, the human is attracted more by object instead of sole pixels. As an alternative approach, we propose to adopt superpixel representation <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref> for ulcer saliency detection in the WCE images.</p><p>A superpixel is a group of pixels under some restriction of local image features such as color, intensity, or texture. It preserves most of the image structure and greatly reduces the complexity of the image processing. Furthermore, the single scale superpixel may not be able to represent the accurate contour of objects <ref type="bibr" target="#b28">[29]</ref>. Thus we propose the saliency calculation method based on the multi-level superpixel representation for the images. Since the ulcer shows significant color and texture information on the WCE mucosa surface as shown in Fig. <ref type="figure" target="#fig_0">1(b)</ref>, we analyze these characteristics to evaluate the corresponding saliency value for each superpixel and obtain the saliency map for each level. The final saliency map is calculated by a fusion strategy that integrates the obtained saliency maps from all levels.</p><p>In the second stage, we employ the obtained saliency maps for the ulcer classification task. Inspired by the promising results of bag-of-words (BoW) or BoF model and its variants <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b29">[30]</ref>, we propose to classify the ulcer images by coding WCE images with a modified Locality-constrained Linear Coding (LLC) method. The proposed modified LLC method integrates the original LLC method <ref type="bibr" target="#b25">[26]</ref> with a saliency based max-pooling to emphasize the salient region for ulcer classification.</p><p>The main contributions of this paper are summarized as follows:</p><p>1) Instead of extracting features from the whole WCE images <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b9">[10]</ref>, we propose a saliency map estimation method to outline the ulcer first and then extract the corresponding feature to better encode WCE images.</p><p>The proposed saliency method is based on multi-level superpixel color and texture representation. 2) Different from utilizing the existed image coding methods <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, a saliency based max pooling method integrated with the original LLC method is proposed to carry out ulcer frame classification tasks.</p><p>The rest of the paper is organized as follows: Section II introduces details of the proposed ulcer saliency estimation method. Section III gives details of the saliency based image coding by the proposed modified LLC method. The experimental results, including ulcer saliency map extraction and classification are illustrated in Section IV. We further discuss the results and the insights in Section V, and draw conclusions at the end of this paper. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. SALIENT REGION DETECTION</head><p>Salient regions are usually defined as regions that could explicitly present the main meaningful or semantic contents. For the implicit images with multiple objects or cluttered scenes, however, there are no uniform saliency metrics that could describe the saliency. In this study, as shown in Fig. <ref type="figure" target="#fig_0">1(b)</ref>, WCE images with ulcer lesions could be explicitly distinguished by color and texture contrasts. By exploiting these characteristics, salient region detection method can segment the salient foreground region from WCE images. Thus, salient region detection method is especially suitable to be used as a first step for the ulcer recognition problem.</p><p>A lot of research effort has already been devoted to design the saliency models for images <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b19">[20]</ref>, while the research on the medical saliency estimation for WCE videos has not been developed yet. In this paper, we propose a framework to detect ulcer abnormalities through visual saliency estimation based on the texture and color contrasts. The proposed saliency detection method consists of three main steps. The first step is multi-level segmentation, which decomposes the input image into multiple superpixels from a coarse level to a fine one. Then, we conduct the regional saliency estimation step by the color and texture contrasts on each superpixel level. In the end, the final saliency map is obtained by fusing multi-level saliency maps together. The flowchart of the construction of the integrated saliency map is shown in Fig. <ref type="figure" target="#fig_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Superpixel Segmentation</head><p>A superpixel is defined as the meaningful entity by grouping spatially neighboring pixels with the similar property. Simple Linear Iterative Clustering (SLIC) <ref type="bibr" target="#b30">[31]</ref> is the state-of-the-art superpixel algorithm that outputs a desired number of regular, compact superpixels with a low computational overhead.</p><p>We propose to apply SLIC superpixels as a pre-processing method for WCE image saliency detection. Because it not only provides good segmentation results, but also generates suitable size of superpixels for WCE image analysis. In the SLIC method, the local K-means clustering is first performed on the pixels based on the color space and spatial distances. Then the isolated small clusters are merged with the largest neighbor clusters to obtain the specific number of the superpixels. Each segmented superpixel is used as a processing unit in the proposed saliency model. Choosing a suitable number of superpixels for the WCE image is empiric and case-specific. This is because that too many numbers of superpixel lead to over-segmentation, while too few superpixels result in loss of the boundary information of the objects. In addition, using a single superpixel size to do segmentation may not be able to describe the boundary well for some cases. Therefore, we propose a multi-level superpixel method that first segments the image by using multiple different numbers of superpixels (a.k.a., multiple levels of superpixels), then fuses all superpixel segmentation in all levels later. The number of superpixels K we tested in this paper is set to be 50, 100, 150, 200, and 250 in each level, which results in level number L = 5. To address this factor, we focus just on the superpixel in the outlined mask as in Fig. <ref type="figure" target="#fig_3">3(a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Saliency Region Detection based on Texture</head><p>In this paper, texture features of superpixel regions are extracted by using Leung-Malik (LM) filter bank <ref type="bibr" target="#b31">[32]</ref>. The LM filter bank is a multi-scale, multi-orientation filter combination Given an image I M×N , the response R M×N to the mth filter ω m out of 48 filters in the LM filter bank can be calculated by</p><formula xml:id="formula_0">R m (x, y) = ∑ s ∑ t ω m (s,t) I(x + s, y + t),<label>(1)</label></formula><p>where (•, •) stands for the matrix entry of the corresponding matrix.</p><p>Then, the texture feature matrix texture F l ∈ R 96×K for the lth level that have K superpixel regions can be defined by using the mean µ and the variance σ 2 of the filter responses in all the regions. Therefore, the texture feature vector of the ith superpixel at the lth level is given as the ith column of the matrix texture F l ,</p><formula xml:id="formula_1">texture F l (i) = [ i µ 1 , i σ 2 1 , . . . , i µ 48 , i σ 2 48 ] T ,<label>(2)</label></formula><p>where texture F l (i) stands for the ith column of the texture feature matrix of the lth superpixel level and l = 1, . . . , L.</p><p>The texture saliency texture Ŝl ∈ R M×N at the lth level for the given image is defined as,</p><formula xml:id="formula_2">texture Ŝl (x, y) = N ∑ j=1, j =i D(i, j) max i, j (D(i, j)) ,<label>(3)</label></formula><p>where D(i, j) is the (i, j) entry of the distance matrix D that represents the Euclidean distance between the texture feature vectors texture F l (i) and texture F l ( j) of the ith and jth regions. max i, j (D(i, j)) is to take the maximum within the matrix. The entry (x, y) for the texture saliency matrix texture Ŝl represents a pixel within the ith superpixel region. The texture saliency value for each pixel is set to be the same within the same superpixel region.</p><p>Eq. ( <ref type="formula" target="#formula_2">3</ref>) indicates that the superpixels whose textures are different from most of the other regions in the image are assigned with a higher value of texture saliency. The final texture saliency texture S is obtained by fusing the different level saliency maps together,</p><formula xml:id="formula_3">texture S = 1 L L ∑ l=1 texture Ŝl .<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Saliency Region Detection based on Color</head><p>Intuitively, the ulcer in a WCE image shows different color information compared with the normal mucosa. However, it is not obvious to choose a color component that contains the most useful information to express the abnormality of ulcer. We take a trial-and-error approach and inspect the ulcer images under different color components of various color spaces such as RGB, CIELAB, CIEXYZ, YUV, YIQ, CMYK, HSV and HSI. Consequently, as shown in Fig. <ref type="figure" target="#fig_5">5</ref>, the second components of the transformed WCE images in the HSV and CMYK color spaces highlight the ulcer regions and separate ulcer mucosa tissues from the uninformative parts. Therefore, we extract the saliency regions for all the WCE images in these two color planes.</p><p>The color feature matrix color F l ∈ R 4×K for the lth level that have K superpixel regions can be defined by using the mean and variance values in the S and M color planes. The ith column of the matrix represents feature vector of the ith superpixel region at lth level.</p><formula xml:id="formula_4">color F l (i) = [ i µ S , i σ 2 S , i µ M , i σ 2 M ] T ,<label>(5)</label></formula><p>where i µ • and i σ 2</p><p>• are the mean and variance of the ith superpixel in the S and M color planes.</p><p>Similar to the texture saliency, the color saliency color Ŝl ∈ R M×N at level l can be obtained by the following equation:</p><formula xml:id="formula_5">color Ŝl (x, y) = N ∑ j=1, j =i D(i, j) max i, j (D(i, j)) ,<label>(6)</label></formula><p>where D(i, j) is the (i, j) entry of the distance matrix D that represents the Euclidean distance between the color feature vectors color F l (i) and color F l ( j). max i, j (D(i, j)) is to take the maximum within the matrix. The entry (x, y) for the color saliency matrix color Ŝl represents a pixel within the ith superpixel region. The color saliency value for each pixel is set to be the same within the same superpixel region. Eq. ( <ref type="formula" target="#formula_5">6</ref>) enables the region with a more different color distribution to obtain higher saliency, and promotes the color saliency values of the regions which contain abnormality of WCE images. We calculate the final saliency color S based on the mean value of the different levels,</p><formula xml:id="formula_6">color S = 1 L L ∑ l=1 color Ŝl .<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Final Saliency Region Fusion</head><p>Based on the texture and color saliency introduced in the aforementioned sections, we derive the proposed multi-level superpixels saliency method for the WCE images in a data fusion manner. Given image I M×N , we have computed two saliency maps texture S and color S based on the texture and color contrast information, respectively. Then the final saliency map can be defined as,</p><formula xml:id="formula_7">f inal S = color S • texture S • K,<label>(8)</label></formula><p>where • stands for matrix Hadamard product. K ∈ R M×N is a Gaussian kernel which is centralized at the image center and gradually declines to the edge to mimic human attention property,</p><formula xml:id="formula_8">K(x, y) = exp(- (x -M/2) 2 + (y -N/2) 2 2σ 2 ).<label>(9)</label></formula><p>For the simplicity, we use equal standard deviation σ in both horizontal and vertical directions in this paper. By applying the matrix Hadamard product, the color and the texture saliency maps could be fused in an arithmetic manner that only the regions with higher values in both texture and color features could achieve higher values in the final saliency. On the contrary, if a region has high value in only one of the saliency maps, the Hadamard product will eliminate it by simply multiplying a small saliency value to it. In this way, the final saliency map emphasizes the regions due to both color and texture contrasts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. SALIENCY BASED IMAGE CODING</head><p>In this paper, we propose a modified LLC method that integrates the obtained saliency map to the max-pooling method for a WCE image. The modified LLC method can be summarized as three steps. In the first step, we extract the local descriptors (dense scale-invariant feature transform (dSIFT) <ref type="bibr" target="#b32">[33]</ref>, dense Histogram of Oriented Gradients (dHOG) <ref type="bibr" target="#b33">[34]</ref>, dense uniform local binary patterns (duniLBP) descriptors <ref type="bibr" target="#b34">[35]</ref>) for each image separately to produce a codebook by using K-means clustering method. In the second step, the original LLC method is used for feature coding. In the last step, the max-pooling is applied on the saliency features and non-saliency features respectively to emphasize the features in the saliency region. Finally, these three local features are concatenated to produce the final image representation. By feeding the final representation features into the supported vector machine (SVM) classifier, we can carry out the ulcer image classification tasks. The work-flow of the proposed method for the WCE images is shown in Fig. <ref type="figure" target="#fig_6">6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Local Descriptors and the Codebook</head><p>In order to capture the diversity of the image characteristics, we extract different descriptors of the WCE images and then combine them together to represent images. The first feature we extracted is the dense SIFT (dSIFT) <ref type="bibr" target="#b35">[36]</ref> features. The SIFT feature <ref type="bibr" target="#b32">[33]</ref> has been recognized as one of the most robust features with respect to different geometrical changes. In the traditional SIFT feature, a local descriptor is created by forming a histogram of gradient orientations and magnitudes of image pixels in a small window.</p><p>In our approach, however, the dSIFT descriptors are extracted at regular image grid points, rather than only at key points. The advantage of this strategy is that the dSIFT descriptor could be independence to the key point detection process which often fails due to missing texture or ill-illuminated images <ref type="bibr" target="#b36">[37]</ref>. The dSIFT descriptor matrix dSIFT X ∈ R 128×P can be calculated from each WCE image and represented by dSIFT X = [ dSIFT x i ], where i ∈ [1, P]. Each column vector dSIFT x i ∈ R 128 corresponds to the dSIFT descriptor extracted from an image patch with size of 16 × 16 and P is the number of the image patches. Similarly, we also extract the dHOG descriptor matrix ( dHOG X ∈ R 81×P ) and duniLBP descriptor matrix ( duniLBP X ∈ R 59×P ) from the same patches of each image to obtain a diverse representation.</p><p>After obtaining the descriptors, we apply the K-means algorithm on each kind of features extracted from the training datasets to generate the visual vocabulary. The resultant cluster centers serve as a vocabulary of visual words and we represent the three codebooks as dSIFT B ∈ R 128×M , dHOG B ∈ R 81×M , and duniLBP B ∈ R 59×M with M cluster centers. Choosing suitable codebooks is important and difficult. For example, if the size of visual words is too small, two key points will be assigned to the same cluster although they are not similar, which leads to the decrease of the discrimination ability. On the other hand, the vocabulary may be difficult to generalize with extra processing overhead. In an attempt to address this problem, we change the M from 10 to 100 and evaluate the corresponding classification performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Descriptor Coding by LLC</head><p>Once obtaining the three kinds of codebooks, we map each image descriptor matrix to the codebooks to obtain the image representation. The original LLC is first applied to encode the individual descriptors of each training and testing image. Without loss of generality, we take dSIFT descriptors as an example for the rest of the introduction and the left superscripts of different descriptors are dismissed for simplicity.</p><p>Let X = [x 1 , x 2 , . . . , x P ] ∈ R 128×P be the set of 128 dimensional descriptor vectors of an input sample and Z = [z 1 , z 2 , . . . , z P ] ∈ R M×P be the code for the image with LLC method. Then the objective function of LLC could be specified as min where λ is a weight parameter for adjusting the weights of locality, denotes element-wise multiplication, and d i ∈ R M is locality adapter that gives different freedom for each basis vector proportional to its similarity to the input descriptor x i .</p><formula xml:id="formula_9">Z N ∑ i=1 x i -Bz i 2 + λ d i z i 2 subject to 1 T z i = 1, ∀i,<label>(10)</label></formula><formula xml:id="formula_10">d i = exp( dist(x i , B) σ ), dist(x i , B) = [dist(x i , b 1 ), • • • , dist(x i , b m )],<label>(11)</label></formula><p>where dist(x i , b • ) is the Euclidean distance between x i and each element of the codebook B. And σ is used for adjusting the weight decay speed for locality adapter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Saliency based Max-Pooling</head><p>After coding each descriptor vector, the codes are pooled together to generate a WCE image representation. The original LLC uses a naive max pooling method which ignores the saliency region of the image. This may have no effect for the recognition task if the input image has a clean background or there is a single object inside image to be recognized. But for the WCE images which usually have complex background and distributed ulcer lesions, the naive max pooling method will ruin the important information of ulcer by blending the ulcer and normal regions together indiscriminately.</p><p>Since the salient regions correspond to the semantic foreground that is probably the abnormal regions, we propose a saliency based max pooling method to emphasize the salient region in the image. First, we segment the obtained LLC code into two parts, salient region part and non-salient region part, according to the previously obtained saliency map for each input image. Given a 16 × 16 patch, if the mean saliency value of this patch is larger than the mean value of the whole saliency map, we designate this patch as a salient region and the corresponding code is a salient code.</p><p>We then apply the max pooling method on the salient and non-salient codes, respectively. By concatenating the results together, the final representation vector of each image will be a M-dimensional feature vector y that can be calculated as,</p><formula xml:id="formula_11">y = max( s z 1 , • • • , s z U ) max( ns z 1 , • • • , ns z P-U ) , (<label>12</label></formula><formula xml:id="formula_12">)</formula><p>where s z • and ns z • are the salient and non-salient codes, max(•) function returns the maximum value in a row-wise manner, U is the number of salient patches. The obtained image representation vector of each input WCE image highlights the salient region features, therefore it can characterize the WCE images better than other representations that calculated without considering saliency.</p><p>Although the aforementioned method takes dSIFT descriptor as an example, the same process could be used for the dHOG and duniLBP descriptors. Thus, the final image representation f inal y ∈ R 3M is a concatenation of the results from three descriptors, f inal y = dSIFT y T dHOG y T duniLBP y T T .</p><p>(</p><formula xml:id="formula_13">)<label>13</label></formula><p>IV. RESULTS</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Image Acquisition and Experimental Setup</head><p>The images used for the development and evaluation of the proposed approach were extracted from 20 patients' WCE videos with ulcerous diseases, such as unexplained ulceration, ulceration from NSAID, ulcerative colitis and Crohn's diseases. The examinations were conducted by Pillcam SB WCE system and the Rapid Reader 6.0 software (Both from Given Imaging Ltd.) was employed to export the images from the video sequence.</p><p>Actually it is difficult to gather large numbers of cases of ulcers in WCE videos since the patients who took the WCE examination may not suffer from the ulcers or they may carry multiple cases. Even if we have a clear ulcer case, bubbles in the GI tract may block the nidi and make it difficult to collect the ulcer images from patients. Thus, in this paper, we composed a dataset that consists of 170 ulcer and 170 normal images from the examination data of 20 patients. The 170 ulcer images were obtained from different ulcer regions to achieve the lowest possible similarity. Furthermore, the normal images including both simple and confusing healthy tissue (folds, villus, bubbles etc) are used to simulate the actual discrimination process. Three clinicians manually traced the boundaries of the ulcer regions on each single WCE image and these ulcers annotations served as ground-truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiment Results for Ulcer Extraction</head><p>first experiment was designed to evaluate the performance of the proposed saliency extraction method for WCE images. After segmenting the WCE images under five different superpixel levels, we first calculated the corresponding texture and color saliency maps. As shown in Fig. <ref type="figure" target="#fig_8">7</ref>(a), we illustrated this strategy by using one ulcer WCE image as an example. Fig. <ref type="figure" target="#fig_8">7(b)</ref> shows the texture and color saliency maps under five different superpixel levels. Then, we fused these two kinds of five-level saliency maps into one texture and one color saliency maps, respectively. From the two fused saliency maps in Fig. <ref type="figure" target="#fig_8">7</ref>(c), it is noted that the texture saliency and color saliency can complement each other. The final saliency map of the original image is built through fusing the two saliency maps together and applying a Gaussian kernel as shown in Fig. <ref type="figure" target="#fig_8">7(d)</ref> and<ref type="figure">(e)</ref>. In Fig. <ref type="figure" target="#fig_9">8</ref>, a visual comparison for estimation saliency maps between the state-of-the-art methods and the proposed one is presented by using four different images that contain ulcers. The column (a) shows the original WCE images while the (b) to (g) columns show the estimated saliency maps by using FT <ref type="bibr" target="#b16">[17]</ref>, CA <ref type="bibr" target="#b13">[14]</ref>, GBVS <ref type="bibr" target="#b18">[19]</ref>, MSSS <ref type="bibr" target="#b15">[16]</ref>, SDSP <ref type="bibr" target="#b12">[13]</ref> and our method. The column (h) shows the ground truth of the ulcer regions labeled by clinicians.</p><p>As discussed in Section I, the traditional saliency extraction methods neither segment images into semantic regions as superpixels clearly nor analyze the color and texture information on the WCE images, therefore fail to outline the ulcer region in WCE images accurately. In contrast, Fig. <ref type="figure" target="#fig_9">8</ref>(g) shows more meaningful saliency maps with well-defined object boundaries by the proposed multi-level superpixel extraction method. By integrating salient information from a coarse saliency map to a fine one, these examples qualitatively show that the proposed approach fits well for finding the boundaries between salient objects and background regions, therefore it is able to characterize the ulcer information well. It is also critical for our saliency detection algorithms to take account of the color and texture variation when locating the ulcer region.</p><p>To quantitatively show the effectiveness of our method, we calculated the precision and recall rates for the salient maps from different methods under 256 salient thresholds ([0, 255]). The averaged results of our method and other ones are plotted as precision versus recall curve in Fig. <ref type="figure" target="#fig_7">9</ref>. It is clear that our method achieves a higher precision value at every given recall than other methods. At maximum recall rates, all the methods converge to the same precision, which indicates there are almost 10% image pixels belonging to the ground truth salient regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiment Results for Ulcer Recognition</head><p>To evaluate the proposed ulcer frame recognition method, we carried the experiments with a dataset of 170 ulcer images and 170 normal images. For each WCE image with a size of 288 × 288, we extracted a 16 × 16 image patch every 8   pixels in both row and column directions, which results in 1225 patches. Three types of features, dSIFT, duniLBP and the dHOG features are extracted at every patch to characterize the properties of the image. Supported vector machine (SVM) <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref> method with Gaussian radial basis function kernel was used to carry out classification. To present a quantitative measure, the performance of the classification was measured by the indexes of accuracy, sensitivity and specificity. In order to achieve as much generalization as possible in the results, the 5-fold cross validation method was applied to the dataset. From the total number of images, 80% of them was used for training and the remaining 20% for testing. This procedure was repeated 200 times by randomly selecting training and test sets, then the mean and standard deviation values of accuracy, sensitivity 92.65 ± 1.2 94.12 ± 2.4 91.18 ± 0.9 † Data are in a mean ± standard deviation form and are calculated from the results of 200 times 5-fold cross validation on the same WCE image dataset. * The vocabulary size of our method is set to be 50 for the best performance. and specificity indeces were calculated.</p><p>Vocabulary size M of the codebook for the modified LLC method has to be carefully tuned during the construction of the image code. Therefore, we varied M from 10 to 100 in the experiments to evaluate how the vocabulary size influences the recognition performance. Fig. <ref type="figure" target="#fig_10">10</ref> shows the recognition performance with different number of codewords. The standard derivations of the recognition results are also illustrated as error bars in the figure to show the stability of classification result. It is clear that the means of accuracy, sensitivity and specificity indexes obtained the greatest values when the vocabulary size is selected to be 50. Moreover, the small standard deviation values of the three indexes indicate the stable performance when 50 codewords are used in the experiment. The mean accuracy, sensitivity and specificity with the size 50 are 92.65%, 94.12% and 91.18%, respectively, demonstrating the effectiveness of the proposed method for detecting the ulcer frames in the WCE images.</p><p>To evaluate the proposed method which uses a saliency based max pooling, we compared its performance with the original LLC method to carry out the ulcer recognition tasks. The 5-fold cross validation method was also carried out for 200 times, and the mean and standard deviation values of accuracy index for both methods are calculated. The comparison of the accuracy index of two methods is showed in Fig. <ref type="figure" target="#fig_11">11</ref>. The performance of the modifed LLC method with the saliency max pooling outperforms the original LLC method under all vocabulary sizes. This result illustrates the effectiveness of our saliency based LLC method.</p><p>To further evaluate the performance of the proposed method, we compared it with the state-of-the-art ulcer diagnosis methods. Table <ref type="table" target="#tab_0">I</ref> shows the averaged accuracy, sensitivity and specificity of 200 times 5-fold cross validation by Li et al. <ref type="bibr" target="#b7">[8]</ref>, CR-ULBP method <ref type="bibr" target="#b8">[9]</ref>, DCT-LAC method <ref type="bibr" target="#b9">[10]</ref>, Yu et al. <ref type="bibr" target="#b10">[11]</ref> and ours, respectively. The best performances of three indexes are highlighted by the gray background. The proposed method shows superior performance with an improvement of 10.30%, 3.16%, 1.91% and 7.21% in accuracy, 2.94%, 7.06%, 7.50% and 8.09% in sensitivity compared with the state-of-theart methods, respectively. This result validates the proposed method possesses superior ability to characterize the WCE images and demonstrates good discriminative capability for ulcer detection.</p><p>In <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b9">[10]</ref>, the authors extracted features of the whole image to represent the image information, but the features extracted from the non-ulcer region may bring noisy and redundant information to classification. Instead, in our proposed method, we first extract the saliency region which corresponds to the ulcer region and then incorporate this information to the feature coding process. In addition, compared with <ref type="bibr" target="#b10">[11]</ref> which using existed BoW model to conduct the ulcer classification problem, we proposed a modified LLC method incorporating the saliency information to better encode the WCE images. Thus, our proposed method shows superior classification performance than those from <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. DISCUSSION A. Saliency Estimation for WCE Images</head><p>As the first stage to describe a WCE image, we proposed a novel saliency estimation method that takes advantage of the superpixel segmentation method and the image contrast information (texture and color contrasts).</p><p>Our method is based on the multi-level superpixel representation to avoid deficiency of pixel representation in the saliency region detection. The proposed multi-level superpixel segmentation method is considered as a superior alternative to the traditional WCE image segmentation method since it can outline abnormal region correctly and compactly.</p><p>Since ulcer regions have particular color and texture characteristics, we utilized this important information and extracted salient maps that emphasize the color and texture contrasts of ulcer regions inside the WCE images. We tested many textures (such as Gabor filters <ref type="bibr" target="#b39">[40]</ref>, wavelets filters <ref type="bibr" target="#b40">[41]</ref>, et al.) to outline the ulcer region and found that the features extracted from LM filter could obtain better performance than the others. This result may be due to the multi-scale and multi-orientation nature of the LM filter. The experimental results shown in Section IV validated the effectiveness of our proposed method and also gave strong evidence to support our arguments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Parameter Tuning for Multi-level Superpixel Approach</head><p>Although the multi-level superpixel approach and contrast information show significant performance, parameter tuning is the core problem that must be hurdled. The number of superpixel levels and superpixel number of each level, for instance, are the important parameters that have to be tuned in our method. Too many superpixels may lead to computational overhead and over-segmentation, while too few will lead to poor segmentation performance.</p><p>In order to illustrate these problems, we should first choose the optimal maximum number of superpixel. We used only one superpixel level to carry out the saliency extraction experiment and plotted the corresponding precision-recall curves for the superpixel number from 50 to 400 in Fig. <ref type="figure" target="#fig_12">12</ref>. By tuning the maximum number of superpixel, it is clear the proposed saliency method achieves the best overall performance when the superpixel number is set to 250. From this comparison, the optimal maximum number of superpixel in our data was chosen to be 250.</p><p>Consequently, we proceeded to choose the optimal number of superpixel levels. In Fig. <ref type="figure" target="#fig_3">13</ref>, it is shown the corresponding precision-recall cureves with the number of superpixel levels (L) set to 1, 3, 5, 7, and 9, respectively. With the optimal maximum superpixel number as 250, the number of superpixels K i at ith level is set as K i = i × 250/L, i = 1, . . . , L. If L = 3, for instance, the numbers of superpixel K i in different level i are set to be K 1 = 83 (i.e., 250/3), K 2 = 166 (i.e., 250 × 2/3), K 3 = 250, respectively.</p><p>It is clear that the performance with superpixel level set to 5 is better than the performances of 1 and 3 levels, and is similar to 7 and 9. However, the more superpixel levels are, the longer the computing time becomes. Considering the trade-off of the time and performance, we chose 5 superpixel levels as the optimal selection the experiment. This results in the numbers of superpixel at each level to be 50, 100, 150, 200, 250, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Image Coding Parameters</head><p>The proposed modified LLC image coding method acts as the second stage for WCE image description. The original LLC method gives a compact description to represent WCE images by taking the locality and sparsity into consideration. In our methods, by carrying out max pooling after dividing the original LLC code into salient and non-salient parts, the WCE images can be better encoded to emphasize the salient regions in the recognition process.</p><p>The ulcer recognition task is highly dependent on the codebook size (or vocabulary size). Intensive investigations have also been conducted to find out the relation between the recognition performance and the codebook size. The results suggested that the optimal performance could be reached by choosing the number of codewords to 50, with corresponding 92.65% accuracy and 94.12% sensitivity.</p><p>The comparison between the modified and the original LLC method shows that the modified method robustly outperforms the original method under all vocabulary sizes. This result indicates our saliency-based approach suits the WCE ulcer diagnosis very well. Furthermore, compared with other ulcer detection methods <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b10">[11]</ref>, the viability of our method is fully demonstrated in terms of higher accuracy, sensitivity and satisfactory specificity and shows viability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Limitations and Future Work</head><p>There is still room for the improvement of the proposed method. In order to make the proposed method practically useful in hospital clinical trials, further tests using a much larger number of datasets are critical to validate the effectiveness and the robustness of the proposed classification strategy. Furthermore, this study is only proposed to implement automatic ulcer recognition task for the WCE images. Other tasks, such as bleeding detection, cancer recognition, etc, that use different datasets and features will be investigated in the near future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, we proposed a two-staged fully automated computer-aided detection system to detect ulcer from WCE images. A saliency map extraction approach which is based on multi-level superpixel was proposed to segment the ulcer candidates in the first stage. In the second stage, the obtained saliency map is incorporated with the image features for performing the ulcer image recognition tasks. Since the ulcer usually corresponds to the saliency region, we propose a saliency max-pooling method integrated with the Locality-constrained Linear Coding (LLC) method to characterize the images. Experiment results achieve promising 92.65% accuracy and 94.12% sensitivity, validating the effectiveness of the proposed method. Furthermore, the comparison experiments showed that our method outperforms the state-of-the-art methods on the WCE ulcer classification task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Wireless capsule endoscopy and one captured image. (a) A typical WCE device. (b) An example image of ulcer.</figDesc><graphic coords="1,441.28,188.26,59.90,59.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Illustration of multi-level superpixel saliency extraction method. (a) WCE images as input. (b) Multi-level SLIC superpixel segmentation. (c) Saliency map estimation based on texture contrast. (c') Saliency map estimation based on color contrast. (d) Saliency map fusion across multiple levels for both texture and color saliency. (e) Final saliency map obtained by the fused texture and color saliency maps.</figDesc><graphic coords="3,320.69,260.87,72.16,72.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Illustration of the mask and the superpixel segmentation of different superpixels. (a) Image mask that outlines useful regions from WCE images. (b) A WCE image is segmented into 50 superpixels, the white lines are the boundaries of each superpixel. (c) The same image is segmented into 250 superpixels.</figDesc><graphic coords="3,478.83,261.23,72.00,72.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 (</head><label>3</label><figDesc>b), 3(c) show an example of superpixel representation of a WCE image with superpixel number of 50 and 250. Additionally, WCE images are often obscured by the large black background and obvious bounders as shown in Fig. 1(b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. An illustration of the LM filter bank that is used to extract texture contrast in the WCE images.</figDesc><graphic coords="4,77.52,56.90,193.68,65.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. WCE images with ulcer and the corresponding different color spaces images. (a) Original WCEimages. (b, c) Corresponding images of the second components of the HSV and CMYK color spaces, respectively.</figDesc><graphic coords="4,471.29,187.85,57.60,57.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Illustration of the modified LLC coding descriptor formation. (a) WCE images as input. (b) The descriptors of dSIFT, dHOG, and duniLBP are extracted independently on each image. (c) Different codebooks for the three kinds of descriptors are formed. (d) Saliency map estimated by the aforementioned method. (e) The dSIFT, dHOG, and duniLBP codes are obtained independently by the original LLC coding method. (f) Saliency based max pooling is carried out to the three kinds of codes, and the concatenated results are obtained to represent each WCE image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Fig.9. Precision-recall figure that compares the saliency estimation results between the state-of-the-art methods and the proposed one. The blue dash line, the brown dot line, the cyan dot-dash line, the circle-marked black line, the cross-marked violet line and the red line represent CA, FT, GBVS, MSSS, SDSP, and our results, respectively.</figDesc><graphic coords="7,345.38,56.83,184.09,151.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. An example of saliency map extraction of an ulcer WCE image by using the proposed approach. (a) The original WCE image. (b) Saliency maps that extracted by texture and color contrasts under different of superpixel numbers. (c) The fused saliency maps of texture and color contrasts across different superpixel levels. (d) Saliency map obtained by fusing the texture and color maps. (e) Final saliency map after applying a Gaussian kernel.</figDesc><graphic coords="8,130.79,214.89,350.16,174.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Comparison of saliency estimation results between the state-of-the-art methods and the proposed one by using four ulcer WCE images as an example. (a) original images. (b) FT. (c) CA. (d) GBVS. (e) MSSS. (f) SDSP. (g) the proposed method. (h) the binary-labeled ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. The WCE image ulcer recognition performance by the proposed LLC method under different vocabulary sizes. The red circle, blue cross and green square error bars indicate the accuracy, sensitivity, and specificity indexes, respectively. The markers show the means and the error bars show the standard deviations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. A of modified LLC method and the original one. The red square and blue circle error bars stand for the means and standard deviations of the modified LLC and the original one, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Precision-recall figure that compares the saliency estimation results under different numbers of superpixels. The black dash line, the cyan dot line, the circle-marked yellow line, the red solid line, the diamond-marked green line and the square-marked blue line represent 100, 150, 200, 250, 300, and 400, respectively</figDesc><graphic coords="10,82.36,56.73,184.25,184.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I PERFORMANCE</head><label>I</label><figDesc>COMPARISON OF ULCER DIAGNOSIS METHODS † .</figDesc><table><row><cell></cell><cell>Accuracy</cell><cell>Sensitivity</cell><cell>Specificity</cell></row><row><cell></cell><cell>(%)</cell><cell>(%)</cell><cell>(%)</cell></row><row><cell>Li et al. [8]</cell><cell cols="3">89.49 ± 0.12 87.06 ± 0.38 91.91 ± 0.13</cell></row><row><cell>CR-ULBP [9]</cell><cell>90.74 ± 0.07</cell><cell>86.62 ± 0.2</cell><cell>94.85 ± 0.15</cell></row><row><cell cols="3">DCT-LAC [10] 85.44 ± 0.18 86.03 ± 0.65</cell><cell>84.84 ± 0.4</cell></row><row><cell>Yu et al. [11]</cell><cell cols="3">82.35 ± 0.93 91.18 ± 0.05 73.53 ± 3.89</cell></row><row><cell>Ours  *</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>0278-0062 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMI.2015.2418534, IEEE Transactions on Medical Imaging</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank Thomas Yuen Tung Lam and Professor Justin Che Yuen Wu, CUHK Jockey Club Bowel Center Education Centre, Institute of Degestive Disease of the Chinese University of Hong Kong, for their professional suggestions on labeling the dataset.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Enhanced ulcer recognition from capsule endoscopic images using texture analysis</title>
		<author>
			<persName><forename type="first">V</forename><surname>Charisis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hadjileontiadis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sergiadis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">New Advances in the Basic and Clinical Gastroenterology</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="185" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Intrinsic higher-order correlation and lacunarity analysis for wce-based ulcer classification</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Charisis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Hadjileontiadis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Barroso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Sergiadis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer-Based Medical Systems (CBMS), 2012 25th International Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A randomized trial comparing wireless capsule endoscopy with push enteroscopy for the detection of smallbowel lesions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Appleyard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Fireman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Glukhovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shreiver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kadirkamanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lavy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lewkowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Scapa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shofti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Swain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zaretsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Gastroenterology</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1431" to="1438" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Small bowel enteroscopy</title>
		<author>
			<persName><forename type="first">B</forename><surname>Upchurch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vargo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Reviews in gastroenterological disorders</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">169177</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Single-balloon enteroscopy</title>
		<author>
			<persName><forename type="first">M</forename><surname>Manno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Manta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Conigliaro</surname></persName>
		</author>
		<editor>Ileoscopy, A. Trecca, Ed. Springer Milan</editor>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="79" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Wireless capsule endoscopy</title>
		<author>
			<persName><forename type="first">G</forename><surname>Iddan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Meron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Glukhovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Swain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">405</biblScope>
			<biblScope unit="page">417</biblScope>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">10 years of capsule endoscopy: an update</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Eisen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert review of gastroenterology &amp; hepatology</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="503" to="512" />
			<date type="published" when="2010-08">August 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Texture analysis for ulcer detection in capsule endoscopy images</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Q</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-H</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision computing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1336" to="1342" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Computer-aided capsule endoscopy images evaluation based on color rotation and texture features: An educational tool to physicians</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Charisis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Katsimerou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Hadjileontiadis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Liatsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Sergiadis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer-Based Medical Systems (CBMS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="203" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A curvelet-based lacunarity approach for ulcer detection from wireless capsule endoscopy images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Eid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Charisis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Hadjileontiadis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Sergiadis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer-Based Medical Systems (CBMS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="273" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ulcer detection in wireless capsule endoscopy images</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition (ICPR), 2012 21st International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="45" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Detection of small bowel polyps and ulcers in wireless capsule endoscopy videos</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karargyris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bourbakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2777" to="2786" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>IEEE Transactions on</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sdsp: A novel saliency detection method by combining simple priors</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="171" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Context-aware saliency detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Goferman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1915" to="1926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Global contrast based salient region detection</title>
		<author>
			<persName><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="409" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Saliency detection using maximum symmetric surround</title>
		<author>
			<persName><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Processing (ICIP), 2010 17th IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2653" to="2656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Frequency-tuned salient region detection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hemami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009">2009. 2009. 2009</date>
			<biblScope unit="page" from="1597" to="1604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sun: A bayesian framework for saliency using natural statistics</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of vision</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">32</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Graph-based visual saliency</title>
		<author>
			<persName><forename type="first">J</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="545" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2169" to="2178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Polyp classification based on bag of features and saliency in wireless capsule endoscopy</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Q</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-H</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="3930" to="3935" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Wireless capsule endoscopy image classification based on vector sparse coding</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE China Summit &amp; International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="582" to="586" />
		</imprint>
	</monogr>
	<note>Signal and Information Processing</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Image patch-based method for automated classification and detection of focal liver lesions on ct</title>
		<author>
			<persName><forename type="first">M</forename><surname>Safdari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pasari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Greenspan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPIE Medical Imaging. International Society for Optics and Photonics</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">700</biblScope>
			<biblScope unit="page" from="86" to="700Y" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bag-of-features based medical image retrieval via multiple assignment and visual words weighting</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1996" to="2011" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Localityconstrained linear coding for image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="3360" to="3367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Superpixel lattices</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Prince</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Warrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Mohammed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008">2008. 2008. 2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Turbopixels: Fast superpixels using geometric flows</title>
		<author>
			<persName><forename type="first">A</forename><surname>Levinshtein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Kutulakos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Dickinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Siddiqi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2290" to="2297" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>IEEE Transactions on</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multi-scale region-based saliency detection using w2 distance on n-dimensional normal distributions</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Frintrop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="176" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Linear spatial pyramid matching using sparse coding for image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009">2009. 2009. 2009</date>
			<biblScope unit="page" from="1794" to="1801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Slic superpixels compared to state-of-the-art superpixel methods</title>
		<author>
			<persName><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2274" to="2282" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>Pattern Analysis and Machine Intelligence</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Representing and recognizing the visual appearance of materials using three-dimensional textons</title>
		<author>
			<persName><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="44" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005. 2005. 2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multiresolution gray-scale and rotation invariant texture classification with local binary patterns</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Maenpaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="971" to="987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Boosting dense sift descriptors and shape contexts of face images for gender recognition</title>
		<author>
			<persName><forename type="first">J.-G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-Y</forename><surname>Yau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
			<biblScope unit="page" from="96" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Automatic head pose estimation using randomly projected dense sift descriptors</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="153" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Support-vector networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Libsvm: a library for support vector machines</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology (TIST)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Unsupervised texture segmentation using gabor filters</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Farrokhnia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Systems, Man and Cybernetics</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1990">1990. 1990</date>
			<biblScope unit="page" from="14" to="19" />
		</imprint>
	</monogr>
	<note>Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Statistical texture characterization from discrete wavelet representations</title>
		<author>
			<persName><forename type="first">G</forename><surname>Van De Wouwer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Scheunders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Van Dyck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="592" to="598" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
	<note>Image Processing</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
