<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Under review as a conference paper at ICLR 2021 ADAPTIVE UNIVERSAL GENERALIZED PAGERANK GRAPH NEURAL NETWORK</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<title level="a" type="main">Under review as a conference paper at ICLR 2021 ADAPTIVE UNIVERSAL GENERALIZED PAGERANK GRAPH NEURAL NETWORK</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In many important graph data processing applications the acquired information includes both node features and observations of the graph topology. Graph neural networks (GNNs) are designed to exploit both sources of evidence but they do not optimally trade-off their utility and integrate them in a manner that is also universal. Here, universality refers to independence on homophily or heterophily graph assumptions. We address these issues by introducing a new Generalized PageRank (GPR) GNN architecture that adaptively learns the GPR weights so as to jointly optimize node feature and topological information extraction, regardless of the extent to which the node labels are homophilic or heterophilic. Learned GPR weights automatically adjust to the node label pattern, irrelevant on the type of initialization, and thereby guarantee excellent learning performance for label patterns that are usually hard to handle. Furthermore, they allow one to avoid feature over-smoothing, a process which renders feature information nondiscriminative, without requiring the network to be shallow. Our accompanying theoretical analysis of the GPR-GNN method is facilitated by novel synthetic benchmark datasets generated by the so-called contextual stochastic block model. We also compare the performance of our GNN architecture with that of several state-ofthe-art GNNs on the problem of node-classification, using well-known benchmark homophilic and heterophilic datasets. The results demonstrate that GPR-GNN offers significant performance improvement compared to existing techniques on both synthetic and benchmark data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph-centered machine learning has received significant interest in recent years due to the ubiquity of graph-structured data and its importance in solving numerous real-world problems such as semisupervised node classification and graph classification <ref type="bibr" target="#b41">(Zhu, 2005;</ref><ref type="bibr" target="#b31">Shervashidze et al., 2011;</ref><ref type="bibr" target="#b24">Lü &amp; Zhou, 2011)</ref>. Usually, the data at hand contains two sources of information: Node features and graph topology. As an example, in social networks, nodes represent users that have different combinations of interests and properties captured by their corresponding feature vectors; edges on the other hand document observable friendship and collaboration relations that may or may not depend on the node features. Hence, learning methods that are able to simultaneously and adaptively exploit node features and the graph topology are highly desirable as they make use of their latent connections and thereby improve learning on graphs.</p><p>Graph neural networks (GNN) leverage their representational power to provide state-of-the-art performance when addressing the above described application domains. Many GNNs use message passing <ref type="bibr" target="#b12">(Gilmer et al., 2017;</ref><ref type="bibr" target="#b3">Battaglia et al., 2018)</ref> to manipulate node features and graph topology. They are constructed by stacking (graph) neural network layers which essentially propagate and transform node features over the given graph topology. Different types of layers have been proposed and used in practice, including graph convolutional layers (GCN) <ref type="bibr" target="#b6">(Bruna et al., 2014;</ref><ref type="bibr" target="#b18">Kipf &amp; Welling, 2017)</ref>, graph attention layers (GAT) <ref type="bibr" target="#b34">(Velickovic et al., 2018)</ref> and many others <ref type="bibr" target="#b14">(Hamilton et al., 2017;</ref><ref type="bibr">Wijesinghe &amp; Wang, 2019;</ref><ref type="bibr" target="#b39">Zeng et al., 2020;</ref><ref type="bibr" target="#b2">Abu-El-Haija et al., 2019)</ref>.</p><p>However, most of the existing GNN architectures have two fundamental weaknesses which restrict their learning ability on general graph-structured data. First, most of them seem to be tailor-made to work on homophilic (associative) graphs. The homophily principle <ref type="bibr" target="#b26">(McPherson et al., 2001)</ref> in the context of node classification asserts that nodes from the same class tend to form edges. Homophily is also a common assumption in graph clustering <ref type="bibr" target="#b35">(Von Luxburg, 2007;</ref><ref type="bibr" target="#b33">Tsourakakis, 2015;</ref><ref type="bibr" target="#b8">Dau &amp; Milenkovic, 2017)</ref> and in many GNNs design <ref type="bibr" target="#b19">(Klicpera et al., 2018)</ref>. Methods developed for homophilic graphs are nonuniversal in so far that they fail to properly solve learning problems on heterophilic (disassortative) graphs <ref type="bibr" target="#b28">(Pei et al., 2019;</ref><ref type="bibr" target="#b4">Bojchevski et al., 2019;</ref><ref type="bibr">2020)</ref>. In heterophilic graphs, nodes with distinct labels are more likely to link together (For example, many people tend to preferentially connect with people of the opposite sex in dating graphs, different classes of amino acids are more likely to connect within many protein structures <ref type="bibr">(Zhu et al., 2020) etc)</ref>. GNNs model the homophily principle by aggregating node features within graph neighborhoods. For this purpose, they use different mechanisms such as averaging in each network layer. Neighborhood aggregation is problematic and significantly more difficult for heterophilic graphs <ref type="bibr" target="#b16">(Jia &amp; Benson, 2020)</ref>.</p><p>Second, most of the existing GNNs fail to be "deep enough". Although in principle an arbitrary number of layers may be stacked, practical models are usually shallow (including 2-4 layers) as these architectures are known to achieve better empirical performance than deep networks. A widely accepted explanation for the performance degradation of GNNs with increasing depth is feature-oversmoothing, which may be intuitively explained as follows. The process of GNN feature propagating represents a form of random walks on "feature graphs," and under proper conditions, such random walks converge with exponential rate to their stationary points. This essentially levels the expressive power of the features and renders them nondiscriminative. This intuitive reasoning was first described for linear settings in <ref type="bibr" target="#b23">Li et al. (2018)</ref> and has been recently studied in <ref type="bibr" target="#b27">Oono &amp; Suzuki (2020)</ref> for a setting involving nonlinear rectifiers.</p><p>We address these two described weaknesses by combining GNNs with Generalized PageRank techniques (GPR) within a new model termed GPR-GNN. The GPR-GNN architecture is designed to first learn the hidden features and then to propagate them via GPR techniques. The focal component of the network is the GPR procedure that associates each step of feature propagation with a learnable weight. The weights depend on the contributions of different steps during the information propagation procedure, and they can be both positive and negative. This departures from common nonnegativity assumptions <ref type="bibr" target="#b19">(Klicpera et al., 2018)</ref> allows for the signs of the weights to adapt to the homophily/heterophily structure of the underlying graphs. The amplitudes of the weights trade-off the degree of smoothing of node features and the aggregation power of topological features. These traits do not change with the choice of the initialization procedure and elucidate the process used to combine node features and the graph structure so as to achieve (near)-optimal predictions. In summary, the GPR-GNN method can simultaneously learn the node label patterns of disparate classes of graphs and prevent feature over-smoothing.</p><p>The excellent performance of GPR-GNN is demonstrated empirically, on real world datasets, and further supported through a number of theoretical findings. In the latter setting, we show that the GPR procedure relates to general polynomial graph filtering, which can naturally deal with both high and low frequency parts of the graph signals. In contrast, recent GNN models that utilize Personalized PageRanks (PPR) with fixed weights <ref type="bibr" target="#b36">(Wu et al., 2019;</ref><ref type="bibr" target="#b19">Klicpera et al., 2018;</ref><ref type="bibr">2019)</ref> inevitably act as low-pass filters. Thus, they fail to learn the labels of heterophilic graphs. We also establish that GPR-GNN can provably mitigate the feature-over-smoothing issue in an adaptive manner even after large-step propagation (i.e., after a large number of propagation steps). Hence, the method is able to make use of informative large-step propagation.</p><p>To test the performance of GPR-GNN on homophilic and heterophilic node label patterns and determine the trade-off between node and topological feature exploration, we first describe the recently proposed contextual stochastic block model (cSBM) <ref type="bibr" target="#b10">(Deshpande et al., 2018)</ref>. The cSBM allows for smoothly controlling the "informativeness ratio" between node features and graph topology, where the graph can vary from being highly homophilic to highly heterophilic. We show that GPR-GNN outperforms all other baseline methods for the task of semi-supervised node classification on the cSBM consistently from strong homophily to strong heterophily. We then proceed to show that GPR-GNN offers state-of-the-art performance on node-classification benchmark real-world datasets which contain both homophilic and heterophilic graphs. Due to the space limit, we put all proofs, formal theorem statements, and the conclusion section in the Supplement. The learnt GPR weights of the GPR-GNN on real world datasets. Cora is homophilic while Texas is heterophilic (Here, H stands for the level of homophily defined below). An interesting trend may be observed: For the heterophilic case the weights alternate from positive to negative with dampening amplitudes (more examples are provided in Section 5). The shaded region corresponds to a 95% confidence interval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES</head><p>Let G = (V, E) be an undirected graph with nodes V and edges E. Let n denote the number of nodes, assumed to belong to one of C ≥ 2 classes. The nodes are associated with the node feature matrix X ∈ R n×f , where f denotes the number of features per node. Throughout the paper, we use X i: to indicate the i th row and X :j to indicate the j th column of the matrix X, respectively. The symbol δ ij is reserved for the Kronecker delta function. The graph G is described by the adjacency matrix A, while Ã stands for the adjacency matrix for a graph with added self-loops. We let D be the diagonal degree matrix of Ã and Ãsym = D−1/2 Ã D−1/2 denote the symmetric normalized adjacency matrix with self-loops.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">GPR-GNNS: MOTIVATION AND CONTRIBUTIONS</head><p>Generalized PageRanks. Generalized PageRank (GPR) methods were first used in the context of unsupervised graph clustering where they showed significant performance improvements over <ref type="bibr">Personalized PageRank (Kloumann et al., 2017;</ref><ref type="bibr" target="#b22">Li et al., 2019)</ref>. The operational principles of GPRs can be succinctly described as follows. Given a seed node s ∈ V in some cluster of the graph, a one-dimensional feature vector k) , where the parameters γ k ∈ R, k = 0, 1, 2, . . ., are referred to as the GPR weights. Clustering of the graph is performed locally by thresholding the GPR score. Certain PangRank methods, such as Personalized PageRank or heat-kernel PageRank <ref type="bibr" target="#b7">(Chung, 2007)</ref>, are associated with specific choices of GPR weights <ref type="bibr" target="#b22">(Li et al., 2019)</ref>. For an excellent in-depth discussion of PageRank methods, the interested reader is referred to <ref type="bibr" target="#b13">(Gleich, 2015)</ref>. The work in <ref type="bibr" target="#b22">Li et al. (2019)</ref> recently introduced and theoretically analyzed a special form of GPR termed Inverse PR (IPR) and showed that long random walk paths are more beneficial for clustering then previously assumed, provided that the GPR weights are properly selected (Note that IPR was developed for homophilic graphs and optimal GPR weights for heterophilic graphs are not currently known).</p><formula xml:id="formula_0">H (0) ∈ R n×1 is initialized according to H (0) v: = δ vs . The GPR score is defined as ∞ k=0 γ k Ãk sym H (0) = ∞ k=0 γ k H (</formula><p>Equivalence of the GPR method and polynomial graph filtering. If we truncate the infinite sum in the definition of GPR at some natural number K, K k=0 γ k Ãk sym corresponds to a polynomial graph filter of order K. Thus, learning the optimal GPR weights is equivalent to learning the optimal polynomial graph filter. Note that one can approximate any graph filter using a polynomial graph filter <ref type="bibr" target="#b32">(Shuman et al., 2013)</ref> and hence the GPR method is able to deal with a large range of different node label patterns. Also, increasing K allows one to better approximate the underlying optimal graph filter. This once again shows that large-step propagation is beneficial.</p><p>Universality with respect to node label patterns: Homophily versus heterophily. In their recent work, <ref type="bibr" target="#b28">Pei et al. (2019)</ref> proposed an index to measure the level of homophily of nodes in a graph H(G) = 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>|V | v∈V</head><p>Number of neighbors of v ∈ V that have the same label as v Number of neighbors of v</p><p>. Note that H(G) → 1 corresponds to strong homophily while H(G) → 0 indicates strong heterophily. Figures <ref type="figure">1 (b</ref>) and (c) plot the GPR weights learnt by our GPR-GNN method on a homophilic (Cora) and heterophilic (Texas) dataset. The learnt GPR weights from Cora match the behavior of IPR <ref type="bibr" target="#b22">(Li et al., 2019)</ref>, which verifies that large-step propagation is indeed of great importance for homophilic graphs. The GPR weights learnt from Texas behave significantly differently from all known PR variants, taking a number of negative values. These differences in weight patterns are observed under random initialization, demonstrating that the weights are actually learned by the network and not forced by specific initialization. Furthermore, the large difference in the GPR weights for these two graph models illustrates the learning power of GPR-GNN and their universal adaptability.</p><p>The over-smoothing problem. One of the key components in most GNN models is the graph convolutional layer, described by</p><formula xml:id="formula_1">H (k) GCN = ReLU Ãsym H (k−1) GCN W (k) , PGCN = softmax Ãsym H (K−1) GCN W (k) ,</formula><p>where H (0) GCN = X and W (k) represents the trainable weight matrix for the k th layer. The key issue that limits stacking multiple layers is the over-smoothing phenomenon: If one were to remove ReLU in the above expression, lim k→∞ Ãk sym H (0) = H (∞) , where each row of H (∞) only depends on the degree of the corresponding node, provided that the graph is irreducible and aperiodic. This shows that the model looses discriminative information provided by the node features as the number of layers increases.</p><p>Mitigating graph heterophily and over-smoothing issues with the GPR-GNN model. GPR-GNN first extracts hidden state features for each node and then uses GPR to propagate them. The GPR-GNN process can be mathematically described as:</p><formula xml:id="formula_2">P = softmax(Z), Z = K k=0 γ k H (k) , H (k) = Ãsym H (k−1) , H (0) i: = f θ (X i: ),<label>(1)</label></formula><p>where f θ (.) represents a neural network with parameter set {θ} that generates the hidden state features H (0) . The GPR weights γ k are trained together with {θ} in an end-to-end fashion. The GPR-GNN model is easy to interpret: As already pointed out, GPR-GNN has the ability to adaptively control the contribution of each propagation step and adjust it to the node label pattern. Examining the learnt GPR weights also helps with elucidating the properties of the topological information of a graph (i.e., determining the optimal polynomial graph filter), as illustrated in Figure <ref type="figure">1</ref> (b) and (c).</p><p>Placing GPR-GNNs in the context of related prior work. Among the methods that differ from repeated stacking of GCN layers, APPNP <ref type="bibr" target="#b19">(Klicpera et al., 2018)</ref> represents one of the state-of-theart GNNs that is related to our GPR-GNN approach. It can be easily seen that APPNP as well as SGC <ref type="bibr" target="#b36">(Wu et al., 2019)</ref> are special cases of our model since APPNP fixes</p><formula xml:id="formula_3">γ k = α(1−α) k , γ K = (1− α) K ,</formula><p>while SGC removes all nonlinearities with γ k = δ kK , respectively. These two weight choices correspond to Personalized PageRank (PPR) <ref type="bibr" target="#b15">(Jeh &amp; Widom, 2003)</ref>, which is known to be suboptimal compared to the IPR framework when applied to homophilic node classification <ref type="bibr" target="#b22">(Li et al., 2019)</ref>.</p><p>Fixing the GPR weights makes the model unable to adaptively learn the optimal propagation rules which is of crucial importance: As we will show in Section 4, the fixed PPR weights corresponds to low-pass graph filters which makes them inadequate for learning on heterophilic graphs. The recent work <ref type="bibr" target="#b19">(Klicpera et al., 2018)</ref> showed that fixed PPR weights (APPNP) can also provably resolve the over-smoothing problem. However, the way APPNP prevents over-smoothing is independent on the node label information. In contrast, the escape of GPR-GNN from over-smoothing is guided by the node label information (Theorem 4.2). A detailed discussion of this phenomena along with illustrative examples is delegated to the Supplement.</p><p>Among the GCN-like models, JK-Net <ref type="bibr" target="#b37">(Xu et al., 2018)</ref> exhibits some similarities with GPR-GNN.</p><p>It also aggregates the outputs of different GCN layers to arrive at the final output. On the other hand, the GCN-Cheby method <ref type="bibr" target="#b9">(Defferrard et al., 2016;</ref><ref type="bibr" target="#b18">Kipf &amp; Welling, 2017</ref>) is related to polynomial graph filtering, where each convolutional layer propagates multiple steps and the graph filter is related to Chebyshev polynomials. In both cases, the depth of the models is limited in practice <ref type="bibr" target="#b19">(Klicpera et al., 2018)</ref> and they are not easy to interpret as our GPR-GNN method. Some prior work also emphasizes adaptively learning the importance of different steps <ref type="bibr" target="#b1">(Abu-El-Haija et al., 2018;</ref><ref type="bibr">Berberidis et al., 2018)</ref>. Nevertheless, none of the above works is applicable for semisupervised learning with GNNs and considers heterophilic graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">THEORETICAL PROPERTIES OF GPR-GNNS</head><p>Graph filtering aspects of GPR-GNNs. As mentioned in Section 3, the GPR component of the network may be viewed as a polynomial graph filter. Let Ãsym = UΛU T be the eigenvalue decomposition of Ãsym . Then, the corresponding polynomial graph filter equals</p><formula xml:id="formula_4">K k=0 γ k Ãk sym = Ug γ,K (Λ)U T , where g γ,K (Λ) is applied element-wise and g γ,K (λ) = K k=0 γ k λ k . We estab- lished the following result. Theorem 4.1 (Informal). Assume that the graph G is connected. If γ k ≥ 0 ∀k ∈ {0, 1, ..., K}, K k=0 γ k = 1 and ∃k &gt; 0 such that γ k &gt; 0, then g γ,K (•) is a low-pass graph filter. Also, if γ k = (−α) k , α ∈ (0, 1) and K is large enough, then g γ,K (•) is a high-pass graph filter.</formula><p>By Theorem 4.1 and from our discussion in Section 3, we know that both APPNP and SGC will invariably suppress the high frequency components. Thus, they are inadequate for use on heterophilic graphs. In contrast, if one allows γ k to be negative and learned adaptively the graph filter will pass relevant high frequencies. This is what allows GPR-GNN to perform exceptionally well on heterophilic graphs (see Figure <ref type="figure" target="#fig_1">2(c)</ref>).</p><p>GPR-GNN can escape from over-smoothing. As already emphasized, one crucial innovation of the GPR-GNN method is to make the GPR weights adaptively learnable, which allows GPR-GNN to avoid over-smoothing and trade node and topology feature informativeness. Intuitively, when largestep propagation is not beneficial, it increases the training loss. Hence, the corresponding GPR weights should decay in magnitude. This observation is captured by the following result, whose more formal statement and proof are delegated to the Supplement due to space limitations. Theorem 4.2 (Informal). Assume the graph G is connected and the training set contains nodes from each of the classes. Also assume that k is large enough so that the over-smoothing effect occurs for H (k) , ∀k ≥ k which dominate the contribution to the final output Z. Then, the gradients of γ k and γ k are identical in sign for all k ≥ k .</p><p>Theorem 4.2 shows that as long as over-smoothing happens, |γ k | will approach 0 for all k ≥ k when we use an optimizer such as stochastic gradient descent (SGD) which has a suitable learning rate decay. This reduces the contribution of the corresponding steps H (k) in the final output Z. When the weights |γ k | are small enough so that H (k) no longer dominates the value of the final output Z, the over-smoothing effect is eliminated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS FOR NEW CSBM SYNTHETIC AND REAL-WORLD DATASETS</head><p>Synthetic data. In order to test the ability of label learning of GNNs on graphs with arbitrary levels of homophily and heterophily, we propose to use cSBMs <ref type="bibr" target="#b10">(Deshpande et al., 2018)</ref> to generate synthetic graphs. We consider the case with two equal-size classes. In cSBMs, the node features are Gaussian random vectors, where the mean of the Gaussian depends on the community assignment. The difference of the means is controlled by a parameter µ, while the difference of the edge densities in the communities and between the communities is controlled by a parameter λ. Hence µ and λ capture the "relative informativeness" of node features and the graph topology, respectively. Moreover, positive λ s correspond to homophilic graphs while negative λ s correspond to heterophilic graphs. The information-theoretic limits of reconstruction for the cSBM are characterized in <ref type="bibr" target="#b10">Deshpande et al. (2018)</ref>. The results show that, asymptotically, one needs λ 2 + µ 2 /ξ &gt; 1 to ensure a vanishing ratio of the misclassified nodes and the total number of nodes, where ξ = n/f and f as before denotes the dimension of the node feature vector.</p><p>Note that given a tolerance value &gt; 0, λ 2 + µ 2 /ξ = 1 + is an arc of an ellipsoid for which λ ≥ 0 and µ ≥ 0. To fairly and continuously control the extent of information carried by the node features and graph topology, we introduce a parameter φ = arctan( λ</p><formula xml:id="formula_5">√ ξ µ ) × 2 π .</formula><p>The setting φ = 0 indicates that only node features are informative, while |φ| = 1 indicates that only the graph topology is informative. Moreover, φ = 1 corresponds to strongly homophilic graphs while φ = −1 corresponds to strongly heterophilic graphs. Note that the values φ and −φ convey the same amount of information regarding graph topology. This is due to the fact that λ 2 = (−λ) 2 . Ideally, GNNs that are able to optimally learn on both homophilic and heterophilic graph should have similar performances for φ and −φ. Due to space limitation we refer the interested reader to <ref type="bibr" target="#b10">(Deshpande et al., 2018)</ref> for a review of all formal theoretical results and only outline the cSBM properties needed for our analysis. Additional information is also available in the Supplement.</p><p>Our experimental setup examines the semi-supervised node classification task in the transductive setting. We consider two different choices for the random split into training/validation/test samples, which we call sparse splitting (2.5%/2.5%/95%) and dense splitting (60%/20%/20%), respectively. The sparse splittnig is more similar to the original semi-supervised setting considered in <ref type="bibr" target="#b18">Kipf &amp; Welling (2017)</ref> while the dense setting is considered in <ref type="bibr" target="#b28">Pei et al. (2019)</ref> for studying heterophilic graphs. For all datasets, we run each experiment 100 times with multiple random splits and different initializations.</p><p>Methods used for comparisons. We compare GPR-GNN with 6 baseline models: MLP, GCN <ref type="bibr" target="#b18">(Kipf &amp; Welling, 2017)</ref>, GAT <ref type="bibr" target="#b34">(Velickovic et al., 2018)</ref>, JK-Net <ref type="bibr" target="#b37">(Xu et al., 2018)</ref>, GCN-Cheby <ref type="bibr" target="#b9">(Defferrard et al., 2016)</ref>, APPNP <ref type="bibr" target="#b19">(Klicpera et al., 2018)</ref>, SGC <ref type="bibr" target="#b36">(Wu et al., 2019)</ref>, SAGE <ref type="bibr" target="#b14">(Hamilton et al., 2017)</ref> and Geom-GCN <ref type="bibr" target="#b28">(Pei et al., 2019)</ref>. For all these architectures, we use the corresponding Pytorch Geometric library implementations <ref type="bibr" target="#b11">(Fey &amp; Lenssen, 2019)</ref>. For Geom-GCN, we directly use the code provided by the authors<ref type="foot" target="#foot_0">1</ref> . We could not test Geom-GCN on cSBM and other datasets not originally tested in the paper due to a preprocessing subroutine that is not publicly available <ref type="bibr" target="#b28">(Pei et al., 2019)</ref>.</p><p>The GPR-GNN model setup and hyperparameter tuning. We choose random walk path lengths with K = 10 and use a 2-layer (MLP) with 64 hidden units for the NN component. For the GPR weights, we use different initializations including PPR with α ∈ {0.1, 0.2, 0.5, 0.9}, γ k = δ 0k or δ Kk and the default random initialization in pytorch. Similarly, for APPNP we search the optimal α within {0.1, 0.2, 0.5, 0.9}. For other hyperparameter tuning, we optimize the learning rate over {0.002, 0.01, 0.05} and weight decay {0.0, 0.0005} for all models. For Geom-GCN, we use the best variants in the original paper for each dataset. Finally, we use GPR-GNN(rand) to describe the results obtained with random initialization of the GPR weights. Further experimental settings are discussed in the Supplement.</p><p>Results. We examine the robustness of all baseline methods and GPR-GNN using cSBM-generated data with φ ∈ {−1, −0.75, −0.5, ..., 1}, which includes graphs across the heterophily/homophily spectrum. The results are summarized in Figure <ref type="figure" target="#fig_1">2</ref>. For both the sparse and dense setting, GPR-GNN significantly outperforms all other baseline models whenever φ &lt; 0 (heterophilic graphs). On the other hand, all baseline GNNs can be worse then simple MLP when the graph information is weak (φ = 0, −0.25). This shows that existing GNNs cannot apply to arbitrary graphs, while GPR-GNN is clearly more robust. APPNP methods have the worst performance on strongly heterophilic graphs. This is in agreement with the result of Theorem 4.1 which asserts that APPNP intrinsically acts a low-pass filter and is thus inadequate for strong heterophily settings. JKNet, GCN-Cheby and SAGE are the only three baseline models that are able to learn strongly heterophilic graphs under dense splitting. This is also to be expected since JKNet is the only baseline model that combines results from different steps at the last layer, which is similar to what is done in GPR-GNN. GCN-Cheby uses multiple steps in each layers which allows it to partially adapt to heterophilic settings as each layer is related to a polynomial graph filter of higher order compared to that of GCN. SAGE treats ego-embeddings and embeddings from neighboring nodes  Also, we observe that random initialization of our GPR weights only results in slight performance drops under dense splitting (The drop is more evident for sparse splitting setting but our method still outperforms baseline models by a large margin for strongly heterophilic graphs). This is also to be expected as we have less label information in the sparse splitting setting where the implicit bias provided by good GPR initialization is helpful. The implicit bias becomes irrelevant for the dense splitting setting, since the available label information is sufficiently rich.</p><p>Besides the strong performance of GPR-GNN, the other benefit is its interpretability. In Figure <ref type="figure" target="#fig_3">3</ref>, we demonstrate the learnt GPR weights by our GPR-GNN on cSBM with random initialization. When the graph is weak homophilic (φ = 0.25), the learnt GPR weights are decreasing. This is similar to the PPR weights used in APPNP, despite that the decaying speed is different. When the graph is strong homophilic (φ = 0.75), the learnt GPR weights are increasing which is significantly different from the PPR weights. This result matches the recent finding in <ref type="bibr" target="#b22">Li et al. (2019)</ref> and behave similar to IPR proposed by the authors. On the other hand, the learnt GPR weights have zig-zag shape when the graph is heterophilic. This again validates Theorem 4.1 as GPR weights with alternating signs correspond to a high-pass filter. Interestingly, when φ = −0.25 the magnitude of learnt GPR weight is decreasing. This is because the graph information is weak and the node feature information is more important in this case. It makes sense that the learnt GPR weight focus on the first few steps. Hence, we have validated the interpretablity of GPR-GNN. In practice, one can use the learnt GPR weights to better understand the graph structured data at hand. We showcase this benefit in the results of real world benchmark datasets.</p><p>Real world benchmark datasets. We use 5 homophilic benchmark datasets available from the Pytorch Geometric library, including the citation graphs Cora, CiteSeer, PubMed <ref type="bibr" target="#b29">(Sen et al., 2008;</ref><ref type="bibr" target="#b38">Yang et al., 2016)</ref> and the Amazon co-purchase graphs Computers and Photo <ref type="bibr" target="#b25">(McAuley et al., 2015;</ref><ref type="bibr" target="#b30">Shchur et al., 2018)</ref>. We also use 5 heterophilic benchmark datasets tested in <ref type="bibr" target="#b28">Pei et al. (2019)</ref>, including Wikipedia graphs Chameleon and Squirrel, the Actor co-occurrence graph, and webpage graphs Texas and Cornell from WebKB<ref type="foot" target="#foot_1">2</ref> . We summarize the dataset statistics in Table <ref type="table" target="#tab_0">1</ref>. Results on real-world datasets. We use accuracy (the micro-F1 score) as the evaluation metric along with a 95% confidence interval. The relevant results are summarized in Table <ref type="table" target="#tab_1">2</ref>. For homophilic datasets, we provide results for sparse splitting which is more aligned with the original setting used   in <ref type="bibr" target="#b18">Kipf &amp; Welling (2017)</ref>; <ref type="bibr" target="#b30">Shchur et al. (2018)</ref>. For the heterophilic datasets, we adopt dense splitting which is used in <ref type="bibr" target="#b28">Pei et al. (2019)</ref>.</p><p>Table <ref type="table" target="#tab_1">2</ref> shows that, in general, GPR-GNN outperforms all tested methods. On homophilic datasets, GPR-GNN achieves the state-of-the-art performance. On heterophilic datasets, GPR-GNN significantly outperforms all the other baseline models. It is important to point out that there are two different patterns to be observed among the heterophilic datasets. On Chameleon and Squirrel, MLP and APPNP perform worse then other baseline methods such as GCN and JKNet. In contrast, MLP and APPNP outperform the other baseline methods on Actor, Texas and Cornell. We conjecture that this is due to the fact that the graph topology information is strong and weak, respectively. Note that these two patterns match the results of the cSBM experiments for φ close to −1 and 0, respectively (Figure <ref type="figure" target="#fig_1">2</ref>). Furthermore, the homophily measure H(G) proposed by <ref type="bibr" target="#b28">Pei et al. (2019)</ref> cannot characterize such differences in heterophilic datasets. We relegate the more detailed discussion of this topic along with illustrative examples to the Supplement.</p><p>For fairness, we also repeated the experiment involving GeomGCN on homophilic datasets using a dense split -the observed performance pattern tends to be similar which can be found in Supplement.</p><p>We also examined the learned GPR weights on real datasets in Figure <ref type="figure" target="#fig_5">4</ref>. Due to space limitations, a more comprehensive GPR weight analysis for other datasets is deferred to the Supplement. We can see that learned GPR weights are all positive for homophilic datasets (PubMed and Photo). In contrast, some GPR weights learned from heterophilic datasets (Actor and Squirrel) are negative. These results agree with the patterns observed on cSBMs. Interestingly, the learned weight γ 0 has the largest magnitude for the Actor dataset. This indicates that most of the information is contained in node features. From Table <ref type="table" target="#tab_1">2</ref> we can also see that MLPs indeed outperforms most baseline GNNs (this is similar to the case of cSBM(φ = −0.25)). On the other hand, GPR weights learned from Squirrel have a zig-zag pattern. This implies that graph topology is more informative for Squirrel compared to Actor. From Table <ref type="table" target="#tab_1">2</ref> we also see that baseline GNNs also outperform MLPs on Squirrel.</p><p>Escaping from over-smoothing and dynamics of learning GPR weights. To demonstrate the ability of GPR-GNNs to escape from over-smoothing, we choose the initial GPR weights to be γ k = δ kK . This ensures that over-smoothing effects are present with high probability at the very beginning of the learning process. On cSBM(φ = −1) with dense splitting, we find that for 96 out of 100 runs, GPR-GNN predicts the same labels for all nodes at epoch 0, which implies that over-smoothing indeed occurs immediately. The final prediction is 98.79% accurate which is much larger than the initial accuracy of 50.07% at epoch 0. Similar results can be observed for other datasets and this verifies our theoretical findings. We plot the dynamics of the learned GPR weights in Figure <ref type="figure" target="#fig_5">4</ref>(e)-(h), which shows that the peak at last step is indeed reduced while the GPR weights for other steps are significantly increased in magnitude. More results on the dynamics of learning GPR weights may be found in the Supplement.</p><p>Efficiency analysis. We also examine the computational complexity of GPR-GNNs compared to other baseline models. We report the empirical training time in Table <ref type="table" target="#tab_2">3</ref>. Compared to APPNP, we only need to learn K + 1 additional GPR weights for GPR-GNN, and usually K ≤ 20 (i.e. we choose K = 10 in our experiments). This in turn requires K + 1 gradient computations which are dominated by the computations performed by the neural network module f θ . We can observe from Table <ref type="table" target="#tab_2">3</ref> that indeed GPR-GNN has a running time similar to that of APPNP. It is nevertheless worth pointing out that the authors of <ref type="bibr" target="#b5">Bojchevski et al. (2020)</ref> successfully scaled APPNP to operate on large graphs. Whether the same techniques may be used to scale GPR-GNNs is an interesting open question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS</head><p>We addressed two fundamental weaknesses of existing GNNs: Failing to act as universal learners by not generalizing to heterophilic graphs and making use of large number of propagation steps. We developed a novel GPR-GNN architecture which combines adaptive generalized PageRank (GPR) scheme with GNNs. We theoretically showed that our method does not only mitigates feature oversmoothing but also works on highly diverse node label patterns. We also tested GPR-GNNs on both homophilic and heterophilic node label patterns, and proposed a novel synthetic benchmark datasets generated by the contextual stochastic block model. Our experiments on real-world benchmark datasets showed clear performance gains of GPR-GNN over the state-of-the-art methods. Moreover, we showed that GPR-GNN has desirable interpretability properties which could be of independent interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX</head><p>A.1 DETAILED DISCUSSION ON PREVENTING OVER-SMOOTHING.</p><p>As mentioned in Section 4, another method -APPNP -can also provably prevents oversmoothing <ref type="bibr" target="#b19">Klicpera et al. (2018)</ref>. The authors of this study use the fact that the PPR propagation will converge to Π ppr H (0) , where Π ppr = α(I n − (1 − α) Ãsym ) −1 is independent on the node label information provided in the training data. Each row of Π ppr H (0) still depends on H (0) and thus APPNP will not suffer from the over-smoothing effect. However, since Π ppr is independent of the label information, it can cause undesired consequences that we discuss in what follows.    <ref type="formula">c</ref>) are identical and the only difference is the class label assignment. In Figure <ref type="figure" target="#fig_8">5</ref> (b), the graph is homophilic and hence the optimal graph filter should emphasize the low-frequency part of the graph signal. In contrast, in Figure <ref type="figure" target="#fig_8">5</ref> (c), the graph is heterophilic as the graph is bipartite with respect to the labels. Hence, the optimal graph filter should emphasize the high-frequency part of the graph signal. This example illustrates that the optimal graph filter should depend on both the graph topology and the node label information. Recall that the equivalent graph filter that APPNP uses in the asymptotic regime is Π ppr which is independent on the node label information. Also, Theorem 4.1 established that APPNP intrinsically utilizes a low-pass filter. In contrast, GPR-GNN learns the GPR weights guided by the node label information which allows it to account for both cases (homophilic and heterophilic) shown.  , where the color of the nodes indicates their label. In case 1, blue and green nodes link to all orange and purple nodes. In case 2, blue nodes only link to orange nodes and green nodes only link to purple nodes. From the definition of H(G) one can see that both cases have H(G) = 0, since in both cases nodes do not link to other nodes of the same label. However, it is obvious that the graph topology carries more node label information in case 2 compared to case 1. In fact, for case 1 it is impossible to distinguish blue and green nodes merely from the graph topology (and the same is true of orange and purple nodes). One possible alternative for the homophily measure is the Chernoff-Hellinger divergence Abbe (2017) of the empirical edge probability matrix B; here B ij is the empirical probability of an edge with one end node labeled i and the other labeled j. The intuition behind our suggestion lies in the fact that the Chernoff-Hellinger divergence characterizes the fundamental limit of SBMs. However, as many practical graph generative processes may significantly differ from SBMs, investigating alternative homophily/heterophily measures is another interesting open problem. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 DISCUSSION ON THE INSUFFICIENCY OF HOMOPHILY MEASURE H(G)</head><formula xml:id="formula_6">G is connected. Let λ 1 ≥ λ 2 ≥ ... ≥ λ n be the eigenvalues of Ãsym . If γ k ≥ 0 ∀k ∈ {0, 1, ..., K}, K k=0 γ k = 1 and ∃k &gt; 0 such that γ k &gt; 0, then |g γ,K (λ i )/g γ,K (λ 1 )| &lt; |λ i /λ 1 | ∀i ≥ 2. Also, if γ k = (−α) k , α ∈ (0, 1) and K → ∞, then | lim K→∞ g γ,K (λ i )/ lim K→∞ g γ,K (λ 1 )| &gt; |λ i /λ 1 | ∀i ≥ 2. Note that |g γ,K (λ i )/g γ,K (λ 1 )| &lt; |λ i /λ 1 | ∀i ≥ 2 implies</formula><p>that after applying the graph filter g γ,K , the lowest frequency component (correspond to λ 1 ) further dominates. Hence g γ,K acts like a low pass filter in this case. In contrast,</p><formula xml:id="formula_7">| lim K→∞ g γ,K (λ i )/ lim K→∞ g γ,K (λ 1 )| &gt; |λ i /λ 1 | ∀i ≥ 2</formula><p>implies that after applying the graph filter, the lowest frequency component (correspond to λ 1 ) no longer dominates. This correspond to the high pass filter case.</p><p>Proof. We start with the low pass filter result. From basic spectral analysis <ref type="bibr" target="#b35">(Von Luxburg, 2007)</ref> we know that λ 1 = 1 and |λ i | &lt; 1, ∀i ≥ 2. One can also find the analysis in the proof of our Lemma A.2 in the Supplement. Then by assumption we know that</p><formula xml:id="formula_8">g γ,K (λ 1 ) = K k=0 γ k = 1. Hence, proving Theorem A.1 is equivalent to show |g γ,K (λ i )| &lt; |λ i | ∀i ≥ 2.</formula><p>This is obvious since g γ,K (λ) = K k=0 γ k λ k is a polynomial of order K with nonnegative coefficients. It is easy to check that ∀k ≥ 1, |λ| k &lt; |λ|, ∀|λ| &lt; 1. Combine with the fact that all γ k 's are nonnegative we have</p><formula xml:id="formula_9">|g γ,K (λ i )| ≤ K k=0 γ k |λ k | = K k=0 γ k |λ| k (a) ≤ K k=0 γ k |λ| = |λ|.</formula><p>Finally, note that the only possibility that the inequality (a) holds is γ k = δ 0,K since ∀k ≥ 1, |λ| k &lt; |λ|, ∀|λ| &lt; 1. However, by assumption K k=0 γ k = 1 and ∃k &gt; 0 such that γ k &gt; 0 we know that this is impossible. Hence (a) is a strict inequality &lt;. Together we complete the proof for low pass filtering part.</p><p>For the high pass filter result, it is not hard to see that</p><formula xml:id="formula_10">lim K→∞ g γ,K (λ) = lim K→∞ K k=0 γ k λ k = lim K→∞ K k=0 (−αλ) k = 1 1 + αλ ,</formula><p>where the last step is due to the fact that α ∈ (0, 1) and thus lim K→∞ (−αλ) K = 0, ∀|λ| ≤ 1. Thus we have</p><formula xml:id="formula_11">lim K→∞ g γ,K (λ i ) lim K→∞ g γ,K (λ 1 ) = 1 + α 1 + αλ i (b) &gt; 1 (c) &gt; |λ i | ∀i ≥ 2.</formula><p>Both strict inequalities (b) and (c) are from the fact that</p><formula xml:id="formula_12">|λ i | &lt; 1, ∀i ≥ 2. Notably, sup λ∈[1,−1) 1 1+αλ</formula><p>happens at the boundary λ = −1, which corresponds the the bipartite graph. It further shows that the graph filter with respect to the choice γ k = (−α) k emphasizes high frequency components and thus it is indeed acting as a high pass filter.</p><p>A.4 PROOF OF THEOREM 4.2</p><p>We start by introducing some additional notation, lemmas and definition before we proceed to the formal statement of Theorem 4.2. The label matrix is denoted by Y ∈ R n×C , where each row is a one-hot vector. We use 1[β] ∈ R C to denote the argmax of the vector β ∈ R C : we have 1[β] i = 1 if and only if β i = max(β) (ties are broken evenly), and 1[β] i = 0 otherwise. Let us replace the softmax(•) with softmax η (•), where we let softmax η (β) i = e ηβi /( j e ηβj ) stand for the softmax with a smooth parameter η &gt; 0. Note that for η = 1 we recover the standard softmax. With a slight abuse of notation, for the vector β we write exp(β) to denote element-wise exponentiation. We use</p><p>•, • to denote the standard Euclidean inner product. Also we use L for the cross entropy loss where</p><formula xml:id="formula_13">L = i∈V − log( Pi: , Y i: ).</formula><p>Lemma A.2. Assume that the nodes in an undirected and connected graph G have one of C labels.</p><p>Then, for k large enough, we have</p><formula xml:id="formula_14">H (k) :j = β j π + o k (1) ∀j ∈ [C], where π i = Dii v∈V Dvv and β T = π T H (0) .<label>(2)</label></formula><p>For any H (0) and large enough k ≤ K, if the label prediction is dominated by H (k) , all nodes will have a representation proportional to γ k β. Hence, we will arrive at the same label for all nodes. This is what we refer to as the over-smoothing phenomenon.</p><p>Definition A.3 (The over-smoothing phenomenon). First, recall that Z = k γ k H (k) . If oversmoothing occurs in the GPR-GNN for K sufficiently large, we have</p><formula xml:id="formula_15">Z :j = c 0 β j π, ∀j ∈ [C] for some c 0 &gt; 0 if γ k &gt; 0 and Z :j = −c 0 β j π, ∀j ∈ [C] for some c 0 &gt; 0 if γ k &lt; 0. Lemma A.4. Let L = i∈T L i = i∈T −log( Pi: , Y i:</formula><p>) be the cross entropy loss and let T be the training set. Under the same assumption as given in Lemma A.2, the gradient of γ k for k large enough is ∂L ∂γ k = i∈T ηπ i Pi: − Y i: , β + o k (1). Lemma A.5. For any real vector β ∈ R C and η &gt; 0 large enough, we have softmax η</p><formula xml:id="formula_16">(β) = 1[β] + o η (1).</formula><p>Now we are ready to state the formal version of Theorem 4.2. Theorem A.6 (Formal version of Theorem 4.2). Under the same assumptions as those listed in Lemma A.2, if the training set contains nodes from each class, then the GPR-GNN method can always avoid over-smoothing. More specifically, for k, η large enough we have</p><formula xml:id="formula_17">∂L ∂γ k = i∈T ηπ i max j∈[C] β j − β 1[Yi:] + o k (1) + o η (1), when γ k &gt; 0. (<label>3</label></formula><formula xml:id="formula_18">) ∂L ∂γ k = i∈T ηπ i min j∈[C] β j − β 1[Yi:] + o k (1) + o η (1), when γ k &lt; 0.<label>(4)</label></formula><p>Note that when γ k &gt; 0, (3) ≥ 0 when ignoring the o(1) term. The equality is achieved if and only if</p><formula xml:id="formula_19">max j∈[C] β j = β 1[Yi:]</formula><p>. This means that over-smoothing results in a prediction that perfectly aligns with the ground truth label in the training set. However, if our training set contains at least one node from each class then the equality can never be attained. Thus, the gradient of γ k will always be positive when γ k &gt; 0. Similarly when γ k &lt; 0, (4) ≤ 0 when ignoring the o(1) term. The equality is achieved if and only if</p><formula xml:id="formula_20">min j∈[C] β j = β 1[Yi:]</formula><p>. By the same reason we know that under the assumption on training set the equality can never be attained. Thus, the gradient of γ k will always be negative when γ k &lt; 0. Finally, it is not hard to check that the gradient is bounded in magnitude.</p><p>Together we have shown that the gradient of γ k and γ k are of the same sign. This directly implies that |γ k | will approach to 0 until we escape from over-smoothing when we use a decreasing learning rate for the optimizer (i.e. SGD).</p><p>Proof. First, let us assume the over-smoothing takes place and the γ k &gt; 0 for the dominate term. By Definition A.3, we know that Z :j = c 0 β j π, ∀j ∈ [C] for some c 0 &gt; 0 and K sufficiently large. By Lemma A.4 we have</p><formula xml:id="formula_21">∂L ∂γ k = i∈T ηπ i e ηZi: j∈[C] e ηZij − Y i: , β + o k (1)<label>(5)</label></formula><formula xml:id="formula_22">= i∈T ηπ i e ηc0πiβ j∈[C] e ηc0πiβj − Y i: , β + o k (1),<label>(6)</label></formula><p>where the last step follows from Definition A.3. Next, by Lemma A.5, we may approximate the softmax η by the true argmax for η &gt; 0 large enough according to</p><formula xml:id="formula_23">i∈T ηπ i 1[c 0 π i β] − Y i: , β + o k (1) + o η (1) (7) = i∈T ηπ i 1[β] − Y i: , β + o k (1) + o η (1) (8) = i∈T ηπ i max j∈[C] β j − β 1[Yi:] + o k (1) + o η (1).<label>(9)</label></formula><p>The first equality is due to the fact that c 0 &gt; 0 and π i &gt; 0. Recall that by Lemma A.2,</p><formula xml:id="formula_24">π i = √ Dii v∈V Dvv</formula><p>. Since we have a self-loop for each node, Dii &gt; 0 and thus π i &gt; 0. For the case γ k &lt; 0, the same analysis still valid until (7). Hence we have</p><formula xml:id="formula_25">i∈T ηπ i 1[−c 0 π i β] − Y i: , β + o k (1) + o η (1) (10) = i∈T ηπ i 1[−β] − Y i: , β + o k (1) + o η (1) (11) = i∈T ηπ i min j∈[C] β j − β 1[Yi:] + o k (1) + o η (1).<label>(12)</label></formula><p>Together we complete the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 CSBM DETAILS</head><p>The cSBM adds Gaussian random vectors as node features on top of the classical SBM. For simplicity, we assume C = 2 equally sized communities with node labels v i in {+1, −1}. Each node i is associate with a f dimensional Gaussian vector b</p><formula xml:id="formula_26">i = µ n v i u + Zi √ f</formula><p>where n is the number of nodes, u ∼ N (0, I/f ) and Z i ∈ R f has independent standard normal entries. The (undirected) graph in cSBM is described by the adjacency matrix A defined as</p><formula xml:id="formula_27">P (A ij = 1) = d+λ √ d n if v i v j &gt; 0 d−λ √ d n otherwise .</formula><p>Similar to the classical SBM, given the node labels the edges are independent. The symbol d stands for the average degree of the graph. Also, recall that µ and λ control the information strength carried by the node features and the graph structure respectively.</p><p>One reason for using the cSBM to generate synthetic data is that the information-theoretic limit of the model is already characterized in <ref type="bibr" target="#b10">Deshpande et al. (2018)</ref>. This result is summarized below.</p><p>Theorem A.7 (Informal main result in <ref type="bibr" target="#b10">Deshpande et al. (2018)</ref>). Assume that n, f → ∞, n f → ξ and d → ∞. Then there exists an estimator v such that lim inf n→∞</p><formula xml:id="formula_28">| v,v | n</formula><p>is bounded away from 0 if and only if λ 2 + µ 2 ξ &gt; 1.</p><p>In our experiment, we set n = 5000, f = 2000 and thus have ξ = 2.5. We vary µ and λ along the arc λ 2 + µ 2 /ξ = 1 + for some &gt; 0 to ensure that we are in the achievable parameter regime. We also choose = 3.25 for all our experiment.</p><p>A.6 PROOF OF LEMMA A.2</p><p>Note that the proof of Lemma A.2 reduces to a standard analysis of random walks on graph. We include it for completeness and refer the interested readers to the <ref type="bibr">tutorial Von Luxburg (2007)</ref>.</p><p>We start by showing that the symmetric graph Laplacian</p><formula xml:id="formula_29">Lsym = I − D−1/2 Ã D−1/2 = I − Ãsym (13)</formula><p>is positive semi-definite. Let u be any real vector of unit norm and f = D−1/2 u, then we have</p><formula xml:id="formula_30">u T Lsym u = u T u − u T D−1/2 Ã D−1/2 u = n i=1 u 2 i − n i,j=1 f i f j Ãij (14) = n i=1 Dii f 2 i − n i,j=1 f i f j Ãij = 1 2 ( n i=1 Dii f 2 i − 2 n i,j=1 f i f j Ãij + n j=1 Djj f 2 j ) (15) = 1 2 n i,j=1 Ãij (f i − f j ) 2 , (<label>16</label></formula><formula xml:id="formula_31">)</formula><p>where the last step follows from the definition of the degree.</p><p>Next we show that 0 is indeed an eigenvalue of Lsym associated with the unit eigenvector π where</p><formula xml:id="formula_32">π = √ Dii √ v Dvv</formula><p>.</p><p>Let 1 be the all one vector. Then, a direct calculation reveals that</p><formula xml:id="formula_33">Lsym π = π − D−1/2 Ã D−1/2 π = π − D−1/2 Ã D−1/2 D1/2 1 × 1 v Dvv (17) = π − D−1/2 Ã1 × 1 v Dvv = π − D−1/2 D1 × 1 v Dvv (18) = π − D1/2 1 × 1 v Dvv = π − π = 0. (<label>19</label></formula><formula xml:id="formula_34">)</formula><p>Combining this result with the positive semi-definite property of the Laplacian shows that 0 is indeed the smallest eigenvalue of Lsym associated with the eigenvector π. Moreover, from ( <ref type="formula" target="#formula_30">16</ref>) and the assumption that the graph is connected, it is not hard to see that the multiplicity of the eigenvalue 0 is exactly 1 (See Proposition 2 and 4 in Von Luxburg (2007) for more detail). Finally, from (13) it is obvious that the the largest eigenvalue of Ãsym is 1, which correspond to the eigenvector π. Hence all other eigenvalues of Ãsym 1 &gt; λ 2 ≥ ... ≥ λ n .</p><p>Next, we prove that |λ n | &lt; 1. This can also be shown directly from ( <ref type="formula" target="#formula_30">16</ref>). Note that</p><formula xml:id="formula_35">u T Lsym u = 1 2 n i,j=1 Ãij (f i − f j ) 2 (20) ≤ n i,j=1 Ãij (f 2 i + f 2 j ) = 2 n i,j=1 Ãij f 2 i = 2 n i,j=1 Ãij u 2 i Dii (21) = 2 n i=1 u 2 i Dii n j=1 Ãij = 2 n i=1 u 2 i Dii Dii = 2 n i=1 u 2 i = 2. (<label>22</label></formula><formula xml:id="formula_36">)</formula><p>The inequality follows from an application of the Cauchy-Schwartz inequality. </p><p>Hence, for any H (0) we have</p><formula xml:id="formula_38">lim k→∞ Ãk sym H (0) = ππ T H (0) = πβ T . (<label>24</label></formula><formula xml:id="formula_39">)</formula><p>Note that this can also be written with the o k (1) term as ).</p><formula xml:id="formula_40">Ãk sym H (0) = πβ T + o k (1). (<label>25</label></formula><p>(26)</p><p>Then by taking the partial derivative of the loss function with respect to γ k we have</p><formula xml:id="formula_41">∂L ∂γ k = ∂ ∂γ k i∈T (log( C m=1 e ηZim ) − ηZ i: , Y i: ).<label>(27)</label></formula><p>Next, recall that for GPR-GNN we also have Z = K k=0 γ k H (k) . Plugging this expression into the previous formula and applying the chain rule we obtain</p><formula xml:id="formula_42">∂ ∂γ k i∈T (log( C m=1 e ηZim ) − ηZ i: , Y i: ) = i∈T ( C m=1 e ηZim ∂ηZim ∂γ k C m=1 e Zim − ηH (k ) i: , Y i: ) (28) = i∈T ( C m=1 e ηZim ηH (k ) im C m=1 e ηZim − ηH (k ) i: , Y i: )<label>(29)</label></formula><p>Settin k = k for large enough k, it follows from Lemma A.2 that</p><formula xml:id="formula_43">∂L ∂γ k = i∈T η( C m=1 e ηZim H (k) im C m=1 e ηZim − H (k) i: , Y i: ) (30) = i∈T η( C m=1 e ηZim (π i β m + o k (1)) C m=1 e ηZim − π i β + o k (1), Y i: ) (31) = i∈T π i η( C m=1 e ηZim β m C m=1 e ηZim − β, Y i: ) + o k (1) (32) = i∈T π i η( C m=1 Pim β m − β, Y i: ) + o k (1) = i∈T ηπ i Pi: − Y i: , β + o k (1).<label>(33)</label></formula><p>Note that in (32) and ( <ref type="formula" target="#formula_43">33</ref>) we used the definition of the soft prediction P = softmax η (Z). This completes the proof. .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.8 PROOF</head><p>(34)</p><p>Note that β − β m &gt; 0 when β m = β and β − β m = 0 when β m = β. Without loss of generality we assume that there are p maxima in β, where 1 ≤ p ≤ C, and let P denote the set of indices of those maxima. Then, taking the limit η → ∞ we have</p><formula xml:id="formula_44">lim η→∞ softmax η (β) j = lim η→∞ e −η( β−βj ) m / ∈P e −η( β−βm) + p = 0, if β j = β 1 p , otherwise.<label>(35)</label></formula><p>This implies that for η &gt; 0 large enough one has</p><formula xml:id="formula_45">softmax η (β) = 1[β] + o η (1).<label>(36)</label></formula><p>The above result completes the proof.</p><p>A.9 ADDITIONAL EXPERIMENTAL DETAILS For all baseline models, we directly use the implementation available in the Pytorch Geometric library <ref type="bibr" target="#b11">Fey &amp; Lenssen (2019)</ref>.We use early stopping 200 and a maximum number of epochs equal to 1000 for both real benchmark dataset and our cSBM synthetic datasets. All models use the Adam optimizer <ref type="bibr" target="#b17">Kingma &amp; Ba (2014)</ref>. Note that the early stopping criteria is exactly the same as in Pytorch Geometric -when the epoch is greater than half of the maximum epoch, we check if the current validation loss is lower than the average over the past 200 epochs. If it is not lower, we stop the training process.</p><p>For GCN, we use 2 GCN layers with 64 hidden units. For GAT, we use 2 GAT convolutional layers, where the first layer has 8 attention heads and each head has 8 hidden units; the second layer has 1 attention head and 64 hidden units. For GCN-Cheby, we use 2 steps propagation for each layer with 32 hidden units. Note that the number of equivalent hidden units for each layer is64 for this case.</p><p>For JK-Net, we use the GCN-based model with 2 layers and 16 hidden units in each layer. As for the layer aggregation part, we use a LSTM with 16 channels and 4 layers. For the MLP, we choose a 2-layer fully connected network with 64 hidden units. For APPNP we use the same 2-layer MLP with 10 steps of propagation. Besides the GPR-GNN, we fix the dropout rate for the NN part to be 0.5 as APPNP and optimize the dropout rate for the GPR part among {0, 0.5, 0.7}. For Geom-GCN, we choose the datasets already tested in the paper were the method was first described <ref type="bibr" target="#b28">(Pei et al., 2019)</ref>. For SGC, we use the default K = 2 layers after test among {2, 3}. For SAGE, we use 2 SAGE convolutional layers with 64 hidden units.</p><p>The heterophilic datasets used in <ref type="bibr" target="#b28">(Pei et al., 2019)</ref>. The graphs Chameleon, Actor, Squirrel, Texas and Cornell in their original form are directed graphs (see the github repository of <ref type="bibr" target="#b28">(Pei et al., 2019)</ref>). Since the usual setting for semi-supervised node classifications involves undirected graph, we transformed the graphs into undirected to test them on all previously described benchmark methods. We keep the input graph directed for Geom-GCN as the method uses a fixed preprocessing scheme that   Table <ref type="table">8</ref>: Additional experiments illustrating that GPR-GNN escapes over-smoothing. We initialize the GPR weights γ k = δ kK as described in Section 5. We report the mean accuracy at Epoch 0 and after training (Final epoch). The over-smoothing ratio indicates how many time out of the 100 runs that GPR-GNN started with lead to the same label for all nodes. For an illustration of how GPR weights change over different epochs, please check Figure <ref type="figure" target="#fig_19">9</ref>.   Note that the learned GPR weights are all positive for every homophilic dataset. There is at least one negative learned GPR weight for every heterophilic dataset.   <ref type="table">8</ref>. Note that the GPR weights {γ k } K k=0 are identical to {−γ k } K k=0 in terms of graph filtering.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 1: (a) Hidden state feature extraction is performed by a neural networks using individual node features propagated via GPR. Note that both the GPR weights γ k and parameter set {θ} of the neural network are learned simultaneously in an end-to-end fashion (as indicated in red). (b)-(c)The learnt GPR weights of the GPR-GNN on real world datasets. Cora is homophilic while Texas is heterophilic (Here, H stands for the level of homophily defined below). An interesting trend may be observed: For the heterophilic case the weights alternate from positive to negative with dampening amplitudes (more examples are provided in Section 5). The shaded region corresponds to a 95% confidence interval.</figDesc><graphic url="image-13.png" coords="3,312.52,98.48,89.88,64.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Accuracy of tested models on cSBM. Error bars indicate 95% confidence interval.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) φ = 0.25, (H(G) = 0.189) (b) φ = 0.75, (H(G) = 0.688) (c) φ = −0.25, (H(G) = 0.029) (d) φ = −0.75, (H(G) = 0.002)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Figure (a)-(d) shows the learnt GPR weights by GPR-GNN with random initialization on cSBM, dense split. The shaded region indicates 95% confidence interval. differently and does not simply average them out. This allows SAGE to adapt to the heterophilic case since the ego-embeddings prevent nodes from being overwhelmed by information from their neighbors. Nevertheless, JKNet, GCN-Cheby and SAGE are not deep in practice. Moreover, JKNet fails to learn under the sparse splitting model while GCN-Cheby and SAGE fail to learn well when the graph information is strong (|φ| ≥ 0.5), again under the sparse splitting model.</figDesc><graphic url="image-15.png" coords="7,113.64,87.09,89.62,63.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Figures (a)-(d) show the learned GPR weights of our GPR-GNN method with random initialization on various datasets, for dense splitting. Figures (e)-(f) show the learned weights of our GPR-GNN method with initialization δ kK on cSBM(φ = −1), for dense splitting. The shaded region indicates a 95% confidence interval.</figDesc><graphic url="image-23.png" coords="8,111.37,327.11,94.17,69.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: A simple example demonstrating how GPR-GNN escapes over-smoothing. Let us consider a simple example shown in Figure 5 involving a connected and undirected graph G = (V, E) (Figure 5 (a)). Consider two different node label assignments shown in Figure 5 (b) and Figure 5 (c). Obviously, the graph topologies depicted in Figure 5 (b) and (c) are identical and the only difference is the class label assignment. In Figure5(b), the graph is homophilic and hence the optimal graph filter should emphasize the low-frequency part of the graph signal. In contrast, in Figure5(c), the graph is heterophilic as the graph is bipartite with respect to the labels. Hence, the optimal graph filter should emphasize the high-frequency part of the graph signal. This example illustrates that the optimal graph filter should depend on both the graph topology and the node label information. Recall that the equivalent graph filter that APPNP uses in the asymptotic regime is Π ppr which is independent on the node label information. Also, Theorem 4.1 established that APPNP intrinsically utilizes a low-pass filter. In contrast, GPR-GNN learns the GPR weights guided by the node label information which allows it to account for both cases (homophilic and heterophilic) shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>(a) Case 1.(b) Case 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: A simple example for explaining the insufficiency of homophily measure H(G).As mentioned in Section 5, the homophily measure H(G) is inadequate for characterizing whether a heterophilic graph topology is informative or not. Consider two simple examples depicted in Fig-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>A. 3</head><label>3</label><figDesc>PROOF OF THEOREM 4.1 first state the formal version of Theorem 4.1. Theorem A.1 (Formal version of Theorem 4.1). Assume the graph</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>OF LEMMA A.5 Let β = max(β). Then by the definition of softmax η for η &gt; 0 we have softmax η</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Figures (a)-(i) show the learned GPR weights by GPR-GNN with random initialization on cSBM, dense splitting. The shaded region indicates a 95% confidence interval.</figDesc><graphic url="image-33.png" coords="21,114.74,364.99,124.04,86.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>Actor, (H(G) = 0.008) (h) Squirrel, (H(G) = 0.055) (i) Texas, (H(G) = 0.016) (j) Cornell, (H(G) = 0.137)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Figures (a)-(j) show the learned GPR weights by GPR-GNN with random initialization on various benchmark datasets, dense splitting. The shaded region indicates a 95% confidence interval.Note that the learned GPR weights are all positive for every homophilic dataset. There is at least one negative learned GPR weight for every heterophilic dataset.</figDesc><graphic url="image-45.png" coords="22,247.55,508.34,116.79,85.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>(</head><label></label><figDesc>(a) Cora, epoch 0 (b) Cora, epoch 50 (c) Cora, epoch 100 (d) Cora, epoch 150 (e) Cora, epoch 200 Texas, epoch 0 (q) Texas, epoch 50 (r) Texas, epoch 100 (s) Texas, epoch 150 (t) Texas, epoch 200</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Learned GPR weights by GPR-GNN with initialization γ k = δ kK (last step) on various benchmark datasets, dense splitting. The shaded region indicates a 95% confidence interval. Also, please check Table8. Note that the GPR weights {γ k } K k=0 are identical to {−γ k } K k=0 in terms of graph filtering.</figDesc><graphic url="image-61.png" coords="23,114.95,371.20,71.15,51.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>(Figure 10 :</head><label>10</label><figDesc>Figure 10: The dynamics of learning GPR weights with random initialization on various benchmark datasets, dense splitting. The shaded region indicates a 95% confidence interval.</figDesc><graphic url="image-81.png" coords="24,113.70,376.91,73.64,50.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Benchmark dataset properties and statistics.</figDesc><table><row><cell cols="11">Dataset Cora Citeseer PubMed Computers Photo Chameleon Squirrel Actor Texas Cornell</cell></row><row><cell>Classes</cell><cell>7</cell><cell>6</cell><cell>5</cell><cell>10</cell><cell>8</cell><cell>5</cell><cell>5</cell><cell>5</cell><cell>5</cell><cell>5</cell></row><row><cell cols="2">Features 1433</cell><cell>3703</cell><cell>500</cell><cell>767</cell><cell>745</cell><cell>2325</cell><cell>2089</cell><cell>932</cell><cell>1703</cell><cell>1703</cell></row><row><cell cols="2">Nodes 2708</cell><cell>3327</cell><cell>19717</cell><cell>13752</cell><cell>7650</cell><cell>2277</cell><cell>5201</cell><cell>7600</cell><cell>183</cell><cell>183</cell></row><row><cell cols="2">Edges 5278</cell><cell>4552</cell><cell>44324</cell><cell>245861</cell><cell>119081</cell><cell>31371</cell><cell cols="3">198353 26659 279</cell><cell>277</cell></row><row><cell cols="3">H(G) 0.656 0.578</cell><cell>0.644</cell><cell>0.272</cell><cell>0.459</cell><cell>0.024</cell><cell cols="4">0.055 0.008 0.016 0.137</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results on real world benchmark datasets: Mean accuracy (%) ± 95% confidence interval. Boldface letters are used to mark the best results while underlined boldface letters indicate results within the given confidence interval of the best result. 51±0.36 67.63±0.38 85.07±0.09 82.90±0.37 91.93±0.26 67.48±0.40 39.30±0.27 49.93±0.53 92.92±0.61 91.36±0.70 APPNP 79.41±0.38 68.59±0.30 85.02±0.09 81.99±0.26 91.11±0.26 51.91±0.56 38.86±0.24 34.77±0.34 91.18±0.70 91.80±0.63 MLP 50.34±0.48 52.88±0.51 80.57±0.12 70.48±0.28 78.69±0.30 46.72±0.46 38.58±0.25 31.28±0.27 92.26±0.71 91.36±0.70 SGC 70.81±0.67 58.98±0.47 82.09±0.11 76.27±0.36 83.80±0.46 63.02±0.43 29.39±0.20 43.14±0.28 55.18±1.17 47.80±1.50 GCN 75.21±0.38 67.30±0.35 84.27±0.01 82.52±0.32 90.54±0.21 60.96±0.78 30.59±0.23 45.66±0.39 75.16±0.96 66.72±1.37 GAT 76.70±0.42 67.20±0.46 83.28±0.12 81.95±0.38 90.09±0.27 63.9±0.46 35.98±0.23 42.72±0.33 78.87±0.86 76.00±1.01 SAGE 70.89±0.54 61.52±0.44 81.30±0.10 83.11±0.23 90.51±0.25 62.15±0.42 36.37±0.21 41.26±0.26 79.03±1.20 71.41±1.24 JKNet 73.22±0.64 60.85±0.76 82.91±0.11 77.80±0.97 87.70±0.70 62.92±0.49 33.41±0.25 44.72±0.48 75.53±1.16 66.73±1.73 GCN-Cheby 71.39±0.51 65.67±0.38 83.83±0.12 82.41±0.28 90.09±0.28 59.96±0.51 38.02±0.23 40.67±0.31 86.08±0.96 85.33±1.04 GeomGCN 20.37±1.13 20.30±0.90 58.20±1.23 NA NA 61.06±0.49 31.81±0.24 38.28±0.27 58.56±1.77 55.59±1.59</figDesc><table><row><cell>Cora</cell><cell>Citeseer</cell><cell>PubMed Computers</cell><cell>Photo</cell><cell>Chameleon</cell><cell>Actor</cell><cell>Squirrel</cell><cell>Texas</cell><cell>Cornell</cell></row><row><cell>GPRGNN 79.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Efficiency on selected real world benchmark datasets: Average running time per epoch(ms)/average total running time(s). Note that Geom-GCN requires a preprocessing procedure so we do not include it in the table. Complete efficiency table for all benchmark datasets is in Supplementary due to space limit.</figDesc><table><row><cell></cell><cell>Cora</cell><cell>Pubmed</cell><cell>Computers</cell><cell>Chameleon</cell><cell>Actor</cell><cell>Squirrel</cell><cell>Texas</cell></row><row><cell cols="8">GPRGNN 17.62ms / 3.74s 20.19ms / 5.53s 39.93ms / 11.40s 16.74ms / 3.40s 19.31ms / 4.49s 25.28ms / 5.12s 17.56ms / 3.55s</cell></row><row><cell>APPNP</cell><cell cols="7">17.16ms / 4.00s 18.47ms / 6.29s 39.59ms / 20.00s 17.01ms / 3.44s 16.32ms / 4.04s 22.93ms / 4.63s 15.96ms / 3.24s</cell></row><row><cell>MLP</cell><cell cols="2">4.14ms / 0.92s 5.43ms / 2.86s</cell><cell>5.33ms / 2.77s</cell><cell cols="2">3.41ms / 0.69s 4.84ms / 0.98s</cell><cell>5.19ms / 1.05s</cell><cell>3.81ms / 1.04s</cell></row><row><cell>SGC</cell><cell cols="2">3.31ms / 3.31s 3.81ms / 3.81s</cell><cell>4.36ms / 4.36s</cell><cell cols="2">3.13ms / 3.13s 3.98ms / 1.00s</cell><cell>4.79ms / 4.79s</cell><cell>2.86ms / 2.09s</cell></row><row><cell>GCN</cell><cell cols="7">9.25ms / 1.97s 14.11ms / 4.17s 32.45ms / 16.29s 13.83ms / 2.79s 12.39ms / 2.50s 27.11ms / 5.56s 10.22ms / 2.06s</cell></row><row><cell>GAT</cell><cell cols="7">14.78ms / 3.42s 21.52ms / 6.70s 61.45ms / 24.28s 16.63ms / 3.63s 18.91ms / 3.86s 47.46ms / 10.05s 15.50ms / 3.13s</cell></row><row><cell>SAGE</cell><cell cols="7">12.06ms / 2.44s 28.82ms / 6.32s 171.36ms / 71.94s 64.43ms / 13.02s 27.95ms / 5.65s 343.47ms / 69.38s 6.08ms / 1.28s</cell></row><row><cell>JKNet</cell><cell cols="7">18.97ms / 4.41s 24.48ms / 6.61s 35.02ms / 14.96s 20.03ms / 5.15s 23.52ms / 4.75s 29.89ms / 6.67s 19.67ms / 4.01s</cell></row><row><cell cols="8">GCN-cheby 22.96ms / 4.75s 45.76ms / 12.02s 218.82ms / 96.58s 89.41ms / 18.06s 43.94ms / 8.88s 440.55ms / 88.99s 12.34ms / 3.08s</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Consequently, the largest eigenvalue of Lsym is bounded by 2 which means that |λ n | ≤ 1. Note that equality holds if and only if the underlying graph is bipartite. However, this is impossible in our setting since we have added a self loop to each node. Hence |λ n | &lt; 1. This means</figDesc><table><row><cell>lim k→∞</cell><cell>Ãk sym = ππ T .</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>The values of the homophily measure for cSBM datasets. Linux Machine with 48 cores, 376GB of RAM, and a NVIDIA Tesla P100 GPU with 12GB of GPU memory. For the training set, we ensure that number of nodes from each class is approximately the same an keep the total number of training nodes close to 2.5%/60%. For the validation set, we randomly sample 2.5%/20% of the nodes and place the remaining ones into the test set.</figDesc><table><row><cell>φ</cell><cell>−1</cell><cell cols="3">−0.75 −0.5 −0.25</cell><cell>0</cell><cell>0.25</cell><cell>0.5</cell><cell>0.75</cell><cell>1</cell></row><row><cell cols="2">H(G) 0.001</cell><cell>0.002</cell><cell>0.009</cell><cell>0.029</cell><cell cols="4">0.077 0.189 0.419 0.688 0.809</cell></row><row><cell cols="4">All experiments are performed on a</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>Results on homophilic real-world benchmark datasets tested in<ref type="bibr" target="#b28">(Pei et al., 2019)</ref>, dense splitting: Mean accuracy (%) ± 95% confidence interval. Boldface values indicate the best results found while boldface, underlined values indicates results within the confidence interval with respect to the best result.</figDesc><table><row><cell></cell><cell>Cora</cell><cell>Citeseer</cell><cell>PubMed</cell></row><row><cell>GPRGNN</cell><cell>88.65±0.28</cell><cell>80.01±0.28</cell><cell>89.18±0.15</cell></row><row><cell>APPNP</cell><cell>88.1±0.23</cell><cell>80.5±0.26</cell><cell>89.15±0.13</cell></row><row><cell>MLP</cell><cell>76.44±0.30</cell><cell>76.25±0.28</cell><cell>86.43±0.13</cell></row><row><cell>SGC</cell><cell>86.58±0.26</cell><cell>76.23±0.29</cell><cell>83.52±0.10</cell></row><row><cell>GCN</cell><cell>86.87±0.25</cell><cell>79.28±0.25</cell><cell>86.97±0.12</cell></row><row><cell>GAT</cell><cell>87.52±0.24</cell><cell>80.56±0.31</cell><cell>86.64±0.11</cell></row><row><cell>SAGE</cell><cell>86.58±0.26</cell><cell>78.24±0.30</cell><cell>86.85±0.11</cell></row><row><cell>JKNet</cell><cell>86.97±0.27</cell><cell>77.69±0.35</cell><cell>87.38±0.13</cell></row><row><cell>GCN-Cheby</cell><cell>86.46±0.26</cell><cell>78.66±0.26</cell><cell>88.2±0.09</cell></row><row><cell>GeomGCN</cell><cell>85.4±0.26</cell><cell>76.42±0.37</cell><cell>88.51±0.08</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 9 :</head><label>9</label><figDesc>Efficiency on homophilic real world benchmark datasets: Average running time per epoch(ms)/average total running time(s).</figDesc><table><row><cell></cell><cell>Cora</cell><cell>Citeseer</cell><cell>Pubmed</cell><cell>Computers</cell><cell>Photo</cell></row><row><cell>GPRGNN</cell><cell cols="2">17.62ms / 3.74s 19.28ms / 3.89s</cell><cell>20.19ms / 5.53s</cell><cell>39.93ms / 11.40s</cell><cell>21.61ms / 6.18s</cell></row><row><cell>APPNP</cell><cell cols="2">17.16ms / 4.00s 15.97ms / 3.26s</cell><cell>18.47ms / 6.29s</cell><cell>39.59ms / 20.00s</cell><cell>20.10ms / 10.93s</cell></row><row><cell>MLP</cell><cell>4.14ms / 0.92s</cell><cell>5.30ms / 1.13s</cell><cell>5.43ms / 2.86s</cell><cell>5.33ms / 2.77s</cell><cell>4.63ms / 2.72s</cell></row><row><cell>SGC</cell><cell>3.31ms / 3.31s</cell><cell>11.45ms / 2.31s</cell><cell>3.81ms / 3.81s</cell><cell>4.36ms / 4.36s</cell><cell>19.12ms / 8.75s</cell></row><row><cell>GCN</cell><cell>9.25ms / 1.97s</cell><cell>17.46ms / 3.53s</cell><cell>14.11ms / 4.17s</cell><cell>32.45ms / 16.29s</cell><cell>32.56ms / 11.33s</cell></row><row><cell>GAT</cell><cell cols="2">14.78ms / 3.42s 19.94ms / 4.47s</cell><cell>21.52ms / 6.70s</cell><cell>61.45ms / 24.28s</cell><cell>24.57ms / 11.61s</cell></row><row><cell>SAGE</cell><cell cols="2">12.06ms / 2.44s 41.40ms / 8.36s</cell><cell>28.82ms / 6.32s</cell><cell cols="2">171.36ms / 71.94s 108.88ms / 42.18s</cell></row><row><cell>JKNet</cell><cell>18.97ms / 4.41s</cell><cell>3.99ms / 3.99s</cell><cell>24.48ms / 6.61s</cell><cell>35.02ms / 14.96s</cell><cell>3.66ms / 3.66s</cell></row><row><cell cols="5">GCN-cheby 22.96ms / 4.75s 23.16ms / 4.68s 45.76ms / 12.02s 218.82ms / 96.58s</cell><cell>82.38ms / 30.48s</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 10 :</head><label>10</label><figDesc>Efficiency on heterophilic real world benchmark datasets: Average running time per epoch(ms)/average total running time(s).</figDesc><table><row><cell></cell><cell>Chameleon</cell><cell>Squirrel</cell><cell>Actor</cell><cell>Texas</cell><cell>Cornell</cell></row><row><cell>GPRGNN</cell><cell>16.74ms / 3.40s</cell><cell>25.28ms / 5.12s</cell><cell cols="3">19.31ms / 4.49s 17.56ms / 3.55s 18.42ms / 3.72s</cell></row><row><cell>APPNP</cell><cell>17.01ms / 3.44s</cell><cell>22.93ms 4.63s</cell><cell cols="3">16.32ms / 4.04s 15.96ms / 3.24s 14.66ms / 3.09s</cell></row><row><cell>MLP</cell><cell>3.41ms / 0.69s</cell><cell>5.19ms / 1.05s</cell><cell>4.84ms / 0.98s</cell><cell>3.81ms / 1.04s</cell><cell>3.46ms / 0.89s</cell></row><row><cell>SGC</cell><cell>13.83ms / 2.79s</cell><cell>27.11ms / 5.56s</cell><cell cols="3">12.39ms / 2.50s 10.22ms / 2.06s 10.38ms / 2.10s</cell></row><row><cell>GCN</cell><cell>16.63ms / 3.63s</cell><cell>47.46ms / 10.05s</cell><cell cols="3">18.91ms / 3.86s 15.50ms / 3.13s 13.67ms / 2.76s</cell></row><row><cell>GAT</cell><cell>20.03ms / 5.15s</cell><cell>29.89ms / 6.67s</cell><cell cols="3">23.52ms / 4.75s 19.67ms / 4.01s 19.35ms / 3.91s</cell></row><row><cell>SAGE</cell><cell cols="5">89.41ms / 18.06s 440.55ms / 88.99s 43.94ms / 8.88s 12.34ms / 3.08s 12.15ms / 2.69s</cell></row><row><cell>JKNet</cell><cell>3.13ms / 3.13s</cell><cell>4.79ms / 4.79s</cell><cell>3.98ms / 1.00s</cell><cell>2.86ms / 2.09s</cell><cell>2.81ms / 1.18s</cell></row><row><cell cols="4">GCN-cheby 64.43ms / 13.02s 343.47ms / 69.38s 27.95ms / 5.65s</cell><cell>6.08ms / 1.28s</cell><cell>6.05ms / 1.44s</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://github.com/graphdml-uiuc-jlu/geom-gcn</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-11/www/wwkb</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>was unfortunately not made public by the authors. Our homophily measure values H(G) in Table <ref type="table">1</ref> are all based on undirected graphs and hence the numbers are different from those reported in <ref type="bibr" target="#b28">(Pei et al., 2019)</ref>.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.10 ADDITIONAL EXPERIMENTAL RESULTS</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Community detection and stochastic block models: recent developments</title>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Abbe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="6446" to="6531" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Watch your step: Learning node embeddings via graph attention</title>
		<author>
			<persName><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9180" to="9190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mixhop: Higher-order graph convolutional architectures via sparsified neighborhood mixing</title>
		<author>
			<persName><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nazanin</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hrayr</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Ver Steeg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aram</forename><surname>Galstyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dimitris Berberidis, Athanasios Nikolakopoulos, and Georgios B Giannakis. Adaptive diffusions for scalable learning over graphs</title>
		<author>
			<persName><forename type="first">Jessica</forename><forename type="middle">B</forename><surname>Peter W Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinicius</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
	</analytic>
	<monogr>
		<title level="m">Mining and Learning with Graphs Workshop @ ACM KDD</title>
				<imprint>
			<date type="published" when="2018">2018. 2018. 2018</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Relational inductive biases, deep learning, and graph networks</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Is pagerank all you need for scalable graph neural networks?</title>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Blais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Lukasik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM KDD, MLG Workshop</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Scaling graph neural networks with approximate pagerank</title>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Blais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benedek</forename><surname>Rózemberczki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Lukasik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2464" to="2473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR2014)</title>
				<imprint>
			<date type="published" when="2014-04">April 2014. 2014</date>
			<biblScope unit="page" from="http" to="openreview" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The heat kernel as the pagerank of a graph</title>
		<author>
			<persName><forename type="first">Fan</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">50</biblScope>
			<biblScope unit="page" from="19735" to="19740" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Latent network features and overlapping community discovery via boolean intersection representations</title>
		<author>
			<persName><forename type="first">Hoang</forename><surname>Dau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olgica</forename><surname>Milenkovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Networking</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="3219" to="3234" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Contextual stochastic block models</title>
		<author>
			<persName><forename type="first">Yash</forename><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhabrata</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Montanari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elchanan</forename><surname>Mossel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8581" to="8593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Fast graph representation learning with pytorch geometric</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Eric Lenssen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.02428</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Samuel S Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<author>
			<persName><surname>David F Gleich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pagerank beyond the web</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="321" to="363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Scaling personalized web search</title>
		<author>
			<persName><forename type="first">Glen</forename><surname>Jeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Widom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th international conference on World Wide Web</title>
				<meeting>the 12th international conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="271" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Outcome correlation in graph neural network regression</title>
		<author>
			<persName><forename type="first">Junteng</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Austin</forename><surname>Benson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08274</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Diffusion improves graph learning</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Weißenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="13333" to="13345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Block models and personalized pagerank</title>
		<author>
			<persName><forename type="first">Johan</forename><surname>Isabel M Kloumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Ugander</surname></persName>
		</author>
		<author>
			<persName><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="33" to="38" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Optimizing generalized pagerank methods for seedexpansion community detection</title>
		<author>
			<persName><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olgica</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName><surname>Milenkovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="11705" to="11716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Link prediction in complex networks: A survey</title>
		<author>
			<persName><forename type="first">Linyuan</forename><surname>Lü</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica A: statistical mechanics and its applications</title>
		<imprint>
			<biblScope unit="volume">390</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1150" to="1170" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Image-based recommendations on styles and substitutes</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Targett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinfeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
				<meeting>the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Birds of a feather: Homophily in social networks</title>
		<author>
			<persName><forename type="first">Lynn</forename><surname>Miller Mcpherson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">M</forename><surname>Smith-Lovin</surname></persName>
		</author>
		<author>
			<persName><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual review of sociology</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="415" to="444" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Graph neural networks exponentially lose expressive power for node classification</title>
		<author>
			<persName><forename type="first">Kenta</forename><surname>Oono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taiji</forename><surname>Suzuki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Geom-gcn: Geometric graph convolutional networks</title>
		<author>
			<persName><forename type="first">Hongbin</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingzhe</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Chen-Chuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="93" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Mumme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.05868</idno>
		<title level="m">Pitfalls of graph neural network evaluation</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Weisfeiler-lehman graph kernels</title>
		<author>
			<persName><forename type="first">Nino</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">77</biblScope>
			<biblScope unit="page" from="2539" to="2561" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">I</forename><surname>Shuman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="83" to="98" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Provably fast inference of latent features from networks: With applications to learning social circles and multilabel classification</title>
		<author>
			<persName><forename type="first">Charalampos</forename><surname>Tsourakakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web</title>
				<meeting>the 24th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1111" to="1121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJXMpikCZ" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">WOK Asiri Suranga Wijesinghe and Qing Wang. Dfnets: Spectral cnns for graphs with feedbacklooped filters</title>
		<author>
			<persName><forename type="first">Ulrike</forename><surname>Von</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luxburg</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2007">2007. 2019</date>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="6007" to="6018" />
		</imprint>
	</monogr>
	<note>A tutorial on spectral clustering</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amauri</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5453" to="5462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on International Conference on Machine Learning</title>
				<meeting>the 33rd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Graph-SAINT: Graph sampling based inductive learning method</title>
		<author>
			<persName><forename type="first">Hanqing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajitesh</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajgopal</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viktor</forename><surname>Prasanna</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BJe8pkHFwS" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Jiong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingxiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Heimann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11468</idno>
		<title level="m">Generalizing graph neural networks beyond homophily</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Semi-supervised learning literature survey</title>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Xiaojin</surname></persName>
		</author>
		<author>
			<persName><surname>Zhu</surname></persName>
		</author>
		<idno>GPRGNN 98.83±0.06 98.19±0.08 94.23±0.14 86.06±0.20 82.22±0.20 86.48±0.20 94.34±0.13 98.46±0.08 98.84±0.06</idno>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>University of Wisconsin-Madison Department of Computer Sciences</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title/>
		<idno>96.11±0.37 95.33±0.25 87.98±0.56 59.61±0.49 63.28±0.10 80.23±0.36 93.28±0.15 98.33±0.07 98.22±0.07</idno>
	</analytic>
	<monogr>
		<title level="j">JKNet</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
