<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AECA601A0C9007F194B2FA9D45CBB7E0</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The iSLIP Scheduling Algorithm for Input-Queued Switches</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Nick McKeown, Senior Member, IEEE</head><p>Abstract-An increasing number of high performance internetworking protocol routers, LAN and asynchronous transfer mode (ATM) switches use a switched backplane based on a crossbar switch. Most often, these systems use input queues to hold packets waiting to traverse the switching fabric. It is well known that if simple first in first out (FIFO) input queues are used to hold packets then, even under benign conditions, head-of-line (HOL) blocking limits the achievable bandwidth to approximately 58.6% of the maximum. HOL blocking can be overcome by the use of virtual output queueing, which is described in this paper. A scheduling algorithm is used to configure the crossbar switch, deciding the order in which packets will be served. Recent results have shown that with a suitable scheduling algorithm, 100% throughput can be achieved. In this paper, we present a scheduling algorithm called iSLIP. An iterative, round-robin algorithm, iSLIP can achieve 100% throughput for uniform traffic, yet is simple to implement in hardware. Iterative and noniterative versions of the algorithms are presented, along with modified versions for prioritized traffic. Simulation results are presented to indicate the performance of iSLIP under benign and bursty traffic conditions. Prototype and commercial implementations of iSLIP exist in systems with aggregate bandwidths ranging from 50 to 500 Gb/s. When the traffic is nonuniform, iSLIP quickly adapts to a fair scheduling policy that is guaranteed never to starve an input queue. Finally, we describe the implementation complexity of iSLIP. Based on a two-dimensional (2-D) array of priority encoders, single-chip schedulers have been built supporting up to 32 ports, and making approximately 100 million scheduling decisions per second.</p><p>Index Terms-ATM switch, crossbar switch, input-queueing, IP router, scheduling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>I N AN ATTEMPT to take advantage of the cell-switching capacity of the asynchronous transfer mode (ATM), there has recently been a merging of ATM switches and Internet Protocol (IP) routers <ref type="bibr" target="#b30">[29]</ref>, <ref type="bibr" target="#b33">[32]</ref>. This idea is already being carried one step further, with cell switches forming the core, or backplane, of high-performance IP routers <ref type="bibr" target="#b27">[26]</ref>, <ref type="bibr" target="#b32">[31]</ref>, <ref type="bibr" target="#b6">[6]</ref>, <ref type="bibr" target="#b3">[4]</ref>. Each of these high-speed switches and routers is built around a crossbar switch that is configured using a centralized scheduler, and each uses a fixed-size cell as a transfer unit. Variable-length packets are segmented as they arrive, transferred across the central switching fabric, and then reassembled again into packets before they depart. A crossbar switch is used because it is simple to implement and Manuscript received <ref type="bibr">November 19, 1996;</ref><ref type="bibr">revised</ref>  <ref type="bibr">February 9, 1998</ref>; approved by IEEE/ACM TRANSACTIONS ON NETWORKING Editor H. J. Chao.</p><p>The author is with the Department of Electrical Engineering, Stanford University, Stanford, CA 94305-9030 USA (e-mail: nickm@stanford.edu).</p><p>Publisher Item Identifier S 1063-6692(99)03593-1.</p><p>is nonblocking; it allows multiple cells to be transferred across the fabric simultaneously, alleviating the congestion found on a conventional shared backplane. In this paper, we describe an algorithm that is designed to configure a crossbar switch using a single-chip centralized scheduler. The algorithm presented here attempts to achieve high throughput for best-effort unicast traffic, and is designed to be simple to implement in hardware.</p><p>Our work was motivated by the design of two such systems: the Cisco 12 000 GSR, a 50-Gb/s IP router, and the Tiny Tera: a 0.5-Tb/s MPLS switch <ref type="bibr" target="#b8">[7]</ref>. Before using a crossbar switch as a switching fabric, it is important to consider some of the potential drawbacks; we consider three here. First, the implementation complexity of an -port crossbar switch increases with making crossbars impractical for systems with a very large number of ports. Fortunately, the majority of high-performance switches and routers today have only a relatively small number of ports (usually between 8 and 32). This is because the highest performance devices are used at aggregation points where port density is low. 1 Our work is, therefore, focussed on systems with low port density. A second potential drawback of crossbar switches is that they make it difficult to provide guaranteed qualities of service. This is because cells arriving to the switch must contend for access to the fabric with cells at both the input and the output. The time at which they leave the input queues and enter the crossbar switching fabric is dependent on other traffic in the system, making it difficult to control when a cell will depart. There are two common ways to mitigate this problem. One is to schedule the transfer of cells from inputs to outputs in a similar manner to that used in a timeslot interchanger, providing peak bandwidth allocation for reserved flows. This method has been implemented in at least two commercial switches and routers. 2 The second approach is to employ "speedup," in which the core of the switch runs faster than the connected lines. Simulation and analytical results indicate that with a small speedup, a switch will deliver cells quickly to their outgoing port, apparently independent of contending traffic <ref type="bibr" target="#b28">[27]</ref>, <ref type="bibr" target="#b38">[37]</ref>- <ref type="bibr" target="#b42">[41]</ref>. While these techniques are of growing importance, we restrict our focus in this paper to the efficient and fast scheduling of best-effort traffic. 1 Some people believe that this situation will change in the future, and that switches and routers with large aggregate bandwidths will support hundreds or even thousands of ports. If these systems become real, then crossbar switches-and the techniques that follow in this paper-may not be suitable. However, the techniques described here will be suitable for a few years hence. 2 A peak-rate allocation method was supported by the DEC AN2 Gigaswitch/ATM <ref type="bibr" target="#b1">[2]</ref> and the Cisco Systems LS2020 ATM Switch. A third potential drawback of crossbar switches is that they (usually) employ input queues. When a cell arrives, it is placed in an input queue where it waits its turn to be transferred across the crossbar fabric. There is a popular perception that inputqueued switches suffer from inherently low performance due to head-of-line (HOL) blocking. HOL blocking arises when the input buffer is arranged as a single first in first out (FIFO) queue: a cell destined to an output that is free may be held up in line behind a cell that is waiting for an output that is busy. Even with benign traffic, it is well known that HOL can limit thoughput to just <ref type="bibr" target="#b17">[16]</ref>. Many techniques have been suggested for reducing HOL blocking, for example by considering the first cells in the FIFO queue, where <ref type="bibr" target="#b9">[8]</ref>, <ref type="bibr" target="#b14">[13]</ref>, <ref type="bibr" target="#b18">[17]</ref>. Although these schemes can improve throughput, they are sensitive to traffic arrival patterns and may perform no better than regular FIFO queueing when the traffic is bursty. But HOL blocking can be eliminated by using a simple buffering strategy at each input port. Rather than maintain a single FIFO queue for all cells, each input maintains a separate queue for each output as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. This scheme is called virtual output queueing (VOQ) and was first introduced by Tamir et al. in <ref type="bibr" target="#b35">[34]</ref>. HOL blocking is eliminated because cells only queue behind cells that are destined to the same output; no cell can be held up by a cell ahead of it that is destined to a different output. When VOQ's are used, it has been shown possible to increase the throughput of an input-queued switch from 58.6% to 100% for both uniform and nonuniform traffic <ref type="bibr" target="#b26">[25]</ref>, <ref type="bibr" target="#b29">[28]</ref>. Crossbar switches that use VOQ's have been employed in a number of studies <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b15">[14]</ref>, <ref type="bibr" target="#b20">[19]</ref>, <ref type="bibr" target="#b24">[23]</ref>, <ref type="bibr" target="#b35">[34]</ref>, research prototypes <ref type="bibr" target="#b27">[26]</ref>, <ref type="bibr" target="#b32">[31]</ref>, <ref type="bibr" target="#b34">[33]</ref>, and commercial products <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b6">[6]</ref>. For the rest of this paper, we will be considering crossbar switches that use VOQ's.</p><p>When we use a crossbar switch, we require a scheduling algorithm that configures the fabric during each cell time and decides which inputs will be connected to which outputs; this determines which of the VOQ's are served in each cell time. At the beginning of each cell time, a scheduler examines the contents of the input queues and determines a conflictfree match between inputs and outputs. This is equivalent to finding a bipartite matching on a graph with vertices [2], <ref type="bibr" target="#b26">[25]</ref>, <ref type="bibr" target="#b36">[35]</ref>. For example, the algorithms described in <ref type="bibr" target="#b26">[25]</ref> and <ref type="bibr" target="#b29">[28]</ref> that achieve 100% throughput, use maximum weight bipartite matching algorithms <ref type="bibr" target="#b36">[35]</ref>, which have a running-time complexity of</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Maximum Size Matching</head><p>Most scheduling algorithms described previously are heuristic algorithms that approximate a maximum size<ref type="foot" target="#foot_0">3</ref> matching [1], <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b9">[8]</ref>, <ref type="bibr" target="#b19">[18]</ref>, <ref type="bibr" target="#b31">[30]</ref>, <ref type="bibr" target="#b37">[36]</ref>. These algorithms attempt to maximize the number of connections made in each cell time, and hence, maximize the instantaneous allocation of bandwidth. The maximum size matching for a bipartite graph can be found by solving an equivalent network flow problem <ref type="bibr" target="#b36">[35]</ref>; we call the algorithm that does this maxsize. There exist many maximum-size bipartite matching algorithms, and the most efficient currently known converges in time <ref type="bibr" target="#b13">[12]</ref>. <ref type="foot" target="#foot_1">4</ref> The problem with this algorithm is that although it is guaranteed to find a maximum match, for our application it is too complex to implement in hardware and takes too long to complete.</p><p>One question worth asking is "Does the maxsize algorithm maximize the throughput of an input-queued switch?" The answer is no; maxsize can cause some queues to be starved of service indefinitely. Furthermore, when the traffic is nonuniform, maxsize cannot sustain very high throughput <ref type="bibr" target="#b26">[25]</ref>. This is because it does not consider the backlog of cells in the VOQ's, or the time that cells have been waiting in line to be served.</p><p>For practical high-performance systems, we desire algorithms with the following properties.</p><p>• High Throughput: An algorithm that keeps the backlog low in the VOQ's; ideally, the algorithm will sustain an offered load up to 100% on each input and output. • Starvation Free: The algorithm should not allow a nonempty VOQ to remain unserved indefinitely. • Fast: To achieve the highest bandwidth switch, it is important that the scheduling algorithm does not become the performance bottleneck; the algorithm should therefore find a match as quickly as possible. • Simple to Implement: If the algorithm is to be fast in practice, it must be implemented in special-purpose hardware, preferably within a single chip. The algorithm presented in this paper is designed to meet these goals, and is currently implemented in a 16port commercial IP router with an aggregate bandwidth of 50 Gb/s <ref type="bibr" target="#b6">[6]</ref>, and a 32-port prototype switch with an aggregate bandwidth of 0.5 Tb/s <ref type="bibr" target="#b27">[26]</ref>.</p><p>is based on the parallel iterative matching algorithm (PIM) <ref type="bibr" target="#b1">[2]</ref>, and so to understand its operation, we start by describing PIM. Then, in Section II, we describe and its performance. We then consider some small modifications to for various applications, and finally consider its implementation complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Parallel Iterative Matching</head><p>PIM was developed by DEC Systems Research Center for the 16-port, 16 Gb/s AN2 switch <ref type="bibr" target="#b1">[2]</ref>. <ref type="foot" target="#foot_2">5</ref> Because it forms the basis of the algorithm described later, we will describe the scheme in detail and consider some of its performance characteristics.</p><p>PIM uses randomness to avoid starvation and reduce the number of iterations needed to converge on a maximal-sized match. A maximal-sized match (a type of on-line match) is one that adds connections incrementally, without removing connections made earlier in the matching process. In general, a maximal match is smaller than a maximum-sized match, but is much simpler to implement. PIM attempts to quickly converge on a conflict-free maximal match in multiple iterations, where each iteration consists of three steps. All inputs and outputs are initially unmatched and only those inputs and outputs not matched at the end of one iteration are eligible for matching in the next. The three steps of each iteration operate in parallel on each output and input and are shown in Fig. <ref type="figure" target="#fig_1">2</ref>. The steps are:</p><p>Step 1: Request. Each unmatched input sends a request to every output for which it has a queued cell.</p><p>Step 2: Grant. If an unmatched output receives any requests, it grants to one by randomly selecting a request uniformly over all requests.</p><p>Step 3: Accept. If an input receives a grant, it accepts one by selecting an output randomly among those that granted to this output.</p><p>By considering only unmatched inputs and outputs, each iteration only considers connections not made by earlier iterations.</p><p>Note that the independent output arbiters randomly select a request among contending requests. This has three effects: first, the authors in <ref type="bibr" target="#b1">[2]</ref> show that each iteration will match or eliminate, on average, at least of the remaining possible connections, and thus, the algorithm will converge to a maximal match, on average, in iterations. Second, it ensures that all requests will eventually be granted, ensuring Fig. <ref type="figure">3</ref>. Example of unfairness for PIM under heavy oversubscribed load with more than one iterations. Because of the random and independent selection by the arbiters, output 1 will grant to each input with probability 1/2, yet input 1 will only accept output 1 a quarter of the time. This leads to different rates at each output.</p><p>that no input queue is starved of service. Third, it means that no memory or state is used to keep track of how recently a connection was made in the past. At the beginning of each cell time, the match begins over, independently of the matches that were made in previous cell times. Not only does this simplify our understanding of the algorithm, but it also makes analysis of the performance straightforward; there is no time-varying state to consider, except for the occupancy of the input queues.</p><p>Using randomness comes with its problems, however. First, it is difficult and expensive to implement at high speed; each arbiter must make a random selection among the members of a time-varying set. Second, when the switch is oversubscribed, PIM can lead to unfairness between connections. An extreme example of unfairness for a 2 2 switch when the inputs are oversubscribed is shown in Fig. <ref type="figure">3</ref>. We will see examples later for which PIM and some other algorithms are unfair when no input or output is oversubscribed. Finally, PIM does not perform well for a single iteration; it limits the throughput to approximately 63%, only slightly higher than for a FIFO switch. This is because the probability that an input will remain ungranted is hence as increases, the throughput tends to Although the algorithm will often converge to a good match after several iterations, the time to converge may affect the rate at which the switch can operate. We would prefer an algorithm that performs well with just a single iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. THE SLIP ALGORITHM WITH A SINGLE ITERATION</head><p>In this section we describe and evaluate the SLIP algorithm. This section concentrates on the behavior of SLIP with just a single iteration per cell time. Later, we will consider SLIP with multiple iterations.</p><p>The SLIP algorithm uses rotating priority ("round-robin") arbitration to schedule each active input and output in turn. The main characteristic of SLIP is its simplicity; it is readily implemented in hardware and can operate at high speed. We find that the performance of SLIP for uniform traffic is high; for uniform independent identically distributed (i.i.d.) Bernoulli arrivals, SLIP with a single iteration can achieve 100% throughput. This is the result of a phenomenon that we encounter repeatedly; the arbiters in SLIP have a tendency to desynchronize with respect to one another.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Basic Round-Robin Matching Algorithm</head><p>SLIP is a variation of simple basic round-robin matching algorithm (RRM). RRM is perhaps the simplest and most Step 1: Request. Each input sends a request to every output for which it has a queued cell.</p><p>Step 2: Grant. If an output receives any requests, it chooses the one that appears next in a fixed, roundrobin schedule starting from the highest priority element. The output notifies each input whether or not its request was granted. The pointer to the highest priority element of the round-robin schedule is incremented (modulo to one location beyond the granted input.</p><p>Step 3: Accept. If an input receives a grant, it accepts the one that appears next in a fixed, round-robin schedule starting from the highest priority element. The pointer to the highest priority element of the round-robin schedule is incremented (modulo to one location beyond the accepted output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Performance of RRM for Bernoulli Arrivals</head><p>As an introduction to the performance of the RRM algorithm, Fig. <ref type="figure" target="#fig_3">5</ref> shows the average delay as a function of offered load for uniform independent and identically distributed (i.i.d.)  Bernoulli arrivals. For an offered load of just 63% RRM becomes unstable. <ref type="foot" target="#foot_3">6</ref>The reason for the poor performance of RRM lies in the rules for updating the pointers at the output arbiters. We illustrate this with an example, shown in Fig. <ref type="figure" target="#fig_4">6</ref>. Both inputs 1 and 2 are under heavy load and receive a new cell for both outputs during every cell time. But because the output schedulers move in lock-step, only one input is served during each cell time. The sequence of requests, grants, and accepts for four consecutive cell times are shown in Fig. <ref type="figure" target="#fig_5">7</ref>. Note that the grant pointers change in lock-step: in cell time 1, both point to input 1, and during cell time 2, both point to input 2, etc. This synchronization phenomenon leads to a maximum throughput of just 50% for this traffic pattern.</p><p>Synchronization of the grant pointers also limits performance with random arrival patterns. Fig. <ref type="figure" target="#fig_6">8</ref> shows the number of synchronized output arbiters as a function of offered load. The graph plots the number of nonunique 's, i.e., the number of output arbiters that clash with another arbiter. Under low  Results obtained using simulation for a 16 2 16 switch. offered load, cells arriving for output will find in a random position, equally likely to grant to any input. The probability that for all is which for implies that the expected number of arbiters with the same highest priority value is 9.9. This agrees well with the simulation result for RRM in Fig. <ref type="figure" target="#fig_6">8</ref>. As the offered load increases, synchronized output arbiters tend to move in lockstep and the degree of synchronization changes only slightly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THE SLIP ALGORITHM</head><p>The algorithm improves upon RRM by reducing the synchronization of the output arbiters. achieves this by not moving the grant pointers unless the grant is accepted.</p><p>is identical to RRM except for a condition placed on updating the grant pointers. The Grant step of RRM is changed to:</p><p>Step 2: Grant. If an output receives any requests, it chooses the one that appears next in a fixed round-robin schedule, starting from the highest priority element. The output notifies each input whether or not its request was granted. The pointer to the highest priority element of the round-robin schedule is incremented (modulo to one location beyond the granted input if, and only if, the grant is accepted in <ref type="bibr">Step 3.</ref> This small change to the algorithm leads to the following properties of with one iteration: Property 1: Lowest priority is given to the most recently made connection. This is because when the arbiters move their pointers, the most recently granted (accepted) input (output) becomes the lowest priority at that output (input). If input successfully connects to output both and are updated and the connection from input to output becomes the lowest priority connection in the next cell time.</p><p>Property 2: No connection is starved. This is because an input will continue to request an output until it is successful. The output will serve at most other inputs first, waiting at most cell times to be accepted by each input. Therefore, a requesting input is always served in less than cell times. Property 3: Under heavy load, all queues with a common output have the same throughput. This is a consequence of Property 2: the output pointer moves to each requesting input in a fixed order, thus providing each with the same throughput.</p><p>But most importantly, this small change prevents the output arbiters from moving in lock-step leading to a large improvement in performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. SIMULATED PERFORMANCE OF SLIP</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. With Benign Bernoulli Arrivals</head><p>Fig. <ref type="figure" target="#fig_3">5</ref> shows the performance improvement of SLIP over RRM. Under low load, SLIP's performance is almost identical to RRM and FIFO; arriving cells usually find empty input queues, and on average there are only a small number of inputs requesting a given output. As the load increases, the number of synchronized arbiters decreases (see Fig. <ref type="figure" target="#fig_6">8</ref>), leading to a large-sized match. In other words, as the load increases, we can expect the pointers to move away from each, making it more likely that a large match will be found quickly in the next cell time. In fact, under uniform 100% offered load, the SLIP arbiters adapt to a time-division multiplexing scheme, providing a perfect match and 100% throughput. Fig. <ref type="figure" target="#fig_7">9</ref> is an </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. With Bursty Arrivals</head><p>Real network traffic is highly correlated from cell to cell and so in practice, cells tend to arrive in bursts, corresponding perhaps to a packet that has been segmented or to a packetized video frame. Many ways of modeling bursts in network traffic have been proposed <ref type="bibr" target="#b12">[11]</ref>, <ref type="bibr" target="#b16">[15]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b23">[22]</ref>. Leland et al. <ref type="bibr" target="#b33">[32]</ref> have demonstrated that measured network traffic is bursty at every level making it important to understand the performance of switches in the presence of bursty traffic.</p><p>We illustrate the effect of burstiness on using an on-off arrival process modulated by a two-state Markov chain. The source alternately produces a burst of full cells (all with the same destination) followed by an idle period of empty cells. The bursts and idle periods contain a geometrically distributed number of cells. Fig. <ref type="figure" target="#fig_8">10</ref> shows the performance of under this arrival process for a 16 16 switch, comparing it with the performance under uniform i.i.d. Bernoulli arrivals. The burst length indicated in the graph represents the average length of each busy period. As we would expect, the increased burst size leads to a higher queueing delay. In fact, the average latency is proportional to the expected burst length. With bursty arrivals, the performance of an input-queued switch becomes more and more like an output-queued switch under the save arrival conditions <ref type="bibr" target="#b10">[9]</ref>. This similarity indicates that the performance for bursty traffic is not heavily influenced by the queueing policy or service discipline. Burstiness tends to concentrate the conflicts on outputs rather than inputs; each burst contains cells destined for the same output, and each input will be dominated by a single burst at a time, reducing input contention. As a result, the performance becomes limited by output contention, which is present in both input and output queued switches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. As a Function of Switch Size</head><p>Fig. <ref type="figure" target="#fig_9">11</ref> shows the average latency imposed by a scheduler as a function of offered load for switches with 4, 8, 16, and 32 ports. As we might expect, the performance degrades with the number of ports.  However, the performance degrades differently under low and heavy loads. For a fixed low offered load, the queueing delay converges to a constant value. However, for a fixed heavy offered load, the increase in queueing delay is proportional to</p><p>The reason for these different characteristics under low and heavy load lies once again in the degree of synchronization of the arbiters. Under low load, arriving cells find the arbiters in random positions and performs in a similar manner to the single iteration version of PIM. The probability that the cell is scheduled to be transmitted immediately is proportional to the probability that no other cell is waiting to be routed to the same output. Ignoring the (small) queueing delay under low offered load, the number of contending cells for each output is approximately which converges with increasing to <ref type="foot" target="#foot_4">7</ref> Hence, for constant small the queueing delay converges to a constant as increases. Under heavy load, the algorithm serves each FIFO once every cycles, and the queues will behave similarly to an M/D/1 queue with arrival rates and deterministic service time cell times. For an M/G/1 queue with random service times arrival rate and service rate the queueing delay is given by ( <ref type="formula">1</ref>) So, for the switch under a heavy load of Bernoulli arrivals, the delay will be approximately <ref type="bibr" target="#b1">(2)</ref> which is proportional to</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Burstiness Reduction</head><p>Intuitively, if a switch decreases the average burst length of traffic that it forwards, then we can expect it to improve the performance of its downstream neighbor. We can expect any scheduling policy that uses round-robin arbiters to be burst-reducing 8 this is also the case for is a deterministic algorithm serving each connection in strict rotation. We therefore expect that bursts of cells at different inputs contending for the same output will become interleaved and the burstiness will be reduced. This is indeed the case, as is shown in Fig. <ref type="figure" target="#fig_10">12</ref>. The graph shows the average burst length at the switch output as a function of offered load. Arrivals are on-off processes modulated by a two-state Markov chain with average burst lengths of 16, 32, and 64 cells.</p><p>Our results indicate that reduces the average burst length, and will tend to be more burst-reducing as the offered load increases. This is because the probability of switching between multiple connections increases as the utilization increases. When the offered load is low, arriving bursts do not encounter output contention and the burst of cells is passed unmodified. As the load increases, the contention increases and small N: For example, 1 0 [(N 0 1)=N ] N01 equals 0.6073 when N = 8, and 0.6202 when N = 16 and 0:63 when N is infinite. 8 There are many definitions of burstiness, for example the coefficient of variation <ref type="bibr" target="#b37">[36]</ref>, burstiness curves <ref type="bibr" target="#b21">[20]</ref>, maximum burst length <ref type="bibr">[10]</ref>, or effective bandwidth <ref type="bibr" target="#b22">[21]</ref>. In this section, we use the same measure of burstiness that we use when generating traffic: the average burst length. We define a burst of cells at the output of a switch as the number of consecutive cells that entered the switch at the same input. bursts are interleaved at the output. In fact, if the offered load exceeds approximately 70%, the average burst length drops to exactly one cell. This indicates that the output arbiters have become desynchronized and are operating as time-division multiplexers, serving each input in turn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. ANALYSIS OF SLIP PERFORMANCE</head><p>In general, it is difficult to accurately analyze the performance of a switch, even for the simplest traffic models. Under uniform load and either very low or very high offered load, we can readily approximate and understand the way in which operates. When arrivals are infrequent, we can assume that the arbiters act independently and that arriving cells are successfully scheduled with very low delay. At the other extreme, when the switch becomes uniformly backlogged, we can see that desynchronization will lead the arbiters to find an efficient time division multiplexing scheme and operate without contention. But when the traffic is nonuniform, or when the offered load is at neither extreme, the interaction between the arbiters becomes difficult to describe. The problem lies in the evolution and interdependence of the state of each arbiter and their dependence on arriving traffic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Convergence to Time-Division Multiplexing Under Heavy Load</head><p>Under heavy load, will behave similarly to an M/D/1 queue with arrival rates and deterministic service time cell times. So, under a heavy load of Bernoulli arrivals, the delay will be approximated by (2). To see how close approximates to time-division multiplexing under heavy load, Fig. <ref type="figure" target="#fig_11">13</ref> compares the average latency for both and an M/D/1 queue (2). Above an offered load of approximately 70%, behaves very similarly to the M/D/1 queue, but with a higher latency. This is because the service policy is not constant; when a queue changes between empty and nonempty, the scheduler must adapt to the new set of queues that require service. This adaptation takes place over many cell times while the arbiters desynchronize again. During this time, the throughput will be worse than for the M/D/1 queue and the queue length will increase. This in turn will lead to an increased latency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Desynchronization of Arbiters</head><p>We have argued that the performance of is dictated by the degree of synchronization of the output schedulers. In this section, we present a simple model of synchronization for a stationary and sustainable uniform arrival process.</p><p>In [24, Appendix 1], we find an approximation for the expected number of synchronized output schedulers at time</p><p>The approximation is based on two assumptions: 1) inputs that are unmatched at time are uniformly distributed over all inputs; 2) the number of unmatched inputs at time has zero variance. This leads to the approximation (3) where number of ports; arrival rate averaged over all inputs; . We have found that this approximation is quite accurate over a wide range of uniform workloads. Fig. <ref type="figure" target="#fig_12">14</ref> compares the approximation in (3) with simulation results for both i.i.d. Bernoulli arrivals and for an on-off arrival process modulated by a two-state Markov-chain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. THE SLIP ALGORITHM WITH MULTIPLE ITERATIONS</head><p>Until now, we have only considered the operation of with a single iteration. We now examine how the algorithm must change when multiple iterations are performed.</p><p>With more than one iteration, the iterative algorithm improves the size of the match; each iteration attempts to add connections not made by earlier iterations. Not surprisingly, we find that the performance improves as we increase the number of iterations (up to about for an switch). Once again, we shall see that desynchronization of the output arbiters plays an important role in achieving low latency.</p><p>When multiple iterations are used, it is necessary to modify the algorithm. The three steps of each iteration operate in parallel on each output and input and are as follows:</p><p>Step 1: Request. Each unmatched input sends a request to every output for which it has a queued cell. The 3 2 3 switch is heavily loaded, i.e., all active connections have an offered load of 1 cell per cell time. The sequence of grants and accepts repeats after two cell times, even though the (highlighted) connection from input 1 to output 2 has not been made. Hence, this connection will be starved indefinitely.</p><p>Step 2: Grant. If an unmatched output receives any requests, it chooses the one that appears next in a fixed, roundrobin schedule starting from the highest priority element. The output notifies each input whether or not its request was granted. The pointer to the highest priority element of the round-robin schedule is incremented (modulo to one location beyond the granted input if and only if the grant is accepted in Step 3 of the first iteration.</p><p>Step 3: Accept. If an unmatched input receives a grant, it accepts the one that appears next in a fixed round-robin schedule starting from the highest priority element. The pointer to the highest priority element of the round-robin schedule is incremented ( to one location beyond the accepted output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Updating Pointers</head><p>Note that pointers and are only updated for matches found in the first iteration. Connections made in subsequent iterations do not cause the pointers to be updated. This is to avoid starvation. To understand how starvation can occur, we refer to the example of a 3 3 switch with five active and heavily loaded connections, shown in Fig. <ref type="figure" target="#fig_13">15</ref>. The switch is scheduled using two iterations of the algorithm, except in this case, the pointers are updated after both iterations. The figure shows the sequence of decisions by the grant and accept arbiters; for this traffic pattern, they form a repetitive cycle in which the highlighted connection from input 1 to output 2 is never served. Each time the round-robin arbiter at output 2 grants to input 1, input 1 chooses to accept output 1 instead.</p><p>Starvation is eliminated if the pointers are not updated after the first iteration. In the example, output 2 would continue to grant to input 1 with highest priority until it is successful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Properties</head><p>With multiple iterations, the algorithm has the following properties:</p><p>Property 1: Connections matched in the first iteration become the lowest priority in the next cell time.</p><p>Property 2: No connection is starved. Because pointers are not updated after the first iteration, an output will continue to grant to the highest priority requesting input until it is successful.</p><p>Property 3: For with more than one iteration, and under heavy load, queues with a common output may each have a different throughput. repeats every three cell times.</p><p>Property 4: The algorithm will converge in at most iterations. Each iteration will schedule zero, one, or more connections. If zero connections are scheduled in an iteration, then the algorithm has converged; no more connections can be added with more iterations. Therefore, the slowest convergence will occur if exactly one connection is scheduled in each iteration. At most connections can be scheduled (one to every input and one to every output), which means the algorithm will converge in at most iterations. Property 5: The algorithm will not necessarily converge to a maximum sized match. At best, it will find a maximal match: the largest size match without removing connections made in earlier iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. SIMULATED PERFORMANCE OF ITERATIVE SLIP</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. How Many Iterations?</head><p>When implementing with multiple iterations, we need to decide how many iterations to perform during each cell time. Ideally, from Property 4 above, we would like to perform iterations. However, in practice there may be insufficient time for iterations, and so we need to consider the penalty of performing only iterations, where</p><p>In fact, because of the desynchronization of the arbiters, will usually converge in fewer than iterations. An interesting example of this is shown in Fig. <ref type="figure" target="#fig_14">16</ref>. In the first cell time, the algorithm takes iterations to converge, but thereafter converges in one less iteration each cell time. After cell times, the arbiters have become totally desynchronized and the algorithm will converge in a single iteration.</p><p>How many iterations should we use? It clearly does not always take One option is to always run the algorithm to completion, resulting in a scheduling time that varies from cell to cell. In some applications this may be acceptable. In others, such as in an ATM switch, it is desirable to maintain a fixed scheduling time and to try and fit as many iterations into that time as possible.</p><p>Under simulation, we have found that for an switch it takes about iterations for to converge. This is similar to the results obtained for PIM in <ref type="bibr" target="#b1">[2]</ref>, in which the authors prove that (4) where is the number of iterations that PIM takes to converge. For all the stationary arrival processes we have tried for However, we have not been able to prove that this relation holds in general.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. With Benign Bernoulli Arrivals</head><p>To illustrate the improvement in performance of when the number of iterations is increased, Fig. <ref type="figure" target="#fig_15">17</ref> shows the average queueing delay for one, two, and four iterations under uniform i.i.d. Bernoulli arrivals. We find that multiple iterations of significantly increase the size of the match and, therefore, reduce the queueing delay. In fact, can achieve 100% throughput for one or more iteration with uniform i.i.d. Bernoulli arrivals. Intuitively, size of the match increases with the number of iterations; each new iteration potentially adds connections not made by earlier iterations. This is illustrated in Fig. <ref type="figure" target="#fig_16">18</ref>, which compares the size of matching with the size of the maximum matching for the same instantaneous queue occupancies. Under low offered load, the arbiters move randomly and the ratio of the match size to the maximum match size decreases with increased offered load. But when the load exceeds approximately 65%, the ratio begins to increase linearly. As expected, the ratio increases  with the number of iterations indicating that the matching gets closer to the maximum-sized match, but only up to a point. For a switch under this traffic load, increasing the number  to divide the throughput to an output nonuniformly among competing inputs. The bandwidth from input to output is now a ratio subject to the admissibility constraints</p><p>In the basic algorithm each arbiter maintains an ordered circular list</p><p>In the Weighted algorith,m the list is expanded at output to be the ordered circular list where and input appears times in IX. IMPLEMENTING SLIP An important objective is to design a scheduler that is simple to implement. To conclude our description of we consider the complexity of implementing in hardware. We base our discussion on single-chip versions of that have been implemented for 16-port <ref type="bibr" target="#b6">[6]</ref> and 32-port <ref type="bibr" target="#b27">[26]</ref> systems.</p><p>As illustrated in Fig. <ref type="figure" target="#fig_18">20</ref>, each arbiter consists of a priority encoder with a programmable highest priority, a register to hold the highest priority value, and an incrementer to move the pointer after it has been updated. The decoder indicates to the next bank of arbiters which request was granted.</p><p>Fig. <ref type="figure" target="#fig_19">21</ref> shows how arbiters at each input and at each output) and an -bit memory are interconnected to construct an scheduler for an switch. The state memory records whether an input queue is empty or nonempty. From this memory, an -bit wide vector presents bits to each of grant arbiters, representing:</p><p>Step 1: Request. The grant arbiters select a single input among the contending requests, thus implementing Step 2.</p><p>Step 2: Grant. The grant decision from each grant arbiter is then passed to the accept arbiters, where each arbiter selects at most one output on behalf of an input, implementing Step 3.</p><p>Step 3: Accept. The final decision is then saved in a decision register and the values of the and pointers are updated. The decision register is used to notify each input which cell to transmit and to configure the crossbar switch. Reference <ref type="bibr" target="#b43">[42]</ref> focuses on the implementation of the algorithm, but it suffices here to make the following observations. First, the area required to implement the scheduler is dominated by the programmable priority encoders. The number of inverter equivalents required to implement the programmable priority encoders for prioritized-is shown in Table <ref type="table">I</ref>. <ref type="foot" target="#foot_5">9</ref> The number of gates for a 32-port scheduler is less than 100 000, making it readily implementable in current CMOS technologies, and the total number of gates grows approximately with</p><p>We have observed in two implementations that the regular structure of the design makes routing relatively straightforward. Finally, we have observed that the complexity of the implementation is (almost) independent of the number of iterations. When multiple iterations are used, the number of arbiters remain unchanged. The control overhead necessary to implement multiple iterations is very small.</p><p>In some implementations, it may be desirable to reduce the number of arbiters, sharing them among both the grant and accept steps of the algorithm. Such an implementation requiring only arbiters 10 is shown in Fig. <ref type="figure" target="#fig_1">22</ref>. When the results from the grant arbiter have settled, they are registered and fed back to the input for the second step. Obviously each arbiter must maintain a separate register for the and pointers, selecting the correct pointer for each step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>X. CONCLUSION</head><p>The Internet requires fast switches and routers to handle the increasing congestion. One emerging strategy to achieve this is to merge the strengths of ATM and IP, building IP routers around high-speed cell switches. Current cell switches can employ shared output queueing due to relatively low bandwidths. Unfortunately, the growth in demand for bandwidth far the growth in memory bandwidth, making it inevitable that switches will maintain queues at their inputs. We believe that these switches will use virtual output queueing, and hence will need fast, simple, fair, and efficient scheduling algorithms to arbitrate access to the switching fabric.</p><p>To this end, we have introduced the algorithm, an iterative algorithm that achieves high throughput, yet is simple to implement in hardware and operate at high speed. By using round-robin arbitration, provides fair access to output lines and prevents starvation of input queues. By careful control of the round-robin pointers, the algorithm can achieve 100% throughput for uniform traffic. When the traffic is nonuniform, the algorithm quickly adapts to an efficient round-robin policy among the busy queues. The simplicity of the algorithm allows the arbiter for a 32-port switch to be placed on single chip, and to make close to 100 million arbitration decisions per second.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. An input-queued switch with VOQ. Note that head of line blocking is eliminated by using a separate queue for each output at each input.</figDesc><graphic coords="2,43.98,59.58,249.36,151.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. An example of the three steps that make up one iteration of the PIM scheduling algorithm<ref type="bibr" target="#b1">[2]</ref>. In this example, the first iteration does not match input 4 to output 4, even though it does not conflict with other connections. This connection would be made in the second iteration. (a) Step 1: Request. Each input makes a request to each output for which it has a cell. This is shown here as a graph with all weights w ij = 1: (b) Step 2: Grant. Each output selects an input uniformly among those that requested it. In this example, inputs 1 and 3 both requested output 2. Output 2 chose to grant to input 3. (c) Step 3: Accept. Each input selects an output uniformly among those that granted to it. In this example, outputs 2 and 4 both granted to input 3. Input 3 chose to accept output 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Example of the three steps of the RRM matching algorithm. (a) Step 1: Request. Each input makes a request to each output for which it has a cell. Step 2: Grant. Each output selects the next requesting input at or after the pointer in the round-robin schedule. Arbiters are shown here for outputs 2 and 4. Inputs 1 and 3 both requested output 2. Since g 2 = 1; output 2 grants to input 1. g 2 and g 4 are updated to favor the input after the one that is granted. (b) Step 3: Accept. Each input selects at most one output. The arbiter for input 1 is shown. Since a 1 = 1; input 1 accepts output 1. a 1 is updated to point to output 2. (c) When the arbitration is completed, a matching of size two has been found. Note that this is less than the maximum sized matching of three.</figDesc><graphic coords="4,70.80,161.82,195.60,51.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Performance of RRM and iSLIP compared with PIM for i.i.d. Bernoulli arrivals with destinations uniformly distributed over all outputs.Results obtained using simulation for a 16 2 16 switch. The graph shows the average delay per cell, measured in cell times, between arriving at the input buffers and departing from the switch.</figDesc><graphic coords="4,320.10,59.58,222.96,258.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. 2 22 switch with RRM algorithm under heavy load. In the example of Fig. 7, synchronization of output arbiters leads to a throughout of just 50%.</figDesc><graphic coords="4,335.16,388.68,192.96,69.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Illustration of low throughput for RRM caused by synchronization of output arbiters. Note that pointers [g i ] stay synchronized, leading to a maximum throughput of just 50%.</figDesc><graphic coords="5,43.02,59.58,251.04,263.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Synchronization of output arbiters for RRM and iSLIP for i.i.d. Bernoulli arrivals with destinations uniformly distributed over all outputs.</figDesc><graphic coords="5,63.42,375.12,210.24,274.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Illustration of 100% throughput for iSLIP caused by desynchronization of output arbiters. Note that pointers [g i ] become desynchronized at the end of Cell 1 and stay desynchronized, leading to an alternating cycle of 2 cell times and a maximum throughput of 100%.</figDesc><graphic coords="6,54.12,59.58,228.96,116.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. The performance of iSLIP under two-state Markov-modulated Bernoulli arrivals. All cells within a burst are sent to the same output. Destinations of bursts are uniformly distributed over all outputs.</figDesc><graphic coords="6,319.38,59.58,224.40,267.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. The performance of iSLIP as function of switch size. Uniform i.i.d. Bernoulli arrivals.</figDesc><graphic coords="6,320.28,378.84,222.72,266.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Average burst length at switch output as a function of offered load. The arrivals are on-off processes modulated by a two-state DTMC. Results are for a 16 2 16 switch using the iSLIP scheduling algorithm.</figDesc><graphic coords="7,326.10,59.58,210.96,266.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Comparison of average latency for the iSLIP algorithm and an M/D/1 queue. The switch is 16 2 16 and, for the iSLIP algorithm, arrivals are uniform i.i.d. Bernoulli arrivals.</figDesc><graphic coords="8,58.86,59.58,219.36,267.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. Comparison of analytical approximation and simulation results for the average number of synchronized output schedulers. Simulation results are for a 16 216 switch with i.i.d. Bernoulli arrivals and an on-off process modulated by a two-state Markov chain with an average burst length of 64 cells. The analytical approximation is shown in (3).</figDesc><graphic coords="8,303.66,59.58,255.84,287.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. Example of starvation, if pointers are updated after every iteration.</figDesc><graphic coords="9,42.30,59.58,252.48,250.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 16 .</head><label>16</label><figDesc>Fig. 16. Example of the number of iterations required to converge for a heavily loaded N 2 N switch. All input queues remain nonempty for the duration of the example. In the first cell time, the arbiters are all synchronized. During each cell time, one more arbiter is desynchronized from the others.After N cell times, all arbiters are desynchronized and a maximum sized match is found in a single iteration.</figDesc><graphic coords="10,39.66,59.58,257.76,311.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 17 .</head><label>17</label><figDesc>Fig. 17. Performance of iSLIP for 1, 2, and 4 iterations compared with FIFO and output queueing for i.i.d. Bernoulli arrivals with destinations uniformly distributed over all outputs. Results obtained using simulation for a 16 2 16 switch. The graph shows the average delay per cell, measured in cell times, between arriving at the input buffers and departing from the switch.</figDesc><graphic coords="10,320.64,59.58,222.00,266.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 18 .</head><label>18</label><figDesc>Fig. 18. Comparison of the match size for iSLIP with the size of a maximum sized match for the same set of requests. Results are for a 16 2 16 switch and uniform i.i.d. Bernoulli arrivals.</figDesc><graphic coords="10,325.08,396.42,213.12,267.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 19 .</head><label>19</label><figDesc>Fig. 19. Performance of iSLIP for one, two, and four iterations under bursty arrivals. Arrival process is a two-state Markov-modulated on-off process. Average burst lengths are 16, 32, and 64 cells.</figDesc><graphic coords="11,64.86,59.58,207.36,266.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Fig. 20 .</head><label>20</label><figDesc>Fig. 20. Round-robin grant arbiter for iSLIP algorithm. The priority encoder has a programmed highest priority g i . The accept arbiter at the input is identical.</figDesc><graphic coords="12,56.70,59.58,223.68,107.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Fig. 21 .</head><label>21</label><figDesc>Fig. 21. Interconnection of 2N arbiters to implement iSLIP for an N 2 N switch.</figDesc><graphic coords="12,337.26,59.58,188.64,106.92" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>In some literature, the maximum size matching is called the maximum cardinality matching or just the maximum bipartite matching.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>This algorithm is equivalent to Dinic's algorithm<ref type="bibr" target="#b10">[9]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>This switch was commercialized as the Gigaswitch/ATM.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3"><p>The probability that an input will remain ungranted is (N 0 1=N) N ; hence as N increases, the throughput tends to 1 0 (1=e) 63%:</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4"><p>Note that the convergence is quite fast, and holds approximately even for</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_5"><p>These values were obtained from a VHDL design that was synthesized using the Synopsis design tools, and compiled for the Texas Instruments TSC5000 0.25-m CMOS ASIC process. The values for regular iSLIP will be smaller.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>Along the way, the development of was helped by discussions with T. Anderson, R. Edell, J. Walrand, and P. Varaiya, all at the University of California at Berkeley. The gate counts shown in Section IX were obtained by P. Gupta at Stanford University, Stanford, CA.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>of iterations beyond four does not measurably increase the average match size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. With Bursty Arrivals</head><p>We illustrate the effect of burstiness on using an on-off arrival process modulated by a two-state Markov-chain. Fig. <ref type="figure">19</ref> shows the performance of under this arrival process for a <ref type="bibr" target="#b17">16</ref> 16 switch, comparing the performance for one, two, and four iterations. As we would expect, the increased burst size leads to a higher queueing delay whereas an increased number of iterations leads to a lower queueing delay. In all three cases, the average latency is proportional to the expected burst length. The performance for bursty traffic is not heavily influenced by the queueing policy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. VARIATIONS ON SLIP</head><p>A. Prioritized SLIP Many applications use multiple classes of traffic with different priority levels. The basic algorithm can be extended to include requests at multiple priority levels with only a small performance and complexity penalty. We call this the Prioritized algorithm. In Prioritized each input now maintains a separate FIFO for each priority level and for each output. This means that for an switch with priority levels, each input maintains FIFO's. We shall label the queue between input and output at priority level where As before, only one cell can arrive in a cell time, so this does not require a processing speedup by the input.</p><p>The Prioritized algorithm gives strict priority to the highest priority request in each cell time. This means that will only be served if all queues are empty.</p><p>The algorithm is modified as follows.</p><p>Step 1: Request. Input selects the highest priority nonempty queue for output</p><p>The input sends the priority level of this queue to the output Step 2: Grant. If output receives any requests, it determines the highest level request, i.e., it finds The output then chooses one input among only those inputs that have requested at level</p><p>The output arbiter maintains a separate pointer, for each priority level. When choosing among inputs at level the arbiter uses the pointer and chooses using the same round-robin scheme as before. The output notifies each input whether or not its request was granted. The pointer is incremented (modulo to one location beyond the granted input if and only if input accepts output in step 3 of the first iteration.</p><p>Step 3: Accept. If input receives any grants, it determines the highest level grant, i.e., it finds</p><p>The input then chooses one output among only those that have requested at level</p><p>The input arbiter maintains a separate pointer, for each priority level. When choosing among outputs at level the arbiter uses the pointer and chooses using the same round-robin scheme as before. The input notifies each output whether or not its grant was accepted. The pointer is incremented (modulo to one location beyond the accepted output.</p><p>Implementation of the Prioritized algorithm is more complex than the basic algorithm, but can still be fabricated from the same number of arbiters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Threshold SLIP</head><p>Scheduling algorithms that find a maximum weight match outperform those that find a maximum sized match. In particular, if the weight of the edge between input and output is the occupancy of input queue then we will conjecture that the algorithm can achieve 100% throughput for all i.i.d. Bernoulli arrival patterns. But maximum weight matches are significantly harder to calculate than maximum sized matches <ref type="bibr" target="#b36">[35]</ref>, and to be practical, must be implemented using an upper limit on the number of bits used to represent the occupancy of the input queue.</p><p>In the Threshold algorithm, we make a compromise between the maximum-sized match and the maximum weight match by quantizing the queue occupancy according to a set of threshold levels. The threshold level is then used to determine the priority level in the Priority algorithm. Each input queue maintains an ordered set of threshold levels where If then the input makes a request of level</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Weighted SLIP</head><p>In some applications, the strict priority scheme of Prioritized may be undesirable, leading to starvation of lowpriority traffic. The Weighted algorithm can be used</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A neural network implementation of an input access scheme in a high-speed packet switch</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. GLOBECOM &apos;89</title>
		<meeting>GLOBECOM &apos;89</meeting>
		<imprint>
			<biblScope unit="page" from="1192" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">High speed switch scheduling for local area networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Owicki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Thacker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="319" to="352" />
			<date type="published" when="1993-11">Nov. 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Stochastic theory of a datahandling system with multiple sources</title>
		<author>
			<persName><forename type="first">D</forename><surname>Anick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Sondhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell Syst. Tech. J</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="1871" to="1894" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">GRF Multigigabit Router</title>
		<ptr target="http://www.ascend.com/230.html" />
	</analytic>
	<monogr>
		<title level="j">GRF IP Switch Tech. Product Description, Ascend Communications</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural network design of a Banyan network controller</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">X</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Select. Areas Commun</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1289" to="1298" />
			<date type="published" when="1990-10">Oct. 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A slight performance penalty is introduced by registering the output of the grant step and feeding back the result as the input to the accept step</title>
		<imprint/>
	</monogr>
	<note>This is likely to be small in practice</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Performing Internet Routing and Switching at Gigabit Speeds</title>
	</analytic>
	<monogr>
		<title level="m">GSR 12000 Tech. Product Description, Cisco Systems</title>
		<meeting><address><addrLine>San Jose, CA</addrLine></address></meeting>
		<imprint/>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Http</forename><surname>Available</surname></persName>
		</author>
		<ptr target="http://www.cisco.com/warp/public/733/12000/index.shtml" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cisco systems&apos; tag switching architecture overview</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rekhter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Davie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rosen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Swallow</surname></persName>
		</author>
		<ptr target="http://info.internet.isi.edu/in-notes/rfc/files/rfc2105.txt" />
	</analytic>
	<monogr>
		<title level="m">Internet RFC 2105, Cisco Systems</title>
		<meeting><address><addrLine>San Jose, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A fast algorithm for multi-channel/port traffic scheduling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Georganas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Supercom/ICC &apos;94</title>
		<meeting>IEEE Supercom/ICC &apos;94</meeting>
		<imprint>
			<biblScope unit="page" from="96" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Implementation of a three-stage Banyan-based architecture with input and output buffers for large fast packet switches</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Chiussi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Tobagi</surname></persName>
		</author>
		<idno>Rep. CSL-93-577</idno>
		<imprint>
			<date type="published" when="1993-06">June 1993</date>
			<pubPlace>Stanford, CA, Stanford CSL Tech</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A calculus for network delay, Part I: Network elements in isolation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cruz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="114" to="121" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A Markov modulated characterization of packetized voice and data traffic and related statistical multiplexer performance</title>
		<author>
			<persName><forename type="first">H</forename><surname>Heffes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Lucantoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Select. Areas Commun</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="856" to="868" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An algorithm for maximum matching in bipartite graphs</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Hopcroft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Karp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Soc. Ind. Appl. Math. J. Computation</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="225" to="231" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Starlite: A wideband digital switch</title>
		<author>
			<persName><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Knauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. GLOBECOM &apos;84</title>
		<meeting>GLOBECOM &apos;84</meeting>
		<imprint>
			<biblScope unit="page" from="121" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A broadband packet switch for integrated transport</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Arthurs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Select. Areas Commun</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1264" to="1273" />
			<date type="published" when="1987-10">Oct. 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Packet trains: Measurements and a new model for computer network traffic</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Routhier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Select. Areas Commun</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="986" to="995" />
			<date type="published" when="1986-04">Apr. 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Input versus output queueing on a space division switch</title>
		<author>
			<persName><forename type="first">M</forename><surname>Karol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hluchyj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Morgan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1347" to="1356" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Queueing in high-performance packetswitching</title>
		<author>
			<persName><forename type="first">M</forename><surname>Karol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hluchyj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Select. Areas Commun</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1587" to="1597" />
			<date type="published" when="1988-12">Dec. 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improving the performance of inputqueued ATM packet switches</title>
		<author>
			<persName><forename type="first">M</forename><surname>Karol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Eng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Obara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INFOCOM &apos;92</title>
		<meeting>INFOCOM &apos;92</meeting>
		<imprint>
			<biblScope unit="page" from="110" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Two-dimensional round-robin schedulers for packet switches with multiple input queues</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">O</forename><surname>Lamaire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Serpanos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Networking</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="471" to="482" />
			<date type="published" when="1993-10">Oct. 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Burstiness bounds for some burst reducing servers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Varaiya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INFOCOM &apos;93</title>
		<meeting>INFOCOM &apos;93</meeting>
		<imprint>
			<biblScope unit="page" from="2" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Effective bandwidths for multiclass Markov fluids and other ATM sources</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kesidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Walrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-S</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Networking</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="424" to="428" />
			<date type="published" when="1993-08">Aug. 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On the self-similar nature of Ethernet traffic</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Leland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Willinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Taqqu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGCOMM</title>
		<meeting>SIGCOMM<address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993-09">Sept. 1993</date>
			<biblScope unit="page" from="183" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fair prioritized scheduling in an input-buffered switch</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Reingold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IFIPIEEE Conf. Broadband Commun. &apos;96</title>
		<meeting>IFIPIEEE Conf. Broadband Commun. &apos;96<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996-04">Apr. 1996</date>
			<biblScope unit="page" from="358" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Scheduling algorithms for input-queued cell switches</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mckeown</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
		<respStmt>
			<orgName>Univ. California at Berkeley</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Achieving 100% throughput in an input-queued switch</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Anantharam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Walrand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE INFOCOM &apos;96</title>
		<meeting>IEEE INFOCOM &apos;96<address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="296" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The tiny tera: A small high-bandwidth packet switch core</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Izzard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mekkittikul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ellersick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Horowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="26" to="33" />
			<date type="published" when="1997-02">Jan.-Feb. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Matching output queueing with a combined input output queued switch</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Prabhakar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Select. Areas Commun</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A practical scheduling algorithm for achieving 100% throughput in input-queued switches</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mekkittikul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INFOCOM &apos;98</title>
		<meeting>INFOCOM &apos;98<address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="792" to="799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">IP switching: ATM under IP</title>
		<author>
			<persName><forename type="first">P</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Minshall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lyon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Networking</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="117" to="129" />
			<date type="published" when="1998-04">Apr. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Optimum architecture for input queueing ATM switches</title>
		<author>
			<persName><forename type="first">H</forename><surname>Obara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Electron. Lett</title>
		<imprint>
			<biblScope unit="page" from="555" to="557" />
			<date type="published" when="1991-03">Mar. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A fifty gigabit per second IP router</title>
		<author>
			<persName><forename type="first">C</forename><surname>Partridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Networking</title>
		<imprint/>
	</monogr>
	<note>to be published</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">aItPm: A strategy for integrating IP with ATM</title>
		<author>
			<persName><forename type="first">G</forename><surname>Parulkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGCOMM &apos;95</title>
		<meeting>ACM SIGCOMM &apos;95<address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="287" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Symmetric crossbar arbiters for VLSI communication switches</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-C</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Parallel Dist. Syst</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="13" to="27" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">High performance multi-queue buffers for VLSI communication switches</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Frazier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 15th Annu</title>
		<meeting>15th Annu</meeting>
		<imprint>
			<date type="published" when="1988-06">June 1988</date>
			<biblScope unit="page" from="343" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Data structures and network algorithms</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Tarjan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Soc. Ind. Appl. Mathematics</title>
		<imprint>
			<date type="published" when="1983-11">Nov. 1983</date>
		</imprint>
	</monogr>
	<note>PA</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Hopfield neural network architecture for crossbar switch control</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Troudet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Walters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="42" to="57" />
			<date type="published" when="1991-01">Jan. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A broadband packet switch architecture with input and output queueing</title>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Paulraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kailath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Globecom &apos;94</title>
		<meeting>Globecom &apos;94</meeting>
		<imprint>
			<biblScope unit="page" from="448" to="452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Performance of packet switches with input and output queueing</title>
		<author>
			<persName><forename type="first">I</forename><surname>Iliadis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Denzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICC &apos;90</title>
		<meeting>ICC &apos;90<address><addrLine>Atlanta, GA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990-04">Apr. 1990</date>
			<biblScope unit="page" from="747" to="753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Analysis of a packet switch with input and output buffers and speed constraints</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Georganas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INFOCOM &apos;91</title>
		<meeting>INFOCOM &apos;91<address><addrLine>Bal Harbour, FL</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="694" to="700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">nE ect of speedup in nonblocking packet switch</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Oie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Murata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kubota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Miyahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICC &apos;89</title>
		<meeting>ICC &apos;89<address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1989-06">June 1989</date>
			<biblScope unit="page" from="410" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Throughput analysis, optimal buffer allocation, and traffic imbalance study of a generic nonblocking packet switch</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Stern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Select. Areas Commun</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="439" to="449" />
			<date type="published" when="1991-04">Apr. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Nick McKeown (S&apos;91-M&apos;95-SM&apos;97) received the Ph.D. degree from the University of California at Berkeley in 1995. He is an</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="20" to="28" />
			<date type="published" when="1986">Jan.-Feb. 1999. 1986 to 1989</date>
			<pubPlace>Bristol, U.K.; San Jose, CA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Assistant Professor of Electrical Engineering and Computer Science at Stanford University, CA ; Communications Research Group, Hewlett-Packard Labs</orgName>
		</respStmt>
	</monogr>
	<note>During the spring of 1995, he was with Cisco Systems. where he architected their GSR 12000</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">His research involves techniques for high-speed networks, including high-speed Internet routing and architectures for high-speed switches. More recently, he has worked on the analysis and design of cell scheduling algorithms, switch and buffer architectures, and lookup algorithms. Dr. McKeown is an Editor for the IEEE TRANSACTIONS ON COMMUNICATIONS</title>
		<author>
			<persName><surname>Router</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>He is the Robert Noyce Faculty Fellow at Stanford, and recipient of a Fellowship from the Alfred P. Sloan Foundation</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
