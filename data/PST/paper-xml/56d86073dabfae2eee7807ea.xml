<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deception Detection using Real-life Trial Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Verónica</forename><surname>Pérez-Rosas</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mohamed</forename><surname>Abouelenien</surname></persName>
							<email>zmohamed@umich.edu</email>
						</author>
						<author>
							<persName><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
							<email>mihalcea@umich.edu</email>
						</author>
						<author>
							<persName><forename type="first">Mihai</forename><surname>Burzo</surname></persName>
							<email>mburzo@umich.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Engineering</orgName>
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<postCode>48109</postCode>
									<settlement>Ann Arbor</settlement>
									<region>MI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Computer Science and Engineering</orgName>
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<postCode>48109</postCode>
									<settlement>Ann Arbor</settlement>
									<region>MI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Computer Science and Engineering</orgName>
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<postCode>48109</postCode>
									<settlement>Ann Arbor</settlement>
									<region>MI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Mechanical Engineering University of Michigan</orgName>
								<address>
									<postCode>48502</postCode>
									<settlement>Flint Flint</settlement>
									<region>MI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deception Detection using Real-life Trial Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/2818346.2820758</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>multimodal</term>
					<term>verbal</term>
					<term>non-verbal</term>
					<term>real-life trial</term>
					<term>deception detection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Hearings of witnesses and defendants play a crucial role when reaching court trial decisions. Given the high-stake nature of trial outcomes, implementing accurate and effective computational methods to evaluate the honesty of court testimonies can offer valuable support during the decision making process. In this paper, we address the identification of deception in real-life trial data. We introduce a novel dataset consisting of videos collected from public court trials. We explore the use of verbal and non-verbal modalities to build a multimodal deception detection system that aims to discriminate between truthful and deceptive statements provided by defendants and witnesses. We achieve classification accuracies in the range of 60-75% when using a model that extracts and fuses features from the linguistic and gesture modalities. In addition, we present a human deception detection study where we evaluate the human capability of detecting deception in trial hearings. The results show that our system outperforms the human capability of identifying deceit.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>With thousands of trials and verdicts occurring daily in courtrooms around the world, the chance of using deceptive statements and testimonies as evidence is growing. Given the high-stake nature of trial outcomes, implementing accurate and effective computational methods to evaluate the honesty of provided testimonies can offer valuable support during the decision making process.</p><p>The consequences of falsely accusing the innocents and freeing the guilty can be severe. For instance, in U.S. along there are tens of thousand of criminal cases filed every year. In 2013, there were 89,936 criminal cases filings in U.S. District Courts and in 2014 the number was 80,262. <ref type="foot" target="#foot_0">1</ref> Moreover, the average number of exonerations per year increased from 3.03 in 1973-1999 to 4.29 between 2000 and 2013. The National Registry of Exonerations reported on 873 exonerations from 1989 to 2012, with a tragedy behind each case <ref type="bibr" target="#b16">[17]</ref>. Hence, the need arises for a reliable and efficient system to detect deceptive behavior and discriminate between liars and truth tellers.</p><p>In criminal settings, the polygraph test has been used as a standard method to identify deceptive behavior. This becomes impractical in some cases, as this method requires the use of skin-contact devices and human expertise. In addition, the final decisions are subject to error and bias <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b14">15]</ref>. Furthermore, using proper countermeasures, offenders can deceive these devices as well as the human experts. Given the difficulties associated with the use of polygraph-like methods, learning-based approaches have been proposed to address the deception detection task using a number of modalities, including text <ref type="bibr" target="#b12">[13]</ref> and speech <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b28">29]</ref>. Unlike the polygraph method, learning-based methods for deception detection rely mainly on data collected from deceivers and truth-tellers. The data is usually elicited from human contributors, in a lab setting or via crowdsourcing <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b32">33]</ref>, for instance by asking subjects to narrate stories in deceptive and truthful manner <ref type="bibr" target="#b27">[28]</ref>, by performing one-on-one interviews, or by participating in "Mock crime" scenarios <ref type="bibr" target="#b32">[33]</ref>. However, an important drawback identified in this datadriven research on deception detection is the lack of real data and the absence of true motivation while eliciting deceptive behavior. Because of the artificial setting, the subjects may not be emotionally aroused thus making it difficult to generalize findings to reallife scenarios.</p><p>In this paper, we describe what we believe is a first attempt at building a multimodal system that detects deception in real-life trial data using text and gestures modalities. While there is research work that has used court trial transcripts to identify deceptive statements <ref type="bibr" target="#b13">[14]</ref>, we are not aware of any previous work that took into consideration modalities other than text for deception detection on trial court data.</p><p>We present a novel dataset consisting of 121 deceptive and truthful video clips, from real court trials. We use the transcription of these videos to extract several linguistic features, and we manually annotate the videos for the presence of several gestures that are used to extract non-verbal features. We then build a system that jointly uses the verbal and non-verbal modalities to automatically detect the presence of deception. Our experiments show that the multimodal system can identify deception with an accuracy in the range of 60-75%, which is significantly above the chance level. As deception detection research suggests that humans perform slightly above the chance level, we also place our results in context by performing a study where humans evaluate the presence of deception in court statements in single or multimodal data streams. Results show that our system outperforms humans on this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Verbal Deception Detection</head><p>To date, several research publications on verbal-based deception detection have explored the identification of deceptive content in a variety of domains, including online dating websites <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b17">18]</ref>, forums <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b22">23]</ref>, social networks <ref type="bibr" target="#b20">[21]</ref>, and consumer report websites <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b23">24]</ref>. Research findings have shown the effectiveness of features derived from text analysis, which frequently includes basic linguistic representations such as n-grams and sentence count statistics <ref type="bibr" target="#b27">[28]</ref>, and also more complex linguistic features derived from syntactic CFG trees and part of speech tags <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b42">43]</ref>. Some studies have also incorporated the analysis of psycholinguistics aspects related to the deception process. Research work has relied on the Linguistic Inquiry and Word Count (LIWC) lexicon <ref type="bibr" target="#b33">[34]</ref> to build deception models using machine learning approaches <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b3">4]</ref> and showed that the use of psycholinguistic information was helpful for the automatic identification of deceit. Following the hypothesis that deceivers might create less complex sentences in an effort to conceal the truth and being able to recall their lies more easily, several researchers have also studied the relation between text syntactic complexity and deception <ref type="bibr" target="#b43">[44]</ref>.</p><p>While most of the data used in related research was collected under controlled settings, only few works have explored the used of data from real-life scenarios. This can be partially attributed to the difficulty of collecting such data, as well as the challenges associated with verifying the deceptive or truthful nature of realworld data. To our knowledge, there is very little work focusing on real-life high-stake data. The work closest to ours is presented by Fornaciari and Poesio <ref type="bibr" target="#b13">[14]</ref>, which targets the identification of deception in statements issued by witnesses and defendants using a corpus collected from hearings in Italian courts. Following this line of work, we present a study on deception detection using real-life trial data and explore the use of multiple modalities for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Non-verbal Deception Detection</head><p>Earlier approaches on non-verbal deception detection relied on polygraph tests to detect deceptive behavior. These tests are mainly based on physiological features such as heart rate, respiration rate, and skin temperature. Several studies <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b9">10]</ref> indicated that relying solely on such physiological measurements can be biased and misleading. Chittaranjan et al. <ref type="bibr" target="#b6">[7]</ref> created an audio-visual recordings of the "Are you a Werewolf?" game in order to detect deceptive behaviour using non-verbal audio cues and to predict the subjects' decisions in the game. In order to improve lie detection in criminalsuspect interrogations, Sumriddetchkajorn and Somboonkaew <ref type="bibr" target="#b36">[37]</ref> developed an infrared system to detect lies by using thermal variations in the periorbital area and by deducing the respiration rate from the thermal nostril areas. Granhag and Hartwig <ref type="bibr" target="#b15">[16]</ref> proposed a methodology using psychologically informed mind-reading to evaluate statements from suspects, witnesses, and innocents.</p><p>For hand gestures, blob analysis was used to detect deceit by tracking the hand movements of subjects <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b39">40]</ref>, or using geometric features related to the hand and head motion <ref type="bibr" target="#b26">[27]</ref>. Caso et al. <ref type="bibr" target="#b5">[6]</ref> identified several hand gestures that can be related to the act of deception using data from simulated interviews. Cohen et al. <ref type="bibr" target="#b7">[8]</ref> found that fewer iconic hand gestures were a sign of a deceptive narration, and Hillman et al. <ref type="bibr" target="#b18">[19]</ref> determined that increased speech prompting gestures were associated with deception while increased rhythmic pulsing gestures were associated with truthful behavior. Also related is the taxonomy of hand gestures developed by Maricchiolo et al. <ref type="bibr" target="#b25">[26]</ref> for applications such as deception detection and social behavior.</p><p>Facial expressions also play a critical role in the identification of deception. Ekman <ref type="bibr" target="#b10">[11]</ref> defined micro-expressions as relatively short involuntary expressions, which can be indicative of deceptive behavior. Moreover, these expressions were analyzed using smoothness and asymmetry measurements to further relate them to an act of deceit <ref type="bibr" target="#b11">[12]</ref>. Tian et al. <ref type="bibr" target="#b37">[38]</ref> considered features such as face orientation and facial expression intensity. Owayjan et al. <ref type="bibr" target="#b31">[32]</ref> extracted geometric-based features from facial expressions, and Pfister and Pietikainen <ref type="bibr" target="#b35">[36]</ref> developed a micro-expression dataset to identify expressions that are clues for deception.</p><p>Recently, features from different modalities were integrated in order to find a combination of multimodal features with superior performance <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b21">22]</ref>. A multimodal deception dataset consisting of linguistic, thermal, and physiological features was introduced in <ref type="bibr" target="#b34">[35]</ref>, which was then used to develop a multimodal deception detection system <ref type="bibr" target="#b1">[2]</ref>. An extensive review of approaches for evaluating human credibility using physiological, visual, acoustic, and linguistic features is available in <ref type="bibr" target="#b29">[30]</ref>.</p><p>To our knowledge, no work to date has considered the problem of deception detection on multimodal real-life trial data, which is the task that we are addressing in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">DATASET</head><p>Our goal is to build a multimodal collection of occurrences of real deception during court trials, which will allow us to analyze both verbal and non-verbal behaviors in relation to deception.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Collection</head><p>To collect the dataset, we start by identifying public multimedia sources where trial hearing recordings were available, and deceptive and truthful behavior could be fairly observed and verified.</p><p>We specifically target trial recordings on which some of the constraints imposed by current data processing technologies could be enforced: the defendant or witness in the video should be clearly identified; her or his face should be visible enough during most of the clip duration; visual quality should be clear enough to identify the facial expressions; and finally, audio quality should be clear enough to hear the voices and understand what the person is saying.</p><p>We considered three different trial outcomes that helped us to correctly label a certain trial video clip as deceptive or truthful: guilty verdict, non-guilty verdict, and exoneration. Thus, for guilty verdicts, deceptive clips are collected from a defendant in a trial and truthful videos are collected from witnesses in the same trial. In some cases, deceptive videos are collected from a suspect denying a crime he committed and truthful clips are taken from the same suspect when answering questions concerning some facts that were verified by the police as truthful. For the witnesses, testimonies that were verified by police investigations are labeled as truthful </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Truthful</head><p>Deceptive We proceeded to step back into the living room in front of the fireplace while William was sitting in the love seat. And he was still sitting there in shock and so they to repeatedly tell him to get down on the ground. And so now all three of us are face down on the wood floor and they just tell us "don't look, don't look" And then they started rummaging through the house to find stuff... No, no. I did not and I had absolutely nothing to do with her disappearance. And I'm glad that she did. I did. I did. Um and then when Laci disappeared, um, I called her immediately. It wasn't immediately, it was a couple of days after Laci's disappearance that I telephoned her and told her the truth. That I was married, that Laci's disappeared, she didn't know about it at that point.</p><p>Table <ref type="table">1</ref>: Sample transcripts for deceptive and truthful clips in the dataset.</p><p>whereas testimonies in favor of a guilty suspect are labeled as deceptive. Exoneration testimonies are collected as truthful statements.</p><p>Examples of famous trials included in the dataset are the trials of Jodi Arias, Donna Scrivo, Jamie Hood, Andrea Sneiderman, Mitchelle Blair, Amanda Hayes, Crystal Mangum, Marissa Devault, Carlos Miller, Michael Dunn, Bessman Okafor, Jonathan Santillan, among other trials. Clips containing exonerees testimonies are obtained from "The Innocence Project" website. 2  Given our goals and constraints, data collection ended up being a lengthy and laborious process consisting of several iterations of Web mining, data processing and analysis, and content validation.</p><p>The final dataset consists of 121 videos including 61 deceptive and 60 truthful trial clips. The average length of the videos in the dataset is 28.0 seconds. The average video length is 27.7 seconds and 28.3 seconds for the deceptive and truthful clips, respectively. The data consists of 21 unique female and 35 unique male speakers, with their ages approximately ranging between 16 and 60 years.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Transcriptions</head><p>Our goal is to analyze both verbal and non-verbal behavior to understand their relation to deception. All the video clips are transcribed via crowd sourcing using Amazon Mechanical Turk. We specifically asked transcribers to include word repetitions and fillers such as um, ah, and uh, as well as indicate intentional silence using ellipsis. Obtained transcriptions were manually verified to avoid spam and ensure their quality. The final set of transcriptions consists of 8,055 words, with an average of 66 words per transcript. Table <ref type="table">1</ref> shows transcriptions of sample deceptive and truthful statements. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hands</head><p>Hand Trajectory</p><formula xml:id="formula_0">1&amp; &amp; &amp; 26&amp; &amp; &amp; &amp; 15&amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; 29&amp; &amp; &amp; &amp; 35&amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; 61&amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; 25&amp; &amp; 13&amp; &amp; &amp; &amp; &amp; &amp; 55&amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; 48&amp; &amp; &amp; &amp; &amp; 5&amp; &amp; 5&amp; &amp; 20&amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; 76&amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; 20&amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; 92&amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; 29&amp; &amp; 4&amp; 4&amp; &amp; &amp; 28&amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; 85&amp; 1&amp; &amp; &amp; 30&amp; &amp; &amp; &amp; 7&amp; &amp; 9&amp; &amp; 10&amp; &amp; 4&amp; &amp; 11&amp; &amp; &amp; &amp; &amp; 45&amp; &amp; &amp; &amp; &amp; 4&amp; &amp; &amp; 26&amp; &amp; &amp; &amp; &amp; &amp; &amp; 53&amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; 42&amp; &amp; &amp; 19&amp; &amp; 5&amp; &amp; &amp; &amp; &amp; 54&amp; &amp; &amp; &amp; &amp; &amp; 4&amp; &amp; &amp; &amp; &amp; 39&amp;</formula><p>Figure <ref type="figure">2</ref>: Distribution for nine facial displays and hand gestures</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Non-verbal Behavior Annotations</head><p>We annotate the gestures<ref type="foot" target="#foot_2">3</ref> observed during the interactions in the video clips. Since our data occurs in interview-based scenarios where deceivers and truth-tellers are interacting with the interviewers, we decided to annotate the gesture behavior using a scheme that has been specifically designed for interpersonal communication.</p><p>We specifically focus on the annotation of facial displays and hand movements, as they have been previously found to correlate with deceptive behavior <ref type="bibr" target="#b8">[9]</ref>. The gesture annotation is performed using the MUMIN coding scheme <ref type="bibr" target="#b2">[3]</ref>, which is a standard multimodal annotation scheme for interpersonal interactions.</p><p>In the MUMIN scheme, facial displays consist of several different facial expressions associated with overall facial expressions, eyebrows, eyes and mouth movements, gaze direction, as well as head movements. In addition, the scheme includes a separate category for general face displays, which codes four facial expressions: smile, laughter, scowl, and other. Hand movements are also labeled in terms of handedness and trajectory. Figure <ref type="figure">2</ref> shows the nine gesture categories considered during the annotation.</p><p>The multimodal annotation was performed by two annotators using the Elan software. During the annotation process, annotators were allowed to watch each video clip as many times as they needed. They were asked to identify the facial displays and hand gestures that were most frequently observed or dominating during the entire clip duration. For each video clip, the annotators had to choose one label for each of the nine gestures listed in Figure <ref type="figure">2</ref>. Annotations were performed at video level in accordance with the overall judgment of truthfulness and deceitfulness, which is based on the whole video content. During the annotation process, annotators chose only one label per gesture for every video clip.</p><p>Note that the "Other" category indicates cases where none of the other gestures was observed. For instance, in the case of gestures associated with hand movements, the "Other" label also accounted for those cases where the speaker's hands were not moving or were not visible.</p><p>Before all the video clips were annotated for gestures, we measured the inter-annotator agreement in a subset of 56 videos. Table <ref type="table" target="#tab_0">2</ref> shows the observed annotation agreement between the two annotators, along with the Kappa statistic. The agreement measure represents the percentage of times the two annotators agreed on the same label for each gesture category. For instance, 80.03% of the time the annotators agreed on the labels assigned to the Eyebrows category. On average, the observed agreement was measured at 75.16%, with a Kappa of 0.57 (macro-averaged over the nine categories). Differences in annotation were reconciled through discussions. After this, the remaining videos were split between the two annotators, and were labeled by only one annotator. Figure <ref type="figure">2</ref> shows Frown" Scowl" Eyebrows"raising" Eyes"closing" repeated"</p><p>Gaze"interlocutor" Open"mouth" Head"repeated" nods"</p><p>Head"repeated" shake"</p><p>Both"hands"</p><p>DecepJve"" Truthful" </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">FEATURES OF VERBAL AND NON-VERBAL BEHAVIORS</head><p>Given the multimodal nature of our dataset, we focus on both the linguistic and gesture components of the recordings included in our collection. In this section, we describe the sets of features extracted from each modality, which will then be used to build classifiers of deception.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Verbal Features</head><p>The verbal features consist of unigrams and bigrams derived from the bag-of-words representation of the video transcripts. These features are encoded as word and word pairs frequencies and include all the words present in the transcripts with frequencies greater than 10. The frequency threshold cut was chosen using a small development set.</p><p>Previous work has also considered features derived from semantic lexicons, e.g., LIWC <ref type="bibr" target="#b33">[34]</ref>. However, while these are great features to consider in order to gain insights into the semantic categories of words that represent useful clues for deception, their performance is often similar to that of the n-grams features <ref type="bibr" target="#b1">[2]</ref>. Since in our current work we are not focusing on the insights that can be gained from linguistic analyses, we are not using these features in our current experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Non-verbal Features</head><p>The non-verbal features are derived from the annotations performed using the MUMIN coding scheme as described in Section 3.3. We create a binary feature for each of the 40 available gesture labels. Each feature indicates the presence of a gesture only if it is observed during the majority of the interaction duration. The generated features represent nine different gesture categories covering facial displays and hand movements.</p><p>Facial Displays. These are facial expressions or head movements displayed by the speaker during the deceptive or truthful interaction. They include all the behaviors listed in Figure <ref type="figure">2</ref> under the General Facial Expressions, Eyebrows, Eyes, Mouth Openness, Mouth Lips, and Head Movements.</p><p>Hand Gestures. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTS</head><p>We start our experiments with an analysis of the non-verbal behaviors occurring in deceptive and truthful videos. We compare the percentage of each behavior as observed in each class. For instance, there is a total of 61 videos in the dataset that include the Eyebrows raising feature (as shown in Figure <ref type="figure">2</ref>), out of which 24 are part of the deceptive set of 61 videos, and 37 are part of the truthful set (60 videos). Hence, the percentages of existence of this feature are 39% in the deceptive class and 61% in the truthful class. Figure <ref type="figure" target="#fig_3">3</ref> shows the percentages of all the non-verbal features for which we observe noticeable differences for the deceptive and truthful groups. As the figure suggests, eyebrow and eye gestures help differentiate between the deceptive and truthful conditions. For instance, we can observe that truth-tellers appear to raise their eyebrows (Eyebrows raising), shake their head (Head repeated shake), and blink (Eyes closing repeated) more frequently than deceivers. Interestingly, deceivers seem to blink and shake their head less frequently than truth-tellers.</p><p>Motivated by these results, we proceed to conduct further experiments to evaluate the performance of the extracted features using a machine learning approach.</p><p>We run our learning experiments on the trial dataset introduced earlier. Given the distribution between deceptive and truthful clips, the baseline on this dataset is 50.4%. For each video clip, we create feature vectors formed by combinations of the verbal and nonverbal features described in the previous section.</p><p>We build deception classifiers using two classification algorithms: Decision Trees (DT) and Random Forest (RF). <ref type="foot" target="#foot_3">4</ref> We run several comparative experiments using leave-one-out cross-validation. Table <ref type="table" target="#tab_1">3</ref> shows the accuracy figures obtained by the two classifiers on the major feature groups described in Section 4. As shown in this table, the combined classifier that uses all the features (using Decision Trees) and the individual classifier that relies on the facial displays features (using Random Forest) achieve the best results. We also evaluate classifiers that rely on combined sets of features, with the non-verbal features clearly outperforming the verbal features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0"</head><p>0.2" 0.4" 0.6" 0.8" 1"</p><p>Frowning"</p><p>Eyebrows"raising"</p><p>Head"side"turn"</p><p>Lip"corners"up"</p><p>Lips"protruded"</p><p>Lips"retracted" </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Analysis of Feature Contribution</head><p>To better understand the contribution of the different feature sets to the overall classifier performance, we conduct an ablation study where we remove one group of features at a time. Given that Decision Trees had the most consistent performance across different settings in our initial set of experiments, we run all our analysis experiments only using this classifier.</p><p>Table <ref type="table" target="#tab_2">4</ref> shows the accuracies obtained when one feature group is removed and the deception classifier is built using the remaining features. From this table we made interesting findings for the combined model. Not surprisingly, the facial displays seem to contribute the most to the classifier performance, followed by the unigram features. This further suggests that a better feature fusion method may be beneficial, and we plan to explore this in future work.</p><p>For a closer look at the contribution of individual features included in the group of Facial Displays, we determine and compare the weights assigned by the learning algorithm to the features in this group, as shown in Figure <ref type="figure" target="#fig_4">4</ref>. <ref type="foot" target="#foot_4">5</ref> The five most predictive features are the presence of frowning (Frowning), eyebrows movement (Eyebrows raising), lip gestures (Lip corners up, Lips protruded, Lips retracted), and head turns (Head side turn). These gestures were frequently portrayed by defendants and witnesses while being interrogated.  question can shed light on the difficulty of the task, and can also place our results in perspective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">HUMAN PERFORMANCE</head><p>We conduct a study where we evaluate the human ability to identify deceit on trial recordings when exposed to four different modalities: Text, consisting of the language transcripts; Audio, consisting of the audio track of the clip; Silent video, consisting of only the video with muted audio; and Full video, where audio and video are played simultaneously.</p><p>We create an annotation interface that shows an annotator instances for each modality in random order, and ask him or her to select a label of either "Deception" or "Truth" according to his or her perception of truthfulness or falsehood. The annotators did not have access to any information that would reveal the true label of an instance. The only exception to this could have been the annotators' previous knowledge of some of the public trials in our dataset. A discussion with the annotators after the annotation took place indicated however that this was not the case.</p><p>To avoid annotation bias, we show the modalities in the following order: first we show either Text or Silent video, then we show Audio, followed by Full video. Note that apart from this constraint, which is enforced over the four modalities belonging to each video clip, the order in which instances are presented to an annotator is random.</p><p>Three annotators labeled all 121 video clips in our dataset. Since four modalities were extracted from each video, each annotator annotated a total of 484 instances. Annotators were not offered a monetary reward and we considered their judgments to be honest as they participated voluntarily in this experiment. Table <ref type="table" target="#tab_3">5</ref> shows the observed agreement and Kappa statistics among the three annotators for each modality. <ref type="foot" target="#foot_5">6</ref> The agreement for most modalities is rather low and the Kappa scores show mostly slight agreement. As noted before <ref type="bibr" target="#b30">[31]</ref>, this low agreement can be interpreted as an indication that people are poor judges of deception.</p><p>We also determine each annotator performance for each modality. The results, shown in Table <ref type="table" target="#tab_5">6</ref>, additionally support the argument that human judges have difficulty performing the deception detection task. Not surprisingly, human detection of deception on silent video is more challenging than the rest of the modalities due to the lesser amount of deception cues available to the raters. An interesting, yet perhaps unsurprising observation is that the human performance increases with the availability of modalities. The  poorest accuracy is obtained in Silent video, followed by Text, Audio, and Full video where the judges have the highest performance. Overall, our study indicates that detecting deception is indeed a difficult task for humans and further verifies previous findings where human ability to spot liars was found to be slightly better than chance <ref type="bibr" target="#b0">[1]</ref>. Moreover, the performance of the human annotators appears to be significantly below that of our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSIONS</head><p>In this paper we presented a study of multimodal deception detection using real-life high-stake occurrences of deceit. We introduced a novel dataset from public real trials, and used this dataset to perform both qualitative and quantitative experiments. Our analysis of non-verbal behaviors occurring in deceptive and truthful videos brought insight into the gestures that play a role in deception. We also built classifiers relying on individual or combined sets of verbal and non-verbal features, and showed that we can achieve accuracies in the range of 60-75%. Additional analyses showed the role played by the various feature sets used in the experiments.</p><p>We also performed a study of human ability to spot liars in single or multimodal data streams. The study revealed high disagreement and low deception detection accuracies among human annotators. Our automatic system outperformed humans using different modalities with a relative percentage improvement of up to 51%.</p><p>To our knowledge this is the first work to automatically detect instances of deceit using both verbal and non-verbal features extracted from real trial recordings. Future work will address the use of automatic gesture identification and automatic speech transcription, with the goal of taking steps towards a real-time deception detection system.</p><p>The dataset introduced in this paper is available upon request.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Sample screenshots showing facial displays and hand gestures from real-life trial clips. Starting at the top left-hand corner: deceptive trial with forward head movement (Move forward), deceptive trial with both hands movement (Both hands), deceptive trial with one hand movement (Single hand), truthful trial with raised eyebrows (Eyebrows raising), deceptive trial with scowl face (Scowl), and truthful trial with an up gaze (Gaze up).</figDesc><graphic url="image-1.png" coords="3,59.22,53.91,491.13,226.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Distribution of non-verbal features for deceptive and truthful groups</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Weights of top non-verbal features</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Gesture annotation agreement</figDesc><table><row><cell>Gesture Category</cell><cell>Agreement</cell><cell>Kappa</cell></row><row><cell>General Facial Expressions</cell><cell>66.07%</cell><cell>0.328</cell></row><row><cell>Eyebrows</cell><cell>80.03%</cell><cell>0.670</cell></row><row><cell>Eyes</cell><cell>64.28%</cell><cell>0.465</cell></row><row><cell>Gaze</cell><cell>55.35%</cell><cell>0.253</cell></row><row><cell>Mouth Openness</cell><cell>78.57%</cell><cell>0.512</cell></row><row><cell>Mouth Lips</cell><cell>85.71%</cell><cell>0.690</cell></row><row><cell>Head Movements</cell><cell>69.64%</cell><cell>0.569</cell></row><row><cell>Hand Movements</cell><cell>94.64%</cell><cell>0.917</cell></row><row><cell>Hand Trajectory</cell><cell>82.14%</cell><cell>0.738</cell></row><row><cell>Average</cell><cell>75.16%</cell><cell>0.571</cell></row></table><note>2 http://www.innocenceproject.org/</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>The second broad category covers gestures made with the hands, and it includes the Hand Movements and Hand Trajectories listed in Figure2. Deception classifiers using individual and combined sets of verbal and non-verbal features.</figDesc><table><row><cell>Feature Set</cell><cell>DT</cell><cell>RF</cell></row><row><cell>Unigrams</cell><cell cols="2">60.33% 56.19%</cell></row><row><cell>Bigrams</cell><cell cols="2">53.71% 51.20%</cell></row><row><cell>Facial displays</cell><cell cols="2">70.24% 76.03%</cell></row><row><cell>Hand gestures</cell><cell cols="2">61.98% 62.80%</cell></row><row><cell cols="3">Uni+Facial displays 66.94% 57.02%</cell></row><row><cell>All verbal</cell><cell cols="2">60.33% 50.41%</cell></row><row><cell>All non-verbal</cell><cell cols="2">68.59% 73.55%</cell></row><row><cell>All features</cell><cell cols="2">75.20% 50.41%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>An important remaining question is concerned with the human performance on the task of deception detection. An answer to this Feature ablation study.</figDesc><table><row><cell cols="2">Feature Set</cell><cell>DT</cell></row><row><cell>All</cell><cell></cell><cell>75.20%</cell></row><row><cell cols="2">-Hand gestures</cell><cell>71.90%</cell></row><row><cell cols="3">-Facial displays 59.50%</cell></row><row><cell>-Bigrams</cell><cell></cell><cell>66.94%</cell></row><row><cell cols="2">-Unigrams</cell><cell>61.98%</cell></row><row><cell>Modality</cell><cell cols="2">Agreement</cell><cell>Kappa</cell></row><row><cell>Text</cell><cell cols="2">59.80%</cell><cell>0.071</cell></row><row><cell>Audio</cell><cell cols="2">62.00%</cell><cell>0.196</cell></row><row><cell>Silent video</cell><cell cols="2">51.50%</cell><cell>0.014</cell></row><row><cell>Full video</cell><cell cols="2">57.60%</cell><cell>0.127</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Agreement among three human annotators on text, audio, silent video, and full video modalities.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Performance of three annotators (A1, A2, A3) and the developed automatic system (Sys) on the real-deception dataset over four modalities.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">www.uscourts.gov</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1">DOI: http://dx.doi.org/10.1145/2818346.2820758</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">As done in the Human-Computer Interaction community, we use the term "gesture" to broadly refer to body movements, including facial expressions and hand gestures.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">We use the implementation available in the Weka toolkit with the default parameters.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">In the figure, the features are normalized with respect to the largest feature weight.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5">Inter-rater agreement with multiple raters and variables. https: //mlnl.net/jg/software/ira/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We are grateful to Hagar Mohamed, Aparna Garimella, Shibamouli Lahiri, and Charles Welch for assisting us with the data collection and the human evaluations. This material is based in part upon work supported by National Science Foundation awards #1344257 and #1355633, by grant #48503 from the John Templeton Foundation, and by DARPA-BAA-12-47 DEFT grant #12475008. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation, the John Templeton Foundation, or the Defense Advanced Research Projects Agency.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Who can best catch a liar? a meta-analysis of individual differences in detecting deception</title>
		<author>
			<persName><forename type="first">M</forename><surname>Aamodt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Custer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Forensic Examiner</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="6" to="11" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deception detection using a multimodal approach</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abouelenien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pérez-Rosas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Burzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on Multimodal Interaction, ICMI &apos;14</title>
				<meeting>the 16th International Conference on Multimodal Interaction, ICMI &apos;14<address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="58" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The mumin coding scheme for the annotation of feedback, turn management and sequencing phenomena</title>
		<author>
			<persName><forename type="first">J</forename><surname>Allwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cerrato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jokinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Navarretta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Paggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language Resources and Evaluation</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="273" to="287" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Seeing through deception: A computational approach to deceit detection in written communication</title>
		<author>
			<persName><forename type="first">A</forename><surname>Almela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Valencia-García</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cantos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Computational Approaches to Deception Detection</title>
				<meeting>the Workshop on Computational Approaches to Deception Detection<address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012-04">April 2012</date>
			<biblScope unit="page" from="15" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Detecting concealment of intent in transportation screening: A proof of concept</title>
		<author>
			<persName><forename type="first">J</forename><surname>Burgoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Twitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Meservy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Adkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kruse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Deokar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tsechpenakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nunamaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Younger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="103" to="112" />
			<date type="published" when="2009-03">March 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The impact of deception and suspicion on different hand movements</title>
		<author>
			<persName><forename type="first">L</forename><surname>Caso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Maricchiolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bonaiuto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vrij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Nonverbal Behavior</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Are you awerewolf? detecting deceptive roles and outcomes in a conversational role-playing game</title>
		<author>
			<persName><forename type="first">G</forename><surname>Chittaranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE International Conference on Acoustics Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<date type="published" when="2010-03">March 2010</date>
			<biblScope unit="page" from="5334" to="5337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Nonverbal indicators of deception: How iconic gestures reveal thoughts that cannot be suppressed</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Beattie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shovelton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Semiotica</title>
		<imprint>
			<biblScope unit="issue">182</biblScope>
			<biblScope unit="page" from="133" to="174" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cues to deception</title>
		<author>
			<persName><forename type="first">B</forename><surname>Depaulo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Malone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lindsay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Muhlenbruck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Charlton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cooper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="page" from="74" to="118" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Control and resistance in the psychology of lying</title>
		<author>
			<persName><forename type="first">M</forename><surname>Derksen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theory and Psychology</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="196" to="212" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Telling Lies: Clues to Deceit in the Marketplace</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Politics and Marriage. Norton, W.W. and Company</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Darwin, deception, and facial expression</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of the New York Academy of Sciences</title>
		<imprint>
			<biblScope unit="page" from="205" to="221" />
			<date type="published" when="1000">1000. 2003</date>
		</imprint>
	</monogr>
	<note>EMOTIONS INSIDE OUT: 130 Years after Darwin&apos;s The Expression of the Emotions in Man and Animals</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Syntactic stylometry for deception detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Short Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="171" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automatic deception detection in Italian court cases</title>
		<author>
			<persName><forename type="first">T</forename><surname>Fornaciari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Poesio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Law</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="303" to="340" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Gannon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Beech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<title level="m">Risk Assessment and the Polygraph</title>
				<imprint>
			<publisher>John Wiley and Sons Ltd</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="129" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A new theoretical perspective on deception detection: On the psychology of instrumental mind-reading</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Granhag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hartwig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychology, Crime &amp; Law</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="189" to="200" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Exonerations in the united states, 1989 -2012</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Warden</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
		<respStmt>
			<orgName>National Registry of Exonerations</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dating deception: Gender, online dating, and exaggerated self-presentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Guadagno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Okdie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kruse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Hum. Behav</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="642" to="647" />
			<date type="published" when="2012-03">Mar. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The effect of deception on specific hand gestures</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hillman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vrij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><surname>Um</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">they were wearing</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="336" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distinguishing deceptive from non-deceptive speech</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hirschberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Benus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Brenier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Enos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gilman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Graciarena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kathol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Michaelis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech 2005 -Eurospeech</title>
				<meeting>Interspeech 2005 -Eurospeech</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1833" to="1836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Guess who? an empirical study of gender deception and detection in computer-mediated communication</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hollister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the American Society for Information Science and Technology</title>
				<meeting>the American Society for Information Science and Technology</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Automatic, multimodal evaluation of human interaction. Group Decision and Negotiation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Meservy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Burgoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nunamaker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="367" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Explanations for the perpetration of and reactions to deception in a virtual community</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Joinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dietz-Uhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Science Computer Review</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="275" to="289" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Towards a general rule for identifying deceptive opinion spam</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06">June 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Blob analysis of the head and hands: A method for deception detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tsechpenakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kruse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th Annual Hawaii International Conference on System Sciences (HICSS&apos;05), HICSS &apos;05</title>
				<meeting>the 38th Annual Hawaii International Conference on System Sciences (HICSS&apos;05), HICSS &apos;05<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="20" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Coding hand gestures: A reliable taxonomy and a multi-media support</title>
		<author>
			<persName><forename type="first">F</forename><surname>Maricchiolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gnisci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bonaiuto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cognitive Behavioural Systems</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Esposito</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Esposito</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Vinciarelli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hoffmann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Muller</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">7403</biblScope>
			<biblScope unit="page" from="405" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deception detection through automatic, unobtrusive analysis of nonverbal behavior</title>
		<author>
			<persName><forename type="first">T</forename><surname>Meservy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kruse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Twitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tsechpenakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Burgoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nunamaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="36" to="43" />
			<date type="published" when="2005-09">September 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The lie detector: Explorations in the automatic recognition of deceptive language</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Strapparava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
				<meeting>the Association for Computational Linguistics<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Lying words: Predicting deception from linguistic styles</title>
		<author>
			<persName><forename type="first">M</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pennebaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Berry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Richards</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Personality and Social Psychology Bulletin</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Establishing a foundation for automated human credibility screening</title>
		<author>
			<persName><forename type="first">J</forename><surname>Nunamaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Burgoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Twyman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Proudfoot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schuetzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Giboney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE International Conference on Intelligence and Security Informatics (ISI)</title>
				<imprint>
			<date type="published" when="2012-06">June 2012</date>
			<biblScope unit="page" from="202" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Finding deceptive opinion spam by any stretch of the imagination</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hancock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="309" to="319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The design and development of a lie detection system using facial micro-expressions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Owayjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kashour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Alhaddad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fadel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Alsouki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 2nd International Conference on Advances in Computational Tools for Engineering Applications (ACTEA)</title>
				<imprint>
			<date type="published" when="2012-12">Dec 2012</date>
			<biblScope unit="page" from="33" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Human behaviour: Seeing through the face of deception</title>
		<author>
			<persName><forename type="first">I</forename><surname>Pavlidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Eberhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">415</biblScope>
			<biblScope unit="issue">6867</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Linguistic inquiry and word count: LIWC</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pennebaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Francis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Erlbaum Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A multimodal dataset for deception detection</title>
		<author>
			<persName><forename type="first">V</forename><surname>Perez-Rosas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Narvaez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Burzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Language Resources and Evaluations (LREC 2014)</title>
				<meeting>the Conference on Language Resources and Evaluations (LREC 2014)<address><addrLine>Reykjavik, Iceland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-05">May 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Electronic imaging &amp; signal processing automatic identification of facial clues to lies</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SPIE Newsroom</title>
		<imprint>
			<date type="published" when="2012-01">January 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Thermal analyzer enables improved lie detection in criminal-suspect interrogations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sumriddetchkajorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Somboonkaew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPIE Newsroom: Defense &amp; Security</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Facial expression analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Face Recognition</title>
				<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="247" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Reading between the lines: linguistic cues to deception in online dating profiles</title>
		<author>
			<persName><forename type="first">C</forename><surname>Toma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hancock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 ACM conference on Computer supported cooperative work, CSCW &apos;10</title>
				<meeting>the 2010 ACM conference on Computer supported cooperative work, CSCW &apos;10<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="5" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Hmm-based deception recognition from visual cues</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tsechpenakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Adkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kruse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Burgoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Meservy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Twitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Deokar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nunamaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo</title>
				<imprint>
			<date type="published" when="2005-07">2005. 2005. July 2005</date>
			<biblScope unit="page" from="824" to="827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Detecting Lies and Deceit: The Psychology of Lying and the Implications for Professional Practice. Wiley series in the psychology of crime</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vrij</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Warrants and deception in computer mediated communication</title>
		<author>
			<persName><forename type="first">D</forename><surname>Warkentin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Woodworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hancock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cormier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 ACM conference on Computer supported cooperative work</title>
				<meeting>the 2010 ACM conference on Computer supported cooperative work</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="9" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Using deep linguistic features for finding deceptive opinion spam</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2012: Posters</title>
				<meeting>COLING 2012: Posters<address><addrLine>Mumbai, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-12">December 2012</date>
			<biblScope unit="page" from="1341" to="1350" />
		</imprint>
	</monogr>
	<note>The COLING 2012 Organizing Committee</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Automatic detection of deception in child-produced speech using syntactic complexity features</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yancheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Rudzicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-08">August 2013</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="944" to="953" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
