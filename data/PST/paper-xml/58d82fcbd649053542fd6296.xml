<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Brain-inspired Cognitive Model with Attention for Self-Driving Cars</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Student Member, IEEE</roleName><forename type="first">Shitao</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Songyi</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jinghao</forename><surname>Shang</surname></persName>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Badong</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
						</author>
						<title level="a" type="main">Brain-inspired Cognitive Model with Attention for Self-Driving Cars</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">37A2C0586C3CBDD4389631823CF5EB34</idno>
					<idno type="DOI">10.1109/TCDS.2017.2717451</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TCDS.2017.2717451, IEEE Transactions on Cognitive and Developmental Systems IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS 1 This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TCDS.2017.2717451, IEEE Transactions on Cognitive and Developmental Systems</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>autonomous mental development</term>
					<term>cognitive robotics</term>
					<term>end-to-end learning</term>
					<term>path planning</term>
					<term>vehicle driving</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The perception-driven approach and end-to-end system are two major vision-based frameworks for self-driving cars. However, it is difficult to introduce attention and historical information into the autonomous driving process, which are essential for achieving human-like driving in these two methods. In this study, we propose a novel model for self-driving cars called the brain-inspired cognitive model with attention (CMA). This model comprises three parts: a convolutional neural network for simulating the human visual cortex, a cognitive map to describe the relationships between objects in a complex traffic scene, and a recurrent neural network, which we combine with the real-time updated cognitive map to implement the attention mechanism and long-short term memory. An advantage of our model is that it can accurately solve three tasks simultaneously: i) detecting the free space and boundaries for the current and adjacent lanes, ii) estimating the distances to obstacles and vehicle attitude, and iii) learning the driving behavior and decision-making process of a human driver. Importantly, the proposed model can accept external navigation instructions during an end-to-end driving process. To evaluate the model, we built a large-scale road-vehicle dataset containing over 40,000 labeled road images captured by three cameras placed on our self-driving car. Moreover, human driving activities and vehicle states were recorded at the same time.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>A UTOMATIC scene understanding is a core technological requirement for self-driving cars as well as being a primary aim of computer vision research. During recent decades, research into vision-based self-driving cars has achieved considerable progress and development. It is well known that most of the information required for self-driving cars can be obtained by cameras, where this process is inspired by the driving behavior of humans. In addition, the mechanism of attention can help to choose effective data from memory to identify the objects that are presented in the current image and their relationships in order to make correct decisions at the right time. Therefore, it is important to develop a self-driving car based only on vision <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref> where the mechanism of attention should be implemented in a realistic manner.</p><p>At present, two main vision-based paradigms are popular for self-driving cars: the perception-driven method and the end-to-end method. In the perception-driven method <ref type="bibr" target="#b3">[4]</ref>, it S. Chen, S. Zhang, J. Shang, B. Chen, and N. Zheng are with the Department of Electronic and Information Engineering, Xi'an Jiaotong University, Xi'an, Shaanxi 710049, P.R. China; E-mail: chenshitao, zhangsongyi, sjh19950419@stu.xjtu.edu.cn; chenbd, nnzheng@mail.xjtu.edu.cn.</p><p>* Correspondence: nnzheng@mail.xjtu.edu.cn is necessary to establish a detailed representation of the real world. After fusing multi-sensor data, a typical world representation usually contains descriptions of the main objects in various traffic scenes, including lane boundaries, free space, pedestrians, cars, traffic lights, and traffic signs. According to these descriptions, a path planning module and control module are used to determine the actual movements of the vehicle.</p><p>During the path planning process, in addition to accurate perception results and high-precision map information, it is often necessary to design some extra rules manually. The motion trajectory needs to be adjusted and updated in realtime according to the state of the vehicle during each moment by considering temporal dependences in order to formulate a correct trajectory sequence. Using the motion trajectory calculated by the planning module, the vehicle can be steered to track each task point on the planned path under the guidance of high-precision positioning information.</p><p>In the end-to-end method <ref type="bibr" target="#b4">[5]</ref>, based on convolutional neural networks (CNN) <ref type="bibr" target="#b5">[6]</ref> and GPU technology, a deep neural network can learn the entire processing pipeline required to control a vehicle directly from human driving behavior. Instead of using the manually designed features employed in the perception-driven method, the CNN can learn the most valuable image features automatically and map them directly to facilitate the control of the steering angle. The actual control of the car is related only to the velocity and steering angle, so this method for directly mapping images to control the direction is more efficient and effective in some scenarios.</p><p>The perception-based method has been used most widely in recent decades. It can be applied to most challenging tasks, but its disadvantages are that all the features and task plans must be designed manually, and the entire system lacks the capacity for self-learning. In recent years, the end-to-end learning strategy for self-driving has gradually emerged due to the success of deep learning <ref type="bibr" target="#b6">[7]</ref>. The end-to-end strategy only requires some visual information, and it is capable of learning from human driving behavior. However, its disadvantage is that when the system structure is simple, external information cannot be introduced to control the behavior of the self-driving system. Therefore, while the system is running, we have no way of knowing where the vehicle is going and we cannot control the system. In addition, temporal information is never considered in the end-to-end process.</p><p>We consider that a highly effective and reasonable autonomous system should be inspired by the cognitive process of the human brain. First, it needs to be capable of perceiving the environment in a rational manner similar to the visual cortex, before processing the perception results appropriately. Next, the system plays the role of the motor cortex in order to plan and control driving behavior. During the whole process, the concept of human-computer collaborative hybridaugmented intelligence <ref type="bibr" target="#b7">[8]</ref> is considered so the self-driving system can learn in a smart manner from human driving behavior.</p><p>In this study, we propose a brain-inspired cognitive model with attention (CMA). When a person views a scene, the message flows through the LGN to V1, onward to V2, and then to V4 and IT <ref type="bibr" target="#b9">[9]</ref>, which occurs within the first 100 ms of glancing at an object. It has been shown that this process is very similar to the operating principle of the CNN. Thus, in our model, we employ CNNs to process the visual information in order to simulate the processing of information by the visual cortex. Similarly, Hawkins <ref type="bibr" target="#b10">[10]</ref> and Hu et al. <ref type="bibr" target="#b11">[11]</ref> argued that time plays a vital role in memory forming and problem solving for the brain. We consider that the brain must deal with spatial and temporal information simultaneously because the spatial patterns need to coincide with the temporal patterns. Therefore, it is necessary to simulate the functions of the motor cortex, so when dealing with planning and control problems, a long-term memory must be employed to formulate the optimal driving strategy at the current time. Motivated by this requirement, it is necessary to introduce an attention mechanism into the cognitive computing model for self-driving cars, thereby allowing the model to choose reasonable information from a large set of long-term memory data during each computing step.</p><p>Moreover, Mountcastle et al. <ref type="bibr" target="#b12">[12]</ref> noted that the functional areas in the cerebral cortex have similarities and consistency.</p><p>They stated that the regions of the cortex responsible for controlling muscles are similar to the regions that handle auditory or optical inputs in terms of their structure and function. Thus, we argue that a recurrent neural network (RNN), which performs well at processing sequential data where it has been applied successfully to video sequence classification and natural language processing tasks, is also capable of solving planning and control problems simultaneously in the same manner as the human motor cortex. The background given above is an important motivation for us to implement planning and control decision with a RNN.</p><p>In order to introduce attention mechanism into the proposed cognitive model, and to solve the problem that general endto-end models cannot introduce external information to guide, we define the concept of cognitive map in real traffic. The term of cognitive map was first coined by Edward Tolman <ref type="bibr" target="#b13">[13]</ref> as a type of mental representation of the layout of one's physical environment. Thereafter, this concept was widely used in the fields of neuroscience <ref type="bibr" target="#b14">[14]</ref> and psychology. The research <ref type="bibr" target="#b15">[15]</ref> proposed a computational model that simulates the hippocampal place cells and entorhinal grid cells in brain to help indoor robots with positioning and navigation. It inspired us greatly to construct a new model of autonomous driving.</p><p>To apply this concept to the field of self-driving, cognitive map for traffic scene is built in this work. In our opinion, intelligence represents a model of characterization and facilitates a better ultimate cognition. A type of cognitive "pattern' arises in the mind can be thought as a world model constructed with prior knowledge. This model contains three types of relationships: interaction, causality, and control. The world model can be considered a cognitive map of the human brain, which resembles an image of the environment. It is a comprehensive representation of the local environment, including not only simple sequences of events but also directions, distance, and even time information. In our study, from the perspective of information processing theory <ref type="bibr" target="#b16">[16]</ref>, a cognitive map (or cognitive mapping) is a dynamic process with steps of data acquisition, encoding, storage, processing, decoding and using external information. Our purpose is not only to absolutely simulate the cell structures in brain or to realize the explanation of cognitive map in neuroscience. We aim to construct a novel framework for self-driving cars that is more human-like and intelligent than ever before.</p><p>The definition of a cognitive map in traffic scene is proposed in our work. It comprises vehicle states and essentials of traffic scenarios, so as to form time sequences with all of this information. A typical cognitive map for traffic scene covers effective objects, such as lane boundaries, free space, pedestrians, automobiles, and traffic lights, along with the relationships among these objects, including direction and distance. Moreover, some related prior knowledge is considered, e.g., traffic rules and temporal information. Hence, it is a comprehensive representation of the local traffic scene. In this paper, we use the cognitive map as the description of the traffic scene and vehicle states, while memory for longer time period is formed by it.</p><p>In particular, our framework first extracts valid information from the traffic scene in each moment by using a CNN to formulate the cognitive map, which comprises temporal information and a long-term memory. We then add external control information to some descriptions of the cognitive map, e.g., guidance information from the navigation map. Finally, we utilize a RNN to model an attention mechanism based on historical states and scene data over time in order to perform path planning and control the vehicle.</p><p>In our model, a novel self-driving framework is combined with an attention mechanism inspired by the human brain. Thus, the framework is referred to as brain-inspired cognitive model with attention. This framework can handle spatiotemporal relationships to implement basic self-driving missions. In this study, we propose a self-driving system that only uses vision sensors, which perform well at path planning and at producing control commands for vehicles with an attention mechanism. Figure <ref type="figure" target="#fig_0">1</ref> shows the main scheme of our CMA method. The remainder of this paper is organized as follows.</p><p>In Section II, we review some previous studies of self-driving cars. In Section III, we describe our approach in detail. In Sections IV and V, we present our large-scale labeled selfdriving dataset and an evaluation of the proposed method. Finally, we give our conclusions in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In recent decades, remarkable progress <ref type="bibr" target="#b17">[17]</ref>, <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b3">[4]</ref> has been made with the perception-driven method in the field of self-driving cars. Several methods have been proposed for detecting car and lane boundaries in order to build a description of the local environment.</p><p>Many lane detection methods in <ref type="bibr" target="#b18">[18]</ref>, <ref type="bibr" target="#b19">[19]</ref>, <ref type="bibr" target="#b20">[20]</ref>, <ref type="bibr" target="#b21">[21]</ref> have been developed to locate the lane position with Canny edge detection or Hough transformation. However, these methods lack suitable geometric constraints for locating an arbitrary lane boundary. Therefore, Nan et al. <ref type="bibr" target="#b22">[22]</ref> presented a spatiotemporal knowledge model for fitting the line segments, which finally outputs the lane boundaries. Huval et al. <ref type="bibr" target="#b23">[23]</ref> introduced a deep learning model for lane detection at a high frame rate. In contrast to the traditional lane boundary detection approach where the output is the pixel location of the lane boundary, the method proposed by <ref type="bibr" target="#b17">[17]</ref> employs a novel approach that uses a CNN to map an input image directly to the deviation between the vehicle and lane boundary. In this method, the output of the neural network can be used directly for controlling the vehicle without coordinate transformation. However, the limitation of this model is that training for a specific vehicle is needed.</p><p>Previous studies <ref type="bibr" target="#b24">[24]</ref>, <ref type="bibr" target="#b25">[25]</ref> of the object detection task employed a method that generates a bounding box to describe the location of the object. However, in the self-driving task, it is not necessary to determine the precise location of the bounding box because we only need to know whether there is an obstacle in the current lane and the distance to the obstacle. This is a more convenient and efficient way of representing an obstacle as a point instead of a bounding box.</p><p>The concept of the end-to-end learning method was originally inspired by Pomerleau et al. <ref type="bibr" target="#b26">[26]</ref> and it was developed further by <ref type="bibr" target="#b27">[27]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b28">[28]</ref>. Pomerleau et al. <ref type="bibr" target="#b26">[26]</ref> attempted to use a neural network to navigate an autonomous land vehicle. Breakthroughs in deep learning mean that the DAVE-2 system proposed by <ref type="bibr" target="#b4">[5]</ref> can learn a criterion to steer a vehicle automatically. Similarly, Xu et al. presented a FCN-LSTM architecture <ref type="bibr" target="#b28">[28]</ref>, which can predict the ego-motion of a vehicle based on its previous state. However, all these methods lack the capacity to supervise the action of the vehicle, and thus we have no way of knowing where the vehicle is going, although the vehicle may drive safely on the road.</p><p>Several control strategies have been proposed that use the deep learning approach to control robots. A vision-based reinforcement learning method and evolving neural network as a controller in a TORCS game were reported by <ref type="bibr" target="#b29">[29]</ref>. The reinforcement learning approach proposed by <ref type="bibr" target="#b30">[30]</ref>, <ref type="bibr" target="#b31">[31]</ref>, <ref type="bibr" target="#b32">[32]</ref> was used successfully to train an artificial agent with the capacity to play several games. The combination of CNN and reinforcement learning exhibits good performance in some strategic games <ref type="bibr" target="#b33">[33]</ref> but this is because the decision-making process in these games usually relies on short-term time information or information about the current image. However, in complex tasks such as self-driving cars, the planning and control decisions must be based on long-term information in order to formulate the optimal driving strategy for the current real traffic scene. In <ref type="bibr" target="#b17">[17]</ref>, a direct perception approach was used to manipulate a virtual car in a TORCS game, where the controller was a manually designed linear function that directly employs the vehicle's position and pose. This approach may perform well in a game environment but the action generated by this function is different from human behavior and it cannot be applied in real traffic. The path planning and control methods reviewed for self-driving cars by <ref type="bibr" target="#b34">[34]</ref> usually require real-time GPS information to formulate a real trajectory. However, we know that a human can drive a car based only on visual information, so it would be useful to develop a model that can handle planning and control simultaneously based only on vision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. BRAIN-INSPIRED COGNITIVE MODEL WITH ATTENTION</head><p>The human visual cortex and motor cortex play the leading roles while driving. The visual cortex contributes to perception of the environment and the formation of a cognitive map of the road scene by combining memories of traffic with external information, such as map navigation information. Planning and control are determined by the motor cortex. Using information obtained from the cognitive map in longterm memory, the mechanism of attention helps people to identify the most significant information over time in order to formulate a planning and control strategy. Thus, the overall driving behavior comprising sensing, planning, and control is guided and determined mainly by the visual cortex and motor cortex in the brain.</p><p>Similarly, a brain-inspired model can be constructed based on human perception, memory, and attention mechanisms. In this study, we assume that the primary perception depends on a single frame of the road scene. However, multiple frames and many historical states of the vehicle are required to formulate the long-or short-term memory for the planning and control processes to actually manipulate a self-driving car.</p><p>Fig. <ref type="figure" target="#fig_0">1</ref> shows our CMA method for self-driving cars. Our ultimate goal with this network is to build a cognitive model with attention mechanism, which can handle sensing, planning and control at the same time. In contrast to other deep learning or end-to-end learning methods that only map input images to uncontrollable driving decisions, our network can make a car run on a road as well as accepting external control inputs to guide the actions of the car. To achieve this goal, the road images are first processed by multiple CNNs to simulate the function of the human visual cortex and to form a basic cognitive map similar to that produced by the human brain, which is a structured description of the road scene. This description of a road contains humandefined features and latent variables learned by the network. A more explicit cognitive map can be constructed based on the contents of the basic cognitive map and then combined with prior knowledge of traffic, vehicle states, and external traffic guidance information. Therefore, the cognitive map is built based on the description of the road scene, the current vehicle states, and the driving strategy in the near future. Using a RNN, the cognitive map formed of each frame can be modeled to determine the temporal dependency of motion control, as well as the long-and short-term memory of past motion states, thereby imitating the human motor cortex. Finally, real motion sequences can be generated by considering the planning and control commands for self-driving cars.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Perception Simulating Human Visual Cortex</head><p>The aim of the CMA framework is to solve the problem where conventional end-to-end learning methods cannot incorporate external control signals, so the vehicle can only produce an action based on the input image and it does not know where it will go. By constructing a cognitive map, additional control information can be added to the end-toend self-driving framework. The establishment of a cognitive map depends mainly on perception of the environment. It is well known that perception of the environment is the main focus and challenge in self-driving missions. Thus, in the CMA framework, inspired by the architecture of the human visual cortex, we use a state-of-art CNN to learn and generate basic descriptions of road scenes. We fix three cameras in our self-driving car in order to capture the scene from the current lane and the lanes on both sides. In contrast to the conventional method, the scene representations in our approach are learned by convolutional neural network but not hand-crafted. We employ several convolutional layers to process images captured by cameras on board the vehicle. In our approach, we use multiple CNNs to extract different types of road information from various camera views. Instead of using the network directly as a classifier, we utilize it as a regressor to directly map an input image onto several key pixel points, which are used for path planning and control. Using the pixel points extracted by multiple CNNs, we can calculate and construct a basic cognitive map to describe the local environment surrounding the vehicle, as shown in Fig. <ref type="figure" target="#fig_1">2</ref>.</p><p>We define the input images as I t = {I t m , I t l , I t r }, which are captured by the middle, left, and right cameras, respectively. Based on the input images, a self-driving car needs to know the accurate geometry of the lane and the positions of the obstacles. Thus, the output vector X t of the CNN for each camera is composed with five different point identities, that is</p><formula xml:id="formula_0">X t = [p l t , p l b , p r t , p r b , p o ],<label>(1)</label></formula><p>where p l t and p l b represent the x-coordinates for the intersections of a line extended from the left lane boundary with the top and the bottom edges of the image plane, respectively, p r t , p r b denote the x-coordinates corresponding to the points on the right lane, and the p o is the y-coordinate of the obstacle point in the corresponding lane. The architecture of our CNN is very simple and shallow in order to achieve high performance in real time. Five convolutional layers are utilized to extract the spatial features from each image I t . The configurations of the five convolutional layers are the same as those defined by <ref type="bibr" target="#b4">[5]</ref>. The first three layers are stride convolutional layers with a 2 × 2 stride and a 5 × 5 kernel. The last two convolutional layers have a 3 × 3 kernel and no stride. We use the network to locate feature points rather than classify them, so the pooling layer that makes the representation invariant to small translations of the input is unnecessary in our network. The convolution operation without a pooling layer is expressed as</p><formula xml:id="formula_1">Z i,j,k = l,m,n [V l,(j-1)×s+m,(k-1)×s+n K i,l,m,n ],<label>(2)</label></formula><p>where Z and V are the output feature maps with i channels and input feature maps with l channels, respectively, s denotes the number of strides, and j and k are the row and column indexes, respectively. A rectified linear unit (ReLU) is used for each hidden neuron in the convolutional layers. Following the five convolutional layers, three fully connected layers are employed to map the representations extracted by the convolutional layers to the output vector X t . According to the five descriptors in X t , we can calculate the physical quantities shown in Fig. <ref type="figure" target="#fig_1">2</ref>. We assume that D m lef t and D m right are the lateral distances to the vehicle from the left and right lane boundaries, respectively, in the view of the middle camera. D l lef t , D l right , D r lef t , and D r right are the two distances in the views of the left and right camera, respectively, in a similar manner to the middle camera. We define the angle between the vehicle and road as V a , and the distances to obstacles in each lane are O C line , O L line , and O R line . By using the distances to obstacles, the driving intention D t i can be derived using Algorithm 1. In order to calculate the physical distances described above, we define any two pixel points in a lane boundary as (l xm , l ym ), and (l xb , l yb ), and the y-coordinate of the obstacle is o y . According to the optical center (u 0 , v 0 ) and the height of the camera H, the positions of two points (X m , Z m ), (X b , Z b ) in the vehicle coordinate system can be represented as</p><formula xml:id="formula_2">Right Camera View Left Camera View Middle Camera View O C _ line O L _ line O R _ line 3 a D r _ right D m _ right</formula><formula xml:id="formula_3">(X m , Z m ) = ( (l xm -v 0 ) • H (l ym -u 0 ) , f • H l ym -u 0 ),<label>(3)</label></formula><formula xml:id="formula_4">(X b , Z b ) = ( (l xb -v 0 ) • H (l yb -u 0 ) , f • H l yb -u 0 ).<label>(4)</label></formula><p>The distance D from the vehicle to the lane boundary can be obtained by</p><formula xml:id="formula_5">D = |X m Z b -X b Z m | (X b -X m ) 2 + (Z b -Z m ) 2 . (<label>5</label></formula><formula xml:id="formula_6">)</formula><p>The angle between the vehicle and the lane boundary V a is</p><formula xml:id="formula_7">V a = arctan X b -X m Z b -Z m .<label>(6)</label></formula><p>Similarly, the obstacle distance in each lane is represented as</p><formula xml:id="formula_8">O = f • H o y -u 0 .<label>(7)</label></formula><p>By using Eq. 3, 4, 5, 6 and 7, we can obtain the perception results {X t m , X t l , X t r } from the views of the three cameras.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Planning and Control to Simulate the Human Motor Cortex</head><p>The structured description of cognitive map C t with the vehicle states V States formulated as</p><formula xml:id="formula_9">C t = X t m X t l X t r D t i V states ,<label>(8)</label></formula><p>is extracted from each frame obtained by the three cameras.</p><p>Based on these representations, the CMA method models the temporal dynamic dependency of planning and control. In a standard self-driving framework, path planning and vehicle control are two separate tasks. However, in contrast to the traditional method, our new approach employs the memories of past states and generates control commands with the RNN, so the two tasks are completed simultaneously.</p><p>Long short-term memory (LSTM) is a basic unit of the RNN, which is employed for processing sequential data and modeling temporal dependencies. Many varieties of LSTMs are possible and a simple type is used in our CMA framework. One cell in a LSTM unit is controlled by three gates (input gate, output gate, and forget gate). The forget gate and input gate use a sigmoid function, and the output and cell state are transformed by tanh. Using these gates, a LSTM network can learn the long-term dependencies in sequential data and model the attention mechanism over time. We employ the main characteristics of LSTM in our CMA framework, so it can learn human behavior during a long-term driving process. The memory cell is used to store information over time, such as the historical vehicle states in its vectors, which can be chosen by the attention mechanism in the network. The dimensions of the hidden states should be selected according to the input representation C t .</p><p>Based on the driving intention D i in cognitive map C t , a lateral distance D o from the current point to the objective lane is calculated by Algorithm 2. A more complex cognitive map C t is then built as</p><formula xml:id="formula_10">C t = X t m , X t l , X t r , D t i , V t states , D t o .<label>(9)</label></formula><p>Before importing the representation of cognitive map C t into the LSTM block, a fully connected layer is utilized to organize the various types of information in C t . The representation R t can be denoted by the fully connected layer with a weight matrix W C f and a bias vector b C f as</p><formula xml:id="formula_11">R t = W c f C t + b c f .<label>(10)</label></formula><p>The descriptor R t contains large amounts of latent information organized by the fully connected layer, which comprises a description of traffic scene and driving intentions. The effectiveness of this descriptor can be improved by an appropriate training process based on human driving behavior.</p><p>In the proposed CMA framework, we use three LSTM layers to allow the network to learn higher-level temporal representations. The first two LSTM blocks return their full output sequences, but the last only returns the last step in its output sequence, thereby mapping the long input sequence to a single control vector for vehicles. The mapping Φ R contains three LSTM layers with parameters Θ R to capture temporal clues in the representation set</p><formula xml:id="formula_12">{R t }, t = 1, 2, ..., n,<label>(11)</label></formula><p>which contains multiple cognitive results in different time steps. The hidden state h t 3 is the tth output of the third layer and it represents the result obtained by a temporal model that processes the path planning and control tasks. The hidden state h t 3 is represented as</p><formula xml:id="formula_13">h t 3 = Φ R (Θ R , {h t }, {R t }),<label>(12)</label></formula><p>where {h t }, t = 1, 2, ..., n, is the set of hidden states for each LSTM layer. Next, a fully connected layer defined by weights W R and bias b R will map the hidden state h t 3 to a driving decision used to control self-driving cars. Thus, by using the memory and predictive abilities in an LSTM block, we consider the planning and controlling process as a regression problem.</p><p>Steering and velocity commands are used to control vehicles in the automatic driving mode. In our self-driving framework, we generate these two commands separately. The velocity command is determined by traffic knowledge and traffic rules. The real velocity of the vehicle is treated as a part of the vehicle state V t states in order to form a cognitive map of the current time step. According to the long-term cognitive map {C t }, the steering angle S a is generated by</p><formula xml:id="formula_14">S a = W R • Φ R (Θ R , {h t }, {W c f C t + b c f }) + b R ,<label>(13)</label></formula><p>which is described in detail above. We assume that D m lef t and D m right are the lateral distances from the left and right lane boundaries to the vehicle in the view of the middle camera, respectively. D l lef t , D l right , D r lef t , and D r right are the two distances in the left and right camera views, respectively, in the same manner as the middle camera. We define the angle between the vehicle and road as V a . We use V states to represent the vehicle states, which can be obtained through the OBD port in the vehicle.</p><p>Using these notations, the entire workflow of the CMA framework is summarized in Algorithm 2. In the self-driving procedure, a cognitive map is first constructed using multiple convolutional neural networks. Subsequently, based on the cognitive map and vehicle states over a period of time, the final control command will be generated by the recurrent neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DATA COLLECTION AND DATASET</head><p>In order to develop a new framework for self-driving cars and to evaluate the proposed method, we prepared a novel dataset called the Road-Vehicle Dataset (RVD) for training and testing our model. <ref type="foot" target="#foot_0">1</ref> The platform used for data collection is shown in Fig. <ref type="figure" target="#fig_5">3a</ref>. The data were collected on a wide variety of roads under different lighting and weather conditions. In total, we recorded more than 10 hours of traffic scenarios using different sensors, such as color cameras, a high-precision inertial navigation system, and a differential GPS system. Three cameras were used to record images and driver behavior, which were reflected through the steering angle and pedal states, and they were recorded by the CAN bus on the OBD interface. A GPS/IMU inertial navigation system was used to recorded accurate attitude and positional data for our vehicle. In order to generate images from different viewpoints, we used the viewpoint transformation method to augment our dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Sensors and Calibration</head><p>The sensor setup is shown in Fig. <ref type="figure" target="#fig_5">3b</ref>, Fig. <ref type="figure" target="#fig_5">3c</ref>, and Fig. <ref type="figure" target="#fig_5">3d</ref>: if D i = stay in line then TERRASTAR-C accuracy 4cm; • 1 × OXTS RT2000 inertial and GNSS navigation system, 6 axis, 100Hz. For the three vehicle-mounted cameras, we used the trilinear method <ref type="bibr" target="#b35">[35]</ref> to calibrate the extrinsic parameters and the method proposed by Zhang et.al <ref type="bibr" target="#b36">[36]</ref> to calibrate the intrinsic parameters. The position matrix is denoted as T , the pitch angle matrix as R, and the internal parameters matrix as I. These parameters were used in the experiment.</p><formula xml:id="formula_15">• 3 × IDS UI-5240CP color</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. RVD Dataset</head><p>In the data acquisition process, drivers were asked to drive in diverse weather and road conditions at different times of the day. Furthermore, in order to collect large amounts of driving behavior data, we required the drivers to perform lane changing and turning operations in suitable cases. A key problem in the self-driving mission is to make the vehicle recover from error states. Therefore, by using the viewpoint transformation method, we combined the data from the three cameras to simulate the visual data for the error-state vehicle and generated additional road images from a variety of viewpoints.</p><p>In particular, our dataset comprised the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Diverse Visual data:</head><p>Visual data including the types of roads, such as urban roadways and single-lane, doublelane, and multi-lane highways. The data were collected under different weather conditions, such as day, night, sunny, cloudy, and foggy, using three changeable viewpoint cameras, which extended our dataset to 146,980 images.</p><p>2) Vehicle states data: We recorded the real-time vehicle states while collecting the road video, which included more than 100 types of internal and external vehicle information, such as speed, attitude, and acceleration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Driver behavior data:</head><p>We collected real-time behavior data (operation to the vehicle) of the drivers at each moment, including the steering angle, and control to the accelerator pedal and brake pedal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Artificially tagged data:</head><p>In our collected video data, we manually tagged 43,621 pieces of road image data where the lane position, obstacle location, etc. were marked.</p><p>Our dataset is innovative in following two aspects. i) The dataset contains most of the visual data in self-driving scenes, where all of the data were collected simultaneously from three viewpoints and the dataset was expanded later by viewpoint transformation. ii) The dataset contains abundant records of vehicle states and human driving behavior, thereby enhancing the testing and training processes for exploring end-to-end frameworks for self-driving cars.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head><p>In this section, we present the results of experiments that we performed to demonstrate the effectiveness of the proposed CMA model. Comprehensive experiments were performed to evaluate the cognitive maps (free space, lane boundaries, obstacles, etc.) produced by CNNs based on the data from real traffic scene videos. We also evaluated the path planning and vehicle control performance of the RNN, which integrates the cognitive map with the LSTM. In addition, a simulation environment was constructed because some experiments were difficult to conduct in reality. All of the experiments were based on the Challenger self-driving car, as shown in Fig. <ref type="figure" target="#fig_5">3</ref>, which is a tuning vehicle based on the Chery Tiggo.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Constructing Cognitive Maps with Multiple Convolutional Neural Networks 1) Visual Data and Visual Data Augmentation:</head><p>Training a CNN to generate descriptions of road scenarios requires a large amount of image data. We had many images captured by the vehicle-mounted camera, but it was still difficult to encompass all of the possible situations that a self-driving vehicle might encounter. Only recording images from the driver's viewpoint is inadequate and our method should have the capacity to adjust the vehicle to recover from an error state. For example, the samples captured from a human-driven vehicle cannot cover the situation where the yaw angle is very large because a human driver does not allow a car to deviate greatly from the middle of the road. Thus, the collected data could have been unbalanced, thereby making it difficult to train a network that can handle various road situations. In order to augment our training data and simulate a variety of attitudes of a vehicle driving in a lane, we used a simple viewpoint transformation method to generate images with rotations that differed from the direction of the lane. We recorded images from three different viewpoints using three cameras mounted on the vehicle and we then simulated other viewpoints by transforming the images captured by the nearest camera. Viewpoint transformation requires the precise depth of each pixel, which we could not acquire. However, in this study, we only concerned about the lanes on the road. Thus, we assumed that every point on the road was on a horizontal plane so as to accurately calculate the depth of each pixel on the road area based on the height of the camera.</p><p>If we assume that a point p(x, y) in the image coordinate system is known and the pitch angle of the camera is approximately 0, then using the basic camera model illustrated in Fig. </p><formula xml:id="formula_16">Z = Y × f y ,<label>(14)</label></formula><formula xml:id="formula_17">X = Z × x f ,<label>(15)</label></formula><p>where Y equals the height h of the camera and the focal length of the camera is f . A point P ′ (X ′ , Y ′ , Z ′ ) in the simulated camera coordinate system can then be derived as</p><formula xml:id="formula_18">  X ′ Y ′ Z ′   = R ×   X Y Z   + T,<label>(16)</label></formula><p>where R is the rotation matrix and T is the translation vector. Therefore, the augmented samples were generated in this manner, as shown in Fig. <ref type="figure" target="#fig_7">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Effects of Multiple CNNs in Constructing Cognitive</head><p>Map: Within the proposed CMA framework, the CNN regressor is responsible for the construction of a basic cognitive map. In a vision-based self-driving car, the precision of the surrounding environment perceived by the visual module is of particular importance because the perception results directly affect planning and control. In most self-driving scenarios, the main indicators comprise the detection of free space, the current and adjacent lanes, and obstacle vehicles. Therefore, we mainly evaluated our model by detecting these items.</p><p>Our CNN regressor was built using TensorFlow <ref type="bibr" target="#b37">[37]</ref>. As shown in Table <ref type="table" target="#tab_2">I</ref>, there were 5 convolutional layers in our model. An image at a resolution of 320 × 240 was processed by these convolutional layers to form a tensor with dimensions of 25 × 33 × 128. And 4 fully connected layers with output sizes of 500, 100, 20, and 5 were used to map 128 features to a 5-dimensional vector, which represented the locations of the lane boundaries and obstacles in the input image. We also considered other configurations for the convolutional layer, such as VGG16 and AlexNet. Based on several comparative experiments, we finally chose the Nvidia settings because the performance of this configuration was more stable than the   others. And it was easier to be trained with the current network setting. Besides, our next step is to implement our method in an embedded system at a high frame rate. So we used shallow layers instead of other complex networks.</p><p>The free space estimation performance was evaluated using a segmentation-based approach. In our approach, we could estimate the free space in the ego-lane or adjacent lanes. The free space in a lane is determined by the two lane boundaries and the positions of obstacles in the lane. We evaluated the consistency between the model output and the accurately labeled ground truth. In <ref type="bibr" target="#b38">[38]</ref>, the authors recommended estimating the free space from a bird's eye view regardless of the type of traffic scenario, but we performed the evaluation based on perspective image pixels in our model. Since we were concerned with the free space in a lane, so it was more convenient for us to employ this perspective. We used the precision, recall, and F-measure as criteria to evaluate the performance of our model, which are defined by:</p><formula xml:id="formula_19">P recision = N T P N T P + N F P , (<label>17</label></formula><formula xml:id="formula_20">)</formula><formula xml:id="formula_21">Recall = N T P N T P + N F N ,<label>(18)</label></formula><formula xml:id="formula_22">F 1 = 2 P recision -1 + Recall -1 = 2N T P 2N T P + N F P + N F N ,<label>(19)</label></formula><p>where N T P is the number of free space pixels labeled correctly as ground truth, N F P is the number of free space pixels in the model output but not in the ground truth labeling, and N F N is the number of free space pixels in the ground truth but not in the model output. In our study, the three lanes (ego-lane and two adjacent lanes) were captured by three different cameras.</p><p>We only present the evaluation results for the images obtained by the middle camera and the left camera because the images obtained by the right camera were similar to those obtained by the left camera.</p><p>To facilitate comparisons, the free space detection performance was evaluated based on the RVD and Nan's dataset <ref type="bibr" target="#b22">[22]</ref>. Table <ref type="table" target="#tab_2">II</ref> shows the quantitative analysis of our model in different scenarios. Some free space detection results based on the testing set are shown in Fig. <ref type="figure">6</ref>. The lane boundary detection results were evaluated according to the criteria presented in <ref type="bibr" target="#b22">[22]</ref>. If the horizontal Fig. <ref type="figure">6</ref>. Free space detection results. The proposed model was evaluated based on RVD and Nan's dataset <ref type="bibr" target="#b22">[22]</ref>, which included various traffic scenarios such as night, rainy day, and complex illumination.</p><p>distance between the detected boundary and ground truth labeling was smaller than a predefined threshold, the detected boundary was regard as a true positive. We compared our approach with the state-of-art method. The two methods were evaluated using RVD and Nan's dataset <ref type="bibr" target="#b22">[22]</ref>. The experimental results showed that the precision of our approach was fairly consistent with that of the state-of-art approach in typical traffic scenes. However, for challenging scenarios, such as rainy, snowy, and hazy days, our model performed better, thereby indicating its relatively high robustness. In addition, the utilization of CNN allowed our model to be processed in parallel on a GPU, which yielded a higher frame rate comparing with the state-of-art method.</p><p>The quantitative lane boundary detection results obtained by our model are presented in Table <ref type="table" target="#tab_4">III</ref>. The lane boundary detection results obtained by our model with different scenes and datasets are presented in Fig. <ref type="figure" target="#fig_8">7</ref>.</p><p>The estimated obstacle position obtained using our model was evaluated based on the perspective image pixels because the real distance between an obstacle vehicle and our car was related to the extrinsic parameters of the camera, which could   1) Experiment Setup: As shown in Fig. <ref type="figure" target="#fig_3">9</ref>, the simulation environment mainly comprised the road conditions, vehicle model, and internal controller (including the driver model). The simulator was used to evaluate the performance of our method and to augment the driver behavior data. In our simulator, the car model interacted with the road conditions and their states were sent to the internal controller as inputs to simulate human driving behavior in order to generate control sequences for adjusting the attitude of the car. These three modules constituted a complete closed-loop simulation cycle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Generating Control Command Sequences with Recurrent Neural Network</head><formula xml:id="formula_23">                                           U</formula><p>2) Effects of Control Sequence Generation: Training a RNN to learn human driving behavior requires a large amount of driving behavior data. However, many unexpected factors may influence real-world driving data. For example, human driving behavior is based partly on subjective decisions, which may be completely different even under the same conditions. The presence of these issues in our training data may reduce the confidence in the output of our network. Therefore, in order to evaluate the planning and control part, we used the data from our dataset, which contained marked road data and the actions that the driver actually performed, as well as the data obtained from the simulation environment, as shown in Fig. <ref type="figure" target="#fig_3">9</ref>. After setting appropriate parameters for both the vehicle and the road module, reliable driving data could be generated from the simulation environment, such as the driving trajectory and commands for steering the wheel of the car. We set multiple parameters for the road module and ran the simulator repeatedly to obtain information about the vehicle states (vehicle attitude, speed, etc.) as well as the deviation between the vehicle driving trace and the lane so as to improve and extend our driving behavior dataset. As shown in Table <ref type="table" target="#tab_5">IV</ref>, we built an LSTM network to model the temporal dependencies during the driving process. Path planning and vehicle control were accomplished simultaneously in the LSTM network. In order to quantitative analyze the path planning and control efficiency of our method, we evaluated the proposed method using the simulator instead of implementing it in a real vehicle. In the simulator, we set two lanes on the road and the speed of the vehicle was set (but not limited) to 40 Km/h. In addition, two obstacles were placed in the lane to test whether our model could achieve humanlevel control in the lane changing scenario. As shown in Fig. <ref type="figure" target="#fig_3">9</ref>, the proposed method replaced the internal controller (driver model) in the simulator. The steering angle required to control the car module in real time could be generated by the proposed model based on the car states and environmental data.</p><p>Figure <ref type="figure" target="#fig_10">10</ref> shows the driving trajectories generated by our method and Chen's method <ref type="bibr" target="#b17">[17]</ref> for different lane-changing scenarios. In the testing procedure, we randomly set obstructions in front of the vehicle to test its performance during lane-changing operation when faced with obstacles at different distances. The gray dotted line in Fig. <ref type="figure" target="#fig_10">10</ref> shows the trajectory of the vehicle produced using Chen's model. Chen's method worked well at correcting the lateral distance, but compared with normal human behavior, the process was excessively fast, thus it is not suitable for a real environment. As shown in Fig. <ref type="figure" target="#fig_10">10</ref>, the blue line is a trajectory curve produced by our model. It is obvious that our model has the characteristics of smoothness, and the trajectory curve is closer to the performance of the vehicle driven by a human. Our approach considered the temporal dependence, thereby implying that the states of the vehicle were memorized over a period of time. Therefore, the trajectory of our model was consistent with the ideal curve and the vehicle could drives in a steady and smooth state. The overall process was similar to operation by human drivers where the driver behavior was learnt completely by the model.</p><note type="other">Ideal Track Proposed Model Chen's Method</note><p>VI. DISCUSSION AND CONCLUSION In this paper, we proposed a cognitive model with attention for self-driving cars. The proposed model is inspired by the human brain, and it can simulate the roles of the human visual and motor cortices during sensing, planning, and control. The mechanism of attention is modeled by an recurrent neural network over time. In addition, we introduced the concept of a cognitive map of a traffic scene and described it in detail. A labeled dataset called the RVD was constructed for training and evaluation purposes. We tested the planning and control performance of the proposed model in three visual tasks. The experimental results showed that our model can achieve some basic self-driving tasks with only cameras.</p><p>However, the performance of the proposed model depends greatly on the volume of training data, hence the performance may degrade in some unknown environments. To eliminate this limitation, we will collect more traffic data and make further attempts at feasible data augmentation operations. When directly generating the control commands with the LSTM network, the proposed model can learn human driving behavior in lane-changing, free-driving, and vehicle-following scenarios. However, the proposed model needs to be further developed for other complex driving scenarios.</p><p>In addition to the attention mechanism, the permanent memory plays a crucial role in human cognition. In our future research, we will consider how to incorporate permanent memory into the proposed cognitive model. Many abnormal events also occur in actual traffic scenes so the development of an efficient cognitive model to handle these situations could be an interesting topic for future study.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. Framework of our cognitive model with attention (CMA). The road images are first processed by multiple convolutional neural networks to simulate the function of the human visual cortex. A cognitive map is built comprising the vehicle states, navigation inputs, and perception results. Based on the information in the cognitive map, a recurrent neural network models the attention mechanism using historical states and scene data over time to make the driving decision in each computing step.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Illustration of constructing a basic cognitive map. The convolutional neural network will output the pixel location of lane boundary and obstacle vehicles. And based on the output, a description of local environment can be generated.</figDesc><graphic coords="5,199.13,161.92,63.48,50.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1 4 : 5 :D t i = change to left 6 :</head><label>1456</label><figDesc>Generating Driving Intentions with a Basic Cognitive Map in Real Traffic Input: G navi : guidance information from the navigation signal Output: D t i : driving intention based on obstacle distances and navigation signal; 1: if G navi = stay in line ∧ O C line ≥ safe distance then 2: return D i = stay in line 3: else if G navi = stay in line ∧ O C line ≤ safe distance then if O L line ≥ safe distance then return else if O R line ≥ safe distance then 7: return D t i = change to right 8:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>else 9 :</head><label>9</label><figDesc>return D t i = break and stay in line 10: end if 11: else if G navi = change to left ∧ O L line ≥ safe distance then 12: return D t i = change to left 13: else if G navi = change to right ∧ O R line ≥ safe distance then 14: return D t i = change to right 15: else 16: return D t i = break and stay in lane 17: end if</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>pedal commands based on the desired speed 7 :• 3 ×</head><label>73</label><figDesc>else if D i = change to right then 8: d = D r right -D r lef t 9: while changing lanes do 10: if D m right &lt; D r right then 11: the car is still in the current lane 12: D o = D r right -d 2 13: generate steering angle based on the RNN 14: generate pedal commands based on desired speed 15: else if D m right == D r right ∨ the car is on the boundary of the lane then 16: if (D m right -D m light ) ≫ d then 17: the car is on the boundary of the lane 18: D o = D m rightelse if D i = change to lef t then 29: KOWA LM3NCM megapixel lens, 3.5mm, horizontal angle of view 89.00 • , vertical angle of view 73.8 • ; • 1 × NovAtel ProPak6 Triple-Frequency GNSS Receiver,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Challenger self-driving car. (a) Challenger is based on a CHERY TIGGO. The TIGGO has four-wheel drive system (4WD), and a 4-speed auto transmission. (b) Computing system and GPS/IMU inertial navigation system in the vehicle's trunk. (c) Three cameras on the top of the vehicle. Each camera monitors a corresponding lane. (d) A mechanism with a DC motor installed on the steering is used to control the steering electronically.</figDesc><graphic coords="7,312.00,276.96,125.50,94.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Principle of the viewpoint transformation process.</figDesc><graphic coords="8,311.83,55.98,252.00,168.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Data augmentation process. Images were generated from different viewpoints using the viewpoint transformation method based on the actual images captured by the three cameras.</figDesc><graphic coords="9,433.24,90.84,71.21,56.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Lane boundary detection results on different datasets.</figDesc><graphic coords="10,312.00,549.91,251.00,91.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .Fig. 9 .</head><label>89</label><figDesc>Fig. 8. Box plot of the estimated error based on the distance from obstacles. The error distribution for the estimated obstacle distance is shown for different traffic scenarios.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Simulation of the planning and control process with the CMA model. Simulated vehicle trajectories based on diverse lanes. In 10a to 10f, the obstacles were placed at 50, 80, 100, 200, 100, and 100 meters from the self-driving car, respectively, and they occurred at random times to test the planning and control capacity of the models. The lanes were 3.5 meters in width in 10a -10d, and 4 meters in width in 10e -10f</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I PARAMETERS</head><label>I</label><figDesc>FOR THE FIVE CONVOLUTIONAL LAYERS</figDesc><table><row><cell>Layers</cell><cell>Operations</cell><cell>Attributions</cell></row><row><cell></cell><cell>Convolution</cell><cell>Size: [5 × 5 × 3 × 24]</cell></row><row><cell>1st</cell><cell>Activation</cell><cell>ReLU</cell></row><row><cell></cell><cell>Max pooling</cell><cell>(Not Used)</cell></row><row><cell></cell><cell>Convolution</cell><cell>Size: [5 × 5 × 24 × 36]</cell></row><row><cell>2nd</cell><cell>Activation</cell><cell>ReLU</cell></row><row><cell></cell><cell>Max pooling</cell><cell>(Not Used)</cell></row><row><cell></cell><cell>Convolution</cell><cell>Size: [5 × 5 × 36 × 48]</cell></row><row><cell>3rd</cell><cell>Activation</cell><cell>ReLU</cell></row><row><cell></cell><cell>Max pooling</cell><cell>(Not Used)</cell></row><row><cell></cell><cell>Convolution</cell><cell>Size: [5 × 5 × 48 × 64]</cell></row><row><cell>4th</cell><cell>Activation</cell><cell>ReLU</cell></row><row><cell></cell><cell>Max pooling</cell><cell>(Not Used)</cell></row><row><cell></cell><cell>Convolution</cell><cell>Size: [5 × 5 × 64 × 128]</cell></row><row><cell>5th</cell><cell>Activation</cell><cell>ReLU</cell></row><row><cell></cell><cell>Max pooling</cell><cell>(Not Used)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III COMPARISON</head><label>III</label><figDesc>WITH OTHER LANE BOUNDARY DETECTION METHODS FOR DIFFERENT TRAFFIC SCENES</figDesc><table><row><cell>Traffic Scenes</cell><cell>Methods</cell><cell cols="2">P recision[%] Frame Rate</cell></row><row><cell>Highway</cell><cell>Ours Nan's</cell><cell>99.9 99.9</cell><cell>93 28</cell></row><row><cell>Moderate Urban</cell><cell>Ours Nan's</cell><cell>96.2 97.7</cell><cell>90 21</cell></row><row><cell>Heavy Urban</cell><cell>Ours Nan's</cell><cell>96.1 95.4</cell><cell>90 16</cell></row><row><cell>Illumination</cell><cell>Ours Nan's</cell><cell>95.8 100.0</cell><cell>89 23</cell></row><row><cell>Night</cell><cell>Ours Nan's</cell><cell>90.7 99.4</cell><cell>94 35</cell></row><row><cell>Rainy &amp; Snowy Day</cell><cell>Ours Nan's</cell><cell>87.1 47.3</cell><cell>89 21</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV PARAMETERS</head><label>IV</label><figDesc>FOR RNN WITH THE LSTM BLOCK</figDesc><table><row><cell>Layers</cell><cell>Operations</cell><cell>Attributions</cell></row><row><cell>Dense</cell><cell>Input Output</cell><cell>Size: 20 × C t Size: [20 × 16]</cell></row><row><cell>LSTM</cell><cell>Input Output</cell><cell>Size: [20 × 16] Size: [20 × 64]</cell></row><row><cell>LSTM</cell><cell>Input Output</cell><cell>Size: [20 × 64] Size: [20 × 64]</cell></row><row><cell>LSTM</cell><cell>Input Output</cell><cell>Size: [20 × 64] Size: [64]</cell></row><row><cell>Dense</cell><cell>Input Output</cell><cell>Size: [64] Steering angle Sa</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The Road-Vehicle Dataset is available at iair.xjtu.edu.cn/xszy/RVD.htm</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ACKNOWLEDGMENT This research was partially supported by the National Natural Science Foundation of China (No. 61627811, L1522023), the Programme of Introducing Talents of Discipline to University (No. B13043), 973 Program (No. 2015CB351703).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Badong Chen (M'10, SM <ref type="bibr">'13)</ref>  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A visioncentered multi-sensor fusing approach to self-localization and obstacle perception for robotic cars</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers of Information Technology &amp; Electronic Engineering</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="122" to="138" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Vision-based vehicles in japan: Machine vision systems and driving control systems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tsugawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Electronics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="398" to="405" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Vits-a vision system for autonomous land vehicle navigation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Turk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Morgenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Gremban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="342" to="361" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A perceptiondriven autonomous urban vehicle</title>
		<author>
			<persName><forename type="first">J</forename><surname>Leonard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>How</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Teller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fiore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fletcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frazzoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Field Robotics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="727" to="774" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">End to end learning for self-driving cars</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bojarski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Del</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Testa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dworakowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Firner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Flepp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jackel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Monfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.07316</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Z.-Y</forename><surname>N.-N. Zheng</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hybrid-augmented intelligence: collaboration and cognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>-J. Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>-Q. Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>-T. Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>-Y. Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>-R. Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F.-Y</forename><surname>-D. Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers of Information Technology &amp; Electronic Engineering</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="153" to="179" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">On intelligence</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Blakeslee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Macmillan</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">How the brain formulates memory: A spatio-temporal model</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computational Intelligence Magazine</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="56" to="68" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">An organizing principle for cerebral function: the unit model and the distributed system</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mountcastle</surname></persName>
		</author>
		<editor>The Mindful Brain, G. Edelman and V. Mountcastle</editor>
		<imprint>
			<date type="published" when="1978">1978</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, Mass</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Cognitive maps in rats and men</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Tolman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1948">1948</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Path integration and the neural basis of the&apos;cognitive map</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Mcnaughton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">I</forename><surname>Moser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-B</forename><surname>Moser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Neuroscience</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="663" to="678" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An entorhinalhippocampal model for simultaneous cognitive map building</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">A</forename><surname>Shim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Ninth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nadel</surname></persName>
		</author>
		<title level="m">The hippocampus as a cognitive map</title>
		<meeting><address><addrLine>Oxford</addrLine></address></meeting>
		<imprint>
			<publisher>Clarendon Press</publisher>
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deepdriving: Learning affordance for direct perception in autonomous driving</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kornhauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2722" to="2730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Lane detection and tracking using b-snake</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">K</forename><surname>Teoh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision computing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="269" to="280" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Real time lane detection for autonomous vehicles</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Assidiq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">O</forename><surname>Khalifa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCCE 2008. International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="page" from="82" to="88" />
		</imprint>
		<respStmt>
			<orgName>Computer and Communication Engineering</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multiple lane boundary detection using a combination of low-level image features</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Gans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Transportation Systems (ITSC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="1682" to="1687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A lane detection method for lane departure warning system</title>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Optoelectronics and Image Processing (ICOIP), 2010 International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="28" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Efficient lane boundary detection with spatial-temporal knowledge filtering</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">1276</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">An empirical evaluation of deep learning on highway driving</title>
		<author>
			<persName><forename type="first">B</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kiske</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pazhayampallil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Migimatsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cheng-Yue</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.01716</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Region-based convolutional networks for accurate object detection and segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="142" to="158" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Alvinn, an autonomous land vehicle in a neural network</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Pomerleau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University, Computer Science Department, Tech. Rep.</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Off-road obstacle avoidance through end-to-end learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cosatto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Flepp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="739" to="746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">End-to-end learning of driving models from large-scale video datasets</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.01079</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Evolving large-scale neural networks for vision-based reinforcement learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Koutník</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cuccu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th annual conference on Genetic and evolutionary computation</title>
		<meeting>the 15th annual conference on Genetic and evolutionary computation</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1061" to="1068" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT press Cambridge</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Mastering the game of go with deep neural networks and tree search</title>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7587</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Playing atari with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5602</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A survey of motion planning and control techniques for self-driving urban vehicles</title>
		<author>
			<persName><forename type="first">B</forename><surname>Paden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Čáp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yershov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frazzoli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Vehicles</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="33" to="55" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Calibration of external parameters of vehicle-mounted camera with trilinear method</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>-N. Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Opto-electronic Engineering</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">23</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A flexible new technique for camera calibration</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1330" to="1334" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for largescale machine learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI)</title>
		<meeting>the 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI)<address><addrLine>Savannah, Georgia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A new performance measure and evaluation benchmark for road detection algorithms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fritsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kuhnl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Transportation Systems-(ITSC), 2013 16th International IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1693" to="1700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">He is currently a Ph.D. candidate in the Institute of Artificial Intelligence and Robotics, Xi&apos;an Jiaotong University. His research interests mainly include image analysis, pattern recognition, computer vision and deep learning. Jinghao Shang received the B.E. degree in electrical engineering from Xi&apos;an Jiaotong University</title>
		<imprint>
			<date type="published" when="2014">2014. 2017. 2016</date>
			<pubPlace>Xi&apos;an, China; Xi&apos;an</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Xi&apos;an Jiaotong University ; Institute of Artificial Intelligence and Robotics, Xi&apos;an Jiaotong University</orgName>
		</respStmt>
	</monogr>
	<note>His research interests include computer vision, pattern recognition, deep learning and image processing, and hardware implementation of intelligent systems. Songyi Zhang received the B.E. degree in Electronic and Information Engineering from Xi&apos;an Jiaotong University. He is currently pursuing a M.E. degree in the. His main research interests include computer vision, deep learning and machine learning</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
