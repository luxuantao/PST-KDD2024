<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Delayed Impact of Fair Machine Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lydia</forename><forename type="middle">T</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sarah</forename><surname>Dean</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Esther</forename><surname>Rolf</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Max</forename><surname>Simchowitz</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
						</author>
						<title level="a" type="main">Delayed Impact of Fair Machine Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fairness in machine learning has predominantly been studied in static classification settings without concern for how decisions change the underlying population over time. Conventional wisdom suggests that fairness criteria promote the longterm well-being of those groups they aim to protect. We study how static fairness criteria interact with temporal indicators of well-being, such as long-term improvement, stagnation, and decline in a variable of interest. We demonstrate that even in a one-step feedback model, common fairness criteria in general do not promote improvement over time, and may in fact cause harm in cases where an unconstrained objective would not. We completely characterize the delayed impact of three standard criteria, contrasting the regimes in which these exhibit qualitatively different behavior. In addition, we find that a natural form of measurement error broadens the regime in which fairness criteria perform favorably. Our results highlight the importance of measurement and temporal modeling in the evaluation of fairness criteria, suggesting a range of new challenges and trade-offs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Machine learning commonly considers static objectives defined on a snapshot of the population at one instant in time; consequential decisions, in contrast, reshape the population over time. Lending practices, for example, can shift the distribution of debt and wealth in the population. Job advertisements allocate opportunity. School admissions shape the level of education in a community.</p><p>Existing scholarship on fairness in automated decisionmaking criticizes unconstrained machine learning for its potential to harm historically underrepresented or disad-Proceedings of the 35 th International Conference on Machine <ref type="bibr">Learning, Stockholm, Sweden, PMLR 80, 2018.</ref> Copyright 2018 by the author(s). vantaged groups in the population (Executive Office of the President, 2016; <ref type="bibr" target="#b0">Barocas &amp; Selbst, 2016)</ref>. Consequently, a variety of fairness criteria have been proposed as constraints on standard learning objectives. Even though, in each case, these constraints are clearly intended to protect the disadvantaged group by an appeal to intuition, a rigorous argument to that effect is often lacking.</p><p>In this work, we formally examine under what circumstances fairness criteria do indeed promote the long-term well-being of disadvantaged groups measured in terms of a temporal variable of interest. Going beyond the standard classification setting, we introduce a one-step feedback model of decision-making that exposes how decisions change the underlying population over time.</p><p>Our running example is a hypothetical lending scenario. There are two groups in the population with features described by a summary statistic, such as a credit score, whose distribution differs between the two groups. The bank can choose thresholds for each group at which loans are offered. While group-dependent thresholds may face legal challenges <ref type="bibr" target="#b16">(Ross &amp; Yinger, 2006)</ref>, they are generally inevitable for some of the criteria we examine. The impact of a lending decision has multiple facets. A default event not only diminishes profit for the bank, it also worsens the financial situation of the borrower as reflected in a subsequent decline in credit score. A successful lending outcome leads to profit for the bank and also to an increase in credit score for the borrower.</p><p>When thinking of one of the two groups as disadvantaged, it makes sense to ask what lending policies (choices of thresholds) lead to an expected improvement in the score distribution within that group. An unconstrained bank would maximize profit, choosing thresholds that meet a break-even point above which it is profitable to give out loans. One frequently proposed fairness criterion, sometimes called demographic parity, requires the bank to lend to both groups at an equal rate. Subject to this requirement the bank would continue to maximize profit to the extent possible. Another criterion, originally called equality of opportunity, equalizes the true positive rates between the two groups, thus requiring the bank to lend in both groups at an equal rate among individuals who repay their loan. Other criteria are natural, but for clarity we restrict our attention to these three. Do these fairness criteria benefit the disadvantaged group? When do they show a clear advantage over unconstrained classification? Under what circumstances does profit maximization work in the interest of the individual? These are important questions that we begin to address in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Contributions</head><p>We introduce a one-step feedback model that allows us to quantify the long-term impact of classification on different groups in the population. We represent each of the two groups A and B by a score distribution π A and π B , respectively. The support of these distributions is a finite set X comprising the possible values that the score can assume. We think of the score as highlighting one variable of interest in a specific domain such that higher score values correspond to a higher probability of a positive outcome. An institution chooses selection policies τ A , τ B : X → [0, 1] that assign to each value in X a number representing the rate of selection for that value. In our example, these policies specify the lending rate at a given credit score within a given group. The institution will always maximize their utility (see ( <ref type="formula" target="#formula_0">1</ref>)) subject to either (a) no constraint, or (b) equality of selection rates, or (c) equality of true positive rates.</p><p>We assume the availability of a function ∆ : X → R that provides the expected change in score for a selected individual at a given score. The central quantity we study is the expected difference ∆µ j in the mean score in group j ∈ {A, B} that results from the selection policy. When modeling the problem, the expected mean difference can also absorb external factors such as "reversion to the mean" so long as they are mean-preserving. Qualitatively, we distinguish between long-term improvement (∆µ j &gt; 0), stagnation (∆µ j = 0), and decline (∆µ j &lt; 0).</p><p>Our findings can be summarized as follows.</p><p>1. Both fairness criteria (equal selection rates, equal true positive rates) can lead to all possible outcomes (improvement, stagnation, and decline) in natural parameter regimes. We provide a complete characterization of when each criterion leads to each outcome in section 3.</p><p>• There are a class of settings where equal selection rates cause decline, whereas equal true positive rates do not (Theorem 3.5),</p><p>• Under a mild assumption, the institution's optimal unconstrained selection policy can never lead to decline (Proposition 3.1).</p><p>2. We introduce the notion of an outcome curve (Figure <ref type="figure">1</ref>) which succinctly describes the different regimes in which one criterion is preferable over the others.</p><p>3. We perform experiments on FICO credit score data from 2003 and show that under various models of bank utility and score change, the outcomes of applying fairness criteria are in line with our theoretical predictions.</p><p>4. We discuss how certain types of measurement error (e.g., the bank underestimating the repayment ability of the disadvantaged group) affect our comparison. We find that measurement error narrows the regime in which fairness criteria cause decline, suggesting that measurement should be a factor when motivating these criteria.</p><p>5. We consider alternatives to hard fairness constraints.</p><p>• We evaluate the optimization problem where fairness criterion is a regularization term in the objective. Qualitatively, this leads to the same findings.</p><p>• We discuss the possibility of optimizing for group score improvement ∆µ j directly subject to institution utility constraints. The resulting solution provides an interesting possible alternative to existing fairness criteria.</p><p>We focus on the impact of a selection policy over a single epoch. The motivation is that the designer of a system usually has an understanding of the time horizon after which the system is evaluated and possibly redesigned. Formally, nothing prevents us from repeatedly applying our model and tracing changes over multiple epochs. In reality, however, it is plausible that over greater time periods, economic background variables might dominate the effect of selection.</p><p>Reflecting on our findings, we argue that careful temporal modeling is necessary in order to accurately evaluate the impact of different fairness criteria on the population. Moreover, an understanding of measurement error is important in assessing the advantages of fairness criteria relative to unconstrained selection. Finally, the nuances of our characterization underline how intuition may be a poor guide in judging the long-term impact of fairness constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Related work</head><p>Recent work by <ref type="bibr" target="#b9">Hu &amp; Chen (2018)</ref> considers a model for long-term outcomes in the labor market. They propose imposing the demographic parity constraint in a temporary labor market in order to provably achieve an equitable longterm equilibrium in the permanent labor market, reminiscent of economic arguments for affirmative action (e.g. <ref type="bibr" target="#b6">Foster &amp; Vohra, 1992;</ref><ref type="bibr" target="#b3">Coate &amp; Loury, 1993)</ref>. Our general framework is complementary to this type of domain specific approach. Demographic parity and related formulations have been considered in numerous papers (e.g. <ref type="bibr" target="#b1">Calders et al., 2009;</ref><ref type="bibr" target="#b18">Zafar et al., 2017)</ref>. <ref type="bibr" target="#b8">Hardt et al. (2016)</ref> introduced the equality of opportunity constraint and demonstrate limitations of a broad class of criteria. <ref type="bibr" target="#b13">Kleinberg et al. (2017)</ref> and Chouldechova (2016) point out the tension between "calibration by group" and equal true/false positive rates. These trade-offs carry over to some extent to the case where we only equalize true positive rates <ref type="bibr" target="#b15">(Pleiss et al., 2017)</ref>.</p><p>A growing literature on fairness in the "bandits" setting of learning (see <ref type="bibr">Joseph et al., 2016, et seq.)</ref> deals with online decision making that ought not to be confused with our onestep feedback setting. Finally, there has been much work in the social sciences on analyzing the effect of affirmative action (see e.g., <ref type="bibr" target="#b12">Keith et al., 1985;</ref><ref type="bibr" target="#b11">Kalev et al., 2006)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Problem Setting</head><p>We consider two groups A and B, which comprise a g A and g B = 1 − g A fraction of the total population, and an institution which makes a binary decision for each individual in each group, called selection. Individuals in each group are assigned scores in X := [C], and the scores for group j ∈ {A, B} are distributed according π j ∈ Simplex C−1 . The institution selects a policy τ := (τ A , τ B ) ∈ [0, 1] 2C , where τ j (x) corresponds to the probability the institution selects an individual in group j with score x. One should think of a score as an abstract quantity which summarizes how well an individual is suited to being selected; an example is provided at the end of this section.</p><p>We assume that the institution is utility-maximizing, but may impose certain constraints to ensure that the policy τ is fair, in a sense described in Section 2.2. We assume that there exists a function u : X → R, such that the institution's expected utility for a policy τ is given by</p><formula xml:id="formula_0">U(τ ) = j∈{A,B} g j x∈X τ j (x)π j (x)u(x).<label>(1)</label></formula><p>Novel to this work, we focus on the effect of the selection policy τ on the groups A and B. We quantify these outcomes in terms of an average effect that a policy τ j has on group j. Formally, for a function ∆(x) : X → R, we define the average change of the mean score µ j for group j</p><formula xml:id="formula_1">∆µ j (τ ) := x∈X π j (x)τ j (x)∆(x) .<label>(2)</label></formula><p>We remark that many of our results also go through if ∆µ j (τ ) simply refers to an abstract change in well-being, not necessarily a change in the mean score. Lastly, we assume that the success of an individual is independent of their group given the score; that is, the score summarizes all relevant information about the success event, so there exists a function ρ : X → [0, 1] such that individuals of score x succeed with probability ρ(x).</p><p>We introduce the specific domain of credit scores as a running example in the rest of the paper. Other examples showing the broad applicability of our model can be found in Appendix A.</p><p>Example 2.1 (Credit scores). In the setting of loans, scores</p><p>x ∈ [C] represent credit scores, and the bank serves as the institution. The bank chooses to grant or refuse loans to individuals according to a policy τ . Both bank and personal utilities are given as functions of loan repayment, and therefore depend on the success probabilities ρ(x), representing the probability that any individual with credit score x can repay a loan within a fixed time frame. The expected utility to the bank is given by the expected return from a loan, which can be modeled as an affine function of ρ(x):</p><formula xml:id="formula_2">u(x) = u + ρ(x) + u − (1 − ρ(x))</formula><p>, where u + denotes the profit when loans are repaid and u − the loss when they are defaulted on. Individual outcomes of being granted a loan are based on whether or not an individual repays the loan, and a simple model for ∆(x) may also be affine in ρ(x):</p><formula xml:id="formula_3">∆(x) = c + ρ(x) + c − (1 − ρ(x))</formula><p>, modified accordingly at boundary states. The constant c + &gt; 0 denotes the gain in credit score if loans are repaid and c − &lt; 0 is the score penalty in case of default.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">The Outcome Curve</head><p>We now introduce important outcome regimes, stated in terms of the change in average group score. A policy (τ A , τ B ) is said to cause active harm to group j if ∆µ j (τ j ) &lt; 0, stagnation if ∆µ j (τ j ) = 0, and improvement if ∆µ j (τ j ) &gt; 0. We denote the policy that maximizes the institution's utility in the absence of constraints as MaxUtil.</p><p>Under our model, MaxUtil policies can be chosen in a standard fashion which applies the same threshold τ MaxUtil for both groups, and is agnostic to the distributions π A and π B .</p><p>Hence, if we define</p><formula xml:id="formula_4">∆µ MaxUtil j := ∆µ j (τ MaxUtil ) (3)</formula><p>we say that a policy causes relative harm to group j if ∆µ j (τ j ) &lt; ∆µ MaxUtil j , and relative improvement if ∆µ j (τ j ) &gt; ∆µ MaxUtil j . In particular, we focus on these outcomes for a disadvantaged group, and consider whether imposing a fairness constraint improves their outcomes relative to the MaxUtil strategy. From this point forward, we take A to be the disadvantaged or protected group. characterization is possible when considering decision rules based on (possibly randomized) score thresholding, in which all individuals with scores above a threshold are selected. In Appendix B, we justify the restriction to such threshold policies by showing it preserves optimality. In Appendix B.1, we show that the outcome curve is concave, thus implying that it takes the shape depicted in Figure <ref type="figure">1</ref>. To explicitly connect selection rates to decision policies, we define the rate function r π (τ j ) which returns the proportion of group j selected by the policy. We show that this function is invertible for a suitable class of threshold policies, and in fact the outcome curve is precisely the graph of the map from selection rate to outcome β → ∆µ A (r −1 π A (β)). Next, we define the values of β that mark boundaries of the outcome regions.</p><p>Definition 2.1 (Selection rates of interest). Given the protected group A, the following selection rates are of interest in distinguishing between qualitatively different classes of outcomes (Figure <ref type="figure">1</ref>). We define β MaxUtil as the selection rate for A under MaxUtil; β 0 as the harm threshold, such that ∆µ A (r −1 π A (β 0 )) = 0; β * as the selection rate such that ∆µ A is maximized;</p><formula xml:id="formula_5">β as the outcome-complement of the MaxUtil selec- tion rate, ∆µ A r −1 π A (β)) = ∆µ A (r −1 π A (β MaxUtil )) with β &gt; β MaxUtil .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Decision Rules and Fairness Criteria</head><p>We will consider policies that maximize the institution's total expected utility, potentially subject to a constraint: τ ∈ C ∈ [0, 1] 2C which enforces some notion of "fairness". Formally, the institution selects τ * ∈ argmax U(τ ) s.t. τ ∈ C. We consider the three following constraints: Definition 2.2 (Fairness criteria). The maximum utility (MaxUtil) policy corresponds to the null-constraint C = [0, 1] 2C , so that the institution is free to focus solely on utility.</p><p>The demographic parity (DemParity) policy results in equal selection rates between both groups. Formally, the constraint is</p><formula xml:id="formula_6">C = (τ A , τ B ) : x∈X π A (x)τ A = x∈X π B (x)τ B .</formula><p>The equal opportunity (EqOpt) policy results in equal true positive rates (TPR) between both group, where TPR is defined as TPR j (τ ) := x∈X π j (x)ρ(x)τ (x) x∈X π j (x)ρ(x) . EqOpt ensures that the conditional probability of selection given that the individual will be successful is independent of the population, formally enforced by the constraint</p><formula xml:id="formula_7">C = {(τ A , τ B ) : TPR A (τ A ) = TPR B (τ B )} .</formula><p>Just as the expected outcome ∆µ can be expressed in terms of selection rate for threshold policies, so can the total utility U. In the unconstrained case, U varies independently over the selection rates for group A and B; however, in the presence of fairness constraints the selection rate for one group determines the allowable selection rate for the other. The selection rates must be equal for DemParity, but for EqOpt we can define a transfer function, G (A→B) , which for every loan rate β in group A gives the loan rate in group B that has the same true positive rate. Therefore, when considering threshold policies, decision rules amount to maximizing functions of single parameters. This idea is expressed in Figure <ref type="figure" target="#fig_2">2</ref>, and underpins the results to follow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Results</head><p>In order to clearly characterize the outcome of applying fairness constraints, we make the following assumption. Assumption 1 (Institution utilities). The institution's individual utility function is more stringent than the expected score changes, u(x) &gt; 0 =⇒ ∆(x) &gt; 0. (For the linear form presented in Example 2.1, u− u+ &lt; c− c+ is necessary and sufficient.)</p><p>This simplifying assumption quantifies the intuitive notion that institutions take a greater risk by accepting than the individual does by applying. For example, in the credit setting, a bank loses the amount loaned in the case of a default, but makes only interest in case of a payback. Using Assumption 1, we can restrict the position of MaxUtil on the outcome curve in the following sense. Proposition 3.1 (MaxUtil does not cause active harm). Under Assumption 1, 0 ≤ ∆µ MaxUtil ≤ ∆µ * .</p><p>We direct the reader to Appendix F for the proof of the above proposition, and all subsequent theorems presented in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Prospects and Pitfalls of Fairness Criteria</head><p>We begin by characterizing general settings under which fairness criteria act to improve outcomes over unconstrained MaxUtil strategies. For this result, we will assume that group A is disadvantaged in the sense that the MaxUtil acceptance rate for B is large compared to relevant acceptance rates for A. </p><formula xml:id="formula_8">&lt; β &lt; β &lt; β such that β MaxUtil B &gt; G (A→B) (β), G (A→B) (β ), there exist population proportions g 2 &lt; g 3 &lt; 1 such that, for all g A ∈ [g 2 , g 3 ], β MaxUtil A &lt; β EqOpt A &lt; β. That is, EqOpt causes relative improvement.</formula><p>This result gives the conditions under which we can guarantee the existence of settings in which fairness criteria cause improvement relative to MaxUtil. Relying on machinery proved in the appendix, the result follows from comparing the position of optima on the utility curve to the outcome curve. Figure <ref type="figure" target="#fig_2">2</ref>  the contributions of each group are weighted by their population proportions g j , and thus the resulting selection rates are sensitive to these proportions.</p><p>As we see in the remainder of this section, fairness criteria can achieve nearly any position along the outcome curve under the right conditions. This fact comes from the potential mismatch between the outcomes, controlled by ∆, and the institution's utility u.</p><p>The next theorem implies that DemParity can be bad for long term well-being of the protected group by being over-generous, under the mild assumption that ∆µ A (β MaxUtil B ) &lt; 0: Theorem 3.3 (DemParity can cause harm by being over-eager). Fix a selection rate β.</p><p>Assume that</p><formula xml:id="formula_9">β MaxUtil B &gt; β &gt; β MaxUtil A .</formula><p>Then, there exists a population proportion g 0 such that, for all g A ∈ [0, g 0 ], β DemParity A &gt; β.</p><p>In particular, when β = β 0 , DemParity causes active harm, and when β = β, DemParity causes relative harm.</p><p>The assumption ∆µ A (β MaxUtil B ) &lt; 0 implies that a policy which selects individuals from group A at the selection rate that MaxUtil would have used for group B necessarily lowers average score in A. This is one natural notion of protected group A's 'disadvantage' relative to group B. In this case, DemParity penalizes the scores of group A even more than a naive MaxUtil policy, as long as group proportion g A is small enough. Again, small g A is another notion of group disadvantage.</p><p>Using credit scores as an example, Theorem 3.3 tells us that an overly aggressive fairness criterion will give too many loans to people in a protected group who cannot pay them back, hurting the group's credit scores on average. In the following theorem, we show that an analogous result holds for EqOpt.</p><p>Theorem 3.4 (EqOpt can cause harm by being over-eager).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Suppose that β</head><formula xml:id="formula_10">MaxUtil B &gt; G (A→B) (β) and β &gt; β MaxUtil A .</formula><p>Then, there exists a population proportion g 0 such that, for all g A ∈ [0, g 0 ], β EqOpt A &gt; β. In particular, when β = β 0 , EqOpt causes active harm, and when β = β, EqOpt causes relative harm.</p><p>We remark that in Theorem 3.4, we rely on the transfer function, G (A→B) , which for every loan rate β in group A gives the loan rate in group B that has the same true positive rate. Notice that if G (A→B) were the identity function, Theorems 3.3 and Theorem 3.4 would be exactly the same. Indeed, our framework (detailed in Appendix E) unifies the analyses for a large class of fairness constraints that includes DemParity and EqOpt as specific cases, and allows us to derive results about impact on ∆µ using general techniques. In the next section, we present further results that compare the fairness criteria, demonstrating the usefulness of our technical framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Comparing EqOpt and DemParity</head><p>Our analysis of the acceptance rates of EqOpt and DemParity in Appendix C suggests that it is difficult to compare DemParity and EqOpt without knowing the full distributions π A , π B , which is necessary to compute the transfer function G (A→B) . In fact, we have found that settings exist both in which DemParity causes harm while EqOpt causes improvement and in which DemParity causes improvement while EqOpt causes harm. There cannot be one general rule as to which fairness criteria provides better outcomes in all settings. We now present simple sufficient conditions on the geometry of the distributions for which EqOpt is always better than DemParity in terms of ∆µ A .</p><p>Theorem 3.5 (EqOpt may avoid active harm where DemParity fails). Fix a selection rate β. Suppose π A , π B are identical up to a translation with µ A &lt; µ B , i.e. π A (x) = π B (x + (µ B − µ A )). For simplicity, take ρ(x) to be linear in x. Suppose</p><formula xml:id="formula_11">β 0 &gt; x&gt;µ A π A .</formula><p>Then there exists an interval [g 1 , g 2 ] ⊆ [0, 1], such that ∀g A &gt; g 1 , β EqOpt &lt; β while ∀g A &lt; g 2 , β DemParity &gt; β.</p><p>In particular, when β = β 0 , this implies DemParity causes active harm but EqOpt causes improvement for g A ∈ [g 1 , g 2 ], but for any g A such that DemParity causes improvement, EqOpt also causes improvement.</p><p>To interpret the conditions under which Corollary 3.5 holds, consider when we might have β 0 &gt; x&gt;µ A π A . This is precisely when ∆µ A ( x&gt;µ A π A ) &gt; 0, that is, ∆µ A &gt; 0 for a policy that selects every individual whose score is above the group A mean, which is reasonable in reality. Indeed, the converse would imply that group A has such low scores that even selecting all above average individuals in A would hurt the average score. In such a case, Corollary 3.5 suggests that EqOpt is better than DemParity at avoiding active harm, because it is more conservative. A natural question then is: can EqOpt cause relative harm by being too stingy? Theorem 3.6 (DemParity never loans less than MaxUtil, but EqOpt might). Recall the definition of the TPR functions ω j , and suppose that the MaxUtil policy τ MaxUtil is such that</p><formula xml:id="formula_12">β MaxUtil A &lt; β MaxUtil B and TPR A (τ MaxUtil ) &gt; TPR B (τ MaxUtil ). Then β EqOpt A &lt; β MaxUtil A &lt; β DemParity A .</formula><p>That is, EqOpt causes relative harm by selecting at a rate lower than MaxUtil.</p><p>The above theorem shows that DemParity is never stingier than MaxUtil to the protected group A, as long as a A is disadvantaged in the sense that MaxUtil selects a larger proportion of B than A. On the other hand, EqOpt can select less of group A than MaxUtil, and by definition, cause relative harm. This is a surprising result about EqOpt, and this phenomenon arises from high levels of in-group inequality for group A. Moreover, we show in Appendix F that there are parameter settings where the conditions in Theorem 3.6 are satisfied even under a stringent notion of disadvantage we call CDF domination, described therein.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Relaxations of Constrained Fairness</head><p>Regularized fairness: In many cases, it may be unrealistic for an institution to ensure that fairness constraints are met exactly. However, one can consider "soft" formulations of fairness constraints which either penalized the differences in acceptance rate (DemParity) or the differences in TPR (EqOpt). In Appendix E, we formulate these soft constraints as regularized objectives. For example, a soft-DemParity can be rendered as</p><formula xml:id="formula_13">max τ :=τ A ,τ B U(τ ) − λΦ( π A , τ A − π B , τ B ) ,<label>(4)</label></formula><p>where λ &gt; 0 is a regularization parameter, and Φ(t) is a convex regularization function. We show that the solutions to these objectives are threshold policies, and can be fully characterized in terms of the group-wise selection rate. We also make rigorous the notion that policies which solve the soft-constraint objective interpolate between MaxUtil policies at λ = 0 and hard-constrained policies (DemParity or EqOpt) as λ → ∞. This fact is clearly demonstrated by the form of the solutions in the special case of the regularization function Φ(t) = |t|, provided in the appendix.</p><p>Fairness under measurement error: Next, consider the implications of an institution with imperfect knowledge of scores. Under a simple model in which the estimate of an individual's score X ∼ π is prone to errors e(X) such that X + e(X) := X ∼ π. Constraining the error to be negative results in the setting that scores are systematically underestimated. In this setting, it is equivalent to consider the CDF of underestimated distribution π to be dominated by the CDF true distribution π, that is</p><formula xml:id="formula_14">x≥c π(x) ≤ x≥c π(x) for all c ∈ [C].</formula><p>Then we can compare the institution's behavior under this estimation to its behavior under the truth. Because fairness criteria encourage a higher selection rate for disadvantaged groups (Theorem 3.2), systematic underestimation widens the regime of their applicability. Furthermore, since the estimated MaxUtil policy underloans, the region for relative improvement in the outcome curve (Figure <ref type="figure">1</ref>) is larger, corresponding to more regimes under which fairness criteria can yield favorable outcomes. Thus potential measurement error should be a factor when motivating these criteria.</p><p>Outcome-based alternative: As explained in the preceding sections, fairness criteria may actively harm disadvantaged groups. It is thus natural to consider a modified decision rule which involves the explicit maximization of ∆µ A . In this case, imagine that the institution's primary goal is to aid the disadvantaged group, subject to a limited profit loss compared to the maximum possible expected profit U MaxUtil . The corresponding problem is as follows.</p><formula xml:id="formula_15">max τ A ∆µ A (τ A ) s.t. U MaxUtil A − U(τ ) &lt; δ .</formula><p>(5)</p><p>Unlike the fairness constrained objective, this objective no longer depends on group B and instead depends on our model of the mean score change in group A, ∆µ A .</p><p>Proposition 4.2 (Outcome-based solution). In the above setting, the optimal bank policy τ A is a threshold policy with selection rate β = min{β * , β max }, where β * is the outcome-optimal loan rate and β max is the maximum loan rate under the bank's "budget".</p><p>The above formulation's advantage over fairness constraints is that it directly optimizes the outcome of A and can be approximately implemented given reasonable ability to predict outcomes. Importantly, this objective shifts the focus to outcome modeling, highlighting the importance of domain specific knowledge. Future work can consider strategies that are robust to outcome model errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Simulations</head><p>We  <ref type="bibr">et al. (2016)</ref>. These scores, corresponding to x in our model, range from 300 to 850 and are meant to predict credit risk. Empirical data labeled by race allows us to estimate the distributions π j , where j represents race, which is restricted to two values: white non-Hispanic (labeled "white" in figures), and black. Using national demographic data, we set the population proportions to be 18% and 82%.</p><p>Individuals were labeled as defaulted if they failed to pay a debt for at least 90 days on at least one account in the ensuing 18-24 month period; we use this data to estimate the success probability given score, ρ j (x), which we allow to vary by group to match the empirical data. Our outcome curve framework allows for this relaxation; however, this discrepancy can also be attributed to group-dependent mismeasurement of score, and adjusting the scores accordingly would allow for a single ρ(x). We use the success probabilities to define the affine utility and score change functions defined in Example 2.1. We model individual penalties as a score drop of c − = −150 in the case of a default, and in increase of c + = 75 in the case of successful repayment.  In Figure <ref type="figure" target="#fig_4">3</ref>, we display the empirical CDFs along with selection rates resulting from different loaning strategies for two different settings of bank utilities. In the case that the bank experiences a loss/profit ratio of u− u+ = −10, no fairness criteria surpass the active harm rate β 0 ; however, in the case of u− u+ = −4, DemParity overloans, in line with the statement in Theorem 3.3. These results are further examined in Figure <ref type="figure" target="#fig_5">4</ref>, which displays the normalized outcome curves and the utility curves for both the white and the black group. To plot the MaxUtil utility curves, the group that is not on display has selection rate fixed at β MaxUtil . In this figure, the top panel corresponds to the average change in credit scores for each group under different loaning rates β; the bottom panels shows the corresponding total utility U (summed over both groups and weighted by group population sizes) for the bank.</p><p>Figure <ref type="figure" target="#fig_5">4</ref> highlights that the position of the utility optima in the lower panel determines the loan (selection) rates. In this specific instance, the utility and change ratios are fairly close, u− u+ = −4 and c− c+ = −2, meaning that the bank's profit motivations align with individual outcomes to some extent. Here, we can see that EqOpt loans much closer to optimal than DemParity, similar to the setting suggested by Theorem 3.2.</p><p>Although one might hope for decisions made under fairness constraints to positively affect the black group, we observe the opposite behavior. The MaxUtil policy (solid orange line) and the EqOpt policy result in similar expected credit score change for the black group. However, DemParity (dashed green line) causes a negative expected credit score change in the black group, corresponding to active harm. For the white group, the bank utility curve has almost the same shape under the fairness criteria as it does under MaxUtil, the main difference being that fairness criteria lowers the total expected profit from this group. This behavior stems from a discrepancy in the outcome and profit curves for each population. While incentives for the bank and positive results for individuals are somewhat aligned for the majority group, under fairness constraints, they are more heavily misaligned in the minority group, as seen in graphs (left) in Figure <ref type="figure" target="#fig_5">4</ref>. We remark that in other settings where the unconstrained profit maximization is misaligned with individual outcomes (e.g., when u− u+ = −10), fairness criteria may perform more favorably for the minority group by pulling the utility curve into a shape consistent with the outcome curve.</p><p>By analyzing the resulting effects of MaxUtil, DemParity, and EqOpt on actual credit score lending data, we show the applicability of our model to real-world applications. In particular, results shown in Section 3 hold empirically for the FICO TransUnion TransRisk scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and Future Work</head><p>We argue that without a careful model of delayed outcomes, we cannot foresee the impact a fairness criterion would have if enforced as a constraint on a classification system. However, if such an accurate outcome model is available, we show that there are more direct ways to optimize for positive outcomes than via existing fairness criteria.</p><p>Our formal framework exposes a concise, yet expressive way to model outcomes via the expected change in a variable of interest caused by an institutional decision. This leads to the natural concept of an outcome curve that allows us to interpret and compare solutions effectively. In essence, the formalism we propose requires us to understand the two-variable causal mechanism that translates decisions to outcomes. Depending on the application, such an understanding might necessitate greater domain knowledge and additional research into the specifics of the application. This is consistent with much scholarship that points to the context-sensitive nature of fairness in machine learning.</p><p>An interesting direction for future work is to consider other characteristics of impact beyond the change in population mean. Variance and individual-level outcomes are natural and important considerations. Moreover, it would be interesting to understand the robustness of outcome optimization to modeling and measurement errors.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 Figure 1 .</head><label>11</label><figDesc>Figure1displays the important outcome regimes in terms of selection rates β j := x∈X π j (x)τ j (x). This succinct</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Theorem 3.2 (Fairness criteria can cause relative improvement). (a) Under the assumption that β MaxUtil A population proportions g 0 &lt; g 1 &lt; 1 such that, for all g A ∈ [g 0 , g 1 ], β MaxUtil A &lt; β DemParity A &lt; β. That is, DemParity causes relative improvement. (b) Under the assumption that there exist β MaxUtil A</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Both outcomes ∆µ and institution utilities U can be plotted as a function of selection rate for one group. The maxima of the utility curves determine the selection rates resulting from various decision rules.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Proposition 4.1 (Underestimation causes underselection). Fix the distribution of B as π B and let β be the acceptance rate of A when the institution makes the decision using perfect knowledge of the distribution π A . Denote β as the acceptance rate when the group is instead taken as π A . Then β MaxUtil A further such that the true TPR dominates the estimated TPR, it is also true that β EqOpt A &gt; β EqOpt A .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. The empirical CDFs of both groups are plotted along with the decision thresholds resulting from MaxUtil, DemParity, and EqOpt for a model with bank utilities set to (a) u − u + = −4 and (b) u − u + = −10. The threshold for active harm is displayed; in (a) DemParity causes active harm while in (b) it does not. EqOpt and MaxUtil never cause active harm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. The outcome and utility curves are plotted for both groups against the group selection rates. The relative positions of the utility maxima determine the position of the decision rule thresholds. We hold u − u + = −4 as fixed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>examine the outcomes induced by fairness constraints in the context of FICO scores for two race groups. FICO scores are a proprietary classifier widely used in the United States to predict credit worthiness. Our FICO data is based on a sample of 301,536 TransUnion TransRisk scores from 2003 (US Federal Reserve, 2007), preprocessed by Hardt</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Department of Electrical Engineering and Computer Sciences, University of California at Berkeley, Berkeley, California, USA. Correspondence to: Lydia T. Liu &lt;lydiatliu@berkeley.edu&gt;.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Lily Hu, Aaron Roth, and Cathy O'Neil for discussions and feedback on an earlier version of the manuscript. We thank the students of CS294: Fairness in Machine Learning (Fall 2017, University of California, Berkeley) for inspiring class discussions and comments on a presentation that was a precursor of this work. This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No. DGE 1752814.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Big data&apos;s disparate impact</title>
		<author>
			<persName><forename type="first">Solon</forename><surname>Barocas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">D</forename><surname>Selbst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">California Law Review</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Building classifiers with independency constraints</title>
		<author>
			<persName><forename type="first">Toon</forename><surname>Calders</surname></persName>
		</author>
		<author>
			<persName><surname>Kamiran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mykola</forename><surname>Pechenizkiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICDMW, ICDMW &apos;09</title>
				<meeting>IEEE ICDMW, ICDMW &apos;09</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Fair prediction with disparate impact: A study of bias in recidivism prediction instruments</title>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Chouldechova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>FATML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Will affirmative-action policies eliminate negative stereotypes?</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Coate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Glenn</forename><surname>Loury</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="1220" to="1240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Danielle</forename><surname>Ensign</surname></persName>
		</author>
		<author>
			<persName><surname>Friedler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sorelle</surname></persName>
		</author>
		<author>
			<persName><surname>Neville</surname></persName>
		</author>
		<author>
			<persName><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Scheidegger</surname></persName>
		</author>
		<author>
			<persName><surname>Venkatasubramanian</surname></persName>
		</author>
		<author>
			<persName><surname>Suresh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.09847</idno>
		<title level="m">Runaway feedback loops in predictive policing</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Executive Office of the President. Big data: A report on algorithmic systems, opportunity, and civil rights</title>
		<imprint>
			<date type="published" when="2016-05">May 2016</date>
			<publisher>White House</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An economic argument for affirmative action</title>
		<author>
			<persName><forename type="first">Dean</forename><forename type="middle">P</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><surname>Vohra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rakesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Rationality and Society</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="176" to="188" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Predictably unequal? the effects of machine learning on credit markets</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Fuster</surname></persName>
		</author>
		<author>
			<persName><surname>Goldsmith-Pinkham</surname></persName>
		</author>
		<author>
			<persName><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tarun</forename><surname>Ramadorai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ansgar</forename><surname>Walther</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>SSRN</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Equality of opportunity in supervised learning</title>
		<author>
			<persName><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Price</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Srebro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nati</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 30th NIPS</title>
				<meeting>30th NIPS</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A short-term intervention for long-term fairness in the labor market</title>
		<author>
			<persName><forename type="first">Lily</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiling</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 27th WWW</title>
				<meeting>27th WWW</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fairness in learning: Classic and contextual bandits</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><surname>Morgenstern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jamie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 30th NIPS</title>
				<meeting>30th NIPS</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="325" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Best Practices or Best Guesses? Assessing the Efficacy of Corporate Affirmative Action and Diversity Policies</title>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Kalev</surname></persName>
		</author>
		<author>
			<persName><surname>Dobbin</surname></persName>
		</author>
		<author>
			<persName><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erin</forename><surname>Kelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Sociological Review</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="589" to="617" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Effects of affirmative action in medical schools</title>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">N</forename><surname>Keith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">M</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">August</forename><forename type="middle">G</forename><surname>Swanson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><forename type="middle">P</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New England Journal of Medicine</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">24</biblScope>
			<biblScope unit="page" from="1519" to="1525" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Inherent trade-offs in the fair determination of risk scores</title>
		<author>
			<persName><forename type="first">Jon</forename><forename type="middle">M</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><surname>Mullainathan</surname></persName>
		</author>
		<author>
			<persName><surname>Sendhil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manish</forename><surname>Raghavan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 8th ITCS</title>
				<meeting>8th ITCS</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Racial bias in motor vehicle searches: Theory and evidence</title>
		<author>
			<persName><forename type="first">John</forename><surname>Knowles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicola</forename><surname>Persico</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petra</forename><surname>Todd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Political Economy</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="203" to="229" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On fairness and calibration</title>
		<author>
			<persName><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><surname>Manish</surname></persName>
		</author>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Felix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5684" to="5693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The Color of Credit: Mortgage Discrimination, Research Methodology, and Fair-Lending Enforcement</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Yinger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Report to the congress on credit scoring and its effects on the availability and affordability of credit</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
		<respStmt>
			<orgName>US Federal Reserve</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fairness Constraints: Mechanisms for Fair Classification</title>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Zafar</surname></persName>
		</author>
		<author>
			<persName><surname>Bilal</surname></persName>
		</author>
		<author>
			<persName><surname>Valera</surname></persName>
		</author>
		<author>
			<persName><surname>Isabel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Rogriguez</surname></persName>
		</author>
		<author>
			<persName><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><forename type="middle">P</forename><surname>Gummadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 20th AISTATS</title>
				<meeting>20th AISTATS</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="962" to="970" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
