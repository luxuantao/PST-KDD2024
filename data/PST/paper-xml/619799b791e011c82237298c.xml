<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pruning In Time (PIT): A Lightweight Network Architecture Optimizer for Temporal Convolutional Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-03-28">28 Mar 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Matteo</forename><surname>Risso</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Politecnico di Torino</orgName>
								<address>
									<settlement>Turin</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alessio</forename><surname>Burrello</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Bologna</orgName>
								<address>
									<settlement>Bologna</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniele</forename><forename type="middle">Jahier</forename><surname>Pagliari</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Politecnico di Torino</orgName>
								<address>
									<settlement>Turin</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Francesco</forename><surname>Conti</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Bologna</orgName>
								<address>
									<settlement>Bologna</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lorenzo</forename><surname>Lamberti</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Bologna</orgName>
								<address>
									<settlement>Bologna</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Enrico</forename><surname>Macii</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Politecnico di Torino</orgName>
								<address>
									<settlement>Turin</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Luca</forename><surname>Benini</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Bologna</orgName>
								<address>
									<settlement>Bologna</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Massimo</forename><surname>Poncino</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Politecnico di Torino</orgName>
								<address>
									<settlement>Turin</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Pruning In Time (PIT): A Lightweight Network Architecture Optimizer for Temporal Convolutional Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-03-28">28 Mar 2022</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/DAC18074.2021.9586187)</idno>
					<idno type="arXiv">arXiv:2203.14768v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Neural Architecture Search</term>
					<term>Temporal Convolutional Networks</term>
					<term>Edge Computing</term>
					<term>Deep Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Temporal Convolutional Networks (TCNs) are promising Deep Learning models for time-series processing tasks.</p><p>One key feature of TCNs is time-dilated convolution, whose optimization requires extensive experimentation. We propose an automatic dilation optimizer, which tackles the problem as a weight pruning on the time-axis, and learns dilation factors together with weights, in a single training. Our method reduces the model size and inference latency on a real SoC hardware target by up to 7.4? and 3?, respectively with no accuracy drop compared to a network without dilation. It also yields a rich set of Pareto-optimal TCNs starting from a single model, outperforming hand-designed solutions in both size and accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Deep learning (DL) models achieve state-of-the-art performance in many time-series processing tasks, such as biosignals analysis <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, predictive maintenance <ref type="bibr" target="#b2">[3]</ref> and sound classification <ref type="bibr" target="#b3">[4]</ref>. Recurrent Neural Networks (RNNs) have long been considered the de facto standard for these tasks <ref type="bibr" target="#b4">[5]</ref>, but recently, Temporal Convolutional Networks (TCNs) -a subclass of Convolutional Neural Networks (CNNs) dedicated to time series -have been shown to provide comparable accuracy, whilst offering several advantages from a computational standpoint: smaller memory footprint, more data reuse opportunities and higher arithmetic intensity <ref type="bibr" target="#b5">[6]</ref>.</p><p>The deployment of DL models for direct inference on Internet of Things (IoT) edge devices is an emerging paradigm for the aforementioned time-series analysis tasks, as it provides several benefits compared to a cloud-centric approach. Edge computing reduces the pressure on the network, hence improving scalability, and guarantees predictable response latency, especially in presence of unreliable/unstable connectivity. Moreover, it may also improve energy efficiency and privacy, by avoiding the highly-consuming wireless transmission of large amounts of raw (and possibly sensitive) data to the cloud <ref type="bibr" target="#b6">[7]</ref>. However, running DL inference at the edge implies deploying models on cost-constrained devices such as microcontrollers (MCUs), with extremely limited memory and low operating frequencies.</p><p>In this context, TCNs are especially interesting due to their hardware-friendly properties. Nonetheless, their deployment at the edge still requires a careful tuning of all hyper-parameters, in order to meet the accuracy requirements of the target application at the minimum cost in terms of number of parameters or operations. For standard CNNs, popular in computer vision, this tuning is increasingly performed with automatic tools, in a process known as Neural Architecture Search (NAS). Recent years have seen the appearance of a plethora of different NAS approaches, based on as many different search strategies, some specifically targeting edge devices <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b11">[12]</ref>.</p><p>While TCNs have most of their hyper-parameters in common with standard CNNs (e.g. the filter sizes, the channels in each layer, etc.) there is one fundamental peculiarity of these networks that requires an orthogonal approach, i.e., time dilation. As explained in Sec. II-A, dilation allows enlarging the receptive field of a TCN on the time axis without increasing the number of parameters. Tuning dilation parameters, therefore, offers a clear pathway to make TCN topologies smaller and more hardware-friendly.</p><p>In this work, we propose Pruning In Time (PIT), a novel and light-weight automated method to simultaneously optimize the dilation of all layers in a TCN, producing Paretooptimal solutions in terms of accuracy and complexity within a single training run. This is achieved modeling the problem as a structured weight pruning along the time axis: starting from maximally-sized filters with no dilation, we concurrently train the model weights and increase the dilation by pruning regularly-spaced weights slices on the time dimension. To the best of our knowledge, PIT is the first architecture optimizer that seamlessly targets dilation as the main optimization knob; state-of-the-art tools (e.g. <ref type="bibr" target="#b11">[12]</ref>) require explicitly listing all possible parameter combinations for every layer. With experiments on two different time-series processing tasks, we show that our method can find dilation factors that reduce the model size and inference latency on the GAP8 System-on-Chip (SOC) <ref type="bibr" target="#b12">[13]</ref> by up to 7.4? and 3? respectively, with no accuracy drop compared to a network without dilation. Moreover, it can produce a rich set of Pareto-optimal TCNs starting from a single seed, including solutions that outperform hand-designed models, reducing the number of parameters by up to 54% without accuracy drop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND AND RELATED WORKS A. Temporal Convolutional Networks</head><p>TCNs share with 1-dimensional (1D) CNNs the main building blocks, i.e. convolutional, pooling and linear layers. However, to improve the processing of time series, TCN convolutions include two new properties. Causality forces the padding to be applied only on left side of the sequence; therefore, the outputs y t are only functions of inputs x t with t ? t, so that output samples are not obtained taking information from the future. Dilation increases the receptive field of 1D convolutions without increasing the filters sizes, by applying a fixed step d between the input samples processed by each filter. Therefore, a 1D-convolution in a TCN is expressed by the following function:</p><formula xml:id="formula_0">y m t = Conv (x) = K-1 i=0 Cin-1 l=0 x l t-d i ? W l,m i (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where t is the time index, W the filter weights, C in the number of input channels, m the output channel, and K the filter size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Neural Architecture Search</head><p>Neural Architecture Search (NAS) is increasingly used to automatically design deep neural networks, avoiding the hand tuning of hyper-parameters, e.g. layer channels, filter dimensions, etc. NAS automatically explores a large design space of possible hyper-parameters settings, co-optimizing the accuracy of the resulting network with its cost, measured as the memory footprint <ref type="bibr" target="#b9">[10]</ref>, the number of Floating Point Operations (FLOPs) or even the latency <ref type="bibr" target="#b11">[12]</ref>. NAS outputs are sets of Pareto-optimal architectures in the complexity-accuracy space, from which users can select the best solution given their problem constraints. Due to this capability, the exploration of NAS solutions tuned for extreme edge devices has recently started getting traction <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>.</p><p>We can categorize NAS methods in three main groups. Early approaches used reinforcement learning or evolutionary algorithms to improve the structure of the network at the cost of thousands of training runs (and GPU hours), thus being often prohibitive for real-world tasks <ref type="bibr" target="#b7">[8]</ref>. More recent research has focused on differentiable NAS solutions (DNAS) <ref type="bibr" target="#b15">[16]</ref>, that model the problem as the optimization of a so-called supernet, which includes different alternative implementations of each layer. These methods jointly train a set of binary variables that determines a selection of layer implementations from the supernet (called a path) and the weights of that path. Although more efficient than the previous group, memory occupation and training time remain critical for these algorithms, since the supernet includes all possible combinations of layers implementations. ProxylessNAS <ref type="bibr" target="#b11">[12]</ref> tackles the memory problem by training a single path of the supernet per batch, thus limiting the memory occupation of the whole model, but not exploring the full solution space at the same time.</p><p>Lastly, an emerging set of DNAS methods is based on socalled DMaskingNAS <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, which aims at reducing the training time by limiting the search space to a single seed network. DMaskingNAS approaches add trainable parameters ("masks") to the seed network to tweak architectural features such as the number of channels or the filter dimensions. Specifically, a vector of ? (l) i is defined per each layer l, and combined with the normal weights of the model, in such a way that setting each of the ? (l) i to zero has an effect on the architecture (e.g. eliminating a channel or reducing the filter size). Then, the non-zero ? (l i are minimized during training with an approach similar to weight pruning, thus reducing the network size. For instance, MorphNet <ref type="bibr" target="#b9">[10]</ref> uses the already present multiplicative terms in batch normalization (BN) layers as ? (l) i , to zero-out entire channels. While the search space of DMaskingNAS algorithms is slightly more restricted than DNAS, it is still large enough to produce highquality solutions; and at the same time, it can be explored at similar cost to a single network training.</p><p>In our work, we propose the first DMaskingNAS solution tuned specifically at exploiting the main architectural feature of TCNs -namely, dilation -to find Pareto-optimal alternatives in terms of accuracy/model size, to ease deployment on memory-starved IoT devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PRUNING IN TIME</head><p>Despite the large number of NAS methods recently proposed in literature, most of them have focused on optimizing hyper-parameters that are relevant for standard CNNs, such as the number of channels in each layer and the kernel sizes <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref>, with particular focus on 2D CNNs for computer vision. To the best of our knowledge, no existing NAS has explicitly targeted the automatic optimization of dilation factors in 1D CNNs (d in Eq. 1). Nonetheless, dilation is fundamental for the accuracy of these networks, as detailed in <ref type="bibr" target="#b5">[6]</ref>, since it controls the relation between their receptive field in time (i.e., the range of samples covered by each convolution step) and their filter sizes (which determine the model complexity). Setting dilation factors by hand requires an empirical and time consuming trial and error process.</p><p>In this paper, we make up for this lack by proposing Pruning In Time (PIT), a light-weight and efficient DMaskingNAS method that learns the optimal set of dilation factors for a TCN given as seed network. The functionality of PIT is shown in Fig. <ref type="figure" target="#fig_0">1</ref>: our algorithm starts from a seed network with maximally-sized filters and dilation d = 1 in all layers, and concurrently learns the TCN weights and the optimal dilations for each layer, based on a cost metric. Specifically, in this work we consider model size as the target metric, but the method is easily extendable to other types of optimizations (e.g., FLOPs reduction). The optimization of dilation factors is obtained by modeling the problem as a weight pruning in time, as explained in the following, hence the name of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Making Dilation Differentiable</head><p>In order to build a DNAS algorithm for dilated convolutions, the primary issue to be solved is how to embed the different dilation factors in a (differentiable) component of the  loss function, so that they can be optimized during training.</p><formula xml:id="formula_2">K 0 K 8 K 1 K 2 K 3 K 4 K 5 K 6 K 7 K 0 K 8 K 1 K 2 K 3 K 4 K 5 K 6 K 7 K 0 K 8 K 1 K 3 K 4 K 5 K 7 K 2 K 6 K 0 K 8 K 1 K 3 K 4 K 5 K 7 K 2 K 6 K 0 K 8 K 1 K 2 K 3 K 4 K 5 K 6 K 7 K 0 K 8 K 1 K 2 K 3 K 4 K 5 K 6 K 7 K 0 K 1 K 2 K 3 K 4 K 5 K 6 K 7 K 8 K 0 K 1 K 2 K 3 K 4 K 5 K 6 K 7 K 8 =? 1 ?? 2 ?? 3 =? 1 ?? 2 =? 1 ? 1 =0 ? 1 =1 ? 2 =0 Dilation=8 Dilation=4 ? 1 =1 ? 2 = 1 ? 3 =0 Dilation=2 W 0 W 1 W 2 W 3 W 4 W 5 W 6 W 7 W 8 ? 3 ? 0 ? 1 ? 3 ? 0 ? 2 ? 0 ? 1 ? 0 ? 0 ? 1 ? 2 W 0 W 8 W 1 W 2 W 3 W 4 W 5 W 6 W 7 W 0 W 8 W 1 W 2 W 3 W 4 W 5 W 6 W 7 W 0 W 8 W 1 W 3 W 4 W 5 W 7 W 2 W 6 =1 ? 3<label>(1) (1)</label></formula><p>Similarly to other DMaskingNAS techniques, our approach is built upon additional trainable parameters, which we call ?.</p><p>Each temporal convolution in a TCN is characterized by a certain kernel size k and a certain dilation factor d. Together, these two parameters define the receptive field rf of the layer: rf = (k -1) ? d. Notice that, in this formulation, the dilation is regular, i.e., the number of skipped time-steps is constant in each convolutional layer. PIT limits the search space to such solutions, which are the only ones supported by currentgeneration inference libraries for MCUs <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, and generally enable better low-level optimizations and more regular memory access patterns. In particular, we focus on dilation factors d that are expressed as powers of 2. These maintain a high degree of freedom in enlarging or reducing the receptive field, while restricting the solution space and enabling the simple formalization that we treat in the following.</p><p>For every temporal convolution, PIT starts by defining a vector of binary parameters ?, containing L = log 2 (rf max -1) +1 elements, where rf max is the maximum supported rf . In particular, ? 0 is constant and always equal to 1, and is added just to simplify the mathematical notation. The remaining ? 1:L-1 , instead, are the main knobs that control dilation. We binarize ? following a BinaryConnect-like approach <ref type="bibr" target="#b18">[19]</ref>. In forward-propagation, we use a Heaviside step function and a threshold ?, fixed to 0.5 in our experiments:</p><formula xml:id="formula_3">H( ?i -?) = 1, for ?i ? ? 0, for ?i &lt; ? (2)</formula><p>where ?i is the floating point version of ? i . In backward passes, because the derivative of this function is zero almost everywhere, its gradient is treated with a straight-through estimator <ref type="bibr" target="#b18">[19]</ref>, i.e., the step is replaced with an identity function for the purpose of propagating gradients.</p><p>To restrict the search space to regular dilation patterns, the trainable parameters in ? are combined together in the way depicted in the red part of Fig. <ref type="figure" target="#fig_1">2</ref>. We start by multiplying together the elements in ? to form a new set of ?, which will be the actual values used to perform the masking:</p><formula xml:id="formula_4">? i = L-1-i k=0 ? k<label>(3)</label></formula><p>Notice that, in Fig. <ref type="figure" target="#fig_1">2</ref>, ? 0 has been directly replaced with 1.</p><p>Intuitively, the parameters ? have to act as on/off selectors that enable or disable entire time slices of the convolution filter, encoding regular patterns of power-of-two dilation.</p><p>As shown in the figure, d = 1 is achieved only when all trainable parameters are 1, i.e., when</p><formula xml:id="formula_5">? 0 = ? 0 ? ? 1 ? ? ? ? L-1 = 1.</formula><p>To encode patterns with larger dilation, every time we double d we remove the condition on one ? i , so for d = 2 we use</p><formula xml:id="formula_6">? 1 = ? 0 ?? 1 ? ? ? ? L-2 = 1,</formula><p>etc., up to the highest supported dilation (d = 2 L-1 ), which is obtained with the (always true) condition ? L-1 = ? 0 = 1.</p><p>To obtain this behavior, we further combine the elements of ? in a mask vector M , of length rf max , as shown in Fig. <ref type="figure" target="#fig_1">2</ref>. Each element of M is then multiplied with all filter weights relative to the corresponding time-step (and all channels) to perform the actual masking. The right side of Fig. <ref type="figure" target="#fig_1">2</ref> shows how the various dilation alternatives are mapped to the trainable ? through the masking procedure described above, for the case of rf max = 9 (L = 4).</p><p>In order to train ? with a DNAS method, the constructive description of the mask vector M given above must be expressed in a differentiable form. To do so, we use the following tensor transformation:</p><formula xml:id="formula_7">M = columns {[(? ? 1 1?L ) T + (1 L?L -T )] ? K} (4)</formula><p>where columns indicates the product of all elements in each column of the final matrix, 1 i?j is a matrix of 1s of size i ? j and is the Hadamard product. T and K are two constant matrices of 0s and 1s. An example of these two matrices and of the entire transformation is shown in Fig. <ref type="figure">3</ref>, for the same rf max and L used in Fig. <ref type="figure" target="#fig_1">2</ref>. Again, ? 0 has been directly replaced with 1 for clarity. Notice that T is just an upper triangular matrix with inverted columns, whereas K can be generated</p><formula xml:id="formula_8">1 ? 1 ? 2 t = ? 1 ? 1 ? 1 ? 2 ? 2 ? 2 ? 3 ? 3 ? 3 1 1 1 1 1 0 1 0 0 = + = = ? 1 ? 2 ? 3 1 1 1 1 1 1 1 1 0 0 0 ? 1 ? 1 ? 3 ? 1 ? 2 ? 2 1 0 1 ? 3 1 0 1 0 0 0 0 1 1 1 1 0 0 0 0 0 1 0 0 0 1 1 0 1 1 1 0 ? 1 ? 1 ? 1 ? 2 ? 2 1 ? 3 1 1 1 1 1 1 1 1 1 = 1 1 1 ? 1 ? 1 ? 1 ? 2 ? 2 ? 2 1 ? 1 1 1 ? 1 ? 2 1 ? 1 ? 2 1 ? 1 ? 2 1 1 1 1 1 1 ? 3 1 ? 3 1 ? 3 1 ? 3 1 1 rf max 1 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 1 Fig. 3.</formula><p>Example of generation of the M mask vector with differentiable tensor operations, for the same layer size as Fig. <ref type="figure" target="#fig_1">2</ref>.</p><p>procedurally for any value of rf max , by repeating a pattern of 0s and 1s (procedure not shown for sake of space). Finally, the complete layer equation for dilated convolution in PIT becomes:</p><formula xml:id="formula_9">y m t = rf max -1 i=0 Cin-1 l=0 x l t-i ? (M i W l,m i )<label>(5)</label></formula><p>where all symbols have been defined in previous Eq. 1-4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Dilation Regularizer</head><p>Having modified all convolutional layers with ? vectors, PIT performs a standard training, where the loss function is augmented with a Lasso regularization term, to promote the sparsification (pruning) of the ? i , thus enabling the exploration of architectures with d &gt; 1.</p><p>The regularization term can be customized to direct the search towards a target cost metric. In this work, we consider the network size as our proxy for cost. Therefore, we use the following regularizer:</p><formula xml:id="formula_10">L size R (?) = ? layers l=1 C (l) in ? C (l) out L-1 i=1 round rf max -1 2 L-i |? (l) i |<label>(6</label></formula><p>) where ? controls the strength of the regularization and superscript (l) refers to the l-th layer.</p><formula xml:id="formula_11">C (l) in and C (l)</formula><p>out are the number of input/output channels in the l-th layer, both of which influence the layer size in a way which is linearly proportional to the amount of non-pruned time-slices in the filter matrix. Finally, round rf max -1 2 L-i is the number of alive (i.e. non-masked) filter time-slices added by each non-zero ? (l) i (see Fig. <ref type="figure" target="#fig_1">2</ref>).</p><p>Globally, the loss function optimized in the pruning phase by PIT is the following:</p><formula xml:id="formula_12">L P IT (W , ?) = L perf (W ) + L size R (?)<label>(7)</label></formula><p>where L perf (W ) is the loss term related to the TCN's performance, e.g., the classification accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Training Procedure</head><p>Algorithm 1 summarizes the training procedure implemented in PIT. As shown, the process is composed of three separate phases. Initially, we perform a warmup lasting Steps wu training steps. In this phase, all elements in ? vectors are initialized to 1. Therefore, we only train the weights of the seed network, with maximally sized filters and d = 1 in all convolution layers, and considering only the network performance as objective. Next, the main pruning loop is initiated. Here, we concurrently update the weights and the ? vectors with the regularized loss. Once the pruning phase reached convergence (i.e., loss not improving on the validation set), we enter the third phase, where all ? are frozen to their latest binarized values, and the resulting network with dilation is fine-tuned, again considering only performance in the loss. We found that both warmup and fine-tuning significantly improve the final accuracy of the pruned networks. Update W based on ? W L perf (W ) 9: end for The procedure of Algorithm 1 can be repeated with different regularizer strenghts (? in Eq. 6) to explore the performance versus cost design space. Similarly, also the number of warmup steps has an impact on the same trade-off. Indeed, as explained in <ref type="bibr" target="#b11">[12]</ref>, a shorter warmup, yielding a less accurate network at the beginning of pruning, tends to favor model simplifications, as their impact on accuracy is less at the beginning of the pruning phase.</p><p>Importantly, PIT can be easily integrated with other DMask-ingNAS techniques that affect different hyper-parameters, e.g., <ref type="bibr" target="#b9">[10]</ref> to tune the number of channels in each layer, simply by adding further regularization terms and masking parameters, to perform a wider exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Setup</head><p>Datasets &amp; architectures. We benchmark PIT using two different datasets and 1D-CNN seed networks. The first is Notthingham, a polyphonic music dataset extracted from 1200 American and British folk tunes <ref type="bibr" target="#b5">[6]</ref>. Each input is a sequence of samples, each represented by a 88-bit binary code, corresponding to the 88 keys of a piano. As a starting point to build the seed model for this dataset, we use the TCN presented in <ref type="bibr" target="#b5">[6]</ref>, here refered as ResTCN. In detail, to automatically search for the best dilation parameters, we start from ResTCN with identical receptive fields as the one of [6], but setting d = 1 in each layer. The second dataset is PPGDalia <ref type="bibr" target="#b19">[20]</ref>, the largest publicly available dataset for PPGbased heart rate estimation. It includes measurements from a PPG-sensor and a 3D-accelerometer, together with golden HR values for 15 subject and a total of 37.5 hours of recording. In this case we use TEMPONet, first introduced in [1], as our seed architecture, since this network achieved excellent results on similar bio-signal processing tasks. Again, the seed is obtained setting d = 1 in all layers while maintaining the receptive fields. The PIT code and the results for all the other benchmarks introduced in <ref type="bibr" target="#b5">[6]</ref> are released open source at https://github.com/matteorisso/PIT and are not reported here for sake of space.</p><p>Deployment. To measure latency and energy gains obtained by PIT, we deploy models produced by our tool on the Green-Waves Technologies' GAP8 System-on-Chip (SoC) <ref type="bibr" target="#b12">[13]</ref>, a parallel ultra-low-power platform that features one I/O core and an 8-core cluster with a RISC-V Instruction Set Architecture extension for enhanced digital signal processing. The SoC includes a two level memory hierarchy: a singlecycle clock latency 64 kB L1 scratchpad, and a 512 kB L2 memory. Additionally, an off-chip L3 memory can be plugged to expand the storage capability. GAP8 also includes two general-purpose Direct Memory Access (DMA) controllers to move data between memories, reducing the memory access bottleneck.</p><p>We deploy int8-quantized models using GreenWaves' proprietary neural network deployment flow, called NN-Tool, which supports dilated 1D convolutions. We set the I/O core frequency and the 8-core cluster frequency to 100 MHz.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Design Space Exploration</head><p>Starting from the two seed architectures detailed above, we perform a complete design space exploration, by tweaking  the ? regularization-strength of PIT and the warmup duration (Steps wu ). Results are summarized in Fig. <ref type="figure">4</ref>. Considering all possible power-of-two dilations achievable given the seed network's receptive fields, PIT operates in a search space of ?10 5 different solutions for the ResTCN, spanning from NNs with 400k to 3M parameters. For TEM-PONet, the search includes ?10 4 alternatives, ranging from 400k to 900k parameters. The dilations obtained by PIT for three different architectures, namely the largest (large), the smallest (small) and the closest in size to the original hand-designed ResTCN/TEMPONet (medium) are reported in Table <ref type="table" target="#tab_1">I</ref>. For both benchmarks, PIT finds new Pareto-optimal architectures that improve both the accuracy and the network size simultaneously, compared to the seed baselines without dilation (black squares in the figure). In detail, we find a ResTCN-variant with 2.54? less parameters and an accuracy improvement of 13% (from 3.12 to 2.72 Negative Log-Likelihood -NLL loss) with respect to the seed; concerning TEMPONet, the top-performing architecture shows a compression of 1.35? with a Mean Absolute Error (MAE) reduction of 0.16. PIT also finds a new ResTCN-based architecture that dominates the hand-tuned original network presented in <ref type="bibr" target="#b5">[6]</ref> in the Pareto sense, resulting in a 54% reduction of the parameters with almost identical performance. While the same is not true for TEMPONet, the hand-engineered network sits on the Pareto frontier in this case, showing the good quality of the architectures identified by PIT. PIT. For sake of space, the comparison is reported only on TEMPONet as seed architecture with PPGDalia as dataset. In Table <ref type="table" target="#tab_2">II</ref>, we report three different architectures found by each algorithm (small, medium and large, selected with the same rationale of Table <ref type="table" target="#tab_1">I</ref>). Both algorithms converge to same small network, while solutions are similar for the other two cases. Noteworthy, in the large case, PIT finds an architecture that is both smaller (694k vs. 731k parameters) and more accurate (4.92 vs 5.15 MAE) than ProxylessNAS. In Fig. <ref type="figure">5</ref>, we compare the training time required by PIT and ProxylessNAS to find the three networks. All the experiments have been performed with the same hardware set-up, namely a single NVIDIA-GTX 1080Ti GPU, and the same training algorithm parameters, including batch size = 128 and an early-stop patience of 50 epochs. PIT reduces the search time compared to ProxylessNAS by up to 10.4?, being only 1.3?-2.3? slower than the training of a single hand-designed TEMPONet architecture. This is mainly due to the concurrent training of all weights and masking parameters performed by PIT. In contrast, ProxylessNAS trains only one path of the supernet (and the corresponding weights) in each iteration of the training loop, causing a significant time overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison with state-of-the-art</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Deployment on GAP8</head><p>We deployed the original hand-engineered TCNs, the seed networks with unitary dilation, and the three different PIT outputs for each dataset reported in Table <ref type="table" target="#tab_1">I</ref>, on the 8-cores cluster of GAP8. Table III reports the results in terms of network size, loss (either NLL or MAE for the two datasets), latency and energy.</p><p>Deploying the "medium" PIT ResTCN, we obtain a loss equivalent to that of the hand-tuned network presented in <ref type="bibr" target="#b5">[6]</ref> with 2.2? less parameters, and 1.5? lower latency and energy. For TEMPONet, we obtain a medium network which is equivalent to the hand-engineered one in every metric.</p><p>Noteworthy, our small (medium) ResTCN networks have a 20% higher (0.9% lower) loss compared to the seed network, but are 9.5? (7.4?) smaller and 3.0? (3.0?) faster. On TEMPONet, the small (medium) models obtain a 6.9% (3.9%) worse error compared to the seed, with 2.5? (2.1?) less weights and 2.1? (1.9?) lower latency and energy.</p><p>V. CONCLUSIONS NAS methods are becoming essential tools for deep learning, enabling a fast exploration of the model architecture design space without the tedious trial-and-error approach that characterized the development of new neural networks in the past. Therefore, it is fundamental to develop efficient NAS methods, which do not require extreme hardware resources and/or an enormous training times. At the same time, these tools must be capable of exploring all important hyperparameters, which in case of a TCN include the dilation. The proposed PIT is a light-weight NAS framework that goes in that direction, and successfully generates size-optimized and hardware-friendly TCNs. PIT is able to generate improved versions of existing state-of-the-art architectures, with a compression of up to 54% with negligible accuracy drop, enabling their efficient deployment on resource-constrained MCUs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Training flow of the proposed Pruning In Time architecture optimizer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>MFig. 2 .</head><label>2</label><figDesc>Fig. 2. Combination of ? elements with each other and with convolution filter weights to form different dilation patterns. Example for max = 9.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Fig. 4. PIT Pareto frontiers obtained starting from two seed architectures on the Nottingham dataset (top) and on PPGDalia (bottom). Each plot also reports the seed model (square) and the original hand-engineered TCN (triangle).</figDesc><graphic url="image-13.png" coords="5,34.85,147.00,262.13,131.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 1 PIT -Pruning in Time 1: for i ? 1, . . . , Steps wu do #warmup loop 2: Update W based on ? W L perf (W ) 3: end for 4: while not converged do #pruning loop</figDesc><table /><note><p><p>5:</p>Update W and ? based on ? W ,? L P IT (W , ?) 6: end while 7: for i ? 1, . . . , Steps ft do #fine-tuning loop 8:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I DILATIONS</head><label>I</label><figDesc>OBTAINED FOR THE DIFFERENT CONVOLUTIONAL LAYERS OF RESTCN AND TEMPONET IN DIFFERENT PIT OUTPUTS.</figDesc><table><row><cell>network</cell><cell>PIT dilations</cell></row><row><cell>ResTCN dil=hand-tuned</cell><cell>(1, 1, 2, 2, 4, 4, 8, 8)</cell></row><row><cell>PIT ResTCN small</cell><cell>(4, 4, 8, 8, 16, 16 32, 32)</cell></row><row><cell>PIT ResTCN medium</cell><cell>(4, 1, 4, 8, 16, 16, 32, 32)</cell></row><row><cell>PIT ResTCN large</cell><cell>(1, 4, 8, 8, 16, 16, 8, 1)</cell></row><row><cell cols="2">TEMPONet dil=hand-tuned (2, 2, 1, 4, 4, 8, 8)</cell></row><row><cell>PIT TEMPONet small</cell><cell>(2, 4, 4, 8, 8, 16, 16)</cell></row><row><cell>PIT TEMPONet medium</cell><cell>(1, 2, 4, 2, 1, 8, 16)</cell></row><row><cell>PIT TEMPONet large</cell><cell>(1, 1, 1, 1, 1, 1, 16)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II COMPARISON</head><label>II</label><figDesc>BETWEEN PIT AND PROXYLESSNAS, WITH TEMPONET AS SEED ARCHITECTURE AND PPGDALIA AS DATASET.</figDesc><table><row><cell></cell><cell></cell><cell>ProxylessNAS</cell><cell cols="2">Pruning in Time</cell></row><row><cell></cell><cell cols="4"># weights Perf. [MAE] # weights Perf. [MAE]</cell></row><row><cell>small</cell><cell>381k</cell><cell>5.43</cell><cell>381k</cell><cell>5.43</cell></row><row><cell cols="2">medium 517k</cell><cell>5.21</cell><cell>440k</cell><cell>5.28</cell></row><row><cell>large</cell><cell>731k</cell><cell>5.15</cell><cell>694k</cell><cell>4.92</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Table II compares solutions found by PIT with the ones of the state-of-the-art ProxylessNAS<ref type="bibr" target="#b11">[12]</ref>, a DNAS algorithm based on the supernet idea, that can be adapted to search over different dilation factors in a 1D-CNN by manually including all layer variants in the supernet. Specifically, for each layer, ProxylessNAS chooses between multiple alternatives, that we manually specified by increasing d and keeping C in and C out constant, so to match exactly the search space explored by Published as a conference paper at 2021 58th ACM/IEEE DAC(https://doi.org/10.1109/DAC18074.2021.9586187)</figDesc><table><row><cell></cell><cell></cell><cell>PIT</cell><cell cols="2">ProxylessNAS</cell><cell></cell><cell>No-NAS training</cell></row><row><cell></cell><cell>180</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Training time [min]</cell><cell>150 120 90 60 30</cell><cell>5.3x faster</cell><cell>slower 2.3x</cell><cell>10.4x faster</cell><cell>slower 1.4x</cell><cell cols="2">slower 1.3x faster 8.2x</cell></row><row><cell></cell><cell>0</cell><cell cols="6">TEMPONet small TEMPONet medium TEMPONet large</cell></row><row><cell cols="8">Fig. 5. Comparison of training time for ProxylessNAS, PIT, and a single NN</cell></row><row><cell cols="8">training using a seed TEMPONet network and the PPGDalia dataset.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>TABLE III</cell><cell></cell><cell></cell></row><row><cell cols="8">PIT SOLUTIONS COMPARED TO ORIGINAL SEED NETWORKS WITHOUT</cell></row><row><cell cols="8">DILATION AND WITH HAND-TUNED DILATION DEPLOYED ON GAP8 SOC.</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2"># weights loss</cell><cell></cell><cell>latency</cell><cell>energy</cell></row><row><cell cols="3">ResTCN dil=1</cell><cell>3.53M</cell><cell cols="2">3.12 NLL</cell><cell>1002 ms</cell><cell>262.7 mJ</cell></row><row><cell cols="3">ResTCN dil=h.-t.</cell><cell>1.05M</cell><cell cols="2">3.07 NLL</cell><cell>500 ms</cell><cell>131 mJ</cell></row><row><cell cols="3">PIT ResTCN s.</cell><cell>0.37M</cell><cell cols="2">3.79 NLL</cell><cell>336.7 ms</cell><cell>88.2 mJ</cell></row><row><cell cols="3">PIT ResTCN m.</cell><cell>0.48M</cell><cell cols="2">3.09 NLL</cell><cell cols="2">335.9 ms 87.9 mJ</cell></row><row><cell cols="3">PIT ResTCN l.</cell><cell>1.39M</cell><cell cols="2">2.72 NLL</cell><cell>539.2 ms</cell><cell>141.3 mJ</cell></row><row><cell cols="3">TEMPONet dil=1</cell><cell>939K</cell><cell cols="2">5.08 MAE</cell><cell>112.6 ms</cell><cell>29.5 mJ</cell></row><row><cell cols="3">TEMPONet dil=h.-t.</cell><cell>423K</cell><cell cols="2">5.31 MAE</cell><cell>58.8 ms</cell><cell>15.4 mJ</cell></row><row><cell cols="3">PIT TEMPONet s.</cell><cell></cell><cell cols="2">5.43 MAE</cell><cell>54.8 ms</cell><cell>14.4 mJ</cell></row><row><cell cols="3">PIT TEMPONet m.</cell><cell>440K</cell><cell cols="2">5.28 MAE</cell><cell>59.8 ms</cell><cell>15.7 mJ</cell></row><row><cell cols="3">PIT TEMPONet l.</cell><cell>694K</cell><cell cols="2">4.92 MAE</cell><cell>86.3 ms</cell><cell>22.6 mJ</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Robust real-time embedded emg recognition framework using temporal convolutional networks on a multicore iot processor</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zanghieri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TBioCAS</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="244" to="256" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for seizure prediction using intracranial and scalp electroencephalogram</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Truong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page" from="104" to="111" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Data-driven structural health monitoring and damage detection through deep learning: State-of-the-art review</title>
		<author>
			<persName><forename type="first">M</forename><surname>Azimi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">2778</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep neural networks for multiple speaker detection and localization</title>
		<author>
			<persName><forename type="first">W</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE ICRA</title>
		<imprint>
			<biblScope unit="page" from="74" to="79" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A critical review of recurrent neural networks for sequence learning</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<idno>1506.00019</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<idno>1803.01271</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Edge computing: Vision and challenges</title>
		<author>
			<persName><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet Things J</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="637" to="646" />
			<date type="published" when="2016-10">Oct 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01578</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fbnetv2: Differentiable neural architecture search for spatial and channel dimensions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="965" to="974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Morphnet: Fast &amp; simple resource-constrained structure learning of deep networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<biblScope unit="page" from="1586" to="1595" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mnasnet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2820" to="2828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Proxylessnas: Direct neural architecture search on target task and hardware</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00332</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Gap-8: A risc-v soc for ai at the edge of the iot</title>
		<author>
			<persName><forename type="first">E</forename><surname>Flamand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE ASAP</title>
		<imprint>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mcunet: Tiny deep learning on iot devices</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Once-for-all: Train one network and specialize it for efficient deployment</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Darts: Differentiable architecture search</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.09055</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">X-cube-ai</title>
		<author>
			<persName><surname>St Microelectronics</surname></persName>
		</author>
		<ptr target="https://www.st.com/en/embedded-software/x-cube-ai.html" />
		<imprint>
			<date type="published" when="2017-11">2017. Nov. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Gap8 nntool</title>
		<author>
			<persName><forename type="first">Greenwaves</forename><surname>Technologies</surname></persName>
		</author>
		<ptr target="https://greenwaves-technologies.com/manuals/" />
		<imprint>
			<date type="published" when="2019-11">2019. Nov. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1</title>
		<author>
			<persName><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02830</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep ppg: large-scale heart rate estimation with convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Reiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page">3079</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
