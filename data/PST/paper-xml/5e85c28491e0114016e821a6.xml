<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Code Prediction by Feeding Trees to Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-05-21">21 May 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Seohyun</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName><roleName>Facebook Inc</roleName><forename type="first">U</forename><forename type="middle">S A Jinman</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jinman</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Wisconsin-Madison, U.S.A. YUCHI TIAN</orgName>
								<orgName type="institution" key="instit2">Columbia University</orgName>
								<address>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Wisconsin-Madison, U.S.A</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Columbia University, U.S.A</orgName>
								<address>
									<settlement>Yuchi Tian</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Facebook Inc</orgName>
								<orgName type="institution">U.S.A</orgName>
								<address>
									<settlement>Satish Chandra</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Code Prediction by Feeding Trees to Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-05-21">21 May 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2003.13848v2[cs.SE]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We advance the state-of-the-art in the accuracy of code prediction (next token prediction) used in autocomplete systems. First, we report that using the recently proposed Transformer architecture even out-of-the-box outperforms previous neural and non-neural systems for code prediction. We then show that by making the Transformer architecture aware of the syntactic structure of code, we further increase the margin by which a Transformer-based system outperforms previous systems. With this, it outperforms the accuracy of an RNN-based system (similar to Hellendoorn et al. 2017b) by 37.0%, the Deep3 system (Raychev et al., 2016a) by 29.7%, and an adaptation of Code2Seq <ref type="bibr" target="#b7">(Alon et al., 2019a)</ref> for code prediction by 30.0%. These are significant margins.</p><p>We present in the paper several ways of communicating the code structure to the Transformer, which is fundamentally built for processing sequence data. The most effective of these is when we enrich the selfattention mechanism of the Transformer. We enable the mechanism to learn weights-that is, how much to focus on each preceding token in the input-not only on the basis of a token's value, but also on the basis of the spatial relationships, as in their positions in the abstract syntax tree, between each pair of tokens.</p><p>We provide a comprehensive experimental evaluation of our proposal, along with alternative design choices, on a standard Python dataset, as well as on a Facebook internal Python corpus. Our code and data preparation pipeline will be available in open source.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Last several years have witnessed exciting progress in the application of machine learning (ML) techniques to developer productivity tools <ref type="bibr" target="#b5">(Allamanis et al., 2018a)</ref>, and in particular, to code prediction <ref type="bibr" target="#b13">(Brockschmidt et al., 2019</ref><ref type="bibr" target="#b24">, Hindle et al., 2016</ref><ref type="bibr" target="#b28">, Li et al., 2018</ref><ref type="bibr">, Raychev et al., 2016a)</ref>. The idea of code prediction in general is to predict the next code element given previously written code.</p><p>Code prediction is commonly used in an IDE for autocomplete, where based on the developer's cursor position and the code already written up to the cursor position, the IDE offers the most likely next tokens (perhaps as a drop down list to choose from). Autocomplete, not only saves the developer from having to type in the next token(s), but is also an effective code learning mechanism: for instance, a developer might not know the name of an API call they need off the top of their head, but is able to choose among the choices shown by an auto-complete tool.</p><p>Consider the Python code fragment shown in Fig 1 <ref type="figure">.</ref> Suppose a developer has written code up to string following by a dot. At this point, it will be helpful for the IDE to prompt the developer with attribute names that are likely to follow, preferably, with atoi ranked at the top because in this case that is the correct next token.</p><p>A typical IDE that does not use ML-based autocomplete is unable to offer the intended completion among the top few choices. For instance, the autocomplete model used in the IDE (Jedi, which ranks the suggestions alphabetically) shown in Fig 2 ranks atoi fairly low, needing the developer to scroll a menu or type a prefix of the intended name.   Here, "atoi" is ranked low since the predictions are ordered alphabetically.</p><p>Our goal is to devise models that better leverage the code fragment and its context to provide a more accurate next token prediction. As we show later in the introduction, by training on existing code corpora, the models that we introduce are able to rank atoi as the first choice.</p><p>In this introductory section, we first discuss the related work focused on ML-based code completion, then give an overview of the models that we introduce, and finally give a preview of our key results and contributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related Work</head><p>Type-based Code Completion. Even early IDEs attempted to provide code completion features. For example, Eclipse<ref type="foot" target="#foot_0">2</ref> and Jedi<ref type="foot" target="#foot_1">3</ref> both provide type-based suggestions for next tokens. Type-based means that a compiler provides to the IDE a list of type-compatible names that could appear in the context. Generally these names were organized alphabetically, though people have tried sorting them by occurrence frequency or even applying per-user Bayesian optimization to rank the list <ref type="bibr" target="#b18">(DâĂŹSouza et al., 2016</ref><ref type="bibr" target="#b33">, Proksch et al., 2015)</ref>. However, a type-based autocomplete is not very effective. For example, as mentioned above, in Figure <ref type="figure" target="#fig_2">2</ref>, atoi is ranked low because Jedi ranks the autocomplete suggestions alphabetically. Additionally, for dynamic languages, it is extremely difficult to gather an accurate list of tokens that could occur in a context.</p><p>Machine learning methods hold promise to offer a better ranking of code completion suggestions provided to a user by learning the statistical property of code, exploiting naturalness <ref type="bibr" target="#b5">(Allamanis et al., 2018a)</ref>.</p><p>Traditional ML-based techniques for Code Completion. Some of the early ML models for code prediction relied on n-gram language models <ref type="bibr" target="#b24">(Hindle et al., 2016</ref><ref type="bibr" target="#b31">, Nguyen et al., 2013</ref>). An n-gram language model computes the probability of the next token given previous n tokens (n-grams) as context. Specifically, the model estimates these probabilities by tabulating the frequency of various n-grams in a code corpus used for training.</p><p>A different line of work carries out code prediction based on statistics (in the training corpus) on syntactic structure of code, as opposed to seeing code as text. These include probabilistic contextfree grammars <ref type="bibr" target="#b3">(Allamanis and Sutton, 2014)</ref> and probabilistic higher-order grammars <ref type="bibr" target="#b11">(Bielik et al., 2016a</ref><ref type="bibr">, Raychev et al., 2016a,b)</ref>. This class of models considers code artifacts as abstract syntax trees, and make their predictions based on information gleaned selectively across paths in the code's AST. We include the best-performing technique <ref type="bibr" target="#b35">(Raychev et al., 2016a)</ref> from this line of works in our evaluation. Their model, Deep3 , learns a decision tree model that uses this information essentially as features.</p><p>Deep learning techniques for Code Completion. With the advent of deep learning, many researchers have investigated using deep learning for code prediction, as it is able to capture longer dependency within a sequence. In this way, the model can make connections between non-consecutive tokens to provide a richer sense of context. The most common neural technique for code prediction is use Recurrent Neural Networks (RNNs) and their variants and feed as input code as a linear sequence. Used in this way, the RNN essentially learns a language model over the training corpus, and it is generally believed to do so better than n-grams <ref type="bibr" target="#b27">(Karampatsis et al., 2020)</ref>. Indeed, several papers in the literature, including work done in industrial contexts <ref type="bibr" target="#b10">(Aye and Kaiser, 2020</ref><ref type="bibr" target="#b34">, Raychev et al., 2014</ref><ref type="bibr" target="#b39">, Svyatkovskiy et al., 2019)</ref> has used RNNs. Attempts has been made on feeding RNNs (LSTMs) with serialized ASTs <ref type="bibr" target="#b29">(Liu et al., 2016)</ref>; accuracy is further improved by using more AST guided architectures <ref type="bibr" target="#b28">(Li et al., 2018</ref><ref type="bibr" target="#b30">, Liu et al., 2020)</ref>. We include a variation of an RNN (LSTM)-based model in our evaluation.</p><p>Among other flavors of code completion, such as where program after the predicting location is available <ref type="bibr" target="#b6">(Allamanis et al., 2018b</ref><ref type="bibr" target="#b9">, Alon et al., 2020</ref><ref type="bibr">, Brockschmidt et al., 2019</ref><ref type="bibr" target="#b34">, Raychev et al., 2014)</ref> or where the granularity of prediction is smaller (e.g. characters <ref type="bibr" target="#b12">(Bielik et al., 2016b)</ref> or subtokens <ref type="bibr" target="#b27">(Karampatsis et al., 2020)</ref>) or larger (e.g. sub-ASTs <ref type="bibr" target="#b9">(Alon et al., 2020</ref>)), we focus on predicting next token given only partial program up to the predicting location.</p><p>Transformers. Researchers in the natural language processing (NLP) community have recently developed Transformers, a new neural architecture for even more effective natural language processing <ref type="bibr" target="#b42">(Vaswani et al., 2017)</ref>. Transformers overcome a major drawback of RNNs' ineffectiveness in capturing long-term dependencies by solely relying on attention mechanisms -this way, any token from the sequence can more directly affect the next token prediction. Transformers have achieved or exceeded state-of-the-art results <ref type="bibr" target="#b16">(Devlin et al., 2018</ref><ref type="bibr" target="#b17">, Dong et al., 2019</ref><ref type="bibr" target="#b33">, Radford et al., 2019)</ref> for a variety of NLP tasks such as language modeling, question answering, and sentence entailment.</p><p>There has been a surge of interest since 2019 in extending Transformer models to handle beyond sequential structures for NLP <ref type="bibr" target="#b1">(Ahmed et al., 2019</ref><ref type="bibr" target="#b32">, Nguyen et al., 2020</ref><ref type="bibr">, Wang et al., 2019)</ref>. In the realm of learning over source code using transformers, it has been shown that taking tree structure into account helped code correction <ref type="bibr" target="#b20">(Harer et al., 2019)</ref> and code translation <ref type="bibr">(Shiv and Quirk, 2019)</ref>. A recent work on code summarization <ref type="bibr" target="#b23">(Hellendoorn et al., 2020)</ref> used a different kind of structural information-static analysis results-in conjunction with transformers.</p><p>There is practical interest in the topic of code prediction using transformers. Galois<ref type="foot" target="#foot_2">4</ref> is an open source project that uses <ref type="bibr">GPT-2 (Radford et al., 2019)</ref> for code completion. The approach is similar to our SeqTrans model, notwithstanding their use of non-standard tokenizer and a subtoken segmenter. TabNine™ published a blog post <ref type="bibr" target="#b40">(TabNine, 2019)</ref> in July 2019 mentioning the use of GPT-2 in their code completion but revealed no technical detail. We believe we are the first to systematically evaluate transformers for code prediction and compare them to previous work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep learning techniques over Code, beyond Code</head><p>Completion. There are many other uses of deep learning techniques for code, beyond code completion. These include techniques for code summarization <ref type="bibr" target="#b8">(Alon et al., 2019b)</ref>, bug finding <ref type="bibr" target="#b6">(Allamanis et al., 2018b)</ref>, repair <ref type="bibr" target="#b41">(Vasic et al., 2019)</ref> and many others, too numerous to list. An interesting aspect of this body of work is in the different ways in which they represent a program as an input to a neural architecture. These representations have ranged from linear token sequence (as for code prediction <ref type="bibr" target="#b21">(Hellendoorn and Devanbu, 2017a</ref><ref type="bibr" target="#b27">, Karampatsis et al., 2020</ref><ref type="bibr" target="#b28">, Li et al., 2018)</ref>), to paths in an AST <ref type="bibr" target="#b7">(Alon et al., 2019a</ref><ref type="bibr">(Alon et al., ,b, 2020))</ref>, and sometimes even ways to convey static analysis information to the neural network <ref type="bibr" target="#b6">(Allamanis et al., 2018b</ref><ref type="bibr">, Brockschmidt et al., 2019</ref><ref type="bibr" target="#b23">, Hellendoorn et al., 2020</ref><ref type="bibr" target="#b45">, Yang and Xiang, 2019)</ref>. We include an adaptation of path-based Code2Seq <ref type="bibr" target="#b7">(Alon et al., 2019a)</ref> in our evaluations and show that our models significantly outperforms Code2Seq .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Overview</head><p>The goal of this paper is to investigate the ways using the Transformer architecture to obtain the best possible accuracy for code prediction. We present our narrative roughly in the order in which we carried out this research. Do Transformers, taken out-of-the-box and processing code as text, work better than previous sequence-based models for code prediction?</p><p>For the first question, we simply compared a recent implementation of RNN (specifically an LSTM) with an implementation of a Transformer, passing along code as a token sequence to both. The results indicated that Transformers, even out-of-the-box, clearly outperform RNN-based models, as well as a couple of other state-of-the-art models (see Key Results later.) This gives us the green light to explore further. The next questions are motivated by previous work. <ref type="bibr" target="#b35">Raychev et al. (Raychev et al., 2016a)</ref> had found that-for the code prediction problem-a non-neural but tree aware engine could outperform RNNs. In the same spirit, Alon et al. <ref type="bibr" target="#b8">(Alon et al., 2019b)</ref> had found-for code summarization problem (though not for code prediction in their paper)-that embedding the AST structure of code even in limited ways vastly outperformed purely sequence-based methods. These, and other, papers suggest that when models know about the syntactic structure of code, they can do better than if they were to only know about the linear token sequence.</p><p>Can we utilize the Transformer better by exposing to it the syntactic structure of code?</p><p>The interesting question is how, because we cannot simply jam an AST into the Transformer, which is in the end a sequence processing device. There are two popular ways in which people have considered capturing the (partial) structure of an AST, in a way that is suitable for sequentialization: one, based on a tree traversal order, and the other, based on decomposing the tree into paths. The two models we consider are representative of each of these two ways, though design variations are certainly possible:</p><p>(1) TravTrans , which is based on a depth-first traversal, and (2) PathTrans , which is based on a path decomposition.</p><p>Our evaluation shows that incorporating syntactic structure of code does indeed perform better than processing code simply as text.</p><p>Can we extend the Transformer (no longer out-of-the-box) to even better utilize the syntactic structure of code? We propose a model TravTrans+ that communicates additional-in addition to the partial AST information already in one of the above two models-structure of the code directly to the selfattention mechanism of the Transformer. We call this adaptation of the Transformer as Transformer+; the sense in which we adapt the standard Transformer is to incorporate additional model parameters into the model (see Sec 2 for details.) TravTrans+ is built on top of TravTrans , though it could also have been built on top of PathTrans .</p><p>The key novelty of our work is this adaptation of the Transformer architecture to enable it to better learn the structural relationships in code. This is a way in which mechanisms in a deep neural network-here, additional model parameters in the self-attention mechanism-work in conjunction with information that comes from programming language processing-here construction of an AST. Our evaluation shows that this significantly improves the results, when compared against the Transformer models as well as models from previous work (SeqRNN <ref type="bibr" target="#b22">(Hellendoorn and Devanbu, 2017b)</ref>, Deep3 <ref type="bibr" target="#b35">(Raychev et al., 2016a)</ref>, Code2Seq <ref type="bibr" target="#b7">(Alon et al., 2019a)</ref>).</p><p>Fig 3 puts these models in perspective. Along the x-axis is an indication whether the model considers only source code tokens as text, or whether the model considers information gleaned from the AST of the program. Along the y-axis are various ML model families that are used to build the prediction models. We include in the figure three code prediction models from previous work.</p><p>The differences between the power of these models is evident even in our running example of Fig 1 <ref type="figure">.</ref> We show the results-in terms of rank (lower is better) of the correct prediction-in Table <ref type="table">1</ref>. As the table shows, TravTrans+ clearly outperforms the other code completion techniques. The paper is about the design of a system to get us there.</p><p>Scope of this paper. To bound the scope of this paper, we limit our investigation to techniques that do not require any compiler support beyond constructing the AST; thus, we exclude ways to communicate program analysis results such as def-use information, as in Allamanis et al. <ref type="bibr" target="#b6">(Allamanis et al., 2018b)</ref>.</p><p>We also limit our study to the Transformer architecture, and compare it to existing published methods for code completion on a common training and test corpus py150 ( 2016). We purposely exclude the graph neural networks (GNN), because (1) there is already an active debate between GNN and Transformer for NLP applications that transcends code completion, and (2) GNNs have  <ref type="table">1</ref>. Ranks for the predictions for the leaf nodes listed in Fig 4 <ref type="figure">.</ref> &gt;10 means the model did not get the right answer in the top 10 results. TravTrans+ is our most powerful model. also not been used for code completion before, so to adapt GNN for code completion would require a significant orthogonal effort.</p><formula xml:id="formula_0">5 Code2Seq &gt;10 1 1 1 &gt;10 &gt;10 &gt;10 1 1 2 &gt;10 PathTrans 10 2 1 1 8 1 2 1 1 1 &gt;10 Our SeqTrans &gt;10 1 1 6 &gt;10 &gt;10 1 10 1 1 &gt;10 work TravTrans &gt;10 1 5 1 4 1 1 1 1 1 &gt;10 TravTrans+ 3 1 1 1 1 1 4 1 1 1 1 Table</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Key Results and Contributions</head><p>In this paper, we show that our novel model TravTrans+ is able to significantly outperform several previous models for code prediction. We report results based on training and evaluating various models of code prediction on the py150 ( 2016) dataset. The reciprocal rank (reported as a percentage, see Sec 4) improves:</p><p>• from 36.4% to 73.6% when comparing an RNN implementation (SeqRNN ) vs. TravTrans+ ; • from 43.9% to 73.6% when comparing a non-neural tree based model (Deep3 <ref type="bibr" target="#b35">(Raychev et al., 2016a</ref>)) vs. TravTrans+ ; • from 43.6% to 73.6% when comparing Code2Seq <ref type="bibr" target="#b7">(Alon et al., 2019a)</ref>  Thus, we argue that our proposal of using Transformer+ with tree inputs for code prediction is both practical and surpasses previous work by a wide margin.</p><p>In summary, our contributions are as follows:</p><p>• We show that Transformers can provide significantly better accuracy on the code prediction task compared to previous state of the art approaches; and we show that communicating the syntactic structure of code to the Transformer is key to the large improvement in accuracy. • Our main technical novelty is in the design of methods to inform the Transformer about the syntactic structure of code. We do this both with and without augmenting the Transformer self-attention mechanism, and we found that the latter gives vastly superior results. • We provide a preliminary investigation into why the Transformer model that is aware of tree structure works better than one without, by using saliency maps <ref type="bibr" target="#b38">(Simonyan et al., 2014)</ref>. • We also evaluated our trained model on a dataset selected from a Python code repository internal to Facebook, and found the relative benefits of the Transformer models to be similar to those on the py150 dataset. This indicate that the relative advantage of Transformer models carries over to other datasets, though the absolute accuracy numbers can vary. We have made our model implementations available at https://github.com/facebookresearch/ code-prediction-transformer.</p><p>Outline. Sec 2 explains the models that we have used for our evaluation. This section gives a primer on the Transformer and describes how to communicate tree structure to the Transformer and Transformer+. Sec 3 provides a recap of the previous work, focusing on the ones against which we compare our models. Sec 4 describes our datasets and implementation. Sec 5 presents our quantitative results. Sec 6 takes a closer look into why our models worked well (or did not.) Sec 7 lists some threats to validity. We conclude the paper with our future work in Sec 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">MODELS</head><p>In this section, we give details to the models that we use for code prediction. We start by defining the code prediction task we examine in this work, as well as providing a unified view of our various solutions. This is followed by an brief introduction to the original Transformer model. We then describe various proposals of feeding sequence to Transformers and feeding ASTs to Transformers. The next section discusses the previous state-of-the-art methods of code prediction that we use for comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Code Prediction Task</head><p>Code prediction task studied in this work is to predict the next code unit given the static partial program up to the point of prediction. Let p * (unit | ctx) be the empirical distribution of code unit unit given the partial program observed from a large text corpus as context ctx. Our task is to learn to approximate p * using a machine learning model, here Transformers. More specifically, we device Transformer models to take in some representation of partial program and then yields a distribution of possible code units. The learned distribution can be viewed as</p><formula xml:id="formula_1">p(unit | ctx) = T rans(ctx; θ ),</formula><p>where θ represents the trainable parameters of the Transformer. We train the models by minimizing the KL-divergence between p and p * , or equivalently, minimizing the cross-entropy loss l at all code prediction locations.</p><formula xml:id="formula_2">minimize θ i l (T rans(ctx i ; θ ), unit i ) ,</formula><p>where (unit i , ctx i ) is the i-th pair of code unit and accompanying partial program representation from the training corpus. For the ease of notation, we will not explicitly write out the trainable parameter part of the models throughout the paper.</p><p>We explore several ways of representing partial program and predicting different kinds of code units. When using source code token as code unit and representing partial program as sequence of source code tokens (SeqTrans ), the problem aligns with the traditional notion of language modeling -predicting the next token in a sequence given all previous tokens: p(t i | t 1 , . . . , t i−1 ) where t i is the i-th token.</p><p>More interestingly, we explore various representations of a partial program to better utilize its AST information. The intuition is that the more we can utilize the syntactic information provided by the AST, the better we can predict the next token. As an example, in our TravTrans+ model, partial program is represented as a sequence of pre-order traversal of AST nodes, along with a matrix that captures the tree relation between each of the nodes. The challenge here is that because Transformers are originally designed as a sequential model, we have to propose innovative ways to convey AST information to Transformers, as we are about see in the rest of the section. Note that when representing partial programs with ASTs, the natural code unit becomes an AST node, which can only be viewed as a source token if it is a leaf.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">A Primer on Transformers</head><p>We start with a brief introduction of Transformers. Readers familiar with Transformers can skip ahead to Section 2.3.1.</p><p>Transformers belong to a class of deep neural networks that are designed for sequence processing. Transformers eschew the hidden states of earlier generation sequence networks in favor of exposing the entire input sequence simultaneously, solely relying on attention mechanisms. In Transformers, information from any previous location of the sequence can directly affect the encoding of the next token, through a mechanism called self-attention, which helps greatly improve the connectivity in long sequences. Transformers also uses multiple heads of these self-attention blocks, called multi-headed attention, which enables the model to simultaneously consider different ways of attending to previous information within one block and also across other blocks.</p><p>To be precise, a Transformer used in this word is a stack of Attention blocks (AttnBlock) preceded by an input embedding layer (Emb) and followed by a classification layer (Clsfr).</p><formula xml:id="formula_3">T rans(ctx) = Clsfr(AttnBlock(. . . (AttnBlock(Emb(ctx))) . . . ))</formula><p>where AttnBlock is repeated n block times. The classifier we use is a simple fully-connected layer followed by a softmax that converts internal representation into a probabilistic distribution over possible code unit as output. The embedding layer maps the partial program ctx into vector forms. The exact embedding behaviour depends on the way we represent partial program and will be described in more detail in the following subsections. Because we are going to talk about different ways of representing partial programs and subsequently different embedding schemes, for the ease of notation, we use T rans to also mean the part of Transformer after the embedding layer. However, it is not to be confused that the embedding layer is always trained together with the rest part of the model.  <ref type="table">3</ref> for where the numbers come from. Transformer+ takes a relations matrix as another input to enhance the self-attention mechanism of the Transformer model. The matrix R contains information on the relations between the tokens in the input sequence (in TravTrans+ , the information is the path between two nodes in the AST.)</p><p>Attention blocks constitute the main body of a Transformer. An attention block is further composed of a self-attention layer (AttnLayer) and a feed-forward layer (FF), applied sequentially with skip-connections (+) and layer normalizations (LN). Given H as the internal representation obtain from the previous block or the embedding layer,</p><formula xml:id="formula_4">AttnBlock(H ) = LN(FF(A) + H ) where A = LN(AttnLayer(H ) + H ).</formula><p>The self-attention layer is the crux of the model. The intuition here is to attend the elements in the input sequence differently according to their relevance to the predicting location. Given E as the representation from the previous layer, we first create query Q, key K, value V matrices as</p><formula xml:id="formula_5">Q = EW q , K = EW k , V = EW v ,</formula><p>where W q ,W K ,W v are trainable parameters. The self-attention then works as "querying keys using queries and then using the result to summarize values" through the attention function:</p><formula xml:id="formula_6">Attn(Q, K, V ) = softmax QK ⊤ √ d k V</formula><p>where d k is the dimension of key vectors. An operational example of self-attention for our SeqTrans model is presented in Sec 2.3.1. For other details (especially on the multi-head attention), please refer to <ref type="bibr" target="#b42">Vaswani et al. (2017)</ref> and in particular, <ref type="bibr">GPT-2 (Radford et al., 2019)</ref>, for a more thorough description.</p><p>The next sections discuss various ways of feeding code fragments into this Transformer architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Feeding Sequence to Transformers</head><p>2.3.1 SeqTrans . Our first attempt is to apply a Transformer over source token sequences, which can be easily obtained by applying a tokenizer. Here the code unit is a source code token and the partial program is represented as source token sequences. This is a straightforward application of the original Transformer design, and as well functions as a baseline for our later attempts that take more AST information. The model can be written as</p><formula xml:id="formula_7">o = T rans (e t ) t ∈src_seq</formula><p>where o is a distribution over all possible tokens, and e t is the embedding for source token t for every t in the partial program represented as a source token sequence src_seq. It does next token prediction by taking all preceding source tokens, up to the point of prediction, as input. This setting manifests the same inputs and outputs as the baseline RNN model (see SeqRNN in Sec 3.1). As we show in the experiments, SeqTrans turns out to be an already strong model as a direct comparison to SeqRNN .</p><p>Operational Example of Self-attention for SeqTrans . We illustrate the operation of the selfattention part of SeqTrans by taking an example input sequence: ["map", "(", "string", ". "], and the target token being "atoi. " This input sequence is first fed through the initial embedding layer to give: E = e map , e ( , e string , e . . Then, this embedding is used as input to three fully-connected networks (W q ,W k ,W v ) to create a query, key, and value embedding for the sequence:</p><formula xml:id="formula_8">Q = EW q , K = EW k , V = EW v</formula><p>In our example, Q = q map , q ( , q string , q . , K = k map , k ( , k string , k . , and V = v map , v ( , v string , v . . We use the query vector Q to "query" the "keys" K to see which token relationships are the most important by calculating QK ⊺ . This results in a matrix of size n ×n, as seen in Table <ref type="table">2</ref>, where n is the length of the input sequence. Each row is then normalized (by square root of d k ) and passed through a softmax layer so all the scores are positive and add up to 1. Table <ref type="table">3</ref> shows an example of the self-attention weights 5 ; looking at the last row, we can see that most of the self-attention is given to ". ", meaning it has a greater factor in predicting the next token "atoi". Also note how the matrix is a lower triangular matrix -this is because self-attention cannot be applied to tokens that have not been seen before. Finally, this matrix is multiplied with the value vector to weight the token embeddings A = Attn(Q, K, V ). In our example, A = 0.2 * v map , 0.1 * v ( , 0.2 * v string , 0.4 * v . . A is then fed through a fully-connected network, coupled with skip connections and layer normalizations. This process is repeated n block times. Finally, the output of the last block goes through a classification layer at the end to generate predictions for the next token.</p><p>In the next subsection, we will vary the inputs and the outputs to the transformer, but the principles of operation will remain the same as in SeqTrans .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Feeding Trees To Transformers</head><p>The question that interests us is: can Transformer-based models also benefit from syntactic structure, and if so, how can we communicate the syntactic structure to Transformer?</p><p>In this section, we explain the models of our own creation: PathTrans , TravTrans , Trav-Trans+ . As mentioned in the introduction, PathTrans is a particular instance of a path-based sequentialization of a tree; and TravTrans is a particular instance of a traversal-based sequentialization of a tree. TravTrans+ augments the self-attention mechanism of the Transformer to capture additional tree structure.</p><p>... map ( string . map q map k map ( q ( k map q ( k ( string q st r inд k map q st r inд k ( q st r inд k st r inд .</p><p>q . k map q . k ( q . k st r inд q . k .</p><p>Table <ref type="table">2</ref>. Matrix for calculating the self-attention "scores" for each token combination in the input sequence for Transformers. We use the query vector Q to "query" the "keys" K to see which tokens are the most relevant for the next token prediction. The matrix multiplication is calculated with QK ⊺ .</p><p>... map ( string . map 0.9 ( 0.6 0.1 string 0.1 0.1 0.7 . 0.2 0.1 0.2 0.4</p><p>Table <ref type="table">3</ref>. Example matrix for the numerical self-attention "scores" after taking the softmax over the normalized values in Table <ref type="table">2</ref>. Note that the rows listed here do not sum up to exactly 1 since there are previous tokens in the input sequence (not shown in this matrix) that self-attention gives scores to as well. In PathTrans , we present the partial program as all AST nodes up to the node to be predicted, with the points of prediction being the leaf nodes of the AST. Thus, the loss is taken over only the leaf AST nodes, and it predicts only leaf tokens.</p><p>In TravTrans and TravTrans+ , the code unit to be predicted is every AST node, and we present the partial program as all AST nodes up to the node to be predicted. Consequently, the code prediction objective is defined over all AST nodes. This means that loss is taken over all AST nodes; and that both TravTrans and TravTrans+ predicts leaf tokens and internal tokens. The root-paths are first fed into a sequence encoder (in our case an LSTM<ref type="foot" target="#foot_3">6</ref> ), coupled with the leaf node, and is fed through the Transformer:</p><formula xml:id="formula_9">o = T rans e t + p t t ∈leaf _seq</formula><p>where o is a distribution over all possible leaf nodes, and e t is the embedding for AST node t and p t = LSTM (e u ) u ∈Rootpat h(t ) is the summarized representation by an LSTM of root-path from t for every leaf node t in the leaf sequence leaf _seq. + is used here following the convention of Transformer computations and to keep the embedding dimension the same for every components. The hope here is that the root-paths captures the local syntactical information and thus can help the prediction of leaf tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2">TravTrans .</head><p>As a Transformer naturally only take a sequence as input, we provide the AST nodes as a sequence in pre-order traversal, or a depth-first-search (DFS) order. For Fig 4, for node 29, the previous nodes in DFS order would be: [..., "Call", "NameLoad", "map", "AttributeLoad", "NameLoad", "string", "Attr"] (also seen in the top part of Fig 7 <ref type="figure">.</ref>)</p><p>The TravTrans model can be written as:</p><formula xml:id="formula_10">o = T rans (e t ) t ∈AST _seq</formula><p>where o is a distribution over all possible tokens, and e t is the embedding for AST token t for every t in the partial program represented as a AST token sequence AST _seq in DFS order.</p><p>TravTrans presents the tree nodes in a pre-determined order, but still does not retain detailed structural relationship between nodes. For example, consider the sequence of nodes 26 -28 in Fig 4 <ref type="figure">.</ref> This would be represented as ["NameLoad", "string", "attr"], the three nodes appearing consecutively in DFS order. Looking at the AST, we can see that the relations between ("NameLoad" &amp; "string", and "string" &amp; "attr") are actually quite different: "NameLoad" is one node up from "string", while "string" is two nodes up and one node down from "attr". This path-based relation between the nodes provides richer information about the actual structure of the tree.</p><p>While TravTrans itself shows relatively small improvement on SeqTrans (Table <ref type="table">5</ref>), it allows us to augment it with the richer information indicated above, leading to the TravTrans+ model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.3">TravTrans+ .</head><p>We extend the TravTrans model to incorporate even more tree structure. Specifically, given any two nodes a and b in the AST, we want to capture the (unique) path needed to reach from a to b, and communicate this to the Transformer. The path from a to b is represented abstractly only in terms of up and down moves:</p><formula xml:id="formula_11">UDpath(a, b) = U i D j</formula><p>where i, and j are the number of up and down nodes, respectively, node a has to travel to reach node b.</p><p>We create a matrix R ∈ R n×n that contains the one-dimensional embedding of UDpath(a, b) for each pair of nodes (a, b) from the partial program of n AST nodes. Fig <ref type="figure" target="#fig_8">7</ref> shows an example of a matrix R for nodes 22-29 in the AST. Note that QK ⊤ it the key mechanism of a Transformer to attend to relevant information from previous tokens. By introducing R to the process, we provides a way for the model to learn the strength of the attention to pay to previous tokens, considering the AST relationship between pairs of nodes as well. Specifically, we replace the Attn function with the following Attn TreeRel function.</p><formula xml:id="formula_12">Attn TreeRel (Q, K, V , R) = softmax R ⊙ (QK ⊤ ) √ d k V (<label>1</label></formula><formula xml:id="formula_13">)</formula><p>where ⊙ is element-wise product. This idea of incorporating R into the Attn TreeRel function is inspired by <ref type="bibr" target="#b37">Shaw et. al ( 2018)</ref>. where o is a distribution over all possible tokens, and e t is the embedding for AST token t for every t in the partial program represented as a AST token sequence AST _seq in DFS order.</p><p>Note that this method of using Transformer+ to enhance the self-attention mechanism is a technique not limited to only enhancing TravTrans . Indeed, similar relationship information could have been used to enhance PathTrans . However, as PathTrans and TravTrans perform comparably (see Sec 5), we focused our enhancement on TravTrans .</p><p>To recap, our key insight-one that leads to the best accuracy-is to fortify the selfattention mechanism of the Transformer to enable it to learn weights on the basis of AST relationships between tokens as well.</p><p>The next section will cover three previous models from literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXISTING MODELS FOR COMPARISONS</head><p>In this section, we give details to three previous works for code prediction. We chose Deep3 <ref type="bibr" target="#b35">(Raychev et al., 2016a)</ref> as it is a state-of-the-art technique (numbers validate this) and it also feeds code as ASTs into an ML model (decision tree). We chose SeqRNN as a close competitor as a representative of recent RNN-based methods used in recent papers for code prediction (Aye and Kaiser, 2020, Hellendoorn and Devanbu, 2017b, <ref type="bibr" target="#b27">Karampatsis et al., 2020</ref><ref type="bibr" target="#b34">, Raychev et al., 2014</ref><ref type="bibr" target="#b39">, Svyatkovskiy et al., 2019)</ref>; this model works on token sequences. We also include Code2Seq <ref type="bibr" target="#b7">(Alon et al., 2019a)</ref> to compare our efforts against a popular code embedding technique that works on ASTs, albeit one that is not meant for next token prediction; interestingly though, it performed comparably to Deep3 on the code prediction task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">SeqRNN</head><p>For next token prediction, a popular method is to feed the source sequence tokens into an RNN (or LSTM) (Aye and Kaiser, 2020, Hellendoorn and Devanbu, 2017b, <ref type="bibr" target="#b34">Raychev et al., 2014</ref><ref type="bibr" target="#b39">, Svyatkovskiy et al., 2019)</ref>. In this way, we create a language model that computes the probability of the next word w t +1 , given some window of preceding words: P(w t +1 |w t w t −1 w t −2 . . .). <ref type="foot" target="#foot_4">7</ref>As the name suggests, RNNs consume input tokens recurrently, one per time step, and produce output tokens one per time step as well. The bottom layer of the RNN embeds input tokens into a vector: x t = emb(w t ), where w t is the source token seen at the t'th time step. The hidden state h t +1 at the (t + 1)-th time step is computed as h t +1 = rnn (x t , h t ), where rnn is the trainable RNN unit. The output is a vector of probabilities of various tokens computed by using a fully-connected layer followed by a softmax over h t 's; the diagram shows the top-ranked predictions or the ground truth.</p><p>The pertinent point to note is that the hidden state h t encodes the knowledge of not just the current token, but of last several previous tokens via the propagation of information in previous hidden states.</p><p>A limitation of RNNs is the difficulty they have in tracking long-range dependence, even with various proposals to mitigate the problem (e.g. long-short-term-memory (LSTM) cells <ref type="bibr" target="#b25">(Hochreiter and Schmidhuber, 1997)</ref>, which we do use in our implementation, attention on top of RNNs <ref type="bibr" target="#b26">(Iyer et al., 2016)</ref>, and skip-connections between sequence locations <ref type="bibr" target="#b43">(Vinyals et al., 2015)</ref>).</p><p>In our experiments, we feed the source code tokens into an LSTM and call this model SeqRNN .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Deep3</head><p>Raychev et al. 2016a presented a system, Deep3, based on a learned decision tree combined with count-based probabilities at the leaves of the decision tree. We provide only a sketch here, highlighting how they use paths on an AST.  A TGEN program is learned-on a specific corpus-by a genetic search procedure that simultaneously selects paths and grows the decision tree from the training data, with an entropy minimization objective. The details are not important for this paper; in this paper, we use their pretrained model (pho, 2017) as well as their Python dataset (py1, 2016) for our experiments.</p><p>The reader will notice that the notion of UDpath in Section 2.4.3 is akin to the AST paths expressed in TGEN programs. The paths in TGEN are more general, but at a high-level, the idea that certain "spatial" relation between nodes is important is common to both approaches. This, along with the competitive quality of results of the Deep3 model in Table <ref type="table">1</ref>, makes it an interesting comparison. We explore this similarity further in Sec 6.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Code2Seq</head><p>Code2Seq is a model by <ref type="bibr" target="#b7">Alon et al. 2019a</ref> that embeds code snippets by embedding AST paths in a neural network.</p><p>At a high-level, given an AST, Code2Seq creates path representations for all leaf-to-leaf paths. In Fig <ref type="figure">4</ref>, the nodes at the bottom are leaf nodes -terminal nodes with no children. For example, Fig <ref type="figure" target="#fig_11">9</ref> shows three paths for nodes 22-29 from the full AST. For each path, a path representation is created containing three parts: 1. tokenized starting leaf value, tokenized by snake and camel case, 2. the path itself, and 3. tokenized ending leaf value. A path representation is embedded in the following way: the starting and ending tokens are embedded using LSTMs, the path representations are embedded using bi-directional LSTMs, then these three embeddings are concatenated and then fed through a feed forward network. Finally, all of the path represention embeddings in the AST are combined using a simple attention mechanism. This final outcome embedding aims to capture the semantic meaning of the entire AST. A decoder is then used to generate target method summarization.</p><p>The original task of Code2Seq was method summarization: given a method body, how well can Code2Seq generate the correct method name? The training proposed in <ref type="bibr" target="#b7">(Alon et al., 2019a)</ref> is not well suited for next token prediction. In code summarization, a set of leaf-to-leaf paths needs to be created one time for a method. By contrast, in code prediction, a new set of leaf-to-leaf paths has to be created for each point of prediction.</p><p>For example, to predict atoi (node 29) in Fig <ref type="figure">4</ref>, we must first create an representative embedding for the partially completed AST up to node 29. To do so, all leaf-to-leaf paths available up to node 29 are used. Paths that end in atoi are also used, with atoi replaced with a placeholder token to prevent information leak. Paths 2 and 3 in Fig 9 <ref type="figure">includes</ref> placeholder. The representative embedding is then fed through a classification layer to generate predictions (note that this is different than the generative decoder that Code2Seq uses.)</p><p>By treating each point of prediction as a separate data point (compared to a language model, where one sequence is considered one data point), the number of training data points, along with the effort to create them makes Code2Seq computationally very expensive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">IMPLEMENTATION AND DATASETS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>We train our models using the py150 dataset (py1, 2016) used in <ref type="bibr" target="#b35">Raychev et al. (2016a)</ref>. The dataset consists of 150k Python 2 source code files from GitHub repositories, along with their parsed ASTs, split into 100k for training and 50k for evaluation. From the ASTs extracted from the py150 dataset, we modify the AST to ensure that the internal nodes only has types and the leaf nodes only have values. For implementation details, please refer to AppendixA.1. To incorporate large trees (greater than 1000 nodes), we deploy a technique adopted by <ref type="bibr" target="#b2">(Al-Rfou et al., 2018)</ref>, which slices a large tree into shorter segments with a sliding window to maintain part of the previous context. For implementation details, please refer to AppendixA.2.</p><p>We evaluate our models on two evaluation datasets:</p><p>• py150: We use the evaluation dataset used in <ref type="bibr" target="#b35">Raychev et al. (2016a)</ref>, which consists of 50k Python ASTs. We perform the two modifications as listed above before feeding them into our models, there are 16,003,628 leaf nodes. • internal: We also created an evaluation dataset consisting of 5000 Python files from a code repository internal to FAcebook. With this dataset, we can evaluate how our trained model can generalize to a different dataset, even if the code comes from disjoint projects. After the modifications, there are 1,669,085 leaf nodes. Recent works <ref type="bibr">(Hellendoorn and</ref><ref type="bibr">Devanbu, 2017a, Karampatsis et al., 2020)</ref> have divided evaluations into static and dynamic, where in the dynamic evaluations, the model continues to update its parameters during evaluation. This may increase accuracy by having the model adapt to the characteristics of the evaluation dataset. In our experiments, we choose to evaluate statically, and realize that evaluating dynamically may improve accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation</head><p>Transformers. For the models that use Transformers (SeqTrans , PathTrans , TravTrans , TravTrans+ ), we adapt the Pytorch implementation<ref type="foot" target="#foot_5">8</ref> of <ref type="bibr">GPT-2 small (Radford et al., 2019)</ref>. We use six Transformer blocks, six heads in each block, number of context = 1000, and embedding dimension = 300. We borrow other hyperparameters from <ref type="bibr" target="#b33">Radford et al. (2019)</ref>. We limit the token vocabulary size to 100k, which covers over 90% of the tokens used in the training dataset. For TravTrans+ , we limit the vocabulary to 250, which covers over 95% of the path relations. For PathTrans , we limit the maximum length of the path from leaf node to root to be 13, which covers over 90% of the nodes. For any path longer than 13, we keep the nodes closest to the leaf, and truncate the nodes near the root.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RNN.</head><p>For the SeqRNN model, we adapt the PyTorch example implementation<ref type="foot" target="#foot_6">9</ref> of a word-level language model LSTM. We use embedding dimension d model = 300, with dropout = 0.5 and n_layers = 1. We limit the token vocabulary size to 100K, which covers over 90% of the tokens.</p><p>Code2Seq . For the Code2Seq model, we used a PyTorch adaptation of the publicly released model<ref type="foot" target="#foot_7">10</ref> , using the same hyperparameters, except changing the vocab size to 100k for consistency with the other models. For selecting 200 (max number of paths) paths per AST, we first picked paths that ended with the target (to maximize the amount of local context). Since for each prediction point in the AST, a new set of leaf-to-leaf paths have to be generated, the data processing for Code2Seq takes a substantial amount of time (magnitude of days worth of time). For convergence, Code2Seq took 10 epochs, with each epoch taking 3.5 hours.</p><p>We trained all models on Nvidia Tesla V100 (using 4 GPUs at a time) until the loss converged, with all of the parameters randomly initialized. We used the Adam optimizer with the learning rate set to 1e-3. Table <ref type="table">4</ref>. Implementation details for all the models -number of epochs until convergence, training time (minutes per epoch), inference time (to evaluate over the py150 dataset), model size. Note that some information about Deep3 is not available since the authors have shared only the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep3</head><p>. For the Deep3 model, since the authors have shared only the model and not the training algorithm, we used the model pretrained on py150.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation Task</head><p>We evaluate the models on next token prediction for the leaf tokens. Since the leaf tokens in an AST has a direct correspondence to tokesn in the source code, comparison across both representations of code can easily be made. We report numbers for all leaf token predictions, as well as breaking down into more interesting categories: attribute access, numeric constant, name (variable, module), and function parameter name.</p><p>To measure performance on these tasks, we use mean reciprocal rank (MRR). The rank is defined as</p><formula xml:id="formula_14">MRR = 1 n n i=1 1 rank i (2)</formula><p>where n is the number of predicting locations and rank i is the rank of the correct label given by the model for the i t h data point. We present MRR as a percentage, in keeping with prior work <ref type="bibr">(Hellendoorn and</ref><ref type="bibr">Devanbu, 2017a, Karampatsis et al., 2020)</ref>. While Acc@1 only gives score when the correct label is ranked at the top, MRR also give scores when the true label is not ranked as the top, but among top few prediction. Comparing to the hit-or-miss style metric (Acc@1), this is closer to the realistic scenario when completion suggestions are presented to developers. With this practical perspective and for ease of computation, we only consider rank i ≤ 10 for each location i (all rank i &gt; 10 will have a score of 0). We will share our data processing scripts and model implementation. We share our data processing scripts and model implementations at https://github.com/facebookresearch/code-prediction-transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EVALUATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Research Questions</head><p>At a high level, we want to answer the following research questions. RQ1 Overall, do Transformer-based models provide better accuracy compared to prior state-ofthe-art methods of code prediction? RQ2 Does syntactic structure of code help get better accuracy out of Transformers, and if so, by how much? We describe the experiments to answer the research questions RQ1 and RQ2. For RQ1, we ask two specific questions: RQ1.1 Is the Transformer-based model more accurate than the RNN-based model?</p><p>To answer this question, we compare SeqRNN against SeqTrans . RQ1.2 Are the Transformer-based models more accurate than prior state-of-the-art work?</p><p>To answer this question, we compare three models from previous work: SeqRNN , Deep3 , and Code2Seq , with our models: PathTrans , TravTrans , and TravTrans+ . For RQ2, we ask two sub-questions: RQ2.1 Does a Transformer model based on an AST outperform a Transformer model that takes the corresponding source token sequences?</p><p>To answer this question, we compare SeqTrans against our two models that incorporate syntactic structure: PathTrans and TravTrans . RQ2.2 Does providing even more detailed structural information help with accuracy?</p><p>To answer this question, we compare TravTrans against its direct enhancement model, TravTrans+ , which incorporates more explicitly provided structural information. We also compare PathTrans its variant that strips away the syntactic information and TravTrans+ to its variant that enriches the up-down path vocab.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>Our main evaluation results are reported in Table <ref type="table" target="#tab_4">5 and Table 6.</ref> Q1. Yes, we have found that Transformer-based models to provide better MRR than prior stateof-the-art methods for code prediction.</p><p>For RQ1.1, see the SeqRNN and SeqTrans columns in Table <ref type="table">5</ref> and Table <ref type="table" target="#tab_4">6</ref>. For the py150 dataset, we can see a significant improvement in MRR for predicting all leaf tokens, from 36.6% to 50.1% for the SeqRNN and SeqTrans models, respectively. The same holds for comparing on the internal dataset: 23.8% vs 36.5%. Consistent improvements can be seen for specific types of leaf tokens.</p><p>For RQ1.2, we compare Deep3 and Code2Seq against PathTrans , TravTrans and Trav-Trans+ models. Overall, we found that all three models, along with SeqTrans , achieve better scores than both Deep3 and Code2Seq for all leaf tokens as well as for specific types of leaf tokens. Our best performing model, TravTrans+ , improves Deep3 's MRR by 29.7% (from 43.9% to 73.6%), and Code2Seq 's MRR by 30.0% (from 43.6% to 73.6%). Similar results can be seen for the internal dataset, as shown in Table <ref type="table" target="#tab_4">6</ref>. Q2. Yes, we have found that incorporating syntactic information of code significantly improves MRR.</p><p>To answer RQ2.1, we compare SeqTrans against PathTrans and TravTrans . Table <ref type="table">5</ref> shows that PathTrans outperforms SeqTrans by 5.0% and TravTrans outperforms SeqTrans by 7.9% for predicting all leaf tokens. SeqTrans performed better for predicting function parameter names than PathTrans (66.2% vs. 65.8%), but it is by a non-significant margin. Similar results can be seen for the intern dataset. These results demonstrate that even with using the same Transformer model, we are able to see improvement by providing more syntactical information as input to the model.</p><p>For RQ2.2, we compare the results between TravTrans and TravTrans+ , as TravTrans+ is a direct extension to TravTrans that incorporates even more tree structure. Table <ref type="table">5</ref> shows significant improvements to the accuracy, achieving 15.6% higher MRR for all leaf token prediction. Similar trends can be seen for the internal dataset in Table <ref type="table" target="#tab_4">6</ref>. Overall, TravTrans+ was the best performing model of all the models presented in this paper.</p><p>Next, we compare PathTrans with its variant that only takes the leaf nodes as input to determine the importance of root-path information (which provide syntactical information). In  Table <ref type="table">7</ref>. MRR of the alternate models and variations of models for py150. PathTrans is compared against a variant that remove root-paths from the input, and TravTrans+ is compared against its variant that contains a richer up-down path vocab. in MRR (55.1% vs 41.9% for all leaf nodes). This shows that the information captured by the leaf to root paths gives a solid boost to MRR. Finally, we compare TravTrans+ with its variant that uses a richer vocabulary to the up-down paths to include some child index information, as it provides extra information about the tree structure. In this model, we expand the down D vocabulary with D last , D middle ; this describes whether the node is either the first child, the last child, or somewhere in in between, respectively. We chose this minimal extension to limit the possible exponential growth in path vocabulary size; even with this minor extension, our path vocabulary increases from 250 to 100k to cover more than 90 % of the vocab (with a long right tail). The rest of the model is the same as TravTrans+ . Table <ref type="table">7</ref> shows that this variant did not outperform TravTrans+ , which shows that simply expanding the up-down vocab may not be the right approach in exposing child index information to the model. Areas of explorations may include whether a vocabulary size of 100k is too sparse for the models to learn effectively, or whether child indices are inherently not as crucial for code prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">MODEL INSPECTION</head><p>In this section, we study the internal behaviour of our two best-performing models -TravTrans and TravTrans+ , in the hope to understand the reason of their good accuracy at code prediction. The research questions we would like to explore here are: What did the Transformer models learn from the code? Did the features they learn aligns with our knowledge or what previous models have learned about code?</p><p>Specifically, we study how each input token attributes to the models' predictions (Sec 6.1) and which UDpaths are learned to be important by TravTrans+ (Sec 6.2). For the former, we found that both TravTrans and TravTrans+ have learned to attribute the prediction to relevant previous token. Furthermore, TravTrans+ tends to attribute more towards terminal values relevant to the predicting value, providing a possible reason for its better accuracy. For the latter, we found that TravTrans+ has learned high weight for UDpaths that correspond to several meaningful local syntactical context. We further found similarities between highly weighted UDpaths by TravTrans+ and heavily utilized TGEN paths by Deep3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Attribution Analysis</head><p>We try to understand the reason of our learned TravTrans and TravTrans+ models' prediction by looking into how they attribute their predictions to input tokens. The attribution is computed by taking the partial derivative of the loss function with respect to each input token embedding (saliency maps <ref type="bibr" target="#b38">(Simonyan et al., 2014)</ref>). Fig 10 visualizes the magnitudes of the gradients falls at each input token when the model predicts a particular output. Intuitively, the larger the value for a particular token, the more sensitive the output is to the variations at that token.</p><p>Examining the saliency maps for TravTrans and TravTrans+ , we first observe that that parent node of the AST (the internal node right above the leaf) is generally important for both models. On an orthogonal note, we also observe that for many predicting locations, the magnitude of gradients are very small, suggesting the robustness of the model in the sense that it is less sensitive to minor perturbations of the input sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Learned Weights for UDpaths</head><p>TravTrans+ learns weights for various UDpaths between a node and other nodes in its context as a component of self-attention. In this part, we inspect the learned weights for UDpaths in the TravTrans+ model in order to understand which UDpaths are the most important for the model's prediction.</p><p>There are six attention layers and six attention heads within each layer in TravTrans+ . All of them collectively determine the importance of each previous node in the prediction of the next token. We look into the maximally and minimally weighted UDpaths at each attention head. The results are shown in Fig 11 <ref type="figure">.</ref> Presumably, the extreme-weighted UDpaths are the most salient features for the model's prediction. The more extreme the weight is, the more conspicuous the path is among other paths for the particular head.</p><p>For example, we found that U 1 , U 1 D 1 , U 1 D 2 , U 2 and U 2 D 2 are important across multiple heads. U 1 , U 1 D 1 and U 12 D 1 are particularly up-weighted by some heads; while U 1 D 1 , U 1 D 2 , U 6 D 17 and U 1 are particularly down-weighted by some heads. The frequent presences of U 1 , U 1 D 1 and U 1 D 2 suggest the importance of syntactical local context in the next value prediction. The extreme weights of very long paths, e.g. U 12 D 1 , is at first baffling. However, we found cases where they can be useful in, for example, referring to class names or to related variable names under similar scopes (see examples in Appendix C).</p><p>Comparing to Deep3. As mentioned in Sec ??, Deep3 also relies on the values collected by their tree-walk programs (in TGEN, see Fig <ref type="figure" target="#fig_10">8</ref>) executed over ASTs.</p><p>Deep3's TGEN programs are strictly more expressive than our UDpaths, which are based on only up and down counts. However, for many of the tree walks, we can find corresponding UDpaths that represent the same movement in an AST. For example, TGEN expression [Up][Up][WRITE_TYPE] is similar to our U 2 . WRITE is disregarded as our models naturally have access to the values associated at the destination. We collected the most frequently used TGEN's tree-walk expressions when Equivalent UDpath Count U 1 1.8 × 10 7 U 2 D 1 4.7 × 10 6 U 3 4.2 × 10 6 U 2 3.4 × 10 6 U 4 3.0 × 10 6 U 2 D 2 2.9 × 10 6</p><p>Table <ref type="table">8</ref>. Top UDpath-convertible tree-walks used by E13 when predicting values over py150. evaluating their model (E13) over the py150 testing set. Table <ref type="table">8</ref> lists the top equivalent UDpaths and their counts, assuming the node to be predicted is a leaf with a left sibling leaf. We found that U 1 , U 2 and U 2 D 2 are at the both extremely weighted by many heads in our TravTrans+ and heavily utilized in Deep3. However, some of the potentially useful UDpaths heavily used by Deep3 are not often extremely weighted by TravTrans+ . For example U 3 , potentially useful for knowing the scope of the value to be predicted, only appears once as the maximally value in layer 5, head 5 of TravTrans+ (Fig <ref type="figure" target="#fig_1">11a</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">THREATS TO VALIDITY</head><p>SrcRNN Implementation. Our SrcRNN implementation is based on a PyTorch implementation<ref type="foot" target="#foot_8">11</ref> whereas related papers have generally built off of a Tensorflow implementation.<ref type="foot" target="#foot_9">12</ref> As the hyperparameters were similar (dropout = 0.5, num_layers = 1, hidden_size = 512 vs 300) to recent publications, we do expect our implementation to be comparable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BPE.</head><p>We have not integrated byte-pair encoding (BPE) <ref type="bibr" target="#b27">(Karampatsis et al., 2020)</ref> into our RNN model. We expect BPE to benefit both RNN and transformer models, and plan to explore this in future work.</p><p>Training Corpus. While larger Python corpora have appeared, py150 is still sizable at 500MB; we do not expect the larger corpora to reverse our findings.</p><p>Python specificity. We have only carried out evaluations on Python, and have not demonstrated that our results would carry over (in trends) to other languages. The Deep3 paper did find their results (in trends) to roughly carry over from Python to JavaScript.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION AND FUTURE WORK</head><p>In this paper, we presented ways to using the Transformer model for code prediction. We showed that while a straightforward use of the Transformer already outperforms existing models for code prediction, when supplied with code's structural information, surpasses existing models by a wide margin. Our most effective model uses the Transformer's self-attention mechanism to better enable it to learn the important spatial relations between AST nodes that matter for the purpose of code prediction. We also carried out a model interpretability study to examine where the richer model does better than the more basic model. In all, our work is a powerful argument for using Transformer models, and furthermore, for feeding trees to Transformers.</p><p>There are several avenues of future work that we intend to pursue.</p><p>Handling Out-of-Vocabulary Words. Source code presents a difficulty shared with natural language processing in handling large vocabularies and rare words. The token/word to be predicted in test data may not appear in the training data. This is even more challenging when predicting identifiers, such as method names, variable names, and so on, as developers can come up with arbitrary identifier names. Possible mitigation includes copying mechanism <ref type="bibr" target="#b4">(Allamanis et al., 2016</ref><ref type="bibr">, Brockschmidt et al., 2019</ref><ref type="bibr" target="#b19">, Fernandes et al., 2019)</ref> and open-vocabulary models <ref type="bibr" target="#b14">(Cvitkovic et al., 2019</ref><ref type="bibr" target="#b27">, Karampatsis et al., 2020)</ref>.</p><p>Exposing Tree Structure even more completely. We saw significant improvement in performance by providing more tree structure (TravTrans vs TravTrans+ ). Our attempt at DFSud+ , a variation to TravTrans+ that enhances the path relation vocabulary, did not improve performance. This leaves open the possibility that our way of representing AST paths needs to be improved.</p><p>Predicting Multiple Tokens at a Time. This paper focused on predicting the next token. Next we want to explore generating multiple tokens at a time (or even a complete expression statement). We hope to work towards an even more useful autocomplete tool. A IMPLEMENTATION DETAILS A.1 Modifying the AST For the AST, we want the internal AST nodes to only have type information, and the leaf nodes to have value information. This way, our model can predict one information given a node (instead of both type and value). However, in the py150 dataset, there are internal and leaf nodes with both type and value information. To accomodate for this, we slightly modify the trees to fit our definition of ASTs. For nodes with both type and value information, we take the value information, and create a new node (now a leaf node) as the node's first child. Fig 12 illustrates an example of the modification. This increases the average number of nodes in a tree from 623.4 to 951.9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Splitting Large Trees</head><p>For neural network models, we need to set a maximum number of nodes in the tree that the model can take as input. Ideally, we would want to set the maximum to be high enough to take in any tree of any length; however, in practice, this is infeasible due to memory constraints (and the number of nodes could be infinitely large hypothetically.) We choose the maximum number of context (number of nodes) to be 1000, inspired by the maximum number of context set by GPT2 models and as this covers &gt; 70% of the training data. For trees with number of nodes greater than 1000, we deploy a technique adopted by <ref type="bibr" target="#b2">(Al-Rfou et al., 2018)</ref>. Given a large tree, we slice it into shorter segments with a sliding window (in our implementation, we used 500, which is half the context). For example, if a tree has 1700 nodes, we would have 3 new shorter trees: from nodes 0-999, nodes 500-1499, and 699-1699. For the last two trees, we would take loss and evaluate only on the nodes that the model has not seen before (1000-1499 and 1500-1699, respectively). In this way, we provide each subsequent shorter segment with some previous context, while increasing the number of training and testing datapoints at a reasonable amount (in our datasets, it doubled the number). An improvement to this sliding window technique would be to maintain the hidden states at each segment to pass along more context information, as explained in <ref type="bibr" target="#b15">(Dai et al., 2019)</ref>.</p><p>A.3 Why not Positional Encoding? Some Transformers uses positional encoding <ref type="bibr" target="#b42">(Vaswani et al., 2017)</ref> or positional embedding <ref type="bibr" target="#b33">(Radford et al., 2019)</ref> to provide model extra positional information over elements. However, our early trials with LeafSeq suggested positional embedding is rather hurting than helping. Thus, we do not use positional encoding or embedding for all our models. Recently, Shiv and Quirk (2019) tried to introduce tree structures to Transformer models via positional encoding. However, their relative improvement is small compared to what we see with tree-relational prior in Section 5.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B EXTRA EVALUATION RESULTS</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>. gethostbyname ( host ) [ port , request_size , num_requests , num_conns ] = map ( string . atoi , sys . argv [2:] ) chain = build_request_chain ( num_requests , host , request_size ) ...</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Running example of Python code. The code snippet 1 is from the py150 dataset (py1, 2016).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Screenshot of an IDE showing autocomplete predictions from Jedi, a type-based autocomplete tool.Here, "atoi" is ranked low since the predictions are ordered alphabetically.</figDesc><graphic url="image-1.png" coords="2,169.05,203.37,147.90,73.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Overview of the models considered in this paper, arranged by two axes. The first axis shows the information from the code that is presented to the model: linear sequence of tokens or information from its AST. The second axis is the underlying ML model: decision tree, RNN, Attention, Transformer, and Transformer+. Models in blue font are models from previous work.</figDesc><graphic url="image-2.png" coords="5,144.44,84.68,197.12,126.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Schematic of a GPT2 Transformer (left) and GPT2 Transformer+ (right). The self-attention layer is able to consider all tokens in the input up to the point of prediction. Here the self-attention box depicts the information flow when predicting next token after the "."; see Table3for where the numbers come from. Transformer+ takes a relations matrix as another input to enhance the self-attention mechanism of the Transformer model. The matrix R contains information on the relations between the tokens in the input sequence (in TravTrans+ , the information is the path between two nodes in the AST.)</figDesc><graphic url="image-3.png" coords="9,82.20,84.68,321.60,190.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Fig 5 illustrates the architecture of the Transformer used in this work.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Example of an input to the PathTrans model. It takes all leaf nodes, along with its path to root, to make the next leaf token prediction. The root paths are embedded using an LSTM (red blocks), and the leaf tokens are embedded using an embedding layer (blue block). These two embeddings are added together to create a path representation embedding, which is then fed to the Transformer as shown in Fig 5. The classification layer of the Transformer outputs leaf tokens.</figDesc><graphic url="image-4.png" coords="11,73.60,330.10,338.80,102.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>2.4.1 PathTrans . PathTrans enhances SeqTrans by exposing tree structure to the Transformer via root-paths. A root-path Rootpath(t) is the path from the leaf node t to the root of the AST by traversing up its ancestors, recording all the nodes along with it, and thus a sequence of internal AST nodes. Fig 6 shows an example of an input datapoint for predicting node 29 of Fig 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Example of an input to the TravTrans (left part with only the DFS sequence) and TravTrans+ (both the DFS sequence and R matrix). Some inputs to the R matrix are colored, with the corresponding paths indicated by the same color arrows on the AST (due to space constraints, not all path relations could be drawn). In TravTrans , the embedded DFS tokens become the input to the Transformer model as shown in Fig 5. In TravTrans+ , along with the DFS tokens, the embedding of the R matrix is multiplied element-wise by the self-attention matrix. The classification layer of the Transformer outputs AST nodes.</figDesc><graphic url="image-5.png" coords="13,81.45,84.68,323.10,137.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Fig 5 shows the complete model architecture of incorporating R matrix.The rest of the TravTrans+ model is the same as TravTrans 's. However, since the Attn is replaced with Attn TreeRel and now the model takes tree-relation matrix R described above as extra input, we denote the Transformer here as Trans ud . The overall computation can thus be writte as o = Trans ud (e t ) t ∈AST _seq , R ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Fragment of a TGEN program encoding a decision tree on the left (bold words are the steps that comprise a path), with the corresponding paths shown on the AST on the right.</figDesc><graphic url="image-6.png" coords="14,122.25,84.68,241.50,107.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Example of an input for Code2Seq , which consists of leaf-to-leaf path representations given a partial AST. A path representation is made of tokenized starting tokens, path, and tokenized ending tokens. If the path ends with the target node (in this example, it's atoi), the value is replaced by &lt;placehholder&gt;.</figDesc><graphic url="image-7.png" coords="15,87.20,84.68,311.60,107.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig 8</head><label>8</label><figDesc>Fig 8 shows part of a learned decision tree, written in the form of program in a specialized language they call TGEN. Given an AST t and a starting node n, a TGEN program walks certain paths in t starting from n. For example, Up WriteValue (line 1) goes to the parent of n and records the label. If the label is Attr, it walks a different path (line 2) in the vicinity of n. The branch outcomes and observations collected by running this TGEN program on (t, n) form a context, which is then used to look up a probability distribution conditioned on that context. For the AST in Fig 4, starting with node 29, the TGEN program will produce a context for which the probabilities of different tokens for node 29 might be: [atoi: 60%, split: 30%, ...]. The flexibility of focusing on arbitrary paths in the AST allows the model to condition selectively on nodes farther back in the AST.A TGEN program is learned-on a specific corpus-by a genetic search procedure that simultaneously selects paths and grows the decision tree from the training data, with an entropy minimization objective. The details are not important for this paper; in this paper, we use their pretrained model (pho, 2017) as well as their Python dataset (py1, 2016) for our experiments.The reader will notice that the notion of UDpath in Section 2.4.3 is akin to the AST paths expressed in TGEN programs. The paths in TGEN are more general, but at a high-level, the idea that certain "spatial" relation between nodes is important is common to both approaches. This, along with the competitive quality of results of the Deep3 model in Table1, makes it an interesting comparison. We explore this similarity further in Sec 6.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>Fig 6, only the leaf tokens (right side of each input) is fed through a Transformer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>Fig. 10. Influence of previous nodes in value prediction of the example in Fig 4 by TravTrans and TravTrans+ . x-axis is labeled with the input values. y-axis is labeled with the values to be predicted. Color indicates the model's prediction is correct or wrong.</figDesc><graphic url="image-9.png" coords="20,241.24,85.09,197.16,132.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>From Fig 10b, we can see TravTrans+ is influenced by string when predicting atoi and by request_size when predicting num_requests. It is not shown in the figure but when predicting 2, TravTrans+ is influenced by the previous occurrence of sys.argv indexed by 0 and 1. Looking at the differences between Fig 10a and Fig 10b, we found that TravTrans+ is influenced by ip while predicting gethostbyname correctly but TravTrans is not while predicting it wrong. Generally, we found that TravTrans+ attributes more towards terminal values relevant to the values to be predicted, while TravTrans attributes little to values other than non-terminals. This provides an evidence that TravTrans+ is more likely to have learned the right features for next value prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>Fig. 11. Maximally (a) or minimally (b) weighted tree-relations and their weights at each attention head in TravTrans+ . Red means more extremal values.</figDesc><graphic url="image-10.png" coords="22,125.60,238.37,92.93,87.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Example AST and our modification to allow nodes to have either only value or type information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>#</head><label></label><figDesc>(a) legofy_gui.py 14 , highlighting U 12 D 1 "objects\":[]}" , mimetype= "application/json" ) resp = encode_queryset(featureset) return HttpResponse(resp, mimetype= "application/json" ) # ... (b) views.py 15 , highlighting U 6 D 17</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Two code excerpts from py150 evaluation set. Highlighted tokens are picked by some long UDpath in prediction of the underlined tokens.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>and context window of 1000, and were able to train TravTrans+ for 21 epochs in 20 hours. Compared with SeqRNN training of 7 hours, this is more, but the payoff is much higher. (Refer to Table 4 for details of model sizes and training times.)</head><label></label><figDesc></figDesc><table><row><cell>vs. TravTrans+ ;</cell></row><row><cell>• from 58.0% to 73.6% when comparing a Transformer model (TravTrans ) vs. a Transformer+</cell></row><row><cell>model (TravTrans+ ); by making the Transformer aware of the syntactic structures of code</cell></row><row><cell>by leveraging the self-attention mechanism, we are able to achieve superior results.</cell></row><row><cell>At the same time, the memory and training time cost of the Transformer need not be prohibitive.</cell></row><row><cell>We used 6 layers, 6 heads,</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Implementation details regarding number of epochs until convergence, training time (minutes per epoch), inference time (to evaluate over the py150 dataset), and model size, are listed in Table 4.</figDesc><table><row><cell></cell><cell></cell><cell>Prior work</cell><cell></cell><cell></cell><cell></cell><cell>Our work</cell><cell></cell></row><row><cell></cell><cell cols="7">SeqRNN Deep3 Code2Seq SeqTrans PathTrans TravTrans TravTrans+</cell></row><row><cell>Num epochs</cell><cell>9</cell><cell>n/a</cell><cell>10</cell><cell>9</cell><cell>16</cell><cell>11</cell><cell>21</cell></row><row><cell>Training time ( min / epoch)</cell><cell>45</cell><cell>n/a</cell><cell>210</cell><cell>45</cell><cell>45</cell><cell>60</cell><cell>60</cell></row><row><cell>Inference time</cell><cell>40</cell><cell>75</cell><cell>45</cell><cell>40</cell><cell>20</cell><cell>50</cell><cell>50</cell></row><row><cell>Model size (MB)</cell><cell>233</cell><cell>n/a</cell><cell>149</cell><cell>163</cell><cell>280</cell><cell>163</cell><cell>163</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>Table5. MRR of various types of next token predictions for py150. MRR of various types of next token value prediction for internal dataset.</figDesc><table><row><cell></cell><cell></cell><cell>Prior work</cell><cell></cell><cell></cell><cell></cell><cell>Our work</cell><cell></cell></row><row><cell>Applications</cell><cell cols="7">SeqRNN Deep3 Code2Seq SeqTrans PathTrans TravTrans TravTrans+</cell></row><row><cell>Attribute access</cell><cell>39.3</cell><cell>45.3</cell><cell>39.2</cell><cell>55.9</cell><cell>57.2</cell><cell>60.5</cell><cell>75.6</cell></row><row><cell>Numeric constant</cell><cell>40.6</cell><cell>53.2</cell><cell>49.2</cell><cell>55.9</cell><cell>58.1</cell><cell>63.5</cell><cell>83.1</cell></row><row><cell>Name (variable, module)</cell><cell>38.2</cell><cell>48.9</cell><cell>45.9</cell><cell>54.1</cell><cell>63.5</cell><cell>66.6</cell><cell>79.8</cell></row><row><cell>Function parameter name</cell><cell>57.7</cell><cell>58.1</cell><cell>56.5</cell><cell>66.2</cell><cell>65.8</cell><cell>67.2</cell><cell>87.1</cell></row><row><cell>All leaf tokens</cell><cell>36.6</cell><cell>43.9</cell><cell>43.6</cell><cell>50.1</cell><cell>55.1</cell><cell>58.0</cell><cell>73.6</cell></row><row><cell></cell><cell></cell><cell>Prior work</cell><cell></cell><cell></cell><cell></cell><cell>Our work</cell><cell></cell></row><row><cell>Applications</cell><cell cols="7">SeqRNN Deep3 Code2Seq SeqTrans PathTrans TravTrans TravTrans+</cell></row><row><cell>Attribute access</cell><cell>26.4</cell><cell>38.5</cell><cell>26.4</cell><cell>41.0</cell><cell>41.5</cell><cell>44.7</cell><cell>59.3</cell></row><row><cell>Numeric constant</cell><cell>32.2</cell><cell>46.5</cell><cell>45.4</cell><cell>51.7</cell><cell>56.1</cell><cell>61.5</cell><cell>84.0</cell></row><row><cell>Name (variable, module)</cell><cell>25.0</cell><cell>41.0</cell><cell>31.2</cell><cell>39.3</cell><cell>48.0</cell><cell>50.7</cell><cell>62.8</cell></row><row><cell>Function parameter name</cell><cell>45.5</cell><cell>50.6</cell><cell>39.3</cell><cell>54.3</cell><cell>52.1</cell><cell>53.3</cell><cell>73.7</cell></row><row><cell>All leaf tokens</cell><cell>23.8</cell><cell>36.1</cell><cell>31.0</cell><cell>36.5</cell><cell>40.8</cell><cell>43.9</cell><cell>58.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Table 7 shows a significant drop</figDesc><table><row><cell cols="5">Applications PathTrans PathTrans variant TravTrans+ TravTrans+ variant</cell></row><row><cell>All leaf tokens</cell><cell>55.1</cell><cell>41.9</cell><cell>73.6</cell><cell>73.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Table 9 and Table10show non-terminal value prediction for both py150 and internal dataset, respectively.13   Table9. MRR of various type predictions for py150.Table10. MRR of various types of next token type prediction for internal dataset.C MODEL INSPECTION SUPPLEMENTSCode examples of possibly useful long UDpaths that refer to class names (U 12 D 1 in Fig 13a)or to related variable names under similar scopes (U 6 D 17 in Fig13b).13 We do not include Code2Seq comparison for non-terminal node predictions due to the overhead required to prepare and process the dataset. Since the main part of the paper was on leaf token prediction, and we have shown that Trav-Trans+ performs significantly better than Code2Seq , we did not deem it essential to include the results on non-terminal value predictions. 14 data/JuanPotato/Legofy/legofy/legofy_gui.py 15 data/Miserlou/OpenWatch/openwatch/map/views.py</figDesc><table><row><cell># ...</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">class Permissions (unittest.TestCase) :</cell><cell></cell><cell></cell><cell></cell></row><row><cell>new_roles = {}</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">@utils.allow(services=list_permissions)</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">def setUp(self) : acc = self.account</cell><cell cols="2">Prior work</cell><cell></cell><cell>Our work</cell></row><row><cell cols="4">Applications if acc.service in list_permissions: self . test_folder = utils.create_or_get_test_folder(acc) Deep3</cell><cell cols="2">TravTrans TravTrans+</cell></row><row><cell cols="3">Function call self . test_file = utils.create_test_file(acc) # ...</cell><cell>81.6</cell><cell>88.5</cell><cell>98.7</cell></row><row><cell># ...</cell><cell>Assignment</cell><cell></cell><cell>76.5</cell><cell>78.9</cell><cell>98.7</cell></row><row><cell cols="4">Return def test_folder_permissions_set(self) : if self.account.service in change_folder_permissions: 52.8</cell><cell>67.8</cell><cell>97.8</cell></row><row><cell cols="2">List self.new_roles = { # ...</cell><cell></cell><cell>59.4</cell><cell>76.0</cell><cell>97.1 )</cell></row><row><cell cols="3">Dictionary result = self . test_folder .permissions.create( } data=self.new_roles)</cell><cell>66.3</cell><cell>15.0</cell><cell>83.8</cell></row><row><cell cols="3">Raise self.assertIsInstance(result.permissions, list) self.list_helper(self.test_folder)</cell><cell>35.0</cell><cell>63.3</cell><cell>97.0</cell></row><row><cell># ...</cell><cell>All types</cell><cell></cell><cell>81.9</cell><cell>87.3</cell><cell>98.7</cell></row><row><cell cols="2">def test_file_permissions_set(self) :</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">if self.account.service in change_file_permissions:</cell><cell></cell><cell></cell></row><row><cell cols="2">self.new_roles = {</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2"># ...</cell><cell></cell><cell></cell><cell></cell></row><row><cell>}</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">result = self . test_file .permissions.create(</cell><cell></cell><cell></cell></row><row><cell cols="2">data=self.new_roles)</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">self.assertIsInstance(result.permissions, list)</cell><cell></cell><cell></cell></row><row><cell cols="2">self.list_helper(self.test_file)</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Prior work</cell><cell></cell><cell>Our work</cell></row><row><cell># ...</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell># ...</cell><cell>Applications</cell><cell cols="2">Deep3</cell><cell cols="2">TravTrans TravTrans+</cell></row><row><cell></cell><cell>Function call</cell><cell></cell><cell>78.2</cell><cell>86.0</cell><cell>97.8</cell></row><row><cell></cell><cell>Assignment</cell><cell></cell><cell>78.5</cell><cell>79.7</cell><cell>98.7</cell></row><row><cell></cell><cell>Return</cell><cell></cell><cell>59.9</cell><cell>72.2</cell><cell>97.6</cell></row><row><cell></cell><cell>List</cell><cell></cell><cell>40.8</cell><cell>63.1</cell><cell>94.3</cell></row><row><cell></cell><cell>Dictionary</cell><cell></cell><cell>39.8</cell><cell>23.5</cell><cell>81.0</cell></row><row><cell></cell><cell>Raise</cell><cell></cell><cell>33.5</cell><cell>59.3</cell><cell>96.4</cell></row><row><cell></cell><cell>All types</cell><cell></cell><cell>79.9</cell><cell>87.7</cell><cell>98.0</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">https://www.eclipse.org/pdt/help/html/working_with_code_assist.htm</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">https://github.com/davidhalter/jedi</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2">https://github.com/iedmrc/galois-autocompleter</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3">We could have used a Transformer to embed the path sequences in lieu of an LSTM, but since the path sequences are short (capped at 13 tokens), and LSTMs perform adequately well for shorter sequences, we decided to use an LSTM.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4">Here we use an RNN to compute a language model; n-grams would be another choice. The jury seems to be out on which one is necessarily better for the task(Hellendoorn and Devanbu, 2017a, Karampatsis et al., 2020).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_5">https://github.com/graykode/gpt-2-Pytorch. We do not use positional encoding. Refer to Appendix A.3 for the explanation.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_6">https://github.com/pytorch/examples/tree/master/word_language_model</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_7">https://github.com/tech-srl/code2seq</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_8">https://github.com/pytorch/examples/blob/master/word_language_model/model.py</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_9">https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/ptb_word_lm.py</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pretrained probabilistic models for code</title>
		<ptr target="https://github.com/eth-sri/ModelsPHOG" />
	</analytic>
	<monogr>
		<title level="m">150k python dataset</title>
				<imprint>
			<date type="published" when="2016">2016. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">You only need attention to traverse trees</title>
		<author>
			<persName><forename type="first">Mahtab</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Rifayat Samee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Mercer</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/P19-1030/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="316" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Character-level language modeling with deeper self-attention</title>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dokook</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandy</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mining idioms from source code</title>
		<author>
			<persName><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering</title>
				<meeting>the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="472" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A convolutional attention network for extreme summarization of source code</title>
		<author>
			<persName><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v48/allamanis16.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning</title>
				<editor>
			<persName><forename type="first">Maria</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Florina</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<meeting>The 33rd International Conference on Machine Learning<address><addrLine>New York, New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-22">20-22 Jun 2016</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="2091" to="2100" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A survey of machine learning for big code and naturalness</title>
		<author>
			<persName><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Earl</forename><forename type="middle">T</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Premkumar</forename><surname>Devanbu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="37" />
			<date type="published" when="2018">2018a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning to represent programs with graphs</title>
		<author>
			<persName><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahmoud</forename><surname>Khademi</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BJOFETxR-" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generating sequences from structured representations of code</title>
		<author>
			<persName><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eran</forename><surname>Yahav</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=H1gKYo09tX" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019a</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning distributed representations of code</title>
		<author>
			<persName><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meital</forename><surname>Zilberstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eran</forename><surname>Yahav</surname></persName>
		</author>
		<idno type="DOI">10.1145/3290353</idno>
		<ptr target="https://doi.org/10.1145/3290353" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Programming Languages</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="29" />
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Structural language models for any-code generation</title>
		<author>
			<persName><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Sadaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eran</forename><surname>Yahav</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HylZIT4Yvr" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Sequence model design for code completion in the modern ide</title>
		<author>
			<persName><forename type="first">Ari</forename><surname>Gareth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gail</forename><forename type="middle">E</forename><surname>Aye</surname></persName>
		</author>
		<author>
			<persName><surname>Kaiser</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Phog: probabilistic model for code</title>
		<author>
			<persName><forename type="first">Pavol</forename><surname>Bielik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Raychev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Vechev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2016">2016a</date>
			<biblScope unit="page" from="2933" to="2942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Program synthesis for character level language modeling</title>
		<author>
			<persName><forename type="first">Pavol</forename><surname>Bielik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Raychev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Vechev</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=ry_sjFqgx" />
		<imprint>
			<date type="published" when="2016">2016b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generative code modeling with graphs</title>
		<author>
			<persName><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">L</forename><surname>Gaunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Polozov</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Bke4KsA5FX" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Open vocabulary learning on source code with a graphstructured cache</title>
		<author>
			<persName><forename type="first">Milan</forename><surname>Cvitkovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Badal</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Animashree</forename><surname>Anandkumar</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v97/cvitkovic19b.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
				<editor>
			<persName><forename type="first">Kamalika</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</editor>
		<meeting>the 36th International Conference on Machine Learning<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-15">09-15 Jun 2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="1475" to="1485" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Transformer-XL: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1285</idno>
		<ptr target="https://www.aclweb.org/anthology/P19-1285" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-07">July 2019</date>
			<biblScope unit="page" from="2978" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unified language model pre-training for natural language understanding and generation</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiao-Wuen</forename><surname>Hon</surname></persName>
		</author>
		<ptr target="https://papers.nips.cc/paper/9464-unified-language-model-pre-training-for-natural-language-understanding-and-generation" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="13042" to="13054" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Collective intelligence for smarter api recommendations in python</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Dâăźsouza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">V</forename><surname>Lopes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE 16th International Working Conference on Source Code Analysis and Manipulation (SCAM)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="51" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Structured neural summarization</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=H1ersoRqtm" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Tree-Transformer: A transformer-based method for correction of tree-structured data</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Harer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Reale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Chin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.00449</idno>
		<ptr target="https://arxiv.org/abs/1908.00449" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Are deep neural networks the best choice for modeling source code?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prem</forename><forename type="middle">T</forename><surname>Hellendoorn</surname></persName>
		</author>
		<author>
			<persName><surname>Devanbu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering</title>
				<meeting>the 2017 11th Joint Meeting on Foundations of Software Engineering</meeting>
		<imprint>
			<date type="published" when="2017">2017a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Are deep neural networks the best choice for modeling source code?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Premkumar</forename><surname>Hellendoorn</surname></persName>
		</author>
		<author>
			<persName><surname>Devanbu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering</title>
				<meeting>the 2017 11th Joint Meeting on Foundations of Software Engineering</meeting>
		<imprint>
			<date type="published" when="2017">2017b</date>
			<biblScope unit="page" from="763" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Global relational models of source code</title>
		<author>
			<persName><forename type="first">J</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Hellendoorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishabh</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petros</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><surname>Maniatis</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=B1lnbRNtwr" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">On the naturalness of software</title>
		<author>
			<persName><forename type="first">Abram</forename><surname>Hindle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Earl</forename><forename type="middle">T</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Gabel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhendong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Premkumar</forename><surname>Devanbu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communications of the ACM</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="122" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Summarizing source code using a neural attention model</title>
		<author>
			<persName><forename type="first">Srinivasan</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvin</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1195</idno>
		<ptr target="https://www.aclweb.org/anthology/P16-1195" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016-08">August 2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2073" to="2083" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Big code != big vocabulary: Open-vocabulary models for source code</title>
		<author>
			<persName><forename type="first">Rafael-Michael</forename><surname>Karampatsis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hlib</forename><surname>Babii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romain</forename><surname>Robbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Janes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Software Engineering (ICSE)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Code completion with neural attention and pointer networks</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Joint Conference on Artificial Intelligence, IJCAIâĂŹ18, page 4159âĂŞ25</title>
				<meeting>the 27th International Joint Conference on Artificial Intelligence, IJCAIâĂŹ18, page 4159âĂŞ25</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>ISBN 9780999241127</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJbPBt9lg" />
		<title level="m">Neural code completion</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Modeling programs hierarchically with stack-augmented lstm</title>
		<author>
			<persName><forename type="first">Fang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jss.2020.110547</idno>
		<ptr target="https://doi.org/10.1016/j.jss.2020.110547" />
	</analytic>
	<monogr>
		<title level="j">Journal of Systems and Software</title>
		<imprint>
			<biblScope unit="page">110547</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A statistical semantic language model for source code</title>
		<author>
			<persName><forename type="first">Thanh</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoan</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tien</forename><forename type="middle">N</forename><surname>Nguyen</surname></persName>
		</author>
		<idno type="DOI">10.1145/2491411.2491458</idno>
		<ptr target="https://doi.org/10.1145/2491411.2491458" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering, ESEC/FSE 2013</title>
				<meeting>the 2013 9th Joint Meeting on Foundations of Software Engineering, ESEC/FSE 2013<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="532" to="542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Tree-structured attention with hierarchical accumulation</title>
		<author>
			<persName><forename type="first">Xuan-Phi</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HJxK5pEYvr" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Proksch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Lerch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mira</forename><surname>Mezini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" />
	</analytic>
	<monogr>
		<title level="m">TSEM</title>
				<imprint>
			<date type="published" when="2015">2015. 2019</date>
		</imprint>
	</monogr>
	<note>Intelligent code completion with bayesian networks</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Code completion with statistical language models</title>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Raychev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Vechev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eran</forename><surname>Yahav</surname></persName>
		</author>
		<idno type="DOI">10.1145/2594291.2594321</idno>
		<ptr target="https://doi.org/10.1145/2594291.2594321" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation</title>
				<meeting>the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="419" to="428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Probabilistic model for code with decision trees</title>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Raychev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavol</forename><surname>Bielik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Vechev</surname></persName>
		</author>
		<idno type="DOI">10.1145/2983990.2984041</idno>
		<ptr target="https://doi.org/10.1145/2983990.2984041" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications, OOPSLA 2016</title>
				<meeting>the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications, OOPSLA 2016<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2016">2016a</date>
			<biblScope unit="page" from="731" to="747" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Learning programs from noisy data. SIGPLAN Not</title>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Raychev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavol</forename><surname>Bielik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Vechev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</author>
		<idno type="DOI">10.1145/2914770.2837671</idno>
		<ptr target="https://doi.org/10.1145/2914770.2837671" />
		<imprint>
			<date type="published" when="2016-01">January 2016b</date>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="761" to="774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Vighnesh Shiv and Chris Quirk. Novel positional encodings to enable tree-based transformers</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<ptr target="https://papers.nips.cc/paper/9376-novel-positional-encodings-to-enable-tree-based-transformers" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018. 2019</date>
			<biblScope unit="page" from="12058" to="12068" />
		</imprint>
	</monogr>
	<note>Self-attention with relative position representations</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=cO4ycnpqxKcS9" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pythia: Ai-assisted code completion system</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Svyatkovskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengyu</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neel</forename><surname>Sundaresan</surname></persName>
		</author>
		<idno type="DOI">10.1145/3292500.3330699</idno>
		<ptr target="http://dx.doi.org/10.1145/3292500.3330699" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019-07">Jul 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Autocompletion with deep learning</title>
		<author>
			<persName><surname>Tabnine</surname></persName>
		</author>
		<ptr target="https://tabnine.com/blog/deep" />
		<imprint>
			<date type="published" when="2019-07">Jul 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Neural program repair by jointly learning to localize and repair</title>
		<author>
			<persName><forename type="first">Marko</forename><surname>Vasic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petros</forename><surname>Maniatis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Bieber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishabh</forename><surname>Singh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">Pointer networks</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Tree transformer: Integrating tree structures into self-attention</title>
		<author>
			<persName><forename type="first">Yaushian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung-Yi</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D19-1098/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1060" to="1070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Improve language modelling for code completion through learning general token repetition of source code</title>
		<author>
			<persName><forename type="first">Yixiao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">31st Int. Conf. Software Engineering and Knowledge Engineering</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="667" to="777" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
