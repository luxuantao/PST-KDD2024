<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CLIFF: Contrastive Learning for Improving Faithfulness and Factuality in Abstractive Summarization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shuyang</forename><surname>Cao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Engineering</orgName>
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor</settlement>
									<region>MI</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
							<email>wangluxy@umich.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Engineering</orgName>
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor</settlement>
									<region>MI</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CLIFF: Contrastive Learning for Improving Faithfulness and Factuality in Abstractive Summarization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We study generating abstractive summaries that are faithful and factually consistent with the given articles. A novel contrastive learning formulation is presented, which leverages both reference summaries, as positive training data, and automatically generated erroneous summaries, as negative training data, to train summarization systems that are better at distinguishing between them. We further design four types of strategies for creating negative samples, to resemble errors made commonly by two state-of-the-art models, BART and PEGASUS, found in our new human annotations of summary errors. Experiments on XSum and CNN/Daily Mail show that our contrastive learning framework is robust across datasets and models. It consistently produces more factual summaries than strong comparisons with post error correction, entailmentbased reranking, and unlikelihood training, according to QA-based factuality evaluation. Human judges echo the observation and find that our model summaries correct more errors.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Large pre-trained Transformers have yielded remarkable performance on abstractive summarization <ref type="bibr" target="#b28">(Liu and Lapata, 2019;</ref><ref type="bibr" target="#b24">Lewis et al., 2020;</ref><ref type="bibr" target="#b46">Zhang et al., 2020a)</ref> with impeccable fluency, yet their summaries often contain factually inconsistent content <ref type="bibr" target="#b31">(Maynez et al., 2020;</ref><ref type="bibr" target="#b47">Zhang et al., 2020b;</ref><ref type="bibr" target="#b12">Goyal and Durrett, 2020)</ref>, even for stateof-the-art models. Three types of remedies have been proposed: running a separately learned error correction component <ref type="bibr" target="#b5">(Dong et al., 2020)</ref>, removing noisy training samples <ref type="bibr" target="#b32">(Nan et al., 2021;</ref><ref type="bibr" target="#b13">Goyal and Durrett, 2021)</ref>, and modifying the Transformer architecture <ref type="bibr" target="#b19">(Huang et al., 2020;</ref><ref type="bibr" target="#b32">Zhu et al., 2021)</ref>. Yet they either rely on heuristically created data for error handling, falling short of generalization, or require learning a large number of new parameters, and summary informativeness is often sacrificed.</p><p>XSum Article: The Fermanagh MLA Phil Flanagan tweeted after Tom Elliott appeared on a BBC radio programme in May 2014. . . . "I wonder if he will reveal how many people he harassed and shot as a member of the UDR.". . . Contrastive learning (our method): A Sinn Féin MLA has been ordered to apologise and pay compensation to a former member of the Ulster Defence Regiment (UDR).</p><p>Cross-entropy: A Sinn Féin MLA has agreed to pay compensation to a former Ulster Unionist Party (UDR) MP after he tweeted that he had harassed and shot people as a member of the party.</p><p>Entailment reranking: A Sinn Féin MLA has agreed to pay compensation to a former Ulster Unionist Party (UDR) councillor for a tweet he sent about him.</p><p>Unlikelihood: An MLA has been ordered to apologise and pay compensation to a former loyalist MP for a remark he made about him while serving in the Ministry of Defence.</p><p>Figure <ref type="figure">1</ref>: Sample article and system summaries by different methods. Our contrastive learning model trained on low confidence system outputs correctly generates the full name. Comparisons using cross-entropy loss, beam reranking by entailment scores <ref type="bibr" target="#b22">(Kryscinski et al., 2020)</ref>, and unlikelihood objective <ref type="bibr" target="#b42">(Welleck et al., 2020)</ref> over negative samples all produce unfaithful content.</p><p>Our goal is to train abstractive summarization systems that generate both faithful and informative summaries in an end-to-end fashion. We observe that, while the commonly used maximum likelihood training optimizes over references, there is no guarantee for the model to distinguish references from incorrect generations <ref type="bibr" target="#b18">(Holtzman et al., 2020;</ref><ref type="bibr" target="#b42">Welleck et al., 2020)</ref>. Therefore, potential solutions reside in designing new learning objectives that can effectively inform preferences of factual summaries over incorrect ones.</p><p>Concretely, we hypothesize that including factually inconsistent summaries (i.e., negative samples) for training, in addition to references (i.e., positive samples), let models become better at differentiating these two types of summaries. Although using negative samples has been effective at text representation learning, e.g., word2vec <ref type="bibr">(Mikolov et al., arXiv:2109.09209v1 [cs.CL]</ref> 19 <ref type="bibr">Sep 2021</ref><ref type="bibr">Sep 2013) )</ref> and <ref type="bibr">BERT (Devlin et al., 2019)</ref>, there exist two major challenges for it to succeed in concrete language tasks. First, a suitable training objective is critical to avoid performance degradation <ref type="bibr" target="#b39">(Saunshi et al., 2019)</ref>. Second, it is nontrivial to construct "natural" samples that mimic the diverse errors made by state-of-the-art systems that vary in words and syntax <ref type="bibr" target="#b13">(Goyal and Durrett, 2021)</ref>.</p><p>To address both challenges, we first propose a new framework, CLIFF, that uses contrastive learning for improving faithfulness and factuality of the generated summaries. 1 Contrastive learning (CL) has obtained impressive results on many visual processing tasks, such as image classification <ref type="bibr" target="#b20">(Khosla et al., 2020;</ref><ref type="bibr" target="#b4">Chen et al., 2020)</ref> and synthesis <ref type="bibr" target="#b36">(Park et al., 2020;</ref><ref type="bibr" target="#b45">Zhang et al., 2021b)</ref>. Intuitively, CL improves representation learning by compacting positive samples while contrasting them with negative samples. Here, we design a task-specific CL formulation that teaches a summarizer to expand the margin between factually consistent summaries and their incorrect peers.</p><p>Moreover, we design four types of strategies with different variants to construct negative samples by editing reference summaries via rewriting entity-/relation-anchored text, and using system generated summaries that may contain unfaithful errors. Importantly, these strategies are inspired by our new annotation study on errors made by state-of-the-art summarizers-models fine-tuned from BART <ref type="bibr" target="#b24">(Lewis et al., 2020)</ref> and PEGA-SUS <ref type="bibr" target="#b46">(Zhang et al., 2020a)</ref>-on two benchmarks: XSum <ref type="bibr" target="#b33">(Narayan et al., 2018)</ref> and CNN/DailyMail (CNN/DM) <ref type="bibr" target="#b16">(Hermann et al., 2015)</ref>.</p><p>We fine-tune pre-trained large models with our contrastive learning objective on XSum and CNN/DM. Results based on QuestEval <ref type="bibr">(Scialom et al., 2021)</ref>, a QA-based factuality metric of high correlation with human judgments, show that our models trained with different types of negative samples uniformly outperform strong comparisons, including using a summarizer with post error correction and reranking beams based on entailment scores to the source. Moreover, compared with unlikelihood training method that penalizes the same negative samples <ref type="bibr" target="#b42">(Welleck et al., 2020)</ref>, our summaries also obtain consistently better QuestEval scores. Human evaluation further confirms that our models consistently reduce both extrinsic and 1 Our code and annotated data are available at https:// shuyangcao.github.io/projects/cliff_summ. intrinsic errors over baseline across datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Factuality Improvement and Evaluation. Neural abstractive summaries often contain unfaithful content with regard to the source <ref type="bibr" target="#b7">(Falke et al., 2019)</ref>. To improve summary factuality, three major types of approaches are proposed. First, a separate correction model is learned to fix errors made by the summarizers <ref type="bibr">(Zhao et al., 2020;</ref><ref type="bibr" target="#b3">Chen et al., 2021)</ref>, including replacing entities absent from the source <ref type="bibr" target="#b5">(Dong et al., 2020)</ref> or revising all possible errors <ref type="bibr" target="#b1">(Cao et al., 2020)</ref>. The second type targets at modifying the sequence-to-sequence architecture to incorporate relation triplets <ref type="bibr" target="#b2">(Cao et al., 2018)</ref>, knowledge graphs <ref type="bibr" target="#b32">(Zhu et al., 2021), and</ref><ref type="bibr">topics (Aralikatte et al., 2021)</ref> to inform the summarizers of article facts. Yet additional engineering efforts and model retraining are often needed. Finally, discarding noisy samples from model training has also been investigated <ref type="bibr" target="#b32">(Nan et al., 2021;</ref><ref type="bibr" target="#b13">Goyal and Durrett, 2021)</ref>, however, it often leads to degraded summary informativeness. In comparison, our contrastive learning framework allows the model to be end-to-end trained and does not require model modification, thus providing a general solution for learning summarization systems.</p><p>Alongside improving factuality, we have also witnessed growing interests in automated factuality evaluation, since popular word-matchingbased metrics, e.g., ROUGE, correlate poorly with human-rated factual consistency levels <ref type="bibr" target="#b10">(Gabriel et al., 2021;</ref><ref type="bibr" target="#b6">Fabbri et al., 2021)</ref>. Entailment-based scorers are designed at summary level <ref type="bibr" target="#b22">(Kryscinski et al., 2020)</ref> and finer-grained dependency relation level <ref type="bibr" target="#b12">(Goyal and Durrett, 2020)</ref>. QA models are employed to measure content consistency by reading the articles to answer questions generated from the summaries <ref type="bibr" target="#b41">(Wang et al., 2020;</ref><ref type="bibr" target="#b6">Durmus et al., 2020)</ref>, or considering the summaries for addressing questions derived from the source <ref type="bibr" target="#b40">(Scialom et al., 2019)</ref>. Though not focusing on evaluation, our work highlights that models can produce a significant amount of world knowledge which should be evaluated differently instead of as extrinsic hallucination <ref type="bibr" target="#b31">(Maynez et al., 2020)</ref>. We also show that world knowledge can possibly be distinguished from errors via model behavior understanding.</p><p>Training with negative samples has been investigated in several classic NLP tasks, such as grammatical error detection <ref type="bibr" target="#b9">(Foster and Andersen, 2009)</ref> and dialogue systems <ref type="bibr" target="#b25">(Li et al., 2019)</ref>. Notably, negative sampling plays a key role in word representation learning <ref type="bibr" target="#b32">(Mikolov et al., 2013)</ref> and training large masked language models, such as BERT and ALBERT, to induce better contextual representations <ref type="bibr">(Devlin et al., 2019;</ref><ref type="bibr" target="#b23">Lan et al., 2020)</ref>. For text generation tasks, unlikelihood training is proposed to penalize the generation of negative tokens (e.g., repeated words) and sentences (e.g., contradictory responses in a dialogue system) <ref type="bibr" target="#b42">(Welleck et al., 2020;</ref><ref type="bibr" target="#b26">Li et al., 2020;</ref><ref type="bibr" target="#b15">He and Glass, 2020)</ref>. We use contrastive learning that drives enhanced representation learning to better distinguish between factual and incorrect summaries, which encourages more faithful summary generation.</p><p>Contrastive Learning (CL) for NLP. CL has been a popular method for representation learning, especially for vision understanding <ref type="bibr" target="#b17">(Hjelm et al., 2019;</ref><ref type="bibr" target="#b4">Chen et al., 2020)</ref>. Only recently has CL been used for training language models with selfsupervision <ref type="bibr" target="#b8">(Fang et al., 2020)</ref>, learning sentence representations <ref type="bibr" target="#b11">(Gao et al., 2021)</ref>, and improving document clustering <ref type="bibr" target="#b44">(Zhang et al., 2021a)</ref>. With a supervised setup, <ref type="bibr">Gunel et al. (2021)</ref> adopt the contrastive objective to fine-tune pre-trained models on benchmark language understanding datasets. Using a similar idea, <ref type="bibr">Liu and Liu (2021)</ref> enlarge the distances among summaries of different quality as measured by ROUGE scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">CLIFF: Contrastive Learning Framework for Summarization</head><p>We design a contrastive learning (CL)-based training objective that drives the summarization model to learn a preference of faithful summaries over summaries with factual errors. It is then used for fine-tuning BART <ref type="bibr" target="#b24">(Lewis et al., 2020)</ref> and PEGA-SUS <ref type="bibr" target="#b46">(Zhang et al., 2020a)</ref> for training summarization models. Formally, let an article x have a set of reference summaries P (henceforth positive samples) and another set of erroneous summaries N (negative samples). The contrastive learning objective is <ref type="bibr" target="#b20">(Khosla et al., 2020;</ref><ref type="bibr">Gunel et al., 2021)</ref>:</p><formula xml:id="formula_0">l x cl = − 1 |P | 2 yi,yj ∈P yj =yi log exp(sim(h i , h j )/τ ) y k ∈P ∪N y k =yi exp(sim(h i , h k )/τ )<label>(1)</label></formula><p>where h i , h j , and h k are representations for summaries y i , y j , and y k . sim(•, •) calculates the cosine similarity between summary representations. τ is a temperature and is set to 1.0. Importantly, summaries in P and N are included in the same batch during training, so that the model acquires better representations to differentiate correct summaries from those with errors by comparing the two types of samples, thus maximizing the probabilities of the positive samples and minimizing the likelihoods of the corresponding negative samples. The CL objective on the full training set, denoted as L CL , is the sum of losses l x cl over all samples.</p><p>To effectively employ CL in summarization, we need to address two challenges: (1) how to automatically construct both positive and negative samples, which are critical for CL efficacy <ref type="bibr" target="#b4">(Chen et al., 2020)</ref>, and (2) how to represent the summaries (i.e., h * ). Below we describe positive sample generation and options for h * , leaving the strategies for negative samples to § 5.</p><p>Positive Sample Construction (P ). Summarization datasets often contain a single reference for each article. To create multiple positive samples, in our pilot study, we experiment with paraphrasing with synonym substitution <ref type="bibr" target="#b38">(Ren et al., 2019)</ref>, randomly replacing words based on the prediction of masked language models <ref type="bibr" target="#b21">(Kobayashi, 2018)</ref>, and back-translation <ref type="bibr" target="#b30">(Mallinson et al., 2017)</ref>. We find back-translation to be best at preserving meaning and offering language variation, and thus use NL-PAug<ref type="foot" target="#foot_0">2</ref> to translate each reference to German and back to English. Together with the reference, the best translation is kept and added to P , if no new named entity is introduced.</p><p>Summary Representation (h * ). We use the outputs of the decoder's last layer, and investigate three options that average over all tokens, named entity tokens, and the last token of the decoded summary. Entities and other parsing results are obtained by spaCy <ref type="bibr" target="#b19">(Honnibal et al., 2020)</ref>. We further consider adding a multi-layer perceptron (MLP) with one hidden layer to calculate the final h * .</p><p>The final training objective combines the typical cross-entropy loss L CE and our contrastive learning objective: L = L CE + λL CL , where λ is a scalar and set to 1.0 for all experiments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Summary Error Annotation and Model Behavior Analysis</head><p>We first describe annotating unfaithfulness errors by state-of-the-arts, i.e., models fine-tuned from BART and PEGASUS on XSum and CNN/DM.</p><p>We then probe into the model generation behavior that is indicative of errors, which guides the design of negative sample construction strategies. 600 (150 × 2 × 2) summaries are annotated to demonstrate how often do the models "hallucinate", i.e., generating content not grounded by the source. To characterize errors, we annotate text spans in summaries with (i) intrinsic errors caused by misconstructing phrases or clauses from the source; and (ii) extrinsic errors which include words not in the source that are either unverifiable or cannot be verified by Wikipedia. Content not covered by the article but can be validated by Wikipedia is annotated as world knowledge, and the models' behavior pattern when generating them differs from when they generate errors.</p><p>Two fluent English speakers with extensive experience in summary evaluation and error labeling are hired. For each sample, they are shown the article and two system summaries, and instructed to annotate text spans with the aforementioned errors and world knowledge. After labeling every 50 samples, the annotators discuss and resolve any disagreement. The Fleiss's Kappas on XSum and CNN/DM are 0.35 and 0.45.</p><p>Error statistics are displayed in Fig. <ref type="figure" target="#fig_0">2</ref>. Extrinsic errors dominate both datasets, especially on XSum. 58.7% of summaries by BART (and 44.0% by PE-GASUS) contain at least one extrinsic error. Noticeably, PEGASUS is a newer model pre-trained with a larger amount of data, thus contains less errors than BART and other older models studied for error annotations by <ref type="bibr" target="#b31">Maynez et al. (2020)</ref>. This observation also highlights the usage of our anno- tations for future development and evaluation of summarization models.</p><p>Low confidence generation is indicative of extrinsic errors. Inspired by recent work that studies model prediction confidence <ref type="bibr">(Liu et al., 2021)</ref>, we examine generation probabilities for tokens of different part-of-speech (POS) tags. Fig. <ref type="figure" target="#fig_1">3</ref> shows salient results on the generation probabilities of the first token of a proper noun or a number (with additional analysis provided in Appendix A). As observed, model confidence tends to be lower for the first tokens of proper nouns and numbers if they are part of spans with extrinsic errors. Also note that world knowledge, which cannot be inferred from the source either, often has higher generation probability than extrinsic errors. Take this snippet generated by a fine-tuned BART as an example: "Manchester United captain Wayne Rooney's testimonial game against Manchester City. . .". "Manchester City" is an extrinsic error and "Wayne" is produced as world knowledge. The model assigns a low probability of 0.10 to the first token of "Manchester City" and a high probability of 0.92 to token "Wayne". This implies that model confidence can be a useful indicator for negative sample collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Negative Sample Construction</head><p>Here we describe four strategies for constructing negative samples that modify the references ( § 5.1-5.3) or use system generated summaries (5.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Entity Swap</head><p>Entity swap imitates intrinsic errors, as over 55% of intrinsic errors in our annotations are found to contain named entities. We construct negative samples by swapping named entities in the references with other randomly selected entities of the same entity type in the source (SWAPENT). One sam-REFERENCE: A "rare" short-eared owl found emaciated in Flintshire is now recuperating well, the RSPCA have said.</p><p>SWAPENT: Flintshire → Bettisfield ⇒ A "rare" short-eared owl found emaciated in Bettisfield is now recuperating well, the RSPCA have said.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MASKENT:</head><p>A "rare" short-eared owl found emaciated in [MASK] is now recuperating well, the RSPCA have said. ⇒ A "rare" short-eared owl found emaciated in a field in South Yorkshire is now recuperating well, the RSPCA have said.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MASKREL:</head><p>A "rare" short-eared owl found [MASK] in [MASK] is now recuperating well, the RSPCA have said. ⇒ A "rare" short-eared owl found dead in London is now recuperating well, the RSPCA have said.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REGENENT:</head><p>A "rare" short-eared owl found emaciated in ⇒ A "rare" short-eared owl found emaciated in Nottinghamshire is now at a wildlife centre to recover. REGENREL: A "rare" short-eared owl found ⇒ A "rare" short-eared owl found in the grounds of a former coal mine is being cared for by the RSPCA in Somerset.</p><p>SYSLOWCON: An injured golden owl found in a former coal mine in Lancashire is being cared for by the RSPCA. ple is constructed for each entity in the reference. Though this idea has been studied by <ref type="bibr" target="#b22">Kryscinski et al. (2020)</ref>, they allow entities of different types to be used, e.g., a PERSON can be replaced by a LOCATION. Examples are displayed in Table <ref type="table" target="#tab_0">1</ref>. SWAPENT has the advantage of not depending on any trained model. Yet it only introduces intrinsic errors and lacks the coverage for extrinsic errors, which is addressed by the following generationbased methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Mask-and-fill with BART</head><p>To simulate extrinsic errors, we leverage large unconditional language models' capability of converting a sequence with masked tokens into a fluent and appropriate sequence. Specifically, we replace each named entity in a reference with a [MASK] token and encode it with BART (without any fine-tuning). BART then fills this partially masked summary with newly generated entities (MASKENT). BART is chosen since it can fill [MASK] with varying number of tokens. For each entity in the reference, we sample three summaries and only retain the ones containing at least one entity that is absent from both the source and the reference.</p><p>Up to now, the two introduced strategies both focus on incorrect named entities. To cover more diverse extrinsic and intrinsic errors <ref type="bibr" target="#b12">(Goyal and Durrett, 2020)</ref>, we extend MASKENT to contain relations (MASKREL). We first obtain dependency relations using Stanza <ref type="bibr" target="#b37">(Qi et al., 2020)</ref>, with each relation denoted as &lt;gov, rel, dep&gt;. To incorporate more context, we consider noun phrase spans enclosing the token of gov or dep if it is a content word and the noun phrase contains a named entity. Similar to MASKENT, three negative samples are generated by BART based on the input with both gov and dep spans masked in the reference. Only the samples that introduce any new dependency relation that is not contained in the source nor the reference are kept. Specifically, we consider a match of a dependency relation as the same form or synonyms of its gov and and dep is found in the source or the reference with the same relation.</p><p>Both MASKENT and MASKREL can create more extrinsic errors compared to other strategies introduced in this section, since negative samples are generated without being grounded on the source articles. However, their constructed negative samples may contained drifted topics that can be easily detected by a summarization model, resulting with less efficient training signals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Source-conditioned Regeneration</head><p>To ground negative sample generation with the article, we further design a regeneration strategy based on conditional generation. For each named entity in the reference, we treat the text before it as a prompt. A summarizer, e.g., fine-tuned from BART or PEGASUS, first reads in the source using the encoder, then receives the prompt as the first part of the decoder output, and finally decodes the rest of the content based on nucleus sampling <ref type="bibr" target="#b18">(Holtzman et al., 2020)</ref> with a cumulative probability threshold of 0.7. The prompt and the regenerated text comprise the final negative sample. This method is denoted as REGENENT.</p><p>We also extend entities to relations with expanded governor and dependent spans (REGENREL). Here, we consider a prompt as the text before the gov or dep span, whichever occurs first. For both REGENENT and REGENREL, three negative samples are generated for each prompt, and a sample is kept if it introduces any new entity (for REGENENT) or dependency relation (for REGENREL) with regard to the source and the reference.</p><p>Negative samples generated by both methods are more relevant to the article than the mask-and-fill strategy, yet they may still miss certain types of errors and differ from real model outputs, since they are modified from the reference summaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">System Generation</head><p>Motivated by the model confidence analysis in § 4, we explore using system generated summaries as negative samples. We first run fine-tuned BART or PEGASUS on the same training set to decode summaries. For each summary, we check the model confidence on the first token of each proper noun and number span. If the probability is below a threshold, we keep it as a negative sample (SYSLOWCON). The threshold is tuned by maximizing F1 based on our error annotations.</p><p>We consider all beams at the last decoding step as candidates. We use beam sizes of 6 and 4 for XSum and CNN/DM. Statistics of negative samples constructed by different strategies are in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiment Setup</head><p>Evaluation Metrics. QuestEval <ref type="bibr">(Scialom et al., 2021)</ref> is used as the main metric to evaluate summaries' factual consistency. Given an article and a summary, QuestEval first generates natural language questions for entities and nouns from both. A QA model then consumes the article to answer questions derived from the summary, producing a score. Another score is obtained from a QA model addressing article-based questions after reading the summary. The final QuestEval score is the harmonic mean of the two. We use the version with learned weights for questions, which has shown high correlation with human judged consistency and relevance.</p><p>We further use FactCC <ref type="bibr" target="#b22">(Kryscinski et al., 2020)</ref>, trained based on their negative sample construction method, to measure if the summary can be entailed by the source. We also report <ref type="bibr">ROUGE-L (Lin, 2004)</ref>. Both FactCC and ROUGE-L reasonably correlate with summary factuality as judged by human <ref type="bibr" target="#b35">(Pagnoni et al., 2021)</ref>.</p><p>Based on our error annotations, we report the correlations between each metric and the error ratepercentage of tokens being part of an error span, and the raw number of errors (Table <ref type="table">2</ref>). QuestEval correlates better on both aspects than other metrics.</p><p>Comparisons. In addition to the models finetuned with cross-entropy loss (CRSENTROPY), we consider reranking beams based on FactCC Moreover, we compare with unlikelihood training that penalizes the probabilities of all tokens in a negative sample <ref type="bibr" target="#b26">(Li et al., 2020)</ref>. Given a negative sample y , the loss is defined as − |y | t=1 log(1 − p(y t |y 1:t−1 , x)), where p(y t |y 1:t−1 , x) is the output probability at the t-th step. We combine the unlikelihood training objective with cross-entropy loss with equal weights for fine-tuning.</p><p>Lastly, we compare our negative sample strategies with negative samples constructed for training the FactCC scorer, denoted as FCSAMPLE. For CL only, we compare with using other samples in the same batch as negative samples (BATCH), a common practice for CL-based representation learning <ref type="bibr" target="#b11">(Gao et al., 2021;</ref><ref type="bibr" target="#b44">Zhang et al., 2021a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Automatic Evaluation</head><p>We report results by models fine-tuned from BART and PEGASUS with different objectives and negative samples on XSum and CNN/DM in Tables <ref type="table" target="#tab_2">3  and 4</ref>. CLIFF models use a summary representation of averaging over all tokens with MLP projection, with other variants discussed in § 7.3. Unless explicitly stated, comparison models are fine-tuned from the same large model used by CLIFF.</p><p>First, comparing with other factuality improvement models (top of the tables), almost all CLIFF models trained with different negative samples uni- formly produce higher QuestEval scores across datasets with both large models, with the improvements more pronounced on XSum. Importantly, ROUGE scores for CLIFF models are comparable or better than baselines trained with crossentropy, e.g., on CNN/DM as in Table <ref type="table" target="#tab_2">3</ref>. A similar trend is observed with the FactCC metric, especially when using PEGASUS as the base model (Table <ref type="table">4</ref>). Note that ENTAILRANK tends to yield significantly higher FactCC scores, though it obtains lower QuestEval scores than the cross-entropy baseline. Table <ref type="table">4</ref>: Results of models fine-tuned from PEGASUS on XSum and CNN/DM. We report results on 5, 000 randomly selected samples on CNN/DM, due to long running time of QuestEval. For models of unlikelihood training and CLIFF that use the same negative samples, the better of the two is highlighted with green. * : our model is significantly better than CRSENTROPY (approximation randomization test, p &lt; 0.005).</p><p>unlikelihood training with the same negative samples. According to Table <ref type="table" target="#tab_2">3</ref>, using 7 negative sample construction strategies on two datasets, CLIFF obtains higher QuestEval scores than unlikelihood training in 12 out of the 14 comparisons. Using PEGASUS, CLIFF also outperforms in 11 setups as listed in Table <ref type="table">4</ref>. Similar trends are found on FactCC and ROUGE-L. Another noteworthy piece is that CLIFF's improvements over the crossentropy baseline are more consistent, whereas unlikelihood training occasionally hurts factuality or ROUGE scores significantly. We believe the key advantage of CLIFF resides in its measure of representation similarities between positive and negative samples in the same batch, allowing models to better differentiate between correct and erroneous summaries.</p><p>Finally, among all variants, CLIFF trained with low confidence summaries as negative samples obtains the best QuestEval scores on the more abstractive dataset. As seen in Table <ref type="table" target="#tab_2">3</ref>, using low confidence summaries also improves FactCC scores on both datasets, and enhances ROUGE-L on the more extractive dataset CNN/DM. This indicates that system generated summaries contribute more diverse errors made by existing models organically, which are particularly suitable for our CL framework. As we use summaries generated by the same model for CLIFF training, one future direction is to use outputs by different models. For our mask-and-fill and source-conditioned regeneration strategies, we find that relation-anchored construction often beats their entity-anchored counterparts. This calls for efforts that steer the entity-driven methods to a more relation-focused direction.</p><p>Combining Strategies. We further show results by fine-tuning BARTs using samples based on combined negative sample construction strategies in Table <ref type="table" target="#tab_4">5</ref>. As can be seen, combining SYSLOWCON and other strategies yields better QuestEval scores than models trained with negative samples by any single strategy, except for MASKENT and REGE-NENT on XSum. This signifies the importance of covering diverse types of errors in negative samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Human Evaluation</head><p>Pairwise Comparison with Cross-entropy. We recruit the two human annotators for our summary error study, as well as another experienced annotator, to evaluate summary informativeness and factual consistency. For each article, the judges are shown summaries generated by the CRSENTROPY model and four other systems. They then rate each system summary against the CRSENTROPY summary. All four summaries generated by different factuality-improved models are shown in random order without system names shown, ensuring the fair comparison among them. We randomly pick 100 articles from each dataset used in our error analysis study in § 4, and evaluate summaries generated by ENTAILRANK, unlikelihood training (ULL) with negative samples con-  ) and factual consistency (Factual.) The Krippendorff's αs are 0.33 and 0.62 for the two aspects on XSum, and 0.34 and 0.89 on CNN/DM. Our CL method using low confidence summaries is more frequently rated as better for informativeness and factuality on the more abstractive dataset XSum. structed by MASKENT, and CLIFF models trained with BATCH and SYSLOWCON negative samples. All are fine-tuned from BART. Detailed evaluation guidelines are in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Intrinsic</head><p>Table <ref type="table" target="#tab_6">6</ref> shows that on the more abstractive XSum data CL trained with low confidence samples are more frequently rated as being more informative and more factual than CRSENTROPY summaries. This echos our automatic evaluations with QuestEval in § 7.1. On CNN/DM, all models trained with negative samples produce summaries with better informativeness and faithfulness. In contrast, ENTAILRANK summaries are less distinguishable from outputs by CRSENTROPY on both datasets, as more ties are found. We show sample outputs in extrinsic errors as done in § 4. Fig. <ref type="figure" target="#fig_2">4</ref> shows that CL is more effective at reducing extrinsic errors than unlikelihood training can on both datasets. We also observe slight decreases of world knowledge in the summaries (figure attached in Appendix D).</p><p>Error Correction Operations. Finally, with reference to CRSENTROPY summaries, human judges are instructed to label each system summary as whether it corrects any error by CRSENTROPY using deletion of the incorrect content, substitution with factual information, or both. As seen in Fig. <ref type="figure" target="#fig_3">5</ref>, CL-based models restore factually consistent information, e.g., by replacing erroneous names and numbers with correct ones, more frequently than unlikelihood training or entailment reranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Variants of Summary Representation</head><p>Sample representation is critical for CL to be effective. Here we investigate summary representation variants as discussed in § 3. There are two major considerations: (1) Should we consider all tokens in a summary or only representative ones (e.g., entities or last token)? (2) Should additional transformation, i.e., an MLP, be used?</p><p>Experiments on XSum using three negative sample construction strategies demonstrate that averaging the decoder outputs of all tokens and adding an MLP projection yield the best overall performance, as shown in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We present CLIFF, a contrastive learning-based framework to promote faithfulness and factuality of abstractive summaries. CLIFF uses both references and summaries that are factually inconsistent with the articles to train systems to be better at discriminating errors from factual and salient content. We further study strategies that automatically create erroneous summaries by editing from references or leveraging systems outputs, inspired by our new summary error analysis on state-of-theart models. Both automatic evaluation and human ratings show that CLIFF achieves consistent improvements over competitive comparison methods, and is generalizable across datasets with systems fine-tuned from different large models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Additional Analysis for Summary Error Annotation</head><p>We hire two fluent English speakers to annotate summary errors on XSum and CNN/DailyMail (CNN/DM). They annotate a common batch of 100 summaries generated by summarizers fine-tuned from BART and PEGASUS, with 50 articles in each batch. The two annotators are shown 50 HTML pages in a batch, each of which contains an article and two summaries generated by the two models. The detailed annotation guideline is given in Fig. <ref type="figure">9</ref>.</p><p>For our analysis on token generation probabilities, we additionally show the distributions of the first token's probability for nouns and verbs in Fig. <ref type="figure">6</ref>. We also report the distributions of the nonfirst token's probability for proper nouns, numbers, nouns, and verbs in Fig. <ref type="figure">7</ref>. As can be seen, tokens within extrinsic and intrinsic errors have high generation probabilities when they are non-first tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Statistics for Datasets and Training Samples</head><p>Summarization Datasets. We follow the official data splits for the two datasets, with the number of samples in each split listed in Table <ref type="table" target="#tab_11">8</ref>.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Implementation Details</head><p>We use Fairseq <ref type="bibr" target="#b34">(Ott et al., 2019)</ref> and Huggingface Transformers <ref type="bibr" target="#b43">(Wolf et al., 2020)</ref> for our experiments with BART and PEGASUS. Our experiments are conducted on the RTX 8000 GPU with 48GB memory and the A100 GPU with 40GB memory.</p><p>Training Settings. For hyperparameters, we follow <ref type="bibr" target="#b24">Lewis et al. (2020)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Human Evaluation</head><p>In § 7.2, we demonstrate the percentages of samples containing intrinsic errors and extrinsic errors for each model evaluated by human judges. Here, we report the percentages of samples containing world knowledge in Fig. <ref type="figure" target="#fig_6">8</ref>. On XSum, all models produce less world knowledge compared to the model trained with cross-entropy loss, while gen-  erating similar or greater amounts of samples with world knowledge on CNN/DM.</p><p>Our human evaluation guideline is shown in Fig. <ref type="figure">10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Sample Outputs</head><p>We include more sample outputs in Fig. <ref type="figure" target="#fig_7">11</ref>.</p><p>In this study, you will first read article-summary pairs and then identify three types of text spans in the summaries. These spans include content that is contradicted by or cannot be implied from the article. The description for each type is described below:</p><p>• Intrinsic errors: Text spans that misconstruct phrases or clauses from the article.</p><p>• Extrinsic errors: Text spans that include words that are not in the article and are not verifiable or cannot be verified by Wikipedia.</p><p>• world knowledge: Text spans that contain information that is not covered by the article but can be validated by Wikipedia.</p><p>When selecting spans, you should always make sure the spans are complete words.</p><p>In practice, you should follow the these steps carefully: (1) read the article and summaries carefully;</p><p>(2) figure out if there is content contradicted by or not presented in the article; (3) label the span as an intrinsic error if it misconstructs phrases or clauses from the article; (4) if the span does not belong to intrinsic errors, search within Wikipedia and determine whether the content in the span can be verified;</p><p>(5) label it as world knowledge if the it can be verified by Wikipedia, otherwise label it as an extrinsic error. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Percentage of samples with intrinsic and extrinsic error spans for models fine-tuned from BART and PEGASUS on XSum and CNN/DM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Probability distributions of generating the first tokens of proper nouns and numbers, grouped by extrinsic errors, intrinsic errors, world knowledge, and other correct tokens.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Portions of summaries with errors. CL models consistently reduce both types of errors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Summaries use different portions of error correction operations. Contrastive learning with SYS-LOWCON (CL.SLC) and BATCH (CL.B) substitute errors with correct content more often than unlikelihood training with MASKENT and ENTAILRANK.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>for BART and Zhang et al. (2020a) for PEGASUS. During training, we randomly select 5 and 4 negative samples for each input article in XSum and CNN/DM. Mixedprecision training is not supported by the PEGA-SUS implementation and is utilized on BART only. Decoding Settings. We use the beam search algorithm to decode summaries. For BART, we set the beam sizes as 6 and 4 on XSum and CNN/DM. A beam size of 8 is used for PEGASUS on both datasets. Running Time and Model Sizes. The BARTbased models take 6 and 13 hours for training on XSum and CNN/DM, and it takes 1.5 hour to decode on the two datasets. Meanwhile, training the PEGASUS-based models takes 8 and 25 hours for XSum and CNN/DM, and the decoding takes 1 hour.As for model sizes, our BART-based models and PEGASUS-based models have 400M and 568M parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Percentages of samples containing world knowledge as labeled by human on the outputs of XSum and CNN/DM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Sample generated summaries by fine-tuned BART models. Intrinsic errors are highlighted in red and extrinsic errors are in blue. MASKENT and SYSLOWCON are used for negative sample construction with unlikelihood training and contrastive learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Negative sample construction strategies ( § 5). For summaries edited from the reference, their differences are bolded. Introduced errors are in red. Text before is the prefix for regeneration.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>25.47  *  36.19 51.05 50.05 41.01   Results of models fine-tuned from BART on XSum and CNN/DM. QEval: QuestEval; FC: FactCC; R-L: ROUGE-L. The best result per metric per dataset is bolded. For models of unlikelihood training and CLIFF that use the same negative samples, the better of the two is highlighted with green. * : our model is significantly better than CRSENTROPY (approximation randomization test, p &lt; 0.005).</figDesc><table><row><cell>Model</cell><cell></cell><cell>XSum</cell><cell></cell><cell></cell><cell>CNN/DM</cell></row><row><cell></cell><cell cols="5">QEval FC R-L QEval FC R-L</cell></row><row><cell cols="5">Comparisons without Negative Samples</cell></row><row><cell cols="6">CRSENTROPY 33.09 23.92 37.14 50.94 49.07 40.82</cell></row><row><cell cols="6">ENTAILRANK 32.95 38.45 36.55 51.02 49.84 40.89</cell></row><row><cell cols="6">CORRECTION 33.12 24.14 37.11 50.93 49.06 40.82</cell></row><row><cell>SUBSETFT</cell><cell cols="3">32.25 21.83 30.35</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>FASUM</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">50.73 50.58 37.18</cell></row><row><cell cols="5">Comparisons with Unlikelihood Training</cell></row><row><cell>FCSAMPLE</cell><cell cols="5">32.93 24.46 33.96 50.60 35.09 41.22</cell></row><row><cell>SWAPENT</cell><cell cols="5">32.90 23.94 34.96 49.77 32.37 40.18</cell></row><row><cell>MASKENT</cell><cell cols="5">33.21 24.22 33.89 51.01 48.57 41.23</cell></row><row><cell>MASKREL</cell><cell cols="5">33.18 23.50 34.56 51.00 48.35 41.15</cell></row><row><cell>REGENENT</cell><cell cols="5">32.41 24.12 37.08 50.97 48.59 41.07</cell></row><row><cell>REGENREL</cell><cell cols="5">30.86 24.58 37.18 50.97 48.42 41.14</cell></row><row><cell cols="6">SYSLOWCON 32.01 26.30 32.04 50.82 48.66 40.81</cell></row><row><cell cols="2">Our Method: CLIFF</cell><cell></cell><cell></cell><cell></cell></row><row><cell>BATCH</cell><cell cols="5">33.18 24.88 36.76 50.99 52.18 40.98</cell></row><row><cell>FCSAMPLE</cell><cell cols="5">33.15 24.50 36.72 51.02 49.62 41.06</cell></row><row><cell>SWAPENT</cell><cell cols="5">33.30 25.67  *  35.60 51.05 50.96 40.89</cell></row><row><cell>MASKENT</cell><cell cols="5">33.32  *  25.73  *  36.02 50.98 49.04 41.06</cell></row><row><cell>MASKREL</cell><cell cols="5">33.35  *  25.69  *  35.86 51.03 49.89 41.04</cell></row><row><cell>REGENENT</cell><cell cols="5">33.15 24.64 36.32 51.04 49.91 41.11</cell></row><row><cell>REGENREL</cell><cell cols="5">33.22 25.39 36.21 50.96 49.48 41.11</cell></row><row><cell cols="2">SYSLOWCON 33.35</cell><cell></cell><cell></cell><cell></cell></row></table><note>*  </note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>25.28 38.58 51.00 * 51.80 * 39.37 SWAPENT 33.09 * 25.09 38.58 51.16 * 52.97 * 38.95 MASKENT 33.09 * 25.75 38.12 51.13 * 53.60 * 39.24 MASKREL 33.06 * 25.28 38.37 51.17 * 53.34 * 39.36 REGENENT 33.09 * 24.48 38.33 50.99 * 52.18 * 39.28 REGENREL 33.16 * 24.82 38.30 51.16 * 53.21 * 39.25 SYSLOWCON 33.21 * 25.18 38.18 50.85 * 53.73 * 39.30</figDesc><table><row><cell>Model</cell><cell>XSum</cell><cell>CNN/DM</cell></row><row><cell></cell><cell cols="2">QEval FC R-L QEval FC R-L</cell></row><row><cell cols="2">Comparisons without Negative Samples</cell><cell></cell></row><row><cell cols="3">CRSENTROPY 32.50 25.48 39.07 50.21 44.44 40.39</cell></row><row><cell cols="3">ENTAILRANK 32.42 41.90 38.47 50.15 61.04 40.67</cell></row><row><cell cols="3">CORRECTION 32.55 25.15 39.02 49.48 32.96 39.79</cell></row><row><cell cols="2">Comparisons with Unlikelihood Training</cell><cell></cell></row><row><cell>FCSAMPLE</cell><cell cols="2">32.79 25.37 38.46 50.63 45.45 39.28</cell></row><row><cell>SWAPENT</cell><cell cols="2">32.88 24.76 37.91 50.43 43.02 38.96</cell></row><row><cell>MASKENT</cell><cell cols="2">33.04 26.30 37.51 51.11 52.19 39.34</cell></row><row><cell>MASKREL</cell><cell cols="2">33.08 24.38 38.05 51.14 52.93 39.31</cell></row><row><cell>REGENENT</cell><cell cols="2">32.89 24.46 38.47 51.11 52.90 39.23</cell></row><row><cell>REGENREL</cell><cell cols="2">32.91 24.80 38.46 51.07 53.68 39.45</cell></row><row><cell cols="3">SYSLOWCON 31.66 26.06 34.03 50.92 51.08 39.19</cell></row><row><cell cols="2">Our Method: CLIFF</cell><cell></cell></row><row><cell>BATCH</cell><cell cols="2">32.64 24.96 38.42 51.03  *  51.81  *  39.38</cell></row><row><cell>FCSAMPLE</cell><cell>32.96</cell><cell></cell></row><row><cell>Human inspection finds that ENTAIL-</cell><cell></cell><cell></cell></row><row><cell>RANK can pick up beams with peculiar words of</cell><cell></cell><cell></cell></row><row><cell>high FactCC scores, without improving factuality.</cell><cell></cell><cell></cell></row><row><cell>Moreover, other comparisons based on post COR-</cell><cell></cell><cell></cell></row><row><cell>RECTION and model engineering (FASUM) only of-</cell><cell></cell><cell></cell></row><row><cell>fer incremental gains. The sample selection-based</cell><cell></cell><cell></cell></row><row><cell>method, SUBSETFT, sacrifices ROUGE scores sig-</cell><cell></cell><cell></cell></row><row><cell>nificantly. Overall, CLIFF demonstrates stronger</cell><cell></cell><cell></cell></row><row><cell>generalizability.</cell><cell></cell><cell></cell></row><row><cell>Second, CLIFF is more effective and robust than</cell><cell></cell><cell></cell></row></table><note>*  </note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Results of fine-tuned BART with combinations of negative sample construction strategies.</figDesc><table><row><cell>Strategy</cell><cell>XSum</cell><cell>CNN/DM</cell></row><row><cell></cell><cell cols="2">QEval FC R-L QEval FC R-L</cell></row><row><cell cols="3">SYSLOWCON 33.35 25.47 36.19 51.05 50.05 41.01</cell></row><row><cell cols="3">+ SWAPENT 33.40 25.50 35.50 51.32 53.95 40.57</cell></row><row><cell cols="3">+ MASKENT 33.21 25.47 35.91 51.16 51.90 40.66</cell></row><row><cell cols="3">+ MASKREL 33.39 25.20 35.70 51.24 52.48 40.80</cell></row><row><cell cols="3">+ REGENENT 33.31 25.07 35.94 51.21 51.86 40.91</cell></row><row><cell cols="3">+ REGENREL 33.38 24.97 36.03 51.13 50.85 40.97</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Percentages of summaries that are better than, tied with, or worse than CRSENTROPY, in informativeness (Inform.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 .</head><label>7</label><figDesc>The implications are at least two-fold. First, even for entity-or relationtriggered sample modifications, using more global context helps with CL training. Second, additional transformation can help avoid model degeneration. For instance, more nonsensical and repetitive content is produced by variants without MLP.</figDesc><table><row><cell></cell><cell>SWAPENT</cell><cell>MASKREL SYSLOWCON</cell></row><row><cell cols="3">Rep. MLP QEval FC QEval FC QEval FC</cell></row><row><cell>BART</cell><cell></cell></row><row><cell>Last</cell><cell cols="2">33.15 25.10 33.20 25.29 33.10 24.85</cell></row><row><cell>Last</cell><cell cols="2">+0.13 +0.02 -0.01 -0.32 -0.07 -0.10</cell></row><row><cell>Entity</cell><cell cols="2">33.35 25.41 33.34 25.44 33.32 24.46</cell></row><row><cell>Entity</cell><cell cols="2">-0.13 -0.07 -0.14 -0.05 -0.29 +0.72</cell></row><row><cell>All</cell><cell cols="2">33.30 25.67 33.35 25.69 33.35 25.47</cell></row><row><cell>All</cell><cell cols="2">-0.23 -0.80 -0.04 -0.48 -0.26 -0.40</cell></row><row><cell>PEGASUS</cell><cell></cell></row><row><cell>Last</cell><cell cols="2">33.07 25.45 32.99 25.09 33.18 24.94</cell></row><row><cell>Last</cell><cell cols="2">-0.07 -0.56 +0.01 -0.01 -0.02 -0.04</cell></row><row><cell>Entity</cell><cell cols="2">33.03 25.43 33.05 24.77 33.20 24.59</cell></row><row><cell>Entity</cell><cell cols="2">+0.01 -0.34 -0.05 +0.64 -0.30 +0.05</cell></row><row><cell>All</cell><cell cols="2">33.09 25.09 33.06 25.28 33.21 25.18</cell></row><row><cell>All</cell><cell cols="2">-0.11 +0.25 +0.03 -0.19 -0.02 -0.80</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Comparing different formulations of summary representation in CL. For models without MLP, we display score changes from their counterparts. Overall, using all tokens with MLP produces better summaries.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Numbers of samples in train/validation/test splits of XSum and CNN/DM.</figDesc><table><row><cell>Positive Samples. We observe unfaithful para-</cell></row><row><cell>phrases by back-translation for some reference</cell></row><row><cell>summaries, which are mainly due to the introduc-</cell></row><row><cell>tion of new entities and the rewriting of quoted</cell></row><row><cell>text. Thus, we discard samples generated by back-</cell></row><row><cell>translation that contain new entities and inconsis-</cell></row><row><cell>tent quoted text. Finally, we obtain 182,114 and</cell></row><row><cell>91,468 positive samples by back-translation on</cell></row><row><cell>XSum and CNN/DM.</cell></row><row><cell>Negative Samples. For consistency, we use the</cell></row><row><cell>summarizer fine-tuned from BART in REGENENT,</cell></row><row><cell>REGENREL ( § 5.3), and SYSLOWCON ( § 5.4)</cell></row><row><cell>strategies. We tune a threshold to select negative</cell></row><row><cell>samples from model generations in our SYSLOW-</cell></row><row><cell>CON strategy. The threshold is set to 0.21, with F1</cell></row><row><cell>scores of 73.99 and 40.49 on XSum and CNN/DM</cell></row><row><cell>annotated samples generated by BART.</cell></row><row><cell>The numbers of negative samples constructed by</cell></row><row><cell>each strategy for training on XSum and CNN/DM</cell></row><row><cell>are shown in</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 .</head><label>9</label><figDesc>SYSLOWCON constructs the least negative samples in total, while it achieves the best results as reported in our main paper ( § 7.1), indicating that its negative samples are more effective for training.</figDesc><table><row><cell>Strategy</cell><cell>XSum</cell><cell>CNN/DM</cell></row><row><cell>FCSAMPLE</cell><cell cols="2">936,164 1,291,710</cell></row><row><cell>SWAPENT</cell><cell cols="2">438,003 1,617,764</cell></row><row><cell>MASKENT</cell><cell cols="2">360,795 1,050,200</cell></row><row><cell>MASKREL</cell><cell cols="2">391,224 1,345,317</cell></row><row><cell>REGENENT</cell><cell cols="2">732,986 1,941,886</cell></row><row><cell>REGENREL</cell><cell cols="2">993,694 1,453,044</cell></row><row><cell cols="2">SYSLOWCON 401,112</cell><cell>502,768</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9 :</head><label>9</label><figDesc>Numbers of negative samples constructed by different strategies on XSum and CNN/DM.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">https://github.com/makcedward/nlpaug</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This research is supported in part by Oracle for Research Cloud Credits, National Science Foundation through a CAREER award IIS-2046016, and the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via contract # FA8650-17-C-9116.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Example annotations 1 Article: Isis Academy in Oxford said it had rebranded as "Iffley Academy" to protect its "reputation, integrity and image". The name 'Isis' was originally chosen as the school is near to the section of the River Thames of the same name. Formerly Iffley Mead School, it became Isis Academy in 2013. A statement issued by the school said it had changed name following "the unforeseen rise of ISIS (also known as ISIL and the Islamic State) and related global media coverage of the activities of the group". "Our priority is to remove the detrimental impact which the name 'Isis' had on pupils, their families and our staff." Last year a language school in the city removed Isis from its name for the same reason. The Isis is the name given to the part of the River Thames above Iffley Lock in Oxford. It is also the name of the goddess wife of the god Osiris in Egyptian beliefs. Summary: A school that was named after the Islamic State (IS) militant group has changed its name. Explanation: "was name after" is an intrinsic error contradicted by the article sentence in bold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example annotations 2</head><p>Article: Khalil Dale, 60, was abducted in Quetta in January 2012 and was found dead on a roadside a few months later. He had been beheaded. A note next to his body said he was killed because a ransom had not been paid. Mr Dale was born in York but lived in Dumfries. He spent 30 years working in countries including Somalia, Afghanistan and Iraq. An inquest into his death was held at Chesterfield Coroners Court because he is buried in Derbyshire. The court heard that the Muslim convert, who was formerly known as Kenneth, worked as a humanitarian assistance relief worker. Following his abduction, negotiations were undertaken by the International Committee of the Red Cross with the help of the UK government. His body was found on 29 April 2012. The inquest was told that he died as a result of decapitation. Senior coroner Dr Robert Hunter concluded that Mr Dale was unlawfully killed while providing international humanitarian assistance. Summary: A British aid worker was unlawfully killed by Islamist militants in Pakistan, an inquest has heard. Explanation: "Islamist militant" is an extrinsic error as it can not be found in or inferred from the article. The information is also not verifiable by Wikipedia. "Pakistan" is world knowledge as Quetta in the article is a city in Pakistan according to Wikipedia.</p><p>Figure <ref type="figure">9</ref>: Guideline for our summary error annotation ( § 4).</p><p>In this study, you will evaluate 100 sets of summaries produced by four systems. For each set, its corresponding article and a baseline summary are shown before the four system summaries. The errors in the baseline summary are highlighted. Please first read the article and the baseline summary and then compare each system summary against the baseline summary based on informativeness and factual consistency. In addition, please decide the operations made by the system to achieve better factual consistency. For informativeness and factual consistency, you need to label whether the system summary is better or worse than the baseline summary. You can also label the system summary as tying with the baseline summary.</p><p>You need to consider two types of operations: deletions and substitutions. Please label the system summary as making deletions, substitutions, or both operations. Examples for the aspects and the operations are as follows.</p><p>Article: Alexys Brown, also known as Lexi, died at her home in Emmadale Close, Weymouth, on Thursday. An investigation is under way to discover how she became trapped. A post-mortem examination is due to be carried out this week. It was originally hoped the appeal would raise £2,000. Alison Record, who started the Just Giving appeal, said she was "heart broken" over the death. "Everybody by now has heard of the terrible tragedy the Brown family have suffered with the loss of their beautiful and beloved little girl Lexi," the appeal page reads. Many other comments have been posted on the appeal page. Steph Harris said: "Thinking of you all at this devastating time, fly high beautiful princess. Love Steph and family xxx" Lesley Andrews added: "No amount of money will take away the pain, but so much love comes with every penny. Take care. xx" Aster Group, the housing association responsible for managing the home, is assisting with the police investigation. The Health and Safety Executive (HSE) is also investigating. Dorset County Council said it had not installed the disabled lift at the property. Baseline Summary: An appeal to raise 10,000 pounds for the family of a three-year-old girl who died after becoming trapped in a lift has raised more than 20,000 pounds.</p><p>Informativeness: Whether the summary captures salient content from the input article. Note that incorrect content should be considered as invalid.</p><p>Win. An appeal to raise money for the family of a three-year-old girl who died after getting stuck in a lift was originally hoped for raising £2,000. The target money of the appeal is a salient information.</p><p>Tie. An appeal to raise money for the family of a girl who died after getting stuck in a lift has raised more than £20,000. Compared to the baseline, missing incorrect information does not affect the informativeness.</p><p>Lose. An appeal to raise money for the family of a three-year-old girl has raised more than £20,000. This system summary does not mention the death of the girl, which is a salient content of the article.</p><p>Factual Consistency: Whether the summary is factually correctly based on the article and knowledge from Wikipedia.</p><p>Win. An appeal has been set up for the family of an eight-year-old girl who died after becoming trapped in a lift at her Dorset home. This system summary does not generate the incorrect numbers of money.</p><p>Tie. An appeal to raise 5,000 pounds for the family of a seven-year-old girl who died after becoming trapped in a lift has raised more than 20,000 pounds. This system summary makes similar errors to the baseline.</p><p>Lose. The family of an eight-year-old girl who died after becoming trapped in a lift at her Dorset home have set a fundraising target of 10,000 pounds. This system summary fabricates an event The family have set a fundraising target, which is more severe than errors of modifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deletion:</head><p>The incorrect content in the baseline summary is deleted.</p><p>-An appeal for the family of a three-year-old girl who died after becoming trapped in a lift has raised more than 20,000 pounds. The error "10,000 pounds" is deleted. Substitution: The incorrect content in the baseline summary is replaced with correct one.</p><p>-An appeal to raise 2,000 pounds for the family of a three-year-old girl who died after becoming trapped in a lift has raised more than 20,000 pounds. The error "10,000 pounds" is substituted with "2,000 pounds", which is the correct information. Example 1 CNN/DM Article: At the grand old age of 75, Jack Nicklaus is still capable of hitting aces. The Golden Bear added another magic moment to his storied career at Augusta National in the Par-3 Contest. Stepping up to the tee on the 130-yard fourth, the greatest golfer of all time saw his shot sail beyond the flag before spinning back into the hole. Jack Nicklaus gave the crowd something to cheer with a hole in one on the fourth during the Par-3 Contest. Nicklaus holds up his ball to an adoring crowd as Gary Player (left) and Ben Crenshaw salute the great. Crenshaw and Nicklaus fist pump following his ace on the 130-yard hole at Augusta National. Nicklaus was playing alongside Gary Player and Ben Crenshaw as the first roar of Masters week filled the skies in a sunswept Georgia. 'I had an interview this morning, and I said "well, all I've got to do is go out and win the Par-3 and make a hole and one", and I make a hole in one,' Nicklaus said afterwards. . The ace was Nicklaus' first on the hallowed turf of Augusta, but the veteran saw his achievement eclipsed by that of Camilo Villegas on a day which saw five hole-in-ones, levelling the record from 2002. The Colombian followed Nicklaus with an ace on the fourth and also managed one on the eighth, but he couldn't quite land the title, losing out to world No 74 Kevin Streelman in a play-off. Nicklaus became the youngest person to wear a green jacket in 1963, and collected his sixth in 1986. He is one of five men to complete the career grand slam, an accolade which favourite Rory McIlroy can achieve if he wins his third major in succession.</p><p>Cross-entropy: Jack Nicklaus hit a hole in one on the fourth during the Par-3 Contest. The 75-year-old was playing alongside Gary Player and Ben Crenshaw. Camilo Villegas and Kevin Streelman also hit hole-in-ones on the day. Entailment reranking: Jack Nicklaus hit a hole in one on the fourth during the Par-3 Contest. The 75-year-old was playing alongside Gary Player and Ben Crenshaw. Camilo Villegas and Kevin Streelman also hit hole-in-ones on day one. Unlikelihood training: Jack Nicklaus hit a hole in one on the fourth during the Par-3 Contest. The 75-year-old was playing alongside Gary Player and Ben Crenshaw. Camilo Villegas followed Nicklaus with a hole-in-one on the same hole. Nicklaus is one of five men to complete the career grand slam. Contrastive learning: Jack Nicklaus hit a hole in one on the fourth during the Par-3 Contest. The 75-year-old was playing alongside Gary Player and Ben Crenshaw. Camilo Villegas followed Nicklaus with a hole-in-one on the eighth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example 2</head><p>XSum Article: The British supermarket giant, the world's third largest retailer, has been looking to dispose of its loss-making US food chain for some time. Around 4,000 of Fresh &amp; Easy's 5,000 employees are to transfer to the new business as part of the deal. Tesco said in a statement that this is an "orderly and efficient exit from the US market". Under the terms of the deal, Tesco will loan the new business around £80m. Fresh &amp; Easy, which operates 200 stores across California, Nevada and Arizona, has lost money since opening in 2007. Tesco did not specify which stores are being transferred to Yucaipa, but it is understood the remaining 50 shops will be closed. It will cost Tesco £150m to dispose of the Fresh &amp; Easy brand, but part of that is a loan which may be paid back. Also, if Yucaipa is sucessful and the business becomes profitable, Tesco has the option to buy a stake. Yucaipa is a Los Angeles based private equity company focused on supermaket and restaurant businesses and is run by billionaire investor Ron Burkle. In April of this year, Tesco reported its first fall in annual profits for 20 years.</p><p>Cross-entropy: Tesco has agreed to sell its US food chain Fresh &amp; Easy to Yucaipa in a deal worth about 150m, the BBC understands.</p><p>Entailment reranking: Tesco is to sell its US food chain Fresh &amp; Easy to Yucaipa for an undisclosed sum.</p><p>Unlikelihood training: Tesco has agreed to sell its US food business, Fresh &amp; Easy, to a private equity firm called Yucaipa for an undisclosed sum. Contrastive learning: Tesco has agreed to sell its US food chain, Fresh &amp; Easy, to Yucaipa.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Focus attention: Promoting faithfulness and diversity in summarization</title>
		<author>
			<persName><forename type="first">References</forename><surname>Rahul Aralikatte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Maynez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sascha</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.474</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<title level="s">Long Papers</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6078" to="6095" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Factual error correction for abstractive summarization models</title>
		<author>
			<persName><forename type="first">Meng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiapeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jackie</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kit</forename><surname>Cheung</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.506</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6251" to="6258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Faithful to the original: Fact aware neural abstractive summarization</title>
		<author>
			<persName><forename type="first">Ziqiang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Improving faithfulness in abstractive summarization with contrast candidate generation and selection</title>
		<author>
			<persName><forename type="first">Sihao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazoo</forename><surname>Sone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.475</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5935" to="5941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<title level="s">Long and Short Papers</title>
		<editor>
			<persName><forename type="first">Pmlr</forename><forename type="middle">Jacob</forename><surname>Devlin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</editor>
		<meeting>the 37th International Conference on Machine Learning<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2020. 2019</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Proceedings of the 2019 Conference of the North American Chapter</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-fact correction in abstractive text summarization</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jackie</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kit</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.749</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9320" to="9331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">FEQA: A question answering evaluation framework for faithfulness assessment in abstractive summarization</title>
		<author>
			<persName><forename type="first">Esin</forename><surname>Durmus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">He</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Alexander R Fabbri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Kryściński</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.454</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Richard Socher, and Dragomir Radev</publisher>
			<date type="published" when="2020">2020. 2021</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="391" to="409" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Ranking generated summaries by correctness: An interesting but challenging application for natural language inference</title>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Falke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonardo</forename><forename type="middle">F R</forename><surname>Ribeiro</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1213</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2214" to="2220" />
		</imprint>
	</monogr>
	<note>Prasetya Ajie Utama, Ido Dagan, and Iryna Gurevych</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Cert: Contrastive selfsupervised learning for language understanding</title>
		<author>
			<persName><forename type="first">Hongchao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayuan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengtao</forename><surname>Xie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Gen-ERRate: Generating errors for use in grammatical error detection</title>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oistein</forename><surname>Andersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Workshop on Innovative Use of NLP for Building Educational Applications</title>
				<meeting>the Fourth Workshop on Innovative Use of NLP for Building Educational Applications<address><addrLine>Boulder</addrLine></address></meeting>
		<imprint>
			<publisher>Colorado. Association for Computational Linguistics</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="82" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">GO FIGURE: A meta evaluation of factuality in summarization</title>
		<author>
			<persName><forename type="first">Saadia</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.42</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
				<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="478" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Simcse: Simple contrastive learning of sentence embeddings</title>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingcheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Evaluating factuality in generation with dependency-level entailment</title>
		<author>
			<persName><forename type="first">Tanya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.322</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
				<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3592" to="3603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Annotating and modeling fine-grained factuality in summarization</title>
		<author>
			<persName><forename type="first">Tanya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.114</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1449" to="1462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Alexis Conneau, and Veselin Stoyanov. 2021. Supervised contrastive learning for pre-trained language model fine-tuning</title>
		<author>
			<persName><forename type="first">Beliz</forename><surname>Gunel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Negative training for neural dialogue response generation</title>
		<author>
			<persName><forename type="first">Tianxing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.185</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2044" to="2058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The curious case of neural text degeneration</title>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Knowledge graph-augmented abstractive summarization with semantic-driven cloze reward</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Honnibal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ines</forename><surname>Montani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sofie</forename><surname>Van Landeghem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriane</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">; Luyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.457</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="5094" to="5107" />
		</imprint>
	</monogr>
	<note>spaCy: Industrial-strength Natural Language Processing in Python</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Supervised Contrastive Learning</title>
		<author>
			<persName><forename type="first">Prannay</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="18661" to="18673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Contextual augmentation: Data augmentation by words with paradigmatic relations</title>
		<author>
			<persName><forename type="first">Sosuke</forename><surname>Kobayashi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-2072</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Short Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="452" to="457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Evaluating the factual consistency of abstractive text summarization</title>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Kryscinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.750</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9332" to="9346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.703</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">Marjan Ghazvininejad,. 2020</date>
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sampling matters! an empirical study of negative sampling strategies for learning of matching models in retrievalbased dialogue systems</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongyang</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1128</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1291" to="1296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Don&apos;t say that! making inconsistent dialogue unlikely with unlikelihood training</title>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilia</forename><surname>Kulikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Welleck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y-Lan</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.428</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004">2020. 2004</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
	<note>Text Summarization Branches Out</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">A token-level reference-free hallucination detection benchmark for free-form text generation</title>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Text summarization with pretrained encoders</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1387</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3730" to="3740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">SimCLS: A simple framework for contrastive learning of abstractive summarization</title>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-short.135</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
				<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1065" to="1072" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Paraphrasing revisited with neural machine translation</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Mallinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
				<meeting>the 15th Conference of the European Chapter<address><addrLine>Long Papers; Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="881" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On faithfulness and factuality in abstractive summarization</title>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Maynez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.173</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1906" to="1919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean ; Nan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cicero</forename><surname>Nogueira Dos Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henghui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dejiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</title>
				<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</meeting>
		<imprint>
			<date type="published" when="2013">2013. 2021</date>
			<biblScope unit="page" from="2727" to="2733" />
		</imprint>
	</monogr>
	<note>Entitylevel factual consistency of abstractive text summarization. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Don&apos;t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization</title>
		<author>
			<persName><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1206</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1797" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">fairseq: A fast, extensible toolkit for sequence modeling</title>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT 2019</title>
				<meeting>NAACL-HLT 2019</meeting>
		<imprint>
			<publisher>Demonstrations</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Understanding factuality in abstractive summarization with frank: A benchmark for factuality metrics</title>
		<author>
			<persName><forename type="first">Artidoro</forename><surname>Pagnoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vidhisha</forename><surname>Balachandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Contrastive learning for unpaired image-to-image translation</title>
		<author>
			<persName><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="319" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Stanza: A python natural language processing toolkit for many human languages</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-demos.14</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="101" to="108" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Generating natural language adversarial examples through probability weighted word saliency</title>
		<author>
			<persName><forename type="first">Yihe</forename><surname>Shuhuai Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanxiang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><surname>Che</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1103</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1085" to="1097" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A theoretical analysis of contrastive unsupervised representation learning</title>
		<author>
			<persName><forename type="first">Nikunj</forename><surname>Saunshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orestis</forename><surname>Plevrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Khodak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hrishikesh</forename><surname>Khandeparkar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.12693</idno>
	</analytic>
	<monogr>
		<title level="m">Staiano Jacopo, and Wang Alex. 2021. Questeval: Summarization asks for fact-based evaluation</title>
				<editor>
			<persName><forename type="first">Pmlr</forename><forename type="middle">Thomas</forename><surname>Scialom</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Paul-Alexis</forename><surname>Dray</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Gallinari</forename><surname>Patrick</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Lamprier</forename><surname>Sylvain</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Piwowarski</forename><surname>Benjamin</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5628" to="5637" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>International Conference on Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Answers unite! unsupervised metrics for reinforced summarization models</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Scialom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Lamprier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Piwowarski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacopo</forename><surname>Staiano</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1320</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3246" to="3256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Asking and answering questions to evaluate the factual consistency of summaries</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.450</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5008" to="5020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Neural text generation with unlikelihood training</title>
		<author>
			<persName><forename type="first">Sean</forename><surname>Welleck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilia</forename><surname>Kulikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Dejiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaokai</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shangwen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henghui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<title level="m">Supporting clustering with contrastive learning</title>
				<imprint>
			<date type="published" when="2021">2021a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Cross-modal contrastive learning for text-to-image generation</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Yu Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.04702</idno>
		<imprint>
			<date type="published" when="2021">2021b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Pegasus: Pre-training with extracted gap-sentences for abstractive summarization</title>
		<author>
			<persName><forename type="first">Jingqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020a</date>
			<biblScope unit="page" from="11328" to="11339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Luke Hannant replaces Gus Mafuta. Gus Mafuta (Gateshead) is shown the yellow card. Substitution, Guiseley. Adam Boyes replaces Jake Cassidy. Goal! Gateshead 1, Guiseley 1. Wes York (Gateshead). Substitution, Guiseley. Derek Asamoah replaces Kevan Hurst. Second Half begins Gateshead 0, Guiseley 1. First Half ends, Gateshead 0, Guiseley 1. Simon Walton (Guiseley) is shown the yellow card. Goal! Gateshead 0, Guiseley 1. Jake Lawlor (Guiseley). First Half begins. Lineups are announced and players are warming up. Cross-entropy: Gateshead and Guiseley maintained their unbeaten starts to the season with a 1-1 draw at the International Stadium. Entailment reranking: Gateshead and Guiseley shared the spoils after a 1-1 draw at the International Stadium</title>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Merck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Curtis</forename><surname>Langlotz</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.458</idno>
	</analytic>
	<monogr>
		<title level="m">Optimizing the factual correctness of a summary: A study of summarizing radiology reports</title>
				<meeting><address><addrLine>Gateshead 1, Guiseley 1. Substitution, Guiseley. Michael Rankine replaces Jordan Preston. Substitution, Gateshead</addrLine></address></meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020b</date>
			<biblScope unit="page" from="5108" to="5120" />
		</imprint>
	</monogr>
	<note>Guiseley went ahead on 15 minutes against the run of play when a throw-in found James Hurst who squared to Jake Lawlor to stroke into an empty net. Gateshead defender Liam Hogan superbly blocked Jordan Preston&apos;s effort and Guiseley keeper Jonny Maxted then saved well from Wesley York&apos;s shot just before the break. The hosts, who started the second half well, levelled on 62 minutes when a slip by half-time substitute Derek Asamoah let York curl sweetly into the top-right corner from the edge of the box. Unlikelihood training: Gateshead and Guiseley shared the spoils after a goalless draw in the National League. Contrastive learning: Gateshead and Guiseley shared the spoils after a 1-1 draw at Gateshead</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
