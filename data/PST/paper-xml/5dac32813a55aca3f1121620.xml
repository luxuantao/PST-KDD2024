<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-12-27">27 Dec 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hengshun</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">National Engineering Laboratory for Speech and Language Information Processing (NEL-SLIP)</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="laboratory">National Engineering Laboratory for Speech and Language Information Processing (NEL-SLIP)</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Debin</forename><surname>Meng</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">SIAT-SenseTime Joint Lab</orgName>
								<orgName type="department" key="dep2">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="laboratory">ShenZhen Key Lab of Computer Vision and Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">SIAT-SenseTime Joint Lab</orgName>
								<orgName type="department" key="dep2">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="laboratory">ShenZhen Key Lab of Computer Vision and Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuanyuan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">National Engineering Laboratory for Speech and Language Information Processing (NEL-SLIP)</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="laboratory">National Engineering Laboratory for Speech and Language Information Processing (NEL-SLIP)</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
							<email>xj.peng@siat.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">SIAT-SenseTime Joint Lab</orgName>
								<orgName type="department" key="dep2">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="laboratory">ShenZhen Key Lab of Computer Vision and Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">SIAT-SenseTime Joint Lab</orgName>
								<orgName type="department" key="dep2">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="laboratory">ShenZhen Key Lab of Computer Vision and Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Du</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">National Engineering Laboratory for Speech and Language Information Processing (NEL-SLIP)</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="laboratory">National Engineering Laboratory for Speech and Language Information Processing (NEL-SLIP)</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kai</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">SIAT-SenseTime Joint Lab</orgName>
								<orgName type="department" key="dep2">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="laboratory">ShenZhen Key Lab of Computer Vision and Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">SIAT-SenseTime Joint Lab</orgName>
								<orgName type="department" key="dep2">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="laboratory">ShenZhen Key Lab of Computer Vision and Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">SIAT-SenseTime Joint Lab</orgName>
								<orgName type="department" key="dep2">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="laboratory">ShenZhen Key Lab of Computer Vision and Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">SIAT-SenseTime Joint Lab</orgName>
								<orgName type="department" key="dep2">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="laboratory">ShenZhen Key Lab of Computer Vision and Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="middle">2019</forename><surname>Exploring Emotion</surname></persName>
						</author>
						<title level="a" type="main">Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-12-27">27 Dec 2020</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3340555.3355713</idno>
					<idno type="arXiv">arXiv:2012.13912v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS</term>
					<term>Computer systems organization â†’ Embedded systems</term>
					<term>Redundancy</term>
					<term>Robotics</term>
					<term>â€¢ Networks â†’ Network reliability Emotion Recognition</term>
					<term>Attention Mechanism</term>
					<term>Deep learning</term>
					<term>Affective Computing</term>
					<term>Convolutional Neural Networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The audio-video based emotion recognition aims to classify a given video into basic emotions. In this paper, we describe our approaches in EmotiW 2019, which mainly explores emotion features and feature fusion strategies for audio and visual modality. For emotion features, we explore audio feature with both speech-spectrogram and Log Mel-spectrogram and evaluate several facial features with different CNN models and different emotion pretrained strategies. For fusion strategies, we explore intra-modal and cross-modal fusion methods, such as designing attention mechanisms to highlights important emotion feature, exploring feature concatenation and factorized bilinear pooling (FBP) for cross-modal feature fusion. With careful evaluation, we obtain 65.5% on the AFEW validation set and 62.48% on the test set and rank second in the challenge.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Emotion recognition(ER) has attracted increasing attention in academia and industry due to its wide range of applications such as human-computer interaction <ref type="bibr" target="#b6">[7]</ref>, clinical diagnosis <ref type="bibr" target="#b18">[19]</ref>, and cognitive science <ref type="bibr" target="#b13">[14]</ref>. Although great progress in the face and video analysis has been made <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref>, audio-video emotion recognition in the wild remains a challenging problem due to the expression suffers from the large pose, illumination variance, occlusion, motion blur, etc.</p><p>Audio-Video emotion recognition can be summarized as a simple pipeline shown in Fig <ref type="figure" target="#fig_0">1</ref>, which includes four parts, namely Video preprocessing, Feature Extraction, Feature Fusion, and Classifier. Specifically, video preprocessing refers to extract the spectrogram of the audio, the faces or landmarks of video. Feature extraction and feature fusion respectively extracts emotion features from the audio or visual signal and fuses emotion features into compact feature vectors, which are subsequently fed into a classifier for prediction.</p><p>Reviewing the methods of Audio-Video emotion recognition, we find that some methods emphasize feature extraction and other methods emphasize feature fusion. Yao et al <ref type="bibr" target="#b30">[31]</ref> construct Holonet as discriminative feature extraction, which combines residual structure <ref type="bibr" target="#b11">[12]</ref> and CReLU <ref type="bibr" target="#b21">[22]</ref> to increase network depth and maintain efficiency. The EmotiW2017 winner team <ref type="bibr" target="#b12">[13]</ref> gets robust feature extraction with Supervised Scoring Ensemble (SSE) which adds supervision to intermediate layers and shallow layers. Since SSE only uses high-level representations, Fan et al <ref type="bibr" target="#b7">[8]</ref> further improve SSE by utilizing middle feature maps to provide more discriminative features. These methods mainly use average pooling to obtain video-level representation from frame-level.</p><p>Many feature fusion strategies have been used in previous EmotiW challenges. <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b24">25]</ref> extract CNN-based frame features and use LSTM <ref type="bibr" target="#b9">[10]</ref> or BLSTM <ref type="bibr" target="#b10">[11]</ref> to fuse them. <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17]</ref>    maximum of the frame feature vectors. However, these methods ignore the importance of frames. Besides, all previous methods mainly apply score averaging or feature concatenation for audio-video fusion, which ignores the correlation between the features from different modalities.</p><p>In this paper, we exploit three types of intra-modal fusion methods, namely self-attention, relation-attention, and transformer <ref type="bibr" target="#b23">[24]</ref>. They are used to learn weights for frame features to highlight important frames. For cross-modal fusion, we explore feature concatenation and factorized bilinear pooling (FBP) <ref type="bibr" target="#b31">[32]</ref>. Besides, we evaluate different emotion features, including convolutional neural networks (CNN) for audio information with both speech-spectrogram and Log Mel-spectrogram and several facial features with different CNN models and different emotion pretrained strategies. Finally, we obtain 62.48% and rank second in the challenge.</p><p>Our contributions and finds can be summarized as follows.</p><p>â€¢ We experimentally show that better face recognition CNN models and choosing suitable emotion datasets to further pretrain the face CNN models is important. â€¢ We design three kinds of attention mechanisms for visual and audio feature fusion. â€¢ We apply a Factorized Bilinear Pooling (FBP) for crossmodal feature fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">THE PROPOSED METHOD</head><p>We develop our ER system based on the pipeline of Video preprocessing-Feature Extraction-Feature Fusion-Classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Video preprocessing</head><p>Face detection and alignment. We apply face detection and alignment by Dlib toolbox <ref type="foot" target="#foot_0">1</ref> . We extend the face bounding box with a ratio of 30% and then resize the cropped faces to scale of 224 Ã— 224. We do not apply face detection and alignment Audio processing and Spectrogram calculation. For each audio, the speech spectrogram and log Mel-spectrogram extraction process is consistent with <ref type="bibr" target="#b31">[32]</ref> and <ref type="bibr" target="#b2">[3]</ref> respectively.</p><p>For speech spectrogram, we use the Hamming window with 40 msec window size and 10 msec shift. Finally, the 200dimensional low-frequency part of the spectrogram is used as the input to the audio modality. As for log Mel-spectrogram, we calculate its deltas and delta-deltas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Extraction</head><p>Visual Features. We apply three CNN backbones to extract facial emotion features, namely VGGFace, ResNet18, and IR50 <ref type="bibr" target="#b3">[4]</ref>. The dimensions are 4096, 512, and 512, respectively.</p><p>Audio Feature. We extract the feature maps of the audio from the last Pooling layer of AlexNet. The size of a 3-dimensional feature map is ğ» Ã—ğ‘Š Ã—ğ¶, where the ğ» (ğ‘Š ) is the height(width) of the feature map, and ğ¶ is the number of the channel of the feature map. The feature maps are then split into n vectors(ğ‘› = ğ» Ã— ğ‘Š ). Each vector is C-dimensional.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Intra-modal Feature Fusion</head><p>We apply the attention-based strategies for intra-modal feature fusion. It converts a variable number of emotion features(from audio or visual modality) into a fixed-dimension feature. We explore three attention methods, namely Selfattention, Relation-attention, and Transformer-attention. Formally, we denote a number of emotion features as {ğ‘“ 1 , â€¢ â€¢ â€¢ , ğ‘“ ğ‘› }.</p><p>Self-attention. We apply 1-dimensional Fully-Connected(FC) layer W 0 dÃ—1 and a sigmoid function ğœ for each emotion feature, the weight of the ğ‘–-th feature ğ‘“ ğ‘‡ ğ‘– is defined by:</p><formula xml:id="formula_0">ğ›¼ ğ‘– = ğœ (ğ‘“ ğ‘‡ ğ‘– â€¢ W 0 dÃ—1 )<label>(1</label></formula><p>) With these self-attention weights, we aggregate all the emotion features into a global representation ğ‘“ ğ‘  as follows:</p><formula xml:id="formula_1">ğ‘“ ğ‘  = ğ‘› ğ‘–=1 ğ›¼ ğ‘– ğ‘“ ğ‘– ğ‘› ğ‘—=1 ğ›¼ ğ‘— .<label>(2)</label></formula><p>Relation-attention. </p><formula xml:id="formula_2">ğ›½ ğ‘– = ğœ ( [ğ‘“ ğ‘– : ğ‘“ ğ‘  ] ğ‘‡ â€¢ W 1 dÃ—1 ),<label>(3</label></formula><p>) With Self-attention and Relation-attention weights, all the emotion features was convert into a new feature as follows:</p><formula xml:id="formula_3">ğ‘“ ğ‘Ÿ = ğ‘› ğ‘–=0 ğ›¼ ğ‘– ğ›½ ğ‘– [ğ‘“ ğ‘– : ğ‘“ ğ‘  ] ğ‘› ğ‘—=0 ğ›¼ ğ‘— ğ›½ ğ‘— .<label>(4)</label></formula><p>Transformer-attention. Inspired by the works in <ref type="bibr" target="#b31">[32]</ref> and <ref type="bibr" target="#b29">[30]</ref>, we formulate the attention weight as follows:</p><formula xml:id="formula_4">ğ‘“ â€² ğ‘– = W 2 mÃ—d â€¢ ğ‘“ ğ‘– + ğ‘<label>(5)</label></formula><formula xml:id="formula_5">ğ›¾ ğ‘– = ğ‘’ğ‘¥ğ‘ (u t ğ‘šÃ—1 â€¢ ğ‘¡ğ‘ğ‘›â„(ğ‘“ â€² ğ‘– ))<label>(6)</label></formula><p>To reduce the dimension of the feature ğ‘“ ğ‘– , we use a ğ‘¤ Ã— ğ‘‘dimensional FC layer W 2 mÃ—d in Eq. <ref type="bibr" target="#b4">(5)</ref>. Then the weight of the ğ‘–-th feautre ğ‘“ ğ‘– is processed by a 1-dimensional FC layer u t , ğ‘’ğ‘¥ğ‘ () and ğ‘¡ğ‘ğ‘›â„() function in Eq. <ref type="bibr" target="#b5">(6)</ref>.</p><p>With these transformer-attention weights, we aggregate all the emotion features into a single feature ğ‘“ ğ‘¡ as follows:</p><formula xml:id="formula_6">ğ‘“ ğ‘¡ = ğ‘› ğ‘–=1 ğ›¾ ğ‘– ğ‘“ ğ‘– ğ‘› ğ‘—=1 ğ›¾ ğ‘— .<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-modal Feature Fusion</head><p>We apply Factorized Bilinear Pooling(FBP) for cross-modal feature fusion. Given two features in different modalities,i.e. the audio feature vector ğ’‚ âˆˆ R ğ‘š for a spectrogram and visual feature ğ’— âˆˆ R ğ‘› for frame sequence, the simplest cross-modal bilinear model is defined as follows:</p><formula xml:id="formula_7">ğ‘§ ğ‘– = ğ’‚ ğ‘‡ ğ‘¾ ğ‘– ğ’—<label>(8)</label></formula><p>where ğ‘¾ âˆˆ R ğ‘šÃ—ğ‘› is a projection matrix, ğ’› ğ’Š âˆˆ R is the output of the bilinear model. we use the Eq.( <ref type="formula" target="#formula_8">9</ref>) to obtain the output feature ğ’› = [ğ‘§ 1 , â€¢ â€¢ â€¢ , ğ‘§ ğ‘œ ]. The formula derivation from formula Eq.( <ref type="formula" target="#formula_7">8</ref>) to Eq <ref type="bibr" target="#b8">( 9)</ref> was discribed in the paper <ref type="bibr" target="#b31">[32]</ref>. The implementation of Eq( <ref type="formula" target="#formula_8">9</ref>) is illustrated in Fig2, where Å¨ğ‘‡ ğ’‚ and á¹¼ğ‘‡ ğ’— are implemented by feeding feature ğ’‚ and ğ’— to FC layers, respectively, and the function SumPooling(ğ’™, ğ‘˜) applies sum pooling with non-overlapped windows to ğ’™. Besides, Dropout is adopted to prevent over-fitting. The ğ‘™2normalization (ğ’› â† ğ’›/âˆ¥ğ’›âˆ¥) is used to normalize the energy of ğ’› to avoid the dramatical variation of the output magnitude, due to the introduced element-wise multiplication.</p><formula xml:id="formula_8">ğ’› = [ğ‘§ 1 , â€¢ â€¢ â€¢ , ğ‘§ ğ‘œ ] = SumPooling( Å¨ğ‘‡ ğ’‚ â€¢ á¹¼ğ‘‡ ğ’—, ğ‘˜)<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>In this work we use four emotion datasets to train our models, i.e. AffectNet <ref type="bibr" target="#b19">[20]</ref>, RAF-DB <ref type="bibr" target="#b15">[16]</ref>, FER+ <ref type="bibr" target="#b1">[2]</ref>, AFEW <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>The human-annotated part of AffectNet dataset contains 287,651 training images and 4,000 test images, which are annotated with both emotion labels and arousal valence values. Only emotion labels are used in this task.</p><p>The RAF-DB dataset consists of 15,339 images labeled with 7-class basic emotion and 3,954 labeled with 12-class compound emotion. Only images labeled with basic emotion are used in this study.</p><p>The FER+ dataset contains 28,709 training, 3,589 validation and 3,589 test images. We combine its training data with validation data for the training split and evaluate the model performance on the test data.</p><p>The AFEW contains 773 train, 383 val and 653 test samples, which are collected from movies and TV serials with spontaneous expressions, various poses, and illuminations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exploration of Emotion Features</head><p>We explore emotion features in two perspectives, namely CNN backbones and pretraining emotion datasets.</p><p>For the choice of the CNN model, we compare IR50 <ref type="bibr" target="#b3">[4]</ref>, ResNet18 <ref type="bibr" target="#b11">[12]</ref>, and VGGFace <ref type="bibr" target="#b20">[21]</ref> in the Table <ref type="table" target="#tab_3">1</ref>, where the former two models are pretrained on MS-Celeb-1M dataset and the last one on VGGFace dataset. We find that the large CNN, IR50, is superior to the other two models.</p><p>We use the well-trained IR50 model to extract features and only train softmax classifier using these features. The IR50 models pre-trained on FER+, RAF-DB, and AffectNet achieve 50.13%, 51.436%, and 53.78%, respectively. Therefore, we choose the IR50 model pretrain on AffectNet as our visual features in the following fusion experiments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exploration of Fusion Strategies</head><p>We explore three intra-modal attention strategies with the FBP cross-modal fusion. We use speech spectrogram for audio CNN, which obtains 38% on AFEW validation set individally. In the Table <ref type="table" target="#tab_4">2</ref>, we find the FBP improves performance for all the intra-modal fusion methods. Transformer attention for intra-modal fusion is the best for FBP. We also use log Mel-spectrogram for audio CNN, which obtains a little better performance, but the final results are very similar after intra-and cross-modal fusion. Besides, the concatenation of audio and visual vectors gets 58% accuracy in AFEW validation set with transformer attention. This is 3% lower than FBP which shows the effectiveness of FBP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Enhancement</head><p>In the Table <ref type="table" target="#tab_5">3</ref>, the Basic Features means that we only extract one feature vector for each frame. Besides, We apply 5 kinds of feature enhancement strategies as presented in Table <ref type="table" target="#tab_5">3</ref>. Specifically, for feature ğ¹ -ğ‘€ğ‘’ğ‘ğ‘›, we first obtain 18 transformation frames by using three rotations, three scales, and flipping for a frame. After that, we compute the features of these 18 transformation frames and average these 18 features as the feature ğ¹ -ğ‘€ğ‘’ğ‘ğ‘›. For the feature ğ¹ -ğ‘€ğ‘’ğ‘ğ‘›ğ‘†ğ‘¡ğ‘‘, we compute the average feature and feature standard deviation of these 18 features. We then concatenate the average feature and the standard deviation as ğ¹ -ğ‘€ğ‘’ğ‘ğ‘›ğ‘†ğ‘¡ğ‘‘. For the feature ğ¹ -ğ‘›ğ‘œğ‘Ÿğ‘šğ¹ ğ¹ğ‘‡ , we first compute the Fast Fourier transform(FFT) of the Basic Feature, and then normalize the feature and concatenate the real and imaginary parts as ğ¹ -ğ‘›ğ‘œğ‘Ÿğ‘šğ¹ ğ¹ğ‘‡ . For the feature ğ¹ -ğ´ğ‘…-ğ‘€ğ‘’ğ‘ğ‘›, ğ´ means that the features are extracted by the models pre-trained on Affectnet, and ğ‘… by the models pre-trained on RAF-DB. we concatenate these two mean features of two different pretrained models as ğ¹ -ğ´ğ‘…-ğ‘€ğ‘’ğ‘ğ‘›. Table <ref type="table" target="#tab_5">3</ref> shows that the five feature enhancement methods further improve the performance of FBP where the feature F-MeanStd achieves the best result on the validation set. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results On EmotiW2019</head><p>In the Table <ref type="table" target="#tab_6">4</ref>. The first three submitted models are trained on the training and validation set of AFEW, and the last two models are trained on the training set of AFEW. We find that it is difficult to choose models and fuse models if combining the validation set with the training set. We adopt class weight in all submissions, which means that we reweight the predicted scores by the square root of the sample numbers([0.15, 0.097, 0.129, 0.185, 0.138, 0.082, 0.215]).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CONCLUSIONS</head><p>In this paper, we exploit three types of intra-modal fusion methods, namely self-attention, relation-attention, and transformer. They are mainly used to highlight important emotion feature. For the fusion of audio and visual information, we explore feature concatenation and factorized bilinear pooling (FBP). Besides, we evaluate different emotion features, including an audio feature with both speech-spectrogram and Log Mel-spectrogram and several facial features with different CNN models and different emotion pretrained strategies.</p><p>With careful evaluation, we obtain 62.48% and rank second in the EmotiW 2019 Challenge.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The pipeline of audio-video emotion recognition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Our factorized bilinear pooling(FBP) module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>use Statistical encoding module to aggregate frame features which compute the mean, variance, minimum, and</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Feature Extraction Intra-modal Cross-modal Feature Fusion Video Processing Classifier LResNet50E-IR Self- Attention Factorized Bilinear Pooling SoftMax Relation- Attention AlexNet Happy Perceptron- Attention Feature Concatenation or or or or Speech / Log-mel Spectrogram Audio Visual Feature Maps N feature Vector</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>This attention module was designed to learn weights from the relationship between features. After the self-attention, features are aggregated into a single vectorğ‘“ ğ‘  . Since ğ‘“ ğ‘  inherently contains global representation of these features, we use the sample concatenation of individual features and global represenation [ğ‘“ ğ‘– : ğ‘“ ğ‘  ] to model the global-local relation. Similar to the Self-attention module, with individual emotion features, we apply 1-dimensional FC layer W 1 dÃ—1 and a sigmoid function ğœ. The relation-attention weight of the ğ‘–-th feautre [ğ‘“ ğ‘– : ğ‘“ ğ‘  ] ğ‘‡ is formulated as follows:</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Exploration of CNN models and pretrained emotion datasets.</figDesc><table><row><cell>Model</cell><cell>FER+</cell><cell cols="2">RAF-DB AffectNet</cell></row><row><cell>VGGFace</cell><cell>88.84%</cell><cell>86.93%</cell><cell>51.425%</cell></row><row><cell>ResNet18</cell><cell>88.65%</cell><cell>86.696%</cell><cell>52.075%</cell></row><row><cell>IR50</cell><cell cols="3">89.257% 89.075% 53.925%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Evaluation of intra-modal fusion methods.</figDesc><table><row><cell>Audio</cell><cell>Visual</cell><cell>Self</cell><cell cols="2">Relation Transformer</cell></row><row><cell cols="2">Self</cell><cell>54.6%</cell><cell>56.9%</cell><cell>60.3%</cell></row><row><cell cols="2">Relation</cell><cell>54.0%</cell><cell>57.2%</cell><cell>60%</cell></row><row><cell cols="2">Transformer</cell><cell>54.8%</cell><cell>58%</cell><cell>61.1%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Evaluation of five feature enhancement strategies. The default setting is Rotation âˆˆ [âˆ’2Â°, 0Â°, 2Â°], scale âˆˆ [1, 1.03, 1.07]</figDesc><table><row><cell>Visual Feature</cell><cell>Augmentation details</cell><cell>AFEW Val acc</cell></row><row><cell>Basic Feature</cell><cell>--</cell><cell>61.1%</cell></row><row><cell>Basic Feature_RAF-DB</cell><cell>--</cell><cell>58.5%</cell></row><row><cell>F-Mean</cell><cell>default setting</cell><cell>62.14%</cell></row><row><cell>F-MeanStd</cell><cell>default setting</cell><cell>63.7%</cell></row><row><cell>F-MeanStd-2</cell><cell>ğ‘…ğ‘œğ‘¡ğ‘ğ‘¡ğ‘–ğ‘œğ‘› âˆˆ [âˆ’15Â°, 0Â°, 15Â°] ğ‘ ğ‘ğ‘ğ‘™ğ‘’ âˆˆ [0.75, 1, 1.25]</cell><cell>62.4%</cell></row><row><cell>F-NormFFT</cell><cell>Normalized FFT</cell><cell>61.35%</cell></row><row><cell>F-AR-Mean</cell><cell>default setting</cell><cell>62.92%</cell></row><row><cell>FG-Net</cell><cell>--</cell><cell>59%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Submission results of different model combinations.</figDesc><table><row><cell>Sub</cell><cell>Val</cell><cell>Test</cell><cell>Fusion detail</cell></row><row><cell>(1)</cell><cell>--</cell><cell>62.481%</cell><cell>4 FG-Net-1</cell></row><row><cell>(2)</cell><cell>--</cell><cell>59.112%</cell><cell>2 F-MeabStd-2 + 2 F-AR-Mean</cell></row><row><cell>(3)</cell><cell>--</cell><cell>54.518%</cell><cell>4 FG-Net-2</cell></row><row><cell cols="2">(4) 64.5%</cell><cell>61.41%</cell><cell>4 F-MeanStd</cell></row><row><cell cols="3">(5) 65.5% 62.328%</cell><cell>F-Mean + F-MeanStd + F-NormFFT + F-MeanStd-2 + F-AR-Meam</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">http://dlib.net/ for AffectNet dataset, due to the face bounding box had been provided. For AFEW dataset, If no face is detected in the picture, the entire frame is passed to the network.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">ACKNOWLEDGMENTS</head><p>This work is partially supported by the National Natural Science Foundation of China (U1613211), Shenzhen Basic Research Program (JCYJ20170818164704758), the Joint Lab of CAS-HK.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Emotion recognition in the wild from videos using images</title>
		<author>
			<persName><forename type="first">Adel</forename><surname>Sarah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emad</forename><surname>Bargal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristian</forename><forename type="middle">Canton</forename><surname>Barsoum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cha</forename><surname>Ferrer</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM ICMI</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Training Deep Networks for Facial Expression Recognition with Crowd-Sourced Label Distribution</title>
		<author>
			<persName><forename type="first">Emad</forename><surname>Barsoum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cha</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristian</forename><forename type="middle">Canton</forename><surname>Ferrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyou</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM ICMI</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">3-D convolutional recurrent neural networks with attention model for speech emotion recognition</title>
		<author>
			<persName><forename type="first">Mingyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanji</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1440" to="1444" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Arcface: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07698</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">EmotiW 2019: Automatic Emotion, Engagement and Cohesion Predic-tionTasks</title>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><surname>Goecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shreya</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Gedeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Mutimodal Interaction</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Collecting large, richly annotated facial-expression databases from movies</title>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><surname>Goecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Gedeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE multimedia</title>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Human-computer interaction</title>
		<author>
			<persName><forename type="first">Alan</forename><surname>Dix</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Encyclopedia of database systems</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1327" to="1331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Video-based Emotion Recognition Using Deeply-Supervised Neural Networks</title>
		<author>
			<persName><forename type="first">Yingruo</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacqueline</forename><forename type="middle">Ck</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM ICMI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Video-based emotion recognition using CNN-RNN and C3D hybrid networks</title>
		<author>
			<persName><forename type="first">Yin</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangju</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanliu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM ICMI</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Learning to forget: Continual prediction with LSTM</title>
		<author>
			<persName><forename type="first">JÃ¼rgen</forename><surname>Felix A Gers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fred</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName><surname>Cummins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Framewise phoneme classification with bidirectional LSTM and other neural network architectures</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">JÃ¼rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="602" to="610" />
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning supervised scoring ensemble for emotion recognition in the wild</title>
		<author>
			<persName><forename type="first">Ping</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongqi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shandong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anbang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yurong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM ICMI</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mental models in cognitive science</title>
		<author>
			<persName><forename type="first">Philip</forename><surname>Nicholas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johnson-Laird</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="71" to="115" />
			<date type="published" when="1980">1980. 1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Leveraging large face recognition data for emotion classification</title>
		<author>
			<persName><forename type="first">Boris</forename><surname>Knyazev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Shvetsov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Efremova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Artem</forename><surname>Kuharenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Face &amp; Gesture Recognition (FG 2018)</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="692" to="696" />
		</imprint>
	</monogr>
	<note>13th IEEE</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Reliable Crowdsourcing and Deep Locality-Preserving Learning for Unconstrained Facial Expression Recognition</title>
		<author>
			<persName><forename type="first">Shan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-Feature Based Emotion Recognition for Video Clips</title>
		<author>
			<persName><forename type="first">Chuanhe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianhao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kui</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minghao</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 on International Conference on Multimodal Interaction</title>
				<meeting>the 2018 on International Conference on Multimodal Interaction</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="630" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multiple Spatio-temporal Feature Learning for Video-based Emotion Recognition in the Wild</title>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenming</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaolong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuangao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 on International Conference on Multimodal Interaction</title>
				<meeting>the 2018 on International Conference on Multimodal Interaction</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="646" to="652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Clinical diagnosis of depression in primary care: a meta-analysis</title>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amol</forename><surname>Vaze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Lancet</title>
		<imprint>
			<biblScope unit="volume">374</biblScope>
			<biblScope unit="page" from="609" to="619" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">AffectNet: A Database for Facial Expression, Valence, and Arousal Computing in the Wild</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Mollahosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Behzad</forename><surname>Hasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing PP</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="1949">1949. 1949</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Omkar M Parkhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Understanding and improving convolutional neural networks via concatenated rectified linear units</title>
		<author>
			<persName><forename type="first">Wenling</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Group emotion recognition with individual facial emotion CNNs and global image based CNNs</title>
		<author>
			<persName><forename type="first">Lianzhi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoxing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM International Conference on Multimodal Interaction</title>
				<meeting>the 19th ACM International Conference on Multimodal Interaction</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="549" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Attention Is All You Need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Temporal multimodal fusion for video emotion classification in the wild</title>
		<author>
			<persName><forename type="first">Valentin</forename><surname>Vielzeuf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">StÃ©phane</forename><surname>Pateux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">FrÃ©dÃ©ric</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM ICMI</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cascade Attention Networks For Group Emotion Recognition with Face, Body and Image Cues</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">,</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoxing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM International Conference on Multimodal Interaction</title>
				<meeting>the 19th ACM International Conference on Multimodal Interaction</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Region Attention Networks for Pose and Occlusion Robust Facial Expression Recognition</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.04075</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="499" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep Recurrent Multi-instance Learning with Spatio-temporal Features for Engagement Intensity Prediction</title>
		<author>
			<persName><forename type="first">Jianfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 on International Conference on Multimodal Interaction</title>
				<meeting>the 2018 on International Conference on Multimodal Interaction</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="594" to="598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter</title>
				<meeting>the 2016 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">HoloNet: towards robust emotion recognition in the wild</title>
		<author>
			<persName><forename type="first">Anbang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongqi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shandong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yurong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM ICMI</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Deep Fusion: An Attention Guided Factorized Bilinear Pooling for Audio-video Emotion Recognition</title>
		<author>
			<persName><forename type="first">Yuanyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi-Rui</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.04889</idno>
		<imprint>
			<date type="published" when="2019-06">Jun Du. 2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
