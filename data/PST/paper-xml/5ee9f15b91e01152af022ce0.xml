<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Automatic Validation of Textual Attribute Values in E-commerce Catalog by Learning with Limited Labeled Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-06-23">23 Jun 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yaqing</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">State University of New York at Buffalo</orgName>
								<address>
									<settlement>Buffalo</settlement>
									<region>New York</region>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">State University of New York at Buffalo</orgName>
								<address>
									<settlement>Buffalo</settlement>
									<region>New York</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yifan</forename><forename type="middle">Ethan</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">State University of New York at Buffalo</orgName>
								<address>
									<settlement>Buffalo</settlement>
									<region>New York</region>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">State University of New York at Buffalo</orgName>
								<address>
									<settlement>Buffalo</settlement>
									<region>New York</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">State University of New York at Buffalo</orgName>
								<address>
									<settlement>Buffalo</settlement>
									<region>New York</region>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">State University of New York at Buffalo</orgName>
								<address>
									<settlement>Buffalo</settlement>
									<region>New York</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Xin</forename><forename type="middle">Luna</forename><surname>Dong</surname></persName>
							<email>lunadong@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="institution">State University of New York at Buffalo</orgName>
								<address>
									<settlement>Buffalo</settlement>
									<region>New York</region>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">State University of New York at Buffalo</orgName>
								<address>
									<settlement>Buffalo</settlement>
									<region>New York</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jing</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">State University of New York at Buffalo</orgName>
								<address>
									<settlement>Buffalo</settlement>
									<region>New York</region>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">State University of New York at Buffalo</orgName>
								<address>
									<settlement>Buffalo</settlement>
									<region>New York</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Automatic Validation of Textual Attribute Values in E-commerce Catalog by Learning with Limited Labeled Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-06-23">23 Jun 2020</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3394486.3403303</idno>
					<idno type="arXiv">arXiv:2006.08779v3[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Product catalogs are valuable resources for eCommerce website. In the catalog, a product is associated with multiple attributes whose values are short texts, such as product name, brand, functionality and flavor. Usually individual retailers self-report these key values, and thus the catalog information unavoidably contains noisy facts. It is very important to validate the correctness of these values in order to improve shopper experiences and enable more effective product recommendation. Due to the huge volume of products, an effective automatic validation approach is needed. In this paper, we propose to develop an automatic validation approach that verifies the correctness of textual attribute values for products. This can be formulated as a task as cross-checking a textual attribute value against product profile, which is a short textual description of the product on eCommerce website. Although existing deep neural network models have shown success in conducting cross-checking between two pieces of texts, their success has to be dependent upon a large set of quality labeled data, which are hard to obtain in this validation task: products span a variety of categories. Due to the category difference, annotation has to be done on all the categories, which is impossible to achieve in real practice.</p><p>To address the aforementioned challenges, we propose a novel meta-learning latent variable approach, called MetaBridge, which can learn transferable knowledge from a subset of categories with limited labeled data and capture the uncertainty of never-seen categories with unlabeled data. More specifically, we make the following contributions. (1) We formalize the problem of validating the textual attribute values of products from a variety of categories as a natural language inference task in the few-shot learning setting, and propose a meta-learning latent variable model to jointly process the signals obtained from product profiles and textual attribute values. (2) We propose to integrate meta learning and latent variable in a unified model to effectively capture the uncertainty of various categories. With this model, annotation costs can be significantly reduced as we make best use of labeled data from limited categories. (3) We * Most of the work was conducted when the author was interning at Amazon.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Product catalogs are valuable resources for eCommerce website for the organization, standardization and publishing of product information. Because the majority of product catalogs on eCommerce websites (e.g., Amazon, Ebay, and Walmart) are contributed by individual retailers, the catalog information unavoidably contains noisy facts <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b30">31]</ref>. The existence of such errors results in misleading information delivered to consumers and significantly downgrades the performance of downstream applications, such as product recommendation. As the magnitude of product catalogs does not allow for manual validation, there is an urgent need for the development of automatic yet effective validation algorithms.</p><p>In a product catalog, a product is typically associated with multiple textual attributes, such as name, brand, functionality and flavor, whose values are short texts. Therefore, in this paper, we focus on the important task of validating the correctness of a textual attribute value given a product. A real example is "Ben &amp; Jerry's -Vermont's Finest Ice Cream, Non-GMO -Fairtrade -Cage-Free Eggs -Caring Dairy -Responsibly Sourced Packaging, Americone Dream, Pint (8 Count)", which is the product title of an icecream on Amazon. The attribute "flavor" is a textual attribute, and for this particular icecream, "Americone Dream" is its flavor attribute value. The objective is to automatically output whether this value is correct or not for this product.</p><p>One may consider to model this task as anomaly detection based on the values of the target textual attribute, so that anomalies correspond to wrong values. However, this solution is not applicable to the validation task because: 1) As individual retailers self-report these attribute values, the set of possible values cannot be predetermined, and thus traditional anomaly detection approaches cannot work. 2) Textual anomaly detection has been studied and many methods have been proposed to identify anomalies by extracting distinguishing features from the texts. However, in the validation task, the correctness of a value is highly dependent on the product. For example, "Americone dream" may not be a common piece of textual value, but it is a correct flavor name for Ben&amp;Jerry icecream.</p><p>Motivated by this observation, we propose to verify the correctness of textual attribute value against the text description of the corresponding product. A detailed description of a product can be found from the product webpage, which contains rich information about many attributes of the product. For example, in our example, the title itself already covers the values of several attributes, such as flavor and ingredients. By cross-checking the textual attribute value "Americone-dream" for flavor against this description, we can easily verify that this value is correct. However, this cross-checking cannot be completed by a simple matching of the keywords. We found that a certain amount of errors are because the retailers often abuse the attribute by filling a real value of another attribute. Such errors cannot be detected by simply matching the value with product description text as they indeed can be found there. For example, for value "Non-GMO", it is a wrong value as of flavor, but could be labeled as correct by a simple matching against the product title of this icecream.</p><p>Therefore, we propose to model the validation problem as the task of automatic correctness inference based on an input of a textual attribute value and the description of the corresponding product. This setting is related to the natural language inference (NLI) task, which automatically determines if a hypothesis is true or false based on a text statement. Recently, powerful neural network based models, such as Transformer <ref type="bibr" target="#b23">[24]</ref> and BERT <ref type="bibr" target="#b10">[11]</ref> have shown promising performance towards NLI task. However, their success relies on sufficient high-quality labeled data, which requires the annotation of correctness on a large number of hypothesis-statement pairs. This requirement cannot be satisfied in the textual attribute validation task. There are thousands to millions product categories on eCommerce website, and thus annotating sufficient labeled data for all the categories is impossible. If only limited categories are annotated, such labeled data cannot be applied to other categories. For products in different categories, the product attributes and the vocabularies of the attributes could vary significantly. For example, even for the same attribute "flavor", there is no overlapping values when describing the flavor of seasoning, ice cream and coffee.</p><p>To tackle the aforementioned challenges, we propose a novel meta-learning latent variable approach, namely MetaBridge, for textual attribute validation. The proposed approach effectively leverages a small set of labeled data in limited categories for training category-agnostic models, and utilizes unlabeled data to capture category-specific information. More specifically, the proposed objective function is directly derived from the textual attribute validation task based evidence lower bound, and it seamlessly integrates metalearning principle and latent variable modeling. We then propose to solve this problem via a stochastic neural network which has the sampling and parameter adaptation steps. The benefits of the proposed approach include the following. First, the parameter adaptation step allows more parameter flexibility to capture category-specific information. Second, we enforce the distribution consistences between unlabeled and labeled data via KL Divergence, which makes best use of limited labeled information while extracts most useful information from unlabeled data. Third, the proposed model is a stochastic neural network where sampling step is beneficial to the prevention of overfitting. The insights behind our objective function are explored in our experiments. Experimental results on two large real-world datasets show that proposed model can effectively generalize to new product categories and outperforms the state-of-the-art approaches.</p><p>The main contributions of this paper can be summarized as follows:</p><p>• We formally define the important problem of textual attribute validation on eCommerce website as an automatic correctness inference task based on a model taking an input pair of attribute-value and corresponding product description. We propose an effective meta-learning latent variable model which can make category-specific decision even though labeled data are only collected from limited categories. • The proposed MetaBridge method combines meta learning and latent variable in a joint model to make best use of limited labeled data and vast amounts of unlabeled data. The proposed solution enhances the ability of capturing category uncertainty and preventing overfitting via effective sampling. • We empirically show that the proposed method MetaBridge can effectively infer the correctness of attribute values and significantly outperform the state-of-the-art models on two real-world datasets collected from Amazon.com.</p><p>The rest of the paper is organized as follows: problem setting and preliminaries are introduced in Section 2, and the details of the proposed framework are presented in Section 3. Experimental results are presented in Section 4. Related literature survey is summarized in Section 5, and the study is concluded in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PROBLEM SETTING AND PRELIMINARY</head><p>In this section, we first introduce our problem and the few-shot learning setting, then we present the representative algorithm of meta-learning, its limitations and our intuitions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem Setting</head><p>Given a set of product profiles presented as unstructured text data like titles and their corresponding textual attribute values, our objective is to identify incorrect attribute values based on corresponding product profiles. Note that we have open world assumption thus we cannot construct a golden list to filter out never-seen attribute values. As the the categories of product are from thousands to millions and annotation job requires corresponding knowledge, we can only obtain a small set of annotated data about a subset of product categories. But for each category, unlabeled data are easily collected. We next formally define the problem we are solving. DEFINITION. Given a set of product categories C and corresponding products I = {I c : c ∈ C}, product profiles P = {p i : i ∈ I }, attribute values as V = {v i : i ∈ I }, we aim to identify X = (P, V ) pair that are incorrect for product I .</p><p>After defining our problem, we introduce our learning setting. Following the few-shot learning setting <ref type="bibr" target="#b25">[26]</ref>, in each category c ∼ C, we have a few unlabeled examples x s c = {x s c,i } N i=1 to constitute the support set D s c and have a small set of labeled examples {x q c , y q c } = {x q c,i , y q c,i } N +K i=N +1 as the query set D q c . We need to learn from a subset of categories a well-generalized model which can facilitate training in a new category based on unlabeled support set D s c and infer the correctness of attribute values for corresponding products I c in the same category c.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">MAML</head><p>We give an overview of Model-Agnostic Meta-Learning method <ref type="bibr" target="#b11">[12]</ref> which is a representative algorithm of optimization-based metalearning approaches. First, we use our problem as an example to introduce the general learning setting of meta-learning methods. The learning of meta-learning are split into two stages: meta-training and meta-testing. During the meta-training stage, the baseline learner f θ with parameter set θ will be adapted to specific category c as f θ c with the help of meta-learner M(•) on support set D s c , i.e., θ c = M(θ, D s c ). Such category specific learner f θ c is evaluated on the corresponding query set D q c . During the meta-testing stage, the baseline learner f θ will be adapted to testing category c on D s c using the same procedure with meta-training stage, i.e., θ c = M(θ, D s c ), and make predictions for the D q c . In the MAML, it updates parameter vector θ using one or more gradient descent updates on the category c. For example, when using one gradient update:</p><formula xml:id="formula_0">θ c = M(f θ , D s c ) = θ − β ▽ θ L(f θ , D s c</formula><p>), where β is inner step size and D s c is a support set for given category c. The model parameters are trained by optimizing for the performance of f θ c with respect to θ across categories. More concretely, the meta-objective is as follows:</p><formula xml:id="formula_1">min θ L(f θ ) = c ∈C L(f θ −β ▽ θ L(f θ , D s c ) , D q c ),</formula><p>where D q c is a query set for given category c. Limitations: MAML captures category uncertainty with the help of a few labeled data. Such mechanism brings expensive and continuous annotation costs. Although we can change the supervised loss on support set to unsupervised loss like entropy minimization, the adaptation on unlabeled data will undoubtedly increase the difficulty of capturing category uncertainty and further degrade the performance. Moreover, meta-learning methods suffer from overfitting problem especially when only a small set of labeled data is available.</p><p>Key ideas of our solution: To avoid continuous annotation cost, we expect our model to capture the category-uncertainty via unlabeled data. Thus, how we take advantage of unlabeled data to benefit our method is a key problem. A simple intuition is that we need to bridge unlabeled data and labeled data together to stabilize adaptation step. To achieve such goal, we propose a new approach which can integrate latent variable model with meta-learning framework. The latent variable model can capture the category distribution via a latent variable which can construct a connection between unlabeled and labeled data and prevents overfitting with the inherent sampling procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>In this section, we first introduce how we derive our meta-learning latent variable objective function, then we present our model architecture and the algorithm flow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>As shown in Figure <ref type="figure" target="#fig_0">1</ref>, the proposed MetaBridge mainly includes two stages: adaptation and validation. During the adaptation stage, the model parameter is updated on unlabeled support data from given product category; during the validation stage, the category-specific model is used to make textual validation for products from same product category. To capture uncertainty on unlabeled data and prevent overfitting, we propose a meta learning latent variable objective function which includes two terms: inference loss and bridging regularizer. By jointly minimizing both objectives, we enforce the model i) to learn direct signal from labeled data, and ii) internally harmonizes the latent structures of the new category and existing category from unlabeled data. More specifically, the proposed approach is a stochastic neural network which includes sampling and parameter adaptation steps. Furthermore, the proposed model can enforce the distribution consistency between unlabeled and labeled data via KL Divergence. Thus, we are able to train a complicated meta learning Transformer-based model which can jointly processes signals from textual product description and attribute values to conduct effective inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Latent Variable Model</head><p>The goal of the proposed algorithm is to learn to infer on various categories even unseen category with a handful unlabeled training instances. More specifically, for the c-th category, the corresponding support set x s c is given, we aim to infer y q c based on x q c . Here We denote x c = {x s c , x q c }, y c = {y q c } for simplicity and hence our objective function can be represented as follows:</p><formula xml:id="formula_2">log p Θ (y|x) = c ∈C log p Θ (y c |x c ),<label>(1)</label></formula><p>where Θ represents the parameter set of the proposed model. For each category c, we only have a very limited number of labeled data points. To capture category uncertainty, we include a latent variable z that captures category distribution. This latent variable is of particular interest because it can capture the category uncertainty and allows us to sample data for the learned category to prevent overfitting.</p><p>To be clear, we take c-th category as an example. Let p(z, y c |x c ) be a joint distribution over a set of latent variables z ∼ Z and observed variables y c ∈ Y and x c ∈ X for category c. An inference query involves computing posterior beliefs after incorporating evidence into the prior: p(z|y c , x c ) = p(z, y c |x c )/p(y c |x c ). This quantity is often intractable to compute as the marginal likelihood p(y c |x c ) = ∫ z p(z, y c |x c )dz requires integrating or summing over a potentially exponential number of configurations for z. As with variational autoencoders <ref type="bibr" target="#b18">[19]</ref>, we approximate the objective function using the evidence lower bound (ELBO) on the log likelihood. For the purpose of calculating ELBO, let us introduce an encoder model q ϕ (z|x c , y c ): an approximation to the intractable true posterior p(z|x c , y c ) with a parameter set ϕ. In a similar vein, we use a decoder model p θ (y c |x c , z) to approximate the intractable true posterior p(y c |x c , z) with a parameter set θ . Thus, the parameter set Θ includes {ϕ, θ }. After introducing the encoder and decoder, we present how to derive our objective function based on ELBO.  Evidence Lower Bound (ELBO) The ELBO can be shown to decompose into</p><formula xml:id="formula_3">log p Θ (y c |x c ) ≥E q ϕ (z |x c ,y c ) [log p θ (y c |z, x c )] − D K L (q ϕ (z|x c , y c ) || p(z)).<label>(2)</label></formula><p>To better reflect the desired model behavior at test time, i.e., we have a handful training instances as a support set x s c for each category, we explicitly split x c into support and query sets. Our goal is to model the conditional of the query set given the support set. Thus, instead of using prior p(z) in Eq. 2, we propose to use a more informative conditional prior distribution p(z|x s c ) as with <ref type="bibr" target="#b12">[13]</ref> and further rewrite our objective function as follows:</p><formula xml:id="formula_4">log p Θ (y c |x c ) = log p Θ (y q c |x s c , x q c ) ≥E q ϕ (z |x s c ,x q c ,y q c ) [log p θ (y q c |z, x s c , x q c )] − D K L (q ϕ (z|x q c , x s c , y q c ) || p(z|x s c ))<label>(3)</label></formula><p>For the encoder q ϕ (z|x s c , x q c , y q c ), since x q c is given and y q c is implicitly encoded into parameter set ϕ, we assume z is conditional independent with y q c given x q c and ϕ. Thus, our objective function can be simplified as follows:</p><formula xml:id="formula_5">log p Θ (y q c |x s c , x q c ) ≥E q ϕ (z |x s c ,x q c ) [log p θ (y q c |z, x s c , x q c )] − D K L (q ϕ (z|x s c , x q c ) || p(z|x s c ))<label>(4)</label></formula><p>The support set x s c is used to help the proposed model to quickly adapt to new category. Thus, how we take advantage of this set to benefit our framework is a key problem. To tackle this problem, we propose to encode the information from support set into our parameter inspired by MAML <ref type="bibr" target="#b11">[12]</ref> and further we can obtain a category-specific model to accelerate unseen category adaptation. We will introduce how to incorporate information from support set into our framework via parameter adaptation in the next subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Parameter Adaptation</head><p>As introduced in the subsection 2.2, MAML obtains a category specific parameter set using one or more gradient descent updates based on loss from support set x s c . Considering the support set in our problem is unlabeled, we redefine the loss function on unlabeled support set by entropy minimization. Entropy minimization encourages the confidence of predictions and is commonly used in the semisupervised learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20]</ref> and domain adaptation <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b26">27]</ref>. More concretely, the loss function L c s on the support set x s c is defined by entropy as follows:</p><formula xml:id="formula_6">L c s (θ, ϕ, x s c ) = −E q ϕ (z |x s c ) [p θ (z) log p θ (z)]<label>(5)</label></formula><p>and the parameter adaptation step via one step of gradient descent is defined accordingly as follows:</p><formula xml:id="formula_7">{θ c , ϕ c } = {θ, ϕ} − β ▽ θ,ϕ L c s (θ, ϕ, x s c ).<label>(6)</label></formula><p>Here we assume the information of support set is encoded into parameter via gradient descent and then exclude the x s c from conditionals. Moreover, for the decoder p θ (y q c |z, x s c , x q c ), y q c is conditional independent with x q c given z since z is the feature representation of x q c . Thus, we can have simpler equations as follows:</p><formula xml:id="formula_8">Encoder: q ϕ (z|x s c , x q c ) → q ϕ c (z|x q c )<label>(7)</label></formula><p>Decoder: p θ (y</p><formula xml:id="formula_9">q c |z, x s c , x q c ) → p θ c (y q c |z)<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Objective Function</head><p>To optimize our objective function, we still need to approximate conditional prior p θ (z|x s c ) which is intractable. As the parameter adaptation step can encode support set into the model and captures category specific information, hence we propose to use q ϕ c (z|x s c ) as a approximation to p(z|x s c ) and then we have our final objective function as follows:</p><formula xml:id="formula_10">log p Θ (y q c |x s c , x q c ) ≥E q ϕc (z |x q c ) [log p θ c (y q c |z)] − D K L (q ϕ c (z|x q c ) || p(z|x s c )) ≃E q ϕc (z |x q c ) [log p θ c (y q c |z)] − D K L (q ϕ c (z|x q c ) || q ϕ c (z|x s c ))<label>(9)</label></formula><p>The objective function includes two terms: the first term is our supervised inference loss on query samples and the second term is to enforce conditional category distribution q ϕ c (z|x q c ) consistent with conditional distribution q ϕ c (z|x s c ), i.e., distributions of unlabeled and labeled data from same category. The second term can be treated as a explicit bridge between support set and query set. λ is a hyperparameter that needs to be set. We explore the impact of λ in the experiment section 4.6.</p><formula xml:id="formula_11">L c q = −E q ϕc (z |x q c ) [log p θc (y q c |z)] Inference Loss +λ D K L (q ϕc (z |x q c ) | | q ϕc (z |x s c ))</formula><p>Bridging Regularizer <ref type="bibr" target="#b9">(10)</ref> In this paper, we assume q ϕ c (z|x q c ) and q ϕ c (z|x s c ) follow multivariate normal distributions N (µ(x q c ), σ 2 (x q c )I) and N (µ(x s c ), σ 2 (x s c )I) respectively. The KL Divergence D K L (q ϕ c (z|x q c ) || q ϕ (z|x s c )) in Eq. 10 can be analytically integrated:</p><formula xml:id="formula_12">D K L (q ϕc (z |x q c ) | | q ϕc (z |x s c )) = d j =1 log σ j (x s c ) σ j (x q c ) + σ 2 j (x q c ) + (µ j (x q c ) − µ j (x s c )) 2 2σ 2 j (x s c ) − 1 2 , (<label>11</label></formula><formula xml:id="formula_13">)</formula><p>where d is the dimension of z. Thus, we only need to calculate category loss term. To enable distribution q ϕ c (z|x q c ) differentiable, we follow previous work <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b18">19]</ref> to use reparameterization trick to parameterize z. Reparameterization Trick Instead of directly sampling from a complex distribution, we can reparametrize the random variable as a deterministic transformation of an auxiliary noise variable ϵ. In our case, to sample from q ϕ c (z|x q c ), since q ϕ c (z|x q c ) = N (µ(x q c ), σ 2 (x q c )I), one can draw samples by computing z = µ(x q c ) + σ (x q c ) ⊙ ϵ, where ϵ ∼ N (0, I) and ⊙ signify an element-wise product. By passing in auxiliary noise, our proposed model is stochastic and if we do not pass in any auxiliary noise, then the model is deterministic.</p><p>After introducing our final objective function, we will present the detailed architecture and algorithm flow in the next subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Model Architecture</head><p>Our model mainly includes two components: encoder and decoder.</p><p>Encoder The encoder in use is Transformer <ref type="bibr" target="#b23">[24]</ref>, which is a context-aware model and has been proven powerful in textual classification. The transformer takes a sequence of word tokens as input. In our problem, the input includes two parts: unstructured product profiles and the corresponding product textual attribute values. As the length of two parts are usually very different, we use two Transformers to take two parts separately to obtain fixed-dimensional features. Following <ref type="bibr" target="#b10">[11]</ref>, the first token of every sequence is always a special classification token ([CLS]). Accordingly, the final hidden state corresponding to this token is used as the aggregate sequence representation. We concatenate the two final hidden states from Transformers and then feed them into two fully connected layers with weight matrix W 2d ×d µ and W 2d ×d σ to output mean µ and log(σ ) as suggested in <ref type="bibr" target="#b18">[19]</ref>.</p><p>Decoder The decoder is a fully connected layer with weight matrix W d ×2 o to take samples from inferred normal distribution and output the probability of given attribute values being incorrect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Training and inference procedures</head><p>The training procedure is summarized in Algorithm 1. We first sample a batch of categories and get corresponding support set and query set for each category. Given the support set , we first update the parameter of encoder and decoder to get category-specific parameter set θ c , ϕ c according to Eq. 5 and Eq. 6. The category-specific encoder takes query set x q c and support set x s c to output the parameters for the distribution p(z|x q c ) and p(z|x s c ) respectively. Then we can calculate the Bridging Regularizer in the Eq. 10. We then sample z ′ s from the posterior p(z|x q t,i ) and the category-specific decoder takes z ′ s as input to infer the correctness of attribute values. Thus, our model is stochastic during the training stage. During the testing stage, the inference procedure is similar with it in the training procedure, the only difference is that for any data query data x q c,i , its inferred latent code is set to be the conditional mean µ(x q c,i ) = E q ϕ (z |x q c,i ) [z] and the category-specific decoder takes u(x q c,i ) as input. In other words, we use the deterministic model in the testing stage to obtain stable inference results without sampling step. 5: Compute loss L c s according to Eq. 5 6:</p><p>Parameter fast adaptation with gradient descent:</p><formula xml:id="formula_14">7: {θ c , ϕ c } = {θ, ϕ } − β ▽ θ , ϕ L c s (θ, ϕ, x s c ). 8:</formula><p>end for 9:</p><p>Update {θ, ϕ } = {θ, ϕ } − α c ∈C ▽ {θ , ϕ } L c q 10: end for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we introduce the dataset used in the experiments, present the compared state-of-the-art baseline models, validate the effectiveness and explore insights of the proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>To fairly evaluate the performance of the proposed approach, we use two internal Amazon datasets on attributes Flavor and Ingredient respectively. The products in the two datatset are from thousands of product categories across different domains. When preprocessing the datasets, we first exclude the products which do not have the attribute of interest. Then we randomly select 100 products as support set and randomly select 10 products from the rest as query set in each category. We send query set to ask Amazon Mturkers to identify the correctness of attribute values based on corresponding product profiles. Each data point is annotated by 3 Amazon Mturkers and the final label is decided by majority voting. To evaluate the performance of attribute validation models for never-seen product categories, we split the datasets into the training, validation, testing sets according to their product categories. Thus, we ensure that they do not contain any common product category. To evaluate the performance of models under a small data setting, we only use a small portion of product categories for training purpose and the number of product category in training, validation and testing are in a 3:1:6 ratio. The detailed statistics are shown in Table <ref type="table" target="#tab_1">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Setup</head><p>Metric. We use Precision-Recall AUC (PR AUC) and Recall@Precision (R@P) to evaluate the performance of the models. PR AUC is defined as the area under the precision-recall curve. Such a metric is a useful measurement of prediction when the classes are imbalanced. R@P is defined as the recall value at a given precision. Such a measure is widely used to evaluate the model performance when a specific precision requirement need to be satisfied. Baselines. To validate the effectiveness of the proposed model, we choose baselines from the following three categories: supervised learning, fine-tune and meta-learning settings.</p><p>•Supervised Learning We use Logistic Regression (LR), Support Vector Machine (SVM) and Random Forest (RF) as baselines. The supervised learning models are only trained with labeled query data and are not updated when testing. The feature vectors are formed by concatenation of counting the frequencies of specific attribute value in the product textual description, the position of first appearance of attribute value in the description and the average of attribute value word embeddings.</p><p>•Fine-tune Attribute validation is related to natural language inference (NLI) problem. We select three state-of-the-art models ESIM <ref type="bibr" target="#b9">[10]</ref>, Transformer <ref type="bibr" target="#b23">[24]</ref>, BERT <ref type="bibr" target="#b10">[11]</ref> as baselines. All sublayers of ESIM produce the output with dimension d = 16 except the last output layer. For the BERT model, we use the output from BERTbase's last second layer and feed the output into a fully connected layer with weight matrix W 768×16 with ReLU activation function. The Transformer architecture is described in detail in subsection 4.3. Then the output goes through a fully connected layer to output inference results. In the fine-tune setting, the training data include unlabeled support data and labeled query data. We use the entropy minimization to define the loss on unlabeled data as <ref type="bibr" target="#b14">[15]</ref> and use the cross-entropy to define the loss on labeled data. The ratio of labeled loss and unlabeled loss is set as 10:1. In the testing stage, the pre-trained model is first fine-funed on the unlabeled support data of given task with entropy minimization, and then conduct inference on testing query data.</p><p>•Meta-Learning We select two state-of-the-art meta learning models MAML <ref type="bibr" target="#b11">[12]</ref> and Meta-SGD <ref type="bibr" target="#b20">[21]</ref> as baselines. The model architectures of two baselines are identical with Transformers in fine-tune setting. The meta learning setting is that we use entropy minimization loss on unlabeled support data to adapt the parameter of models to given tasks, the task-specific parameters will be evaluated on the query data from same task during training stage. In the testing stage, the baselines is first fine-funed on the unlabeled support data with fixed steps of gradient updates and then conduct inference on the testing query data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation Details</head><p>The 300 dimensional FastText pre-trained word-embedding weights <ref type="bibr" target="#b4">[5]</ref> are used to initialize the parameters of the word embedding layer for deep learning models except for BERT. The encoder of our model is based on Transformer. For our encoder design, we first remove the decoder of original Transformer and only keep Transformer's encoder part. Then we change 6 identical layers of original Transformer's encoder to one instead. The multi-head number is set to 2. All sub-layers of our encoder produce outputs of dimension d = 16 and the dropout rate is selected as 0.3 based on validation set. The identical architecture with our proposed model is employed for the baselines Transformer, MAML and Meta-SGD. The main difference between baselines and our model in architecture is that out model have sampling step and baselines are deterministic. We implement all the deep learning baselines and the proposed approach with Py-Torch 1.2. For training models, we use Adam <ref type="bibr" target="#b17">[18]</ref> optimizer in the default setting. The learning rate α is 0.0001. We use mini-batch size of 64 and training epochs of 400. The parameter gradient update step is set to 1 and inner learning rate β is set to 0.3 for all fine-tune and meta-learning models. The traditional models (LR, SVM, RF) are implemented by scikit-learn package <ref type="bibr" target="#b22">[23]</ref>. The best parameters are selected based on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Performance Comparison</head><p>Table <ref type="table" target="#tab_2">2</ref> shows the performance of different approaches on the Flavor and Ingredient datasets. We use 100 unlabeled data as support set and 5 labeled data as query set per product category. We can observe that that the proposed framework achieves the best results in terms of all the evaluation metrics on both datasets.</p><p>On the Flavor dataset, the LR, SVM and RF achieves the similar performance compared with RNN. The results show that the traditional models can achieve comparable performance with deep learning models when a small set of labeled data is given. Among the fine-tune models, we can observe that BERT achieves the better performance compared with RNN, ESIM and Transformer. The main difference between BERT and other baselines lies in the embedding. The improvement suggests the pre-trained embedding of BERT is informative. The RNN, ESIM and Transformer use the same pre-trained fasttext word embedding layer. The comparison between the three baselines indicate that Transformer architecture can take advantage of training data effectively compared with other two baselines. For the meta-learning setting, we can observe that MAML achieves more than 2% improvement in terms of PR AUC compared with Transformer with identical structure. The reason is that MAML can achieve a base parameter which can easily adapt to new task compared with semi-supervised loss learning. Besides a good base parameter, Meta-SGD also learns update directions and learning rates during training procedure. Thus, Meta-SGD achieves better performance compared with vanilla MAML. It is worth noting that the Meta-SGD achieves comparable performance with the best baseline BERT but uses much less parameters. The proposed approach MetaBridge achieves 3.66% improvement over Meta-SGD and 3.33% compared with BERT respectively in terms of PR AUC. The improvement can also be observed from recall at given precision. Since R@P=0.8 is similar with annotators' precision, we also compare the approaches in terms of this metric. The proposed framework achieves more than 10% improvement compared with best baseline BERT in terms of R@P=0.8.</p><p>On the Ingredient dataset, the RF achieves better performance compared with deep learning models RNN and ESIM. This further reveals the challenges of deep learning model in the small data </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Setting</head><p>Method Flavor Ingredient PR AUC R@P=0.7 R@P=0.8 R@P=0.9 R@P=0.95 PR AUC R@P=0.7 R@P=0.8 R@P=0.9 R@P=0.95 Supervised Learning LR 0.6830 ± 0.0000 48.67 ± 0.00 23.24 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.4520 ± 0.0000 18.71 ± 0.00 14.08 ± 0.00 11.67 ± 0.00 11.47 ± 0.00 SVM 0.6408 ± 0.0000 42.37 ± 0.00 13.56 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.3863 ± 0.0000 19.72 ± 0.00 3. learning setting. Among fine-tuned models, similar results can be observed as those in the Flavor dataset. BERT achieves the best performance compared with other fine-tuned models. This result confirms the effectiveness of pre-trained procedure in the small data learning setting. However, a contradict result with Flavor dataset can be observed from comparison between BERT and Meta-learning models. The MAML and Meta-SGD achieves the comparable and even better performance with BERT. The reason is that the vocabularies of ingredients are rarely used in other contexts hence the information is difficult to be captured without training on the given task dataset. This improvement shows the potentials of meta-learning models for the downstream tasks, which needs models to rapidly learn with a small set of data. Accordingly, the proposed framework achieves 6.98% improvement in terms of PR AUC compared with BERT. Compared with best baseline Meta-SGD, the proposed framework achieves 6.51% in terms of PR AUC. The similar improvement can be also observed from performance comparison on R@P=0.8, the proposed framework improves more than 16% compared with the second best result. Furthermore, we can observe that the proposed MetaBridge achieves the best performance compared with all the baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Study</head><p>Compared with MAML, our derived objective function has two main differences: stochastic characteristic and KL Divergence between support and query data. Thus, we are interested in their roles in the performance improvements. As introduced in the Section 3.4, we cannot simply remove one of them considering the KL Divergence and sampling are tightly coupled with each other. Instead, we propose two variants of MAML as baselines to explore the role of stochastic and KL Divergence respectively. To explore the role of stochastic characteristic, we add random noise into the input to last layer of MAML and denote it as stochastic variant. To explore the role of KL Divergence, we reduce sampling step and assume that the posterior distributions of support and query data are from normal distributions with fixed variances N (µ(x s t ), 1) and N (µ(x q t ), 1). The proposed variant is denotes as KL variant.</p><p>We use Flavor dataset as an example. As can be seen from Fig. <ref type="figure" target="#fig_2">2</ref>, the highest PR AUC score of stochastic variant is similar with that of MAML. However, unlike MAML, the stochastic variant remains highest value without dropping. This shows that the stochastic characteristic can help prevent overfitting issue. By the comparison between KL variant and MAML, we can observe the KL variant can achieve a better PR AUC during the all training epochs. This shows the KL Divergence can construct an effective information flow between support and query data to further improve the performance. However, the KL variant simply assumes that posterior distributions are from normal distribution with fixed variances, and the oversimplistic assumption limits the potential of KL Divergence. By incorporating variances estimation, our proposed framework avoids the over-simplistic distribution assumption and can achieve better performance compared with KL variant. In overall, our proposed framework enjoys the benefits of stochastic characteristic and KL Divergence simultaneously. In our objective function, we use hyperparameter λ to control the strength between Inference loss KL Divergence. In this study, we aim to explore the impact of λ in the proposed framework. We train the proposed framework using different hyperparameter λ on the Flavor Dataset. Fig. <ref type="figure" target="#fig_3">3</ref> shows the PR AUC changes of the proposed model with respect to different λ's. When λ is set to 0, the sampling procedure is removed and the model is equivalent to MAML. We can observe that such a variant cannot effectively take advantage of unlabelled support data and the best PR AUC score is lower than that of other approach variants. And, such a variant suffers from the overfitting issue and converges to worst PR AUC value compared with other models. After changing the λ from 0 to 0.1, we can observe that the PR AUC values are stably higher than that of the variant with λ = 0. As the λ value further increases from 0.1 to 1, the proposed framework achieves significant improvement around 4% in terms of PR AUC compared with the variant λ = 0. This illustrates that our objective function can take advantage of unlabeled and small labeled data effectively and improves the generalize ability of the model. When we change value of λ to 3, the PR AUC of model increases slowly in the first 220 epochs compared with other models. But after 220 epochs, the model can archive a high PR AUC value. This further confirms the effectiveness of KL Divergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Hyperparameter Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Varying Size of Labels</head><p>To analyze the impact of the query data size per product category, we train the proposed approach with different number of query data as 3, 5, 10 per category. The procedure is repeated five times and we report average performance with corresponding standard deviation. To be simple, we denote model variant by its name and number of query data. For example, the MAML which is trained with 3 query data per category is denoted as MAML 3 . Figure <ref type="figure" target="#fig_4">4</ref> shows the performance comparison of the models with different number of query data in terms of PR AUC (Fig. <ref type="figure" target="#fig_4">4a</ref>) and R@P=0.8 (Fig. <ref type="figure" target="#fig_4">4b</ref>). When query data number is 3, our proposed framework achieves around 5.5% improvement compared with MAML 3 in terms of PR AUC. This demonstrates the effectiveness of our model with a smaller set of labeled data available. The reason is that our proposed framework can caputre category uncertainty via unlabeled data and enforce distribution consistence between unlabeled support and labeled query data. Thus, the improvement of our proposed framework over MAML is larger when the number of query data is smaller. As the number of query data increases, the performance values of MAML and our proposed framework improve significantly. This shows that meta-learning models can effectively take advantage of labeled data. For all three settings, our proposed framework shows significant improvement compared with MAML. The improvement further confirms the superiority of our proposed framework.</p><p>The similar results can be observed from Fig. <ref type="figure" target="#fig_4">4b</ref>. The R@P is an important metric when we evaluate our model in the real setting. Our model achieves around 40% and 30% improvement respectively over MAML in terms of R@P=0.8 when the number of query data is set to 3 and 5. When the number of query data is set to 10, the R@P of our model is 53.6% which is higher than that of our proposed framework with 5 query data more than 6%. This reveals the potential of our model if more labeled data is available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head><p>Attribute validation task is related to anomaly detection which aims to find patterns in data that do not conform to expected behavior <ref type="bibr" target="#b8">[9]</ref>. In the anomaly detection, the most related line of research is log anomaly detection which aims to find text, which can indicate the reasons and the nature of the failure of a system <ref type="bibr" target="#b7">[8]</ref>. The traditional methods typically extract features from unstructured texts and then detect anomalies based on hand-craft features. Compared with traditional learning, deep learning models have achieved an improvement in the performance of anomaly detection due to their powerful abilities <ref type="bibr" target="#b7">[8]</ref>. The deep learning anomaly detection (DAD) approaches <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b29">30]</ref> model the log data as a natural language sequence and apply RNN and CNN to detect anomalies. Different with log anomaly detection, our problem needs to infer the correctness of attribute values based on product profile information. Attribute validation task is also related to natural language inference (NLI). NLI is a classification task where a system is asked to classify the relationship between a pair of premise and hypothesis as either entailment, contradiction or neutral. Large annotated datasets such as the Stanford Natural Language Inference <ref type="bibr" target="#b5">[6]</ref> (SNLI) and Multi-Genre Natural Language Inference <ref type="bibr" target="#b27">[28]</ref> (MultiNLI) corpus have promoted the development of many different neural NLI models <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b23">24]</ref> that achieve promising performance. However, NLI task usually requires large annotated datasets for training purpose. While pre-training is beneficial, it is not optimized to allow fine-tuning with limited supervision and such models can still require large amounts of task-specific data for fine-tuning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b28">29]</ref>. Thus, how to train a NLI model with a small set of dataset for a specific domain is still a very challenging problem.</p><p>Another related and complementary line of research is metalearning. Meta-learning has long been proposed as a form of learning that would allow systems to systematically build up and re-use knowledge across different but related tasks <ref type="bibr" target="#b24">[25]</ref>. More specifically, meta-Learning approaches can be broadly classified into three categories: optimization-based, model-based and metric-learning based models. Optimization-based methods aim to modify the gradient descent based learning procedure for new task quick adaptation. In the optimization-based methods, MAML <ref type="bibr" target="#b11">[12]</ref> is a recent promising model which learns a set of model parameters that are used to rapidly learn novel task with a small set of labeled data. Following this direction, Meta-SGD <ref type="bibr" target="#b20">[21]</ref> learns step sizes and updates directions besides initialization parameters in the training procedure. In our problem, we propose to use unlabeled data to capture task uncertainty to avoid constant labeling efforts. Towards this end, our objective function has two terms: inference loss and bridging regularizer. The proposed objective function constructs an effective information flow between unlabeled support data and labeled query data via the bridging regularizer and the sampling procedure can further prevent overfitting. Thus, we can train a Transformer-like models to capture rich feature representations for specific task with limited labeled data and the proposed framework can quickly adapt to new task with unlabeled data only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS</head><p>In this paper, we proposed to solve an important but challenging problem faced by many eCommerce websites, which is the automatic validation of textual attribute value associated with a product. Due to the large number of product categories and the huge variety among products in different categories, we cannot obtain sufficient training data for all the categories, which are needed for training deep learning models. In light of this challenge, we proposed a novel meta-learning latent variable approach, MetaBridge, that can leverage labeled data for limited categories and utilize unlabeled data for effective correctness inference. The proposed model captures category-uncertainty via unlabeled data and trains a Transformerbased model with limited labeled data. The proposed framework has shown significantly improved performance in textual attribute validation. This was demonstrated in a series of experiments conducted on two real-world datasets from hundreds of product categories across domains on Amazon.com.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The proposed approach MetaBridge. The proposed approach mainly includes two stages: adaptation and Validation. During the adaptation stage, the model parameter Θ is updated to Θ c accordingly to capture the uncertainty of category c. During the validation stage, the adapted model Θ c is used to validate textual attributes for products on the category c.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1</head><label>1</label><figDesc>Training Procedure. Require: Task data, learning rate α and inner step size β ; 1: for epoch l ← 1 to L do 2: Sample a batch of categories C; 3: for all c ∈ C do 4: Get support set D s c and query set D q c</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The changes of PR AUC for the models in term of the number of Epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The changes of PR AUC with different λ's.</figDesc><graphic url="image-22.png" coords="7,366.08,500.49,144.00,108.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The performance comparison of models with different numbers of query data per product category.</figDesc><graphic url="image-23.png" coords="8,322.07,93.65,124.85,93.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>𝐷 𝐾𝐿 (𝑞 𝜙 𝑐 (𝑧|𝑥 𝑐 𝑞 )||𝑞 𝜙 𝑐 𝑧 𝑥 𝑐 𝑠 )</figDesc><table><row><cell></cell><cell cols="2">Validation</cell><cell></cell><cell></cell><cell cols="2">KL Divergence</cell></row><row><cell>s 𝒙 𝒄</cell><cell>Product Profile</cell><cell>Attributes</cell><cell>Encoder 𝜙 𝑐</cell><cell>𝜇 𝜎</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>+</cell></row><row><cell>𝒙 𝒄 𝒒</cell><cell>Product Profile</cell><cell>Attributes</cell><cell>Encoder 𝜙 𝑐</cell><cell>𝜇 𝜎</cell><cell>Decoder 𝜃 𝑐</cell><cell>Cross Entropy −𝐸 𝑞 𝜙𝑐 (𝑧|𝑥 𝑐 𝑞 ) [log 𝑝 𝜃 𝑐 (𝑦 𝑐 𝑞 |𝑧)]</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>𝜇</cell><cell></cell><cell></cell></row><row><cell>s 𝒙 𝒄</cell><cell>Product Profile</cell><cell>Attributes</cell><cell></cell><cell>𝜎</cell><cell>Decoder 𝜃</cell><cell>−𝐸 𝑞</cell></row><row><cell></cell><cell cols="2">Adaptation</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>𝜙 (𝑧|𝑥 𝑐 s ) [𝑝 𝜃 z log 𝑝 𝜃 ( 𝑧)]</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The Statistics of the Amazon Datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="3"># of Product Categories # of unlabeled Data # of labeled Data</cell></row><row><cell>Flavor</cell><cell>321</cell><cell>32,100</cell><cell>3,210</cell></row><row><cell>Ingredient</cell><cell>658</cell><cell>65,800</cell><cell>6,580</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The performance comparison of different methods in the Flavor and Ingredient data.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank the anonymous referees for their valuable comments and helpful suggestions, would like to thank Ron Benson, Christos Faloutsos, Jun Ma, Giannis Karamanolakis, Haw-Shiuan Chang, Junheng Hao, Zhengyang Wang, Yuning Mao and Wei Hao for their insightful comments on the project, and Saurabh Deshpande, Prashant Shiralkar, Tong Zhao for their constructive feedback on data integration for the experiments. This work is supported in part by the US National Science Foundation under grant NSF-IIS 1747614. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Trapit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishikesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.03863</idno>
		<title level="m">Learning to Few-Shot Learn Across Diverse Natural Language Classification Tasks</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep generative stochastic networks trainable by backprop</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Laufer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Alain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="226" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Estimating or propagating gradients through stochastic neurons for conditional computation</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Léonard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.3432</idno>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5050" to="5060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Enriching Word Vectors with Subword Information</title>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Gabor</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.05326</idno>
		<title level="m">A large annotated corpus for learning natural language inference</title>
				<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Recurrent neural network attention mechanisms for interpretable system log anomaly detection</title>
		<author>
			<persName><forename type="first">Andy</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Tuor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicole</forename><surname>Nichols</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Machine Learning for Computing Systems</title>
				<meeting>the First Workshop on Machine Learning for Computing Systems</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deep learning for anomaly detection: A survey</title>
		<author>
			<persName><forename type="first">Raghavendra</forename><surname>Chalapathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Chawla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.03407</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Anomaly detection: A survey</title>
		<author>
			<persName><forename type="first">Varun</forename><surname>Chandola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arindam</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vipin</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM computing surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Enhanced LSTM for Natural Language Inference</title>
		<author>
			<persName><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Inkpen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1657" to="1668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
		<title level="s">the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Long and Short Papers</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Model-agnostic metalearning for fast adaptation of deep networks</title>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Marta</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><surname>Teh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.01622</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Neural processes. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">DR-BiLSTM: Dependent Reading Bidirectional LSTM for Natural Language Inference</title>
		<author>
			<persName><forename type="first">Reza</forename><surname>Ghaeini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sadid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joey</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathy</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashequl</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Qadir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaditya</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoli</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oladimeji</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName><surname>Farri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1460" to="1469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by entropy minimization</title>
		<author>
			<persName><forename type="first">Yves</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="529" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Towards environment independent device free human activity recognition</title>
		<author>
			<persName><forename type="first">Wenjun</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenglin</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fenglong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuochao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongfei</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitrios</forename><surname>Koutsonikolas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MobiCom</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="289" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">TXtract: Taxonomy-Aware Knowledge Extraction for Thousands of Product Categories</title>
		<author>
			<persName><forename type="first">Giannis</forename><surname>Karamanolakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Luna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2004.13852</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
				<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName><forename type="first">Dong-Hyun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Challenges in Representation Learning, ICML</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Meta-SGD: Learning to learn quickly for few-shot learning</title>
		<author>
			<persName><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09835</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Minimal-entropy correlation alignment for unsupervised deep domain adaptation</title>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Morerio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacopo</forename><surname>Cavazza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vittorio</forename><surname>Murino</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.10288</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine Learning in Python</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A perspective view and survey of meta-learning</title>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Vilalta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youssef</forename><surname>Drissi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence review</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="77" to="95" />
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Advent: Adversarial entropy minimization for domain adaptation in semantic segmentation</title>
		<author>
			<persName><forename type="first">Tuan-Hung</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Himalaya</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxime</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2517" to="2526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><surname>Samuel R Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05426</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cyprien</forename><surname>De Masson D'autume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerome</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Chrzanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.11373</idno>
		<title level="m">Learning and evaluating general linguistic intelligence</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">AlphaMEX: a smarter global pooling method for convolutional neural networks</title>
		<author>
			<persName><forename type="first">Boxue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenquan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuchang</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">321</biblScope>
			<biblScope unit="page" from="36" to="48" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Opentag: Open attribute value extraction from product profiles</title>
		<author>
			<persName><forename type="first">Guineng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhabrata</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Luna Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feifei</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
