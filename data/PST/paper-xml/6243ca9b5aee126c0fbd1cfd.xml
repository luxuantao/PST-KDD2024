<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GRAPH NEURAL NETWORKS ARE DYNAMIC PROGRAMMERS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-03-29">29 Mar 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Andrew</forename><surname>Dudzik</surname></persName>
							<email>adudzik@deepmind.com</email>
						</author>
						<author>
							<persName><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
							<email>petarv@deepmind.com</email>
						</author>
						<title level="a" type="main">GRAPH NEURAL NETWORKS ARE DYNAMIC PROGRAMMERS</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-03-29">29 Mar 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2203.15544v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent advances in neural algorithmic reasoning with graph neural networks (GNNs) are propped up by the notion of algorithmic alignment. Broadly, a neural network will be better at learning to execute a reasoning task (in terms of sample complexity) if its individual components align well with the target algorithm. Specifically, GNNs are claimed to align with dynamic programming (DP), a general problem-solving strategy which expresses many polynomial-time algorithms. However, has this alignment truly been demonstrated and theoretically quantified? Here we show, using methods from category theory and abstract algebra, that there exists an intricate connection between GNNs and DP, going well beyond the initial observations over individual algorithms such as Bellman-Ford. Exposing this connection, we easily verify several prior findings in the literature, and hope it will serve as a foundation for building stronger algorithmically aligned GNNs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>One of the principal pillars of neural algorithmic reasoning <ref type="bibr">(Veli?kovi? &amp; Blundell, 2021)</ref> is training neural networks that execute algorithmic computation in a high-dimensional latent space. While this process is in itself insightful, and can lead to strengthening combinatorial optimisation systems <ref type="bibr" target="#b13">(Nair et al., 2020)</ref>, it is also valuable in terms of expanding the applicability of classical algorithms. First evidence of this value are starting to emerge, with pre-trained algorithmic reasoners utilised in implicit planning <ref type="bibr" target="#b7">(Deac et al., 2021)</ref> and self-supervised learning <ref type="bibr">(Veli?kovi? et al., 2021)</ref>.</p><p>A fundamental question in this space is: which architecture should be used to learn a particular algorithm (or collection of algorithms, as in <ref type="bibr" target="#b25">Xhonneux et al. (2021)</ref>)? Naturally, we seek architectures that have low sample complexity, as they will allow us to create models that generalise better with fewer training examples.</p><p>The key theoretical advance towards achieving this aim has been made by <ref type="bibr" target="#b27">Xu et al. (2019)</ref>. Therein, the authors formalise the notion of algorithmic alignment, which states that we should favour architectures that align better to the algorithm, in the sense that we can separate them into modules, which individually correspond to the computations of the target algorithm's subroutines. It can be proved that architectures with higher algorithmic alignment will have lower sample complexity in the NTK regime <ref type="bibr" target="#b11">(Jacot et al., 2018)</ref>. Further, the theory of <ref type="bibr" target="#b27">Xu et al. (2019)</ref> predicts that graph neural networks (GNNs) algorithmically align with dynamic programming <ref type="bibr">(Bellman, 1966, DP)</ref>. The authors demonstrate this by forming an analogy to the Bellman-Ford algorithm <ref type="bibr" target="#b0">(Bellman, 1958)</ref>.</p><p>Since DP is a very general class of problem-solving techniques that can be used to express many classical algorithms, this finding has placed GNNs as the central methodology for neural algorithmic execution <ref type="bibr" target="#b4">(Cappart et al., 2021)</ref>. However, it quickly became apparent that it is not enough to just train any GNN-for many algorithmic tasks, careful attention is required. Several papers illustrated special cases of GNNs that align with sequential algorithms <ref type="bibr" target="#b20">(Veli?kovi? et al., 2019)</ref>, linearithmic sequence processing <ref type="bibr" target="#b10">(Freivalds et al., 2019)</ref>, physics simulations <ref type="bibr" target="#b15">(Sanchez-Gonzalez et al., 2020)</ref>, iterative algorihtms <ref type="bibr" target="#b17">(Tang et al., 2020)</ref>, data structures <ref type="bibr" target="#b21">(Veli?kovi? et al., 2020)</ref> or auxiliary memory <ref type="bibr" target="#b16">(Strathmann et al., 2021)</ref>. Some explanations for this lack of easy generalisation have arisen-we now have both geometric <ref type="bibr">(Xu et al., 2020)</ref> and causal <ref type="bibr" target="#b2">(Bevilacqua et al., 2021)</ref> views into how better generalisation can be achieved.</p><p>We believe that the fundamental reason why so many isolated efforts needed to look into learning specific classes of algorithms is the fact the GNN-DP connection has not been sufficiently explored. Indeed, the original work of <ref type="bibr" target="#b27">Xu et al. (2019)</ref> merely mentions in passing that the formulation of DP algorithms seems to align with GNNs, and demonstrates one example <ref type="bibr">(Bellman-Ford)</ref>. Our thorough investigation of the literature yielded no concrete follow-up to this initial claim. But DP algorithms are very rich and diverse, often requiring a broad spectrum of computations. Hence what we really need is a framework that could allow us to identify GNNs that could align particularly well with certain classes of DP, rather than assuming a "one-size-fits-all" GNN architecture will exist.</p><p>As a first step towards this, in this paper we interpret the operations of both DP and GNNs from the lens of category theory and abstract algebra. We elucidate the GNN-DP connection by observing a diagrammatic abstraction of their computations, recasting algorithmic alignment to aligning the diagrams of (G)NNs to ones of the target algorithm class. In doing so, several previously shown results will naturally arise as corollaries, and we hope it opens up the door to a broader unification between algorithmic reasoning and the geometric deep learning blueprint <ref type="bibr" target="#b3">(Bronstein et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">GNNS, DYNAMIC PROGRAMMING, AND THE CATEGORICAL CONNECTION</head><p>Before diving into the theory behind our connection, we provide a quick recap on the methods being connected: graph neural networks and dynamic programming. Further, we cite related work to outline why it is sufficient to interpret DP from the lens of graph algorithms.</p><p>We will use the definition of GNNs based on <ref type="bibr" target="#b3">Bronstein et al. (2021)</ref>. Let a graph be a tuple of nodes and edges, G = (V, E), with one-hop neighbourhoods defined as</p><formula xml:id="formula_0">N u = {v ? V | (v, u) ? E}.</formula><p>Further, a node feature matrix X ? R |V |?k gives the features of node u as x u ; we omit edge-and graph-level features for clarity. A (message passing) GNN over this graph is then executed as:</p><formula xml:id="formula_1">h u = ? v?Nu ?(x u , x v )<label>(1)</label></formula><p>where ? : R k ? R k ? R k is a message function, ? : R k ? R k is a readout function, and is a permutation-invariant aggregation function (such as or max). Both ? and ? can be realised as MLPs, but many special cases exist, giving rise to, e.g., attentional GNNs <ref type="bibr" target="#b19">(Veli?kovi? et al., 2017)</ref>. Dynamic programming is defined as a process that solves problems in a divide et impera fashion: imagine that we want to solve a problem instance x. DP proceeds to identify a set of subproblems, ?(x), such that solving them first, and recombining the answers, can directly lead to the solution for x: f (x) = ?({f (y) | y ? ?(x)}). Eventually, we decompose the problem enough until we arrive at a instance for which the solution is trivially given (i.e. f (y) which is known upfront). From these "base cases", we can gradually build up the solution for the problem instance we initially care for in a bottom-up fashion. This rule is often expressed programmatically:</p><formula xml:id="formula_2">dp[x] ? recombine(score(dp[y], dp[x]) for y in expand(x))<label>(2)</label></formula><p>To initiate our discussion on why DP can be connected with GNNs, it is a worthwhile exercise to show how Equation 2 induces a graph structure. To see this, we leverage a categorical analysis of dynamic programming first proposed by De Moor (1994). Therein, dynamic programming algorithms are reasoned about as a composition of three components (presented here on a high level):</p><formula xml:id="formula_3">dp = ? recombine ? ? score ? ? expand (3)</formula><p>Expansion selects the relevant subproblems; scoring computes the quality of each individual subproblem's solution w.r.t. the current problem, and recombining combines these solutions into a solution for the original problem (e.g. by taking the max, or average).</p><p>Therefore, we can actually identify every subproblem as a node in a graph. Let V be the space of all subproblems, and R an appropriate value space (e.g. the real numbers). Then, expansion is defined as ? : V ? P(V ), giving the set of all subproblems relevant for a given problem. Note that this also induces a set of edges between subproblems, E; namely, (x, y) ? E if x ? ?(y). Each subproblem is scored by using a function ? : P(V ) ? P(R). Finally, the individual scores are recombined using the recombination function, ? : P(R) ? R. The final dynamic programming primitive therefore computes a function dp : V ? R in each of the subproblems of interest.</p><p>Therefore, dynamic programming algorithms can be seen as performing computations over a graph of subproblems, which can usually be precomputed for the task at hand (since the outputs of ? are assumed known upfront for every subproblem). One specific popular example is the Bellman-Ford algorithm <ref type="bibr" target="#b0">(Bellman, 1958)</ref>, which computes single-source shortest paths from a given source node, s, in a graph G = (V, E). In this case, the set of subproblems is exactly the set of nodes, V , and the expansion ?(u) is exactly the set of one-hop neighbours of u in the graph. The algorithm maintains distances of every node to the source, d u . The rule for iteratively recombining these predicted distances is as follows:</p><formula xml:id="formula_4">d u ? min d u , min v?Nu d v + w v?u (4)</formula><p>where w v?u is the distance between nodes v and u. The algorithm's base cases are d s = 0 for the source node, d u = +? otherwise. Note that more general forms of Bellman-Ford pathfinding exist, for appropriate definitions of + and min (in general known as a semiring). Several recent research papers such as NBFNet <ref type="bibr">(Zhu et al., 2021)</ref> explicitly call on this alignment in their motivation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THE DIFFICULTY OF CONNECTING GNNS AND DP</head><p>The basic technical obstacle to establishing a rigorous correspondence between neural networks and DP is the vastly different character of the computations they perform. Neural networks are built from linear algebra over the familiar real numbers, while DP, which is often a generalisation of path-finding problems, typically takes place over "tropical" objects like (N ? {?}, min, +)<ref type="foot" target="#foot_0">1</ref> , which are usually studied in mathematics as "degenerations" of Euclidean space. The two worlds cannot clearly be reconciled, directly, with simple equations.</p><p>However, if we define an arbitrary "latent space" R and make as few assumptions as possible, we can observe that many of the behaviors we care about, for both GNNs and DP, arise from looking at functions S ? R, where S is a finite set. R can be seen as the set of real-valued vectors in the case of GNNs, and the tropical numbers in the case of DP.</p><p>So our principal object of study is the category of finite sets, and "R-valued quantities" on it. By "category" here we mean a collection of objects (all finite sets) together with a notion of composable arrows (functions between finite sets).</p><p>To draw our GNN-DP connection, we need to devise an abstract object which can capture both the GNN's message passing/aggregation stages (Equation <ref type="formula" target="#formula_1">1</ref>) and the DP's scoring/recombination stages (Equation <ref type="formula" target="#formula_2">2</ref>). It may seem quite intuitive that these two concepts can and should be relatable, and category theory is a very attractive tool for "making the obvious even more obvious" <ref type="bibr" target="#b9">(Fong &amp; Spivak, 2019)</ref>. Indeed, recently concepts from category theory have enabled the construction of powerful GNN architectures beyond permutation equivariance (de Haan et al., 2020).</p><p>We will construct the integral transform by composing transformations over our input features in a way that will depend minimally on the specific choice of R. In doing so, we will build a computational diagram that will be applicable for both GNNs and DP (and their own choices of R), and hence allowing for focusing on making components of those diagrams as aligned as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">INTEGRAL TRANSFORMS, PULLBACK AND PUSHFORWARD</head><p>The general situation for integral transforms is a diagram of this form, called a span: Each edge e uv is connected to its sender and receiver nodes (u, v) via the span (black arrows). The pullback then "pulls" the node features f (u) along the span, into edge features g(e vu ) = f (u).</p><formula xml:id="formula_5">V E W t s</formula><p>Once all sender features are pulled back to their edges, the pushforward then "collects" all of the edge features that send to a particular receiver, by pushing them along the span.</p><p>Here V, E, W are arbitrary finite sets and s, t are arbitrary functions. Note that when W = V , this is exactly the data needed to give (V, E) the structure of a directed multigraph, where we interpret s(e) as the source of an edge and t(e) as the target of an edge.</p><p>The general question is: given some data f on V assigning features f (v) to each v ? V , how to transform it via this span into data on W ? If we are able to do this, we will be able to characterise both the processes of sending messages between nodes in GNNs and scoring subproblems in DP.</p><p>We need to describe two operations, a pullback s * along s, and a pushforward t * along t. The outcome of performing the pullback of data defined on V is data defined on E. Then the pushforward should take data defined on E and give data defined on W , though we will see that it actually gives a "bag" of data that needs to be aggregated. Conveniently, this process will naturally align with the aggregation step of GNNs, as well as the recombination step of DP.</p><p>For us, our data consists of a function f : V ? R. This makes it trivial to define the pullback: we have s * f := f ? s (map the edge to its sender node, then look up the features in f ).</p><p>However, the pushforward is problematic, as t faces in the wrong direction to use function composition. To get an arrow facing the right direction, we need the preimage t -1 : W ? P(E), which takes values in the power set of E. The preimage is defined as t -1 (w) = {e | t(e) = w}.</p><p>If we now take t * g := g ? t -1 , such a construction doesn't yield a function</p><formula xml:id="formula_6">W ? R but instead a function W ? N[R],</formula><p>taking values in finite multisets over R. So the result of this "formal pushforward" isn't a value, but a "bag" of values. The reason why we require multisets is because there may be several edges in t's preimage that store the same value; this also aligns nicely with related work that studies GNN expressive power using multisets <ref type="bibr" target="#b26">(Xu et al., 2018)</ref>.</p><p>All that is missing to complete our transform is an aggregator, : N[R] ? R. As we will see in the next section, specifying a well-behaved aggregator is the same as imposing a commutative monoid structure on R.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">THE MULTISET MONAD, COMMUTATIVE MONOIDS AND AGGREGATORS</head><p>Given a set S, we define N[S] := {p : S ? N | #{p(r) = 0} &lt; ?}, the natural-valued functions of finite support on S. This has a clear correspondence to multisets over S: p sends each element of S to the amount of times it appears in the multiset. We can write its elements formally as s?S n s s, where all but finitely many of the n s are nonzero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Given a function f : S ? T between sets, we can define a function N[f ] : N[S] ? N[T ], as follows:</head><p>N[f ]( s?S n s s) := s?S n s f (s), which we can write as t?T m t t, where m t = f (s)=t n s .</p><p>For each S, we can also define two special functions. The first is unit : S ? N[S], sending each element to its indicator function (i.e. an element x ? S to the multiset { {x} }). The second is join :</p><formula xml:id="formula_7">N[N[S]] ? N[S]</formula><p>, which interprets a nested sum as a single sum.</p><p>These facts tell us that N[-] is a monad, a special kind of self-transformation of the category of sets. Monads are very general tools for computation, used heavily in functional programming languages (e.g. Haskell) to model the semantics of wrapped or enriched types. Monads provide a clean way for abstracting control flow, as well as gracefully handling functions with side effects <ref type="bibr" target="#b23">(Wadler, 1990)</ref>.</p><p>It is well-known that the algebras for the monad N[-] are the commutative monoids, sets equipped with a commutative and associative binary operation and a unit element.</p><p>Concretely, a commutative monoid structure on a set R is equivalent to defining an aggregator function</p><p>: N[R] ? R compatible with the unit and monad composition. Here, compatibility implies it should correctly handle sums of singletons and sums of sums, in the sense that the following two diagrams commute; that is, they yield the same result regardless of which path is taken:</p><formula xml:id="formula_8">R N[R] N[N[R]] N[R] R N[R] R N[ ] join unit id</formula><p>The first diagram explains that the outcome of aggregating a singleton multiset (i.e. the one produced by applying unit) with is equivalent to the original value placed in the singleton. The second diagram indicates that the operator yields the same results over a nested multiset, regardless of whether we choose to directly apply it twice (once on each level of nesting), or first perform the join function to collapse the nested multiset, then aggregate the collapsed multiset with .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conveniently, such a</head><p>is exactly what is required to appropriately connect the outcomes of our previously derived pullback (s * ) and pushforward (t * ) operations into a commutative diagram:</p><formula xml:id="formula_9">V E W R R N[R] s t s * f t * s * f f id</formula><p>As such, so long as R is endowed with a commutative monoid structure, we can use this construction to define a suitable to serve as our aggregation function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">THE FOUR ARROWS OF THE INTEGRAL TRANSFORM</head><p>As previously hinted, a directed graph G = (V, E) is equivalent to a span:</p><formula xml:id="formula_10">V E V t s</formula><p>where s and t provide the sender and receiver nodes for each edge, respectively.</p><p>Beginning with some input features f : V ? R, we can now use all of the previously described objects to describe an integral transform of f . Initially, we will focus on the cases where the "message" sent along edge e ? E depends only on the sender and not the receiver node. This is certainly the case in many path-finding algorithms, such as Bellman-Ford (Equation <ref type="formula">4</ref>). We provide details on how to generalise the messages to be receiver-conditioned in Appendix A.</p><p>We start by defining a kernel transformation k : [E, R] ? [E, R] that performs some computation<ref type="foot" target="#foot_1">2</ref> over edge features, e.g. in the case of Bellman-Ford, adding the sender distance d s(e) (previously pulled back to the edge via s * ) to the edge weight w s(e)?t(e) . Here we use [E, R] as a shorthand notation for the set of functions E ? R. Using the kernel, we can complete the following diagram:</p><formula xml:id="formula_11">[V, R] [E, R] [E, R] [V, N[R]] [V, R] k t * s *</formula><p>These four arrows, taken together, form our integral transform: starting with node features, performing computations over edges, finally aggregating the edge messages in the receivers. In the process of doing so, we have updated our initial node features (which are elements of [V, R]). We now explain each of the four arrows in turn.</p><p>s * is the previously-defined pullback arrow. We precompose the node features with the source function, as described before. That is, (s * f )(v) := f (s(v)). Then, the kernel is applied to the resulting edge features, integrating the sender's features with any provided edge features (such as edge weights).</p><p>After applying our kernel, we have edge messages m : E ? R as a result. We now need to send these messages to the receiver nodes, for which we employ exactly the pushforward. As before, we define</p><formula xml:id="formula_12">(t * m)(v) = e?t -1 (v) m(e), which we interpret as a formal sum in N[R]. Intuitively, (t * m)(v) is the "bag of incoming values" at v.</formula><p>Finally, we apply the previously defined aggregator , pointwise for every receiver, to compute updated features for every node.</p><p>Note the generality of our construction: nearly all of the operations at play here rely on the properties of the sets E and V . The only arrow which is constraining the choice of R is the aggregator ; it necessitates that R be a commutative monoid, which is a sensible choice for both pathfinding algorithms and GNNs. Hence, we have defined a generic blueprint where arbitrary Rs can be inserted, hence amplifying the connection between GNNs (where R often includes real-valued vectors) and DP (where R may be the previously mentioned "tropical" numbers).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">IMMEDIATE COROLLARIES OF THE INTEGRAL TRANSFORM VIEW</head><p>Since we designed the aforementioned integral transform in a way that it can describe both GNNs (Equation <ref type="formula" target="#formula_1">1</ref>) and DP (Equation <ref type="formula" target="#formula_2">2</ref>), there exist some "embarrassingly obvious" ways of making the architecture of the GNN more algorithmically aligned to the target DP algorithm. Perhaps the clearest one is the final arrow, the aggregator ( ). If we make the GNN's choice of aggregator function match the one used by the target algorithm, this should lead to immediate gains to sample complexity and generalisation.</p><p>Indeed, this aligns well with one of the earliest lines of research in algorithmic reasoning: deploying GNNs with aggregators that align to the problem. Already in <ref type="bibr" target="#b20">Veli?kovi? et al. (2019)</ref>, it was demonstrated that using the max aggregation in GNNs yields superior performance on executing pathfinding algorithms, especially when compared to the more popular choice of . Similar findings were observed by <ref type="bibr" target="#b17">Tang et al. (2020)</ref>; <ref type="bibr" target="#b14">Richter &amp; Wattenhofer (2020)</ref>, who also clearly indicated environments where max is the superior aggregator. Lastly, theoretical works such as <ref type="bibr">Xu et al. (2020)</ref>; <ref type="bibr" target="#b12">Li et al. (2021)</ref> have illustrated the additional useful corollaries to extrapolation and noisylabel training when using aligned aggregators.</p><p>The fact that already such a vast collection of research works can be unified as analysing one arrow in a categorical diagram is a statement to the potential of our proposed analysis. Any invested effort in analysing GNNs and DP in this manner could provide strong returns when organising existing research and proposing future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">IDEAS FOR FUTURE WORK</head><p>What other arrows can be "inspected" in this way? The most natural candidate is the kernel arrow, which corresponds to both the GNN's message function ? (Equation <ref type="formula" target="#formula_1">1</ref>) to the DP scoring function (Equation <ref type="formula" target="#formula_2">2</ref>). Aligning the two more closely may require looking deeper into the kernel arrow to formalise entirely, and we leave such analyses for future work to maintain a focused contribution.</p><p>While it might seem that the pullback and pushforward arrows are fairly static, this is primarily because both the graph structure (in the case of GNNs) and the expansions (in the case of DP) are assumed fixed known upfront. This is a simplifying assumption that need not always hold (for example, if our algorithm relies on any kind of data structure). Another promising avenue for future work could dive deeper into these two arrows as well, and specifically to develop GNNs that automatically propose subproblems within their computations. Lastly, it is not at all unlikely that analyses similar to ours have already been used to describe other fields of science-beyond algorithmic reasoners. The principal ideas of span and integral transform are central to defining Fourier series <ref type="bibr" target="#b24">(Willerton, 2020)</ref>, and appear in the analysis of Yang-Mills equations in particle physics <ref type="bibr" target="#b8">(Eastwood et al., 1981)</ref>. Properly understanding the common ground behind all of these definitions may, in the very least, lead to interesting connections, and a shared understanding between the various fields they span.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">CONCLUSIONS</head><p>In this paper, we describe the use of category theory and abstract algebra to explicitly expand on the GNN-DP connection, which was previously largely handwaved on specific examples. We derived a generic diagram of an integral transform (based on standard categorical concepts like pullback, pushforward and commutative monoids), and argued why it is general enough to support both GNN and DP computations. With this diagram materialised, we were able to immediately unify large quantities of prior work as simply manipulating one arrow in the integral transform. It is our hope that our findings inspire future research into better-aligned neural algorithmic reasoners, especially focusing on generalising or diving into several aspects of this diagram. <ref type="bibr">Keyulu Xu, Mozhi Zhang, Jingling Li, Simon S Du, Ken-ichi Kawarabayashi, and</ref>  A RECEIVER-DEPENDENT KERNELS</p><p>The construction in Section 6 does not immediately support receiver-dependent features, as any feature attached to a non-sender node is lost in the initial pullback step. To get around this issue without losing the spirit of the integral transform, we assume that each e ? E has an opposite e * ? E (s.t. s(e) = t(e * ), s(e * ) = t(e)).</p><p>While such a construction is natural for undirected graphs, it is possible to make it work while having directionality constraints. Specifically, the kernel can mask these extra edges as an implementation detail (e.g. attaching an edge weight of "+?" in the context of pathfinding), so this construction does not lose generality. Now, we can express receiver dependence by factorising the kernel arrow as follows:</p><formula xml:id="formula_13">[E, R] [E, R] ? [E, R] [E, R] (id, * ) k</formula><p>The first arrow sends g(e) to g(e, e * ). This gives our k access to both sender and receiver features. Again, we can perform any necessary receiver masking within k to avoid contributions from "virtual" edges.</p><p>In general, more complex kernels may require additionally enlarging E by a constant factor. For example, if E is given an additional symmetry E ? E we could use this to express ternary kernels, and so on.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The illustration of how pullback and pushforward combine to form the integral transform.Each edge e uv is connected to its sender and receiver nodes (u, v) via the span (black arrows). The pullback then "pulls" the node features f (u) along the span, into edge features g(e vu ) = f (u). Once all sender features are pulled back to their edges, the pushforward then "collects" all of the edge features that send to a particular receiver, by pushing them along the span.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Stefanie Jegelka. How neural networks extrapolate: From feedforward to graph neural networks. arXiv preprint arXiv:2009.11848, 2020. Zhaocheng Zhu, Zuobai Zhang, Louis-Pascal Xhonneux, and Jian Tang. Neural bellman-ford networks: A general graph neural network framework for link prediction. Advances in Neural Information Processing Systems, 34, 2021.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Here we require the addition of a special "?" placeholder object to denote the vertices the DP expansion hasn't reached so far.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Note that, in many cases, this kernel is merely distributing a function ? : R ? R, applying it pointwise, or multiplies by a fixed function E ? R.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On a routing problem</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Bellman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quarterly of applied mathematics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="87" to="90" />
			<date type="published" when="1958">1958</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Dynamic programming</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Bellman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">153</biblScope>
			<biblScope unit="issue">3731</biblScope>
			<biblScope unit="page" from="34" to="37" />
			<date type="published" when="1966">1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Size-invariant graph representations for graph classification extrapolations</title>
		<author>
			<persName><forename type="first">Beatrice</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangze</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Ribeiro</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="837" to="851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Joan</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taco</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petar</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><surname>Veli?kovi?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.13478</idno>
		<title level="m">Geometric deep learning: Grids, groups, graphs, geodesics, and gauges</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Combinatorial optimization and reasoning with graph neural networks</title>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Cappart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Didier</forename><surname>Ch?telat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elias</forename><surname>Khalil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Lodi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.09544</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Natural graph networks</title>
		<author>
			<persName><surname>Pim De Haan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Taco S Cohen</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3636" to="3646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Categories, relations and dynamic programming</title>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="33" to="69" />
			<date type="published" when="1994">1994</date>
			<publisher>Oege De Moor</publisher>
		</imprint>
	</monogr>
	<note>Mathematical Structures in Computer</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neural algorithmic reasoners are implicit planners</title>
		<author>
			<persName><forename type="first">Andreea-Ioana</forename><surname>Deac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ognjen</forename><surname>Milinkovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Luc</forename><surname>Bacon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mladen</forename><surname>Nikolic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cohomology and massless fields</title>
		<author>
			<persName><forename type="first">Roger</forename><surname>Michael G Eastwood</surname></persName>
		</author>
		<author>
			<persName><surname>Penrose</surname></persName>
		</author>
		<author>
			<persName><surname>Wells</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. Math. Phys</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="305" to="351" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">An invitation to applied category theory: seven sketches in compositionality</title>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">I</forename><surname>Spivak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neural shuffle-exchange networks-sequence processing in o</title>
		<author>
			<persName><forename type="first">Karlis</forename><surname>Freivalds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
	<note>Em?ls Ozolin ??, and Agris ?ostaks</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural tangent kernel: Convergence and generalization in neural networks</title>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Jacot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franck</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cl?ment</forename><surname>Hongler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">How does a neural network&apos;s architecture impact its robustness to noisy labels?</title>
		<author>
			<persName><forename type="first">Jingling</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mozhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Dickerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Solving mixed integer programs using neural networks</title>
		<author>
			<persName><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Gimeno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pawel</forename><surname>Ingrid Von Glehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Lichocki</surname></persName>
		</author>
		<author>
			<persName><surname>Lobov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Donoghue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Sonnerat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengming</forename><surname>Tjandraatmadja</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.13349</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Normalized attention without probability cage</title>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Wattenhofer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.09561</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning to simulate complex physics with graph networks</title>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Godwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Pfaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8459" to="8468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Heiko</forename><surname>Strathmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammadamin</forename><surname>Barekatain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.01043</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">Persistent message passing. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Towards scale-invariant graphrelated problem solving by iterative homogeneous gnns</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayuan</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bao-Liang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="15811" to="15822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural algorithmic reasoning. Patterns</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">100273</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Neural execution of graph algorithms</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matilde</forename><surname>Padovano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10593</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Razvan Pascanu, Oriol Vinyals, and Charles Blundell. Pointer graph networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Buesing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Overlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="2232" to="2244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matko</forename><surname>Bo?njak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Lerchner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.08881</idno>
		<title level="m">Raia Hadsell, Razvan Pascanu, and Charles Blundell. Reasoning-modulated representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Comprehending monads</title>
		<author>
			<persName><forename type="first">Philip</forename><surname>Wadler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1990 ACM Conference on LISP and Functional Programming</title>
		<meeting>the 1990 ACM Conference on LISP and Functional Programming</meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="61" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Integral transforms and the pull-push perspective, i. The n-Category Caf?</title>
		<author>
			<persName><forename type="first">Simon</forename><surname>Willerton</surname></persName>
		</author>
		<ptr target="https://golem.ph.utexas.edu/category/2010/11/integral_transforms_and_pullpu.html" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">How to transfer algorithmic reasoning knowledge to learn new algorithms?</title>
		<author>
			<persName><forename type="first">Louis-Pascal</forename><surname>Xhonneux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreea-Ioana</forename><surname>Deac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<title level="m">How powerful are graph neural networks? arXiv preprint</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingling</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mozhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.13211</idno>
		<title level="m">What can neural networks reason about? arXiv preprint</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
