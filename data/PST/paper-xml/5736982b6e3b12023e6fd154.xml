<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MORC: A Manycore-Oriented Compressed Cache</title>
				<funder ref="#_9vPXDTs">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_XySQvaX">
					<orgName type="full">AFOSR</orgName>
				</funder>
				<funder ref="#_7sFGBxr #_tJJw5Fy">
					<orgName type="full">DARPA</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tri</forename><forename type="middle">M</forename><surname>Nguyen</surname></persName>
							<email>trin@princeton.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Wentzlaff</surname></persName>
							<email>wentzlaf@princeton.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MORC: A Manycore-Oriented Compressed Cache</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>caches</term>
					<term>compression</term>
					<term>manycore</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Cache compression has largely focused on improving single-stream application performance. In contrast, this work proposes utilizing cache compression to improve application throughput for manycore processors while potentially harming single-stream performance. The growing interest in throughput-oriented manycore architectures and widening disparity between on-chip resources and off-chip bandwidth motivate re-evaluation of utilizing costly compression to conserve off-chip memory bandwidth. This work proposes MORC, a Manycore ORiented Compressed Cache architecture that compresses hundreds of cache lines together to maximize compression ratio. By looking across cache lines, MORC is able to achieve compression ratios beyond compression schemes which only compress within a single cache line. MORC utilizes a novel log-based cache organization which selects cache lines that are filled into the cache close in time as candidates to compress together. The proposed design not only compresses cache data, but also cache tags together to further save storage. Future manycore processors will likely have reduced cache sizes and less bandwidth per core than current multicore processors. We evaluate MORC on such future manycore processors utilizing the SPEC2006 benchmark suite. We find that MORC offers 37% more throughput than uncompressed caches and 17% more throughput than the next best cache compression scheme, while simultaneously reducing 17% of memory system energy compared to uncompressed caches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Throughput-oriented computing is becoming increasingly important <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b2">2]</ref>. Emerging examples of throughput computing occur in the enterprise (batch record processing, network processing), large data centers (bulk data serving, web-crawling, Map-Reduce applications, image and video transformation), scientific computing (Monte Carlo simulations), and at home (graphics processing in games, virus scanning). Generally, these workloads are latency tolerant, a characteristic often leveraged for better power efficiency. To sustain the growth of these applications, manycore architectures <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr">9]</ref> prioritize energy efficiency and throughput over single-stream performance.</p><p>A major challenge with future manycore architectures is the need for additional off-chip memory bandwidth to feed the increasing number of threads and cores on a single chip. Although Moore's Law <ref type="bibr" target="#b10">[10]</ref> is slowing, which makes including additional cores or computation on a single chip more difficult, the number of pins and the pin frequency that those pins can be toggled improve at an even slower rate, leaving a large and growing disparity in terms of bandwidth per core. This gap is known as the bandwidth-wall portion of the memory-wall <ref type="bibr" target="#b11">[11]</ref>. Recent trends have begun to explore stacked DRAM architectures as a means to increase bandwidth. Unfortunately, stacked DRAM likely only provides a one-time increase in bandwidth and will be unable to keep pace with transistor scaling. Due to the Operation Energy Scale 64b comparison (65nm) <ref type="bibr" target="#b12">[12]</ref> 2pJ 1x 64b access 128KB SRAM (32nm) <ref type="bibr" target="#b13">[13]</ref> 4pJ 2x 64b floating point op (45nm) <ref type="bibr" target="#b14">[14]</ref> 45pJ 22.5x 64b transfer across 15mm on-chip <ref type="bibr" target="#b15">[15]</ref> 375pJ 185x 64b transfer across main-board <ref type="bibr" target="#b16">[16]</ref> 2.5nJ 1250x 64b access to DDR3 <ref type="bibr" target="#b17">[17]</ref> 9.35nJ 4675x</p><p>Table <ref type="table">1</ref>: Energy of on-chip and off-chip operations on 64b of data. The energy consumption of off-chip data loads is almost a thousand times more expensive than on-chip access, which motivates localizing data on-chip as much as possible.</p><p>extreme bandwidth-starved nature of current and future throughput-oriented processors, any reduction in bandwidth can be applied directly to increasing throughput.</p><p>A secondary challenge that throughput-oriented processors need to tackle is the large energy cost associated with accessing off-chip memory. Table <ref type="table">1</ref> shows the energy cost of accessing or operating on 64-bit words. Thousands of on-chip operations can occur for the cost of a single memory access. This fact strongly motivates techniques which can reduce off-chip memory accesses by using inexpensive on-chip computation.</p><p>In this work, we explore extreme cache compression as a means to increase throughput. To increase compression ratio, even at the expense of single-stream performance, we use expensive compression techniques along with inter-line compression. Prior work in cache compression <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b23">23]</ref> has primarily focused on intra-line compression to minimize decompression latency. Even the recent work <ref type="bibr" target="#b24">[24]</ref> which does compress across cache lines, is optimized for single-stream performance.</p><p>In contrast, this work explores cache compression solely to increase throughput, tolerating extremely long cache load latencies. We introduce MORC, a new log-based, Manycore ORiented, compressed Last-Level Cache (LLC) architecture that enables content-aware placement to compress similar cache lines together, previously impractical with commodity set-based caches. By utilizing logs, MORC compresses large numbers of cache lines together into a single active log, maximizing compression ratio while still providing a cache line look-up mechanism. The downside of higher compression ratio is that average access latency can increase due to long decompression time, especially when the required data is at the end of the log.</p><p>Figure <ref type="figure" target="#fig_1">1</ref> illustrates how data is filled into a log-based cache versus a set-based cache. In a log-based cache, data is located based on data commonality patterns, regardless of addresses; therefore, in Figure <ref type="figure" target="#fig_1">1</ref>, addresses 4 and 5 which have similar data can be compressed together into the same log even though their addresses would indicate different cache indices. MORC not only uses logs to store compressed data, but also compresses tags together using base-delta compression. Through tag compression, MORC effectively overcomes tag overhead which can dominate storage in traditional compressed caches.</p><p>To further increase inter-line compression performance,  MORC leverages an important insight to achieve compression ratios far beyond prior work: compressing larger blocks of data (e.g., at page-size) is likely to yield higher compression ratio than with smaller blocks (e.g., at cache block size). MORC takes this concept one step further and compresses similar cache lines regardless of physical address, even across memory pages. To show this potential, Figure <ref type="figure">2</ref> shows the compression ratio and bandwidth difference between an ideal intra-line (compression within a cache line) compression scheme, and an ideal inter-line (compression across cache lines) scheme. <ref type="foot" target="#foot_0">1</ref> . On average, intra-line compression achieves only 2x compression, reducing bandwidth usage by 20%, whereas inter-line achieves a staggering 24x compression ratio and almost 80% bandwidth reduction. In the bandwidth-exhausted regime, an 80% decrease in bandwidth usage can increase throughput up to 5 times. These results echo another study <ref type="bibr" target="#b25">[25]</ref> on the limits of cache compression where caches can be ideally compressed up to 64x.</p><p>We evaluate MORC with the SPEC2006 benchmarks on a manycore processor with 128KB of cache per core and limited memory bandwidth per core which is likely representative of future manycore designs. We find that MORC's inter-line compression averages a 3x compression ratio, 50% more than the next-best design's 2x compression ratio. The increased effective cache size en-ables 17% more throughput than the next best scheme, at the same time MORC decreases memory system energy 17% over a baseline uncompressed cache. In the rest of this paper, we describe the MORC architecture in detail, compare MORC to three previous best-ofbreed cache compression schemes, and explore design trade-offs in MORC. This paper provides the following contributions:</p><p>? The design of a novel log-based, inter-line compression architecture which optimizes for throughputoriented systems by favoring compression ratio over decompression latency. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">BACKGROUND</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Log-based cache</head><p>MORC utilizes log-based caches, in contrast to setbased caches prevalent in modern computers. Log-based caches are loosely based on the concept of log-based data structures widely used in software, where each new data item or modification is always appended to a log, preserving data integrity of previous entries. Appending data is one of the two operations allowed with logs; the other operation is flushing to reclaim cache space. Otherwise, logs and sets share many similarities, including that both data and tags are contained in an entry, tag checks can be done in parallel with data access, and that data is stored in conventional, indexible SRAMs.</p><p>Because logs do not support in-place modifications, they are often of limited use for uncompressed caches, as frequent write-backs can quickly saturate logs with outdated and invalid values. Even so, logs that contain only invalidated cache lines can be reused as if they are clean logs without needing to do a flush first.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Data compression in log-based caches</head><p>Preserving the data stream is necessary to enable modern stream-based compression algorithms. LZ <ref type="bibr" target="#b26">[26]</ref> and gzip <ref type="bibr" target="#b27">[27]</ref> compress data by using pointers to reference data which exists earlier in the uncompressed data stream (log). Thus, if cache writes were allowed to modify the stream (log) in-place, subsequent cache lines that reference the modified data will be corrupted and impossible to reconstruct. For the same reason, log-based compressed caches cannot have exact pointers to individual cache lines; decompression needs to start at the beginning of the stream (log) to build the correct data stream and dictionary. Unfortunately, this also means   the average decompression latency would be longer for a log-based compressed cache than it would be in setbased caches. This is especially true when the required data is at the end of the stream. Stream-based compression is natural and efficient for log-based caches. Compressing and appending a cache line simply requires that there is available space. Conversely, stream-based compression is complicated and costlier with set-based caches. For example, replacing an existing cache line likely means invalidating the subsequent compressed cache lines that reference the modified line, and/or re-compressing them.</p><p>While long access latency can harm single-stream performance, it can be a good trade-off in terms of energy (preventing expensive off-chip memory access) and throughput. Moreover, latency-hiding techniques, such as multi-threading in current manycore architectures, can be used to hide most of the extra access latency. Note that while these techniques can hide latencies, fundamentally they cannot solve the bandwidth-wall <ref type="bibr" target="#b11">[11]</ref>.</p><p>Additionally, log-based compressed caches sidestep two important problems with prior art: internal and external fragmentation. First, internal fragmentation is significant in set-based compressed caches, reducing the physical storage available to compress into. This is because small, fixed-size blocks of storage (segments) are used to support in-place modification. When compressed data is smaller than the allocated segments, the left-over storage cannot be used and can be up to 12.5% of storage for some designs <ref type="bibr" target="#b18">[18]</ref>. Second, frequent defragmentation of segments is needed in a set-based compressed cache, unless the data-store is fully decoupled from the tag-store <ref type="bibr" target="#b19">[19]</ref>. Defragmentation can be an energy-intensive and time consuming process with prior-work showing that it can increase LLC energy by almost 200%. <ref type="bibr" target="#b19">[19]</ref>. Since log-based caches do not use segments, neither problem exists.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">ARCHITECTURE</head><p>Instead of sets, MORC has fixed-size logs into which cache lines are compressed and appended incrementally. In-place modifications are not allowed and write-backs are always appended. Cache lines are filled into logs in temporal order, therefore, a line-map table (LMT) is  needed as an indirection layer to locate data by address.</p><p>Cache addresses are indexed into the LMT, then redirected to the log that the data can be compressed into (for a fill) or can be found in (for a read/write request).</p><p>In this respect, a MORC cache is similar to indirect caches <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b20">20]</ref> except that the LMT is over-provisioned in order to track additional lines resulting from compression. Figure <ref type="figure" target="#fig_5">3</ref> shows a logical view of MORC with the LMT and log entries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">MORC Operations</head><p>To grasp the difference between MORC and set-based caches, we present a detailed description of read, fill, write-back, and eviction operations. We assume a noninclusive cache design where write misses fill private caches first before eventually being evicted and written back to the LLC. Section 5.4.2 explains the rationale. Read request (Figure <ref type="figure" target="#fig_5">3</ref>) first checks the LMT. If the LMT entry is invalid, the cache line is guaranteed to not be in the cache, and the request is forwarded to memory. If valid, MORC still needs to decompress the tags and complete a tag-check (Figure <ref type="figure" target="#fig_7">4</ref>) to ascertain hit or miss. We consider the case where the LMT entry is valid but tag-check indicates a cache-miss to be an "LMT aliased-miss." Once the tag-check verifies validity, MORC decompresses the indicated log only up to the needed data (known from tag location). Fill request from memory is a result of a previous LLC miss. First, MORC allocates an LMT entry for the new cache line, and progresses to choose the best active log. The data is then compressed and appended to the log (as shown in Figure <ref type="figure" target="#fig_8">5</ref>) and the LMT state bits and log index are set accordingly. If the allocated LMT entry is already valid before the fill, then an LMT-conflict occurs and an LMT entry eviction needs to be done first. When a log becomes full, MORC selects a victim log to flush and reclaims space. Write-backs from private caches are also appended to logs similarly to fills from memory. In a non-inclusive cache, the LLC is not required to contain the cache line at write-back time, so an LMT entry is allocated if necessary. If it is already present, MORC changes the The active log register holds the index of the log to be filled with data.</p><p>state to "Modified", and updates the LMT's log-index if the write-back is compressed into a different log.</p><p>Evictions have two forms in MORC: LMT-conflict evictions and a whole log eviction. LMT-conflict eviction happens when a cache fill is allocated to an already valid LMT entry. The tag entry for that cache line is marked invalid, and the data, if modified, needs to be decompressed and sent to memory.</p><p>A whole log eviction sequentially decompresses all entries in the log in order to reclaim space for a new active log. For each valid tag, MORC checks the state of the corresponding LMT entry: if modified, the decompressor decompresses data and writes it back to memory. In both cases, the LMT entry is marked invalid and available for future use. Note that a log flush only happens when a log is evicted ; reused logs don't need to be flushed since all contained lines are invalid. Log evictions are infrequent and off of the critical path since the fill is forwarded to the processor core in parallel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">MORC Design Features</head><p>MORC has five distinct characteristics: (1) log-based storage efficiently enables dictionary-based stream compression algorithms, (2) Line-Map Table (LMT) enables flexible mapping of cache line to log, (3) multiple active logs enable content-aware compression for maximum compression performance, (4) tag compression reduces tag overheads, and (5) Large-Block Encoding (LBE) as a new compression algorithm optimizes inter-line compression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Data storage</head><p>MORC's data storage is divided into multiple fixedsize log entries. Each log, typically limited to 512-bytes each to avoid high decompression latency, holds a variable number of cache lines, depending on the compressibility of the data in that particular log. Logs not only hold compressed data, but also allocate fixed storage for the compressed tags.</p><p>A MORC cache contains many logs, but only marks a subset as active; an active log is simply a log that is not full and can be appended to. Simple MORC implementations can have one active log, while more complex implementations can have multiple active logs, dynamically selected at compression time to maximize compression ratio. When a log is full, MORC closes it and chooses a new log. A victim log is chosen using any typical cache replacement policy, but this work studies FIFO for simplicity. Reusing closed logs with all cache lines invalidated (when the workload frequently writes back the same cache lines causing old copies to be invalidated) is given priority over choosing the next FIFO log. Figure <ref type="figure" target="#fig_8">5</ref> shows data being added to a MORC cache. Here, if the written data does not fit into the active log, the replacement policy selects a victim log to flush, reclaim, and compress new data into.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Line Map Table</head><p>The Line-map Table (LMT), shown in Figure <ref type="figure" target="#fig_5">3</ref> is an indirection layer that redirects memory requests to the respective (compressed) data and tag stores. The LMT is over-sized to track all of the cache lines at the maximum compression ratio.</p><p>As Figure <ref type="figure" target="#fig_5">3</ref> shows, an LMT entry contains two small fields: state bits and log index bits. The log index indicates which log the cache line has been compressed into. The entry does not need to store the tag but rather only the mentioned short log index, which points to the compressed tag-store where MORC decompresses and completes a tag-check. The state bits indicate whether the entry is invalid or valid, and if the cache line is modified. Checking for LMT entry validity substantially reduces average cache miss latency as MORC does not need to decompress the tags to resolve the miss.</p><p>Unlike conventional cache sets, logs and data in logs are not directly related to the LMT indexing. As a result, LMT index bits cannot be removed from the actual tag. Other indirect caches have the same needs <ref type="bibr" target="#b28">[28]</ref>. Though Figure <ref type="figure" target="#fig_5">3</ref> shows a direct-mapped LMT, it can also be arranged to be set-associative. In a set-associative LMT, MORC needs to decompress and check tags in all logs that valid LMT entries point to. Experimental results show that a 2-way set-associative LMT reduces LMT-induced evictions from almost 20% to less than 5% compared to a fully-associated LMT. Hash-rehash or column-associated techniques <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b30">30]</ref> can efficiently implement a 2-way LMT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Multi-log and Content-Aware Compression</head><p>In order to improve compression performance, MORC can have multiple active logs to choose from when appending data. A higher compression ratio is achieved by choosing the most beneficial log to compress into. Intuitively, by having multiple logs instead of one, MORC increases the search space for data commonality, as opposed to a single log where data is strictly written sequentially. We loosely categorize this optimization as content-aware compression. Different compression algorithms could even be used for different data types which are stored in data-type specific logs. For instance, some logs can be dedicated to compress floating-point data using specialized compression algorithms <ref type="bibr" target="#b31">[31]</ref>.</p><p>To exhaustively extract performance out of multi-log compression, the inserting cache line is compressed to all active logs, but only the most fruitful log commits its dictionary state changes. Simpler heuristics could be used to save energy, but since compression energy is minuscule compared to, for example, MORC's decompression energy, we elected not to explore this direction.</p><p>One challenge with optimizing performance for multilog is determining when to diversify distinct data across the active logs, as the policy of always choosing the best log could force the system to only use one log for most of the time. We found that a good diversifying algorithm is to insert a fudge factor, 5% for instance, to the scoring system such that when the best and the worst log compression sizes are within 5%, the cache line is seeded to the least-used log.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Tag Compression</head><p>Tag overhead is a significant limitation of prior compressed caches. Over-allocating tags to support the maximum compression ratio can incur a significant overheads. For instance, assuming 40b tags, 8x allocation inflates the tag-store to 62.4% the size of the data-store, and for 16x to 124.8%. Clearly, extreme compression does not scale well with uncompressed tags.</p><p>To alleviate these overheads, we introduce tag compression. Because MORC appends cache lines in temporal order, tags are usually similar and hence highly compressible. We evaluated various compression algorithms, including LZ-based algorithms, and found that base-delta encoding (e.g., <ref type="bibr" target="#b21">[21]</ref>) compresses tags best. In our implementation, tags are encoded as deltas to their immediate predecessor, using a scheme similar to DE-FLATE's distance encoding <ref type="bibr" target="#b32">[32]</ref>, replicated in Table <ref type="table" target="#tab_2">2</ref>. Some modifications to the encoding include (a) one bit to indicate positive or negative deltas, (b) one bit to indicate validity, and (c) code 30-31 is used to indicate a new base when the difference between tags is larger than 2MB. One more base selection bit is added for the multi-base variation where two bases are tracked instead of just one to increase compression performance.</p><p>MORC is designed such that the tags and data can be accessed serially or in parallel. As base-delta compression is fast, and has been implemented to decompress 64Bytes in one cycle <ref type="bibr" target="#b21">[21]</ref>, we assume that the implemented tag compression can also decode up to 8 tags per cycle. As such, we have chosen in our results to access tags and then data sequentially to save energy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.5">Large-Block Encoding</head><p>Though MORC is flexible enough to be used with existing compression algorithms like C-Pack <ref type="bibr" target="#b23">[23]</ref> or LZ, these compression engines are not optimal. With C-  Pack, its constant pointer overhead (4-bits) per data word (32-bits) limits compression ratio to 8x. Increasing the dictionary size also increases the pointer overhead. For example, with a 512-byte dictionary the maximum compression ratio is reduced to 4.57x. In contrast, while LZ compresses general data well, its algorithm is hard to implement in hardware as commercial implementations can only encode and decode at 4byte/cycle <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b34">34]</ref>.</p><p>We identify that the performance difference between LZ and C-Pack is that an LZ pointer can represent a variable and larger data block than C-Pack's 32-bit blocks. Motivated by this observation, we devise a new algorithm, called Large-Block Encoding (LBE), to efficiently compress multiple cache lines. Unlike LZ where arbitrary block length can be compressed to maximize compression, LBE compresses on aligned boundaries and limits compression to a fixed set of granularities: 32-bits, 64-bits, 128-bits, and 256-bits. Each of the different data sizes has its own logical dictionary, but only the 32-bit dictionary contains data, with the larger sizes pointing to entries in the 32-bit dictionary. Having this course-grain alignment restriction meshes well with data objects which also occur on coarse-grain alignment. Compression algorithm: Logically, LBE reads input in 256-bit chunks. Compared to C-Pack which calculates eight hashes for the eight 32b chunks, LBE computes 7 additional hashes: four 64b, two 128b, and one 256b. LBE first finds matches in the 256b dictionary for the 256b hash. If not matched, it does the same for the 128b, then 64b, then 32b hashes. If there is a match in a dictionary at any point, LBE outputs the appropriate prefix (m32, m64, m128, or m256, as shown in Table <ref type="table" target="#tab_4">3</ref>) and a pointer to that entry. z32 is preferred to m32 for all zero data and has no associated pointer. Otherwise, LBE outputs u32 (incompressible) followed by 32b of data, and makes a new 32b dictionary entry. Either u8 or u16 is used instead of u32 when LBE can truncate the upper 24 or 16 zero bits.</p><p>Finally, before compressing the next 256b chunk, LBE allocates dictionary entries for any of the 64/128/256b chunks that failed to compress. Like a binary tree, each of these entries contains two pointers to dictionary entries one size smaller than itself. For instance, a 64b entry will point to two 32b entries, a 128b to two 64b entries, and a 256b to two 128b entries. During the decompression process, to decode an m256 for example, LBE traverses down the binary tree to retrieve data stored in the 32b dictionary entries. Implementation: We expect LBE to be nearly as simple and power efficient as C-Pack primarily because (a) its 32-bit dictionary is managed similarly (the dictionary is frozen when it is full), (b) there are only 7 additional hashes to compare for each 256b chunk (less than a factor of two), and (c) LBE encodes at the same 2 symbols/cycle rate. In fact, LBE is simpler because it does not try to compress individual 32-bits words besides simple upper zeros truncation. The addition of logical dictionaries is also likely to be energy efficient as matches are done using small hashes. LBE compares small hashes and then verifies that the data matches in the next pipe stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.6">Tag-Data Storage Co-location</head><p>An optimization to further reduce overheads is to completely remove the extra tag storage and co-locate tags with data. When more tag storage is needed, the extra tags overflow to the data log from the right. The behavior of data growing from the left and tags from the right is similar to how the heap grows from the bottom and the stack from the top.</p><p>We call this configuration "MORCMerged ". The main drawback is that storage for compressed cache lines is reduced, leading to reduction in compression performance, although in cases where both tags and data have high compression ratio, co-locating both could produce a more efficient use of storage. Section 5.4.5 in the results shows that merging only decreases compression ratio slightly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Overhead Analysis</head><p>Like prior compression schemes, there are three overheads associated with MORC: tag over-provisioning, compression metadata (the LMT), and compression engines. Assuming a 128KB cache, a 48-bit physical address space, 16-way sets (for prior-work), and 512B logs (for MORC), Table <ref type="table" target="#tab_5">4</ref> compares tags and metadata overheads. The table shows MORC with enough LMT entries to hold 8x compressed data. The last column shows MORCMerged, described in Section 3.2.6, with the same LMT overheads but with no extra tags. Overall, the area overheads of MORCMerged is 17.18%, much less than the next-best prior work (SC2).</p><p>The area and energy overhead of compression engines can be a non-trivial design point. Because LBE is based on C-Pack, we can estimate its area based on the synthesis results presented by Chen <ref type="bibr" target="#b23">[23]</ref>. In the experiments, we sized LBE's dictionary to be 512-bytes. Estimated in 32nm, C-Pack compressor and decompressor are both .01mm 2 in size. We conservatively increase the size eightfold to .08mm 2 . For reference, a 16-way 256KB cache in 32nm is 2.12mm 2 <ref type="bibr" target="#b13">[13]</ref>.</p><p>Another metric to measure area overhead is dictionary size. C-Pack in Adaptive and Decoupled requires    <ref type="bibr" target="#b24">[24]</ref>. As MORC allocates 512-bytes for each compression and decompression engine, the basic single-log implementation needs 1024bytes of dictionary storage. Naive multi-log implementations with 8 active logs contain 8 compression engines and 1 decompression engine, thus increasing the area overheads to .72mm 2 and dictionary size to 4608B, but we assume that timedivision multiplexing can be applied to share one compression engine with multiple active logs, reducing the requirements to .08mm 2 and 1024B respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">METHODOLOGY</head><p>We evaluate our design using the SPEC2006 benchmark suite and PriME <ref type="bibr" target="#b35">[35]</ref> simulator. PriME was augmented to capture data values in the caches to measure compression performance. The default system configuration is shown in Table <ref type="table" target="#tab_7">5</ref>. SPEC2006 benchmarks are run with as many reference inputs as possible; the additional inputs are indicated by an underscore and number.</p><p>We run a set of publicly available <ref type="bibr" target="#b36">[36]</ref> profiled representative regions <ref type="bibr" target="#b37">[37]</ref> for shorter simulations and higher accuracy. Single-program workloads (Section 5.1) use the 130M instruction trace set (100M warm-up, 30M actual instructions), while multi-program (Section 5.2) use 1 billion instruction regions where applications run the full 1B instructions but are only profiled for the first 250M instructions. These multi-program workloads are evaluated with two sets of workloads described in Table 6: (a) 4 sets of "mixed" applications, randomly chosen, and (b) 8 sets of replicated, "same" application.</p><p>Table <ref type="table" target="#tab_9">7</ref> shows the energy model for a 32nm process; access energy numbers are per cache line. For SRAM and caches we use the 32nm models from CACTI <ref type="bibr" target="#b13">[13]</ref> L1  <ref type="bibr" target="#b17">[17]</ref>, assuming a quad-channel, 4-rank per channel, 9 chips per rank DIMM. For LBE, we scale up C-Pack's energy to take into account the increased dictionary size from 64-bytes to 512-bytes and 16B/cycle. Compression ratios are sampled every 10M instructions as the ratio of valid cache lines over uncompressed cache capacity. Bandwidth usage is miss per kilo instruction (MPKI) scaled to 1B instructions. As IPC is mainly a metric of single-thread performance and not of throughput workloads where some latency to memory can be tolerated, we also estimate throughput using a four-thread, coarse-grain, multi-threading model. In this model, one thread is executed continuously until an L1 cache miss, at which point the next thread is swapped in. If the cache miss is serviced before the thread is swapped in again, the cache miss latency is considered hidden and there is no loss in throughput. Contrarily, if the cache miss needs to be serviced from memory which may take thousands of cycles, and other threads are also taking cache misses, then the core is forced to stall. We estimate the latency-tolerance of the workloads by measuring the average number of cycles between L1 misses, then subtract it from the compressed LLC access latency to calculate the core's nonstalling throughput. This latency is workload-dependent: lower for memory-intensive benchmarks and highest for compute-bound workloads.</p><p>In addition to uncompressed caches, we also compare to Adaptive <ref type="bibr" target="#b18">[18]</ref>, Decoupled <ref type="bibr" target="#b19">[19]</ref>, and SC2 <ref type="bibr" target="#b24">[24]</ref> compressed caches. These schemes are evaluated with perfect LRU replacement policy. For fairness, both Adaptive and Decoupled were evaluated with C-Pack even though the original Adaptive design used FPC <ref type="bibr" target="#b38">[38]</ref>. Baseline LLC load latency is 14 cycles; Adaptive, Decoupled, and SC2 all add 4 extra cycles for decompression; and MORC's variable decompression latency is 16 output bytes per cycle. Compression energies are estimated as in Table <ref type="table" target="#tab_9">7</ref>.</p><p>The evaluated MORC is allocated with 2x storage for tag-store, and enough LMT entries for a maximum compression ratio of 8x. The LMT is arranged in columnassociated style <ref type="bibr" target="#b29">[29]</ref> to emulate 2-way associativity. By default, MORC uses 512-byte logs, LBE for compression algorithm, 8 active logs for multi-log compression, and tags compression with 2 bases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">RESULTS</head><p>Our evaluation has two parts. In the first part, we evaluate MORC and other compression schemes on the basis of compressibility, bandwidth savings, IPC, and throughput improvements. We report these results for single-program in Section 5.1, and multi-program in  Section 5.2. The second part studies the energy efficiency of inter-line compression (Section 5.3), and explores MORC's design space (Section 5.4).</p><p>The results show that MORC, by virtue of inter-line compression, is much better at data compression than prior art. As a result, MORC suffers less cache misses, saves more bandwidth, and extracts more throughput from less off-chip communication. Energy-wise, MORC consumes less than other schemes because of fewer accesses to memory, which outweighs the additional decompression energy from inter-line compression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Single-program results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Compression performance</head><p>MORC outperforms prior art in compression ratio. In Figure <ref type="figure" target="#fig_10">6a</ref>, the average effective cache sizes of MORC is 2.9x, with some workloads compressed close to 6x (astar, gcc, omnet, soplex, zeusmp) and many more close to or beyond 3x. Compared to the averages of Adaptive, Decoupled, and SC2 at 1.5x, 1.8x, 1.9x respectively, MORC consistently compresses the workload better.</p><p>Scrutinizing the compression results, we make two observations. First, in workloads with abundant zeros like zeusmp and gcc which should be compressible even with intra-line techniques, prior art runs out of tags. Both Adaptive (max 2x) and Decoupled/SC2 (both max 4x) run into this issue. In contrast, MORC perceives almost no tag restrictions thanks to tags compression.</p><p>Second, we observe that there exists an abundant amount of non-zero data duplication across cache lines, of which MORC exploits to achieve a big leap in compression performance. To support this observation, we profile and plot the usage distribution of LBE's encoding symbols in Figure <ref type="figure" target="#fig_11">7</ref>. To simplify the chart, the mX portion of the left bars include both actual mX and zX symbols. Notably for some workloads, non-zero m256 usage is significant; some examples include cactusADM, gamess, leslie3d, and povray, where the left m256 bar is distinctively higher than the right m256 bar. By  <ref type="table" target="#tab_4">3</ref>. The left columns show the total usage, while the right ones depict the portions that are all zeros. Distribution is weighted to data-size each encoding represents.</p><p>contrast, workloads like gcc are composed mostly of zeros. In both cases, MORC achieves much better compression ratios as an m256 symbol represents 8 times the amount of data as an m32 for the same space usage. Other symbols like m64 and m128 are also useful for workloads where data duplications occur at smaller granularities, like mcf, omnet, and perlbench. As Section 3.2.5 argues, big symbols like m64, m128, m256 are needed to achieve high data compression ratio. Lastly, a subset of benchmarks, especially h264ref, benefits from significance-based compression (u8 /u16 where upper zero bits are truncated).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Bandwidth &amp; Throughput</head><p>Figure <ref type="figure" target="#fig_10">6b</ref> shows the bandwidth usage of the evaluated compressed caches. Note that the bandwidth savings originate solely from larger effective cache sizes; none of the schemes compresses the memory channel. By average-means, MORC reduces off-chip bandwidth by 27.0%, while the next best, SC2, only by 10.8%. While not always true, higher effective cache sizes strongly correlate with higher bandwidth reductions. Some programslike astar, gcc, and perlbench-exhibit more pronounced impacts than others-like dealII and zeusmp. The differences in working set size among different benchmarks explain this behavior: those that fit after compression use much less bandwidth than those that do not. This is prominent for FP workloads which have huge working sets. For example, a characterization study of SPEC2006 <ref type="bibr" target="#b39">[39]</ref> shows that the LLC miss-rate of cactu-sADM is the same for caches between 128KB and 2MB in size. Since the baseline cache is 128KB in size, cache compression ratios from 1x to 15x will all have the same miss-rate to memory.</p><p>Figure <ref type="figure" target="#fig_10">6c</ref> shows the IPC improvements resulting from bandwidth reduction under a 100MB/s per-core bandwidth cap. On average, MORC achieves a 22% IPC gain over the baseline, edging out SC2's 20% even when MORC is not optimized for single-stream performance. When applying multi-threading to cache compression to hide load latency of the LLC, throughput differences between MORC and other schemes increase even further, to 37% for MORC and 20% for SC2 as seen with Figure <ref type="figure" target="#fig_10">6d</ref>. Multi-threading's effectiveness with MORC ranges from no improvements (eg. bzip2, mcf, omnet), to moderate (eg. astar 1, from 42% to 49%), to sig-   <ref type="table" target="#tab_8">6</ref>. All workloads have 16 threads in total.</p><p>nificant (eg. povray, from less than 60% to over 200%). Multi-threading benefits MORC more than other schemes because those are already optimized for IPC and thus cannot take advantage of latency-hiding techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Multi-program</head><p>We simulate multi-program workloads (Table <ref type="table" target="#tab_8">6</ref>) to understand the behavior of a shared LLC when executing a mix of programs. In these simulations, the system has 1600MB/s of sharable bandwidth in total (100MB/s per thread with 16 threads). Consistent with singleprogram results, Figure <ref type="figure" target="#fig_13">8a</ref> shows that MORC has good compression ratio compared to the baselines.</p><p>With almost 4x on average and up to 7x compression ratio, MORC is far superior to the next-best scheme which sees only 1.75x on average. Since MORC extracts commonality across applications, compression performance for the same-benchmark workloads (Sx ) is generally great. The Sx workloads represent systems where like workloads are co-scheduled and grouped to the same machines. However, as evidenced in some workloads (S4, and to some extend S1 ), slight asynchronism in execution phases between threads applies more stress to the compression engines and severely decreases MORC's compression performance. Techniques that identify and synchronize threads at the instruction level <ref type="bibr" target="#b40">[40]</ref> can completely eliminate threads asynchronism and greatly increase compression performance.</p><p>MORC, as well as other compression schemes, performs less well with random mixes of workloads (Mx set). With MORC, compressing data streams from different applications together to the same log (or pool of logs with multi-log) is not as effective as compressing the streams separately. The same situation applies to SC2 where the fix-sized centralized dictionary has to be shared among multiple programs and thus is less effective. Figure <ref type="figure" target="#fig_13">8a</ref> shows neither Decoupled nor SC2 achieving compression not much better than Adaptive.</p><p>Consequentially, in Figure <ref type="figure" target="#fig_13">8b</ref>, off-chip memory bandwidth is greatly reduced with MORC: by 20% on average and up to almost 50%. The magnitude of savings is still subjected to the workloads' working-set sizes, thus workloads with high compression ratios can have little savings (S0 ), while those with low compression ratios can achieve large savings (M3 ). Generally though, MORC compresses multi-program workloads better than prior work and thus achieves better bandwidth savings.</p><p>IPC, in Figures 8c, and completion time, in Figure <ref type="figure" target="#fig_13">8d</ref>, measure different aspects of throughput. The IPC plot measures the geometric mean of IPC across all 16 applications, generally suited to compare the trend of speedups for latency-critical applications. In contrast, the completion time plot measures the run time of the longest running application. This approximates a user using a Hadoop cluster where phases' run time is limited by the tail latency. Short completion time is also significant to performance in GPGPU where the longest running threads limit performance of compute kernels.</p><p>Overall, MORC performs superior to prior-work with the Sx sets and about the same for Mx sets. Measuring IPC, MORC can obtain a performance increase of almost 60% (S5 ), and between 17% to 27% performance increase for workloads in which prior work achieves significantly lower (S1, S3, and S7 ). Especially with the Mx workloads, MORC attains lower-than-expected IPC gains from bandwidth reductions since bandwidth is shared and bandwidth-intensive phases of one application can be overlapped with compute-intensive phases of another application, effectively using bandwidth more efficiently than in single-program simulations. Nevertheless, that just means total system bandwidth is not fully utilized, and throughput can be further increased by running more threads.</p><p>Completion time speedups show a larger advantage for MORC, mainly in Mx workloads where mixed workloads have their tail latency reduced significantly. First, since Figure <ref type="figure" target="#fig_13">8c</ref> plots the unweighted means, and CPUbound applications are negatively affected by longer LLC latency, the positive speedups from memoryintensive workloads are concealed in the unweighted IPCs. The completion time plot in Figure <ref type="figure" target="#fig_13">8d</ref>, however, accentuates performance gain of the most memory-  intensive workloads, and shows MORC completing M3 35% faster, up from 17% for IPC improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Memory Energy Efficiency</head><p>DRAM accesses represent a large portion of both total running time and energy, even after cache compression. Prior work has shown that compressed caches are more efficient than larger uncompressed caches thanks to significantly lower static and dynamic energy power <ref type="bibr" target="#b24">[24]</ref>. Figure <ref type="figure" target="#fig_15">9a</ref> compares the memory subsystem energy, including compression engine (but not CPU core energy) of cache compression schemes (at 128KB) and baselines (at 128KB and 1MB). On average, MORC reduces 17.0% of memory-system energy, more than the other compression schemes and is accomplished by removing more DRAM accesses. Moreover, compared to the 1MB baseline, MORC is more energy efficient due to lower static power of a smaller SRAM.</p><p>Decompression energy is small relative to DRAM access, L1 data array, or LLC energy. Hence, when MORC reduces the number of DRAM accesses, memory system energy goes down proportionately. Figure <ref type="figure" target="#fig_15">9b</ref> normalizes MORC to the baseline with energy broken down; in most cases, the reduction in DRAM accesses (and in some cases static energy due to shorter execution length) outweighs the added energy from compressions.</p><p>Observe that compression is efficient with log-based caches since subsequent compressions are incremental and no cache line compaction is needed. Decompression energy is more substantial in MORC because it needs to decompress from the beginning of the compression stream. Particularly, decompression power is more significant with namd and povray, but is also compensated by substantial reductions in DRAM power. Other workloads (gcc, astar, gobmk ) exhibit noteworthy energy savings, often up to 68%.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Sensitivity Studies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Bandwidth and Cache Size</head><p>The single and multi-program results show that for a future design point (1024 cores with 128KB LLC per core and eight memory channels), MORC can achieve big performance gains. For completeness, we evaluate MORC at other bandwidth availabilities. As expected, MORC's single-stream performance (Figure <ref type="figure" target="#fig_1">10a</ref>) suffers when there is abundant bandwidth-MORC slows the system by almost 7% at 1600MB/s. However, with multi-threading (Figure <ref type="figure" target="#fig_1">10b</ref>), there is no throughput loss; and at extreme bandwidth starvation (12.5MB/s, a possible design point for 2020 <ref type="bibr" target="#b41">[41]</ref>) MORC improves throughput by 63% . In Figure <ref type="figure" target="#fig_1">11</ref>, MORC's bandwidth savings and throughput gains are shown to be strong consistently for different cache sizes from 64KB to 1MB. In this range, bandwidth savings are between 33% and 37%, which translate to throughput improvements from 35% to 46%. Only at 4MB and beyond that most workloads fit in-cache and cache compression is no longer beneficial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Measuring write-back effects on logs</head><p>Frequent writes to the LLC can adversely affect logbased caches as writes append and old space is not reclaimed until the log is flushed. Figure <ref type="figure" target="#fig_17">12</ref> shows that a simple optimization of not inserting the fetched line on first write (non-inclusive) significantly reduces the presence of invalid old data over the inclusive behavior. In the non-inclusive model, only subsequent write-backs are written to the data-store. Additionally, write-back data for most workloads is highly compressible, comparable to compression ratios of valid data in Figure <ref type="figure" target="#fig_10">6a</ref>. Furthermore, reusing logs reduces the frequency that a valid log has to be flushed. Therefore, we were not compelled to devise a fall-back strategy for overwritten data in MORC.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.3">Log-size and number of logs</head><p>Intuitively, larger log sizes (to store longer compression streams) and more active logs (to sort more data patterns) should enable even greater compression performance for MORC. On the contrary, with the limit studies in Figure <ref type="figure" target="#fig_18">13</ref>, assuming unlimited tags and LMT entries, we observe that the configuration of 512-byte logs with 8 active logs (out of total of 512 in 128KB caches) is almost optimal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.4">MORC latency distribution</head><p>Figure <ref type="figure" target="#fig_7">14</ref> explores the variability of decompression latency with MORC. The workloads exhibit fairly even distribution of accesses to both data in the front (lower latency) and in the back of the log (longer latency). In other words, the usefulness of a cache line in the logs is position-independent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.5">Shared data/tag logs</head><p>Figure <ref type="figure" target="#fig_8">15</ref> shows the evaluation of MORCMerged, described in Section 3.2.6, against the default configuration. Overall, MORCMerged sacrifices minimal compression performance, less than 0.5x for most workloads. omnetpp suffers the biggest drop in performance because its memory access pattern is not as compressible as, for example, soplex or zeusmp. Interestingly, even with reduced total storage, co-locating tags and data can use space more efficient and achieve higher compression ratios, as evidenced with workloads like hmmer and lbm. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">RELATED WORK</head><p>Most prior work on LLC compression compresses single cache lines <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b22">22]</ref>. While one recent scheme <ref type="bibr" target="#b24">[24]</ref> compresses inter-line, it still prioritizes single-stream performance over throughput. In contrast, MORC is optimized for throughput. MORC was evaluated against three best-of-breed cache compression schemes: Adaptive <ref type="bibr" target="#b18">[18]</ref>, Decoupled <ref type="bibr" target="#b19">[19]</ref>, and SC2 <ref type="bibr" target="#b24">[24]</ref>.</p><p>Adaptive <ref type="bibr" target="#b18">[18]</ref> uses a basic cache organization with sets and ways. Each set over-provisions tags by 2x for a maximum compression of 2x, and allocates storage for data in segments of 8-bytes. To minimize metadata, segments for a given compressed line are continuous, which necessitates defragmentation when write-backs expand and consume more segments. Decoupled <ref type="bibr" target="#b19">[19]</ref> aims to reduce tag-overheads, increase maximum compression, and eliminate defragmentation by using supertags and pointers to individual segments. Skewed Compressed Cache <ref type="bibr" target="#b42">[42]</ref> has similar performance to Decoupled, but is designed to be easier to implement <ref type="bibr" target="#b42">[42]</ref>. Finally, SC2 <ref type="bibr" target="#b24">[24]</ref> is most similar to MORC because it maintains a system-wide dictionary which can compress data across cache lines. However, as the shared dictionary is limited in size, the amount of data that SC2 can track is limited to the most common words. SC2's architecture, and consequently overheads, is similar to Adaptive's; SC2 cannot scale past 4x maximum compression without incurring expensive tag-overheads. In the evaluation, we observe that while SC2 is better than the other prior-work, it performs worse than MORC due to limited tags and limited frequent data in the dictionary. Lastly, SC2 needs software procedures to adapt the dictionary to new data over time; MORC requires no software.</p><p>Related work to LBE include C-Pack's <ref type="bibr" target="#b23">[23]</ref>, FPC <ref type="bibr" target="#b38">[38]</ref>, and LZ (software <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b26">26]</ref>, hardware <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b43">43]</ref>). LBE is loosely based on C-Pack, but is optimized for inter-line compression. FPC <ref type="bibr" target="#b38">[38]</ref>, strictly an intra-line compression algorithm, performs similarly to C-Pack, in our own evaluations and prior studies <ref type="bibr" target="#b23">[23]</ref>. LZ and its derivatives, are well-known and frequently used in modern software to compress general data. In our (not-shown) studies, we found that LZ, as a direct replacement to LBE, has similar compression performance.</p><p>Memory link compression <ref type="bibr" target="#b44">[44,</ref><ref type="bibr" target="#b45">45]</ref> is complementary to cache compression. MORC does not compress the link and reduces bandwidth demands solely through higher effective cache sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSION</head><p>This work presents MORC, a cache compression scheme that aggressively compresses hundreds of cache lines together to maximize throughput in future bandwidthstarved manycore architectures. Its contributions include a log-based compressed cache architecture, a tag compression technique to lower the tags overheads, a novel data compression algorithm, and a content-aware compression technique. In future systems where throughput and energy efficiency is of greater emphasis than single-stream performance, MORC's bandwidth savings deliver 37% throughput improvement and 17% reduction in memory system energy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Cache line indexing is different between setbased and log-based caches. Log-based caches fill cache lines sequentially, regardless of addresses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Index</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Two examples of a (direct-mapped) line-map table being accessed. Address A is indexed to the first entry of the LMT, and then redirected to log #1. Address B is similarly indexed and found to be invalid and not in the cache.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The decompression (cache access) process. The cache first checks the LMT. If valid, the compressed tags and data are streamed into the decompressor. The first two lines, 1 and 2 , are decompressed first before 3 can be decompressed. Decompression stops after line 3 ; line 4 is not processed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The compression (cache line fill) process. The writing address and data are fed into the compressor. The active log register holds the index of the log to be filled with data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Single-program simulations. Each program is statically allocated 100MB/s of bandwidth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Normalized LBE encodings distribution; encoding symbols are explained in Table3. The left columns show the total usage, while the right ones depict the portions that are all zeros. Distribution is weighted to data-size each encoding represents.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Multi-program simulations. The Mx workloads are mixed programs, and Sx are similar, as listed in Table6. All workloads have 16 threads in total.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Memory subsystem energy. In the breakdowns chart, left columns compare the baseline to MORC's energy usage in the right columns.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 10 :Figure 11 :</head><label>1011</label><figDesc>Figure 10: Normalized performance at other bandwidth availabilities per thread.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>a s t a r b z ip 2 g c c g o b m k h 2 6 4 r e f h m m e r m c f o m n e t p p p e r lb e n c h s je n g x a la n c b m k b w a v e s c a c t uFigure 12 :</head><label>12</label><figDesc>Figure 12: Write-back-induced invalid ratio in MORC. Compression is disabled to accentuate invalidations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: MORC's compression performance across various log sizes and multiple active logs using LBE</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 14 :Figure 15 :</head><label>1415</label><figDesc>Figure 14: Distribution of access latencies with MORC, assuming 16B/cycle output speed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>?</head><label></label><figDesc>Introduction of tag-compression, enabling a larger range of maximum cache compression ratios.? Introduction of a new data compression scheme, Large-Block Encoding (LBE), which provides high compression ratios.</figDesc><table /><note><p>? Introduction of content-aware compression by utilizing multi-log compression architecture. ? An evaluation of MORC versus best-of-breed prior work, measured in terms of off-chip bandwidth, throughput, and memory-subsystem energy.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Distance coding for tags compression</figDesc><table><row><cell cols="3">Code value Distance (64B) Precision bits</cell></row><row><cell>0-3</cell><cell>1-4</cell><cell>0</cell></row><row><cell>4-5</cell><cell>5-8</cell><cell>1</cell></row><row><cell>6-7</cell><cell>9-16</cell><cell>2</cell></row><row><cell>...</cell><cell>...</cell><cell>...</cell></row><row><cell>26-27</cell><cell>8,193-16,384</cell><cell>12</cell></row><row><cell>28-29</cell><cell>16,385-32,768</cell><cell>13</cell></row><row><cell>30-31</cell><cell>New base</cell><cell>0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Compression prefixes for LBE</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Overheads of compressions, normalized to cache capacity</figDesc><table><row><cell>Scheme</cell><cell cols="2">Adaptive Decoupled</cell><cell>SC2</cell><cell cols="2">MORC MORCMerged</cell></row><row><cell>Tags</cell><cell>7.81%</cell><cell>0.00%</cell><cell>23.43%</cell><cell>7.81%</cell><cell>0.00%</cell></row><row><cell>Metadata</cell><cell>10.93%</cell><cell>8.59%</cell><cell>10.15%</cell><cell>17.18%</cell><cell>17.18%</cell></row><row><cell>Tags + Meta</cell><cell>18.74%</cell><cell>8.59%</cell><cell>33.58%</cell><cell>25.00%</cell><cell>17.18%</cell></row><row><cell>Comp. engine</cell><cell>.02mm 2</cell><cell>.02mm 2</cell><cell>NoData</cell><cell>.08mm 2</cell><cell>.08mm 2</cell></row><row><cell>Dict storage</cell><cell>128 Byte</cell><cell cols="3">128 Byte 18 KByte 1024 Byte</cell><cell>1024 Byte</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>System configuration</figDesc><table><row><cell>M0</cell><cell>h264ref 2, soplex, hmmer 1, bzip2, gcc 8, sjeng, perlbench 2, hmmer,</cell></row><row><cell></cell><cell>sphinx3, zeusmp, gobmk 2, perlbench 1, h264ref, dealII, gcc 5, sjeng</cell></row><row><cell cols="2">M1 gobmk 2, gcc 2, astar 1, h264ref 2, gobmk 1, h264ref 1, bzip2 1, gcc 1,</cell></row><row><cell></cell><cell>gobmk 4, bzip2 5, h264ref 2, gcc 4, xalancbmk, astar 1, bzip2 5, bzip2 5</cell></row><row><cell>M2</cell><cell>bzip2 2, perlbench, astar 1, perlbench, bzip2 5, sjeng, omnetpp, gcc 1,</cell></row><row><cell></cell><cell>bzip2, h264ref, gcc, gobmk 4, perlbench 1, omnetpp, omnetpp, gcc 7</cell></row><row><cell>M3</cell><cell>hmmer 1, sjeng, bzip2 2, mcf, gcc 5, bzip2 5, hmmer, gcc 1,</cell></row><row><cell></cell><cell>perlbench 1, gcc 4, hmmer 1, astar 1, astar, astar, gcc 5, h264ref</cell></row><row><cell>S0</cell><cell>bwaves x 16</cell></row><row><cell>S1</cell><cell>bzip2 x 16</cell></row><row><cell>S2</cell><cell>gcc x 16</cell></row><row><cell>S3</cell><cell>h264ref x 16</cell></row><row><cell>S4</cell><cell>hmmer x 16</cell></row><row><cell>S5</cell><cell>perlbench x 16</cell></row><row><cell>S6</cell><cell>sjeng x 16</cell></row><row><cell>S7</cell><cell>soplex x 16</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell>: Multi-program workloads. Each has 16 pro-</cell></row><row><cell>grams, randomly chosen</cell></row><row><cell>64-bytes for compression and decompression each, for</cell></row><row><cell>a total of 128-bytes. SC2 needs 18KB to support the</cell></row><row><cell>Huffman compression flow</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Energy simulation parameters ITRS-LOP. For DRAM energy, we use Micron's power calculator</figDesc><table><row><cell>static power</cell><cell>7.0 mW LLC static power</cell><cell>20.0 mW</cell></row><row><cell>L1 access energy</cell><cell>61.0 pJ LLC data energy</cell><cell>32.0 pJ</cell></row><row><cell>C-Pack compression energy</cell><cell>50.0 pJ LBE compression energy</cell><cell>200 pJ</cell></row><row><cell cols="2">C-Pack decompression energy 37.5 pJ LBE decompression energy</cell><cell>150 pJ</cell></row><row><cell>SC2 compression energy</cell><cell cols="2">144 pJ DRAM static power per core 10.9 mW</cell></row><row><cell>SC2 decompression energy</cell><cell>148 pJ 64B access off-chip energy</cell><cell>74.8 nJ</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The study assumes set-based 128KB caches, where cache lines are compressed into 512-byte sets as much as possible, and evicted with LRU policy. Cache lines are compressed by splitting into 4-byte words and deduplicating them-within the cache line with intra-line, and across all cache lines with inter-line. Small values are further compressed by throwing away the most significant zeros (significance-based compression). Neither models have meta-data overheads (eg. pointers, tags, and fragmentation).</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="8.">ACKNOWLEDGEMENTS</head><p>This work was partially supported by the <rs type="funder">NSF</rs> under Grant No. <rs type="grantNumber">CCF-1217553</rs>, <rs type="funder">AFOSR</rs> under Grant No. <rs type="grantNumber">FA9550-14-1-0148</rs>, and <rs type="funder">DARPA</rs> under Grants No. <rs type="grantNumber">N66001-14-1-4040</rs> and <rs type="grantNumber">HR0011-13-2-0005</rs>. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of our sponsors.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_9vPXDTs">
					<idno type="grant-number">CCF-1217553</idno>
				</org>
				<org type="funding" xml:id="_XySQvaX">
					<idno type="grant-number">FA9550-14-1-0148</idno>
				</org>
				<org type="funding" xml:id="_7sFGBxr">
					<idno type="grant-number">N66001-14-1-4040</idno>
				</org>
				<org type="funding" xml:id="_tJJw5Fy">
					<idno type="grant-number">HR0011-13-2-0005</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Debunking the 100x GPU vs. CPU myth: an evaluation of throughput computing on CPU and GPU</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chhugani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Deisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="451" to="460" />
			<date type="published" when="2010">2010</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Convergence of recognition, mining, and synthesis workloads and its implications</title>
		<author>
			<persName><forename type="first">Y.-K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chhugani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dubey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="790" to="807" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Intel Xeon Phi coprocessor high performance programming</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jeffers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Reinders</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<pubPlace>Newnes</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Larrabee: a many-core x86 architecture for visual computing</title>
		<author>
			<persName><forename type="first">L</forename><surname>Seiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Carmean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sprangle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abrash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Junkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sugerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cavin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An 80-tile sub-100-w teraflops processor in 65-nm cmos</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Vangal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="41" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>Solid-State Circuits</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Energy characterization of a tiled architecture processor with on-chip networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wentzlaff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of international symposium on Low power electronics and design</title>
		<meeting>international symposium on Low power electronics and design</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="424" to="427" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On-chip interconnection architecture of the Tile Processor</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wentzlaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ramey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="15" to="31" />
			<date type="published" when="2007-09">Sept. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Tile64 -processor: A 64-core soc with mesh interconnect</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Solid-State Circuits Conference</title>
		<imprint>
			<publisher>IEEE International</publisher>
			<date type="published" when="2008-02">2008. Feb 2008</date>
			<biblScope unit="page" from="88" to="598" />
		</imprint>
	</monogr>
	<note>ISSCC 2008. Digest of Technical Papers</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Tile-gx100 manycore processor: Acceleration interfaces and architecture</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ramey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Hot Chips Symposium</title>
		<meeting>Hot Chips Symposium</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cramming More Components Onto Integrated Circuits</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronics</title>
		<imprint>
			<date type="published" when="1965-04">Apr. 1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>K?gi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Memory bandwidth limitations of future microprocessors</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<date type="published" when="1996">1996</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A 167-ps 2.34-mW Single-Cycle 64-Bit Binary Tree Comparator With Constant-Delay Logic in 65-nm CMOS</title>
		<author>
			<persName><forename type="first">P.-J</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sachdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gaudet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="160" to="171" />
			<date type="published" when="2014-01">Jan 2014</date>
		</imprint>
	</monogr>
	<note>Circuits and Systems I: Regular Papers</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">S</forename><surname>Thoziyoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Muralimanohar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<idno>CACTI 5.0</idno>
	</analytic>
	<monogr>
		<title level="j">HP Laboratories</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Energy-efficient floating-point unit design</title>
		<author>
			<persName><forename type="first">S</forename><surname>Galal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Horowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="913" to="922" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>Computers</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The future of wires</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Horowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="490" to="504" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">PCB Impedance Calculator</title>
		<author>
			<persName><surname>Technick</surname></persName>
		</author>
		<author>
			<persName><surname>Net</surname></persName>
		</author>
		<ptr target="http://www.technick.net/public/code/cpdpage.php" />
	</analytic>
	<monogr>
		<title level="m">? aiocp dp=util pcb imp microstrip</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">DDR3 System-Power Calculator</title>
		<ptr target="www.micron.com/support/power-calc" />
	</analytic>
	<monogr>
		<title level="j">Micron Technology</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adaptive cache compression for high-performance processors</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Alameldeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Symposium on Computer Architecture</title>
		<meeting>International Symposium on Computer Architecture</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="212" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Decoupled compressed cache: exploiting spatial locality for energy-optimized compressed caching</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sardashti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="62" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A unified compressed memory hierarchy</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">G</forename><surname>Hallnor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Reinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">High-Performance Computer Architecture, 2005. HPCA-11. 11th International Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="201" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Base-delta-immediate compression: practical data compression for on-chip caches</title>
		<author>
			<persName><forename type="first">G</forename><surname>Pekhimenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Seshadri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Kozuch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Mowry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of international conference on Parallel architectures and compilation techniques</title>
		<meeting>international conference on Parallel architectures and compilation techniques</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="377" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Residue cache: a low-energy low-area L2 cache architecture via compression and partial hits</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="420" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">C-Pack: A high-performance microprocessor cache compression algorithm</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lekatsas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Very Large Scale Integration (VLSI) Systems, IEEE Transactions on</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1196" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">SC2: A statistical compression cache scheme</title>
		<author>
			<persName><forename type="first">A</forename><surname>Arelakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stenstrom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM/IEEE 41st International Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="145" to="156" />
		</imprint>
	</monogr>
	<note>Computer Architecture (ISCA)</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A case for a value-aware cache</title>
		<author>
			<persName><forename type="first">A</forename><surname>Arelakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stenstrom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Architecture Letters</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="4" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">LZMA SDK</title>
		<author>
			<persName><forename type="first">I</forename><surname>Pavlov</surname></persName>
		</author>
		<ptr target="www.7-zip.org/sdk.html" />
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">GZIP file format specification version 4.3</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Deutsch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A fully associative software-managed cache design</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">G</forename><surname>Hallnor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Reinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Symposium on Computer Architecture</title>
		<meeting>International Symposium on Computer Architecture</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="107" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Column-associative Caches: A Technique For Reducing The Miss Rate Of Direct-mapped Caches</title>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pudar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Symposium on Computer Architecture</title>
		<meeting>International Symposium on Computer Architecture</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="179" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cache performance of operating system and multiprogramming workloads</title>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hennessy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Horowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer Systems (TOCS)</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="393" to="431" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">FPC: A high-speed compressor for double-precision floating-point data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Burtscher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ratanaworabhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computers, IEEE Transactions on</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="18" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">DEFLATE compressed data format specification version 1.3</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Deutsch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">AHA Data Compression</title>
		<author>
			<persName><surname>Aha</surname></persName>
		</author>
		<ptr target="http://www.aha.com/data-compression/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Indra Products</title>
		<author>
			<persName><surname>Indra</surname></persName>
		</author>
		<ptr target="http://www.indranetworks.com/products.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">PriME: A parallel and distributed simulator for thousand-core chips</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wentzlaff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Performance Analysis of Systems and Software (ISPASS), 2014 IEEE International Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="116" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sniper: Exploring the Level of Abstraction for Scalable and Accurate Parallel Multi-Core Simulations</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Heirman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Eeckhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference for High Performance Computing, Networking, Storage and Analysis</title>
		<imprint>
			<date type="published" when="2011-11">Nov. 2011</date>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="1" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pinballs: Portable and Shareable User-level Checkpoints for Reproducible Analysis and Simulation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Carlson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Reproducible Research Methodologies (REPRODUCE)</title>
		<meeting>the Workshop on Reproducible Research Methodologies (REPRODUCE)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Frequent pattern compression: A significance-based compression scheme for L2 caches</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Alameldeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dept. Comp. Scie., Univ. Wisconsin-Madison, Tech. Rep</title>
		<imprint>
			<biblScope unit="volume">1500</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Memory characterization of workloads using instrumentation-driven simulation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jaleel</surname></persName>
		</author>
		<ptr target="http://www.glue.umd.edu/ajaleel/workload" />
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Execution Drafting: Energy Efficiency Through Computation Deduplication</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Balkind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wentzlaff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="432" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Scaling the power wall: a path to exascale</title>
		<author>
			<persName><forename type="first">O</forename><surname>Villa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>O'connor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis</title>
		<meeting>the International Conference for High Performance Computing, Networking, Storage and Analysis</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="830" to="841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Skewed Compressed Caches</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sardashti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Seznec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="331" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">IBM memory expansion technology (MXT)</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Tremaine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="271" to="285" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Memory-link compression schemes: A value locality perspective</title>
		<author>
			<persName><forename type="first">M</forename><surname>Thuresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Spracklen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stenstrom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computers, IEEE Transactions on</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="916" to="927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Lossless and lossy memory I/O link compression for improving performance of GPGPU workloads</title>
		<author>
			<persName><forename type="first">V</forename><surname>Sathish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Schulte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of international conference on Parallel architectures and compilation techniques</title>
		<meeting>international conference on Parallel architectures and compilation techniques</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="325" to="334" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
