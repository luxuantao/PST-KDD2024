<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B919482F90F66B87B0705DB55D1EE1B1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T05:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>High-Order and Multilayer Perceptron Initialization Georg Thimm and Emile Fiesler, Member, IEEE</head><p>Abstract-Proper initialization is one of the most important prerequisites for fast convergence of feedforward neural networks like high-order and multilayer perceptrons. This publication aims at determining the optimal variance (or range) for the initial weights and biases, which is the principal parameter of random initialization methods for both types of neural networks. An overview of random weight initialization methods for multilayer perceptrons is presented. These methods are extensively tested using eight real-world benchmark data sets and a broad range of initial weight variances by means of more than 30 000 simulations, in the aim to find the best weight initialization method for multilayer perceptrons. For high-order networks, a large number of experiments (more than 200 000 simulations) was performed, using three weight distributions, three activation functions, several network orders, and the same eight data sets. The results of these experiments are compared to weight initialization techniques for multilayer perceptrons, which leads to the proposal of a suitable initialization method for high-order perceptrons. The conclusions on the initialization methods for both types of networks are justified by sufficiently small confidence intervals of the mean convergence times. Index Terms-Activation function, comparison of weight initialization methods, connectionism, high(er) order neural network, high(er) order perceptron, initial weight distribution, initialization, initial weight, learning rate, multilayer perceptron, optimization, neural network initialization, random weight weight initialization, real-world benchmark, sigma-pi connection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>T HE learning speed of multilayer and high-order per- ceptrons 1 depends mainly on the initial values of its weights and biases, its learning rate, its network topology, and on learning rule improvements like the momentum term. The optimal values for these parameters are usually unknown a priori because they depend mainly on the training data set used. In practice it is not feasible to perform a global search for obtaining the optimal values of these parameters, as the convergence behavior of the network might change significantly for small changes in the initial weights, as was demonstrated by Kolen and Pollack <ref type="bibr" target="#b2">[3]</ref>. An extensive search for the optimum values requires therefore much more overhead than performing a relatively small number of simulations using nonoptimal values. Furthermore, current mathematical techniques are insufficient for a complete theoretical study of the learning behavior of these neural networks. Nevertheless, it is important to have a good approximation of the optimal initial value of these parameters, or with the words of Kolen and Pollack: to start the learning process in the "eye of the storm," to reduce the required training time.</p><p>Several weight initialization methods for multilayer perceptrons have been suggested. The simplest method among them is random weight initialization, which is often preferred for its simplicity and its ability to produce multiple solutions, as the weights may, due to their initial randomness, converge to various attractors <ref type="bibr" target="#b2">[3]</ref>. Other methods involve extensive statistical and/or geometrical analysis of the data and are therefore very time consuming. The most rigorous among those is the pseudoinverse method for perceptrons, which, besides being limited to linear separable data, has several other drawbacks (see <ref type="bibr" target="#b3">[4]</ref>). Some other weight initialization methods are based on special properties of a network that can not be applied to high-order or multilayer perceptrons, as for example the weight initialization technique for radial basis function networks by Platt <ref type="bibr" target="#b4">[5]</ref>.</p><p>Rumelhart et al. observed that if all weights in a neural network are initialized with zero, they have the tendency to assume identical values during training. They therefore proposed random weight initialization to avoid this undesired situation by breaking the symmetry <ref type="bibr" target="#b5">[6]</ref>. However, the efficiency of this method depends much on the initial weight distribution. Several researchers therefore proposed random weight initialization methods. An overview of these methods is presented in Section II, and their performance is evaluated in Section IV-B1.</p><p>In order to obtain a thorough insight in the initialization characteristics of high-order networks, which have not been studied before, numerous experiments were performed, varying the following parameters:</p><p>• the shape of the initial weight distribution: uniform, normal or Gaussian<ref type="foot" target="#foot_0">2</ref> , and a novel distribution which is uniform over the intervals and , and zero everywhere else; • the variance of the initial weight distribution; • the order and topology of the network;</p><p>• the activation function. The results of these experiments is a simple weight initialization method using an application independent variance. In Section V, this method is compared to methods developed for multilayer perceptrons, in order to profit from experiences of other researchers and to determine a best method for higher order perceptrons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. WEIGHT INITIALIZATION TECHNIQUES FOR MULTILAYER PERCEPTRONS</head><p>Fahlman performed an early experimental study on the random weight initialization scheme for multilayer perceptrons.</p><p>Based on this study, he proposed to use a uniform distribution with a range of , but found that the best weight range for the data sets in his study varied from to <ref type="bibr" target="#b7">[8]</ref>. Other researchers tried to determine the optimal weight range using network parameters.</p><p>Bottou uses an interval , where is chosen is such way that the weight variance corresponds to the points of the maximal curvature of the activation function (which is approximately 2.38 for a standard sigmoid), and is the fan-in (or in-degree) of a neuron, without justifying this interval further in a theoretical manner. Bottou trains the neural network only on speech data and does not compare this method with others <ref type="bibr" target="#b8">[9]</ref>.</p><p>Boers and Kuiper initialize weights using a uniform distribution over the interval , without any mathematical justification. They state that this interval performed the best on their speech data <ref type="bibr" target="#b9">[10]</ref>.</p><p>Śmieja uses uniformly distributed weights which are normalized to the magnitude for each node. The thresholds of the hidden units are then initialized to a random value in the interval and the thresholds of the output nodes are set to zero. He obtained these values from reasoning about hyperplane spin dynamics, and did not validate his method by experiments <ref type="bibr" target="#b10">[11]</ref>.</p><p>Wessels and Barnard describe two weight initialization methods. The first method sets the initial weight range to a value which assumes that the output of the network and the output patterns have the same variance. The second method puts equally distributed decision boundaries in the input space (without considering input or output patterns), which produces initial weights for the first interlayer weight matrix. The weights of the second interlayer weight matrix are set to 1.0. They compared both methods on generalization for three data sets. They found that the second method performed better in terms of generalization. However, they did not compare convergence speeds <ref type="bibr" target="#b11">[12]</ref>.</p><p>An approach similar to the first method of Wessels and Barnard was introduced by Drago and Ridella <ref type="bibr" target="#b12">[13]</ref>. They aim at avoiding flat regions in the error surface by restricting the number of neurons with absolute activations greater than 0.9. They developed a simple formula to estimate the best weight initialization scheme for multilayer perceptrons and showed for three data sets that this scheme uses satisfactory good initial weight ranges. The weights are uniformly distributed over the interval , with for the input layer and for the output layer (assuming that all input values have the same expected value ).</p><p>Lee et al. showed theoretically that the probability of prematurely saturated neurons (small weight changes cause only negligible changes of the neuron output) in multilayer perceptrons increases with the maximal value of weights. They conclude that a smaller initial weight range increases the learning speed of multilayer perceptrons. Simulations performed using two data sets confirm their reasoning, but they disregard that learning speed also decreases for weight ranges that are too small. Lee et al. do not suggest an optimal weight range <ref type="bibr" target="#b13">[14]</ref>.</p><p>Haffner et al. use a normal initial weight distribution. Unfortunately they do not compare their approach to others, give details, or justify it mathematically <ref type="bibr" target="#b14">[15]</ref>.</p><p>Watrous and Kuhn compared a Gaussian distribution to a uniform distribution and found differences on the conditioning of the Jacobian matrix of a neural network, but found no relation to the convergence speed <ref type="bibr" target="#b15">[16]</ref>.</p><p>Nguyen and Widrow use a multilayer perceptron with piecewise linear activation functions as an approximation of a network with logistic activation functions. Based on this simplification, they calculated an optimal length of for the randomly initialized weight vectors and an optimal bias range of for neurons in the hidden layer, where is the number of hidden nodes. The weights of the neurons in the output layer are randomly initialized in the interval [ 0.5,0.5], without any justification given <ref type="bibr" target="#b16">[17]</ref>.</p><p>Kim and Ra calculated a lower bound for the initial length of the weight vector of a neuron to be , where is the learning rate <ref type="bibr" target="#b17">[18]</ref>.</p><p>Besides these random weight initialization methods, some nonrandom methods are described here for completeness.</p><p>A mixture between a random weight initialization scheme and the pseudoinverse method was developed by Chen and Nutter for perceptrons with one hidden layer. First, the weights in the first interlayer weight matrix of the network are initialized with random values. Then, the weights in the second interlayer weight matrix are calculated using the pseudo inverse method applied to the activation values of the hidden layer. Chen et al. refined this technique further by alternating the adjustment of the first interlayer weight matrix in a backpropagation-like process with the mentioned method of calculating the second interlayer weight matrix. These adjustments are repeated until a convergence criterion is reached, after which the backpropagation training begins. The authors report faster training in number of backpropagation cycles <ref type="bibr" target="#b18">[19]</ref>, but they disregard the computational complexity of the matrix inversions.</p><p>Denoeux and Lengellé initialize a one hidden layer perceptron with prototypes. This method requires a transformation of the input patterns to vectors of unit length and increased size. Additionally, prototypes have to be found by a cluster analysis. The authors reported improvements in training time, robustness versus local minima, and better generalization <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. HIGH-ORDER PERCEPTRONS</head><p>High-order perceptrons are high-order neural networks <ref type="bibr" target="#b20">[21]</ref>, having only unidirectional interlayer connections. They are also a generalization of Sigma-Pi networks <ref type="bibr" target="#b5">[6]</ref>, which are multilayer perceptrons having high-order connections, and functional link networks <ref type="bibr" target="#b21">[22]</ref>.</p><p>A high-order connection connects a set of neurons in one layer to neurons in the next layer (marked in Fig. <ref type="figure" target="#fig_0">1</ref>). Each connection applies its specific splicing function to the activation values of the lower layer. The number of activation values combined by the splicing functions determines the order of the connection and the connection with the highest order determines the order of the network. A network is called a full (nth order) network, if all possible interlayer connections up to this order are present. The network shown in Fig. <ref type="figure" target="#fig_0">1</ref> is, for example, a full two-layer second-order network.</p><p>The results of the splicing functions are fed, together with the activation values of the lower layer, into the next layer of neurons (marked ). Each neuron consists of a summation unit and an activation function (depicted by a and a symbolized function, respectively).</p><p>High-order perceptrons can be trained using the backpropagation algorithm, with possible extensions such as a momentum term <ref type="bibr" target="#b5">[6]</ref> or flat spot elimination <ref type="bibr" target="#b7">[8]</ref>.</p><p>From now on in this publication, only two layer highorder perceptrons are considered. The splicing function used in this study is multiplication, but other functions are also conceivable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. THE SIMULATIONS</head><p>A simulation consists of initializing a neural network and applying the online backpropagation algorithm, alternated by convergence tests, until (non)convergence is observed. A number of simulations starting with the same initial conditions is called an experiment.</p><p>The experiments are performed with two major aims: firstly, to see whether the performance of a network changes for different types of initial weight distributions, and secondly to find the optimal initial weight variance, depending on the activation function of the output neurons and the network order.</p><p>Each experiment consists of at least 50 simulations. The number of simulations per experiment was increased until the size of the 95% confidence interval for the mean convergence time permitted a sound conclusion. The confidence intervals were calculated under the assumption that the mean convergence time is student-t distributed.</p><p>For the simulations performed, a suboptimal learning rate is used, as it is too laborious and computing time consuming to find the optimal learning rate for each combination of data set and network, and as the learning rate and initial weight variance seem to have an independent influence on the learning speed. Because of the suboptimal learning speed, the results do not necessarily allow a comparison between different activation functions and experiments reported elsewhere, as the maximal possible learning rate may differ largely from the one actually used. For example, a third-order network has, for the solar data with a shifted/scaled logistic output function, an optimal learning rate of about 0.05. In contrast, the same network and data set, except for using now a standard logistic output function, has an optimal learning rate of about 4.0, and converges in about the same number of iterations. Some of the convergence criteria chosen in these simulations are rather crude and not related to the task to be solved. This is done in the aim to reduce the high computational expense, which was still several months of Sparc 10 central processing unit (CPU) time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The Data Sets</head><p>Most of the data sets used, and shortly described below, are obtained (if not stated otherwise) from an anonymous-ftp server at the University of California <ref type="bibr" target="#b22">[23]</ref>, which also contains further references and documentation. In the following list of data sets, the two numbers in brackets behind the name of the data set are the number of input and output values, respectively.</p><p>Solar (12,1) contains the sun spot activity for the years 1700 to 1990. The task is to predict the sun spot activity for one of those years, given the activity of the preceding 12 years ( real valued inputs). The data are scaled to the interval [0,1] and the networks trained to a mean square error of 0.06 (0.08 for first-order perceptrons). Wine <ref type="bibr" target="#b12">(13,</ref><ref type="bibr" target="#b2">3)</ref> is the result of a chemical analysis of wines grown in a region in Italy derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines. A wine has to be classified using these values, which are scaled to the interval [0,1]. The output patterns use Boolean values, encoded as 1 and 1 and the trained networks are required to correctly classify 90% of the patterns (80% for first-order perceptrons). CES (2,1) is the output of the constant elasticity of a substitution production function for thirty pairs of labor and capital input (see <ref type="bibr">[24, pp. 195, 210]</ref>). The patterns have two real valued inputs and one real valued output, none of them scaled and the networks are trained to a mean square error of 0.08 (0.14 for first-order perceptrons). Servo (12,1) was created by K. Ulrich at the Masssachusetts Institute of Technology, Cambridge, in 1986 and contains a very nonlinear phenomenon: predicting the rise time of a servomechanism in terms of two (continuous) gain settings and two (discrete) choices of mechanical linkages. The input is coded into two groups of five Boolean values each, and two discrete inputs, one assuming four, the other five values. The output is real valued, and like all real valued inputs, scaled to the interval [0,1]. The networks are trained until a mean square error of 0.08 is reached (0.14 for first-order perceptrons). Vowels <ref type="bibr" target="#b19">(20,</ref><ref type="bibr" target="#b4">5)</ref> is a subset of 300 patterns of the vowels data set, obtainable via ftp from cochlea.hut.fu (130.233.168.48) with the LVQ-package (lvq\_pak). An input pattern consists of 20 unscaled cepstral coefficients obtained from continuous Finnish speech. The task is to determine whether the pronounced phoneme is a vowel, and, in the case it is, which of the five possible ones. The Boolean output values are encoded as 1 and 1 and the networks are required to recognize 90% of the patterns (65% for firstorder perceptrons with a linear sigmoidal function, 80% for other first-order perceptrons). Auto (7,1) concerns city-cycle fuel consumption of cars in miles per gallon, to be predicted in terms of three multivalued discrete and four continuous attributes. All values are scaled to the interval [0,1] (incomplete patterns have been removed) and the networks are trained to a mean square error of 0.06. Glass (9,1) consists of eight scaled weight percentages of certain oxides and a seven-valued code for the type of glass (window glass, head lamps, etc.). The output is the refractive index of the glass, scaled to [0,1] and the networks trained to a mean square error of 0.03 (0.04 for simple perceptrons). Digits (256,10) consists of 500 handwritten digits (50 patterns for each of the ten digits) of the NIST Special Database 3 <ref type="bibr" target="#b24">[25]</ref>. Each digit was scaled to fit into an image of <ref type="bibr" target="#b15">16</ref> 16 points, and each pixel is represented by an eight bit value. The input values are scaled to the interval [ 1,1] and the Boolean output values are encoded as 1 and 1 and the networks are required to classify 95% of the patterns correctly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The Experiments for Multilayer Perceptrons</head><p>In the aim to validate and compare the performance of the random weight initialization techniques for multilayer perceptrons mentioned in Section II, a large number of experiments has been performed using the data sets listed in the previous section. The network topology used has one hidden layer which is fully interlayer connected to both input and output layer. The network has no intralayer or supralayer connections, and all activation functions in the hidden and output layer are hyperbolic tangents. No optimization technique was used for training.  <ref type="bibr" target="#b16">[17]</ref> nor the references mentioned in it state why. To make the simulations fair (leaving out the bias makes learning more difficult), a bias is used in the all simulations reported in this publication.</p><p>1) The Results for Multilayer Perceptrons: The outcome of the experiments for the multilayer perceptrons are shown in Figs. <ref type="figure" target="#fig_1">2</ref> and<ref type="figure" target="#fig_3">3</ref>. These figures list in the first three columns the name of the data set, the number of neurons in the hidden layer , and the learning rate . The subsequent columns, labeled with the initial weight variances in Fig. <ref type="figure" target="#fig_1">2</ref> and names in Fig. <ref type="figure" target="#fig_3">3</ref>, respectively, contain the outcome of the experiments. The names in Fig. <ref type="figure" target="#fig_3">3</ref> refer to the random weight initialization schemes described in Section II. A " " in these columns corresponds to the smallest mean number of required online learning cycles until convergence (an online learning cycle is a presentation of all patterns with a weight update after each presentation), a " " represents a mean which is too close to the best value to statistically conclude that this value is worse. A " " means that the network always converged but in a significantly longer time. A " " means that some or all simulations in an experiment did not converge in the prescribed number of iterations (which was set to at least three times the smallest number needed for convergence).</p><p>The best fixed value for an initial weight variance averaged over these experiments is 0.2, but this value is not necessarily the optimal value for a specific data set.</p><p>The on average best method for the determination was presented by Wessels.</p><p>2) Analysis of the Simulations for Multilayer Perceptrons: The average convergence behavior of a multilayer perceptron is depicted in Fig. <ref type="figure" target="#fig_4">4</ref>, where region indicates the optimum initial weight variances that have been encountered and region  nonconvergence. As the curve is flatter on the left side of the optimal initial weight variance than on the right, the loss in performance is much more tolerable for initial weight variances smaller than the optimal value as compared to bigger variances. Moreover, nonconvergence was only encountered for simulations using initial weight variances bigger than the optimal value.</p><p>The rather small differences obtained for an initial weight variance of 0.2 as compared to the optimal result, suggests to use this value for a simple weight initialization method. A comparison between the results for this simple method and Fig <ref type="figure" target="#fig_3">3</ref> shows that the weight initialization method of Wessels et al., which uses the same weight variances as the method of Boers, performs the best. 3 This is also valid compared to an initial weight variance of 0.2, with a saving of between 1%</p><p>50% training cycles. 3 The raw data have not been included for readability of the paper. Remark: Some of the weight initialization methods presented in Section II scale the upper and lower bound of the initial random weight interval by the reciprocal square root of the fan-in. This corresponds to scaling the initial weight variance by the reciprocal of the fan-in. Hence, these methods assume a negative correlation between the fan-in of a neuron and the best initial weight variance. This correlation can not be confirmed or rejected by the results of the experiments; more experiments with other data sets are necessary for this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. The Experiments with High-Order Perceptrons</head><p>The networks used in the simulations are usually full and the biases are initialized with a random value of the same distribution as the weights. The only exception is the network trained on the digits data set. This network includes all firstorder connections and only second-order connections with both inputs corresponding to different pixels in the same row or the same column in the image. This configuration should allow the extraction of sufficient features to learn the digits. Training sessions on the in Section IV-A described digits data set gave an acceptable recognition of untrained digits, despite the small training set used.</p><p>The three different initial random weight distributions used are: uniform on the interval (with ), normal (restricted to an absolute value of ), and uniform over the intervals and (with ) while zero everywhere else. The three types of activation functions used are: a linear , a hyperbolic tangent , and a scaled/shifted hyperbolic tangent , shown in Fig. <ref type="figure" target="#fig_5">5</ref>. The use of the function was motivated by several ideas: the scaling in the direction of the -axis prevents the weights from becoming very big and thus cause the same effect as for example scaling the output data to [ 0.9,0.9] and the change of the steepness and the shifting of the sigmoid in the direction of the -axis were used to force the outcome of the summation step in the neurons to be in the interval [0,1]. Also, experiments with this activation function where performed to see, whether a relation between a deformation of the activation function and the optimal initial weight range exists. The only optimization technique applied to speed up learning is flat-spot elimination. 1) The Results of the Simulations for High-Order Perceptrons: Figs. <ref type="figure" target="#fig_6">6</ref><ref type="figure" target="#fig_7">7</ref><ref type="figure">8</ref>show, besides entries already explained in Section IV-B1, the order of the fully interlayer connected network and the activation function . The entries in Figs. <ref type="figure" target="#fig_7">7</ref> and<ref type="figure">8</ref> with a gray background are those showing a significant difference with Fig. <ref type="figure" target="#fig_6">6</ref>.</p><p>2) Analysis of the Simulations for High-Order Perceptrons: The minimal convergence times for all three initial weight distributions show no difference of statistical significance, mainly their corresonding initial weight variances differ. The average behavior of the learning time as a function of the initial weight variance, which is depicted in Fig. <ref type="figure" target="#fig_9">9</ref>, is explained as follows. The main difference for the three distributions is the value of the weight variance where the convergence time starts increasing drastically. This "edge" (point ) is roughly at the same location for both the uniform distribution and the uniform distribution over two intervals, but slightly shifted to higher variances for the normal distribution. As the optimal weight variance (point ) is similarly displaced, the performance of two networks, initialized with two different weight distributions of the same variance, is difficult to compare. This might explain the better performance for a Gaussian initial weight distribution in the report of P. Haffner. For the various combinations of data set, network order, etc., the optimal weight variance was encountered in region , whereas nonconvergence was, if at all, only observed in region .</p><p>As the three different initial weight distributions yield no significant difference in network performance, only the commonly used uniform distribution is considered from now on. For the shifted/scaled logistic and the linear activation functions, the best fixed weight variance is about (which corresponds to an interval [ 0.017,0.017]). For the logistic activation function, the best value for the weight variance depends a lot on data set and network order. In general, the performance with optimal initial weight variance differs not much more than about 10% from the results obtained with a variance of or even smaller. Therefore a variance of may be used as a simple application independent random weight initialization scheme. This initialization scheme is also justified by a smaller risk: a network performs nearly as good for an initial weight variance smaller than the optimum. The loss in performance for choosing the initial weight variance too small is much less significant than it is for multilayer perceptrons.</p><p>The experiments confirm also that the data set itself has a large influence on the optimal initial weight variance: for the solar, wine, and servo data sets, the networks have about the same size for the same order, but the optimal value for the weight variance differs a lot for the network with the logistic activation function. Further, the optimal value for the initial weights remained for some data sets nearly unchanged for different net orders or even different activation functions, while it changes greatly for other sets. It remains the question, which attribute of the data sets causes this behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. WEIGHT INITIALIZATION TECHNIQUES FOR THREE-LAYER PERCEPTRONS APPLIED TO HIGH-ORDER PERCEPTRONS</head><p>The most remarkable fact is that the rules based on observations (the "rules of thumb"), which seem to perform well for multilayer perceptrons, do not apply directly to highorder perceptrons. Uniform weight distributions with an initial weight range of <ref type="bibr" target="#b0">[ 1,</ref><ref type="bibr" target="#b0">1]</ref> (which corresponds to a variance of about 0.33) or bigger are definitely a poor choice for most of the examples considered. The mean convergence time is for some of the examples more than four times higher than the best initial weight range found or, even worse, they do not converge in a reasonable time.</p><p>The approaches using an interval with a more or less arbitrary constant , do not outperform the weight initialization with a fixed variance of 0.0001 or vice versa. Nevertheless, one would expect that more sophisticated methods for random weight initialization perform better than a scheme with fixed initial weight variance.</p><p>One such a sophisticated scheme was described by Wessels <ref type="bibr" target="#b11">[12]</ref>, and is here recalculated for a second-order net with linear activation functions. The method tries to initialize the weights in such a way that the variance of the network output is equal to the expected variance of the target patterns. Similar formulas can easily be calculated for networks of different orders. The evaluation of these formulas gives an optimal initial weight variance between and for the examples considered in this publication. These values are usually much too high as compared to the results listed in the figures. No effort was therefore done to solve the same problems for the other (nonlinear) activation functions.</p><p>For the activation functions and the heuristic of Bottou comes the closest to the optimal weight variance and may therefore be considered as being better. Comparing the variances suggested by his heuristic to Fig . <ref type="figure" target="#fig_6">6</ref>, one finds that his method may be improved by using about one tenth of the suggested variance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>The experiments show that a suitable and convenient weight initialization method for high-order perceptrons <ref type="foot" target="#foot_1">4</ref> with identity activation function<ref type="foot" target="#foot_2">5</ref> is a random initialization with a rather small variance of about (which corresponds to a weight range of [ 0.017, 0.017]). If a hyperbolic tangent is used as activation function, the best performance is obtained for the interval , where is chosen such that the weight variance corresponds to one third of the distance between the points of the maximal curvature of the activation function <ref type="foot" target="#foot_3">6</ref> (this is approximately 0.8 for an unscaled hyperbolic tangent with steepness one). The "rules of thumb" which perform well for multilayer perceptrons are not suitable for high-order perceptrons (which can be explained by their different topologies).</p><p>The steepness (and/or the horizontal shift) of the activation function has an important influence on the convergence time of high-order perceptrons. A mathematical study, partly inspired by this research, showed that changes of the steepness of the activation function may be compensated for by adapting the initial weight range and the learning rate <ref type="bibr" target="#b25">[26]</ref>. More precisely, two networks with the same topology converge in the same time and have the same response for every input if Fig. <ref type="figure">8</ref>. Performance of high-order perceptrons for a uniform weight distribution over the intervals [02a; 0a] and [a; 2a], with a = 3 2 =7. their learning rates and , the steepness of their sigmoidal functions and , and their weight vectors and fulfill the condition This condition permits to extrapolate the weight initialization schemes to neural network implementations which for example do not use a standard sigmoidal function, as it is the case in optical implementations of neural networks (see <ref type="bibr" target="#b26">[27]</ref>). However, for the weight initialization scheme of Wessels, it can be shown that it already adapts correctly the initial weight range for changes of the steepness as the point of maximal curvature is moving inverse linearly with the steepness of the sigmoidal function. But the condition says also that if the steepness of the sigmoidal function is changed, the learning rate needs to be adapted. More precisely, it has to be divided by in order to obtain the same convergence time as the network with the standard sigmoidal function. Thus the combination of the results in this paper and the adaption rule above are together a very powerful tool for the determination of the initial weight range of multilayer or high-order perceptrons.</p><p>The large difference between the best initialization schemes found for the high-order perceptrons and the multilayer perceptrons might be explained by the (non)presence of a hidden layer. Other researchers observed a difference in the convergence time and best learning rate between multilayer perceptrons with one or two hidden layers, which can be understood as a modification of the initial weight range (see above), an indication that the number of hidden layers has an influence on the best initial weights.</p><p>On the other hand, the shape of the initial weight distribution of three rather different distributions showed no or only very little effect on the optimal convergence time of high-order perceptrons. The main effect observed is a dislocation of the optimal value for the initial weight variance. There is consequently no preference for one of the three distributions as the optimal learning speeds are similar.</p><p>For multilayer perceptrons with one hidden layer, the weight initialization method of Wessels et al. The experiments show that the best initial weight variance for both types of neural networks is determined by the data set. Consequently, some reasoning on the data set has to be included in the determination of this value, if better values than those proposed in this publication are desired. On the other hand, an initial weight variance close to the optimal value is often acceptable, as the impact on the number of required learning cycles is not too big for small deviations in variance. In general, the loss in convergence speed for both types of neural networks is bigger when too high a variance is chosen than when too small a variance is chosen, as compared to the optimal value. The evaluation of the experiments performed in order to find the best weight initialization scheme for high-order and multilayer perceptrons includes the calculation of confidence intervals for the mean convergence time. This is a much more reliable measure than simply counting the number of simulations performed, as used in most other publications.</p><p>Moreover, the simulations showed that some data sets require many more simulations to obtain a sufficiently small size of the confidence interval than others. In the experiments performed in this research, these numbers varied between 50 and 2000. The authors encourage other researchers to report their results in a similar way.</p><p>The experiments were performed without any convergence time decreasing optimizations like the use of a momentum term or an adaptive learning rate since they involve constants which will further increase the dimension of the problem of best initial network configuration. However, the authors expect those techniques to have an insignificant influence on the optimal weight initialization variance (or range) since they are designed to "short-cut" the trajectory the weights follow on the error surface toward the finally assumed minimum, which decreases the convergence time more or less proportionally to the nonoptimized case.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. A two-layer high-order perceptron.</figDesc><graphic coords="3,94.50,59.58,148.08,160.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Performance of multilayer perceptrons for a uniform weight distribution over the interval [0a; a], with a = p 3 2 .</figDesc><graphic coords="4,111.66,55.02,376.80,189.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>For each data set, a sequence of experiments with uniform initial weight distributions of a varying variance were performed (100 simulations per experiment). The outcome of these experiments was used to determine the overall best weight variance as a reference for comparing the random weight initialization schemes of Bottou et al. and Nguyen et al. It should be noted that Nguyen et al. do not seem to use a bias in the output layer. However, neither</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Random weight initialization with the methods of other authors.</figDesc><graphic coords="5,60.00,59.58,217.20,191.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The average behavior of a multilayer perceptron in convergence speed for changing the initial weight variance.</figDesc><graphic coords="5,43.56,284.82,250.08,210.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. The activation functions.</figDesc><graphic coords="5,307.68,59.58,247.92,179.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Performance of high-order perceptrons for a uniform weight distribution over the interval [0a; a], with a = p 3 2 .</figDesc><graphic coords="6,130.80,55.02,338.64,471.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Performance of high-order perceptrons for a normal distribution restricted to the interval [03 2 ; 3 2 ].</figDesc><graphic coords="7,131.40,55.02,337.44,455.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>A random weight initialization allows the calculation of the variance depending on the network topology ( is the number of neurons in the input layer, a weight, an input value) as weights and input values are independent as if or as all weights have the same distribution This for Boolean data and data which are scaled to the interval [ 1,1], and 1/12 for data which are scaled to the interval [0,1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Average behavior of a higher order perceptron in convergence time for changing initial weight variance.</figDesc><graphic coords="10,43.32,59.58,250.56,209.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>performed on average the best, but the performance of the methods proposed by Boers et al., Bottou, Śmieja, Drago et al., and Nguyen et al. are nearly as good. An fixed weight variance of 0.2, which corresponds to a weight range of [ 0.77, 0.77], gave the best mean performance for all the applications tested in this study. This performance is similar or better as compared to those of the other weight initialization methods. Despite the fact that the method of Wessels et al. and Bottou apply the same initial weight variances for the experiments performed for this publication, the first should be preferred, as it scales the initial weight variances depending on the activation function (due to the calculation of the network output variance).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="9,131.10,55.26,337.92,455.52" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>Neural network weights are often assumed to be normally distributed<ref type="bibr" target="#b6">[7]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>The results of this study apply in part to standard (first-order) perceptrons, since high-order perceptrons are a generalization of standard perceptrons.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>A linear activation function of steepness one.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3"><p>This results in about one-tenth of the weight variance.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank the Institute for Logic, Complexity, and Deduction Systems at the University of Karlsruhe for the supply of computing power, which allowed the performance of the simulations.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The authors are with IDIAP, CH-1920 Martigny, Switzerland Publisher Item Identifier S 1045-9227(97)01741-4. <ref type="bibr" target="#b0">1</ref> For a definition of high-order neural networks and associated references see <ref type="bibr" target="#b0">[1]</ref> and <ref type="bibr" target="#b1">[2]</ref>.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Georg Thimm received the master's degree in computer science with an emphasis on logic and artificial intelligence from the University of Karlsruhe, Germany, in 1992. He is currently preparing his Ph.D. dissertation at the Institut Dalle Molle d'Intelligence Artificielle Perceptive in Martigny, Switzerland.</p><p>His research interests include ontogenic methods applied on neural networks, especially in high-order perceptrons.</p><p>Mr. Thimm is Current Events Editor of Neurocomputing. Dr. Fiesler has been an invited speaker for several international panels and is involved in the organization of a number of scientific conferences and meetings. He is a Member of ENNS and is a Phi Kappa Phi Honorary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Emile Fiesler</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural-network topologies</title>
		<author>
			<persName><forename type="first">E</forename><surname>Fiesler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Neural Computation</title>
		<editor>
			<persName><forename type="first">E</forename><surname>Fiesler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Beale</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford Univ. Press and TOP Publishers</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page">82</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Modular object-oriented neuralnetwork simulators and topology generalizations</title>
		<author>
			<persName><forename type="first">G</forename><surname>Thimm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fiesler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Artificial Neural Networks (ICANN&apos;94</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Marinaro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Morasso</surname></persName>
		</editor>
		<meeting>Int. Conf. Artificial Neural Networks (ICANN&apos;94<address><addrLine>London</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="747" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Backpropagation is sensitive to initial conditions</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Kolen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Pollack</surname></persName>
		</author>
		<idno>TR 90-JK-BPSIC</idno>
	</analytic>
	<monogr>
		<title level="j">Lab. Artifical Intell. Res., Comput. Inform. Sci. Dep</title>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Hertz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krogh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Palmer</surname></persName>
		</author>
		<title level="m">I of Computation and Neural Systems Series Lecture Notes, Santa Fe Institute Studies in the Sciences of Complexity</title>
		<meeting><address><addrLine>Redwood City, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Addison-Wesley</publisher>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
	<note>Introduction to the Theory of Neural Computation</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning by combining memorization and gradient descent</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Lippman</surname></persName>
		</editor>
		<meeting><address><addrLine>San Mateo, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="volume">III</biblScope>
			<biblScope unit="page" from="714" to="720" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName><surname>Research Group</surname></persName>
		</author>
		<title level="m">Parallel Distributed Processing: Explorations in the Microstructure of Cognition</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1986">1986</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Do backpropagation trained neural networks have normal weight distributions?</title>
		<author>
			<persName><forename type="first">I</forename><surname>Bellido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fiesler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Artificial Neural Networks, ICANN&apos;93</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Gielen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Kappen</surname></persName>
		</editor>
		<meeting>Int. Conf. Artificial Neural Networks, ICANN&apos;93<address><addrLine>London</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="772" to="775" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An empirical study of learning speed in backpropagation networks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Fahlman</surname></persName>
		</author>
		<idno>CMU-CS-88-162</idno>
	</analytic>
	<monogr>
		<title level="j">School Comput. Sci., Carnegie Mellon Univ</title>
		<imprint>
			<date type="published" when="1988-09">Sept. 1988</date>
			<pubPlace>Pittsburgh, PA</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Reconnaissance de la parole par reseaux multi-couches</title>
		<author>
			<persName><forename type="first">L. -Y</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Wkshp. Neural Networks Applicat., Neuro-Nîmes&apos;88</title>
		<meeting>Int. Wkshp. Neural Networks Applicat., Neuro-Nîmes&apos;88</meeting>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="page" from="197" to="217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Biological metaphors and the design of modular artificial neural networks</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J W</forename><surname>Boers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kuiper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Master&apos;s thesis</title>
		<meeting><address><addrLine>The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992-08">Aug. 1992</date>
		</imprint>
		<respStmt>
			<orgName>Leiden Univ.</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Hyperplane &apos;spin&apos; dynamics, network plasticity and backpropagation learning</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Śmieja</surname></persName>
		</author>
		<author>
			<persName><surname>Gmd</surname></persName>
		</author>
		<author>
			<persName><surname>St</surname></persName>
		</author>
		<author>
			<persName><surname>Augustin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gmd</forename><surname>Germany</surname></persName>
		</author>
		<author>
			<persName><surname>Rep</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991-11-28">Nov. 28, 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Avoiding false local minima by proper initialization of connections</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F A</forename><surname>Wessels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Barnard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="899" to="905" />
			<date type="published" when="1992-11">Nov. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Statistically controlled activation weight initialization (SCAWI)</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">P</forename><surname>Drago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ridella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="627" to="631" />
			<date type="published" when="1992-07">July 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An analysis of premature saturation in backpropagation learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S. -H</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="719" to="728" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fast backpropagation learning methods for neural networks in speech</title>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sawai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shikano</surname></persName>
		</author>
		<idno>TR-1-0058</idno>
	</analytic>
	<monogr>
		<title level="j">ATR Interpreting Telephony Res. Lab., Tech. Rep</title>
		<imprint>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Some considerations on the training of recurrent neural networks for time-varying signals</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Watrous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2nd Wkshp. Neural Networks Speech Processing</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</editor>
		<meeting>2nd Wkshp. Neural Networks Speech essing<address><addrLine>Trieste, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Edizioni LINT</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="5" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improving the learning speed of two-layer neural networks by choosing initial values of the adaptive weights</title>
		<author>
			<persName><forename type="first">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Widrow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf. Neural Networks (IJCNN)</title>
		<meeting>Int. Joint Conf. Neural Networks (IJCNN)<address><addrLine>Ann Arbor, MI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="21" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Weight value initialization for improving training speed in the backpropagation network</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Ra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf. Neural Networks</title>
		<meeting>Int. Joint Conf. Neural Networks</meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="2396" to="2401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improving the training speed of three-layer feedforward neural nets by optimal estimation of the initial weights</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Nutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf. Neural Networks</title>
		<meeting>Int. Joint Conf. Neural Networks</meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="2063" to="2068" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Initializing backpropagation networks with prototypes</title>
		<author>
			<persName><forename type="first">T</forename><surname>Denoeux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lengellé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="351" to="363" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Machine learning using a higher order correlation network</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Doolen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica D: Nonlinear Phenomena</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="276" to="306" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Pao</surname></persName>
		</author>
		<title level="m">Adaptive Pattern Recognition and Neural Networks. Reading</title>
		<meeting><address><addrLine>MA</addrLine></address></meeting>
		<imprint>
			<publisher>Addison-Wesley</publisher>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Aha (Librarians), UCI Repository of machine learning databases [Machine-readable data repository]. ftp: ics. uci. edu:/pub/machine-learning-databases</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The Theory and Practice of Econometrics</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Judge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T. -C</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Wiley Series in Probability and Mathematical Statistics</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Garris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Wilkinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Inst. Standards Technol., Advanced Syst. Division, Image Recognition Group</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="1992-02">Feb. 1992</date>
		</imprint>
	</monogr>
	<note>NIST Special Database</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">The interchangeability of learning rate and gain in backpropagation neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Thimm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Moerland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fiesler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="451" to="460" />
		</imprint>
	</monogr>
	<note>to appear in Neural Computa.</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Adaptive multilayer optical neural network with optical thresholding</title>
		<author>
			<persName><forename type="first">I</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fiesler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optical Eng</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2435" to="2440" />
			<date type="published" when="1995-08">Aug. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
