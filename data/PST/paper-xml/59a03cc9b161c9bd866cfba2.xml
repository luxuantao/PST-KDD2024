<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Resilient Datacenter Load Balancing in the Wild</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hong</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">SING Lab</orgName>
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junxue</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Bai</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">SING Lab</orgName>
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">SING Lab</orgName>
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mosharaf</forename><surname>Chowdhury</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">SING Lab</orgName>
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<addrLine>SIGCOMM &apos;17, August 21-25</addrLine>
									<postCode>2017</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Resilient Datacenter Load Balancing in the Wild</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">21219546B84E2F45FCD06E19821378B2</idno>
					<idno type="DOI">10.1145/3098822.3098841</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Datacenter fabric</term>
					<term>Load balancing</term>
					<term>Distributed</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Production datacenters operate under various uncertainties such as traffic dynamics, topology asymmetry, and failures. Therefore, datacenter load balancing schemes must be resilient to these uncertainties; i.e., they should accurately sense path conditions and timely react to mitigate the fallouts. Despite significant efforts, prior solutions have important drawbacks. On the one hand, solutions such as Presto and DRB are oblivious to path conditions and blindly reroute at fixed granularity. On the other hand, solutions such as CONGA and CLOVE can sense congestion, but they can only reroute when flowlets emerge; thus, they cannot always react timely to uncertainties. To make things worse, these solutions fail to detect/handle failures such as blackholes and random packet drops, which greatly degrades their performance.</p><p>In this paper, we introduce Hermes, a datacenter load balancer that is resilient to the aforementioned uncertainties. At its heart, Hermes leverages comprehensive sensing to detect path conditions including failures unattended before, and it reacts using timely yet cautious rerouting. Hermes is a practical edge-based solution with no switch modification. We have implemented Hermes with commodity switches and evaluated it through both testbed experiments and large-scale simulations. Our results show that Hermes achieves comparable performance to CONGA and Presto in normal cases, and well handles uncertainties: under asymmetries, Hermes achieves up to 10% and 20% better flow completion time (FCT) than CONGA and CLOVE; under switch failures, it outperforms all other schemes by over 32%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Modern datacenter networks enable multiple paths between host pairs and balance traffic among them to deliver good performance to different bandwidth-and latency-sensitive datacenter applications <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b17">18]</ref>. Meanwhile, production datacenters operate under a multitude of uncertainties, such as congestion, asymmetry, and failures <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17]</ref>. Uncertainties arise due to a variety of reasons that include, among others, traffic dynamics, link cuts, device heterogeneity, and switch malfunctions <ref type="bibr" target="#b18">[19]</ref>. Naturally, a datacenter load balancer must adapt to these uncertainties; i.e., they should <ref type="bibr" target="#b0">(1)</ref> accurately sense path conditions, and (2) appropriately split traffic among parallel paths in reaction to path conditions.</p><p>However, the standard multi-path load balancing mechanism used in today's datacenters, ECMP (Equal Cost Multi-Path) <ref type="bibr" target="#b20">[21]</ref>, balances traffic poorly. ECMP randomly stripes flows across available paths using flow hashing. Because it accounts for neither network uncertainties nor flow sizes, it can waste over 50% of the bisection bandwidth <ref type="bibr" target="#b3">[4]</ref>.</p><p>As a result, many load balancing schemes have been proposed to address the problem. Despite significant efforts, prior solutions still have important drawbacks (see §2 and §7 for details). Some of them, such as DRB <ref type="bibr" target="#b11">[12]</ref> and Presto <ref type="bibr" target="#b19">[20]</ref>, are insensitive or even oblivious to path conditions and blindly split traffic at fixed (e.g., packet or flowcell) granularity. These solutions suffer in the presence of asymmetry because the optimal traffic splitting across parallel paths depends on traffic demands and path conditions <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14]</ref>, and schemes that lack visibility of path conditions are unable to make optimal decisions. Furthermore, blindly splitting flows onto different paths on a round-robin basis can adversely affect transport protocols. Besides the well-known packet reordering problem, we further unveil the relatively less understood congestion mismatch problem ( §2.2.2).</p><p>In contrast, other solutions such as CONGA <ref type="bibr" target="#b4">[5]</ref> and CLOVE <ref type="bibr" target="#b23">[24]</ref> are designed to be congestion-aware. Although they can sense congestion, these solutions only reroute when flowlets emerge. While flowlet switching helps to minimize packet re-ordering <ref type="bibr" target="#b32">[33]</ref>, it is inflexible. As the formation of flowlets is decided by many factors such as applications and transport protocols, flowlet-based solutions are inherently passive and cannot always timely react to congestion by rerouting flows when needed. We show that this can easily result in ∼50% performance loss ( §2.2.2).</p><p>Furthermore, existing approaches for congestion sensing have key shortcomings. They are either impractical because they require non-trivial switch modifications <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b34">35]</ref>, or inefficient because they provide limited visibility into network congestion <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>. Moreover, none of them can detect switch failures such as packet blackholes and random packet drops, which are frequently witnessed in production datacenters <ref type="bibr" target="#b18">[19]</ref>. Our experiments show that being unable to detect these failures may lead to unfinished flows and over 100× worse average FCT.</p><p>Given the above inefficiencies, we ask the following question: can we design a resilient load balancing scheme that can gracefully handle all these uncertainties in a practical, readily-deployable fashion? In this paper, we present Hermes to answer this question affirmatively.</p><p>At its heart, Hermes detects path conditions via comprehensive sensing ( §3.1). It makes use of transport-level signals such as ECN and RTT to measure path congestion, while leveraging events such as retransmissions and timeouts to detect packet blackholes and random packet drops caused by malfunctioning switches. To further improve visibility, Hermes employs active probing -guided by the power of two choices technique <ref type="bibr" target="#b27">[28]</ref> -that can effectively increase the scope of sensing at minimal probing cost.</p><p>Given path conditions, how to react to the perceived uncertainties is still non-trivial. Considering the passiveness of flowlets, a natural choice is to actively split flows at a finer granularity and always switching to the best available path instantly. However, our analysis reveals that such vigorous path changing, even coupled with comprehensive sensing, can interfere with transport protocols by causing frequent packet reordering and congestion mismatch problems.</p><p>Therefore, Hermes handles uncertainties timely yet cautiously ( §3.2). On the one hand, rather than being constrained by a fixed or passive granularity, Hermes is capable of reacting timely once it senses congestion or failures. On the other hand, instead of vigorously changing paths, Hermes assesses flow status and path conditions, and makes deliberate rerouting decisions only if they bring performance gains. This enables Hermes to prune unnecessary reroutings and to reduce packet reordering and congestion mismatch, making it transport-friendly.</p><p>Hermes is a practical edge-based solution with no switch modifications. We implemented a Hermes prototype and deployed it in our small-scale testbed ( §4). Testbed experiments as well as largescale simulations with the realistic web-search <ref type="bibr" target="#b5">[6]</ref> and data-mining <ref type="bibr" target="#b17">[18]</ref> workloads show that ( §5):</p><p>• Under symmetric topologies, Hermes achieves 10-38% better FCT than ECMP, outperforms CLOVE-ECN by up to 15% for both workloads, and achieves comparable performance to Presto and CONGA.</p><p>• Under asymmetric topologies, Hermes outperforms CONGA by up to 10% due to its timely rerouting when sensing congestion with the data-mining workload. Furthermore, with better visibility via active probing, it achieves up to 20% better FCT than solutions with limited visibility such as CLOVE-ECN and Let-Flow <ref type="bibr" target="#b13">[14]</ref>.</p><p>• In case of switch failures, Hermes can effectively detect the failures, and it outperforms all other schemes by more than 32%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND AND MOTIVATION</head><p>In this section, we first summarize different sources of uncertainties in datacenters ( §2.1), and then elaborate the limitations of prior solutions in terms of both sensing and reacting to uncertainties ( §2.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Uncertainties</head><p>Datacenters are filled with the following uncertainties:</p><p>• Traffic dynamics: As shown in previous studies <ref type="bibr" target="#b16">[17]</ref>, traffic in production datacenters can be highly dynamic. Congestion can quickly arise as a few high-rate flows start, and it can dissipate as they complete.</p><p>• Asymmetries: Asymmetries are the norm in datacenters. For example, topology asymmetry can arise as a datacenter evolves by adding racks and switches over time, which may also introduce coexistence of heterogenous devices (e.g., both 10G and 40G spine switches). Furthermore, it is reported that datacenters can experience frequent link cuts <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19]</ref>, creating asymmetries.</p><p>• Switch failures: Besides link failures that directly cause topology asymmetry, production datacenters also suffer from a variety of switch failures or malfunctions that traditionally have received less attention. Notably, a recent study of Microsoft production datacenters <ref type="bibr" target="#b18">[19]</ref> reveals two types of switch failures that adversely affect network performance: (1) packet blackholes: packets that meet certain 'patterns' (e.g., certain source-destination IP pairs, or port numbers) are dropped deterministically (i.e., 100%) by the switch; and (2) silent random packet drops: a switch drops packets silently and randomly at a high rate. The root causes of these switch failures include TCAM deficits in the switching ASIC, switching fabric CRC checksum errors, and linecards not being well seated, among others.</p><p>Load balancing traffic under these uncertainties is a challenge. To deal with traffic dynamics and asymmetries, an ideal solution needs to be congestion-aware. To handle failures, it needs to detect them quickly and accurately. Despite continuous efforts in recent years, prior solutions still have important drawbacks as we illustrate in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Drawbacks of Prior Solutions in Handling Uncertainties</head><p>We follow Table <ref type="table" target="#tab_0">1</ref> to discuss the drawbacks of prior solutions, motivating our design of Hermes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Limitations in Sensing</head><p>Uncertainties. First of all, solutions such as DRB, Presto, and ECMP <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref> are either oblivious to congestion or rely only on local traffic conditions. They perform poorly under asymmetry because the optimal traffic splitting across paths in asymmetric topologies depends primarily on traffic demands and path conditions. Schemes without global congestion-awareness cannot effectively balance the traffic <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14]</ref>.</p><p>Second, existing solutions that are designed to be global congestionaware <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b34">35]</ref> have drawbacks too. They are either impractical because they require non-trivial switch modifications <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b34">35]</ref> or inefficient because they only provide limited visibility into network congestion at the end hosts <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>To illustrate this, we quantify network visibility as the average number of concurrent flows observed on parallel paths. We run a trace-driven simulation based on web-search and data-mining workloads in a 8×8 leaf spine topology with 10 Gbps links and 128 servers. We measure the visibility of both ToR switches and end hosts for 2s. The results are shown in Table <ref type="table" target="#tab_1">2</ref>. We find that a source ToR switch can observe congestion status of several parallel paths to each destination ToR simultaneously, while the same is not the case for end host pairs. Overall, current end host based solutions <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Schemes</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sensing Uncertainties</head><p>Reacting to Uncertainties Advanced Hardware Congestion Switch Minimum Switchable Switching Method and Frequency Failure Unit ECMP <ref type="bibr" target="#b20">[21]</ref> Oblivious Oblivious Flow Per-flow random hashing No Presto <ref type="bibr" target="#b19">[20]</ref> Flowcell (small fixed-sized unit) Per-flowcell round robin DRB <ref type="bibr" target="#b11">[12]</ref> Packet Per-packet round robin LetFLow <ref type="bibr" target="#b13">[14]</ref> Oblivious Oblivious Flowlet Per-flowlet random hashing Yes 1 DRILL <ref type="bibr" target="#b15">[16]</ref> Local awareness (Switch) Oblivious Packet Per-packet rerouting (according to local congestion) Yes CONGA <ref type="bibr" target="#b4">[5]</ref> HULA <ref type="bibr" target="#b24">[25]</ref> Global awareness (Switch) Oblivious Flowlet Per-flowlet rerouting (according to global congestion) Yes CLOVE-INT <ref type="bibr" target="#b23">[24]</ref> FlowBender <ref type="bibr" target="#b22">[23]</ref> Global awareness (End host) Oblivious Packet Reactive and random rerouting (when congested) No CLOVE-ECN <ref type="bibr" target="#b23">[24]</ref> Flowlet Per-flowlet weighted round robing (according to global congestion)  which rely primarily on piggybacked information, lack sufficient visibility for making appropriate load balancing decisions. Finally, none of the existing schemes detect switch failures such as packet blackholes and silent random packet drops. We note that some solutions such as Presto <ref type="bibr" target="#b19">[20]</ref> and DRB <ref type="bibr" target="#b11">[12]</ref> rely on a central controller to detect link failures; however, they do not detect switch failures. Moreover, current congestion-aware solutions such as CONGA <ref type="bibr" target="#b4">[5]</ref> estimate congestion level by measuring link utilization; thus, they cannot effectively detect switch failures either. Consider a switch that experiences silent random packet drops; flows traversing this switch tend to have a low sending rate due to frequent packet drops. Then the corresponding paths experiencing switch failures will have even lower measured congestion levels compared to other parallel paths. As a consequence, existing congestion-aware solutions may shift more traffic to these undesirable paths. In our evaluation, this causes CONGA to perform worse than ECMP ( §5.3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Problems with Reacting to Uncertainties.</head><p>Flowlet switching cannot timely react to uncertainties: Many congestion-aware solutions <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref> adopt flowlet switching. The benefit is that flowlets provide a finer-granularity alternative to flows for load balancing without causing much packet reordering. However, because flowlets are decided by many factors such as applications and transport protocols, solutions relying on flowlet switching are inherently passive and cannot always timely react to congestion by splitting traffic when needed.</p><p>Figure <ref type="figure" target="#fig_1">1</ref> shows such an example with CONGA <ref type="bibr" target="#b4">[5]</ref>. We have two small flows (A, B) and two large flows (C, D) from L0 to L1 via parallel paths P 1 and P 2 . We use DCTCP <ref type="bibr" target="#b5">[6]</ref> as the underlying transport protocol. At the beginning, CONGA balances load by placing flow 1 LetFlow <ref type="bibr" target="#b13">[14]</ref> has been implemented for Cisco's datacenter switch production line; however, currently it is not widely supported by commodity switches. For example, most Broadcom switching chipsets do not support flowlet switching.  A, B on P 1 and C, D on P 2 . After flow A and B complete, CONGA senses that P 1 is idle. However, when using a flowlet timeout of 100-500µs <ref type="bibr" target="#b4">[5]</ref>, CONGA cannot identify flowlets for rerouting, mainly because DCTCP is less bursty -this is because DCTCP adjusts its congestion window smoothly, and thus, it is less likely to generate sufficient inactivity gaps that form flowlets. On the other hand, a smaller flowlet timeout value (e.g., 50µs <ref type="bibr" target="#b13">[14]</ref>) triggers frequent flipping, causing severe packet reordering in our simulation. In the ideal scenario, appropriate rerouting can almost halve the FCTs of the large flows.</p><p>Vigorous rerouting is harmful: Considering the deficiency of flowlets, a natural choice is to split flows at a finer granularity and always switching to the best available path instantly. However, such vigorous path changing, even coupled with congestion awareness, can adversely interfere with transport protocols. Besides the wellknown packet re-ordering problem, we further unveil the congestion mismatch problem (defined below) that has previously received little attention.</p><p>Basically, congestion control algorithms adjust the rate (window) of a flow based on the congestion state of the current path. Hence, each rerouting event can cause a mismatch between the sending rate and the state of the new path. With vigorous rerouting within a flow, the congestion states of different paths are mixed together, and congestion on one path may be mistakenly used to adjust the rate on another path. We refer to this phenomenon as congestion mismatch. In the following, we show three cases of congestion mismatch and their impacts. Although we use DCTCP as the transport protocol, the problems demonstrated are not tied to any specific transport protocol. To mask the throughput loss caused by packet reordering, we set DupAckThreshold to 500.</p><p>First, we show that for congestion-oblivious load balancing schemes with small granularity (e.g., Presto <ref type="bibr" target="#b19">[20]</ref> and DRB <ref type="bibr" target="#b11">[12]</ref>), congestion mismatch causes throughput loss and queue oscillations under asymmetries. Consider a simple 3×2 leaf-spine topology in Figure <ref type="figure">2a</ref> 2 with a broken link from L0 to S1. Flow A is a DCTCP flow from L1 to L2, and flow B is a UDP flow from L0 to L2 with a limited rate of 9 Gbps. We adopt Presto with equal weights for different paths.</p><p>As shown in Figure <ref type="figure">2b</ref>, flow A only achieves around 1 Gbps overall throughput, and the queue length of the output port of spine S0 to leaf L2 experiences large variations. Due to vigorous rerouting, the congestion feedback (i.e., ECN) of the upper path constrains the congestion window, resulting in throughput loss in the bottom path. Furthermore, when flow A with a larger window shifts from the bottom path to the upper path, the upper path cannot immediately absorb such a burst, causing queue length oscillations.</p><p>Second, we show that congestion mismatch remains harmful even if we distribute traffic proportionally to path capacity. To illustrate this, consider a heterogenous network with 1 and 10 Gbps paths shown in Figure <ref type="figure">3a</ref>. We spread flowcells using 1:10 ratio to match path capacities and expect both paths to be fully utilized.</p><p>However, as shown in Figure <ref type="figure">3b</ref>, flow A can only achieve an overall throughput of around 5 Gbps. To understand the reason, assume that the first 10 flowcells go through the 10 Gbps path. Because the 2 We flip leaf L2 and omit the unused paths for clarity. path is not congested, DCTCP will increase the congestion window without realizing that the subsequent flowcell will go through the 1 Gbps path. With a large congestion window, the 11th flowcell is sent at a high rate on the 1 Gbps path, causing a rapid queue buildup on the output port of spine S0 to leaf L2. As the queue length exceeds the ECN marking threshold (i.e., 32KB for 1 Gbps link), DCTCP will reduce the window upon receiving ECN-marked ACKs without realizing that such a reduction will affect the following flowcells on the 10 Gbps path. As a result, such a congestion mismatch still causes throughput loss and queue oscillations. Third, we show that for congestion-aware solutions, suboptimal rerouting also leads to severe congestion mismatch. Figure <ref type="figure" target="#fig_3">4a</ref> shows such an example with CONGA. Flow A starts from L0 to L2, and we add a 3ms pause every 10ms to create a flowlet timeout. Flow B keeps sending from L1 to L2. We find that flow A keeps flipping between spine S0 and S1. This is because no matter which path flow A chooses, it does not have explicit feedback packets on the alternative path; thus, it always assumes the alternative path to be empty after an aging period (i.e., 10ms as suggested in <ref type="bibr" target="#b4">[5]</ref>). As a result, flow A will aggressively reroute to the alternative path every time it observes a flowlet.</p><p>We note that imperfect congestion information is unavoidable in a distributed setting. However, current congestion-aware solutions <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b24">25]</ref> do not take this into account, and their aggressive rerouting may lead to congestion mismatch. As shown in Figure <ref type="figure" target="#fig_3">4b</ref>, each time flow A reroutes from spine S0 to S1, its sending rate is much higher than the desired rate at the new path; this causes an acute queue increase at the output port of S1-L2. These periodic spikes in queue occupancy can lead to high tail latencies for small flows or even packet drops when buffer-hungry protocols (e.g., TCP) are used. In large-scale simulations with production workloads ( §5.3.2), we observe that CONGA performs 30% worse with a smaller flowlet timeout of 50µs compared to that of 150µs, even after we mask packet reordering. We believe that such performance degradations are due to congestion mismatch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DESIGN</head><p>The limitations discussed in §2 highlight the following properties of an ideal load balancing solution:</p><p>• Comprehensiveness: it should effectively detect congestion and failures to guide load balancing decisions;   • Timeliness: it should quickly react to various uncertainties and make timely rerouting decisions;</p><p>• Transport-friendliness: it should limit its impact (i.e., packet reordering and congestion mismatch) on transport protocols;</p><p>• Deployability: it should be implementable with commodity hardware in current datacenter environments.</p><p>To this end, we propose Hermes, a resilient load balancing scheme to gracefully handle uncertainties. Figure <ref type="figure">5</ref> overviews the two main modules of Hermes: (1) the sensing module that is responsible for sensing path conditions; and (2) the rerouting module that is responsible for determining when and where to reroute the traffic.</p><p>Akin to previous work <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24]</ref>, we implement a Hermes instance running in the hypervisor of each end host, leveraging its programmability to enable comprehensive sensing ( §3.1) and timely yet cautious rerouting ( §3.2) to meet the aforementioned properties. Table <ref type="table" target="#tab_3">3</ref> and 4 summarize the key variables and parameters used in the design and implementation of Hermes. Table <ref type="table">5</ref>: Outcome of path conditions using ECN and RTT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Comprehensive Sensing</head><p>• RTT directly signals the extent of end-to-end congestion. While it is informative, accurate RTT measurement is difficult in commodity datacenters without advanced NIC hardware <ref type="bibr" target="#b25">[26]</ref>. Thus, in our implementation, we primarily use RTT to make a coursegrained categorization of paths, i.e., to separate congested paths from uncongested ones. While a large RTT does not necessarily indicate a highly congested path (e.g., end host network stack delay can increase RTT), a small RTT is an indicator of an underutilized path.</p><p>• ECN captures congestion on individual hops. It is supported by commodity switches and widely used as congestion signal for many congestion control algorithms <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b33">34]</ref>. Note that ECN is a binary signal that is marked when a local switch queue length exceeds a marking threshold, it can well capture the most congested hop along the path. However, in a highly loaded network where congestion may occur at multiple hops, ECN cannot reflect this. Furthermore, a low ECN marking rate does not necessarily indicate a vacant end-to-end path, especially when there are not enough samples.</p><p>Given that neither RTT nor ECN can accurately indicate the condition of an end-to-end path, to get the best of both, we combine these two signals using a set of simple guidelines in Algorithm 1. Specifically, we characterize a path to be good if both RTT measurement and ECN fraction are low. In contrast, if both RTT measurement and ECN fraction are high, we identify the path to be congested -this is because a large RTT value alone may be caused by the network stack latency at the end host, whereas a high ECN fraction alone from a small number of samples may be inaccurate as well. Otherwise, in all the other cases, we classify the path to be gray. Here we consider a 100×100 leaf-spine topology with 10 5 end hosts and 10Gbps link. A probe packet is typically 64 bytes and the probe interval is set to 500µs.</p><p>3.1.2 Sensing Failures. Hermes leverages packet retransmission and timeout events to infer switch failures such as packet blackholes and random drops. <ref type="foot" target="#foot_0">3</ref>Recall that a switch with blackholes will drop packets from certain source-destination IP pairs (or plus port numbers) deterministically <ref type="bibr" target="#b18">[19]</ref>. To detect a blackhole of a certain source-destination pair, Hermes monitors flow timeouts on each path. Once it observes 3 timeouts on a path, it further checks if any of the packets on that path have been successfully ACKed. If none have been ACKed, Hermes concludes that all packets on this path are being dropped and identifies it as a failed path. Blackholes including port numbers can be detected in a similar way.</p><p>A switch experiencing silent random packet drops introduces high packet drop rates. To detect such failures, we observe that packet drops always trigger retransmissions, which can be captured by monitoring TCP sequence numbers. Based on this observation, Hermes records packet retransmissions on each path, and picks out paths experiencing high retransmission rates (e.g., 1% under DCTCP) for every τms. We set τ to be 10 by default. Congestion can cause frequent retransmissions as well. Therefore, Hermes further checks the congestion status, and identifies uncongested paths with high retransmission rates as failed paths (lines 8-9 in Algorithm 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Improving Visibility.</head><p>Hermes requires visibility into the network for good load balancing decisions. However, visibility comes at a cost. One can exhaustively probe all the paths to achieves full visibility, but it introduces high probing overhead -100× the capacity of a 10Gbps link (Table <ref type="table" target="#tab_6">6</ref>). In contrast, piggybacking used in existing edge-based load balancing solutions <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref> incur little overhead, but it provides very limited visibility.</p><p>We seek a better tradeoff between visibility and probing overhead by leveraging the well-known power-of-two-choices technique <ref type="bibr" target="#b27">[28]</ref>. Unlike DRILL <ref type="bibr" target="#b15">[16]</ref> that applies this for packet routing within a switch, we adopt it at end hosts to probe path-wise information with the following considerations.</p><p>First, we find that there is no need to pursue congestion information for all the paths; instead, probing a small number of them can effectively improve the load balancing performance with affordable probing cost. Second, all path-wise congestion signals have at least one RTT delay. Hence, probing only a small number of paths reduces the risk of many end hosts making identical choices and overwhelming one path while leaving others underutilized. <ref type="foot" target="#foot_1">4</ref> Third, in addition to the two random probes, we add an extra probe on the previously observed best path. This brings better stability <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b27">28]</ref> and increases the chance of finding an underutilized path <ref type="bibr" target="#b28">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Time</head><formula xml:id="formula_0">Rate R 1 0.5R 1 R 2 Estimation error T 1 T 2 Remaining size = R 1 T 1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Do not reroute Reroute</head><p>As shown in Table <ref type="table" target="#tab_6">6</ref>, with such a technique, Hermes ensures that each source can see the status of at least 3 parallel paths to its destination, which can effectively guide load balancing decisions. Meanwhile, it reduces the overhead by over 30× compared to the brute-force approach. To further reduce the overhead, Hermes picks one hypervisor under each rack as the probe agent. In each probe interval, these agents probe each other and share the probed information among all hypervisors under the same rack. This further reduces the overhead by 100×. Note that this approach can introduce inaccuracies when the last hop latencies to different hosts under the same rack are significantly different.</p><p>Overall, Hermes achieves over 300× better visibility than piggybacking. Its overhead is around 3%, which is over 3000× better than the brute-force approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Timely yet Cautious Rerouting</head><p>Even with comprehensive sensing, reacting to the perceived uncertainties is still non-trivial. As shown in §2.2.2, the challenges are two-fold: flowlet switching is passive and cannot always timely react to uncertainties, whereas vigorous rerouting adversely interferes with transport protocols. To address these challenges, Hermes employs timely yet cautious rerouting.</p><p>On the one hand, instead of passively waiting for flowlets, Hermes uses packet as the minimum switchable granularity so that it is capable of timely reacting to uncertainties. On the other hand, because vigorous rerouting can introduce congestion mismatch and packet reordering, Hermes tries to be transport-friendly by reducing the frequency of rerouting. Unlike prior solutions that perform random hashing <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b20">21]</ref>, round-robin <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24]</ref>, or always reroute to the best path <ref type="bibr" target="#b4">[5]</ref>, Hermes cautiously makes rerouting decisions based on both path conditions and flow status.</p><p>Figure <ref type="figure" target="#fig_4">6</ref> illustrates a simplified cost-benefit assessment Hermes performs before making rerouting decisions. Consider a flow that has already sent some data, and due to changes in network conditions, we need to determine whether to reroute. Rerouting to a less congested path may result in packet reordering, which in turn can halve its sending rate (R 1 → 1 2 R 1 ), assuming TCP fast recovery is triggered. In this case, whether a rerouting can improve the flow's completion time depends on many factors, such as the sending rate on the new path (R 2 ), the current path (R 1 ), and the remaining flow size. Note that it is a coarse-grained model because: (1) some metrics such as R 2 cannot be effectively measured; and (2) it does not accurately capture the cost of congestion mismatch caused by frequent rerouting. However, it highlights the need for caution and helps us identify the following heuristics for cautious rerouting.</p><p>First, considering the rate reduction caused by rerouting and the estimation error of R 2 shown in Figure <ref type="figure" target="#fig_4">6</ref>, we note that rerouting is not always beneficial if R 2 is not significantly larger than R 1 . As a result, Hermes reroutes a flow only when it finds a path with notably better condition (evaluated by an RTT threshold ∆ RT T and an ECN threshold ∆ EC N in our implementation) than the current path.</p><p>Second, it indicates that rerouting a flow with a small remaining size may bring limited benefit; this is because it may finish before its sending rate peaks. As a result, Hermes uses the size a flow already sent to estimate the remaining size <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b29">30]</ref> and reroutes flows only when the size sent exceeds a threshold S.</p><p>Finally, although we cannot accurately measure R 2 , we can measure R 1 by using CONGA's DRE algorithm <ref type="bibr" target="#b4">[5]</ref>. If R 1 is already high, the potential benefit of rerouting is likely to be marginal; moreover, the cost would be high if we reroute to a wrong path (as shown in the example of Figure <ref type="figure" target="#fig_3">4b</ref>). Therefore, we avoid rerouting a flow whose sending rate exceeds a threshold R.</p><p>To summarize, Algorithm 2 shows the rerouting logic of Hermes. It is timely triggered for every packet when (1) it belongs to a new flow or a flow experiencing failure/timeout; or (2) the current path is congested. In the former case (lines 3-12), Hermes first tries to select a good path with the least sending rate r p in order to prevent local hotspots. If it fails, Hermes further checks gray paths. If it fails again, Hermes randomly chooses an available path with no failure. For the latter case (lines 13-23), Hermes first tries to evaluate the benefit of rerouting. As discussed above, Hermes checks the sending rate r f and size sent S sent , and decides to reroute only when both conditions for cautious rerouting are met (line 14). The following rerouting procedure is similar to that in the former case, except that the selected path should be notably better than the current path (lines 15 and 19). The flow stays on its original path if no better path is found.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Parameter Settings</head><p>Hermes has a number of parameters as shown in Table <ref type="table" target="#tab_4">4</ref>. We now discuss how we set them up. Note that in this section we only provide several useful rules-of-thumb and leave (automatic) optimal parameter configuration as an important future work.</p><p>Recall that we use 3 parameters to infer congestion: T RT T _low , T EC N and T RT T _hiдh ( §3.1). T RT T _low is a threshold for good path; we set it to be 20-40µs (20µs by default) plus one-way base RTT to ensure that a good path is only lightly loaded. T EC N is an indicator for congested path, we set it to be 40% to ensure the path is heavily loaded. To set T RT T _hiдh , we use per-hop delay as a guideline. Note that each fully loaded hop introduces a relatively stable delay. This value can be calculated by EC N mar kinд thr eshold Link capacity with DCTCP <ref type="bibr" target="#b5">[6]</ref>. We set T RT T _hiдh to be base RTT plus 1.5× of the one hop delay. Such a value suggests that the path is likely to be highly loaded at more than one hop. Due to different settings, T RT T _hiдh is configured to be 300µs in our testbed and 180µs in simulations. Sensitivity analysis in §5. <ref type="bibr" target="#b3">4</ref> shows that Hermes performs well with T RT T _hiдh between 140 to 280µs in simulations.</p><p>For probing, our evaluation suggests that an interval of 100-500µs brings good performance, and we set the default value as 500µs. For rerouting, the key idea is to ensure that each rerouting is indeed necessary. ∆ RT T is the RTT threshold to ensure a path is notably better than another; we set it to be the one hop delay (80µs in simulation and 120µs in testbed) to mask potential RTT measurement inaccuracies. Similarly, we set the ECN fraction threshold ∆ EC N to be 3-10% (5% by default). Moreover, we find that setting S to be 100-800KB can avoid rerouting small flows, and improve the FCT of small flows under high load. Also, setting R to be 20-40% of the link capacity can avoid rerouting flows with high sending rates, which effectively improves the FCT of large flows. The performance of Hermes is fairly stable with the above settings, and we set S to be 600KB and R to be 30% of the link capacity by default.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">IMPLEMENTATION</head><p>We have implemented a Hermes prototype and deployed it in our testbed. We enforce the explicit routing path control at the end host using XPath <ref type="bibr" target="#b21">[22]</ref>. Note that our implementation is compatible with legacy kernel network stacks and requires no modification to switch hardware.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>End host module:</head><p>The end host module is implemented as a Linux kernel module, residing between TCP/IP stacks and Linux qdisc. It can be installed and removed while the system is running without recompiling the OS kernel. The kernel module consists of four components: a Netfilter TX hook, a Netfilter RX hook, a hash-based flow table, and a path table. Note that it operates at both TX and RX paths.</p><p>The operations in the TX path are as follows: (1) The Netfilter TX hook intercepts all outgoing packets and forwards them to the flow table ; (2) We identify the flow entry the packet belongs to and update its state (e.g., size sent, sending rate, sequence number, etc.);</p><p>(3) Then we compute the packet's path using Hermes's rerouting algorithm; (4) Note that each path in XPath <ref type="bibr" target="#b21">[22]</ref> framework can be identified by a unique IP address. So after obtaining the desired path, we add an IP header to the packet and write the desired path IP into the destination address field.</p><p>The operations in the RX path are as follows: (1) The Netfilter RX hook intercepts all incoming packets; (2) We identify the flow entry of this packet and update its flow state; (3) We identify the path this flow uses and update the path state; (4) After updating information, we remove the outer IP header and deliver this packet to the upper layer. In addition to the kernel module, we generate probe packets using a simple client-server application.</p><p>System overhead: To quantify the system overhead introduced by Hermes, we install it on a server with 4-core Intel E5-1410 2.8GHz CPU and a Broadcom BCM57810 NetXtreme II 10Gigabit Ethernet NIC. We generate more than 9Gbps of traffic with more than 5000 concurrent connections. The extra CPU overhead introduced is less than 2% compared to the case where the Hermes end host module is not running. The measured throughput remains the same in both cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Switch configuration:</head><p>We configure L3 forwarding table, ECN/RED marking, and strict priority queueing (two priorities) at the switch. Inspired by previous work <ref type="bibr" target="#b25">[26]</ref>, we classify pure ACK packets in the reverse path into the high priority queue for more accurate RTT measurements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EVALUATION</head><p>We evaluate Hermes using a combination of testbed experiments and large-scale ns-3 simulations. Our evaluation seeks to answer the following questions:</p><p>How does Hermes perform under a symmetric topology? Testbed experiments ( §5.2) show that Hermes achieves 10-38% better FCT than ECMP, outperforms CLOVE-ECN by up to 15%, and achieves comparable performance to Presto. In a large simulated topology, we show that Hermes performs close to (and slightly better than) CONGA for the web-search (and data-mining) workload ( §5.3.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>How does Hermes perform under an asymmetric topology?</head><p>Testbed experiments with a link cut show that Hermes performs <ref type="bibr" target="#b11">12</ref> How does each design component contributes and how robust is Hermes under different parameter settings? ( §5.4) We evaluate the benefits brought by good visibility and cautious yet timely rerouting separately. Results show that both components contribute to over 10% improvements in the overall outcomes. Moreover, we also find that Hermes' performance is stable under a variety of parameter settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Methodology</head><p>Transport: We use DCTCP as the default transport protocol. In our testbed, we use the DCTCP implementation in Linux kernel 3.18.11 <ref type="bibr" target="#b0">[1]</ref>. In our simulator, we implement DCTCP on top of ns-3's TCP New Reno protocol, faithfully capturing its optimizations. The initial window is 10 packets. We set both initial and minimum value of TCP RTO to 10ms. We set other parameters as suggested in <ref type="bibr" target="#b5">[6]</ref>.</p><p>Schemes compared: Besides ECMP, we compare Hermes with the following solutions:</p><p>• CONGA [simulation] We simulate CONGA following the parameter settings in <ref type="bibr" target="#b4">[5]</ref>. However, because DCTCP is less bursty than TCP, the default 500µs flowlet timeout value is too big. For a fair comparison against flowlet-based solutions, we tried different flowlet timeout values and adopted the best one (i.e., 150µs) in our simulations.</p><p>• LetFlow [simulation] We simulate LetFlow with a flowlet timeout value of 150µs (as we do for CONGA). • Presto* [testbed, simulation] Similar to <ref type="bibr" target="#b13">[14]</ref>, we implement a variant of Presto (Presto*) to isolate performance issues from those caused by packet reordering. We spray packets instead of flowcells, and implement a reordering buffer to mask packet reordering.</p><p>• CLOVE-ECN [testbed, simulation] We evaluated two versions of CLOVE: Edge-Flowlet and CLOVE-ECN via both testbed experiments and simulations. We only show the results of CLOVE-ECN since it slightly outperforms Edge-Flowlet in most cases.</p><p>We do not simulate CLOVE-INT since it requires programmable switches and it is shown to be outperformed by CONGA <ref type="bibr" target="#b23">[24]</ref>. We set a 150µs flowlet timeout value in simulations. For testbed, we pick the best flowlet timeout value (i.e., 800µs) after trying different values.</p><p>Remark: We do not compare against MPTCP <ref type="bibr" target="#b30">[31]</ref> because there is a lack of a reliable ns-3 simulation package, and the latest publicly available MPTCP Linux release suffers from performance instability <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b22">23]</ref>. Note that MPTCP suffers from incast issues and has been shown to be outperformed by CONGA in many cases similar to those we consider. Moreover, we have implemented FlowBender <ref type="bibr" target="#b22">[23]</ref> on top of DCTCP in our testbed, and we follow the default settings in <ref type="bibr" target="#b22">[23]</ref>. However, the performance is close to ECMP. One possible reason is that the workload or topology do not suit the default parameter settings. Hence, we omit the results of FlowBender to avoid unfair comparison.</p><p>Workloads: We use two widely-used realistic workloads observed from deployed datacenters: data-mining <ref type="bibr" target="#b17">[18]</ref> and web-search <ref type="bibr" target="#b5">[6]</ref>. As shown in Figure <ref type="figure" target="#fig_5">7</ref>, both distributions are heavy-tailed. Particularly, the data-mining workload is more skewed with 95% of all data bytes belonging to about 3.6% of flows that are larger than 35MB, which makes it more challenging for load balancing <ref type="bibr" target="#b4">[5]</ref>. We adopt the flow generator in <ref type="bibr" target="#b7">[8]</ref>, which generates flows between random senders and receivers under different leaf switches according to Poisson processes with varying traffic loads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metrics:</head><p>We use flow completion time (FCT) as the primary performance metric. Besides the overall average FCT, we also break down the FCT for small flows (&lt;100KB) and large flows (&gt;10MB) in some cases to better understand the results. The results are the average of 5 runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Testbed Experiments</head><p>Testbed setup: Our testbed consists of 12 servers connected to 4 switches. As shown in Figure <ref type="figure" target="#fig_6">8a</ref>, the servers are organized in two racks (6 servers each). We adopt the same topology as used in prior work <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b23">24]</ref>, and all servers and switches are connected with 1Gbps links. Given that, there is a 3:2 oversubscription at the leaf level. We also consider an asymmetric case, where we cut one of the links between a leaf switch and a spine switch as indicated in Figure <ref type="figure" target="#fig_6">8b</ref>.</p><p>Our servers have 4-core Intel E5-1410 2.8GHz CPU and a Broadcom BCM5719 NetXtreme Gigabit Ethernet NIC. All the servers run Linux kernel 3.18.11. The switches are Pronto 3295 48-port Gigabit Ethernet switch. Since the base RTT of our testbed is around 100µs, we set standard ECN marking threshold to 30KB accordingly. We use a simple client-server application to generate traffic according to the two workloads discussed above and measure the FCT on the application layer. We compare Hermes with ECMP, CLOVE-ECN, and Presto*, all of which are implementable on our testbed.</p><p>The symmetric case: Figure <ref type="figure" target="#fig_7">9</ref> shows the results for both the web-search and data-mining workloads in the symmetric case. We see similar trends in both workloads. First, we find that Hermes performs increasingly better (10-38%) compared to ECMP as the load increases. This is as expected because ECMP suffers more from hash collisions at higher loads. Moreover, Hermes performs 9-15% better at 30-70% loads compared to CLOVE-ECN. This is because Hermes has better visibility and can react to congestion more timely than flowlet-based solutions, especially under low and medium loads. Also, we observe that Hermes performs closely to Presto*, which is shown to achieve near-optimal performance in symmetric topologies <ref type="bibr" target="#b19">[20]</ref>.</p><p>Another observation is that Hermes outperforms Presto* by ∼13% at 90% load with the web-search workload. One possible reason is that our testbed is not perfectly symmetric because links may not have exactly the same capacity. As a result, Presto* may always be constrained by the bottlenecked link similar to the case shown in Example 2 of §2.2.2. Such congestion mismatch caused by imperfect symmetry may affect Presto* under very high load. In comparison, Presto* is more stable in the data-mining workload. We believe that this is due to the more bursty nature of the web-search workload, which has a smaller inter-flow arrival time and a larger number of small flows.</p><p>The asymmetric case: We repeat the above experiments for the asymmetric topology. Note that we only consider loads up to 70% relative to the symmetric case, because the bisection bandwidth is only 75% of the symmetric case.</p><p>First, we observe that the performance of ECMP deteriorates after the load exceeds 40-50%. This is because the link between  spine 1 and leaf 1 becomes fully loaded. Moreover, we observe that Hermes performs 12-30% better than CLOVE-ECN at 30-65% loadswith both workloads and among different flow size groups (Figure <ref type="figure" target="#fig_9">11</ref>). Similar to the symmetric case, this demonstrates Hermes's better visibility and more timely reaction to congestion compared with CLOVE-ECN. Note that CLOVE-ECN achieves comparable performance to Hermes at 70% load. One possible reason is that more flowlets are created at high loads, thus CLOVE-ECN can more timely converge to a balanced load.</p><p>For Presto*, we take the path asymmetry into account and assign weights for parallel paths statically to equalize the average load <ref type="bibr" target="#b13">[14]</ref>. However, we find even with topology-dependent weights, Presto* fails to match Hermes's performance. We believe that the root cause is congestion mismatch. As the load increases, different paths begin to experience different levels of congestion because the traffic matrix is not symmetric. As a result, the congestion window of Presto* is always constrained by the most congested path, and ECN signals are also mismatched among different paths. Such mismatch greatly affects the normal transport behavior (as shown in the Examples 2 and 3 of §2.2.2) and causes a dramatic increase in FCT after the load exceeds 60%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Deep Dive with Large Simulations</head><p>Due to testbed limitations, our experiments contain 4 switches, 1 Gbps links, and 1 link failure. Using simulations, we further evaluate Hermes with a larger topology, different degrees of asymmetry, and multiple types of switch failures. Specially, we look into the performance of different schemes in detail, and many of our observations echo our design rationale of timely triggering, cautious rerouting, and enhanced visibility. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Baseline.</head><p>We first inspect the performance of Hermes under a symmetric 8×8 leaf-spine topology with 128 hosts connected by 10Gbps links. We simulate a 2:1 oversubscription at the leaf level, typical of today's datacenter deployments <ref type="bibr" target="#b4">[5]</ref>. Figure <ref type="figure" target="#fig_10">12</ref> shows the overall average FCT for the web-search and data-mining workloads. For the websearch workload, Hermes outperforms ECMP by up to 55% as the traffic load increases. As a readily-deployable solution, we observe that Hermes is always within 17% of CONGA (requiring switch modifications) at all levels of loads. For the data-mining workload, Hermes is 29% better than ECMP at high load. Moreover, unlike the web-search workload, we find that Hermes can slightly outperform CONGA, by up to 4%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis:</head><p>The distinction between CONGA and Hermes under the data-mining and web-search workloads suggests the importance of visibility and timely reaction. On the one hand, CONGA outperforms Hermes for the web-search workload. One key reason is that CONGA achieves better visibility by keeping track of all the flows on a leaf switch, as shown in Table <ref type="table" target="#tab_1">2</ref>. On the other hand, Hermes performs slightly better for the data-mining workload because of its timely reaction to congestion. To explain this, note that the data-mining workload contains more large flows; hence, it is more challenging to deal with because multiple large flows may collide on one path <ref type="bibr" target="#b4">[5]</ref>. Furthermore, the data-mining workload has a much bigger inter-flow arrival time. So when there are no bursty arrivals of new flows, the packet inter-arrival time of these large flows can be quite stable. Therefore, CONGA cannot always timely resolve such bottlenecks because there are not enough flowlet gaps, as shown in the Example 1 of §2.2.2. In comparison, Hermes can timely reroute these flows once congestion is sensed and a vacant path is found.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Impact of Asymmetric Topology.</head><p>We further compare Hermes with CONGA, LetFlow, Clove-ECN and Presto* under an asymmetric topology. We adopt the baseline topology, and reduce the capacity from 10Gbps to 2Gbps for 20% of randomly selected leaf-to-spine links. Note that we normalize the FCT to Hermes in order to better visualize the results.</p><p>Under the web-search workload: As shown in Figure <ref type="figure" target="#fig_1">13</ref>, CONGA performs over 10% better than the other schemes in most cases. Hermes, CLOVE-ECN and LetFlow achieve similar performance. This is because the web-search workload contains many small flows and is also more bursty. Dynamics such as frequent flow arrival are likely to create flowlet gaps to break large flows into small sized flowlets. With enough flowlets, CLOVE-ECN and LetFlow can   To evaluate the impact of congestion mismatch, we try to mask the impact of packet reordering by implementing a reordering buffer <ref type="bibr" target="#b14">[15]</ref>.</p><p>quickly converge to a balanced load even without good visibility.</p><p>In comparison, recall that Hermes outperforms CLOVE-ECN by 9-15% with the web-search workload in testbed experiments. One possible reason is that our 1Gbps testbed has a higher RTT, thus CLOVE-ECN converges more slowly. However, excessive rerouting opportunities may negatively affect small flows without cautious rerouting. As we can see in Figure <ref type="figure" target="#fig_1">13b</ref> and<ref type="figure" target="#fig_1">13d</ref>, the average and the 99th percentile FCTs for small flows grow dramatically for flowlet-based solutions as the load increases. This is because small flows are broken into several flowlets under high loads; thus, they are heavily affected by packet reordering and congestion mismatch. In comparison, Hermes outperforms the competition by 1.5-3.3× at 90% load due to its cautious rerouting.</p><p>Under the data-mining workload: Figure <ref type="figure" target="#fig_12">14</ref> shows that Hermes outperforms CONGA by 5-10%. Compared to the baseline (Figure <ref type="figure" target="#fig_10">12b</ref>), we observe that the timely reaction of Hermes brings more obvious performance gains. This is perhaps because Hermes can effectively resolve collisions of large flows on the 2Gbps links. We also observe that Hermes is 13-20% better than CLOVE-ECN and LetFlow. This is because the data-mining workload is significantly less bursty. When there are not enough rerouting opportunities (i.e., flowlets for CLOVE-ECN and LetFlow), solutions without good visibility can hardly balance the traffic.</p><p>Validating congestion mismatch: Similar to our testbed experiments, we observe that Presto* with topology-dependent weights fails to achieve comparable performance to others under the asymmetric topology. To further validate the effect of congestion mismatch caused by vigorous rerouting, we fix the traffic load at 80% and run CONGA with different flowlet timeout gaps. We mask packet reordering to rule out their impact. As shown in Figure <ref type="figure" target="#fig_13">15</ref>, we find that reducing the flowlet timeout from 500µs to 150µs improves FCT (by ∼6%) due to more rerouting opportunities. However, further reducing the timeout value to 50µs degrades FCT by ∼30%. This indicates that even congestion-aware solutions suffer from congestion mismatch. With such a small flowlet gap, CONGA changes path vigorously. This, in turn, negatively affects the normal behavior of transport protocols as we showed in the Example 4 of §2.2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Impact of Switch</head><p>Failures. We next evaluate Hermes under failure scenarios. We adopt the baseline topology and randomly select one core switch to simulate silent random packet drop and packet blackhole <ref type="bibr" target="#b18">[19]</ref>. Note that because only 7 out of 8 core switches are working appropriately, we consider traffic loads up to 70%. We compare Hermes against CONGA, Presto*, LetFlow, and ECMP.</p><p>Silent random packet drop: To simulate the silent random packet drop scenario, we set the drop rate to 2% on a randomly selected core switch. Figure <ref type="figure" target="#fig_15">16</ref> shows the performance of different schemes.    First of all, we observe that Hermes outperforms all other schemes by over 32%; this is because it can effectively sense the failure and avoid routing through the failed switch. ECMP has 1.7-2.3× higher FCT compared to Hermes even at low loads; this is because about 1/8th of the flows traverse the failed switch using ECMP and are affected by the high packet drop rate. We also observe that CONGA performs similarly to ECMP. To explain this, note that flows traversing the failed switch tend to have a low sending rate due to frequent packet drops. Therefore, CONGA paradoxically shifts more traffic to such undesirable paths because it senses and balances traffic based on network utilization. Presto* is more heavily affected because all the flows have to go through this failed switch and thus be affected. <ref type="foot" target="#foot_2">5</ref> LetFlow is comparatively less affected because random packet drops create more rerouting opportunities on the affected paths. However, without the ability to explicitly detect and avoid failures, LetFlow still performs ∼1.5× worse than Hermes.</p><p>Packet blackhole: To simulate the packet blackhole scenario, we drop packets for half of the source-destination IP pairs from Rack 1 to Rack 8 deterministically on one randomly selected switch. Figure <ref type="figure" target="#fig_17">17</ref> shows the performance of different schemes.</p><p>As expected, Hermes can effectively detect the blackhole after 3 timeouts; hence, all the flows can finish, and Hermes achieves over 1.6× better FCT than others. For ECMP, a fixed group of flows from Rack 1 to Rack 8 will be hashed to the failed switch, which leads to a ∼1.5% of unfinished flows (Figure <ref type="figure" target="#fig_17">17b</ref>). The unfinished flows greatly enlarge the average FCT, which makes ECMP 9-22× worse than Hermes. Similar to the random drop scenario, CONGA paradoxically shifts more flows to the failed switch because it appears to be Here "without both" refers to Hermes without both probing and rerouting. less congested. This results in a higher portion of unfinished flows and higher FCT compared to ECMP. Presto* works in a round-robin manner so all the flows can finish. However, the average FCT is still greatly enlarged because all the corresponding flows are affected. Finally, we see that LetFlow performs the second best because it can timely reroute the affected flows. However, it is still over 1.6× worse than Hermes because it cannot explicitly detect and avoid failures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Performance Breakdown and Robustness</head><p>Effectiveness of probing and rerouting: We have already shown that good visibility and timely yet cautious rerouting together bring good performance. Now we further investigate the incremental benefits of some key design components, e.g., probing and rerouting, using the data-mining workload. Figure <ref type="figure" target="#fig_18">18a</ref> shows that probing and rerouting bring around 20% and 10% improvement to the overall average FCT respectively. A similar trend is observed for the large and small flows as well. This observation validates that active probing can effectively increase the visibility of end hosts, and timely rerouting can effectively resolve hotspots caused by collisions among large flows.</p><p>Impact of probe intervals: Figure <ref type="figure" target="#fig_18">18b</ref> shows that compared to Hermes without probing, a 500µs probe interval brings around 11-15% improvement, while reducing the probe interval to 100µs brings around 1-3% improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sensitivity of parameter settings:</head><p>We next study how different parameter settings affect the performance of Hermes. Figure <ref type="figure" target="#fig_19">19a</ref> and 19b show the sensitivity analysis for ∆ RT T and T RT T _hiдh . First, we observe that the FCT is relatively stable when these two parameters are set around the suggested values. Another observation is that the web-search and data-mining workloads experience different trends as T RT T _hiдh and ∆ RT T increase. To understand this, recall that the web-search workload is more bursty; so it tends to create frequent rerouting opportunities. Therefore, a conservative parameter setting (i.e., high T RT T _hiдh and ∆ RT T ) brings better performance because it can prune excessive reroutings. However, because the data-mining is less bursty, an aggressive parameter setting leads to better performance. We have also tested other parameters and observed that Hermes performs well and relatively stable around the settings suggested in §3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Different transport protocols:</head><p>We finally check the performance of Hermes with TCP under the 8×8 topology in simulation. For Hermes, we rely only on RTT to sense congestion. Since TCP is more bursty and has a larger RTT, we set ∆ RT T andT RT T _hiдh 1.5× larger and keep all the other parameters unchanged. For CONGA, we set the flowlet timout to be 500µs. Under the web-search workload, Hermes is within 10-25% of CONGA at all loads in both baseline and asymmetric topology. Under the data-mining workload, Hermes performs almost identically to CONGA in most cases (figures omitted due to space limitation). We have observed a similar trend for DCTCP in §5.3, except that CONGA performs slightly better relative to Hermes. This is because TCP is more bursty, thus more likely to create sufficient flowlet gaps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION</head><p>Hermes is not a panacea. Here, we discuss some design tradeoffs and potential deployment concerns.</p><p>End host based sensing: The choice of pushing congestion awareness to the network edge makes Hermes readily deployable and resilient to uncertainties at the same time. However, we note that visibility at end hosts, although enhanced by active probing, is still limited in comparison to switch-based solutions <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b34">35]</ref>. Better visibility often leads to better initial routing assignments, which can be especially important for small flows. For example, our evaluation shows that CONGA outperforms Hermes in the web-search workload, which has a relatively smaller average flow size.</p><p>Effectiveness of the rerouting design: Compared to flowletbased solutions, the choice of timely yet cautious rerouting is the key for faster reaction to congestion, especially when traffic is too steady to create enough flowlet gaps. As a result, Hermes outperforms flowlet-based solutions under the relatively stable datamining workload (Figure <ref type="figure" target="#fig_10">12b</ref> and<ref type="figure" target="#fig_12">14</ref>). However, when flows are small and bursty, dynamics such as frequent flow arrival are likely to create enough flowlet gaps, making flowlet-based solutions equally efficient (Figure <ref type="figure" target="#fig_1">13</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of parameters:</head><p>Hermes introduces a number of parameters to effectively sense path conditions and to make deliberate load balancing decisions. As a result, deploying Hermes requires more tuning compared to solutions with much simpler load balancing logics. At this point, we provide some rules of thumb for parameter tuning. A full exploration of the optimal parameter settings together with an automatic parameter tuning procedure would greatly simplify the deployment of Hermes. We consider it as an important future work.</p><p>Burst avoidance and stability: Hermes takes at least one RTT to sense and react to uncertainties, and thus, it does not directly handle microbursts <ref type="bibr" target="#b15">[16]</ref>. As for stability, recent congestion-aware load balancing solutions <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref> have demonstrated that stable performance can be achieved in practice as long as network state is collected at fine-grained timescales. Compared to these solutions, Hermes can more effectively prevent path oscillations and bursts caused by synchronized routing choices because: 1) Hermes leverages the power of two choices to avoid the herd behavior; and 2) Hermes does not reroute small flows and flows with a high sending rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">RELATED WORK</head><p>The literature on datacenter load balancing is vast. Among them, we only discuss some representative ones close to this work. Presto <ref type="bibr" target="#b19">[20]</ref>, DRB <ref type="bibr" target="#b11">[12]</ref> and RPS <ref type="bibr" target="#b12">[13]</ref> are per-packet/flowcell based, congestion-oblivious load balancing schemes. We have shown that they suffer from congestion mismatch under asymmetry.</p><p>CONGA <ref type="bibr" target="#b4">[5]</ref> and Expeditus <ref type="bibr" target="#b34">[35]</ref> employ congestion aware switching on specialized switch chipsets to load balance the network. HULA <ref type="bibr" target="#b24">[25]</ref> and CLOVE-INT <ref type="bibr" target="#b23">[24]</ref> leverage advanced programmable switches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11]</ref> to achieve better visibility.</p><p>LetFlow <ref type="bibr" target="#b13">[14]</ref> leverages flowlet switching to automatically converge to a balanced traffic share among parallel paths. However, we have shown that flowlets cannot always timely react to congestion under stable traffic patterns, which makes LetFlow converge slowly and achieve suboptimal performance. CLOVE-ECN <ref type="bibr" target="#b23">[24]</ref> adopts per-flowlet weighted round-robin at end hosts, where path weights are calculated based on piggybacked ECN signals. Due to its limited visibility and slow reaction to congestion, CLOVE-ECN performs up to 25% worse than Hermes under an asymmetric topology.</p><p>MPTCP <ref type="bibr" target="#b30">[31]</ref> is a transport protocol that routes several subflows concurrently over multiple paths. Because subflows do not change paths, MPTCP does not suffer from the congestion mismatch problem. However, MPTCP has some important drawbacks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref>. First, it modifies the end host networking stacks, which makes it challenging to deploy in multi-tenant environments. Moreover, it performs poorly in incast scenarios because several connections are maintained simultaneously for each flow.</p><p>DRILL <ref type="bibr" target="#b15">[16]</ref> is a switch-local, per-packet load balancing solution with a major goal of resolving micro bursts under heavy load. DRILL does not consider asymmetric topologies in their design, and it reroutes every packet vigorously with only local information. As a result, it also suffers from congestion mismatch under asymmetry.</p><p>FlowBender <ref type="bibr" target="#b22">[23]</ref> reroutes flows blindly whenever congestion is detected by end hosts. Such random and vigorous rerouting brings sub-optimal performance under high loads.</p><p>Finally, all the aforementioned schemes lack the ability to timely react to switch malfunctions, which results in significant performance degradation as we showed in §5.3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>This paper has introduced Hermes, a datacenter load balancer resilient to uncertainties such as traffic dynamics, topology asymmetry, and failures. Hermes leverages comprehensive sensing to detect uncertainties (including switch failures unattended before), and reacts by timely yet cautious rerouting. We have implemented Hermes with commodity switches and evaluated it through testbed experiments and large simulations. We demonstrated that Hermes handles uncertainties well: under asymmetries, Hermes achieves up to 10% (20%) better FCT than CONGA (CLOVE); under switch failures, it outperforms all other schemes by over 32%.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: [Example 1] Flowlet switching cannot timely react to congestion by splitting flows under stable traffic pattern.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: [Example 2] Congestion mismatch results in severe throughput loss and queue oscillation under asymmetric topologies with Presto.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: [Example 4] The hidden terminal scenario: (a) causes flow A to flip between spine S0 and S1 with stale information; (b) causes sudden queue build-up every time when flow A reroutes through S1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: A simplified model to assess the cost and benefit of rerouting. Whether rerouting will shorten a flow's completion time depends on factors such as the rate difference between the new (R 2 ) and current (R 1 ) paths, the gradient of rate change, and the flow size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Traffic distributions used for evaluation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: [Testbed] Topology used for testbed experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: [Testbed] Overall avg FCT in the symmetric case.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: [Testbed] Overall avg FCT in the asymmetric case.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: [Testbed] Web-search workload statistics in the asymmetric case. Note that for large flows we normalize the FCT to Hermes to better visualize the results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: [Simulation] Overall avg FCT (baseline topology).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Figure 13: [Simulation] FCT statistics for the web-search workload in the asymmetric topology (normalized to Hermes).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: [Simulation] FCT statistics for the data-mining workload in the asymmetric topology (normalized to Hermes).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: [Simulation] CONGA with different flowlet timeout values (web-search).To evaluate the impact of congestion mismatch, we try to mask the impact of packet reordering by implementing a reordering buffer<ref type="bibr" target="#b14">[15]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 16 :</head><label>16</label><figDesc>Figure 16: [Simulation] Performance with random packet drops (web-search).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 17 :</head><label>17</label><figDesc>Figure 17: [Simulation] Performance with packet blackhole (web-search).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 18 :</head><label>18</label><figDesc>Figure 18: [Simulation] Hermes deep dive (data-mining).Here "without both" refers to Hermes without both probing and rerouting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 19 :</head><label>19</label><figDesc>Figure 19: [Simulation] Sensitivity to T RT T _hiдh and ∆ RT T .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Summary of prior work under uncertainties. Note that unlike link failures that directly cause topology asymmetry, here switch failure refers to malfunctions such as packet blackholes and silent random packet drops.</figDesc><table><row><cell>Hermes</cell><cell cols="2">Global awareness (End host)</cell><cell>Aware</cell><cell>Packet</cell><cell>Timely yet cautious rerouting (based on global congestion and failure)</cell><cell>No</cell></row><row><cell>Workload</cell><cell cols="4">Data-mining Data-mining Web-search Web-search 60% load 80% load 60% load 80% load</cell><cell></cell></row><row><cell>Switch pair</cell><cell>1.725</cell><cell>2.344</cell><cell>4.173</cell><cell>5.859</cell><cell></cell></row><row><cell>End host pair</cell><cell>0.007</cell><cell>0.009</cell><cell>0.016</cell><cell>0.022</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note><p>The average number of concurrent flows observed on parallel paths between ToR-to-ToR, host-to-host pairs.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Number of timeout events of a path f r et r ansmission Fraction of retransmission events of a path r</figDesc><table><row><cell>Network Traffic</cell><cell>End host Sensing Congestion Active Probing Sensing Module Probe (Re)Routing Module Sensing Failures When &amp; Where to reroute? Hypervisor Feed Trigger</cell></row><row><cell></cell><cell>(Re)Route</cell></row><row><cell></cell><cell>Figure 5: Hermes overview.</cell></row><row><cell cols="2">Flow-level Variable</cell></row><row><cell>r f</cell><cell>Sending rate of a flow</cell></row><row><cell>s sent</cell><cell>Size sent of a flow, used to estimate the remaining size</cell></row><row><cell>i f t imeout</cell><cell>Set if a flow experiences a timeout</cell></row><row><cell cols="2">Path-level Variable</cell></row><row><cell>f EC N</cell><cell>Fraction of ECN-marked packets of a path</cell></row><row><cell>t RT T</cell><cell>RTT measurement of a path</cell></row><row><cell>n t imeout</cell><cell></cell></row></table><note><p>p Aggregate sending rate of all flows of a path ( r f ) type Characterization of path condition</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Variables in Hermes.</figDesc><table /><note><p><p>Parameter</p>Recommended Setting T EC N : threshold for fraction of ECN 40% T RT T _low : threshold for low RTT 20 -40µs + base RTT T RT T _hiдh : threshold for high RTT 1.5×one hop delay + base RTT ∆ RT T : threshold for notably better RTT one hop delay ∆ EC N : threshold for notably better ECN fraction 3-10% R: the highest flow sending rate threshold for rerouting 20-40% of the link capacity S: the smallest flow sent size threshold for rerouting 100 -800KB Probe interval 100 -500µs</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Parameters in Hermes and recommended settings.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>EC N &lt; T EC N and t RT T &lt; T RT T _l ow then EC N &gt; T EC N and t RT T &gt; T RT T _hiдh then</figDesc><table><row><cell>5</cell><cell cols="2">type = conдest ed</cell><cell></cell></row><row><cell>6</cell><cell>else</cell><cell></cell><cell></cell></row><row><cell>7</cell><cell cols="2">type = дr ay</cell><cell></cell></row><row><cell>ECN</cell><cell>RTT</cell><cell>Possible Cause</cell><cell>Characterization</cell></row><row><cell>High</cell><cell>High</cell><cell>Congested</cell><cell>Congested path</cell></row><row><cell cols="2">High Moderate/Low</cell><cell>Not enough ECN samples or all delay is built up at one hop</cell><cell></cell></row><row><cell>Low</cell><cell>High</cell><cell>Not enough ECN samples or the network stack incurs high RTT</cell><cell>Gray path</cell></row><row><cell>Low</cell><cell>Moderate</cell><cell>Moderately loaded</cell><cell></cell></row><row><cell>Low</cell><cell>Low</cell><cell>Underutilized</cell><cell>Good path</cell></row></table><note><p>3.1.1 Sensing Congestion. Hermes leverages both RTT and ECN to detect path conditions: Algorithm 1: Hermes Path Characterization 1 for each path p do 2 if f 3 type = дood 4 else if f 8 if (n t imeout &gt; 3 and no packet is ACKed) or (f r e t r ansmis s ion &gt; 1% and type conдest ed ) then 9 type = f ail ed</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Table 5  summarizes the outcome of the algorithm and reasons behind. Comparison of different probing schemes in terms of visibility and corresponding overhead. Recall that visibility is quantified as the average number of concurrent flows a sender can observe on parallel paths to each end host ( §2.2.1), and overhead is defined as the sending rate of the extra probe traffic introduced over the edge-leaf link capacity.</figDesc><table><row><cell>Scheme</cell><cell cols="4">Piggy-back Brute-force Power of two Hermes ([23, 24]) Probing Choices</cell></row><row><cell>Visibility</cell><cell>&lt; 0.01</cell><cell>100</cell><cell>&gt;3</cell><cell>&gt;3</cell></row><row><cell>Overhead</cell><cell>NA</cell><cell>100×</cell><cell>3×</cell><cell>3%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Algorithm 2 :</head><label>2</label><figDesc>Timely yet Cautious Rerouting 1 for every packet do Assume its corresponding flow is f and path is p 3 if f is a new flow or f .if t imeout ==t r ue or p .type == f ail ed then Ar дmin p ∈{p ′ } (p .r p ) Ar дmin p ∈{p ′′ } (p .r p )</figDesc><table><row><cell>4</cell><cell>{p ′ } = all good paths</cell></row><row><cell>5</cell><cell>if {p ′ } ∅ then</cell></row><row><cell></cell><cell>/* Select a good path with the smallest</cell></row><row><cell></cell><cell>local sending rate</cell><cell>*/</cell></row><row><cell>7</cell><cell>else</cell></row><row><cell>8</cell><cell>{p ′′ } = all gray paths</cell></row><row><cell>9</cell><cell>if {p ′′ } ∅ then</cell></row><row><cell>11</cell><cell>else</cell></row><row><cell>12</cell><cell>p</cell></row></table><note><p><p>2 6 p * = 10 p * = * = a randomly selected path with no failure</p>13 else if p .type == conдest ed then 14 if f .s s e nt &gt; S and f .r f &lt; R then 15 {p ′ } = all good paths notably better than p /* ∀p ′ ∈ {p ′ }, we have p .t RT T -p ′ .t RT T &gt; ∆ RT T and p .f EC N -p ′ .f EC N &gt; ∆ EC N */ 16 if {p ′ } ∅ then 17 p * = Ar дmin p ∈{p ′ } (p .r p ) 18 else 19 {p ′′ } = all gray paths notably better than p 20 if {p ′′ } ∅ then 21 p * = Ar дmin p ∈{p ′′ } (p .r p ) 22 else 23 p * = p /* Do not reroute */ 24 return p * /* The new routing path */</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>We note that existing diagnostic tools<ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b31">32]</ref> take at least 10s of seconds to detect and locate switch failures. As a result, they cannot be directly adopted by Hermes as load balancing requires fast reactions to these failures.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>This herd behavior caused by delayed information update is elaborated in<ref type="bibr" target="#b26">[27]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>Note that we observe good average FCT for small flows in case of Presto*. One possible reason is that Presto* has a lower network utilization on all the paths, as all large flows are heavily affected and cannot send at high rates.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work is supported in part by the Hong Kong RGC ECS-26200014, GRF-16203715, GRF-613113, CRF-C703615G, and the China 973 Program No.2014CB340303. Mosharaf Chowdhury was supported in part by NSF grants CNS-1617773, CCF-1629397, and CNS-1563095. We are thankful to Zhouwang Fu for his help in testbed evaluation. We would also like to thank our shepherd, Dina Papagiannaki, and the anonymous SIGCOMM reviewers for their valuable feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<ptr target="http://kernelnewbies.org/Linux3.18" />
	</analytic>
	<monogr>
		<title level="j">DCTCP in Linux Kernel</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<ptr target="http://p4.org/wp-content/uploads/fixed/INT/INT-current-spec.pdf" />
		<title level="m">Network Telemetry (INT)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Commodity Data Center Network Architecture</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Al-Fares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Loukissas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amin</forename><forename type="middle">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><surname>Scalable</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGCOMM</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Hedera: Dynamic Flow Scheduling for Data Center Networks</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Al-Fares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sivasankar</forename><surname>Radhakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barath</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nelson</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amin</forename><surname>Vahdat</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">CONGA: Distributed Congestion-Aware Load balancing for Datacenters</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Alizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Edsall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarang</forename><surname>Dharmapurikar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramanan</forename><surname>Vaidyanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Fingerhut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><surname>Matus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGCOMM</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>Rong Pan, Navindra Yadav, George Varghese, and others</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Alizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Maltz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Padhye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parveen</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balaji</forename><surname>Prabhakar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudipta</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murari</forename><surname>Sridharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGCOMM 2010</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Information-Agnostic Flow Scheduling for Commodity Data Centers</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongsu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Enabling ECN in Multi-Service Multi-Queue Data Centers</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haitao</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Enabling End-host Network Functions</title>
		<author>
			<persName><forename type="first">Hitesh</forename><surname>Ballani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Gkantsidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Matthew P Grosvenor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lazaros</forename><surname>Karagiannis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg O'</forename><surname>Koromilas</surname></persName>
		</author>
		<author>
			<persName><surname>Shea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGCOMM</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Surviving Failures in Bandwidth-Constrained Datacenters</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bodík</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishai</forename><surname>Menache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mosharaf</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeepkumar</forename><surname>Mani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Maltz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGCOMM</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Pat</forename><surname>Bosshart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Glen</forename><surname>Gibb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Izzard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Rexford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cole</forename><surname>Schlesinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Talayco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amin</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Varghese</surname></persName>
		</author>
		<author>
			<persName><surname>Others</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Programming Protocol-Independent Packet Processors. SIGCOMM CCR</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="87" to="95" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Per-packet Load-balanced, Low-latency Routing for Clos-based Data Center Networks</title>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengkun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanxiong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guohan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihua</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haitao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongqiang</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dave</forename><surname>Maltz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNEXT</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On the Impact of Packet Spraying in Data Center Networks</title>
		<author>
			<persName><forename type="first">Advait</forename><surname>Dixit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pawan</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charlie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramana</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kompella</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INFOCOM</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Let it FLow: Resilient Asymmetric Load Balancing with Flowlet Switching</title>
		<author>
			<persName><forename type="first">Pan</forename><surname>Vanini Erico</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alizadeh</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taheri</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edsall</forename><surname>Parvin</surname></persName>
		</author>
		<author>
			<persName><surname>Tom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">JUGGLER: A Practical Reordering Resilient Network Stack for Datacenters</title>
		<author>
			<persName><forename type="first">Yilong</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vimalkumar</forename><surname>Jeyakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdul</forename><surname>Kabbani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Alizadeh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Micro Load Balancing in Data Centers with DRILL</title>
		<author>
			<persName><forename type="first">Soudeh</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brighten</forename><surname>Godfrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yashar</forename><surname>Ganjali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amin</forename><surname>Firoozshahian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Understanding Network Failures in Data Centers: Measurement, Analysis, and Implications</title>
		<author>
			<persName><forename type="first">Phillipa</forename><surname>Gill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navendu</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nachiappan</forename><surname>Nagappan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGCOMM 2011</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">VL2: A Scalable and Flexible Data Center Network</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">R</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navendu</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srikanth</forename><surname>Kandula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changhoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parantap</forename><surname>Lahiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Maltz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parveen</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudipta</forename><surname>Sengupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGCOMM</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pingmesh: A Large-Scale System for Data Center Network Latency Measurement and Analysis</title>
		<author>
			<persName><forename type="first">Chuanxiong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihua</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingnong</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ray</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dave</forename><surname>Maltz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi-Wei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varugis</forename><surname>Kurien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGCOMM</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Presto: Edge-based Load Balancing for Fast Datacenter Networks</title>
		<author>
			<persName><forename type="first">Keqiang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Rozner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kanak</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wes</forename><surname>Felter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Akella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGCOMM</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Analysis of an Equal-Cost Multi-Path Algorithm</title>
		<author>
			<persName><surname>Ce Hopps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RFC 2992</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Explicit Path Control in Commodity Data Centers: Design and Applications</title>
		<author>
			<persName><forename type="first">Shuihai</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haitao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongze</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanxiong</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NSDI</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">FlowBender: Flow-level Adaptive Routing for Improved Latency and Throughput in Datacenter Networks</title>
		<author>
			<persName><forename type="first">Abdul</forename><surname>Kabbani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balajee</forename><surname>Vamanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jahangir</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabien</forename><surname>Duchene</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">CLOVE: How I Learned to Stop Worrying about the Core and Love the Edge</title>
		<author>
			<persName><forename type="first">Naga</forename><surname>Katta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mukesh</forename><surname>Hira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditi</forename><surname>Ghag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changhoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isaac</forename><surname>Keslassy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Rexford</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">HULA: Scalable Load Balancing Using Programmable Data Planes</title>
		<author>
			<persName><forename type="first">Naga</forename><surname>Katta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mukesh</forename><surname>Hira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changhoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Sivaraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Rexford</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">TIMELY: RTT-based Congestion Control for the Datacenter</title>
		<author>
			<persName><forename type="first">Radhika</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nandita</forename><surname>Dukkipati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Blem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hassan</forename><surname>Wassel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monia</forename><surname>Ghobadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amin</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaogong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wetherall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Zats</surname></persName>
		</author>
		<author>
			<persName><surname>Others</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGCOMM</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">How Useful Is Old Information?</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Mitzenmacher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPDS</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="6" to="20" />
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The Power of Two Choices in Randomized Load Balancing</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Mitzenmacher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPDS</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1094" to="1104" />
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Load Balancing with Memory</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Mitzenmacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balaji</forename><surname>Prabhakar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devavrat</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FOCS</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">The Fundamentals of Heavy-tails: Properties, Emergence, and Identification</title>
		<author>
			<persName><forename type="first">Jayakrishnan</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Wierman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bert</forename><surname>Zwart</surname></persName>
		</author>
		<idno>SIGMET- RICS 2013</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Improving Datacenter Performance and Robustness with Multipath TCP</title>
		<author>
			<persName><forename type="first">Costin</forename><surname>Raiciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastien</forename><surname>Barre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Pluntke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Greenhalgh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damon</forename><surname>Wischik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Handley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGCOMM 2011</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Passive Realtime Datacenter Fault Detection and Localization</title>
		<author>
			<persName><forename type="first">Arjun</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jasmeet</forename><surname>Bagga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Snoeren</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Harnessing TCP&apos;s Burstiness with Flowlet Switching</title>
		<author>
			<persName><forename type="first">Shan</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srikanth</forename><surname>Kandula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dina</forename><surname>Katabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HotNets</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deadline-Aware Datacenter TCP (D2TCP)</title>
		<author>
			<persName><forename type="first">Balajee</forename><surname>Vamanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jahangir</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName><surname>Vijaykumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGCOMM</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Expeditus: Congestion-aware Load Balancing in Clos Data Center Networks</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhixiong</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongsu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongqiang</forename><surname>Xiong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
