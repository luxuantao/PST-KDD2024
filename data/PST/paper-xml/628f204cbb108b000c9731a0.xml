<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Differential Equations for Learning to Program Neural Nets Through Continuous Learning Rules</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kazuki</forename><surname>Irie</surname></persName>
							<email>kazuki@idsia.ch</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The Swiss AI Lab</orgName>
								<orgName type="institution" key="instit2">IDSIA</orgName>
								<orgName type="institution" key="instit3">USI &amp; SUPSI</orgName>
								<address>
									<settlement>Lugano</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Francesco</forename><surname>Faccio</surname></persName>
							<email>francesco@idsia.ch</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The Swiss AI Lab</orgName>
								<orgName type="institution" key="instit2">IDSIA</orgName>
								<orgName type="institution" key="instit3">USI &amp; SUPSI</orgName>
								<address>
									<settlement>Lugano</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The Swiss AI Lab</orgName>
								<orgName type="institution" key="instit2">IDSIA</orgName>
								<orgName type="institution" key="instit3">USI &amp; SUPSI</orgName>
								<address>
									<settlement>Lugano</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">AI Initiative</orgName>
								<orgName type="institution" key="instit2">KAUST</orgName>
								<address>
									<settlement>Thuwal</settlement>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Differential Equations for Learning to Program Neural Nets Through Continuous Learning Rules</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural ordinary differential equations (ODEs) have attracted much attention as continuous-time counterparts of deep residual neural networks (NNs), and numerous extensions for recurrent NNs have been proposed. Since the 1980s, ODEs have also been used to derive theoretical results for NN learning rules, e.g., the famous connection between Oja's rule and principal component analysis. Such rules are typically expressed as additive iterative update processes which have straightforward ODE counterparts. Here we introduce a novel combination of learning rules and Neural ODEs to build continuous-time sequence processing nets that learn to manipulate short-term memory in rapidly changing synaptic connections of other nets. This yields continuous-time counterparts of Fast Weight Programmers and linear Transformers. Our novel models outperform the best existing Neural Controlled Differential Equation based models on various time series classification tasks, while also addressing their scalability limitations. Our code is public. 1  </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural ordinary differential equations (NODEs) <ref type="bibr" target="#b0">[1]</ref> have opened a new perspective on continuoustime computation with neural networks (NNs) as a practical framework for machine learning based on differential equations. While the original approach-proposed as a continuous-depth version of deep feed-forward residual NNs <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>-only covers autonomous ODEs entirely determined by the initial conditions, more recent extensions deal with sequential data (reviewed in Sec. 2.1) in a way similar to what is typically done with standard recurrent NNs (RNNs) in the discrete-time scenario. This potential for continuous-time (CT) sequence processing (CTSP) is particularly interesting, since there are many applications where datapoints are observed at irregularly spaced time steps, and CT sequence models might better deal with such data than their discrete-time counterparts. However, the development of NODEs for CTSP is still at an early stage. For example, a popular approach of Neural Controlled Differential Equations <ref type="bibr" target="#b3">[4]</ref> (NCDEs; also reviewed in Sec. 2.1) has in practice only one architectural variant corresponding to the "vanilla" RNN <ref type="bibr" target="#b4">[5]</ref>. Discrete-time processing, however, exploits many different RNN architectures as well as Transformers <ref type="bibr" target="#b5">[6]</ref>.</p><p>While it is not straightforward to transform the standard Transformer into a CT sequence processor, we'll show that the closely related Fast Weight Programmers (FWPs) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref> and linear Transformers <ref type="bibr" target="#b9">[10]</ref> (reviewed in Sec. 2.3) have direct CT counterparts. In FWPs, temporal processing of shortterm memory (stored in fast weight matrices) uses learnable sequences of learning rules. Hence CT versions of FWPs will require differential equations to model the learning rules. This relates to an ancient trend of the 1980s/90s. Among many old connections between NNs and dynamical systems described by ODEs (e.g., <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>), the theoretical analysis of NN learning rules in the ODE framework has been particularly fruitful. Consider the famous example of Oja's rule <ref type="bibr" target="#b16">[17]</ref> (briefly reviewed in Sec. 2.2): many results on its stability, convergence, and connection to Principle Component Analysis <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> were obtained using its ODE counterpart (e.g., <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>).</p><p>Here we propose a novel combination of Neural ODEs and learning rules, to obtain a new class of sequence processing Neural ODEs which are continuous-time counterparts of Fast Weight Programmers and linear Transformers. To the best of our knowledge, there is no previous work on Neural ODE-based Transformer families, despite their dominance in important types of discrete time computations such as Natural Language Processing and beyond. We also show how our approach solves the fundamental limitation of existing Neural CDEs in terms of model size scalability.</p><p>We conduct experiments on three standard time series classification tasks covering various scenarios (regularly sampled, irregularly sampled with missing values, and very long time series). We demonstrate that our novel models outperform existing Neural ODE-based sequence processors, in some cases by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>We briefly review the main background concepts this work builds upon: NODEs for sequence processing (Sec. 2.1), NN learning rules and their connection to ODEs (Sec. 2.2), and Fast Weight Programmers whose memory update is based on learning rules controlled by an NN (Sec. 2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Neural ODEs (NODEs) and Their Extensions for Sequence Processing</head><p>Here we review the core idea of NODEs <ref type="bibr" target="#b0">[1]</ref>. In what follows, let n, N , d, d in denote positive integers, T be a positive real number, and θ denote an arbitrary set of real numbers. The first step is to consider a residual layer (say, the n-th layer with a dimension d) in an N -layer deep NN which transforms an input h n−1 ∈ R d to an output h n ∈ R d with a parameterised function f θ : R d → R d as follows:</p><formula xml:id="formula_0">h n = h n−1 + f θ (h n−1 )<label>(1)</label></formula><p>This coincides <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b0">1]</ref> with the following equation for = 1</p><formula xml:id="formula_1">h(t n ) = h(t n−1 ) + f θ (h(t n−1 ))<label>(2)</label></formula><p>where h</p><formula xml:id="formula_2">: [0, T ] → R d is a function such that h(t n ) = h n holds for all n : 0 ≤ n ≤ N and t n ∈ [0, T ] such that t n − t n−1 = &gt; 0 if n ≥ 1.</formula><p>This equation is a forward Euler discretisation of the ordinary differential equation defined for all t ∈ (t 0 , T ] as</p><formula xml:id="formula_3">h (t) = f θ (h(t)) or h(t) = h(t 0 ) + t s=t0 f θ (h(s))ds<label>(3)</label></formula><p>where h denotes the first order derivative. This establishes the connection between the ODE and the deep residual net with parameters θ shared across layers<ref type="foot" target="#foot_1">2</ref> : given the initial condition h(t 0 ) = h 0 , the solution to this equation evaluated at time T , i.e., h(T ), corresponds to the output of this deep residual NN, which can be computed by an ODE solver. We denote it as a function ODESolve taking four variables: h(T ) = ODESolve(f θ , h 0 , t 0 , T ). During training, instead of backpropagating though the ODE solver's operations, the adjoint sensitivity method <ref type="bibr" target="#b33">[34]</ref> (which essentially solves another ODE but backward in time) can compute gradients with O(d) memory requirement, constant w.r.t. T <ref type="bibr" target="#b0">[1]</ref>.</p><p>A natural next step is to extend this formulation for RNNs, i.e., the index n now denotes the time step, and we assume an external input x n ∈ R din at each step n to update the hidden state h n−1 to h n as</p><formula xml:id="formula_4">h n = f θ (h n−1 , x n )<label>(4)</label></formula><p>Depending on the property of external inputs (x n ) N n=1 = (x 1 , ..., x N ), there are different ways of defining NODEs for sequence processing. We mainly distinguish three cases.</p><p>First, when there is a possibility to construct a differentiable control signal x : t → x(t) ∈ R din for t ∈ [t 0 , T ] from the inputs (x n ) N n=1 ; an attractive approach by Kidger et al. <ref type="bibr" target="#b3">[4]</ref> handles the corresponding dynamics in a neural controlled differential equation (NCDE):</p><formula xml:id="formula_5">h(t) = h(t 0 ) + t s=t0 F θ (h(s))dx(s) = h(t 0 ) + t s=t0 F θ (h(s))x (s)ds<label>(5)</label></formula><p>where F θ is a parameterised function (typically a few-layer NN) which maps a vector h(s) ∈ R d to a matrix F θ (h(s)) ∈ R d×din (we'll discuss how this component already relates to Fast Weight Programmers) and thus, F θ (h(s))dx(s) denotes a matrix-vector multiplication. The control x : [t 0 , T ] → R din is typically <ref type="bibr" target="#b3">[4]</ref> constructed via natural cubic spline over all data points (x n ) N n=1 such that its differentiability is guaranteed (which, however, makes it incompatible with autoregressive processing <ref type="bibr" target="#b34">[35]</ref>). Since the final equation is again an NODE with a vector field of form g θ,x (s, h(s)) = F θ (h(s))x (s), all methods described above are applicable: ODE solver for evaluation and adjoint method for memory efficient training. A notable extension of Neural CDEs is the use of log-signatures to sub-sample the input sequence <ref type="bibr" target="#b35">[36]</ref>. Resulting NCDEs are called neural rough differential equations (NRDEs), which are relevant for processing long sequences.</p><p>On a side note, the NCDE is often referred to as the "continuous-time analogue" to RNNs <ref type="bibr" target="#b3">[4]</ref>, but this is a bit misleading: discrete-time RNN equations corresponding to the continuous-time Eq. 5 do not reflect the standard RNN of Eq. 4 but:</p><formula xml:id="formula_6">h n = h n−1 + W n−1 (x n − x n−1 ) (6) W n = F θ (h n )<label>(7)</label></formula><p>where one network (Eq. 6) learns to translate the variation of inputs (x n − x n−1 ) into a change in the state space using a weight matrix W n−1 which itself is generated by another network (F θ : R d → R d×din ; Eq. 7) on the fly from the hidden state. This model is thus a kind of Recurrent FWP <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref>. Second, even if x is not differentiable, having access to x defined and bounded over an interval of interest [t 0 , T ] is enough to define a sequence processing NODE, by making it part of the vector field:</p><formula xml:id="formula_7">h(t) = h(t 0 ) + t s=t0 f θ (h(s), x(s))ds<label>(8)</label></formula><p>where the vector field f θ (h(t), x(t)) = g θ,x (t, h(t)) can effectively be evaluated at any time t ∈ [t 0 , T ]. We refer to this second approach as a direct NODE method. While Kidger et al. <ref type="bibr" target="#b3">[4]</ref> theoretically and empirically show that this approach is less expressive than the NCDEs above, we'll show how in our case of learning rules one can derive interesting models within this framework, which empirically perform on par with the CDE variants.</p><p>Finally, when no control function is available (i.e., we only have access to x n at discrete time steps (t n ) N n=1 ), a mainstream approach consists in dissociating the continuous-time hidden state update via ODE for the time between two observations (e.g., Eq. 9 below) from integration of the new data (Eq. 10 below). Notable examples of this category include ODE-RNNs <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40]</ref> which transform the hidden states h n−1 to h n for each observation x n available at time t n as follows:</p><formula xml:id="formula_8">u n = ODESolve(f θ1 , h n−1 , t n−1 , t n ) (9) h n = φ θ2 (x n , u n ) (<label>10</label></formula><formula xml:id="formula_9">)</formula><p>where Eq. 9 autonomously updates the hidden state between two observations using a function f θ1 parameterised by θ 1 , while in Eq. 10, function φ θ2 parameterised by θ 2 integrates the new input x n into the hidden state. In Latent ODE-RNN <ref type="bibr" target="#b38">[39]</ref>, a popular extension of this approach to the variational setting, the initial recurrent state h 0 is sampled from a prior (during training, an additional encoder is trained to map sequences of inputs to parameters of the prior). While this third case is not our focus, we'll also present how to use FWPs in this scenario in Sec. 3.3 for the sake of completeness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Learning Rules and Their Connections to ODEs</head><p>Learning rules of artificial NNs describe the process which modifies their weights in response to some inputs. This includes the standard backpropagation rule (also known as the reverse mode of automatic differentiation) derived for the case of supervised learning, as well as rules inspired by Hebb's informal rule <ref type="bibr" target="#b40">[41]</ref> in "unsupervised" settings. Here we focus on the latter. Let n, d in d out be positive integers. Given a linear layer with a weight matrix W n ∈ R dout×din (the single output neuron case d out = 1 is the focus of the classic works) at time n which transforms input x n ∈ R din to output y n ∈ R dout as y n = W n−1 x n (11) the pure Hebb-style additive learning rule modifies the weights according to</p><formula xml:id="formula_10">W n = W n−1 + η n y n ⊗ x n<label>(12</label></formula><p>) where ⊗ denotes outer product and η n ∈ R + is a learning rate at time n.</p><p>Oja <ref type="bibr" target="#b16">[17]</ref> proposed stability improvements to this rule through a decay term</p><formula xml:id="formula_11">W n = W n−1 + η n y n ⊗ (x n − W n−1 y n )<label>(13</label></formula><p>) whose theoretical analysis has since the 1980s been a subject of many researchers covering stability, convergence, and relation to Principal Component Analysis <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b41">42]</ref>. One key approach for such theoretical analysis is to view the equation above as a discretisation of the following ODE:</p><formula xml:id="formula_12">W (t) = η(t)y(t) ⊗ (x(t) − W (t − 1) y(t))<label>(14)</label></formula><p>On a related note, the study of RNNs has also profited from ODEs <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Fast Weight Programmers &amp; Linear Transformers</head><p>Fast Weight Programmers (FWP; <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>) are general-purpose (auto-regressive) sequence processing NNs. In general, an FWP is a system of two NNs: a slow NN, the programmer, rapidly generates during runtime weight changes of another neural network, the fast NN. The (slow) weights of the slow net are typically trained by gradient descent. Variants of FWPs whose weight generation is based on outer products between keys and values <ref type="bibr" target="#b6">[7]</ref> have been shown <ref type="bibr" target="#b7">[8]</ref> to be equivalent to Linear Transformers <ref type="bibr" target="#b9">[10]</ref> (using the mathematical equivalence known from perceptron/kernel machine duality <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46]</ref>). These FWPs use sequences of learning rules to update short-term memory in form of a fast weight matrix. A practical example of such FWPs is the DeltaNet <ref type="bibr" target="#b7">[8]</ref> which transforms an input x n ∈ R din into an output y n ∈ R dout at each time step n while updating its fast weight matrix W n−1 ∈ R dout×dkey as follows:</p><formula xml:id="formula_13">β n , q n , k n , v n = W slow x n (15) W n = W n−1 + σ(β n )(v n − W n−1 φ(k n )) ⊗ φ(k n ) (16) y n = W n φ(q n ) (<label>17</label></formula><formula xml:id="formula_14">)</formula><p>where the slow net (Eq. 15; with weights W slow ∈ R (1+2 * dkey+dout)×din ) generates vectors key k n ∈ R dkey and value v n ∈ R dout as well as a scalar β n ∈ R to obtain a dynamic learning rate by applying a sigmoid function σ, and φ is an element-wise activation function whose output elements are positive and sum up to one (typically softmax). These fast dynamic variables generated by a slow NN are used in a learning rule (Eq. 16) akin to the classic delta rule <ref type="bibr" target="#b46">[47]</ref> to update the fast weight matrix. The output is finally produced by the forward computation of the fast NN, i.e., by querying the fast weight matrix by the generated query vector q n ∈ R dkey (Eq. 17). An intuitive interpretation of the fast weight matrix is a key-value associative memory with write and read operations defined by Eq. 16 and 17, respectively. This encourages intuitive thoughts about memory capacity (limited by the number of "keys" we can store without interference) <ref type="bibr" target="#b7">[8]</ref>. For instance, if we replace the learning rule (i.e., memory writing operation) of Eq. 16 by a pure additive Hebb-style rule (and a fixed learning rate of 1.0):</p><formula xml:id="formula_15">W n = W n−1 + v n ⊗ φ(k n ),</formula><p>we obtain the Linear Transformer <ref type="bibr" target="#b9">[10]</ref> (we refer to prior work <ref type="bibr" target="#b7">[8]</ref> for further explanations of the omission of attention normalisation). Such a purely additive learning rule often suffers from long term dependencies, unlike the delta rule <ref type="bibr" target="#b7">[8]</ref>. We'll confirm this trend also in the CT models (using the EigenWorms dataset). For later convenience, we introduce a function FWP which denotes generic FWP operations:</p><formula xml:id="formula_16">y n , W n = FWP(x n , W n−1 ; W slow ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Continuous-Time Fast Weight Programmers</head><p>We propose continuous-time counterparts of Fast Weight Programmers (Sec. 2.3) which naturally combine ODEs for learning rules (Sec. 2.2) and existing approaches for sequence processing with NODEs (Sec. 2.1). We present three types of these CT FWP models in line with the categorisation of Sec. 2.1 while the main focus of this work is on the two first cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Direct NODE-based FWPs</head><p>In the direct NODE approach (reviewed in Sec. 2.1), we assume a (bounded) control signal x : t → x(t) defined over an interval [t 0 , T ]. We make it part of the vector field to define an ODE describing a continuous-time learning rule for a fast weight matrix W (t):</p><formula xml:id="formula_17">W (t) = W (t 0 ) + t s=t0 F θ (W (s), x(s))ds<label>(18)</label></formula><p>where W : t → W (t) ∈ R dout×dkey is a function defined on [t 0 , T ], and F θ is an NN parameterised by θ which maps onto R dout×dkey . This is a neural differential equation for learning to train a neural net through continuous learning rules, that is, to train a fast weight matrix W (t) of a fast NN (Eq. 20 below) for each sequential control x. Like in the discrete-time FWPs (Sec. 2.3), the output y(T ) ∈ R dout is obtained by querying this fast weight matrix<ref type="foot" target="#foot_2">3</ref> (e.g., at the last time step T ):</p><formula xml:id="formula_18">q(T ) = W q x(T ) (19) y(T ) = W (T )q(T )<label>(20)</label></formula><p>where W q ∈ R dkey×din is a slow weight matrix used to generate the query q(T ) ∈ R dkey (Eq. <ref type="bibr" target="#b18">19</ref>). Now we need to specify F θ in Eq. 18 to fully define the learning rule. We focus on three variants:</p><formula xml:id="formula_19">F θ (W (s), x(s)) = σ(β(s))    k(s) ⊗ v(s) Hebb-style v(s) ⊗ k(s) − W (s) v(s) Oja-style v(s) − W (s)k(s) ⊗ k(s) Delta-style<label>(21)</label></formula><p>where [β(s), k(s), v(s)] = W slow x(s) with a slow weight matrix W slow ∈ R (1+dkey+dout)×din . As in the discrete-time FWP (Sec. 2.3), the slow NN generates β(s) ∈ R (to which we apply the sigmoid function σ to obtain a learning rate), key k(s) ∈ R dkey and value v(s) ∈ R dout vectors from input x(s). These variants are inspired by the respective classic learning rules of the same name, while they are crucially different from the classic ones in the sense that all variables involved (key, value, learning rate) are continually generated by the slow NN. In the experimental section, we'll comment on how some of these design choices can result in task-dependent performance gaps. In practice, we use the multi-head version of the operations above (i.e., by letting H ∈ N denote the number of heads, query/key/value vectors are split into H sub-vectors and Eqs. 20-21 are conducted independently for each head). The output is followed by the standard feed-forward block like in Transformers <ref type="bibr" target="#b5">[6]</ref>.</p><p>Possible extensions for deeper models are discussed in Appendix C.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">NCDE based FWPs</head><p>Here we present models based on NCDEs (reviewed in Sec. 2.1). We assume availability of a differentiable control signal x(t), whose first order derivative is denoted by x (t). Given the NCDE formulation of Eq. 5, the most straight-forward approach to obtain a CT Fast Weight Programmer is to extend the dimensionality of the recurrent hidden state, i.e., we introduce a parameterised function F θ which maps a matrix W (t) ∈ R dout×dkey to a third-order tensor F θ (W (t)) ∈ R dout×dkey×din :</p><formula xml:id="formula_20">W (t) = W (t 0 ) + t s=t0 F θ (W (s))dx(s) = W (t 0 ) + t s=t0 F θ (W (s))x (s)ds<label>(22)</label></formula><p>However, this approach is obviously not scalable since the input and output dimensions (d out × d key and d out × d key × d in ) of F θ can be too large in practice. A more tractable CDE-based approach can be obtained by providing x and/or x to the vector field:</p><formula xml:id="formula_21">W (t) = W (t 0 ) + t s=t0 F θ (W (s), x(s), x (s))x (s)ds<label>(23)</label></formula><p>While this equation still remains a CDE because of the multiplication from the right by dx = x (s)ds, the additional inputs to the vector field offer a way of making use of various learning rules, as in the case of direct NODE approach above (Sec. 3.1). To be specific, either x and x or only x is required in the vector field to obtain these tractable CDEs. Here we present the version which uses both x and x<ref type="foot" target="#foot_3">4</ref> . The resulting vector fields for different cases are:</p><formula xml:id="formula_22">F θ W (s), x(s), x (s) x (s) = σ(β(s))    W k x(s) ⊗ W v x (s) Hebb W k x(s) − W (s) W v x (s) ⊗ W v x (s) Oja W v x(s) − W (s)W k x (s) ⊗ W k x (s) Delta (<label>24</label></formula><formula xml:id="formula_23">)</formula><p>As can be seen above, the use of CDEs to describe a continuous fast weight learning rule thus naturally results in a key/value memory where x is used to generate either key or value vectors.</p><p>Because of the multiplication from the right by x , the role of x changes depending on the choice of learning rule: x is used to generate the key in the Delta case but the value vector in the case of Oja. In the case of Hebb, the choice made in Eq. 24 of using x for keys and x for values is arbitrary since Eq. 24 is symmetric in terms of roles of keys and values (see an ablation study in Appendix C.2 for the other case where we use x to generate the key and x for the value). The querying operation (analogous to Eqs. 19-20 for the direct NODE case) is also modified accordingly, depending on the choice of learning rule, such that the same input (x or x ) is used to generate both key and query:</p><formula xml:id="formula_24">y(T ) = W (T ) W q x(T ) Hebb and Oja W (T )W q x (T ) Delta (<label>25</label></formula><formula xml:id="formula_25">)</formula><p>Note that since the proposed vector field F θ W (s), x(s), x (s) x (s) is more general than the one used in the original NCDE F θ W (s) x (s), any theoretical results on the CDE remain valid (which, however, does not tell us anything about the best choice for its exact parameterisation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">ODE-RFWP and Latent ODE-RFWP</head><p>The main focus of this work is the setting of Kidger et al. <ref type="bibr" target="#b3">[4]</ref> where we assume the existence of some control signal x. However, here we also show a way of using FWPs in the third case where no control x(t) is available, i.e., we only have access to discrete observations (x n ) N n=0 . Here we cannot directly define the vector field involving continuous transformations using the inputs. We follow the existing approaches (ODE-RNN or Latent ODE; Sec. 2.1) which use two separate update functions: A discrete recurrent state update is executed every time a new observation is available to the model, while a continuous update using an autonomous ODE is conducted in between observations. Unlike with standard recurrent state vectors, however, it is not practical to autonomously evolve high-dimensional fast weight matrices (which would require an expressive matrix to matrix transformation). We therefore opt for using a Recurrent FWP (RFWP) <ref type="bibr" target="#b8">[9]</ref> and combine it with an ODE:</p><formula xml:id="formula_26">u n = ODESolve(f θ1 , h n−1 , t n−1 , t n ) (26) h n , W n = FWP([x n , u n ], W n−1 ; θ 2 )<label>(27)</label></formula><p>where we keep the fast weight learning rule itself discrete (Eq. 27), but evolve the recurrent state vector u n using an ODE (Eq. 26) such that the information to be read/written to the fast weight matrix is controlled by a variable which is continuously updated between observations. We refer to this model as ODE-RFWP and its variational variant as Latent ODE-RFWP. While this case is not of central interest to the present work, as the learning rule remains discrete (Eq. 27), we also provide some experimental results (for model-based reinforcement learning settings) in Appendix C.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We consider three datasets covering three types of time series which are regularly sampled (Speech Commands <ref type="bibr" target="#b47">[48]</ref>), irregularly sampled with partially missing features (PhysioNet Sepsis <ref type="bibr" target="#b48">[49]</ref>), or very long (EigenWorms <ref type="bibr" target="#b49">[50]</ref>). We compare the proposed direct NODE and CDE based FWP models (Sec. 3.1 &amp; 3.2) to NODE baselines previously reported on the same datasets <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b34">35]</ref>. Appendix B provides further experimental details.  <ref type="bibr" target="#b3">[4]</ref>). This demonstrates that with a good parameterisation of the vector field, the direct NODE approach can achieve competitive performance.</p><p>On the other hand, all CDE-based approaches yield similar performance. We also only see slight differences in terms of performance among different learning rules, without a clear winner for this task. This may indicate that the ordinary nature of this task (regularly sampled; short sequences) does not allow for differentiating among these CDE models, including the baseline.</p><p>PhysioNet Sepsis. The next dataset we experiment with is the PhysioNet Sepsis dataset of the sepsis prediction task from the PhysioNet challenge 2019 <ref type="bibr" target="#b48">[49]</ref>. This is again a dataset used by Kidger et al. <ref type="bibr" target="#b3">[4]</ref> to evaluate NCDEs. The task is a binary prediction of sepsis from a time series consisting of measurements of 34 medical features (e.g., respiration rate) of patients' stays at an ICU. Each sequence is additionally labelled by five static features of the patient (e.g., age) which are fed to the model to generate the initial state of the ODE. Sequences are relatively short (≤ 72 frames) but datapoints are irregularly sampled and many entries are missing, which makes this task challenging. It comes in two versions: with and without the so-called observation intensity information (denoted as "OI" and "no-OI") which is one extra input feature indicating each observation's time stamp (providing the models with information on measurement frequency). This distinction is important since the prior work <ref type="bibr" target="#b3">[4]</ref> has reported that existing ODE/CDE-based approaches struggle with the no-OI case of this task. Following the previous work, we report the performance in terms of Area Under the ROC Curve (AUC). The right part of Table <ref type="table" target="#tab_0">1</ref> shows the results. We obtain large improvements in the no-IO case (from 77.6 to 84.5% for the CDEs and from 77.1 to 83.8% for the direct NODEs), while also obtaining small improvements in the OI case (from 85.2 to 88.8% for direct NODEs, and from 88.0 to 90.4% for CDEs). This demonstrates the efficacy of CT FWP model variants for handling irregularly sampled data with partially missing features even in the case without frequency information. Differences between various learning rules are rather small again. The best no-OI single-seed performance is achieved by the Delta rule (87.6%) whose variance across seeds is, however, high (std of 5.4).</p><p>EigenWorms. The EigenWorms dataset (which is part of the UEA benchmark <ref type="bibr" target="#b49">[50]</ref>) is a 5-way classification of roundworm types based on time series tracking their movements. To be more specific, motions of a worm are represented by six features corresponding to its projections to six template movement shapes, called "eigenworms". While this dataset contains only 259 examples, it is notable for its very long sequences (raw sequence lengths exceed 17 K) and long-span temporal dependencies Table <ref type="table">2</ref>: Classification Accuracy (%) on the EigenWorms task. Numbers marked by * are taken from Morrill et al. <ref type="bibr" target="#b35">[36]</ref>. Mean and standard deviation (std) are computed over 5 runs. "Sig-Depth" indicates the depth of the signature (with this number equal to 1, an RDE is reduced to a CDE.). To facilitate comparisons to prior work <ref type="bibr" target="#b35">[36]</ref>, we also add the column "Step" indicating the sequence down-sampling factor (even if we fix it to the best value <ref type="bibr" target="#b35">[36]</ref>   <ref type="table">2</ref> shows the results, where "</p><p>Step" denotes the time sub-sampling rate which is fixed to 4 for which the prior work <ref type="bibr" target="#b35">[36]</ref> reports the best NRDE and NCDE performance. "Sig-Depth" denotes the depth of the logsignature (the deeper, the more log-signature terms we take into account, thus ending up with a larger input feature vector; we refer to the original paper <ref type="bibr" target="#b35">[36]</ref> for further details). We consider two values for this parameter: 1 and 2. When set to 1, the input feature contains only the first derivative x (s) and thus the NRDE is reduced to a NCDE. We take the best NCDE performance from Morrill et al. <ref type="bibr" target="#b35">[36]</ref> as the depth-1 baseline. Morrill et al. <ref type="bibr" target="#b35">[36]</ref> report the best overall performance for the depth-2 NRDE (depth-2 baseline in our table). In both cases, we first note a large performance gap between models with different learning rules. While the naive Hebb and Oja based models struggle with this very long sequence processing (sequence length still exceeds 4 K with a down-sampling step size of 4), the Delta rule performs very well. This confirms the prior result in the discrete-time domain <ref type="bibr" target="#b7">[8]</ref> which motivated the Delta rule design by its potential for handling long sequences (we refer to prior work <ref type="bibr" target="#b7">[8]</ref> for further explanations). Since its performance on other tasks is comparable to the one of Hebb and Oja variants, the Delta rule is a natural default choice for parameterising the CT FWPs.</p><p>In both the depth-1 and depth-2 cases, we obtain large improvements compared to the respective baselines. It is counter-intuitive to find certain depth-2 models underperforming their depth-1 counterparts, but this trend has also been observed in the original NRDEs <ref type="bibr" target="#b34">[35]</ref>. Our best overall performance is obtained in the depth-1 case: 91.8 % (3.4) exceeds the previous best NRDE based model performance of 83.8 % (3.0) <ref type="bibr" target="#b34">[35]</ref>. This almost matches the state-of-the-art accuracy of 92.8 % (1.8) reported by Rusch et al. <ref type="bibr" target="#b51">[52]</ref> (using an ODE-inspired discrete-time model). Our model's performance variance is high (best single seed performance is 97.4% while the standard deviation is 3.4). This calls for future investigations to improve stability of ODE-based models across random initialisations. The wall clock time is similar for our best model (last row in Table <ref type="table">2</ref>) and the NRDE baseline (36s/epoch on a GeForce RTX 2080 Ti) and their sizes are comparable (87 K vs. 65 K parameters respectively).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussions</head><p>Scalability Advantage Compared to Standard NCDEs. Apart from our primary motivation, our FWP approach also addresses an important limitation of existing NCDEs <ref type="bibr" target="#b3">[4]</ref>: their scalability in terms of model size. The vector field in standard NCDEs (Eq. 5) requires an NN F θ which takes a vector h(s) ∈ R d as an input to produce a matrix of size R d×din . This can be very challenging when d in or/and d is large. Actually, the same bottleneck is present in the weight generation of FWPs <ref type="bibr" target="#b6">[7]</ref>. The use of outer products can remediate this issue in discrete FWPs as well as in CT FWPs: the computations in our FWP-based NODE/NCDEs only involve "first-order" dimensions (i.e., no multiplication between different dimensions, such as d × d in ) for NN outputs. This can scale well with increased model size, making feasible larger scale tasks infeasible for existing NCDEs. On the other hand, Kidger et al. <ref type="bibr" target="#b3">[4]</ref> report that using outer products (in their "Sec. 6.1 on limitations") in standard NCDEs does not perform well. Why do outer products work in FWPs but not in the original NCDEs? The answer may be simple. In the original NCDEs (Eq. 5), multiplications occur at each (infinitesimal) time step between the generated rank-one weight matrix F θ (h(s)) and x (s) before the sum. All these transformations are thus of rank one while we expect expressive transformations to be necessary to translate x (s) into changes in the state space. In contrast, in FWPs, the ODE only parameterises the weight generation process of another net, and thus the rank-one matrices are never used in isolation: they are summed up over time (Eq. 18 or 23) to form an expressive weight matrix which is only then used for matrix multiplication (Eq. 20 or 25. The proposed FWP-NODE/NCDEs thus offer scalable alternatives to existing NCDEs, also yielding good empirical performance (Sec. 4).</p><p>Memory Efficient Backpropagation for FWPs Memory efficiency of adjoint backpropagation may be not so important for standard NCDEs of state size O(d), but is crucial for FWPs of state size O(d 2 ) which can quickly become prohibitive for long sequences, as naive backpropagation stores all states used in the forward pass. Prior works on discrete FWPs <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b8">9]</ref> solve this problem by a custom memory-efficient implementation. Here, the adjoint method naturally addresses this problem.</p><p>Related Work on Parameter/Weight ODEs. There are other works which use ODEs to parameterise time-evolving weights of some model. However, they are limited to autonomous ODEs (i.e., no external control x is involved). Zhang et al. <ref type="bibr" target="#b53">[54]</ref> and Choromanski et al. <ref type="bibr" target="#b54">[55]</ref> study coupled ODEs where one ODE is used for temporal evolution of parameters of the main neural ODE. The scope of these two works is limited to autonomous ODEs corresponding to continuous-depth residual NNs with different parameters per depth. Deleu et al. <ref type="bibr" target="#b55">[56]</ref> consider an ODE version of a gradient descent learning process for adaptation, but also formulated as an autonomous ODE. In contrast, our focus is really on sequence processing where the model continuously receives external controls x and translates them into weight changes of another network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We introduced novel continuous-time sequence processing neural networks that learn to use sequences of ODE-based continuous learning rules as elementary programming instructions to manipulate shortterm memory in rapidly changing synaptic connections of another network. The proposed models are continuous-time counterparts of Fast Weight Programmers and linear Transformers. Our new models experimentally outperform by a large margin existing Neural ODE based sequence processors on very long or irregularly sampled time series. Our Neural ODE/CDE based FWPs also address the fundamental scalability problem of the original Neural CDEs, which is highly promising for future applications of ODE based sequence processors to large scale problems.</p><p>FWP based NCDEs using only x . In Sec. 3.2, we noted that tractable FWP based NCDEs can also be obtained by using only x in the vector field (instead of x and x ). The equations for the corresponding models can be obtained by replacing x by x in Eq. 24:</p><formula xml:id="formula_27">F θ W (s), x (s) x (s) = σ(β(s))    W k x (s) ⊗ W v x (s) Hebb W k x (s) − W (s) W v x (s) ⊗ W v x (s) Oja W v x (s) − W (s)W k x (s) ⊗ W k x (s) Delta<label>(38)</label></formula><p>Interestingly, on the Speech Command task, we found these second variants using only x to perform much worse (&lt; 65 %) than the variants using both x and x , across all three learning rule schemes. We are currently investigating potential causes of this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Model-based Reinforcement Learning (RL)</head><p>The main focus of this paper is the one of Kidger et al. <ref type="bibr" target="#b3">[4]</ref> where it is assumed that at least some bounded control signal x(t) defined on [t 0 , T ] is available. However, for the sake of completeness, in Sec. 3.3, we also present the ODE-RFWP and its latent variant in the case where we only have access to discrete inputs. Here we provide some experimental results with working examples of such models in a model-based RL setting previously proposed by Du et al. <ref type="bibr" target="#b56">[57]</ref>.</p><p>Settings. We use the MuJoCo environments <ref type="bibr" target="#b59">[60]</ref>. Our setting follows the version with action repetitions <ref type="bibr" target="#b60">[61]</ref> proposed by Du et al. <ref type="bibr" target="#b56">[57]</ref>, formalised as semi-Markov decision processes <ref type="bibr" target="#b61">[62]</ref>. The core difference to the standard setting is that there is a (state-dependent) time gap τ &gt; 0 between the time when an agent takes an action in state s and when it observes a new state s (and can take a new action), resulting in irregularly timed observations. The time gap τ is a deterministic function of the state s. Du et al. <ref type="bibr" target="#b56">[57]</ref> proposed a model-based planning approach where an environmental model is parameterised as an ODE-RNN or Latent ODE-RNN such that an ODE is used to model the latent state transitions between observations. The model is trained to predict state transitions (using mean squared error for ODE-RNNs and negative evidence lower bounds for Latent ODE-RNNs) and time gaps between observations, and it is used for model predictive control (MPC) <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b64">65]</ref>. The agent is trained by Deep Deterministic Policy Gradient (DDPG) <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b66">67]</ref> with actor-critic <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b69">70]</ref> where the predictions of policy and value networks are conditioned on the recurrent (latent) state of the environment model. In Du et al. <ref type="bibr" target="#b56">[57]</ref>'s work, the Latent ODE-RNN was shown to outperform the ODE-RNN and the model-free baseline in terms of sample efficiency in Hopper, Swimmer and Half Cheetah environments. Therefore, we compare our Latent ODE-RFWP to the baseline Latent ODE-RNN. We conducted our experiments in Hopper and Swimmer environments (we excluded Half Cheetah which was reported <ref type="bibr" target="#b56">[57]</ref> to require larger models and thus more compute). On a side note, there are other works <ref type="bibr" target="#b70">[71]</ref> with a focus on RL in continuous-time environments. However, the available environments are still limited in terms of complexity (e.g., CartPole).</p><p>Training/Model details. Each training iteration consists of interactions with the environment to collect trajectories, training of the environmental model, and policy optimisation. Following Du et al. <ref type="bibr" target="#b56">[57]</ref>, each iteration consists of 5 K environmental steps, and we train for up to 400 K steps (corresponding to a total of 80 iterations) in addition to the initial random interactions of 15 K steps. The planning horizon for MPC is set to 10 steps. We set the hidden state size of our RFWP models to 128 like in the baseline. We use 16 computational heads, and a feedforward block size of 512. We found that we can use the same RNN-based encoder used in the baseline also for our RFWP models without notable difference in terms of performance compared to an FWP-based encoder. The same training/model configurations are used for both Swimmer and Hopper. They are identical to those used by Du et al. <ref type="bibr" target="#b56">[57]</ref> (we use their base implementation), apart from the fact that we use the adjoint method to train all models. For further details, we refer to our code.</p><p>Results. Figures <ref type="figure" target="#fig_1">1a and 1b</ref> show learning curves of baseline Latent ODE-RNN and our RFWP variant in Swimmer and Hopper environments, respectively. Given a high variability of results across seeds, we present an average over ten training runs. We observe that final performance and sample efficiency of both models are comparable on Swimmer, while the RFWP model yields better performance on Hopper. These results indicate that the Latent ODE-RFWP model presented in Sec. 3.3 can effectively in practice be used as a replacement of the standard Latent ODE-RNN. We note, however, that the final performance of baseline Latent ODE-RNN is below the one reported by Du et al. <ref type="bibr" target="#b56">[57]</ref> (which exceeds an expected return of 350). The only change we introduced to the original setting is to consistently use the adjoint method to train all models (we may need further ablation studies to evaluate the impact of this change), and to use ten seeds instead of four.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Return over training iterations in the Swimmer and Hopper environments, averaged over five test episodes. Smoothing is applied on overlapping windows of size 20. One iteration corresponds to 5 K interactions with the environment. Mean/std are computed for ten runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Accuracy (%) on the Speech Commands classification task and AUR (×10 2 ) on the PhysioNet Sepsis prediction task. PhysioNet has two cases: with (OI) or without (no-OI) observational intensity (see text for details). Numbers marked by * are taken from Kidger et al.<ref type="bibr" target="#b3">[4]</ref>. Mean and standard deviation (std) are computed over 5 runs.</figDesc><table><row><cell>Type</cell><cell>Model</cell><cell>Speech Commands</cell><cell cols="2">PhysioNet Sepsis</cell></row><row><cell></cell><cell></cell><cell></cell><cell>OI</cell><cell>no-OI</cell></row><row><cell cols="2">Direct NODE GRU-ODE [4]*</cell><cell>47.9 (2.9)</cell><cell cols="2">85.2 (1.0) 77.1 (2.4)</cell></row><row><cell></cell><cell>Hebb</cell><cell>82.8 (1.1)</cell><cell cols="2">88.1 (3.5) 81.2 (2.5)</cell></row><row><cell></cell><cell>Oja</cell><cell>85.4 (0.9)</cell><cell cols="2">88.8 (0.9) 82.5 (1.1)</cell></row><row><cell></cell><cell>Delta</cell><cell>81.5 (3.8)</cell><cell cols="2">87.9 (3.0) 83.8 (3.7)</cell></row><row><cell>CDE</cell><cell>NCDE [4]*</cell><cell>89.8 (2.5)</cell><cell cols="2">88.0 (0.6) 77.6 (0.9)</cell></row><row><cell></cell><cell>Hebb</cell><cell>89.5 (0.3)</cell><cell cols="2">89.7 (0.3) 84.5 (1.9)</cell></row><row><cell></cell><cell>Oja</cell><cell>90.0 (0.7)</cell><cell cols="2">89.8 (2.5) 79.6 (4.7)</cell></row><row><cell></cell><cell>Delta</cell><cell>90.2 (0.2)</cell><cell cols="2">90.4 (0.6) 83.5 (5.4)</cell></row></table><note>Speech Commands. The Speech Commands<ref type="bibr" target="#b47">[48]</ref> is a single word speech recognition task. The datapoints are regularly sampled, and the sequence lengths are relatively short (≤ 160 frames), which makes this task a popular sanity check. Following prior work on neural CDEs<ref type="bibr" target="#b3">[4]</ref>, we use 20 mel frequency cepstral coefficients as speech features and classify the resulting sequence to one out of ten keywords. The middle column of Table1shows the results. The table is split into the direct NODE (top) and CDE (bottom) based approaches. We first observe that among the direct NODE approaches, all our FWPs largely outperform (≥ 80% accuracy) the baseline GRU-ODE performance of 47.9% (the best direct NODE baseline from Kidger et al.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>of 4). We use the same train/validation/test split ratio as the prior work<ref type="bibr" target="#b35">[36]</ref> which reports neural RDEs (NRDEs) as achieving the best NODE model performance on this dataset. The equations of our CT FWPs for the RDE case can be straightforwardly obtained by replacing the input x in Eqs. 18 and 19 of the direct NODE formulation by the corresponding log-signatures. Table</figDesc><table><row><cell>Model</cell><cell cols="3">Sig-Depth Step Test Acc. [%]</cell></row><row><cell>NRDE [36]*</cell><cell>2</cell><cell>4</cell><cell>83.8 (3.0)</cell></row><row><cell>Hebb</cell><cell>2</cell><cell>4</cell><cell>45.6 (5.9)</cell></row><row><cell>Oja</cell><cell></cell><cell></cell><cell>46.7 (7.5)</cell></row><row><cell>Delta</cell><cell></cell><cell></cell><cell>87.7 (1.9)</cell></row><row><cell>NCDE [36]*</cell><cell>1</cell><cell>4</cell><cell>66.7 (11.8)</cell></row><row><cell>Hebb</cell><cell>1</cell><cell>4</cell><cell>41.0 (6.5)</cell></row><row><cell>Oja</cell><cell></cell><cell></cell><cell>49.7 (9.9)</cell></row><row><cell>Delta</cell><cell></cell><cell></cell><cell>91.8 (3.4)</cell></row><row><cell>[36, 51, 52].</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://github.com/IDSIA/neuraldiffeq-fwp Preprint. Under review.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">Or we make θ dependent of t such that parameters are "depth/layer-dependent" as in standard deep nets.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">In practice, we also apply element-wise activation functions to query/key/value vectors where appropriate, which we omit here for readability. We refer to Appendix A for further details.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">The equations for the version using only x can be obtained by replacing x by x in Eq. 24. We provide an ablation in Appendix C.2. As a side note, we also obtain the equation for the CDE using only x by replacing x by x in Eq. 18 for the direct NODE case.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgements</head><p>We would like to thank Kidger et al. <ref type="bibr" target="#b3">[4]</ref>, Morrill et al. <ref type="bibr" target="#b35">[36]</ref> and Du et al. <ref type="bibr" target="#b56">[57]</ref> for their public code. This research was partially funded by ERC Advanced grant no: 742870, project AlgoRNN, and by Swiss National Science Foundation grant no: 200021_192356, project NEUSYM. We are thankful for hardware donations from NVIDIA and IBM. The resources used for this work were partially provided by Swiss National Supercomputing Centre (CSCS) project s1145 and s1154.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Further Model Specifications</head><p>For better readability, Sec. 3 omitted element-wise activation functions for query/key/value vectors. For example, for Eq. 21, the generation of key and value vectors, k and v, can be fully described as</p><p>Eq. <ref type="bibr" target="#b18">19</ref> for the query generation has to be replaced by q(T ) = softmax(W q x(T ))</p><p>These specifications are analogous in the CDE cases, i.e., in Eqs. 24-25.</p><p>While softmax has already been identified as a crucial component for stability in prior works on discrete-time FWPs <ref type="bibr" target="#b7">[8]</ref>, we apply an additional tanh to the value vectors (Eq. 30). Such usage of tanh in the output of vector fields for CDEs has been advocated by Kidger et al. <ref type="bibr" target="#b3">[4]</ref> to improve the stability of the ODE solver. Indeed, we also generally found this beneficial, and sometimes crucial, for stable training of our models. For the Delta rule (Eq. 24), since we modify the value vector after projection (to take into account the "value" which is currently associated with the key vector), we consider two different ways of applying tanh as follows:</p><p>For the task involving very long sequences (&gt; 4000 frames), we found the "post-delta" version to be crucial for successful training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Further Experimental Details</head><p>Our basic settings for data preparation and training are based on those used by prior works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b35">36]</ref> (we use their public implementations) for fair comparisons with the corresponding baselines.</p><p>Dataset/Task details. The essential information about the datasets has already been presented in Sec. 4. The number of sequences in the training set is about 24 K for the Speech Commands dataset and about 28 K for the PhysioNet Sepsis dataset.</p><p>Model/Training details. As mentioned in Sec. 3, all our FWP models make use of multiple computational heads, and their NODE/NCDE layer is followed by the standard Transformer feedforward block. The number of heads n head and the feedforward inner dimension d ff are hyper-parameters of the model, in addition to the size d model of the NODE/NCDE layer. For all models considered in the main text, the number of layers is one (see Sec. C.1 below for a discussion of deeper models). We conducted hyper-parameter search in the following ranges, and selected the best configuration for each setting based on its validation performance.</p><p>For Speech Commands: For any further details, we refer to our code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Extra Experiments C.1 Deeper Models</head><p>As the datasets used here were rather small, one-layer models yielded satisfactory performance. In general, however, deeper architectures are common for Transformers <ref type="bibr" target="#b57">[58]</ref>. Here we show ways of increasing the depth of FWP-based NODE/NCDE models. While the possibility of stacking multiple NCDE layers has been mentioned previously <ref type="bibr" target="#b58">[59]</ref>, no practical result has been reported.</p><p>The following equations are for the direct NODE case of our FWP models (the CDE case is analogous). Let L and l denote positive integers. The forward computation of layer l in an L-layer model at step t is as follows</p><p>x(t, l) = FFN (W (t, l)softmax(W q x(t, l − 1))) <ref type="bibr" target="#b33">(34)</ref> where FFN denotes the standard Transformer feedforward block, W (t, l) denotes the fast weight matrix, F</p><p>θ l is the vector field with parameters θ l , and x(t, l) denotes the output of layer l ≥ 1 while x(t, 0) is the external control signal. The output of layer l, x(t, l), thus becomes the control signal for the next layer l + 1, and so on. This yields a system of L coupled ODEs. The fast weight matrix for each layer at time T can be obtained by calling the ODE solver for the corresponding coupled ODEs</p><p>The final output x(T, L) can be computed recursively via Eq. 34 for t = T starting from l = 1.</p><p>We conducted experiments in the EigenWorms dataset with the goal of improving our best singlelayer model of Table <ref type="table">2</ref>. We tuned the 2-layer model using the same hyper-parameter search space used for the 1-layer model. However, the best obtained 2-layer performance is 86.7 % (4.1) which is worse than the best 1-layer performance of 91.8 % (3.4). A natural next step towards obtaining potential performance gains from deeper/larger models is to apply regularisation techniques to our models (we leave that to future work).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Ablation Studies on CDE-based Models</head><p>In Sec. 3.2 on NCDE based FWPs, we mentioned two optional model variations skipped in the main text: the Hebb variant which uses x to generate keys and x to generate values, and NCDE based models which only use x (instead of x and x ) in the vector field (see footnote 4). Here we provide the corresponding ablation studies.</p><p>Using x for key and x for value generation in the Hebb variant. In Sec. 3.2 on FWP based CDEs, we noted that there are two possible formulations for the Hebb variant. The following equations highlight the difference between the two formulations (depending on the variable, x or x , used to generate keys/values):</p><p>We also use different equations for the fast net forward computation where, for consistency, we generate the query from the same input (x or x ) used to generate keys (even if this is not a strict requirement): y(T ) = W (T ) W q x(T ) x-keys, x -values W (T )W q x (T ) x-values, x -keys <ref type="bibr" target="#b36">(37)</ref> Experimentally, we found this second variant ("x-values, x -keys") to perform worse on the Speech Command task. It obtained a test accuracy of 83.9 % (0.4) compared to 89.5 % (0.3) achieved by the first variant we reported in the main text (Table <ref type="table">1</ref>).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural ordinary differential equations</title>
		<author>
			<persName><forename type="first">Tian</forename><surname>Qi Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems (NeurIPS)</title>
				<meeting>Advances in Neural Information essing Systems (NeurIPS)<address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12">December 2018</date>
			<biblScope unit="page" from="6572" to="6583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06">June 2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Highway networks</title>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Rupesh K Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the Deep Learning workshop at Int. Conf. on Machine Learning (ICML)</title>
				<meeting><address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07">July 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural controlled differential equations for irregular time series</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Kidger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Morrill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terry</forename><forename type="middle">J</forename><surname>Lyons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems (NeurIPS), Virtual Only</title>
				<meeting>Advances in Neural Information essing Systems (NeurIPS), Virtual Only</meeting>
		<imprint>
			<date type="published" when="2020-12">December 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName><surname>Jeffrey L Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="211" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems (NIPS)</title>
				<meeting>Advances in Neural Information essing Systems (NIPS)<address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12">December 2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning to control fast-weight memories: An alternative to recurrent nets</title>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno>FKI-147-91</idno>
		<imprint>
			<date type="published" when="1991-03">March 1991</date>
		</imprint>
		<respStmt>
			<orgName>Institut für Informatik, Technische Universität München</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Linear Transformers are secretly fast weight programmers</title>
		<author>
			<persName><forename type="first">Imanol</forename><surname>Schlag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuki</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Machine Learning (ICML)</title>
				<meeting>Int. Conf. on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2021-07">July 2021</date>
		</imprint>
	</monogr>
	<note>Virtual only</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Going beyond linear transformers with recurrent fast weight programmers</title>
		<author>
			<persName><forename type="first">Kazuki</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Imanol</forename><surname>Schlag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Róbert</forename><surname>Csordás</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems (NeurIPS), Virtual only</title>
				<meeting>Advances in Neural Information essing Systems (NeurIPS), Virtual only</meeting>
		<imprint>
			<date type="published" when="2021-12">December 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Transformers are RNNs: Fast autoregressive transformers with linear attention</title>
		<author>
			<persName><forename type="first">Angelos</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apoorv</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolaos</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">François</forename><surname>Fleuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Machine Learning (ICML), Virtual only</title>
				<meeting>Int. Conf. on Machine Learning (ICML), Virtual only</meeting>
		<imprint>
			<date type="published" when="2020-07">July 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Approximation of dynamical systems by continuous time recurrent neural networks</title>
		<author>
			<persName><forename type="first">Ken-Ichi</forename><surname>Funahashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuichi</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="801" to="806" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Nonlinear signal processing using neural networks: Prediction and system modelling</title>
		<author>
			<persName><forename type="first">Alan</forename><surname>Lapedes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Farber</surname></persName>
		</author>
		<idno>No. LA-UR-87-2662</idno>
		<imprint>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generalization of back-propagation to recurrent neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><surname>Pineda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review letters</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">19</biblScope>
			<biblScope unit="page">2229</biblScope>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning state space trajectories in recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Barak</surname></persName>
		</author>
		<author>
			<persName><surname>Pearlmutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="263" to="269" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning nonlinear dynamics by recurrent neural</title>
		<author>
			<persName><forename type="first">Masa-Aki</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshihiko</forename><surname>Murakami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Some Problems on the Theory of Dynamical Systems in Applied Sciences-Proceedings of the Symposium</title>
				<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">49</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Discrete-vs. continuous-time nonlinear signal processing of cu electrodissolution data</title>
		<author>
			<persName><forename type="first">Ramiro</forename><surname>Rico-Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Krischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Kevrekidis</surname></persName>
		</author>
		<author>
			<persName><surname>Mc Kube</surname></persName>
		</author>
		<author>
			<persName><surname>Hudson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical Engineering Communications</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="48" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Simplified neuron model as a principal component analyzer</title>
		<author>
			<persName><forename type="first">Erkki</forename><surname>Oja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of mathematical biology</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="267" to="273" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On lines and planes of closest fit to systems of points in space</title>
		<author>
			<persName><forename type="first">Karl</forename><surname>Pearson</surname></persName>
		</author>
		<author>
			<persName><surname>Liii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The London, Edinburgh, and Dublin philosophical magazine and journal of science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="559" to="572" />
			<date type="published" when="1901">1901</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Analysis of a complex of statistical variables into principal components</title>
		<author>
			<persName><forename type="first">Harold</forename><surname>Hotelling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of educational psychology</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">417</biblScope>
			<date type="published" when="1933">1933</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On stochastic approximation of the eigenvectors and eigenvalues of the expectation of a random matrix</title>
		<author>
			<persName><forename type="first">Erkki</forename><surname>Oja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juha</forename><surname>Karhunen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of mathematical analysis and applications</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="69" to="84" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural networks, principal components, and subspaces</title>
		<author>
			<persName><forename type="first">Erkki</forename><surname>Oja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of neural systems</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">01</biblScope>
			<biblScope unit="page" from="61" to="68" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Lyapunov functions for convergence of principal component algorithms</title>
		<author>
			<persName><surname>Mark D Plumbley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="23" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Convergence analysis of local feature extraction algorithms</title>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C-M</forename><surname>Kuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="229" to="240" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Optimal unsupervised learning in a single-layer linear feedforward neural network</title>
		<author>
			<persName><forename type="first">Terence</forename><forename type="middle">D</forename><surname>Sanger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="459" to="473" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Time-domain solutions of Oja&apos;s equations</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">L</forename><surname>Wyatt</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">Ibrahim</forename><forename type="middle">M</forename><surname>Elfadel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="915" to="922" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Convergence of stochastic algorithms: From the Kushner-Clark theorem to the Lyapounov functional method</title>
		<author>
			<persName><forename type="first">Jean-Claude</forename><surname>Fort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gilles</forename><surname>Pages</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in applied probability</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1072" to="1094" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A proposal on machine learning via dynamical systems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Weinan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications in Mathematics and Statistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2017-03">March 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Stable architectures for deep neural networks</title>
		<author>
			<persName><forename type="first">Eldad</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Ruthotto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inverse Problems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">14004</biblScope>
			<date type="published" when="2017-12">December 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning across scales -multiscale methods for convolution neural networks</title>
		<author>
			<persName><forename type="first">Eldad</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Ruthotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elliot</forename><surname>Holtham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seong-Hwan</forename><surname>Jun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf. on Artificial Intelligence</title>
				<meeting>AAAI Conf. on Artificial Intelligence<address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-02">February 2018</date>
			<biblScope unit="page" from="3142" to="3148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Reversible architectures for arbitrarily deep residual neural networks</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lili</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eldad</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Ruthotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Begert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elliot</forename><surname>Holtham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf. on Artificial Intelligence</title>
				<meeting>AAAI Conf. on Artificial Intelligence<address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-02">February 2018</date>
			<biblScope unit="page" from="2811" to="2818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Beyond finite layer neural networks: Bridging deep architectures and numerical differential equations</title>
		<author>
			<persName><forename type="first">Yiping</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aoxiao</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanzheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Machine Learning (ICML)</title>
				<meeting>Int. Conf. on Machine Learning (ICML)<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07">July 2018</date>
			<biblScope unit="page" from="3282" to="3291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-level residual networks from dynamical systems view</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lili</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eldad</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederick</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Begert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Learning Representations (ICLR)</title>
				<meeting><address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04">April 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">NAIS-Net: Stable deep networks from non-autonomous differential equations</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Ciccone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gallieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Osendorfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faustino</forename><forename type="middle">J</forename><surname>Gomez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems (NeurIPS)</title>
				<meeting>Advances in Neural Information essing Systems (NeurIPS)<address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12">December 2018</date>
			<biblScope unit="page" from="3029" to="3039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">LS Pontryagin Selected Works: The Mathematical Theory of Optimal Processes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liev</surname></persName>
		</author>
		<author>
			<persName><surname>Pontryagin</surname></persName>
		</author>
		<author>
			<persName><surname>Boltyanskii</surname></persName>
		</author>
		<author>
			<persName><surname>Gamkrelidze</surname></persName>
		</author>
		<author>
			<persName><surname>Mishchenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1962">1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Neural controlled differential equations for online prediction tasks</title>
		<author>
			<persName><forename type="first">James</forename><surname>Morrill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Kidger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terry</forename><surname>Lyons</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.11028</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Neural rough differential equations for long time series</title>
		<author>
			<persName><forename type="first">James</forename><surname>Morrill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristopher</forename><surname>Salvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Kidger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Foster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Machine Learning (ICML)</title>
				<meeting>Int. Conf. on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2021-07">July 2021</date>
			<biblScope unit="page" from="7829" to="7838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Reducing the ratio between learning complexity and number of time varying variables in fully recurrent nets</title>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks (ICANN)</title>
				<meeting><address><addrLine>Amsterdam, Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993-09">September 1993</date>
			<biblScope unit="page" from="460" to="463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Hypernetworks</surname></persName>
		</author>
		<title level="m">Int. Conf. on Learning Representations (ICLR)</title>
				<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04">April 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Latent ordinary differential equations for irregularly-sampled time series</title>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Qi Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems (NeurIPS)</title>
				<meeting>Advances in Neural Information essing Systems (NeurIPS)<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12">December 2019</date>
			<biblScope unit="page" from="5321" to="5331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">GRU-ODE-Bayes: Continuous modeling of sporadically-observed time series</title>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">De</forename><surname>Brouwer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaak</forename><surname>Simm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Arany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yves</forename><surname>Moreau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems (NeurIPS)</title>
				<meeting>Advances in Neural Information essing Systems (NeurIPS)<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12">December 2019</date>
			<biblScope unit="page" from="7377" to="7388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The organization of behavior; a neuropsycholocigal theory</title>
		<author>
			<persName><forename type="first">Donald</forename><surname>Olding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hebb</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">A Wiley Book in Clinical Psychology</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page">78</biblScope>
			<date type="published" when="1949">1949</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">ODE-inspired analysis for the biological version of Oja&apos;s rule in solving streaming PCA</title>
		<author>
			<persName><forename type="first">Chi-Ning</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mien</forename><surname>Brabeeba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. on Learning Theory (COLT)</title>
				<meeting>Conf. on Learning Theory (COLT)</meeting>
		<imprint>
			<date type="published" when="2020-07">July 2020</date>
			<biblScope unit="page" from="1339" to="1343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A comprehensive review of stability analysis of continuous-time recurrent neural networks</title>
		<author>
			<persName><forename type="first">Huaguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanshan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derong</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1229" to="1262" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Framing RNN as a kernel method: A neural ODE approach</title>
		<author>
			<persName><forename type="first">Adeline</forename><surname>Fermanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Marion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Philippe</forename><surname>Vert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gérard</forename><surname>Biau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems (NeurIPS), Virtual only</title>
				<meeting>Advances in Neural Information essing Systems (NeurIPS), Virtual only</meeting>
		<imprint>
			<date type="published" when="2021-12">December 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Theoretical foundations of potential function method in pattern recognition</title>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">A</forename><surname>Aizerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Emmanuil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lev</forename><forename type="middle">I</forename><surname>Braverman</surname></persName>
		</author>
		<author>
			<persName><surname>Rozonoer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automation and Remote Control</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="917" to="936" />
			<date type="published" when="1964">1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The dual form of neural networks revisited: Connecting test time predictions to training patterns via spotlights of attention</title>
		<author>
			<persName><forename type="first">Kazuki</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Róbert</forename><surname>Csordás</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Machine Learning (ICML)</title>
				<meeting>Int. Conf. on Machine Learning (ICML)<address><addrLine>Baltimore, MD, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-07">July 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Adaptive switching circuits</title>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Widrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcian</forename><forename type="middle">E</forename><surname>Hoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IRE WESCON Convention Record</title>
				<meeting>IRE WESCON Convention Record<address><addrLine>Los Angeles, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1960-08">August 1960</date>
			<biblScope unit="page" from="96" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Speech commands: A dataset for limited-vocabulary speech recognition</title>
		<author>
			<persName><forename type="first">Pete</forename><surname>Warden</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03209</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Early prediction of sepsis from clinical data: the PhysioNet/computing in cardiology challenge</title>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">A</forename><surname>Reyna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Josef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salman</forename><surname>Seyedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russell</forename><surname>Jeter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Supreeth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Brandon</forename><surname>Shashikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Westover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shamim</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gari</forename><forename type="middle">D</forename><surname>Nemati</surname></persName>
		</author>
		<author>
			<persName><surname>Clifford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computing in Cardiology (CinC)</title>
				<meeting>Computing in Cardiology (CinC)<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-09">2019. September 2019</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">The UEA multivariate time series classification archive</title>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Bagnall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anh</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Dau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Lines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Large</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Bostrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eamonn</forename><surname>Southam</surname></persName>
		</author>
		<author>
			<persName><surname>Keogh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00075</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note>Preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">UnICORNN: A recurrent model for learning very long time dependencies</title>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Rusch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhartha</forename><surname>Mishra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Machine Learning (ICML)</title>
				<meeting>Int. Conf. on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2021-07">July 2021</date>
			<biblScope unit="page" from="9168" to="9178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Long expressive memory for sequence modeling</title>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Rusch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhartha</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Benjamin Erichson</surname></persName>
		</author>
		<author>
			<persName><surname>Mahoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Learning Representations (ICLR), Virtual only</title>
				<imprint>
			<date type="published" when="2022-04">April 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Training and generating neural networks in compressed weight space</title>
		<author>
			<persName><forename type="first">Kazuki</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Compression: From Information Theory to Applications -Workshop @ ICLR 2021, Virtual only</title>
				<imprint>
			<date type="published" when="2021-05">May 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">ANODEV2: A coupled neural ODE framework</title>
		<author>
			<persName><forename type="first">Tianjun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Biros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems (NeurIPS)</title>
				<meeting>Advances in Neural Information essing Systems (NeurIPS)<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12">December 2019</date>
			<biblScope unit="page" from="5152" to="5162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Ode to an ODE</title>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">Quincy</forename><surname>Krzysztof Marcin Choromanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valerii</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyou</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Jacques</forename><forename type="middle">E</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Slotine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Varley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Weller</surname></persName>
		</author>
		<author>
			<persName><surname>Sindhwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems (NeurIPS), Virtual only</title>
				<meeting>Advances in Neural Information essing Systems (NeurIPS), Virtual only</meeting>
		<imprint>
			<date type="published" when="2020-12">December 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Continuous-time meta-learning with forward mode differentiation</title>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Deleu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Kanaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leo</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giancarlo</forename><surname>Kerg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Luc</forename><surname>Bacon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Learning Representations (ICLR), Virtual only</title>
				<imprint>
			<date type="published" when="2022-04">April 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Model-based reinforcement learning for semi-Markov decision processes with neural ODEs</title>
		<author>
			<persName><forename type="first">Jianzhun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Futoma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Finale</forename><surname>Doshi-Velez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems (NeurIPS), Virtual only</title>
				<meeting>Advances in Neural Information essing Systems (NeurIPS), Virtual only</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Language modeling with deep Transformers</title>
		<author>
			<persName><forename type="first">Kazuki</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Zeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralf</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
				<meeting>Interspeech<address><addrLine>Graz, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-09">September 2019</date>
			<biblScope unit="page" from="3905" to="3909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">On Neural Differential Equations</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Kidger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
		<respStmt>
			<orgName>Mathematical Institute, University of Oxford</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">MuJoCo: A physics engine for model-based control</title>
		<author>
			<persName><forename type="first">Emanuel</forename><surname>Todorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Tassa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS)</title>
				<meeting><address><addrLine>Vilamoura, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012-10">October 2012</date>
			<biblScope unit="page" from="5026" to="5033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Learning to repeat: Fine grained action repetition for deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Sahil</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravind</forename><forename type="middle">S</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balaraman</forename><surname>Ravindran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Learning Representations (ICLR)</title>
				<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04">April 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Markov decision processes: discrete stochastic dynamic programming</title>
		<author>
			<persName><surname>Martin L Puterman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Robust constrained model predictive control</title>
		<author>
			<persName><forename type="first">Arthur</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richards</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Massachusetts Institute of Technology</title>
				<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Model predictive control</title>
		<author>
			<persName><forename type="first">F</forename><surname>Eduardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Camacho</surname></persName>
		</author>
		<author>
			<persName><surname>Bordons</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">B</forename><surname>David Q Mayne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">V</forename><surname>Rawlings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><forename type="middle">Om</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><surname>Scokaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Constrained model predictive control: Stability and optimality</title>
				<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="789" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Continuous control with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">J</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Learning Representations (ICLR)</title>
				<meeting><address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-05">May 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Deterministic policy gradient algorithms</title>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Lever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Degris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Machine Learning (ICML)</title>
				<meeting>Int. Conf. on Machine Learning (ICML)<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06">June 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Temporal Credit Assignment in Reinforcement Learning</title>
		<author>
			<persName><surname>Richard S Sutton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts Amherst</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Actor-critic algorithms</title>
		<author>
			<persName><forename type="first">Vijay</forename><forename type="middle">R</forename><surname>Konda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">N</forename><surname>Tsitsiklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems (NIPS)</title>
				<meeting>Advances in Neural Information essing Systems (NIPS)<address><addrLine>Denver, CO, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-11">November 1999</date>
			<biblScope unit="page" from="1008" to="1014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Natural actor-critic</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sethu</forename><surname>Vijayakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Schaal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Machine Learning (ECML)</title>
				<meeting>European Conference on Machine Learning (ECML)<address><addrLine>Porto, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-10">October 2005</date>
			<biblScope unit="page" from="280" to="291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Continuous-time model-based reinforcement learning</title>
		<author>
			<persName><forename type="first">Çagatay</forename><surname>Yildiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Heinonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harri</forename><surname>Lähdesmäki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Machine Learning (ICML)</title>
				<meeting>Int. Conf. on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2021-07">July 2021</date>
		</imprint>
	</monogr>
	<note>Virtual only</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
