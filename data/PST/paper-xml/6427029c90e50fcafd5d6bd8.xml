<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RAMBDA: RDMA-driven Acceleration Framework for Memory-intensive ?s-scale Datacenter Applications</title>
				<funder>
					<orgName type="full">Intel Corporation&apos;s Academic</orgName>
				</funder>
				<funder ref="#_nUYWdyJ">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
				<funder>
					<orgName type="full">NVIDIA device donation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yifan</forename><surname>Yuan</surname></persName>
							<email>yifan.yuan@intel.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Intel Labs, ? Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jinghan</forename><surname>Huang</surname></persName>
							<email>jinghan4@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yan</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tianchen</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jacob</forename><surname>Nelson</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dan</forename><forename type="middle">R K</forename><surname>Ports</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yipeng</forename><surname>Wang</surname></persName>
							<email>yipeng1.wang@intel.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Intel Labs, ? Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ren</forename><surname>Wang</surname></persName>
							<email>ren.wang@intel.com</email>
						</author>
						<author>
							<persName><forename type="first">Charlie</forename><surname>Tai</surname></persName>
							<email>charlie.tai@intel.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Intel Labs, ? Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nam</forename><forename type="middle">Sung</forename><surname>Kim</surname></persName>
							<email>nskim@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Intel Labs, ? Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">RAMBDA: RDMA-driven Acceleration Framework for Memory-intensive ?s-scale Datacenter Applications</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Responding to the "datacenter tax" and "killer microseconds" problems for memory-intensive datacenter applications, diverse solutions including Smart NIC-based ones have been proposed. Nonetheless, they often suffer from high overhead of communications over network and/or PCIe links. To tackle the limitations of the current solutions, this paper proposes RAMBDA, a holistic network and architecture co-design solution that leverages current RDMA and emerging cache-coherent offchip interconnect technologies. Specifically, RAMBDA consists of four hardware and software components: (1) unified abstraction of inter-and intra-machine communications synergistically managed by one-sided RDMA write and cache-coherent memory write; (2) efficient notification of requests to accelerators assisted by cache coherence; (3) cache-coherent accelerator architecture directly interacting with NIC; and (4) adaptive device-to-host data transfer for modern server memory systems comprising both DRAM and NVM exploiting state-of-the-art features in CPUs and PCIe. We prototype RAMBDA with a commercial system and evaluate three popular datacenter applications: (1) in-memory key-value store, (2) chain replication-based distributed transaction system, and (3) deep learning recommendation model inference. The evaluation shows that RAMBDA provides 30.1?69.1% lower latency, 0.2?2.5? throughput, and ? 3? higher energy efficiency than the current state-of-the-art solutions, including Smart NIC. For those cases where RAMBDA performs poorly, we also envision future architecture to improve it.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Datacenter networks are evolving rapidly. 100 Gbps Ethernet is widely deployed today, and so will be 400 Gbps Ethernet soon <ref type="bibr" target="#b120">[121]</ref>. To keep pace, a server may have to process hundreds of millions of packets per second. However, singlethread performance of CPUs has remained comparatively stagnant, requiring the CPUs to spend more cores and their cycles for network processing alone -a major component of the "datacenter tax" <ref type="bibr" target="#b78">[79]</ref>. For application processing, accelerators can be used. Yet, conventional accelerators are inefficient for processing many small tasks <ref type="bibr" target="#b146">[147]</ref>, and thus unable to address "killer microsecond" problem <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b109">110]</ref>.</p><p>Current approaches use one of three strategies. (Tab. I). First, kernel-bypass networking, using user-space network stacks <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b70">71]</ref> or two-sided Remote Direct Memory Access (RDMA) <ref type="bibr" target="#b74">[75]</ref><ref type="bibr" target="#b75">[76]</ref><ref type="bibr" target="#b76">[77]</ref><ref type="bibr" target="#b77">[78]</ref><ref type="bibr" target="#b97">98]</ref>, reduce packet processing overhead by delivering data directly to user space, avoiding kernel crossings. ? This research was partially done when the first author was at UIUC. Nonetheless, both user-space network stacks and application processing tax server CPU cores <ref type="bibr" target="#b74">[75]</ref><ref type="bibr" target="#b75">[76]</ref><ref type="bibr" target="#b76">[77]</ref><ref type="bibr" target="#b77">[78]</ref><ref type="bibr" target="#b97">98]</ref>. Second, onesided RDMA <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b110">111,</ref><ref type="bibr" target="#b111">112,</ref><ref type="bibr" target="#b117">118,</ref><ref type="bibr" target="#b159">160,</ref><ref type="bibr" target="#b160">161]</ref> allows clients to bypass the server CPU and directly read or write server memory. However, the limited semantics of one-sided RDMA operations require multiple network round trips to serve a single request from a client <ref type="bibr" target="#b18">[19]</ref>. Finally, Smart NICs can perform more sophisticated remote operations in a single network round trip <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b82">83,</ref><ref type="bibr" target="#b93">94,</ref><ref type="bibr" target="#b129">130,</ref><ref type="bibr" target="#b153">154]</ref>, offering higher execution and energy efficiency than host CPUs <ref type="bibr" target="#b83">[84,</ref><ref type="bibr" target="#b93">94,</ref><ref type="bibr" target="#b171">172]</ref>. Nevertheless, this solution sometimes yields lower performance than the first and second solutions <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b115">116]</ref> for two reasons. First, cost, power, and form factor constraints limit Smart NICs memory capacity to O(10GB) <ref type="bibr" target="#b99">[100]</ref>, and applications that do not fit in on-NIC memory must access main memory over slow PCIe links <ref type="bibr" target="#b115">[116]</ref>. This leads to significant performance degradation <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b83">84,</ref><ref type="bibr" target="#b93">94,</ref><ref type="bibr" target="#b115">116,</ref><ref type="bibr" target="#b140">141]</ref>. Note that many modern datacenter applications have large working sets, often requiring not only DRAM but also higher-density byte-addressable NVM (e.g., Intel Optane Persistent DIMM <ref type="bibr" target="#b66">[67]</ref>) to cost-effectively provide capacity <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b83">84]</ref>. Second, Smart NICs use specialized accelerators and/or energy-efficient wimpy CPUs that are often unable to run applications with high performance. As such, processing must be partitioned and coordinated between Smart NIC and the faster server CPU, but frequent communications over slow PCIe links become a performance bottleneck.</p><p>As a result, system architects face a dilemma: whether to use (many) expensive CPU cores for application processing, or to suffer from the performance overhead incurred by multiple round trips over network or PCIe links. This paper proposes an alternative, RAMBDA that can efficiently serve memory-intensive ?s-scale datacenter applications. Specifically, RAMBDA envisions a server with a standard RDMA NIC (RNIC) and a cache-coherent accelerator (cc-accelerator) <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b68">69]</ref> connected to an cache-coherent off-chip interconnect (cc-interconnect) such as CXL <ref type="bibr" target="#b32">[33]</ref>. The cc-accelerator directly communicates with the RNIC for efficient network and application processing with little involvement of the server CPU. RAMBDA takes a modularized approach -the cc-accelerator is a separate device, not integrated with the RNIC -to allow the use of standard RNICs and support application-specific customization of the ccaccelerator. As such, RAMBDA is a cost-effective and efficient  <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b110">111,</ref><ref type="bibr" target="#b111">112,</ref><ref type="bibr" target="#b117">118,</ref><ref type="bibr" target="#b159">160,</ref><ref type="bibr" target="#b160">161]</ref> High High Low Low Low (Smart)NIC offloading <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b82">83,</ref><ref type="bibr" target="#b83">84,</ref><ref type="bibr" target="#b93">94,</ref><ref type="bibr" target="#b129">130,</ref><ref type="bibr" target="#b135">136,</ref><ref type="bibr" target="#b140">141,</ref><ref type="bibr" target="#b144">145,</ref><ref type="bibr" target="#b153">154]</ref> Low High Low High Low RAMBDA Low Low Low High High approach for accelerating a broader range of applications. To our knowledge, RAMBDA is the first work to explore the role of a cc-accelerator for end-to-end datacenter applications. RAMBDA consists of four logically-coupled software and hardware components. Specifically, we propose:</p><p>? A unified abstraction for inter-and intra-machine communications, using lockless ring buffers to facilitate inter-machine communications with one-sided RDMA write and CPU-accelerator communications with load/store; ? A fast and efficient mechanism to notify the cc-accelerator of requests, exploiting its access to coherence information; ? A cc-accelerator architecture for processing the requests and handling RNIC-CPU-accelerator interactions; and ? An adaptive device-to-host data transfer mechanism for a server with a DRAM-NVM heterogeneous memory system. We prototype RAMBDA using a commercial system based on an Intel Xeon 6138P CPU, which integrates an on-package FPGA connected to the CPU coherence bus. We evaluate three popular ?s-scale datacenter applications: <ref type="bibr" target="#b0">(1)</ref> an in-memory keyvalue store, where requests fully bypass the CPU; (2) a chain replication-based distributed transaction processing systeman example of a latency-sensitive system with NVM; and (3) a deep learning recommendation model (DLRM) inference, which requires collaboration between a server CPU and a cc-accelerator to process requests. We show that RAMBDA provides 30.1-69.1% lower latency, up to 0.2?2.5? throughput, and 3? higher power efficiency than current state-of-the-art solutions. However, RAMBDA is not a panacea -for the cases where it is significantly slower than the current CPU-based solutions, we find that RAMBDA's memory bandwidth can be a critical bottleneck compared to other solutions, limiting the speed data is retrieved in bandwidth-bound workloads. Hence, we also envision and demonstrate that a future cc-accelerator with accelerator-local memory can further improve latency and throughput by 11.2% and 62.1?, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND AND MOTIVATION</head><p>A. RDMA Primer RDMA allows machines to access the memory of remote machines at high bandwidth and low latency. RDMA has now been widely deployed by datacenters <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b165">166,</ref><ref type="bibr" target="#b179">180]</ref> and used to build various research and production systems. RDMA offloads the network transport stack to RNIC hardware and supports operations completely in user space, bypassing the kernel. One-sided RDMA operations (e.g., read/write/atomics) completely bypass the remote server's CPU for remote memory accesses. Meanwhile, two-sided RDMA operations (e.g., send/receive) are similar to conventional network communications (e.g., TCP/UDP) as they involve the CPUs of both clients and servers for data transmission.</p><p>The key data structures in RDMA programming are queue pair (QP) and completion queue (CQ), shared between the host (user space) and the RNIC. A QP consists of two work queues (WQs): a send queue (SQ) and a receive queue (RQ), both ring buffers in the host memory. To post an RDMA operation, the user writes to a work queue entry (WQE) at the tail of the WQ with a pre-defined device-specific format and rings the RNIC's doorbell register using an MMIO write. Upon completion of the RDMA operation, the RNIC (optionally) writes to a completion queue entry (CQE) at the tail of the CQ (also a ring buffer in the host memory) associated with the QP. Hosts learn about operation completion by polling the CQ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Dilemma of Using Smart NIC</head><p>Recent Smart NICs integrate either FPGA or customized lowprofile CPU with NICs. Prior work has shown that Smart NICs running datacenter applications can offer higher performance and energy efficiency than host CPUs <ref type="bibr" target="#b83">[84,</ref><ref type="bibr" target="#b93">94]</ref>. However, Smart NICs have limited memory capacity (O(10 GB) <ref type="bibr" target="#b99">[100]</ref>) under cost, power, and form factor constraints. As such, they often need to access the host memory when running applications with large working sets. Unfortunately, such host memory accesses are not cheap, primarily because they must go through PCIe links. This has been identified by multiple system designs <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b83">84,</ref><ref type="bibr" target="#b93">94,</ref><ref type="bibr" target="#b115">116,</ref><ref type="bibr" target="#b140">141]</ref>. We also quantify this effect using an experiment on a NVIDIA BlueField-2 Smart NIC (see Sec. V for the detail of our testbed). We run a simple benchmark on the NIC's ARM cores, which randomly access data from one of 1 GB pre-allocated buffers, one in the NIC's on-board DRAM and the other in the host DRAM. The benchmark accesses the on-board DRAM with load/store instructions and accesses the host DRAM with one-sided RDMA read/write (we use direct verbs <ref type="bibr" target="#b136">[137]</ref> to minimize the RDMA software stack overhead). In each request from the benchmark, we consecutively access the memory 100 times (64 bytes for each access). We report the request latency for different percentages of local/host access in Fig. <ref type="figure" target="#fig_0">1</ref> (each data point is based on 1M requests). For example, "80%" means 80% of the 100 memory accesses are to the host DRAM and the remaining 20% are to the on-board DRAM. Both the average and 99 th -percentile tail latency values increase linearly with more memory accesses to the host DRAM. This is attributed to long latency of going through the physical PCIe link, memory management unit (MMU), DMA engine, and I/O controller. Such a high overhead can also affect throughput, although host memory accesses can be batched/pipelined to hide latency.</p><p>Consequently, the Smart NIC is only well suited for applications with working sets that are either small enough to fit in the Smart NIC's local on-board memory or effective for caching (i.e., sufficient spatial locality or skewed distributions of memory accesses). Otherwise, the frequent communications between the NIC and the host will adversely affect the end-to-end performance of applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Cache-coherent Interconnects and Accelerators</head><p>Originally, cc-interconnects were developed for NUMA systems where CPUs share the memory space in a cache-coherent manner; recent examples include UPI and Infinity Fabric. More recent cc-interconnect designs such as CXL <ref type="bibr" target="#b32">[33]</ref>, CAPI <ref type="bibr" target="#b147">[148]</ref>, and CCIX <ref type="bibr" target="#b24">[25]</ref> support fine-grained data sharing between CPUs and accelerators (accessed via standard chip-to-chip physical links such as PCIe). Accelerators built on such cc-interconnects are referred to as cc-accelerators. Because host-accelerator communication is cheaper, especially for small data sizes, cc-accelerators can be more efficient than conventional accelerators.This makes cc-accelerators suitable for ?s-scale acceleration/offloading. Currently, some cc-accelerators are commercially available <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b68">69]</ref>, and more are likely to emerge as CXL support becomes more widespread <ref type="bibr" target="#b124">[125]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. RAMBDA SYSTEM ARCHITECTURE</head><p>We depict a high-level system architecture of RAMBDA in Fig. <ref type="figure" target="#fig_1">2</ref>. RAMBDA envisions a system comprising a cc-accelerator, a standard RNIC, and a CPU, all cooperating for efficient network and application processing. Specifically, (1) the cc-accelerator not only offloads parts of application processing from the server CPU but also directly interacts with the RNIC for client communication without involving the server CPU; (2) the standard RNIC handles network stack processing; and (3) the server CPU tackles accelerator-unfriendly (irregular and branch-rich) part of application processing ("fast path") in addition to initialization, control, and management of hardware resources, applications, and network connections ("slow path"). The synergistic orchestrations among (1) -( <ref type="formula">3</ref>) are facilitated by RAMBDA's four software and hardware components described in this section. Also, RAMBDA envisions a unified memory subsystem with both CPU-attached and accelerator-attached physical memory, which should be in the same address space and coherence domain. Depending on different workloads, the latency/bandwidth requirements of the accelerator-attached memory may vary, but we do envision that the capacity is at the same level as the CPU-attached memory <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b67">68]</ref> so that more application data can be mapped to the accelerator-attached memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Inter-and Intra-machine Communication</head><p>RAMBDA provides a fast and efficient communication abstraction for both inter-machine communication (between a server and clients) and intra-machine communication (between a server's CPU and cc-accelerator). Although the underlying data structure (lock-free ring buffer) is not new and has been employed before <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b153">154]</ref>, our abstraction unifies inter-and intra-machine communication using the same programming model with unique features of cache-coherence and RDMA. For each client-server connection, we establish a pair of a request ring buffer (in the server memory) and a response ring buffer (in the client memory) for inter-machine RDMA communications. For example, buffer-1 on server-A and buffer-A on client-1 form a request-response buffer pair for a connection between client-1 and server-A in Fig. <ref type="figure" target="#fig_1">2</ref>. Further, for each accelerator in a server, we establish one request-response ring buffer pair in the server memory for intra-machine communications. One-sided RDMA write is used by both servers and clients for high-performance inter-machine communications through message passing where all the underlying network transport processing is offloaded to the RNIC <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b153">154]</ref>. For intra-machine communications, leveraging the shared coherence domain, the server CPU or cc-accelerator directly writes and read the ring buffers. This abstraction not only guarantees high performance by avoiding extra CPU cycles on the communication stack, but also facilitates the accelerator operations, as it can fetch application data directly (instead of accessing intermediate data first, like any form of descriptor).</p><p>Note that we do not share the ring buffers (and the underlying RDMA QPs for the inter-machine communications) across different client-server connections, to avoid performance overheads with atomic updates or consistency issues at the head/tail of the buffer without atomic updates. However, we do allow sharing the ring buffers (and the RDMA QPs) across threads on the same machine for better scalability, as a software layer/library can manage cross-thread contentions with slight performance overheads <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b112">113,</ref><ref type="bibr" target="#b154">155]</ref>. Specifically, we employ the Flock's method <ref type="bibr" target="#b112">[113]</ref>  thread on the client for request synchronization and dispatch, so that there is only one request-response buffer pair (and QP) per client-server pair per application and observe no performance loss compared to native RDMA primitives. The client is responsible for tracking the tail of the request buffer in the server memory and the head of the response buffer in its local memory, similar to the credit-based flow control <ref type="bibr" target="#b85">[86]</ref>. Whenever it writes a message to the request buffer, it will update its local record of the request buffer's tail; whenever it receives a message in the response buffer (by polling), it will update its local record of the response buffer's head and reset the buffer entry to "0". Only if the request buffer's tail is behind the response buffer's head can the client issue a request. Otherwise, it knows that the buffer is full of onthe-fly requests and should not send more requests. A similar mechanism is applied to the server for request buffer's head and response buffer's tail. This guarantees that any message can be passed by only one network trip without any conflict.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Coherence-assisted Accelerator Notification</head><p>Since messages are directly written to request buffers in server memory, the RAMBDA cc-accelerator needs to detect their arrival. Typically, this would be done with spin-polling. However, frequent polling wastes the limited bandwidth of the cc-interconnect, leaving little available for accessing memory during application processing. Polling also leads to high power consumption <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b51">52]</ref>, fast transistor aging <ref type="bibr" target="#b121">[122]</ref>, and poor scalability with many queues <ref type="bibr" target="#b51">[52]</ref>. Hence, we propose a coherence-assisted notification mechanism, called cpoll.</p><p>Conceptually and semantically, cpoll is similar to MWAIT in the x86 architecture <ref type="bibr" target="#b64">[65]</ref>, HyperPlane's QWAIT <ref type="bibr" target="#b108">[109]</ref>, and PCIe's lightweight notification (LN) proposal <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b131">132]</ref>. cpoll differs from them because it is designed to be portable and platformagnostic (i.e., requires no CPU modification and no specific protocol) for off-chip devices. Specifically, we insert a cpoll checker in the datapath of the coherence controller's port connected to the cc-interconnect. During initialization, we first allocate the inter-and intra-machine communication request buffers (Sec. III-A) in a contiguous region of server memory (i.e., cpoll region), and register this region to the cc-accelerator's cpoll checker for snooping, as depicted in Fig. <ref type="figure" target="#fig_2">3(a)</ref>. Then, when the cc-accelerator's coherence controller receives a coherence signal from the registered address region (e.g., Modified ? Invalid), it will notify the cc-accelerator of arrival of a request. The cpoll checker will only need to monitor a single address region illustrated in Fig. <ref type="figure" target="#fig_2">3(a)</ref>. If the address of a coherence signal falls into this region, the cpoll checker can identify which request buffer (associated with a specific client or the server CPU) received a new request based on its address. By monitoring only one region, and using consecutive, fixed-size buffers, this dispatch is trivially scalable. (Even if buffers are not allocated consecutively in memory, HyperPlane <ref type="bibr" target="#b108">[109]</ref> has demonstrated reasonable address lookup overhead to thousands of buffers.)</p><p>To implement the cpoll mechanism, we propose two approaches. First, we allocate the cpoll region in CPU-attached memory, and then pin the region on the cc-accelerator's local cache. Note that the cc-accelerator resets the request buffer entry associated with a cpoll signal after it processes the request. This makes the cc-accelerator's local cache always own the cpoll region from the cache coherence viewpoint, so any change to the cpoll region by clients or the server CPU triggers a coherence signal. Alternatively -though our prototype platform does not support this -the cpoll region could be allocated from memory attached to the cc-accelerator. As such, any request from the RNIC to the request buffers in server memory will go through the cc-interconnect. Subsequently, they will be delivered to the cc-accelerator's coherence controller which is responsible for monitoring any change to the cpoll region.</p><p>The first approach is feasible with our prototype platform (Sec. V), but the size of request buffers is constrained by the cc-accelerator's local cache size, limiting its scalability at the moment. When the scale of the system is large (i.e., many request buffers) or each request itself is large (i.e., large buffers), we cannot pin the entire cpoll region on the ccaccelerator's cache. To tackle this scalability issue in our setup, we introduce a data structure called pointer buffer where each 4-byte entry corresponds to each inter-or intra-machine request buffer and stores a pointer (or index) to an entry in the request buffer, as depicted in Fig. <ref type="figure" target="#fig_2">3(c</ref>). Subsequently, we register the pointer buffer allocated to a contiguous address space as the cpoll region. When writing a new request to a request buffer in the server memory, a client or the server CPU will also increment the value of the pointer buffer entry corresponding to the request buffer such that the pointer buffer entry points to the request buffer tail. For a remote client, this can be efficiently done by posting two contiguous WQEs (only the second one is signaled) with a batched doorbell to the RNIC <ref type="bibr" target="#b76">[77]</ref> or remapping/interleaving the two buffers with user-mode memory registration (UMR) <ref type="bibr" target="#b119">[120]</ref> and only posting one WQE.</p><p>Note that one additional small PCIe write to the server side is inevitable in both ways. However, since RAMBDA has already reduced the PCIe traffic and mainly leverages the coherence traffic, such overhead will not notably hurt the overall performance, which is confirmed by our experiments in Sec. VI. In addition, as a 4-byte pointer buffer entry covers an entire request buffer, which can be as large as several MBs for some applications such as the one described in Sec. IV-B, it can substantially reduce the memory space requirement for the cpoll region. Finally, coherence signals are not guaranteed to come in the order of actual data writes. However, this does not affect the correctness of cpoll because it is designed to be used with a ring buffer, and leverages the semantics of the ring buffer, i.e., request buffer entries are written in order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. RAMBDA cc-accelerator Architecture</head><p>The RAMBDA coherence controller (Fig. <ref type="figure" target="#fig_3">4</ref>) handles all the coherence traffic (both regular read/write and cpoll) to or from the cc-accelerator, as well as the virtual-physical address translation (i.e., TLB). The local cache is also in the coherence domain and handled by the coherence controller. The RAMBDA cc-accelerator may have its own local memory controller and memory <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b31">32]</ref> that constitutes the unified memory space with the CPU memory. In such an architecture, the CPU may allocate application data to the cc-accelerator's local memory, as the NUMA-aware memory management does in the modern Linux kernel.</p><p>The scheduler fetches cpoll signals associated with different request buffers based on a given scheduling algorithm. The nature of the coherence bus may cause cpoll signals to be coalesced. For example, two updates to the same entry in the pointer buffer in a short period might generate only one cpoll signal. However, we leverage the semantics of the ring buffer, by tracking the previous tail of the request buffer in the cc-accelerator. This allows it to determine how many new requests were received since the last notification and inform the application processing unit (APU) accordingly.</p><p>The RDMA SQ handler is responsible for direct RNIC control. It assembles the response information from the APU into WQE format, then writes it to the corresponding RDMA connection's WQ, and rings the RNIC's doorbell register. Since the cc-accelerator is in the coherence domain with unified address space, we do not need to store the entire WQ into the SQ handler. Instead, during the initialization, only the starting address and number of entries of each WQ is registered to the SQ handler from the software. Later, upon the response, it will write information into the WQ one by one, causing little scalability overhead on the cc-accelerator.</p><p>Since polling the CQ is not on the critical datapath, we do not process it with the cc-accelerator. Instead, we use a single CPU core to handle all the CQs polling and bookkeeping.</p><p>Unsignaled WQE <ref type="bibr" target="#b76">[77]</ref> is applied here so that only the selected operations will notify the CQ of their completion. This can alleviate the overhead of RNIC-CPU communication when the CPU is polling multiple CQs. Besides, this helps reduce unnecessary traffic on the cc-interconnect.</p><p>The APU is the only application-specific part in the entire RAMBDA architecture, yielding a fine balance between RAMBDA programmability and user implementation complexity. It provides the user with standard interfaces for (1) cpoll signal reception, (2) coherent data read/write, and (3) RDMA WQE output. First, a (de)serializer can be optionally used, if the application uses an RPC protocol for inter-machine communications <ref type="bibr" target="#b88">[89]</ref>. Then, to process requests, we typically need a data structure walker <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b84">85,</ref><ref type="bibr" target="#b102">103,</ref><ref type="bibr" target="#b167">168,</ref><ref type="bibr" target="#b170">171]</ref> to find the location of the target data of the request. To maximize the memory-level parallelism and hide the memory access latency, multiple outstanding requests and out-of-order execution should be supported. Inspired by the stateful network function accelerator <ref type="bibr" target="#b132">[133]</ref>, we employ a table-based finite state machine for this purpose, where the outstanding request status is stored in a TCAM or cuckoo hash table <ref type="bibr" target="#b176">[177]</ref> for fast lookup. Upon the arrival of a new request or intermediate result, the corresponding TCAM or hash table entry is updated and then the next-step action is issued to a corresponding functional unit (e.g., ALU or coherence controller).</p><p>The APU should invoke the CPU in two scenarios. The first scenario is when a library call or OS syscall is needed. For example, if the user space memory pool has been pre-allocated by the CPU (malloc/mmap), the APU itself can allocate objects for new data in the memory pool <ref type="bibr" target="#b93">[94]</ref>; if not, malloc is called each time when a new object is needed. The second scenario is when CPU is more suitable than the APU for a certain part of application processing. For example, in a recommender inference system (see Sec. IV), while the APU can handle the embedding reduction and fully-connected layers, the request preprocessing (e.g., transforming a human-readable request to a model input) should still run on the CPU due to its irregularity and complexity. In these scenarios, the cc-accelerator and CPU interact with low latency in a fine-grained manner described in Sec. III-A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Optimizing Device-host Data Transfer: Adaptive DDIO</head><p>Having the RAMBDA system design, we finally consider the optimization of device-memory-cache interaction inside a single machine, or specifically, how to choose between the cache and memory as the data destination for optimal device-host data transfer. Given the data-intensive nature of RAMBDA's usage scenarios, this optimization is notably important for the entire system. As the device I/O speeds increase, Intel introduced data-direct I/O (DDIO) <ref type="bibr" target="#b69">[70]</ref>, a CPU-wide technology, to allow the device to directly inject data to the CPU's last level cache (LLC) instead of main memory. This reduces memory bandwidth consumption and latency required by I/O. DDIO has been proven to be effective <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b86">87,</ref><ref type="bibr" target="#b104">105,</ref><ref type="bibr" target="#b130">131,</ref><ref type="bibr" target="#b152">153,</ref><ref type="bibr" target="#b161">162,</ref><ref type="bibr" target="#b169">170]</ref>, improving the performance of DRAM-based systems <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b86">87]</ref>. However, it does not always improve performance of NVM-based systems <ref type="bibr" target="#b73">[74,</ref><ref type="bibr" target="#b161">162]</ref>. which is increasingly deployed by datacenters to cost-effectively provide large memory capacity for applications such as in-memory database <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b17">18]</ref>. This is mainly because of two reasons. (1) NVM usually has a larger access granularity than DRAM and cache. For example, the access granularity of the Intel ? Optane DIMM is 256 bytes while that of DRAM and cache is 64 bytes in Intel-based system <ref type="bibr" target="#b166">[167]</ref>. When the DDIO-ed data is evicted from LLC to NVM, the write-back to the NVM will be randomized because of the cache replacement policies. As a result, write amplification wastes the bandwidth of NVM <ref type="bibr" target="#b73">[74]</ref>, which is already lower than that of DRAM. (2) CPU caches are typically not persistent; Intel ? eADR <ref type="bibr" target="#b63">[64]</ref> makes cache as part of the persistency domain but it requires a large battery and has high power consumption <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b15">16]</ref>. As such, applications often have to flush data in cache to NVM to remain correct in case of a crash, with performance cost <ref type="bibr" target="#b150">[151]</ref>.</p><p>To tackle the aforementioned limitation, we propose to exploit a rarely-discussed field in the PCIe packet header, TLP processing hints (TPH). It is the 16 th bit in the PCIe header and a performance feature that allows the CPU to prefetch or keep certain PCIe writeback data in LLC for quick consumption by CPU cores <ref type="bibr" target="#b122">[123]</ref>. To our best knowledge, no current commercial I/O device (including SSD and NIC) uses the TPH bit; it is always set to 0 as a placeholder in both hardware and device drivers.</p><p>Our experiment confirms that changing the TPH bit allows us to control the destination of data to either LLC or memory per PCIe packet. Being the first to clarify the DDIO-TPH relationship on modern and general server platforms, we perform an experiment running PCIe-bench <ref type="bibr" target="#b115">[116]</ref> on a VC709 FPGA board <ref type="bibr" target="#b164">[165]</ref> in which we implement a module that allows an API to set the TPH bit on-the-fly for each PCIe packet. The FPGA DMAs (random write) data to the (DRAM-based) host at a constant speed of 3.5GB/s. We measure the memory bandwidth consumption on the host side in four configurations (DDIO on/off + TPH on/off) in Fig. <ref type="figure" target="#fig_4">5</ref>. Only when both DDIO and TPH are off, do we observe large memory bandwidth consumption (i.e., ?3.5GB/s for both read and write), which is aligned with the DMA throughput reported by the FPGA. This indicates that all DMA data is sent to the main memory. Otherwise, if either DDIO or TPH is on, there will be little memory bandwidth consumption, meaning data is sent to the LLC directly.</p><p>Since TPH is applicable to each PCIe packet, we propose two guidelines for future systems with heterogeneous memory, as depicted in Fig. <ref type="figure" target="#fig_5">6</ref>. (1) DDIO should be disabled globally on the CPU by default.</p><p>(2) The device should expose the knob of changing the TPH bit for the programmer. Taking RNIC as an example, one way to do this would be to make it a configuration parameter set when registering a memory region to the RNIC, specifying whether the registered address range belongs to DRAM or NVM (or based on more flexible conditions). Later, when executing an RDMA operation (e.g., write), the RNIC hardware will set the TPH bit only for operations in DRAM regions, avoiding DDIO-induced write amplification on NVM regions. While this requires hardware modifications to the RNIC, our experiment that adds this functionality to the original PCIe-bench shows that it adds almost no cost to the NIC hardware design. Following these two guidelines, we can make DDIO NVM-aware independently for each I/O device.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. RAMBDA Programming Model</head><p>We provide a software user-space framework/interface for upper-level applications. An application needs to register itself to the RAMBDA framework with initialization information, such as connection establishment, the memory region of the application data, and the targeting cc-accelerator. The RAMBDA framework then is responsible for allocating the request/response buffers by mmap/malloc calls and registering the buffers to the RNIC by verbs APIs. Also, it makes the application's and the request/response buffers' virtual memory space visible to the cc-accelerator. To pin the cpoll region to cc-accelerator's local cache, the framework writes the cc-accelerator's configuration registers, which are accessible in user-space as well through MMIO. The coherence controller then will never evict the corresponding cachelines from the local cache. We do not restrict any programming model for the APU, as there could be a huge diversity in different applications, and it is not the focus of this work. Programmers may refer to other related work <ref type="bibr" target="#b103">[104,</ref><ref type="bibr" target="#b170">171,</ref><ref type="bibr" target="#b173">174]</ref> to explore how a single model/instance can support many applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. RAMBDA Scalability</head><p>Scalability with faster network. As the speed of network is growing fast, a critical question is whether RAMBDA will keep up with the speed of future network (e.g., 400Gbps). First, in our implementation (Sec. V) and experiments (Sec. VI), the cc-interconnect's bandwidth is not saturated in RAMBDA KV and RAMBDA TX. Furthermore, accelerator-attached memory with comparable capacity to the CPU <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b67">68]</ref> will further liberate the bandwidth of the cc-interconnect from application-related memory requests. Hence, the cc-interconnect can better serve the RNIC/CPU-cc-accelerator interaction. This means RAMBDA will be bottlenecked by the network bandwidth and can achieve higher performance with newer network technologies (also note that the cc-interconnect performance will evolve as well). Scalability with larger cluster. With the advance in SoC technologies <ref type="bibr" target="#b117">[118,</ref><ref type="bibr" target="#b118">119]</ref> (e.g., larger on-NIC cache for connection information), the previous scalability issues and on-NIC resource constraints of reliable RDMA connections <ref type="bibr" target="#b74">[75,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b89">90]</ref> have been alleviated. Building on top of RDMA, RAMBDA offers the same scalability as RDMA does. The dedicated buffer pair does not limit scalability as each buffer is small, e.g., 1MB for 1K entries that are enough for most request sizes and saturate network bandwidth in modern datacenters <ref type="bibr" target="#b79">[80]</ref>. To support connections from 1K clients, each running a single application, a server only needs to allocate 1GB of its main memory, a small fraction of the total memory capacity of modern servers. Also, sharing the buffers across threads does not limit scalability either <ref type="bibr" target="#b112">[113]</ref>.</p><p>In fact, RAMBDA is not strictly bound to the existing commodity RDMA technology, and more recent remote memory access (RMA) variants <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b142">143,</ref><ref type="bibr" target="#b145">146,</ref><ref type="bibr" target="#b158">159]</ref> (as long as they share the same semantics) can be adopted to RAMBDA for better scalability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. RAMBDA USE CASES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. In-Memory Key-Value Store</head><p>In-memory key-value store (KVS) is a basic building block of many datacenter services. In addition to software optimizations, researchers have leveraged all the three directions mentioned in Sec. I for further KVS acceleration. The major requirement of KVS is high memory access parallelism across requests.</p><p>An RAMBDA design for KVS, dubbed RAMBDA KV aims to fully offload request processing to the cc-accelerator. At the algorithm/data structure level, RAMBDA KV is similar to MICA <ref type="bibr" target="#b97">[98]</ref>, but RAMBDA KV follows the architectural description of Sec. III-C at the hardware level, including a pipelined hash unit for hash value/index calculation. RAMBDA KV performs a GET/UPDATE request by calculating the hashed key value and finding the corresponding entry in the set-associated hash table's bucket. The entry contains a pointer to the actual key-value data. For PUT requests, after finding the address where a new key-value pair should be allocated (i.e., an empty entry in the bucket), the slab allocator will simply put it in the pre-defined memory pool. If the bucket indexed by the hashed key is full (i.e., hash collision), another bucket with the same format will be allocated and linked to the existing bucket by a pointer. Similar to KV-Direct <ref type="bibr" target="#b93">[94]</ref> and MICA <ref type="bibr" target="#b97">[98]</ref>'s study, on average, each GET request requires three memory accesses and each PUT request requires four.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Distributed Transaction with NVM-based Chain Replication</head><p>Distributed transactional systems are widely used by datacenters to provide the ACID feature for distributed storage systems. To this end, cross-machine protocols for data replication are usually needed, and chain replication <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b107">108,</ref><ref type="bibr" target="#b113">114,</ref><ref type="bibr" target="#b123">124,</ref><ref type="bibr" target="#b123">124,</ref><ref type="bibr" target="#b128">129,</ref><ref type="bibr" target="#b137">138,</ref><ref type="bibr" target="#b149">150,</ref><ref type="bibr" target="#b151">152,</ref><ref type="bibr" target="#b155">156,</ref><ref type="bibr" target="#b177">178]</ref> is a popular primary-backup replication protocol. In chain replication, machines are virtually organized into a linear chain. Any change to the data will begin at the head of the chain and pass through the chain. When the last machine in the chain makes the change in its log, it will back-propagate the ACK signal through the chain so that each machine can locally commit the transaction. When the head of the chain commits the transaction, it sends the ACK signal back to the client, marking the end of the transaction.</p><p>The state-of-the-art work, HyperLoop <ref type="bibr" target="#b82">[83]</ref>, leverages the RNIC and NVM to achieve low-latency chain replication with little CPU involvement. Specifically, it proposes and implements group-based RDMA primitives, which can be triggered automatically by the RNIC. One key-value pair (addressed by the offset in the NVM space) is modified in the entire chain once the client initiates a group-based RDMA operation. However, due to the restricted (group-based) RDMA semantics, to process multi-value transactions, the client needs to sequentially issue RDMA operations for each key-value pair, which often leads to long latency in the network and PCIe link.</p><p>An RAMBDA design for such a distributed transaction system, dubbed RAMBDA TX is similar to RAMBDA KV with respect to request processing, but it additionally implements a concurrency control unit in the APU. That is, any single keyvalue pair can only be accessed by one outstanding transaction, and the other related transactions will be buffered in the queue in the order of arrival. The concurrency control unit is a small hash table, and its entries are indexed by the key of the keyvalue pair. Key-value pairs are stored in the NVM and accessed by the address offset relative to the starting address, which is the same as HyperLoop. Also, it adds the functionality of chain-based communication across replica machines. The intermachine communications still rely on ring buffers described in Sec. III-A, but the ring buffers are allocated in the NVM as the redo-log for failure recovery. One log entry (transaction) can contain multiple (data, len, offset) tuples, and the first byte of the log entry indicates the number of tuples. One exception is pure read transactions. Similar to HyperLoop, since the chain replication protocol already provides data consistency, a client can conduct a pure read transaction by directly accessing the chain's head/tail machine with one-sided RDMA read.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. DLRM Inference</head><p>DLRMs have received much attention by Internet giants <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b80">81,</ref><ref type="bibr" target="#b114">115]</ref> as they can offer more revenue and better user experience. In an end-to-end recommender system, the most expensive part of serving an inference request is the embedding reduction step, consuming huge memory capacity <ref type="bibr" target="#b114">[115,</ref><ref type="bibr" target="#b174">175,</ref><ref type="bibr" target="#b175">176</ref>] and 1  2 ? 3 4 of the inference time <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b80">81,</ref><ref type="bibr" target="#b91">92]</ref>. The embedding reduction operation processes queries on a set of features. It finds a (sparse) embedding vector in the embedding table (a high-dimension matrix) and aggregates a value. The values of all features are assembled as the result. Also, the embedding reduction is bounded by memory bandwidth and exhibits poor data locality <ref type="bibr" target="#b80">[81,</ref><ref type="bibr" target="#b91">92,</ref><ref type="bibr" target="#b171">172]</ref>. Last but not least, it also incorporates routines like request parsing and transforming (pre-processing), which are irregular and branch-rich. As such it is not suitable for hardware accelerators. These characteristics make it unsuitable, if not impossible, to be fully offloaded to any Smart NIC or cc-accelerator. In addition to acceleration with specialized hardware like in-memory processing <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b80">81,</ref><ref type="bibr" target="#b87">88,</ref><ref type="bibr" target="#b127">128]</ref>, MERCI <ref type="bibr" target="#b91">[92]</ref> takes an algorithmic way to memoize sub-query grouped results to reduce memory pressure on the commodity server platform. Different from RAMBDA KV and RAMBDA TX, we design RAMBDA DLRM as an example of CPU-accelerator collaboration for request processing. Upon receiving a request from a client, the cc-accelerator first goes through the RPC stack, and then passes the request to the CPU through the ring buffer, where the request is parsed and transformed to model-ready input. Now, the input (request) is passed again to the cc-accelerator's APU, where the full inference, especially embedding reduction, is done. Finally, the cc-accelerator sends the result (response) back to the client through the RNIC. Empirically, we observe that one CPU core with 60% usage can already keep up with the network and the cc-accelerator processing rate. In DLRM, not all memory accesses in a single query need to be serialized. Hence, in the APU, we issue 64 memory requests for each query's iteration so that the memory bandwidth can be fully utilized and the memory access latency can be hidden. Lastly, the ALU is enhanced to support various aggregation operators (e.g., max/min/inner product).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. IMPLEMENTATION AND METHODOLOGY</head><p>We prototype RAMBDA with a system containing two Intel ? Xeon Gold 6138P CPUs with an in-package FPGA. The configurations of the system are listed in Tab. II.</p><p>We implement a round-robin scheduler. The APU can support 256 outstanding requests. Each request buffer has 1024 entries. We adopt HERD's RPC protocol <ref type="bibr" target="#b75">[76,</ref><ref type="bibr" target="#b76">77]</ref> for its simplicity, but any advanced RPC stack could be applied <ref type="bibr" target="#b88">[89]</ref>. The resource utilization numbers in Tab. II reflect our RAMBDA key-value store accelerator (Sec. IV). The utilization results for the other applications we have built are similar, because ? 80% of used resources go to the common components of the coherence controller and the local cache.</p><p>The current implementation has two major limitations. The first one is the performance of the coherence controller. As a soft design in the programmable fabric of the FPGA, it suffers from synthesis constraints and can perform at at most 400 MHz, incurring limited data access performance, which has also been observed by prior work <ref type="bibr" target="#b88">[89]</ref>. However, its counterparts on a regular server CPU can operate at ?2 GHz <ref type="bibr" target="#b0">[1]</ref>. We expect such infrastructural parts can be fixed by hard IP in future FPGAs, offering comparable performance to that of CPUs <ref type="bibr" target="#b67">[68]</ref>. The FPGA also lacks local memory in the same coherence domain or with comparable capacity to CPU-attached memory. Consequently, most application memory requests must go through the cc-interconnect (due to their large working sets), similar to cross-NUMA memory access. Additionally, the cpoll region must be pinned in the cc-accelerator's local cache. We expect these limitations would be lessened when RAMBDA is implemented in CXL-based devices <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b53">54]</ref> or Enzian <ref type="bibr" target="#b29">[30]</ref>.</p><p>To explore the potential of RAMBDA's performance on future platforms we also use a stand-alone U280 FPGA card <ref type="bibr" target="#b163">[164]</ref> with 32 GB DDR4 memory and 8 GB HBM2 to emulate a cc-accelerator with local coherent memory <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b53">54]</ref>. Prior work <ref type="bibr" target="#b157">[158]</ref> has shown that these two types of memory can achieve ?36 GB/s and ?425 GB/s throughput, respectively. Specifically, we adapt the APU to the U280 card using either the DDR4 or HBM2 controller. The application data is mapped and initialized in the FPGA's local memory. Rather than interacting with a real RNIC, we emulate arrival of RDMA requests by generating requests within the FPGA matching the RDMA write rate measured on the testbed. We believe this emulation methodology offers correct and convincing results since coherence (of the application data) makes no big difference here after the data has been allocated and initialized in the FPGA-attached memory; during request processing, most memory traffic does not need to go across the cc-interconnect. For throughput experiments, we measure requests processed on the FPGA per second. For latency experiments, we compute the emulated end-to-end latency by combining application processing time measured on the U280 with the average latency of the rest of the stack. Specifically, we measure the average latency from a request's generation to its completion on the U280, then add the average full-system end-to-end latency without an APU, measured on the client machine. Note that this approach emulates average latency, so does not apply to tail latency measurements. In the following sections, we notate the U280 DDR4-based results as "RAMBDA-LD (local DDR4)" and HBM2-based results as "RAMBDA-LH (local HBM2)".</p><p>Lastly, we use the NVIDIA ConnectX-6-based BlueField-2 Smart NIC <ref type="bibr" target="#b118">[119]</ref> as the RNIC. It also provides eight ARM cores, which we use to compare against a Smart NIC approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EVALUATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Microbenchmark</head><p>To demonstrate the benefits of each RAMBDA component and avoid the network bandwidth as the bottleneck (as we will see in the following sections), we first run a single-machine microbenchmark and analyze RAMBDA's performance against the highly-optimized software solutions on multiple cores. Specifically, we use the CPU cores on the other NUMA node in the server to feed requests to the CPU cores or RAMBDA accelerator on the local NUMA node via shared memory buffer (to emulate the one-sided RDMA behavior). For each request, the core or RAMBDA accelerator will randomly pick a node from a predefined and permuted 10M-node in-memory linked list, traverse the two succeeding nodes, and return the value in the second node. In CPU solutions, we use one, eight, and 16 cores to perform the microbenchmark, each with a batch size of 16 requests (optimized for throughput). In RAMBDA, we have a "RAMBDApolling" variant that replaces cpoll with regular spin-polling to demonstrate cpoll's benefit. Empirically, we choose 30 FPGA clock cycles as the spin-polling interval. Also, we have a "RAMBDA-DDIO" variant on the NVM-based experiment, which always turns DDIO on, to show the benefit of the proposed adaptive DDIO mechanism. All RAMBDA variants have 16 connections to the cores on the other NUMA node. Since the (Skylake) CPU with in-package FPGA does not support Intel Optane DIMMs, we emulate NVM's behavior by adding latency and throttling memory bandwidth in the FPGA and the ARM emulation program. We follow NVM's characteristics from recent Optane-based studies <ref type="bibr" target="#b73">[74,</ref><ref type="bibr" target="#b166">167]</ref> to calibrate our emulation. Fig. <ref type="figure" target="#fig_6">7</ref> plots the throughput of different approaches. All DRAM-based results are normalized to single-core's throughout; and the NVM-based results are normalized to RAMBDA-DDIO. In the DRAM-based experiment, since the microbenchmark has little cross-thread contention, the CPU-based solutions scale almost linearly. On the other hand, RAMBDA-polling, limited by bandwidth of the cc-interconnect, is equivalent to ?8 cores' performance. By reducing the latency and traffic of polling, RAMBDA with cpoll further improves the throughput by ? 21.6%. Having more local memory for better bandwidth and latency characteristics, RAMBDA-LD and RAMBDA-LH further remove the cc-interconnect in the original RAMBDA, bringing 114.4% ? 165.6% more improvement. Also, for the NVM case, reducing the write amplification and thus bandwidth utilization, adaptive DDIO mechanism can improve the throughput by ? 20%, aligned with the number reported in the prior work <ref type="bibr" target="#b73">[74,</ref><ref type="bibr" target="#b161">162]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. In-Memory Key-Value Store</head><p>Due to the limited availability of devices, we run RAMBDA KV on one server and one client. We compare RAMBDA with two state-of-the-art baselines: highly-optimized open-source two-sided RDMA-RPC (MICA-backed) <ref type="bibr" target="#b75">[76,</ref><ref type="bibr" target="#b76">77]</ref> (denoted "CPU") and Smart NIC <ref type="bibr" target="#b93">[94,</ref><ref type="bibr" target="#b144">145]</ref>. For CPU, we use ten threads (cores) on the testbed to maximize KVS throughput. Each thread is fed with requests by one client instance (each with two dedicated Skylake cores) on the client machine (also equipped with the BlueField-2 Smart NIC). For RAMBDA, we also use 10 client instances to feed requests that are processed on the same RAMBDA accelerator. For Smart NIC, we use the Smart NIC's eight ARM cores to emulate the behavior of the specialized hardware in KV-Direct <ref type="bibr" target="#b93">[94]</ref> and StRoM <ref type="bibr" target="#b144">[145]</ref>. The ARM cores process the request, which is sent from the client Intel CPU by RDMA. The ARM cores retrieve necessary data from their server host through RDMA. ARM cores are not as efficient as specialized FPGA designs <ref type="bibr" target="#b93">[94,</ref><ref type="bibr" target="#b144">145]</ref> when processing KVS and accessing host memory, but the ARM cores' frequency is ? 10? higher, alleviating the efficiency gap. When running KVS entirely on the Smart NIC's on-board DRAM, we measure the eight ARM cores' peak throughput to match that of six Intel CPU cores. We pre-load 100M key-value pairs (64 B size, ?7 GB memory in total) and then access them using uniform and Zipfian 0.9 distributions. We test two types of workloads: read-intensive (100% GET), and write-intensive (50% GET, 50% PUT). The MICA-based mechanism <ref type="bibr" target="#b75">[76,</ref><ref type="bibr" target="#b76">77]</ref> eliminates concurrency issues (e.g., only allowing the "owner core" to read/write the data partition). As such, the performance of a heavy PUT workload differs little from the GET-only workload, matching results of prior work <ref type="bibr" target="#b76">[77,</ref><ref type="bibr" target="#b93">94]</ref>.</p><p>In CPU and RAMBDA, both the hash table and key-value pairs are stored in the host memory; in Smart NIC, we allocate 512 MB of the Smart NIC's on-board DRAM as a cache of the most recently accessed hash entries and key-value pairs. The cache-total ratio (512 MB : 7 GB) is roughly the memory capacity ratio <ref type="bibr">(16 GB : 192 GB)</ref>. We also test the impact of batching: in CPU and Smart NIC, we process requests in a batch to improve memory access efficiency <ref type="bibr" target="#b97">[98]</ref>. In RAMBDA, since the APU can already exploit the memory-level parallelism across requests <ref type="bibr" target="#b84">[85,</ref><ref type="bibr" target="#b102">103,</ref><ref type="bibr" target="#b167">168,</ref><ref type="bibr" target="#b170">171]</ref>, request batching is not needed; instead, we batch the doorbell signals to the RNIC <ref type="bibr" target="#b76">[77]</ref> when posting RDMA operations for response. These configurations resemble prior work <ref type="bibr" target="#b76">[77,</ref><ref type="bibr" target="#b88">89,</ref><ref type="bibr" target="#b93">94,</ref><ref type="bibr" target="#b97">98]</ref>.  We first show each design's peak performance (with batch size 32). Fig. <ref type="figure" target="#fig_7">8</ref> shows throughput; we find that the request distribution significantly affects Smart NIC's performance. With a uniform distribution (i.e., more than 90% memory accesses are to the host via PCIe), throughput is 27.2%-28.6% of that with a skewed Zipfian distribution (i.e., most memory accesses are local). And even the throughput with Zipfian distribution is only ?60% of that with pure on-board memory accesses. Conversely, the distribution does not affect CPU and RAMBDA's performance, since even with the Zipfian distribution, the KVS's memory footprint is larger than the CPU or FPGA's cache. Second, we observe that RAMBDA's peak throughput is 2.3%?8.3% higher than CPU. This is because the peak KVS throughput is bounded by the network bandwidth now, and RAMBDA's one-sided RDMA performs a little better than CPU's two-sided RDMA, which is aligned with prior studies <ref type="bibr" target="#b74">[75,</ref><ref type="bibr" target="#b117">118]</ref>. The throughput of RAMBDA-LD and RAMBDA-LH demonstrate this as well -extra memory bandwidth does not help improve performance (in fact, the UPI link is not saturated), since the network has reached its limit. Note that, having higher network bandwidth, using more CPU cores can still saturate it, as long as there is little dependency/contention across requests. And CPU's throughput may still be similar to RAMBDA's, as RAMBDA accelerator is not saturated now. So we believe the current data under 25GbE can faithfully reflect CPU and RAMBDA's difference.</p><p>Regarding latency, taking the 100% GET workload as an example (Fig. <ref type="figure" target="#fig_8">9</ref>), Smart NIC's performance is again affected by the request distribution. The PCIe link adds significant latency even if the accesses are batched. Meanwhile, we observe that RAMBDA's average latency is a bit higher than CPU. This is mainly because, unlike CPU, RAMBDA needs to access data through the UPI link, adding more time on the request processing critical path. This deficiency is overcome with RAMBDA-LD/LH's accelerator-attached memory -it only goes through the UPI to interact with the RNIC. Note that due to HBM's nature, RAMBDA-LH has a higher average latency than RAMBDA-LD since the workload is no longer bounded by memory bandwidth. For tail latency, RAMBDA is 52.0% lower than Smart NIC and 30.1% lower than CPU, because it not only mitigates PCIe overhead, but also has more stable behavior than the CPU core, whose performance is affected by factors like OS scheduling and CPU resource contention.</p><p>We also investigate the impact of the batch size on each design and demonstrate the results in Fig. <ref type="figure" target="#fig_9">10</ref> (since the KVS throughput is now network-bound, we do not include RAMBDA-LD/LH's throughput as they are the same as RAMBDA). For CPU and Smart NIC, batching can significantly improve throughput (e.g., ? 12?) -by batching the data accesses across requests, the memory bandwidth is efficiently utilized and the memory/PCIe latency is hidden. RAMBDA's throughput also benefits from batching (i.e., ? 2?) by reducing MMIO-based doorbell access <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b76">77]</ref> and surrounding sfence signals from the RAMBDA cc-accelerator, which is relatively expensive. On the other hand, unlike CPU and Smart NIC, RAMBDA's latency sub-linearly increases with batch size. This is because RAMBDA does not need to wait for the batch size of arrived requests to start processing, and the RNIC may execute the WQE promptly before the doorbell is rung <ref type="bibr" target="#b105">[106]</ref>.</p><p>Finally, we use the Intel RAPL interface <ref type="bibr" target="#b126">[127]</ref> (for CPU and DIMMs), IPMI tool (for the entire server box), and the FPGA's firmware (for the FPGA chip) to measure power consumption. Take the case in Fig. <ref type="figure" target="#fig_7">8</ref> as an example. We find that the CPU and Smart NIC's Intel/ARM CPUs consume ?90 Watts and ?15 Watts respectively when fully loaded, while RAMBDA's FPGA power is in the range of 24-27W to achieve peak throughput. This demonstrates RAMBDA cc-accelerator's ? 3? power efficiency over the beefy Intel CPU to achieve comparable performance; although the absolute power value of RAMBDA is ?2? higher than the Smart NIC, it still has a significant op per watt advantage, as demonstrated in Tab. III. All these lead to ? 38% power consumption reduction of the entire server box.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Distributed Transaction with NVM-based Chain Replication</head><p>We compare RAMBDA with HyperLoop <ref type="bibr" target="#b82">[83]</ref> since their results show that their mechanism outperforms CPU-based chain replication, especially in multi-tenant cloud environments. Following HyperLoop's example, we adopt RocksDB <ref type="bibr" target="#b43">[44]</ref>, a persistent key-value database, to use the emulated NVM (see Sec. VI-A) as a persistent storage medium for RAMBDA and HyperLoop. Since HyperLoop modifies the RNIC's firmware, which is not open-source, we use the ARM cores on the Smart NIC to emulate its behavior. We disable DDIO on the server.  As with previous experiments, we run the experiments on one client and one server. We use the two ports on the Smart NIC to have two replica machines (instances) in the same physical server; transactions are forwarded between the ports, as shown in Fig. <ref type="figure" target="#fig_10">11</ref>. The client's host CPU initiates a transaction and sends it to the server's port 0 ( 1 ). The corresponding processing unit (either a Smart NIC ARM core or RAMBDA) forwards the transaction to the client's Smart NIC ARM via port 0, which is attached to a RocksDB instance <ref type="bibr" target="#b1">( 2 )</ref>. The client's Smart NIC ARM routes the transaction to the server's port 1, where another RocksDB instance (and the corresponding processing unit) serves as the second replica ( 3 ). Finally, the transaction is sent back to the client's host CPU ( 4 ). Our measurements show that the ARM-based routing adds 2?3?s overhead, which resembles datacenter network latencies.</p><p>We initiate RocksDB with 100K key-value pairs and issue 100K transactions from the client to measure end-to-end latency. We test two key-value pair sizes (64B and 1024B) and two types of transactions with different (read, write) counts ((0,1) and (4,2), representative of real-world transactional systems <ref type="bibr" target="#b77">[78]</ref>). Since the RAMBDA Tx and HyperLoop identical mechanisms for pure-read transactions, we exclude such transactions from the evaluation.</p><p>We show latency results in Fig. <ref type="figure" target="#fig_11">12</ref> (note that since the transactions are issued by the client one by one, the latency improvement also reflects throughput improvements). For the (0,1) transaction, RAMBDA's performance does not differ from HyperLoop, since they experience the same overhead -one PCIe round-trip per replica machine and one round-trip over the 2-machine replication chain; RAMBDA may even be a bit (less than 3%) slower than HyperLoop since it also has the overhead of UPI link. However, when the transaction contains multiple operations, RAMBDA begins to show its advantage.</p><p>Unlike HyperLoop, RAMBDA's client only needs to issue one combined transaction request to the replication chain, and the RAMBDA accelerator will handle the transaction execution and chain replication protocol itself in a near-data manner -still one PCIe round-trip per machine and one round-trip over the chain. This saves network and PCIe latency, offering a 63.2%?66.8% reduction for average end-to-end latency and 64.5%?69.1% for 99th tail latency. Note that RAMBDA's latency can be further reduced with accelerators (FPGA) directly attached to NVM <ref type="bibr" target="#b67">[68]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. DLRM Inference</head><p>We base our experiments on MERCI <ref type="bibr" target="#b91">[92]</ref> and facebook's DLRM model <ref type="bibr" target="#b114">[115]</ref>. Since their open-source ones include single-machine versions only, we extend them to an RDMAbased end-to-end application (the networking part is similar to the optimized HERD <ref type="bibr" target="#b76">[77]</ref>) to reflect a real deployment.</p><p>We follow the configurations in the MERCI paper <ref type="bibr" target="#b91">[92]</ref> for our evaluation. We compare RAMBDA with the CPU-based software version on the popular Amazon Review dataset <ref type="bibr" target="#b59">[60]</ref> (electronics, clothing-shoe-jewelry, home-kitchen, books, sportsoutdoors, and office-products). Both the native embedding reduction and MERCI reduction are evaluated. The data is clustered and loaded into embedding and memoization tables by the CPU in MERCI's way. The embedding dimension is set to the default value of 64; for MERCI reduction, we build memoization tables with 0.25? the size of the original embedding tables.</p><p>Since query lengths (number of features) in the dataset are diverse, it is unfair to measure the per-query latency, so we only measure the throughput. For brevity, we only show the results of inference with MERCI reduction in Fig. <ref type="figure" target="#fig_12">13</ref> because the ones with native reduction show the same trend. The CPU-based version of MERCI scales linearly until eight cores (threads), which is bounded by the host memory bandwidth. For RAMBDA, however, we find poor throughput performance over all the datasets -only 19.7%?31.3% throughput of a single CPU core. After further analysis, we find this phenomenon is because <ref type="bibr" target="#b0">(1)</ref> unlike KVS, the nature of the embedding reduction in DLRM (i.e., pure and dense memory accesses within nested "for" loops, few branches, thousands of memory accesses per query) makes it relatively efficient on the CPU core -the instruction window and the load-store queue can be fully utilized; (2) the CPU core can leverage the entire bandwidth of the memory channels (i.e., ?120GB/s on our testbed) with good parallelism, while the RAMBDA cc-accelerator can only leverage the cc-interconnect's bandwidth, and the memory requests have to be issued serially from the FPGA's wimpy coherence controller (also observed in <ref type="bibr" target="#b41">[42]</ref>); (3) the compute-intensive fully connected layer is relatively lightweight in the model, making RAMBDA's hardware acceleration only a small portion in the end-to-end inference. Hence, with higher frequency and memory bandwidth, CPU outperforms the RAMBDA cc-accelerator. This deficiency can be solved by RAMBDA-LD and RAMBDA-LH, the future ccaccelerator we envision. Fully utilizing the two DDR4 channels (? 1  3 of the CPU memory channels' bandwidth), RAMBDA-LD is able to achieve 52.8%?95.3% throughput of the eight CPU cores. Furthermore, the 32-channel HBM2 eliminates Electro.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Clothing</head><p>Home. Books Sports.</p><p>Office. the memory bandwidth bottleneck in the reduction, leading to RAMBDA-LH's 1.6? ? 3.1? throughput improvement over the CPU <ref type="bibr" target="#b171">[172]</ref>, and the RDMA network becomes the limiting factor for higher end-to-end throughput. Note that, CPUs with integrated HBM2 <ref type="bibr" target="#b30">[31]</ref> in the near future may also achieve similar throughput compared to RAMBDA-LH; but similar to KVS (see Sec. VI-B), even with the same memory bandwidth (and thus the inference throughput), RAMBDA cc-accelerator shows better power efficiency over the CPU-based reduction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. RELATED WORK</head><p>Accelerating ?s-scale datacenter applications with emerging devices. In modern datacenters, commodity networking devices, especially programmable switches and Smart NICs, have been leveraged to accelerate datacenter applications. Such approaches include caching <ref type="bibr" target="#b72">[73,</ref><ref type="bibr" target="#b93">94,</ref><ref type="bibr" target="#b100">101,</ref><ref type="bibr" target="#b141">142]</ref>, compute offloading <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b92">93,</ref><ref type="bibr" target="#b93">94,</ref><ref type="bibr" target="#b106">107,</ref><ref type="bibr" target="#b138">139,</ref><ref type="bibr" target="#b139">140,</ref><ref type="bibr" target="#b144">145]</ref>, protocol offloading <ref type="bibr">[36, 37, 72, 83, 91, 95-97, 134, 141, 157, 169, 179]</ref>, etc. However, due to the limited memory capacity of such devices (e.g., O(10MB) of on-chip SRAM for programmable switches <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b81">82]</ref> and O(10GB) of on-board DRAM for Smart NICs <ref type="bibr" target="#b99">[100,</ref><ref type="bibr" target="#b118">119]</ref>), they may fall short of efficiently handling datacenter applications requiring large memory footprints. Also, extending the memory space to the host side <ref type="bibr" target="#b81">[82,</ref><ref type="bibr" target="#b172">173]</ref> can only handle simple/customized data structures.</p><p>To alleviate interconnect (PCIe) overhead, some research integrates the entire NIC or accelerator to/near the CPU package <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr" target="#b98">99,</ref><ref type="bibr" target="#b116">117,</ref><ref type="bibr" target="#b134">135,</ref><ref type="bibr" target="#b148">149]</ref>. While this provides fast NIC-core interaction, it also has (1) high design and manufacturing cost and (2) low flexibility. For example, a NIC ASIC's die area can be more than 60mm 2 <ref type="bibr" target="#b0">[1]</ref>, which is roughly the area of four server CPU cores <ref type="bibr" target="#b162">[163]</ref>. But network upgrades would require replacing server CPUs too, leading to high total cost of ownership. Dagger <ref type="bibr" target="#b88">[89]</ref> also leverages cc-accelerator for NIC design, but still involves the server CPU for request processing, and only uses the cc-interconnect for lower latency over PCIe.</p><p>Compared to these works, RAMBDA takes a modularized design with low TCO towards a framework for general memory-intensive distributed application acceleration, while still leveraging cache coherence to more efficiently handle applications with irregular/uniform memory access patterns. Also, RAMBDA is not mutual-exclusive with these works, as our goal is to make the most out of each component in the system. Some prior work accelerates applications in datacenters with cc-accelerators <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b88">89,</ref><ref type="bibr" target="#b103">104,</ref><ref type="bibr" target="#b125">126,</ref><ref type="bibr" target="#b143">144]</ref>, but they either accelerate single-machine applications/operations or a specific routine/layer in the system. RAMBDA takes a holistic cross-stack approach to achieve end-to-end datacenter application offloading at ?s-scale. NIC-host co-design framework. With the growing popularity of Smart NICs, researchers have developed frameworks to schedule/offload datacenter applications to their processors <ref type="bibr" target="#b99">[100,</ref><ref type="bibr" target="#b101">102,</ref><ref type="bibr" target="#b129">130]</ref>. However, they are still constrained to separate the Smart NIC and host's memory to two domains and memory-intensive code suffers from high PCIe latency when offloaded. RAMBDA tackles these challenges with its unique neardata processing capability, while keeping the networking part offloaded on the RNIC for low cost and flexibility. Lynx <ref type="bibr" target="#b153">[154]</ref> proposes Smart NIC-based communication offloading for accelerator-rich systems, and FlexDriver <ref type="bibr" target="#b40">[41]</ref> proposes PCIebased NIC control by accelerators. RAMBDA takes one step further to let the client directly communicate with the accelerators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSION</head><p>We present RAMBDA, a holistic system design to offload modern ?s-scale datacenter applications. It leverages the RDMA and cc-accelerator technologies to achieve high throughput and low latency performance with minimal CPU involvement. We apply RAMBDA to three representative datacenter applications. Our evaluation on a real system shows that, compared with CPU-and NIC-based offloading solutions, RAMBDA achieves better and more stable performance with higher power efficiency.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Random memory access request latency from Smart NIC for different percentages of host memory accesses. Each data point contains 100 back-to-back 64B memory accesses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: RAMBDA's high-level system architecture; only the snapshot of client-1 and server-A are demonstrated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Fig.3: cpoll region for cc-accelerator notification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: RAMBDA cc-accelerator architecture.</figDesc><graphic url="image-6.png" coords="5,50.22,50.54,248.61,101.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Memory bandwidth consumption by PCIe-bench's DMA write with different DDIO/TPH settings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: DDIO/TPH configurations in the system with NVM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Normalized throughput performance of different approaches on the microbenchmark.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: Peak throughput performance of different KVS designs. The batch size of 32 is applied.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 :</head><label>9</label><figDesc>Fig. 9: Latency performance of different KVS designs on the 100% GET workload. The batch size of 32 is applied. RAMBDA-LD/LH's tail latency is inapplicable.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 :</head><label>10</label><figDesc>Fig.10:The impact of batch size on throughput and latency (100% GET workload, Zipfian 0.9 distribution). RAMBDA-LD/LH's tail latency is inapplicable.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 11 :</head><label>11</label><figDesc>Fig. 11: Emulated 2-node replication; the Smart NIC (S. NIC in the figure) ARM in the client simply forwards data from port 0 to port 1, and the Smart NIC ARM/RAMBDA accelerator is separated to handle traffic from port 0 and 1 independently.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 12 :</head><label>12</label><figDesc>Fig. 12: Latency comparison with different key-value pair size and transactions with different numbers of (read,write).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 13 :</head><label>13</label><figDesc>Fig. 13: MERCI-based DLRM inference throughput on the Amazon Review dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Taxonomy of hardware-based ?s-scale datacenter applications offloading/acceleration.</figDesc><table><row><cell>Optimization</cell><cell>Net. Overhead</cell><cell>PCIe Overhead</cell><cell>CPU Overhead</cell><cell>Flexibility</cell><cell>Perf. Stability</cell></row><row><cell>Two-sided RDMA/kernel-bypass with multi-core [75-78, 98]</cell><cell>Low</cell><cell>Low</cell><cell>High</cell><cell>High</cell><cell>Low</cell></row><row><cell>One-sided/mixed RDMA</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>, i.e., one dedicated</figDesc><table><row><cell>RNIC</cell><cell>2</cell><cell>Write</cell><cell>3</cell><cell>Triggered</cell><cell></cell><cell>cc-acc</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">?</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell>Register</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">cpoll Region</cell><cell></cell></row><row><cell></cell><cell cols="6">Shared Coherent Memory Space</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">(a) cpoll workf ow</cell><cell></cell></row><row><cell cols="3">Request Bu?ers (cpoll region)</cell><cell></cell><cell>Request Bu?ers</cell><cell cols="2">+</cell><cell>Pointer Bu?er (cpoll region)</cell></row><row><cell></cell><cell></cell><cell>?</cell><cell></cell><cell>?</cell><cell></cell><cell>?</cell></row><row><cell cols="3">(b) Small Scale/Small Request</cell><cell></cell><cell cols="3">(c) Large Scale/Large Request</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>RAMBDA testbed configurations.</figDesc><table><row><cell>2? Intel ? Xeon 6138P CPUs@2.0GHz [69]</cell><cell></cell></row><row><cell cols="2">20 Skylake cores, hyperthreading enabled, running Ubuntu 18</cell></row><row><cell>27.5MB shared LLC</cell><cell></cell></row><row><cell>Six DDR4-2666 channels, 192GB DRAM in total</cell><cell></cell></row><row><cell>In-package Intel ? Arria 10GX FPGA@400MHz [66]</cell><cell></cell></row><row><cell>One UPI link to the CPU, 10.4GT/s (20.8GB/s)</cell><cell></cell></row><row><cell>64KB local cache</cell><cell></cell></row><row><cell>LUT usage</cell><cell>11K (26%)</cell></row><row><cell>Registers usage</cell><cell>130K (8%)</cell></row><row><cell>BRAM blocks usage</cell><cell>387 (14%)</cell></row><row><cell>NVIDIA BlueField-2 Smart NIC SoC [119]</cell><cell></cell></row><row><cell cols="2">2x25Gbps Ethernet ports, backed by ConnectX-6 Dx controller</cell></row><row><cell>RDMA over converged Ethernet V2 (RoCEv2)</cell><cell></cell></row><row><cell>Eight ARM A72 cores@2.5GHz, running Ubuntu 20</cell><cell></cell></row><row><cell>6MB shared LLC</cell><cell></cell></row><row><cell>16GB on-board DDR4-1600 DRAM</cell><cell></cell></row><row><cell>One-sided RDMA for ARM to access the host memory</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III :</head><label>III</label><figDesc>Overall power efficiency of different KVS approaches with GET operations in uniform distribution.</figDesc><table><row><cell></cell><cell>CPU</cell><cell>Smart NIC</cell><cell>RAMBDA</cell></row><row><cell>Kop/W</cell><cell>130.4</cell><cell>25.2</cell><cell>188.7</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENT</head><p>We would like to thank <rs type="person">Narayan Ranganathan</rs>, <rs type="person">Nikhil Rao</rs>, <rs type="person">Rajesh Sankaran</rs>, as well as the anonymous reviewers for their insightful and helpful information and feedback. We also thank <rs type="person">Yixiao Gao</rs>, <rs type="person">Nikita Lazarev</rs>, <rs type="person">Jiacheng Ma</rs>, <rs type="person">Shaojie Xiang</rs>, and <rs type="person">Zeran Zhu</rs> for their technical support and discussion. This research is supported by <rs type="funder">National Science Foundation</rs> (No. <rs type="grantNumber">CNS-1705047</rs>), <rs type="funder">Intel Corporation's Academic</rs> research funding, and <rs type="funder">NVIDIA device donation</rs>. <rs type="person">Nam Sung Kim</rs> has a financial interest in Samsung.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_nUYWdyJ">
					<idno type="grant-number">CNS-1705047</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Private communication with Intel</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Understanding training efficiency of deep learning recommendation models at scale</title>
		<author>
			<persName><forename type="first">B</forename><surname>Acun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hazelwood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA&apos;21)</title>
		<meeting>the 2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA&apos;21)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">NetDIMM: Low-latency near-memory network interface architecture</title>
		<author>
			<persName><forename type="first">M</forename><surname>Alian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO&apos;19)</title>
		<meeting>the 52nd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO&apos;19)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Data direct I/O characterization for future I/O system exploration</title>
		<author>
			<persName><forename type="first">M</forename><surname>Alian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS&apos;20)</title>
		<meeting>the 2020 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS&apos;20)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Chainreaction: A causal+ consistent datastore based on chain replication</title>
		<author>
			<persName><forename type="first">S</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Leit?o</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rodrigues</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th ACM European Conference on Computer Systems (EuroSys&apos;13)</title>
		<meeting>the 8th ACM European Conference on Computer Systems (EuroSys&apos;13)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BBB: Simplifying persistent programming using battery-backed buffers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Alshboul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ramrakhyani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tuck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Solihin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA&apos;21)</title>
		<meeting>the 2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA&apos;21)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Remote memory calls</title>
		<author>
			<persName><forename type="first">E</forename><surname>Amaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ousterhout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ratnasamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM Workshop on Hot Topics in Networks (HotNets&apos;20)</title>
		<meeting>the 19th ACM Workshop on Hot Topics in Networks (HotNets&apos;20)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">FAWN: A fast array of wimpy nodes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kaminsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Phanishayee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGOPS 22nd Symposium on Operating Systems Principles (SOSP&apos;09)</title>
		<meeting>the ACM SIGOPS 22nd Symposium on Operating Systems Principles (SOSP&apos;09)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">FAFNIR: Accelerating sparse gathering by using efficient near-memory intelligent reduction</title>
		<author>
			<persName><forename type="first">B</forename><surname>Asgari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hadidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Shim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-K</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA&apos;21)</title>
		<meeting>the 2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA&apos;21)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Elastic Fabric Adapter -Run HPC and ML applications at scale</title>
		<author>
			<persName><surname>Aws</surname></persName>
		</author>
		<ptr target="https://aws.amazon.com/hpc/efa/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Introducing new product innovations for SAP HANA, expanded AI collaboration with SAP and more</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bablani</surname></persName>
		</author>
		<ptr target="https://azure.microsoft.com/en-us/blog/introducing-new-product-innovations-for-sap-hana-expanded-ai-collaboration-with-sap-and-more/" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">CORFU: A shared log design for flash clusters</title>
		<author>
			<persName><forename type="first">M</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Malkhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Prabhakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wobbler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th USENIX Symposium on Networked Systems Design and Implementation (NSDI&apos;12)</title>
		<meeting>the 9th USENIX Symposium on Networked Systems Design and Implementation (NSDI&apos;12)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Attack of the killer microseconds</title>
		<author>
			<persName><forename type="first">L</forename><surname>Barroso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The case for energy-proportional computing</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Barroso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>H?lzle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">IX: A protected dataplane operating system for high throughput and low latency</title>
		<author>
			<persName><forename type="first">A</forename><surname>Belay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Prekas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Klimovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bugnion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th USENIX Symposium on Operating Systems Design and Implementation</title>
		<meeting>the 11th USENIX Symposium on Operating Systems Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Blanas</surname></persName>
		</author>
		<ptr target="https://www.sigarch.org/from-flops-to-iops-the-new-bottlenecks-of-scientific-computing/" />
		<title level="m">From FLOPS to IOPS: The new bottlenecks of scientific computing</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Achieving 10Gbps line-rate key-value stores with FPGAs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Blott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vissers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>B?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Istv?n</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th USENIX Workshop on Hot Topics in Cloud Computing</title>
		<meeting>the 5th USENIX Workshop on Hot Topics in Cloud Computing</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>HotCloud&apos;13</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Available first on Google Cloud: Intel Optane DC persistent memory</title>
		<author>
			<persName><forename type="first">N</forename><surname>Boden</surname></persName>
		</author>
		<ptr target="https://cloud.google.com/blog/topics/partners/available-first-on-google-cloud-intel-optane-dc-persistent-memory" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">PRISM: Rethinking the RDMA interface for distributed systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dharanipragada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Joyner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Szekeres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R K</forename><surname>Ports</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGOPS 28th Symposium on Operating Systems Principles (SOSP&apos;21)</title>
		<meeting>the ACM SIGOPS 28th Symposium on Operating Systems Principles (SOSP&apos;21)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Understanding host network stack overheads</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vuppalapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 ACM SIGCOMM conference (SIGCOMM&apos;21)</title>
		<meeting>the 2021 ACM SIGCOMM conference (SIGCOMM&apos;21)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rethinking software runtimes for disaggregated memory</title>
		<author>
			<persName><forename type="first">I</forename><surname>Calciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Imran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Puddu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kashyap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Maruf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;21)</title>
		<meeting>the 26th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;21)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Windows Azure storage: A highly available cloud storage service with strong consistency</title>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ogus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nilakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Skjolsvold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mckelvie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Srivastav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Simitci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Haridas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Uddaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Khatri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bedekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mainali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Abbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F U</forename><surname>Haq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I U</forename><surname>Haq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bhardwaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dayanand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Adusumilli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mcnett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sankaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Manivannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rigas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM Symposium on Operating Systems Principles (SOSP&apos;11)</title>
		<meeting>the 23rd ACM Symposium on Operating Systems Principles (SOSP&apos;11)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Accelerating database analytic query workloads using an associative processor</title>
		<author>
			<persName><forename type="first">H</forename><surname>Caminal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chronis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Mart?nez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual International Symposium on Computer Architecture (ISCA&apos;22)</title>
		<meeting>the 49th Annual International Symposium on Computer Architecture (ISCA&apos;22)</meeting>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A cloud-scale acceleration architecture</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Caulfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Putnam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Angepat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fowers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Haselman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Heil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Humphrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kaur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Massengill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ovtcharov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Papamichael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Woods</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lanka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 49th IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">CCIX</title>
		<author>
			<persName><surname>Ccix Consortium</surname></persName>
		</author>
		<ptr target="https://www.ccixconsortium.com" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Improving recommendation quality in Google Drive</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calaci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Abraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Colagrosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD&apos;20)</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD&apos;20)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fast and general distributed transactions using RDMA and HTM</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th European Conference on Computer Systems (EuroSys&apos;16)</title>
		<meeting>the 11th European Conference on Computer Systems (EuroSys&apos;16)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Scalable RDMA RPC on reliable connection with efficient resource sharing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th EuroSys Conference (EuroSys&apos;19)</title>
		<meeting>the 14th EuroSys Conference (EuroSys&apos;19)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">In-depth analysis on microarchitectures of modern heterogeneous CPU-FPGA platforms</title>
		<author>
			<persName><forename type="first">Y.-K</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Reinman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Reconfigurable Technology Systems</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Enzian: An open, general, CPU/FPGA platform for systems software research</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramdas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schwyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Giardino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Turowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hossle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Korolija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Licciardello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Martsenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Achermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Roscoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;22)</title>
		<meeting>the 27th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;22)</meeting>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Intel to Launch Next-Gen Sapphire Rapids Xeon with High Bandwidth Memory</title>
		<author>
			<persName><forename type="first">I</forename><surname>Cutress</surname></persName>
		</author>
		<ptr target="https://www.anandtech.com/show/16795/intel-to-launch-next-gen-sapphire-rapids-xeon-with-high-bandwidth-memory" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Using a PCIe slot to install DRAM: New Samsung CXL.mem expansion module</title>
		<author>
			<persName><forename type="first">I</forename><surname>Cutress</surname></persName>
		</author>
		<ptr target="https://www.anandtech.com/show/16670/using-a-pcie-slot-to-install-dram-new-samsung-cxlmem-expansion-module" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Compute express link (CXL)</title>
		<author>
			<persName><surname>Cxl Consortium</surname></persName>
		</author>
		<ptr target="https://www.computeexpresslink.org" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Manycore network interfaces for in-memory rack-scale computing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Daglis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Novakovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bugnion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Grot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual International Symposium on Computer Architecture (ISCA&apos;15)</title>
		<meeting>the 42nd Annual International Symposium on Computer Architecture (ISCA&apos;15)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">RPCValet: NI-driven tail-aware balancing of ?s-scale RPCs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Daglis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;19)</title>
		<meeting>the 24th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;19)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Network hardware-accelerated consensus</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bressana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Weatherspoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Canini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pedone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Soul?</surname></persName>
		</author>
		<idno>USI-INF- TR-2016-03</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
		<respStmt>
			<orgName>Universit? della Svizzera italiana</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">P4xos: Consensus as a network service</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bressana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Weatherspoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Canini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zilberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pedone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Soul?</surname></persName>
		</author>
		<idno>USI-INF-TR-2018-01</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
		<respStmt>
			<orgName>Universit? della Svizzera italiana</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A new golden age in computer architecture: Empowering the machine-learning revolution</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">FaRM: Fast remote memory</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dragojevi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Hodson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th USENIX Symposium on Networked Systems Design and Implementation</title>
		<meeting>the 11th USENIX Symposium on Networked Systems Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Bandana: Using non-volatile memory for storing deep learning models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Eisenman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Naumov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Smelyanskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pupyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hazelwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cidon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Katti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd SysML Conference (SysML&apos;19)</title>
		<meeting>the 2nd SysML Conference (SysML&apos;19)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">FlexDriver: A network driver for your accelerator</title>
		<author>
			<persName><forename type="first">H</forename><surname>Eran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fudim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Malka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shalom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hermony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Silberstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;22)</title>
		<meeting>the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;22)</meeting>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">FPGA for aggregate processing: The good, the bad, and the ugly</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">F</forename><surname>Eryilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kakaraparthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE 37th International Conference on Data Engineering (ICDE&apos;21)</title>
		<meeting>the IEEE 37th International Conference on Data Engineering (ICDE&apos;21)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">HyperDex: A distributed, searchable key-value store</title>
		<author>
			<persName><forename type="first">R</forename><surname>Escriva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">G</forename><surname>Sirer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGCOMM 2012 Conference (SIGCOMM&apos;12)</title>
		<meeting>the ACM SIGCOMM 2012 Conference (SIGCOMM&apos;12)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">RocksDB: A persistent key-value store for fast storage environments</title>
		<author>
			<persName><surname>Facebook</surname></persName>
		</author>
		<ptr target="https://rocksdb.org" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unlocking energy</title>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Guerraoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Picorel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Trigonakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 USENIX Annual Technical Conference (ATC&apos;16)</title>
		<meeting>the 2016 USENIX Annual Technical Conference (ATC&apos;16)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Make the most out of last level cache in Intel processors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Farshin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roozbeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">Q</forename><surname>Maguire</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kosti?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th European Conference on Computer Systems (EuroSys&apos;19)</title>
		<meeting>the 14th European Conference on Computer Systems (EuroSys&apos;19)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Reexamining direct cache access to optimize I/O intensive applications for multihundred-gigabit networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Farshin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roozbeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">Q</forename><surname>Maguire</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kosti?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2020 USENIX Annual Technical Conference (ATC&apos;20)</title>
		<meeting>2020 USENIX Annual Technical Conference (ATC&apos;20)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Network interface design for low latency request-response protocols</title>
		<author>
			<persName><forename type="first">M</forename><surname>Flajslik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rosenblum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 USENIX Annual Technical Conference (ATC&apos;13)</title>
		<meeting>the 2013 USENIX Annual Technical Conference (ATC&apos;13)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">When cloud storage meets RDMA</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th USENIX Symposium on Networked Systems Design and Implementation (NSDI&apos;21)</title>
		<meeting>the 18th USENIX Symposium on Networked Systems Design and Implementation (NSDI&apos;21)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The Google file system</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gobioff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-T</forename><surname>Leung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM Symposium on Operating Systems Principles (SOSP&apos;03)</title>
		<meeting>the 19th ACM Symposium on Operating Systems Principles (SOSP&apos;03)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Methods and apparatus for implementing PCI express lightweight notification protocols in a CPU/memory complex</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Glasser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hummel</surname></persName>
		</author>
		<idno>US Patent 20130173837A1</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Software data planes: You can&apos;t always spin to win</title>
		<author>
			<persName><forename type="first">H</forename><surname>Golestani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mirhosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 ACM Symposium on Cloud Computing (SoCC&apos;19)</title>
		<meeting>the 2019 ACM Symposium on Cloud Computing (SoCC&apos;19)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Architectural support for server-side PHP processing</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Schlais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Lipasti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th IEEE/ACM International Symposium on Computer Architecture (ISCA&apos;17)</title>
		<meeting>the 44th IEEE/ACM International Symposium on Computer Architecture (ISCA&apos;17)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Direct access, High-Performance memory disaggregation with DirectCXL</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gouk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 USENIX Annual Technical Conference (ATC&apos;22)</title>
		<meeting>the 2022 USENIX Annual Technical Conference (ATC&apos;22)</meeting>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">RDMA over commodity Ethernet at scale</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Soni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Padhye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lipshteyn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM SIGCOMM Conference (SIGCOMM&apos;16)</title>
		<meeting>the 2016 ACM SIGCOMM Conference (SIGCOMM&apos;16)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Clio: A hardwaresoftware co-designed disaggregated memory system</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;22)</title>
		<meeting>the 27th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;22)</meeting>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Sonata: Query-driven streaming network telemetry</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Canini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Feamster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rexford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Willinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 ACM SIGCOMM Conference (SIGCOMM&apos;18)</title>
		<meeting>the 2018 ACM SIGCOMM Conference (SIGCOMM&apos;18)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">DeepRecSys: A system for optimizing end-to-end at-scale neural recommendation inference</title>
		<author>
			<persName><forename type="first">U</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hsia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Saraph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Reagen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-H</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA&apos;20)</title>
		<meeting>the ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA&apos;20)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">The architectural implications of Facebook&apos;s DNN-based personalized recommendation</title>
		<author>
			<persName><forename type="first">U</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Naumov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Reagen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cottel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hazelwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hempstead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-H</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Malevich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mudigere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Smelyanskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 IEEE International Symposium on High-Performance Computer Architecture (HPCA&apos;20)</title>
		<meeting>the 2020 IEEE International Symposium on High-Performance Computer Architecture (HPCA&apos;20)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering</title>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on World Wide Web</title>
		<meeting>the 25th International Conference on World Wide Web</meeting>
		<imprint>
			<publisher>WWW</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Centaur: A chiplet-based, hybrid sparse-dense accelerator for personalized recommendations</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA&apos;20)</title>
		<meeting>the ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA&apos;20)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">The nanoPU: A nanosecond network stack for datacenters</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ibanez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mallery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Arslan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jepsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shahbaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th USENIX Symposium on Operating Systems Design and Implementation (OSDI&apos;21)</title>
		<meeting>the 15th USENIX Symposium on Operating Systems Design and Implementation (OSDI&apos;21)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Data plane development kit (DPDK)</title>
		<ptr target="https://www.dpdk.org" />
		<imprint/>
		<respStmt>
			<orgName>Intel Corporation</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">eADR: New opportunities for persistent memory applications</title>
		<ptr target="https://software.intel.com/content/www/us/en/develop/articles/eadr-new-opportunities-for-persistent-memory-applications.html" />
		<imprint/>
		<respStmt>
			<orgName>Intel Corporation</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Instruction set reference</title>
		<ptr target="https://software.intel.com/content/www/us/en/develop/download/intel-64-and-ia-32-architectures-sdm-combined-volumes-2a-2b-2c-and-2d-instruction-set-reference-a-z.html" />
	</analytic>
	<monogr>
		<title level="m">Intel 64 and IA-32 architectures software developer&apos;s manual</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
		</imprint>
		<respStmt>
			<orgName>Intel Corporation</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<ptr target="https://ark.intel.com/content/www/us/en/ark/products/210381/intel-arria-10-gx-1150-fpga.html" />
		<title level="m">Intel Arria 10 GX 1150 FPGA</title>
		<imprint/>
		<respStmt>
			<orgName>Intel Corporation</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Intel Optane Persistent Memory</title>
		<ptr target="https://www.intel.com/content/www/us/en/architecture-and-technology/optane-dc-persistent-memory.html" />
		<imprint/>
		<respStmt>
			<orgName>Intel Corporation</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<ptr target="https://www.intel.com/content/www/us/en/products/details/fpga/stratix/10/dx.html" />
		<title level="m">Intel Stratix 10 DX FPGAs</title>
		<imprint/>
		<respStmt>
			<orgName>Intel Corporation</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<ptr target="https://ark.intel.com/content/www/us/en/ark/products/139940/intel-xeon-gold-6138p-processor-27-5m-cache-2-00-ghz.html" />
		<title level="m">Intel Xeon Gold 6138P Processor</title>
		<imprint/>
		<respStmt>
			<orgName>Intel Corporation</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Intel? data direct I/O (DDIO)</title>
		<ptr target="https://www.intel.com/content/www/us/en/io/data-direct-i-o-technology.html" />
		<imprint/>
		<respStmt>
			<orgName>Intel Corporation</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">mTCP: A highly scalable user-level TCP stack for multicore systems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Jamshed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ihm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th USENIX Symposium on Networked Systems Design and Implementation (NSDI&apos;14)</title>
		<meeting>the 11th USENIX Symposium on Networked Systems Design and Implementation (NSDI&apos;14)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">NetChain: Scale-free sub-RTT coordination</title>
		<author>
			<persName><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Soul?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th USENIX Symposium on Networked Systems Design and Implementation (NSDI&apos;18)</title>
		<meeting>the 15th USENIX Symposium on Networked Systems Design and Implementation (NSDI&apos;18)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">NetCache: Balancing key-value stores with fast in-network caching</title>
		<author>
			<persName><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Soul?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM Symposium on Operating Systems Principles (SOSP&apos;17)</title>
		<meeting>the 26th ACM Symposium on Operating Systems Principles (SOSP&apos;17)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Challenges and solutions for fast remote persistent memory access</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kalia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kaminsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th ACM Symposium on Cloud Computing (SoCC&apos;20)</title>
		<meeting>the 11th ACM Symposium on Cloud Computing (SoCC&apos;20)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Datacenter RPCs can be general and fast</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kalia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kaminsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Andersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th USENIX Symposium on Networked Systems Design and Implementation (NSDI&apos;19)</title>
		<meeting>the 16th USENIX Symposium on Networked Systems Design and Implementation (NSDI&apos;19)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Using RDMA efficiently for key-value services</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kalia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kaminsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Andersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 ACM SIGCOMM Conference (SIGCOMM&apos;14)</title>
		<meeting>the 2014 ACM SIGCOMM Conference (SIGCOMM&apos;14)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Design guidelines for high performance RDMA systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kalia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kaminsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Andersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 USENIX Annual Technical Conference (ATC&apos;16)</title>
		<meeting>the 2016 USENIX Annual Technical Conference (ATC&apos;16)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">FaSST: Fast, scalable and simple distributed transactions with two-sided (RDMA) datagram RPCs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kalia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kaminsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Andersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th USENIX Symposium on Operating Systems Design and Implementation</title>
		<meeting>the 12th USENIX Symposium on Operating Systems Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Profiling a warehouse-scale computer</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kanev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Darago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hazelwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moseley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd IEEE/ACM International Symposium on Computer Architecture (ISCA&apos;15)</title>
		<meeting>the 42nd IEEE/ACM International Symposium on Computer Architecture (ISCA&apos;15)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">A hardware accelerator for protocol buffers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Karandikar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kennelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nikolic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Asanovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO&apos;21)</title>
		<meeting>the 54th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO&apos;21)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">RecNMP: Accelerating personalized recommendation with nearmemory processing</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Y</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Diril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Firoozshahian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hazelwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-H</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Maher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mudigere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Naumov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schatz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Smelyanskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Reagen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hempstead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA&apos;20)</title>
		<meeting>the ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA&apos;20)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">TEA: Enabling state-intensive network functions on programmable switches</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Seshan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 ACM SIGCOMM Conference (SIGCOMM&apos;20)</title>
		<meeting>the 2020 ACM SIGCOMM Conference (SIGCOMM&apos;20)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">HyperLoop: Group-based NIC-offloading to accelerate replicated transactions in multi-tenant storage systems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Memaripour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Badam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Padhye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Raindel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Swanson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Seshan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 ACM SIGCOMM conference (SIGCOMM&apos;18)</title>
		<meeting>the 2018 ACM SIGCOMM conference (SIGCOMM&apos;18)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">LineFS: Efficient SmartNIC offload of a distributed file system with pipeline parallelism</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Im</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Canini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kosti?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Witchel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGOPS 28th Symposium on Operating Systems Principles (SOSP&apos;21)</title>
		<meeting>the ACM SIGOPS 28th Symposium on Operating Systems Principles (SOSP&apos;21)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Meet the walkers: Accelerating index traversals for in-memory databases</title>
		<author>
			<persName><forename type="first">O</forename><surname>Kocberber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Grot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Picorel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th IEEE/ACM International Symposium on Microarchitecture (MICRO&apos;13)</title>
		<meeting>the 46th IEEE/ACM International Symposium on Microarchitecture (MICRO&apos;13)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Credit-based flow control for ATM networks: Credit update protocol, adaptive credit allocation and statistical multiplexing</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Kung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Blackwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chapman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1994 ACM SIGCOMM Conference (SIGCOMM&apos;94)</title>
		<meeting>the 1994 ACM SIGCOMM Conference (SIGCOMM&apos;94)</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">NetCAT: Practical cache attacks from the network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kurth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Andriesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Giuffrida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Razavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st IEEE Symposium on Security and Privacy</title>
		<meeting>the 41st IEEE Symposium on Security and Privacy</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Oakland&apos;20</note>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">TensorDIMM: A practical near-memory processing architecture for embeddings and tensor operations in deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO&apos;19)</title>
		<meeting>the 52nd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO&apos;19)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Dagger: Efficient and fast RPCs in cloud microservices with near-memory reconfigurable NICs</title>
		<author>
			<persName><forename type="first">N</forename><surname>Lazarev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Adit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Delimitrou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;21)</title>
		<meeting>the 26th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;21)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">On the impact of cluster configuration on RoCE application design</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Malekpourshahraki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Stephens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Akella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Swift</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Asia-Pacific Workshop on Networking</title>
		<meeting>the 3rd Asia-Pacific Workshop on Networking</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>APNet&apos;19</note>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">MIND: In-network memory management for disaggregated data centers</title>
		<author>
			<persName><forename type="first">Y</forename><surname>S.-S. Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><surname>Bhattacharjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGOPS 28th Symposium on Operating Systems Principles (SOSP&apos;21)</title>
		<meeting>the ACM SIGOPS 28th Symposium on Operating Systems Principles (SOSP&apos;21)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">MERCI: Efficient embedding reduction on commodity hardware via sub-query memoization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">U</forename><surname>Sul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Ham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;21)</title>
		<meeting>the 26th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;21)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">The case for network accelerated query processing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lerner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hussein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cudre-Mauroux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th Biennial Conference on Innovative Data Systems Research (CIDR&apos;19)</title>
		<meeting>the 9th Biennial Conference on Innovative Data Systems Research (CIDR&apos;19)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">KV-Direct: High-performance in-memory key-value store with programmable NIC</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Putnam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Symposium on Operating Systems Principles (SOSP&apos;17)</title>
		<meeting>the 26th Symposium on Operating Systems Principles (SOSP&apos;17)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Eris: Coordination-free consistent transactions using in-network concurrency control</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R K</forename><surname>Ports</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM Symposium on Operating Systems Principles (SOSP&apos;17)</title>
		<meeting>the 26th ACM Symposium on Operating Systems Principles (SOSP&apos;17)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Just say NO to Paxos overhead: Replacing consensus with network ordering</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Szekeres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R K</forename><surname>Ports</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th USENIX Symposium on Operating Systems Design and Implementation</title>
		<meeting>the 12th USENIX Symposium on Operating Systems Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Pegasus: Tolerating skewed workloads in distributed storage with in-network coherence directories</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R K</forename><surname>Ports</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th USENIX Symposium on Operating Systems Design and Implementation (OSDI&apos;20)</title>
		<meeting>the 14th USENIX Symposium on Operating Systems Design and Implementation (OSDI&apos;20)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">MICA: A holistic approach to fast in-memory key-value storage</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kaminsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th USENIX Symposium on Networked Systems Design and Implementation (NSDI&apos;14)</title>
		<meeting>the 11th USENIX Symposium on Networked Systems Design and Implementation (NSDI&apos;14)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Thin servers with smart pipes: Designing SoC accelerators for Memcached</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Meisner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Saidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual International Symposium on Computer Architecture (ISCA&apos;13)</title>
		<meeting>the 40th Annual International Symposium on Computer Architecture (ISCA&apos;13)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Offloading distributed applications onto SmartNICs using iPipe</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schuh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 ACM SIGCOMM conference (SIGCOMM&apos;19)</title>
		<meeting>the 2019 ACM SIGCOMM conference (SIGCOMM&apos;19)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">IncBricks: Toward in-network computation with an in-network cache</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Atreya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;17)</title>
		<meeting>the 22nd International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;17)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Energy-efficient microservices on SmartNIC-accelerated servers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Phothilimthana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 USENIX Annual Technical Conference (ATC&apos;19)</title>
		<meeting>the 2019 USENIX Annual Technical Conference (ATC&apos;19)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Livia: Data-centric computing throughout the memory hierarchy</title>
		<author>
			<persName><forename type="first">E</forename><surname>Lockerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Feldmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bakhshalipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stanescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Beckmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;20)</title>
		<meeting>the 25th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;20)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">A hypervisor for shared-memory FPGA platforms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Loughlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Eneyew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kasikci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;20)</title>
		<meeting>the 25th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;20)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Contention-aware performance prediction for virtualized network functions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Manousis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sherry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 ACM SIGCOMM Conference (SIGCOMM&apos;20)</title>
		<meeting>the 2020 ACM SIGCOMM Conference (SIGCOMM&apos;20)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<monogr>
		<title level="m" type="main">Mellanox adapters programmer&apos;s reference manual (PRM)</title>
		<author>
			<persName><surname>Mellanox</surname></persName>
		</author>
		<ptr target="https://www.mellanox.com/related-docs/user_manuals/Ethernet_Adapters_Programming_Manual.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<monogr>
		<title level="m" type="main">Mellanox scalable hierarchical aggregation and reduction protocol (SHARP)</title>
		<author>
			<persName><surname>Mellanox</surname></persName>
		</author>
		<ptr target="http://www.mellanox.com/page/products_dyn?product_family=261&amp;mtag=sharp" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Atomic in-place updates for non-volatile main memories with Kamino-Tx</title>
		<author>
			<persName><forename type="first">A</forename><surname>Memaripour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Badam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Phanishayee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Alagappan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Strauss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Swanson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th European Conference on Computer Systems (EuroSys&apos;17)</title>
		<meeting>the 12th European Conference on Computer Systems (EuroSys&apos;17)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">HyperPlane: A scalable low-latency notification accelerator for software data planes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mirhosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Golestani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd IEEE/ACM International Symposium on Microarchitecture (MICRO&apos;20)</title>
		<meeting>the 53rd IEEE/ACM International Symposium on Microarchitecture (MICRO&apos;20)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Enhancing server efficiency in the face of killer microseconds</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mirhosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sriraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 IEEE International Symposium on High Performance Computer Architecture (HPCA&apos;19)</title>
		<meeting>the 2019 IEEE International Symposium on High Performance Computer Architecture (HPCA&apos;19)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Using one-sided RDMA reads to build a fast, CPU-efficient key-value store</title>
		<author>
			<persName><forename type="first">C</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 USENIX Annual Technical Conference (ATC&apos;13)</title>
		<meeting>the 2013 USENIX Annual Technical Conference (ATC&apos;13)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Balancing CPU and network in the Cell distributed B-Tree store</title>
		<author>
			<persName><forename type="first">C</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Montgomery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 USENIX Annual Technical Conference (ATC&apos;16)</title>
		<meeting>the 2016 USENIX Annual Technical Conference (ATC&apos;16)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Birds of a feather flock together: Scaling RDMA RPCs with Flock</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kashyap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Min</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGOPS 28th Symposium on Operating Systems Principles (SOSP&apos;21)</title>
		<meeting>the ACM SIGOPS 28th Symposium on Operating Systems Principles (SOSP&apos;21)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<monogr>
		<title level="m" type="main">mongoDB manual: Manage chained replication</title>
		<author>
			<persName><surname>Mongodb</surname></persName>
		</author>
		<ptr target="https://docs.mongodb.com/manual/tutorial/manage-chained-replication/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<monogr>
		<title level="m" type="main">Deep learning recommendation model for personalization and recommendation systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Naumov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mudigere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sundaraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Azzolini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dzhulgakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mallevich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Cherniavskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Krishnamoorthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kondratenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Smelyanskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00091</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Understanding PCIe performance for end host networking</title>
		<author>
			<persName><forename type="first">R</forename><surname>Neugebauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Antichi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Zazo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Audzevich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>L?pez-Buedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 ACM SIGCOMM Conference (SIGCOMM&apos;18)</title>
		<meeting>the 2018 ACM SIGCOMM Conference (SIGCOMM&apos;18)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Scale-out NUMA</title>
		<author>
			<persName><forename type="first">S</forename><surname>Novakovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Daglis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bugnion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Grot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;14)</title>
		<meeting>the 19th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;14)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Storm: A fast transactional dataplane for remote data structures</title>
		<author>
			<persName><forename type="first">S</forename><surname>Novakovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Eran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pismenny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsafrir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aguilera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM International Conference on Systems and Storage (SYSTOR&apos;19)</title>
		<meeting>the 12th ACM International Conference on Systems and Storage (SYSTOR&apos;19)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<monogr>
		<title level="m" type="main">NVIDIA BlueField-2 DPU: Data center infrastructure on a chip</title>
		<ptr target="https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/documents/datasheet-nvidia-bluefield-2-dpu.pdf" />
		<imprint/>
		<respStmt>
			<orgName>NVIDIA Corporation</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<monogr>
		<title level="m" type="main">RDMA aware networks programming user manual</title>
		<ptr target="https://docs.mellanox.com/display/RDMAAwareProgrammingv17" />
		<imprint/>
		<respStmt>
			<orgName>NVIDIA Corporation</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<monogr>
		<ptr target="https://nvidianews.nvidia.com/news/nvidia-extends-data-center-infrastructure-processing-roadmap-with-bluefield-3" />
		<title level="m">NVIDIA extends data center infrastructure processing roadmap with BlueField-3</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
		<respStmt>
			<orgName>NVIDIA Corporation</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">ExtraTime: Modeling and analysis of wearout due to transistor aging at microarchitecture-level</title>
		<author>
			<persName><forename type="first">F</forename><surname>Oboril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Tahoori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 IEEE/IFIP International Conference on Dependable Systems and Networks</title>
		<meeting>the 2012 IEEE/IFIP International Conference on Dependable Systems and Networks</meeting>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<monogr>
		<title level="m" type="main">Revisit DCA, PCIe TPH and DDIO</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ohara</surname></persName>
		</author>
		<ptr target="https://www.slideshare.net/hisaki/direct-cacheaccess" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Fast crash recovery in RAMCloud</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ongaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Rumble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Stutsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ousterhout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rosenblum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM Symposium on Operating Systems Principles (SOSP&apos;11)</title>
		<meeting>the 23rd ACM Symposium on Operating Systems Principles (SOSP&apos;11)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<monogr>
		<title level="m" type="main">Intel Sapphire Rapids with HBM2E, CXL 1.1, and PCIe 5.0 by end of 2022</title>
		<author>
			<persName><surname>Optocrypto</surname></persName>
		</author>
		<ptr target="https://optocrypto.com/intel-sapphire-rapids-with-hbm2e-cxl-1-1-and-pcie-5-0-by-end-of-2022/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Centaur: A framework for hybrid CPU-FPGA databases</title>
		<author>
			<persName><forename type="first">M</forename><surname>Owaida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Alonso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 IEEE 25th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM&apos;17)</title>
		<meeting>the 2017 IEEE 25th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM&apos;17)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<monogr>
		<title level="m" type="main">Running average power limit -RAPL</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pandruvada</surname></persName>
		</author>
		<ptr target="https://01.org/blogs/2014/running-average-power-limit-???-rapl" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">TRiM: Enhancing processor-memory interfaces with scalable tensor reduction in memory</title>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Ahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO&apos;21)</title>
		<meeting>the 54th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO&apos;21)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Flex-KV: Enabling high-performance and flexible KV systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Phanishayee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pucha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Povzner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Belluomini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Workshop on Management of Big Data Systems (MBDS&apos;12)</title>
		<meeting>the 2012 Workshop on Management of Big Data Systems (MBDS&apos;12)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Floem: A programming system for NIC-accelerated network applications</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Phothilimthana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kaufmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bodik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI&apos;18)</title>
		<meeting>the 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI&apos;18)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">The benefits of general-purpose on-NIC memory</title>
		<author>
			<persName><forename type="first">B</forename><surname>Pismenny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Morrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsafrir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;22)</title>
		<meeting>the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;22)</meeting>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<monogr>
		<title level="m" type="main">Lightweight Notification</title>
		<author>
			<persName><surname>Plda</surname></persName>
		</author>
		<ptr target="https://www.plda.com/pcie-glossary/lightweight-notification" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">FlowBlaze: Stateful packet processing in hardware</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pontarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bifulco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bonola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cascone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Spaziani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bruschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sanvito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Siracusano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Capone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Honda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Siracusano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th USENIX Symposium on Networked Systems Design and Implementation (NSDI&apos;19)</title>
		<meeting>the 16th USENIX Symposium on Networked Systems Design and Implementation (NSDI&apos;19)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Designing distributed systems using approximate synchrony in datacenter networks</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R K</forename><surname>Ports</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krishnamurthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th USENIX Symposium on Networked Systems Design and Implementation</title>
		<meeting>the 12th USENIX Symposium on Networked Systems Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Cerebros: Evading the RPC tax in datacenters</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pourhabibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Daglis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO&apos;21)</title>
		<meeting>the 54th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO&apos;21)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">RDMA is Turing complete, we just did not know it yet</title>
		<author>
			<persName><forename type="first">W</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Canini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kostic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Peter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th USENIX Symposium on Networked Systems Design and Implementation (NSDI&apos;22)</title>
		<meeting>the 19th USENIX Symposium on Networked Systems Design and Implementation (NSDI&apos;22)</meeting>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<monogr>
		<title level="m" type="main">mlx5dv -Linux manual page</title>
		<author>
			<persName><forename type="first">L</forename><surname>Romanovsky</surname></persName>
		</author>
		<ptr target="https://man7.org/linux/man-pages/man7/mlx5dv.7.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<monogr>
		<title level="m" type="main">How to perform system replication for SAP HANA</title>
		<author>
			<persName><surname>Sap</surname></persName>
		</author>
		<ptr target="https://www.sap.com/documents/" />
		<imprint>
			<date type="published" when="2013">2013/10/26c02b58-5a7c-0010-82c7. 2017</date>
		</imprint>
	</monogr>
	<note>eda71af511fa.html</note>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">In-network computation is a dumb idea whose time has come</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sapio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Abdelaziz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aldilaijan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Canini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kalnis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Workshop on Hot Topics in Networks (HotNets&apos;17)</title>
		<meeting>the 16th Workshop on Hot Topics in Networks (HotNets&apos;17)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Scaling distributed machine learning with in-network aggregation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sapio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Canini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kalnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Moshref</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Ports</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Richt?rik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th USENIX Symposium on Networked Systems Design and Implementation (NSDI&apos;21)</title>
		<meeting>the 18th USENIX Symposium on Networked Systems Design and Implementation (NSDI&apos;21)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Xenic: SmartNIC-accelerated distributed transactions</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">N</forename><surname>Schuh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krishnamurthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGOPS 28th Symposium on Operating Systems Principles (SOSP&apos;21)</title>
		<meeting>the ACM SIGOPS 28th Symposium on Operating Systems Principles (SOSP&apos;21)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">PM-Net: In-network data persistence</title>
		<author>
			<persName><forename type="first">K</forename><surname>Seemakhupt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Senevirathne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shahbaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th IEEE/ACM International Symposium on Computer Architecture (ISCA&apos;21)</title>
		<meeting>the 48th IEEE/ACM International Symposium on Computer Architecture (ISCA&apos;21)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">A cloud-optimized transport protocol for elastic and scalable HPC</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shalev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ayoub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bshara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sabbag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Accelerating pattern matching queries in hybrid CPU-FPGA architectures</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Istv?n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Owaida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Alonso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM International Conference on Management of Data (SIGMOD&apos;17)</title>
		<meeting>the 2017 ACM International Conference on Management of Data (SIGMOD&apos;17)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">StRoM: Smart remote memory</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chiosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Alonso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th European Conference on Computer Systems (EuroSys&apos;20)</title>
		<meeting>the 15th European Conference on Computer Systems (EuroSys&apos;20)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">RMA: Re-envisioning remote memory access for multi-tenant datacenters</title>
		<author>
			<persName><forename type="first">A</forename><surname>Singhvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Akella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wong-Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M K</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mclaren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cauble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M G</forename><surname>Wassel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Montazeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Sabato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Scherpelz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 ACM SIGCOMM Conference (SIGCOMM&apos;20)</title>
		<meeting>the 2020 ACM SIGCOMM Conference (SIGCOMM&apos;20)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Accelerometer: Understanding acceleration opportunities for data center overheads at hyperscale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sriraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dhanotia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;20)</title>
		<meeting>the 25th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;20)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">CAPI: A coherent accelerator processor interface</title>
		<author>
			<persName><forename type="first">J</forename><surname>Stuecheli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Blaner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Johns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Siegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">The NeBuLa RPC-optimized architecture</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Marathe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pnevmatikatos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Daglis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA&apos;20)</title>
		<meeting>the ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA&apos;20)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Replex: A scalable, highly available multi-index data store</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Abraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Malkhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 USENIX Annual Technical Conference (ATC&apos;16)</title>
		<meeting>the 2016 USENIX Annual Technical Conference (ATC&apos;16)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<monogr>
		<title level="m" type="main">RDMA persistent meory extensions</title>
		<author>
			<persName><forename type="first">T</forename><surname>Talpey</surname></persName>
		</author>
		<ptr target="https://www.openfabrics.org/wp-content/uploads/209_TTalpey.pdf" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">Object storage on CRAQ: Highthroughput chain replication for read-mostly workloads</title>
		<author>
			<persName><forename type="first">J</forename><surname>Terrace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Freedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 USENIX Annual Technical Conference (ATC&apos;09)</title>
		<meeting>the 2009 USENIX Annual Technical Conference (ATC&apos;09)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">ResQ: Enabling SLOs in network function virtualization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tootoonchian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Walls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Argyraki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ratnasamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 15th USENIX Symposium on Networked Systems Design and Implementation (NSDI&apos;18)</title>
		<meeting>15th USENIX Symposium on Networked Systems Design and Implementation (NSDI&apos;18)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">Lynx: A SmartNIC-driven accelerator-centric architecture for network servers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Maudlej</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Silberstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;20)</title>
		<meeting>the 25th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;20)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">LITE kernel RDMA support for datacenter applications</title>
		<author>
			<persName><forename type="first">S.-Y</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Symposium on Operating Systems Principles (SOSP&apos;17)</title>
		<meeting>the 26th Symposium on Operating Systems Principles (SOSP&apos;17)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">Chain replication for supporting high throughput and availability</title>
		<author>
			<persName><forename type="first">R</forename><surname>Van Renesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">B</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th USENIX Symposium on Operating Systems Design and Implementation</title>
		<meeting>the 6th USENIX Symposium on Operating Systems Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>OSDI&apos;04</note>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">Concordia: Distributed shared memory with in-network cache coherence</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th USENIX Conference on File and Storage Technologies (FAST&apos;21)</title>
		<meeting>the 19th USENIX Conference on File and Storage Technologies (FAST&apos;21)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Shuhai: Benchmarking high bandwidth memory on FPGAs</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Alonso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 IEEE Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM&apos;20)</title>
		<meeting>the 2020 IEEE Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM&apos;20)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">SRNIC: A scalable architecture for RDMA NICs</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th USENIX Symposium on Networked Systems Design and Implementation</title>
		<meeting>the 20th USENIX Symposium on Networked Systems Design and Implementation</meeting>
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">Deconstructing RDMAenabled distributed transactions: Hybrid is better</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI&apos;18)</title>
		<meeting>the 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI&apos;18)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">Fast in-memory transaction processing using RDMA and HTM</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM Symposium on Operating Systems Principles (SOSP&apos;15)</title>
		<meeting>the 25th ACM Symposium on Operating Systems Principles (SOSP&apos;15)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main">Characterizing and optimizing remote persistent memory with RDMA and NVM</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 USENIX Annual Technical Conference (ATC&apos;21)</title>
		<meeting>the 2021 USENIX Annual Technical Conference (ATC&apos;21)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b162">
	<monogr>
		<author>
			<persName><surname>Wikichip</surname></persName>
		</author>
		<ptr target="https://en.wikichip.org/wiki/intel/microarchitectures/skylake_(server)" />
		<title level="m">Skylake (server -microarchitectures -intel</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<monogr>
		<author>
			<persName><surname>Xilinx</surname></persName>
		</author>
		<ptr target="https://www.xilinx.com/products/boards-and-kits/alveo/u280.html" />
		<title level="m">Alveo U280 data center accelerator card</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<monogr>
		<author>
			<persName><surname>Xilinx</surname></persName>
		</author>
		<ptr target="https://www.xilinx.com/products/boards-and-kits/dk-v7-vc709-g.html" />
		<title level="m">Xilinx Virtex-7 FPGA VC709 connectivity kit</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<analytic>
		<title level="a" type="main">Dart: Divide and specialize for fast response to congestion in RDMA-based datacenter networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">U</forename><surname>Chaudhry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Vamanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Vijaykumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Thottethodi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Networking</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b166">
	<analytic>
		<title level="a" type="main">An empirical guide to the behavior and use of scalable persistent memory</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hoseinzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Izraelevitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Swanson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th USENIX Conference on File and Storage Technologies (FAST&apos;20)</title>
		<meeting>the 18th USENIX Conference on File and Storage Technologies (FAST&apos;20)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b167">
	<analytic>
		<title level="a" type="main">Hardware-based address-centric acceleration of key-value store</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Solihin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA&apos;21)</title>
		<meeting>the 2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA&apos;21)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<analytic>
		<title level="a" type="main">NetLock: Fast, centralized lock management using programmable switches</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Braverman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 ACM SIGCOMM Conference (SIGCOMM&apos;20)</title>
		<meeting>the 2020 ACM SIGCOMM Conference (SIGCOMM&apos;20)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b169">
	<analytic>
		<title level="a" type="main">Don&apos;t forget the I/O when allocating your LLC</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th IEEE/ACM International Symposium on Computer Architecture (ISCA&apos;21)</title>
		<meeting>the 48th IEEE/ACM International Symposium on Computer Architecture (ISCA&apos;21)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b170">
	<analytic>
		<title level="a" type="main">QEI: Query acceleration can be generic and efficient in the cloud</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B R</forename><surname>Chowhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA&apos;21)</title>
		<meeting>the 2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA&apos;21)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b171">
	<analytic>
		<title level="a" type="main">FAERY: An FPGA-accelerated embedding-based retrieval system</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th USENIX Symposium on Operating Systems Design and Implementation</title>
		<meeting>the 16th USENIX Symposium on Operating Systems Design and Implementation</meeting>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
	<note>OSDI&apos;22</note>
</biblStruct>

<biblStruct xml:id="b172">
	<analytic>
		<title level="a" type="main">Tiara: A scalable and efficient hardware acceleration architecture for stateful layer-4 load balancing</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th USENIX Symposium on Networked Systems Design and Implementation (NSDI&apos;22)</title>
		<meeting>the 19th USENIX Symposium on Networked Systems Design and Implementation (NSDI&apos;22)</meeting>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b173">
	<analytic>
		<title level="a" type="main">Hetero-ViTAL: A virtualization stack for heterogeneous FPGA clusters</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th IEEE/ACM International Symposium on Computer Architecture (ISCA&apos;21)</title>
		<meeting>the 48th IEEE/ACM International Symposium on Computer Architecture (ISCA&apos;21)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b174">
	<analytic>
		<title level="a" type="main">Distributed hierarchical GPU parameter server for massive scale deep learning ads systems</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Conference on Machine Learning ans Systems (MLSys&apos;20)</title>
		<meeting>the 3rd Conference on Machine Learning ans Systems (MLSys&apos;20)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b175">
	<analytic>
		<title level="a" type="main">AIBox: CTR prediction model training on a single node</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Information and Knowledge Management (CIKM&apos;19)</title>
		<meeting>the 28th ACM International Conference on Information and Knowledge Management (CIKM&apos;19)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b176">
	<analytic>
		<title level="a" type="main">Scalable, high performance Ethernet forwarding with CuckooSwitch</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kaminsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Andersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th ACM Conference on Emerging Networking Experiments and Technologies (CoNEXT&apos;13)</title>
		<meeting>the 9th ACM Conference on Emerging Networking Experiments and Technologies (CoNEXT&apos;13)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b177">
	<analytic>
		<title level="a" type="main">Fault-tolerant replication with pull-based consensus in MongoDB</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th USENIX Symposium on Networked Systems Design and Implementation (NSDI&apos;21)</title>
		<meeting>the 18th USENIX Symposium on Networked Systems Design and Implementation (NSDI&apos;21)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b178">
	<analytic>
		<title level="a" type="main">Harmonia: Near-linear scalability for replicated storage with in-network conflict detection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R K</forename><surname>Ports</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 International Conference on Very Large Data Bases (VLDB&apos;19)</title>
		<meeting>the 2019 International Conference on Very Large Data Bases (VLDB&apos;19)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b179">
	<analytic>
		<title level="a" type="main">Congestion control for large-scale RDMA deployments</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Eran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Firestone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lipshteyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Padhye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Raindel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Yahia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 ACM SIGCOMM Conference (SIGCOMM&apos;15)</title>
		<meeting>the 2015 ACM SIGCOMM Conference (SIGCOMM&apos;15)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
