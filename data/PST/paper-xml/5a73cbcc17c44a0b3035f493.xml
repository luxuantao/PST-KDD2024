<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Evolving Unsupervised Deep Neural Networks for Learning Meaningful Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Yanan</forename><surname>Sun</surname></persName>
							<email>yanan.sun@ecs.vuw.ac.nz</email>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Gary</forename><forename type="middle">G</forename><surname>Yen</surname></persName>
							<email>gyen@okstate.edu</email>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Zhang</forename><surname>Yi</surname></persName>
							<email>zhangyi@scu.edu.cn.</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="institution">Sichuan University</orgName>
								<address>
									<postCode>610065</postCode>
									<settlement>Chengdu</settlement>
									<country key="CN">CHINA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Victoria University of Wellington</orgName>
								<address>
									<postCode>6140 NEW ZEALAND</postCode>
									<settlement>Wellington</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">School of Electrical and Computer En-gineering</orgName>
								<orgName type="institution">Oklahoma State University</orgName>
								<address>
									<postCode>74075</postCode>
									<settlement>Stillwater</settlement>
									<region>OK</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="institution">Sichuan University</orgName>
								<address>
									<postCode>610065</postCode>
									<settlement>Chengdu</settlement>
									<country key="CN">CHINA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Evolving Unsupervised Deep Neural Networks for Learning Meaningful Representations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1540F18BE62D0398D4298F53869DEA88</idno>
					<idno type="DOI">10.1109/TEVC.2018.2808689</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T04:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep learning</term>
					<term>neural networks</term>
					<term>representation learning</term>
					<term>evolutionary algorithm</term>
					<term>evolving neural networks</term>
					<term>IEEE Transactions on Evolutionary Computation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep Learning (DL) aims at learning the meaningful representations. A meaningful representation gives rise to significant performance improvement of associated Machine Learning (ML) tasks by replacing the raw data as the input. However, optimal architecture design and model parameter estimation in DL algorithms are widely considered to be intractable. Evolutionary algorithms are much preferable for complex and nonconvex problems due to its inherent characteristics of gradientfree and insensitivity to the local optimal. In this paper, we propose a computationally economical algorithm for evolving unsupervised deep neural networks to efficiently learn meaningful representations, which is very suitable in the current Big Data era where sufficient labeled data for training is often expensive to acquire. In the proposed algorithm, finding an appropriate architecture and the initialized parameter values for an ML task at hand is modeled by one computational efficient gene encoding approach, which is employed to effectively model the task with a large number of parameters. In addition, a local search strategy is incorporated to facilitate the exploitation search for further improving the performance. Furthermore, a small proportion labeled data is utilized during evolution search to guarantee the learned representations to be meaningful. The performance of the proposed algorithm has been thoroughly investigated over classification tasks. Specifically, error classification rate on MNIST with 1.15% is reached by the proposed algorithm consistently, which is considered a very promising result against state-of-the-art unsupervised DL algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>D EEP Learning (DL) algorithm, which is materialized by Deep Neural Networks (DNNs) for learning meaningful representations <ref type="bibr" target="#b0">[1]</ref>, is a very hot research area during recent years <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b3">[4]</ref>. Meaningful representation refers to the outcome of the raw input data that goes through multiple nonlinear transformations in the DNNs, and the outcome could remarkably enhance the performance of the subsequent machine learning tasks. The hyper-parameter settings and parameter values in DNNs are substantially interrelated to the performance of DL algorithms. Specifically, hyper-parameters (such as the size of weights, types of nonlinear activation functions, a priori term types, and coefficient values) refer to the parameters that are needed to be assigned prior to training the models, and parameter values refer to the element values of the weights and are determined during the training phase. Due to the deficiencies of the current optimization techniques for searching for optimal hyper-parameter settings and parameter values, the power of DL algorithms cannot be shown fully. To this end, an effective and efficient approach concerning the hyper-parameter settings and parameter values has been proposed in this paper.</p><p>Meaningful Representations Typically, arbitrary DNNs can generate/learn Deep Representations (DRs). However, DRs are not necessarily meaningful, i.e., it is not true that all DRs contributed to the promising performance when they replace the raw data to be fed to machine learning algorithms (e.g., classification). In fact, DRs are the outcomes which have gone through nonlinear transformations from input data more than once <ref type="bibr" target="#b4">[5]</ref>, and are inspired by the mammalian hierarchical visual pathway <ref type="bibr" target="#b5">[6]</ref>. Mathematically, the representations of the input data X ∈ R m are formulated by (1)</p><formula xml:id="formula_0">           R 1 = f 1 (W 1 X) R 2 = f 2 (W 2 R 1 ) • • • R n = f n (W n R n-1 ) R = R n (1)</formula><p>where f 1 , • • • , f n denote a set of element-wise nonlinear activation functions, W 1 , • • • , W n refer to a series of connection weights and R 1 , R 2 , • • • , R n are the learned representations (output) at the depth/layer 1, 2, • • • , and n, among which R = {R i |2 ≤ i ≤ n} refers to the DRs. In addition, Fig. <ref type="figure" target="#fig_0">1</ref> shows the flowchart of deep representation learning and its role in machine learning tasks in a general case. Obviously, multiple different DRs can be learned by varying n in (1), while we only pay attention to the ones that give the highest performance of the associated machine learning tasks. Based on literature reviews <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b8">[9]</ref>, these DRs are often called meaningful representations. Assuming R j are the meaningful representations, it is obvious that the hyperparameter settings (e.g., the number of layers, j, and the chosen activation function types of f 1 , • • • , f j ) and parameter values (e.g., the values of each element in {W 1 , • • • , W j }) would highly reflect the learned R j to be meaningful or not. To this end, the Back-Propagation algorithm (BP) <ref type="bibr" target="#b9">[10]</ref> which relies on the gradient information is the widely employed algorithm in training parameter values. However, its performance is highly affected by the initialized setting due to its local search characteristics that could be easily trapped into local minima <ref type="bibr" target="#b10">[11]</ref>. Although multiple implementations based on BP, such as Stochastic Gradient Descent (SGD), AdaGrad <ref type="bibr" target="#b11">[12]</ref>, RMSProp <ref type="bibr" target="#b12">[13]</ref>, and AdaDelta <ref type="bibr" target="#b13">[14]</ref>, have been presented to expectedly reduce the adverse impact of easily trapping into local minima, extra hyper-parameters (such as the initialization values of momentums and the balance factors) are introduced and also needed to be carefully tuned in advance. Furthermore, multiple algorithms <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref> have been proposed for optimizing the hyper-parameters, but they often require domain knowledge and are problem-dependent. To this end, the grid search method keeping its dominant position in selecting reasonable hyper-parameters was proposed <ref type="bibr" target="#b16">[17]</ref>. However, the grid search method is an exhaustive approach, and would frequently miss the best hyper-parameter combinations when the hyper-parameters are continuous numbers.</p><p>Deep Neural Networks According to literature <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, DL algorithms mainly include Convolutional NNs (CNNs), Deep Belief Networks (DBNs), and stacked Auto-Encoders (AEs). Specifically, CNNs are supervised algorithms for DL, and their numerous variants have been developed for various real-world applications <ref type="bibr" target="#b19">[20]</ref>- <ref type="bibr" target="#b24">[25]</ref>. Although these CNN algorithms have shown promising performance in some tasks, sufficient labeled training data, which is a must for successfully training them, are not easy to acquire. For example in the ImageNet benchmark <ref type="bibr" target="#b26">[27]</ref>, there are 10 9 pictures that can be easily downloaded from the Google and Yahoo websites. It was reported that 48, 940 workers from 167 countries are employed to label these photos. Therefore, the unsupervised NN approaches whose training processes rely solely on unlabeled data become preferable in this situation. DBNs <ref type="bibr" target="#b27">[28]</ref> and stacked AEs <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref> are the mainly unsupervised DL algorithms <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref> for learning meaningful representations. Because of the unknown in training data targets during their training phase, learned representations from them are not necessarily to be meaningful. Therefore, a priori knowledge is needed to be incorporated into their training phase. For example, DBNs and stacked AEs trained with the sparsity constraint a priori with benefits of sparse coding <ref type="bibr" target="#b30">[31]</ref> have been proposed in <ref type="bibr" target="#b31">[32]</ref> and <ref type="bibr" target="#b32">[33]</ref>. Furthermore, denoising AEs <ref type="bibr" target="#b33">[34]</ref> have been proposed by artificially adding noise priori to input data for improving the ability to learn meaningful representations. In addition, Rifar et al. <ref type="bibr" target="#b34">[35]</ref> have presented contractive AEs by introducing the term, which is the derivation of representations with respect to input data, for reducing the sensitivity a priori of representations.</p><p>Evolutionary Algorithms for NNs Evolutionary algorithms (EAs) are one class of population-based meta-heuristic optimization paradigms, and are motivated by the metaphors of biological evolution. During the period of evolution, individuals interact with each other and the beneficial traits are passed down to facilitate population adapting to the environment. Due to the nature of gradient-free and insensitivity to local optima, EAs are preferred in various problem domains <ref type="bibr" target="#b35">[36]</ref>. Therefore, they have been extensively employed in optimizing NNs, which refers to the discipline of neuroevolution, such as for the connection weight optimization <ref type="bibr" target="#b36">[37]</ref>-[39], the architecture setting <ref type="bibr" target="#b39">[40]</ref>- <ref type="bibr" target="#b41">[42]</ref> (more examples can be found in <ref type="bibr" target="#b35">[36]</ref>). Generally, these algorithms employ direct or indirect methods to encode the optimized problems for the evolution. To be specific, each parameter in the connection weights is encoded by the binary numbers <ref type="bibr" target="#b36">[37]</ref> or a real number <ref type="bibr" target="#b42">[43]</ref> in the direct methods, which are effective for the smallscale problems. However, when they are used to encode the problems with a large number of parameters in connection weights, such as for processing the high-dimensional data, these methods become impractical due to the excessive length of the genotype explicitly representing each parameter no matter if coded in binary or real. To this purpose, Stanley and Miikkulainen have proposed the indirect-based Neural Evolution Augmenting Topologies (NEAT) method <ref type="bibr" target="#b43">[44]</ref> for encoding connection weights and architectures with varying lengths of chromosomes. Because NEAT employs one unit to denote combinational information of one connection in the evolved NN, it still cannot effectively solve deep NNs where a large number of parameters exist. To this end, an improved version of NEAT (i.e., HyperNEAT) was proposed in <ref type="bibr" target="#b44">[45]</ref> in which connection weights were evolved by composing different points in a fixed coordinate system with a series of predefined nonlinear functions. Although the indirect methods can reduce the length of the genotype representation, they limit the generalization of the neural networks and the feasible architecture space <ref type="bibr" target="#b35">[36]</ref>. In 2015, Gong et al. <ref type="bibr" target="#b45">[46]</ref> proposed a bi-objective evolutionary algorithm by using Differential Evolution <ref type="bibr" target="#b46">[47]</ref> to concurrently consider the reconstruction error and sparsity of the AE, and chose the optimal sparsity from the knee area of the Pareto front. Recently, Liu et al. <ref type="bibr" target="#b47">[48]</ref> presented a neural network connection pruning method by a multi-objective evolutionary algorithm to simultaneously consider the representation ability and the sparse measurement. Google <ref type="bibr" target="#b48">[49]</ref> proposed their work on evolving CNNs for image classifications with a direct manner over 250 high performance servers for more than 400 hours. In this regard, the evolutionary approaches would surely be capable of evolving deep NNs, although the computational resource is not necessarily available to all interested researchers.</p><p>Contributions Based on the above investigations upon prospects of unsupervised deep NNs for learning meaningful representations and the EAs in evolving deep NNs, an effective and efficient approach named Evolving Unsupervised Deep Neural Networks (EUDNN) for learning meaningful representation through evolving unsupervised deep NNs, exactly evolving the building blocks of unsupervised deep NNs, has been proposed in this paper. In summary, the contributions of this paper are documented as follows:</p><p>1) A computationally efficient gene encoding scheme of evolutionary approaches has been suggested, which is capable of evolving deep neural networks with a large number of parameters for addressing high-dimensional data with limited computational resources. With this design, the proposed algorithm can be smoothly implemented in academic environments with limited computational resources. 2) A fitness evaluation strategy has been employed to drive the unsupervised models towards usefulness in advance, which can drive the learned representations to be meaningful without any carefully designed a priori knowledge. 3) Deep neural networks with a large number of parameters involve a large-scale global optimization problem. As a result, the sole evolutionary scheme cannot generate the best results. To this end, the utilization of a local search strategy is proposed to be incorporated into the proposed algorithm to guarantee the desired performance. Organization The remaining of this paper is organized as follows. First, related works and motivations of the proposed EUDNN are illustrated in Section II. Next, the details and discussions of the proposed algorithm are presented in Section III. To evaluate the performance, a series of experiments are performed by the proposed algorithm against selected peer competitors and the results measured by the chosen performance metric are analyzed in Section IV. Finally, conclusions and future work are drawn in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS AND MOTIVATIONS</head><p>We will detail the unsupervised DL models that motive our work in this paper, highlight their deficiencies in learning meaningful representations, and rationalize our motivations in Subsection II-A. With this same detailed manner, the evolutionary algorithms which demonstrate the potential for evolving deep NNs will be documented in Subsection II-B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Unsupervised Deep Learning Models</head><p>In this subsection, the unsupervised DL models are reviewed first (Subsection II-A1). Then, their building blocks are introduced (Subsection II-A2). Next, the mechanisms guaranteeing the learned representations to be meaningful are formulated and commented (Subsection II-A3). Finally, the motivations of the proposed algorithm in reducing the adverse impact of their deficiencies are elaborated (Subsection II-A4).</p><p>1) Unsupervised DL models cover DBNs <ref type="bibr" target="#b27">[28]</ref> and variants of stacked AEs (i.e., stacked sparse AEs (SAEs) <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, stacked denoising AEs (DAEs) <ref type="bibr" target="#b33">[34]</ref>, and stacked contract AEs (CAEs) <ref type="bibr" target="#b34">[35]</ref>). Moreover, the building block of DBNs is a Restricted Boltzmann machine (RBM) <ref type="bibr" target="#b49">[50]</ref>, and that of stacked AEs is an AE. Furthermore, the parameter values in DBNs and stacked AEs are optimized by the greedy layer-wise training method, which is composed of two phases <ref type="bibr" target="#b50">[51]</ref>: pretraining and fine-tuning. Conveniently, Fig. <ref type="figure" target="#fig_3">2a</ref> depicts the pretraining phase, where a set of three-layer (the input layer, the hidden layer, and the output layer) NNs with varying numbers of units are individually trained by minimizing reconstruction errors. In the fine-tuning phase which is illustrated by Fig. <ref type="figure" target="#fig_3">2b</ref>, these hidden layers are first sequentially stacked together with the parameter values trained in the pre-training phase, then a classification layer (i.e., the classifier) is added to the tail to perform the fine-tuning by optimizing the corresponding loss function determined by the particular task at hand.   2) Unsupervised DL algorithms are considerably preferred mainly due to their requirements upon fewer labeled data especially in the current Big Data era<ref type="foot" target="#foot_0">1</ref> . However, a major issue of training these models is how to guarantee the learned representations to be meaningful. Specifically in the pretraining phase for training one NN unit (see Fig. <ref type="figure">3</ref> as an example), let X ∈ R n denote the input data, W ∈ R n×k denote the connection weight matrix from the input layer to the hidden layer, while W ′ ∈ R k×n denote the connection weight matrix from the hidden layer to the output layer. The NN unit is trying to minimize the reconstruction error L between the input data X and the output data</p><formula xml:id="formula_1">X ′ by (2) 2 W ' W X R ' X Fig. 3. An example of unsupervised deep neural network unit model.    R = f (W X) X ′ = f (W ′ R) L = l(X, X ′ )<label>(2)</label></formula><p>In (2), R denotes the learned representations (i.e., the output of the hidden layer), f denotes the activation function, and l denotes the function to measure the differences between X and X ′ .</p><p>3) It is obvious that the learned representations R are not necessarily meaningful only by minimizing L due to no information of the associated classification task existing in this phase and arbitrary R will lead to a minimal L, while R is meaningful only when they could improve the performance of the associated classification task. To this end, literature have presented unsupervised DL algorithms with different a priori knowledge <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b32">[33]</ref>- <ref type="bibr" target="#b34">[35]</ref> which is denoted as Θ, and then the reconstruction error is transformed to L = l(X, X ′ ) + λΘ where λ denotes a balance factor to determine the weight of the associated a priori term. Although a prior knowledge would help the learned representations to be meaningful, major issues remain:</p><p>• The prior knowledge is designed with different assumptions, which do not necessarily satisfy the current situations.</p><p>• The prior knowledge is presented specifically for general tasks, while it is hopeful that the performance would be improved on particular tasks. • It is difficult to choose the most suitable a priori term for the current task. • The balance factor λ is a hyper-parameter whose value is not easily to be assigned <ref type="bibr" target="#b34">[35]</ref>. 4) Considering this problem, the method that has been developed in our previous work <ref type="bibr" target="#b8">[9]</ref> is employed in this proposed algorithm. To be specific, a small proportion of labeled data is employed during the fitness evaluation of EAs, and the learned representations are directly quantified based on the classification task that is employed in the fine-tuning phase. With the environmental selection in EAs, individuals that have the positive effect on the classification task survive into the current generation and are expected to generate offspring with better performance in the next generation, which ultimately leads to the learned representations to be meaningful. Because the employed labeled data can be injected from the finetuning phase, and the classification task is the same as that in the fine-tuning phase, this strategy for learning meaningful representations would not introduce extra cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evolutionary Algorithms for Evolving Neural Networks</head><p>Although multiple related literature for evolving NNs have been mentioned in Section I, only the works in <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref> (i.e., the NEAT and the HyperNEAT) will be concerned here because our proposed algorithm aims at evolving deep NNs<ref type="foot" target="#foot_2">3</ref> . In the following, the details of NEAT, as well as HyperNEAT and their deficiencies in evolving deep NNs are documented in Subsections II-B1 and II-B2, respectively. Combined with the challenge of EAs in evolving deep NNs, i.e., the upper bound encoding problem, the motivations of the proposed EUDNN are presented in Subsection II-B3. In addition, another challenge, i.e., EAs cannot fully solve the optimization problems with a large number of parameters, and the corresponding motivations are given in Subsection II-B4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1)</head><p>The NEAT <ref type="bibr" target="#b43">[44]</ref> has been proposed with an indirect method for adaptively increasing the complexity of the evolved NNs. Specifically, two types of genes, i.e., the node genes and the connection genes, exist in the NEAT. The node genes, which are used to represent all the units of the evolved NN, are encoded with the type of the unit (i.e., the input unit, the hidden unit, or the output unit) and one identification number. The connection genes that are employed to denote the connection information between the node genes, and one node gene is encoded with five elements (the numbers of the input and output units, the value of the connection, one bit indicating whether the connection is activated or not, and one innovation number which records the index of the connection gene with an increased manner). During the evolution process, the individuals are first initialized only with the input and output units of the network, and the random connections between these units. Then, individuals are recombined and mutated. To be specific, there are two types of mutations including the connection mutations and the node mutations. When the connection mutations occur, one connection gene will be added to the list of the connection genes to denote that a pair of node genes is connected. While for the node mutations, one hidden node is generated, then the corresponding connection gene is created to split one existed connection into two parts. Although the NEAT is flexible to evolve NNs, a deterministic number of the output is required, which is impractical in the DL. Furthermore, due to each connection and unit in NEAT are explicitly encoded, it is not suitable for evolving deep NNs that often have a large number connections and units. For remedying this deficiency regarding the incapacity of evolving deep NNs, the connective compositional pattern producing networks (CPNN) <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b51">[52]</ref> has been presented and led to the HyperNEAT.</p><p>2) The HyperNEAT has been proposed by combining the NEAT with the CPNN encoding scheme. Particularly, the CPNN employs one low-dimensional coordinate system to generate connections for the NEAT by a list of predefined nonlinear functions. To be specific, any point in the coordinate system is picked up, and then fed into a series of compositional functions from the list to complete the transformation from the genotype to the phenotype. Because any number of points can be selected from the low-dimensional coordinate system, numerous connections would be represented with a low computational cost. In this regard, the HyperNEAT has the most potential for evolving a deep NN, while the size of the output still needs to be set in advance, which faces the same problem to NEAT in practice. Furthermore, all the values of the connections in the HyperNEAT are generated by the genetic operators during the evolution, which cannot guarantee the best performance in evolving a deep NN due to the nature of the large-scale global problem. In addition, the recurrent connections or the connections between the same layers are involved in this algorithm, which is also not suitable for learning compact meaningful representations.</p><p>3) As we have discussed in Section I, the performance of DL algorithms is highly affected by the hyper-parameter settings and the parameter values. In the pre-training phases, one of the key hyper-parameters is the size of hidden layers.</p><p>One problem would be naturally raised when EA approaches are employed to search for the sizes, that is how we can ensure the upper bound of the hidden layer sizes given a fixedlength gene encoding strategy. Although the indirect encoding scheme can alleviate this situation somewhat, it limits the generalization of the evolved NNs and the feasible architecture space <ref type="bibr" target="#b36">[37]</ref>. On the other hand, if we employ a larger number as the upper bound, it is difficult to determine how large it is reasonable because too large a number would consume more computational resources, otherwise deteriorate the model performance. Excitingly, Yang et al. <ref type="bibr" target="#b52">[53]</ref> have mathematically pointed out that the meaningful representations of the input data lie at its original space. Supposed that the input data is with n dimension, the size of the associated hidden layer should be no more than n. Furthermore, we know that n orthogonal n-dimensional basis vectors are sufficient to span a n-dimensional space based on Theorem 1. Consequently, we only need to compute one basis r 1 of n-dimensional space, and the other (n -1) n-dimensional basis vectors can be explicitly computed by (3) to find the null space<ref type="foot" target="#foot_3">4</ref> . To this end, we can efficiently model the problem with n 2 parameters by employing a genetic algorithm to explicitly encode about n parameters, which is a computational efficient gene encoding approach.</p><formula xml:id="formula_2">Theorem 1. A set of orthogonal vectors b i ∈ R n (i = 1, • • • , n) is sufficient to span the space S ∈ R n . null space(r 1 ) = {x ∈ R n |r 1 x = 0}<label>(3)</label></formula><p>4) Here, we would point out another challenge to inspire our motivation for evolving deep NNs by employing GAs. In our proposed algorithm, the computationally efficient gene encoding strategy mentioned above is employed to model unsupervised deep NNs where a large number of parameters exist. Although the length of the encoded parameters has been reduced appreciably in this regard, the number of the parameters in the original problems remains constant no matter what encoding method is employed. In fact, the effects of one gene in the employed encoding strategy is equivalent to that of multiple parameters in the original problems. For example, for an NN which has 100, 000 parameters, only 1, 000 genes are employed by the computationally efficient gene encoding strategy proposed herein. As a result, one gene represents 100 parameters in average, and if one gene is changed with the crossover and mutation operators, it will involve the changes of 100 parameters. Moreover, it is well known that performances of EAs are guaranteed by their exploration search (given by mutation operators) and exploitation search (given by crossover operators) which introduce the global search and local search abilities, respectively. Because a slight change of one gene in the proposed algorithm will lead to the changes of many parameters which affect the global behavior, it can be viewed as that EAs lack of the local search from the problem to be solved. In addition, the data which are processed by DL algorithms is common with high dimension, which leads to a large number of decision variables in the encoded chromosomes of EAs, although our employed encoding strategy has saved much space compared to existing approaches. Extensive experiments have quantified that EAs are difficult to reach the best performance upon the problems with high input dimensions. To address this issue, we incorporate a local search strategy into the proposed algorithm for assuring the desirable performance.</p><p>In summary, the difficulties of deep unsupervised NNs for learning meaningful representations and EAs for evolving deep NNs have been clarified first, and then addressed by our motivations in this section. In the next section, the technical details will be implemented based on these motivations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED ALGORITHM</head><p>In this section, the details of the proposed EUDNN are presented. To be specific, the framework which is composed of two distinct stages is depicted at first (Subsection III-A). Next the specifics of each stage are elaborated, respectively (Subsections III-B and III-C). Furthermore, the over-fitting problem preventing mechanism of EUDNN and the significant differences against its peer competitor are discussed (Subsection III-D).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Framework of EUDNN</head><p>In this subsection, the framework of the proposed EUDNN is presented. For convenience of the development, it is assuming that the learned representations are for a classification task in which the meaningful representations can improve its performance in term of a higher Correct Classification Rate (CCR) (the CCR upon the training data is collected during the training/optimization phase, and that upon the test data during the test/experimental phase). Moreover, given a set of data D in this classification task, a portion of D which is denoted by D train = {(x 1 , y 1 ), • • • , (x k , y k )} is considered as the training data in which x i denotes the input data and y i is the corresponding label, while the remaining data is regarded as the test data D test for checking whether the learned representations are meaningful. Furthermore, the flowchart of the proposed EUDNN is illustrated in Fig. <ref type="figure">4</ref>, which clearly shows the two stages of the design: 1) finding the optimal architectures in deep NNs, the desirable initialization of connection weight, and the activation functions (pretraining), and 2) fine-tuning all of the parameter values in connection weights from the desirable initialization.</p><p>To this end, one genetic approach with an efficient strategy introduced in Subsection II-B is employed to encode the potential architectures and the associated large numbers of parameters in connection weights by a set of individuals, and then the EA is utilized to evolve and select the individual who has the best performance based on the fitness measures. For warranting the learned representations being meaningful, the method introduced in Subsection II-A is employed, i. </p><formula xml:id="formula_3">W 1 , • • • , W p ; 7 Y test = C(f p (W p × • • • f 2 (W 2 × f 1 (W 1 × D test )))); 8 Return Y test .</formula><p>the associated classification task to select the ones which give the higher CCR for evolution. Based on the investigations in Subsection II-B, a fine-tuning approach additionally, which introduces the exploitation local search, is utilized in the second stage to archive the best performance ever found, which complements with the exploration global search in the first stage. In summary, these two stages collectively ensure the learned representations to be meaningful through unsupervised deep NNs.</p><p>In addition, the framework of the proposed EUDNN is presented in Algorithm 1. Specifically, lines 2-5 describe the first stage, while line 6 defines the second stage. Finally, the predicted labels of the test data are calculated and returned in lines 7 and 8. Next, the details of these two stages are documented, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Obtaining Optimal Connection Weights and Activation Functions via Evolving</head><p>The process of obtaining all the optimal connection weights and their corresponding activation functions contains a series of repeated subprocesses. In this subsection, we first in Algorithm 2 propose how to obtain one optimal connection weight and its activation function. Then, the entire process is described.</p><p>To be specific in Algorithm 2, m individuals that encode the information of potential optimal connection weights and their corresponding activation functions are initialized first (line 1). Then, the evolution takes effect (lines 2-8) until the stopping conditions, such as exceeding the maximum generations, are met. During each generation, the fitness of all the individuals are evaluated first (line 3). Next, new offspring are generated with the probability ρ, and their parents are selected from P with the binary tournament selection (line 4). Then, all the offspring in Q are mutated with the probability µ (line 5). Furthermore, lines 6-7 describe the environmental selection in which the best individual is preserved first for the elitism, then m -1 individuals are selected from the remaining solutions in P ∪ Q with binary tournament selection. Specifically, two individuals are randomly selected from (P ∪ Q) \ S first. Then the one with better CCR is chosen, and the other is put back. With the same process, this operation is repeated m -1 times.</p><p>When the evolution terminates, the best solution is selected from the current population for transforming the optimal Next, the details of the employed gene encoding strategy will be discussed, although its fundamental principles have been documented in Subsection II-B. It has been pointed out in <ref type="bibr" target="#b52">[53]</ref> that the potential connection weight for obtaining the meaningful representations likely lies in a subspace of the original space. As a consequence, the search for the optimal connection weight can be constrained in the space of input data. Specifically, it is assuming that the input data is ndimensional.  <ref type="bibr" target="#b2">(3)</ref>. It is obvious that {a 1 , a 2 , • • • , a n } are capable of spanning the space of input data. Finally, a part of these bases, which span a subspace of the original space, are selected for constructing the optimal connection weight by a binary encoded string indicating whether the corresponding basis is available. Furthermore, the corresponding activation function is also encoded into the chromosome. Specifically, a list of selected activation functions with different nonlinear capacities is given, then their indexes in this list are chosen to indicate which one is selected. Moreover, Fig. <ref type="figure" target="#fig_6">5</ref> is provided to intuitively illustrate our intention on efficiently encoding the connection weight and activation function. When the optimal connection weight W i and its corresponding activation function f i are found for the i-th layer with Algorithm 2, then that for the (i + 1)-th layer can be optimized with the same algorithm by setting the input data as</p><formula xml:id="formula_4">f i (W i × R i )</formula><p>where R i denote the representations at the i-th layer. In the employed gene encoding approach, each coefficient of b is represented with nine bits in which the leftmost bit denotes the positive or negative of the coefficient. Then, one bit is used to indicate whether the basis a j (j</p><formula xml:id="formula_5">∈ [2, • • • , n])</formula><p>is selected for the connection weight. Finally, two bits are utilized to represent the activation function. In addition to the well-adopted sigmoid and hyperbolic tangent functions, rectifier function <ref type="bibr" target="#b53">[54]</ref>, which is reported recently to have a superior performance in some applications, is also considered as one candidate. As a consequence, one chromosome needs 10n + 1 bits for the n-dimensional input data. If the real number encoding method is employed here, a multiple of eight memory space would be taken, which is the major reason that the proposed EUDNN employs the binary encoding method being a contribution to the so claimed computational efficient gene encoding strategy. Furthermore, the linear Support Vector Machine (SVM) <ref type="bibr" target="#b54">[55]</ref> is employed for evaluating the quality of individuals due to its promising computational efficiency and its linear nature for better discriminating power whether the learned representations are meaningful or not. Next, we will give the details of the fitness evaluation by using SVM based on the design principle described in Subsection II-A4. For convenience of the development, let D train = {X train , Y train } denote the training set where X train are the data and Y train are the corresponding labels, and the selected individual for fitness evaluation is denoted by ind i . Firstly, a small fraction of data denoted by D eval = {X eval , Y eval } is randomly selected from D train . Secondly, the corresponding model is transformed from the encoded individual ind i . Thirdly, the representations (denoted by F eval ) of X eval are calculated based on the formulas in <ref type="bibr" target="#b0">(1)</ref>. Fourthly, {F eval , Y eval } are fed to SVM and the CCR on X eval is estimated. Finally, the CCR is used as the fitness of ind i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Fine-tuning Connection Weights</head><p>To further improve the performance, an exploitation mechanism implemented by local search strategy is incorporated into the second stage to fine-tune parameter values in connection weights. In this stage, the architecture is fixed with the evolved activation functions and the initialization values of the connection weights, and then a local search method is used to tune the connection weights further. Fig. <ref type="figure" target="#fig_8">6</ref> shows an example of this process. Specifically, when all the connection weights and activation functions have been optimized in the first stage, all the hidden layers are connected to a list based on their orders in the first stage by adding one input layer at the top of this list. Then, the connection weights in this list are initialized with the values confirmed in the first stage. Finally, a classifier is added to the tail of this list to perform the fine-tuning process. Note here that the BP algorithm is employed for the fine-tuning. Actually, any local search algorithm can be used in the second stage. The reasons for employing BP are largely due to two aspects: 1) the gradient information in the loss function is always analytical and the BP that is based on the gradient is naturally employed in most designs; 2) multiple libraries of BP have been implemented for accelerating the computation with the Graphics Processing Units (GPUs) and the computational cost can be reduced remarkably, especially in the situations of processing high-dimensional data. Furthermore, when the rectifier activation function that is not differentiable at the point 0 is selected, the value 0 is assigned according to the convention of the community <ref type="bibr" target="#b55">[56]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Discussions</head><p>In this subsection, we mainly discuss the over-fitting problem preventing mechanism utilized by the proposed EUDNN, and the significant differences of the proposed EUDNN against the Direct Evolutionary Feature Extraction algorithm (DEFE) <ref type="bibr" target="#b56">[57]</ref> that employs a similar gene encoding strategy to EUDNN.</p><p>The over-fitting problem implies the poor generalization ability of models, i.e., the trained model reaches a better CCR upon training data at the cost of a worsen CCR upon test data. Because the goal in training a classification model is for obtaining a higher CCR upon test data, the overfitting problem should be prevented by some mechanisms. Commonly, given a number of models which are all capable of solving a particular classification task, the model with a smaller Vapnik Chervonenkis (VC) dimension<ref type="foot" target="#foot_4">5</ref>  <ref type="bibr" target="#b6">[7]</ref> usually has a better generalization ability, which does not lead to an overfitting problem. Because the number of parameters is positive to the value of a VC dimension, and deep NN architectures are generally with the numerous number of parameters, the over-fitting problem easily occurs in these models. More specifically, Fig. <ref type="figure" target="#fig_9">7</ref> illustrates a typical instance in CCR on training data (red curve) and CCR of checking on test data (green curve) as the training process continues. Especially, CCR on both data are continuously growing until the time t 1 , and CCR on the training data continues to increase while CCR on the test data begins to drop when the training time is greater than t 1 , which obviously indicates the presence of an over-fitting problem. As we have claimed that the best performance of the proposed EUDNN cannot be guaranteed during the training in the first stage, and the second stage is introduced to expectedly help the proposed EUDNN arrive at the best performance. To this end, it is concluded that the overfitting problem will not occur in the first stage of the proposed EUDNN (because the first stage terminates prior to the time t 1 , while the over-fitting problem might occur after the time t 1 ), but may occur in the second stage. Consequently, some rules need to be utilized to prevent this problem only in the second stage. Here, the "early stop" approach is utilized for this purpose, i.e., a group of data D validate is uniformly selected from D train as the validate data to replace the checking upon test data in Fig. <ref type="figure" target="#fig_9">7</ref>, when we first observe the CCR of validate data begins to decrease while the CCR of training is still increasing (i.e., the particular time t 1 is found), the finetuning in the second stage is terminated and the optimal model that gives the best performance is obtained. Next, the second concern, i.e., the differences between the proposed EUDNN and the DEFE, will be discussed.</p><p>It has been observed that 1) DEFE learns only linear representations and 2) shallow representations of input data. These two observations cause that DEFE cannot learn the meaningful representations <ref type="bibr" target="#b27">[28]</ref>. Next, the details of these conclusions are discussed. To be specific, the learned representations R of DEFE can be formulated as R = W X <ref type="bibr" target="#b56">[57]</ref> where W is the transformation matrix (i.e., the connection weight in deep NN models) and X is the input data. It is evident that there is no nonlinear transformation upon W X. Consequently, only linear representations would be learned by DEFE, while in the proposed EUDNN, a list of nonlinear activation functions with different nonlinear transformation abilities is incorporated into the evolution for performing nonlinear representation learning. Furthermore, although multiple transformations like that in the proposed EUDNN can be implemented by DEFE to learn deep representations, deep linear transformations are equivalent to a one layer linear representation.</p><p>In summary, DEFE cannot be employed for learning meaningful representations due to its linear nature, while the success of deep NNs is mainly caused by the meaningful representations learned by deep nonlinear transformations, which have been explicitly implemented by the proposed EUDNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In order to examine the performance of the proposed EUDNN, experiments based on a set of image classification benchmarks against selected peer competitors are performed. During the comparisons, the chosen performance metric considers the CCR on the test data. In the following, the employed benchmarks are outlined first. Then the chosen peer competitors are reviewed, and the justification for selecting them is explained further. This is followed by the descriptions of the performance metric chosen and the specifics of parameter settings employed by these compared algorithms. Finally, the quantitative as well as the qualitative experimental results are illustrated and comprehensively analyzed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Benchmark Test Datasets</head><p>Benchmarks used by compared algorithms are the handwritten digits benchmark test dataset MNIST <ref type="bibr" target="#b20">[21]</ref>, basic MNIST dataset (MNIST-basic) <ref type="bibr" target="#b59">[60]</ref>, a rotated version of MNIST (MNIST-rot) <ref type="bibr" target="#b59">[60]</ref>, MNIST with random noise background (MNIST-back-rand) <ref type="bibr" target="#b59">[60]</ref>, MNIST with random image background (MNIST-back-image) <ref type="bibr" target="#b59">[60]</ref>, MNIST-rot with random image background (MNIST-rot-back-image) <ref type="bibr" target="#b59">[60]</ref>, tall and wide rectangles dataset (Rectangles) <ref type="bibr" target="#b59">[60]</ref>, rectangles dataset with random image background (Rectangles-image) <ref type="bibr" target="#b59">[60]</ref>, convex sets recognition dataset (Convex) <ref type="bibr" target="#b59">[60]</ref>, and the gray version of Canadian Institute for Advanced Research object recognition dataset <ref type="bibr" target="#b60">[61]</ref> (Cifar10-bw) over 10 classes, i.e., airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck.  <ref type="table" target="#tab_0">I</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Performance Metric</head><p>Technically speaking, it is difficult to directly evaluate whether the learned representations are meaningful or not because they are intermediate outcomes. A general practice for this is to feed these learned representations to a particular classification task, and then to investigate the CCR by a classifier. Commonly, a higher CCR implies that the learned representations are more meaningful. Because the benchmarks employed in these experiments are multi-class classification tasks, the softmax regression classifier <ref type="bibr" target="#b61">[62]</ref> is employed here to measure the corresponding CCR according to the convention adopted in the community.</p><p>It is assumed that a set of training data and their corresponding labels with k distinct integer values are denoted as {x 1 , • • • , x m }, and {y 1 , • • • , y m }, respectively, where x i ∈ R n and y i ∈ {1, • • • , k}. To be specific, the label of the sample x i (i ∈ {1, • • • , m}) is predicted by ( <ref type="formula" target="#formula_6">4</ref>) with the softmax regression,</p><formula xml:id="formula_6">arg max j p j (x i ) = exp(θ T j x i ) k l=1 exp(θ T l x i )<label>(4)</label></formula><p>where</p><formula xml:id="formula_7">Θ = [θ 1 , • • • , θ k ]</formula><p>T are obtained by minimizing</p><formula xml:id="formula_8">J(Θ) = - 1 m   m i=1 k j=1 f (y i , j)log exp(θ T j x i ) k l=1 exp(θ T l x i )   in which f (y i , j) = 1 if y i = j, otherwise f (y i , j) = 0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Compared Algorithms</head><p>Because of the proposed EUDNN aiming at evolving unsupervised deep neural networks for learning meaningful representations, algorithms related to evolving deep NNs (NEAT <ref type="bibr" target="#b43">[44]</ref>, HyperNEAT <ref type="bibr" target="#b44">[45]</ref>), unsupervised deep NNs (DBNs <ref type="bibr" target="#b50">[51]</ref>, and variants of stacked AEs <ref type="bibr" target="#b32">[33]</ref>) that have been discussed in Section I should be all employed as peer competitors. However, the NEAT and the HyperNEAT cannot be used to learn meaningful representations due to the reasons that have been discussed in Section I and further analyzed in Section II. As a result, they are excluded from the selected compared algorithms. To this end, DBNs and variants of stacked AEs are employed for performing the comparison experiments. Because RBMs <ref type="bibr" target="#b49">[50]</ref> and AEs <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref> are the building blocks to train DBNs and stacked AEs, respectively, these two types of algorithms are considered as the peer competitors in our experiments to compare the performance of the learned representations against that of the proposed algorithm (i.e., we will evolve RBMs and AEs as the unsupervised deep NN models, which are named EUDNN/RBM and EUDNN/AE, respectively, to perform the comparisons against considered peer competitors). Specifically, the variants of AEs, i.e., the Sparse AEs (SAEs) <ref type="bibr" target="#b30">[31]</ref>, the Denoising EAs (DAEs) <ref type="bibr" target="#b33">[34]</ref>, and the Contractive AEs (CAEs) <ref type="bibr" target="#b34">[35]</ref>, have been proposed with different regularization terms for learning meaningful representations in recent years and also have obtained comparable performance in multiple tasks. As a consequence, they are also included as the peer competitors in the experiments, in addition to the DBNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Parameter Settings</head><p>For a fair comparison, multiple parameters in the second stage of the proposed EUDNN and the competing ones are the same. As a consequence, we will first give details of these generic parameter settings in this subsection. Then, the particular parameter settings are individually introduced. Because the best performance of the compared algorithms often strongly depends on the particular benchmark dataset and the corresponding parameter settings, in order to do a fair comparison, we first test these parameters from the range widely used in the community upon the corresponding training data, then the best performance upon test data of each compared algorithm is selected for comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Learning Rate and Batch Size</head><p>The Stochastic Gradient Descent (SGD) algorithm is chosen as the algorithm to train the SAE, the DAE, the CAE, and the softmax regression, and its learning rates as well as the batch sizes vary in {0.0001, 0.001, 0.01, 0.1} and {10, 100, 200}, respectively, according to the community convention.</p><p>2) Number of Runs and Stop Criteria All the compared algorithms are independently performed 30 runs. In addition, a performance monitor is injected into each epoch in training the softmax regression to record the best CCR over the test dataset as the best performance of the algorithm that feeds the HLlearned representations to the softmax regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Unit Number and Depth</head><p>The number of the units for the SAE, the DAE, the CAE, and the RBM in each layer is set to be from 200 to 3, 000 using a log function with an interval 0.5 as recommended by <ref type="bibr" target="#b62">[63]</ref>, and the maximum depth is set to be 5 (this depth is excluded from the input layer, i.e., the maximum number of hidden layers).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Statistical Significance</head><p>The results measured by the selected performance metric need to be statistically compared due to the heuristic natures of the first stage in the proposed EUDNN. In these experiments, the Mann-Whitney-Wilcoxon rank-sum test <ref type="bibr" target="#b63">[64]</ref> with a 5% significant level is employed for this purpose according to the community convention.</p><p>In addition, the sparsity of the SAE, the binary corrupted level of the DAE, and the coefficient of the contractive term in the CAE are set to be 10%, 30%, 50% and 70%, respectively. Because of the nature of the RBM, the CD-k algorithm <ref type="bibr" target="#b64">[65]</ref> is selected as its training algorithm and k is set to be 1 based on the suggestion in <ref type="bibr" target="#b62">[63]</ref>. In order to speed up the proposed algorithm in the first stage, a proportion (i.e., 20%) of the training dataset is randomly selected in each generation for the fitness evaluation. In addition, the connection weights and the biases are respectively set to be between [-4×6/ √ n number , 4×6/ √ n number ] with a uniform sampling and 0, respectively <ref type="bibr" target="#b65">[66]</ref>, if required, where n number denotes the total number of the units in two adjacent layers based on the experiences suggested in <ref type="bibr" target="#b65">[66]</ref>.</p><p>Because parameter settings in the second stage of the proposed EUDNN are the same as that of the peer competitors, parameter settings of the evolution related parameters in the first stage are declared next. Conveniently, one chromosome in this stage can be divided into three parts: main basis related coefficients (Part 1) which are used to represent the vector a 1 in Fig. <ref type="figure" target="#fig_6">5</ref>, projected space related coefficients (Part 2) which are employed to indicate which bases are selected for the connection weight, and the coefficients (Part 3) which denote the type of activation functions. Because Parts 1 and 2 have strong effects on the quality of the connection weight, it is hopefully that crossover operation should be promoted in these two parts for improving the exploitation local search that provides much better performance based on the exploration global search. As a consequence, one point crossover operator is employed in Parts 1 and 2. In addition, three widely used nonlinear activation functions are considered in the proposed algorithm and one is to be selected for the corresponding connection weight. Therefore, it is hopefully that the information representing the activation function is not modified often since it is hard to determine which one is the best. Consequently, Parts 2 and 3 are considered as one part to participate in the crossover operation. It is noted here that, when the value in Part 3 is invalid, a random one is chosen to reset it. Noting that the polynomial mutation <ref type="bibr" target="#b66">[67]</ref> is used here as the mutation operator (distribution index is set to be 20). In addition, the population size is set to be 50. As for the crossover probability and the mutation probability in the proposed algorithm, both of them are set to be the same as that of the community convention (i.e., 0.9 for crossover and 0.1 for mutation). A proportion of 10% is randomly selected from the training set for the fitness evaluation. Codes of the proposed EUDNN can be made available upon request through the first author.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Experimental Results</head><p>Based on the motivation of our design, the proposed EUDNN 1) employs evolutionary algorithm and local search strategy to ensure the learned representations through deep NNs to be meaningful, 2) employs evolutionary approach in the first stage to help the deep NNs find the optimal architectures and the good initialized weights, which give a better starting position for the second stage, and 3) employs the local search strategy in the second stage to improve the intended performance much further. Consequently, a series of experiments are carefully crafted to evaluate the performance of the proposed design.</p><p>1) Performance of the Proposed Algorithm In order to quantify whether the representations learned by the proposed EUDNN are meaningful, a series of experiments are well-designed and comparisons are performed. Specifically, EUDNN/AE and EUDNN/RBM are two implementations of the proposed algorithm over the unsupervised neural network models (i.e., AEs and RBMs, respectively). Then they are used to learn the representations together with the selected peer competitors employing the configurations introduced above. Next, the softmax regression metric is employed to measure whether the learned representations improve the associated classification tasks through CCR, which in turn indicates the learned representations being meaningful or not.</p><p>Particularly, the mean values and standard derivations of CCR resulted by these compared algorithms over 30 independent runs are listed in Table <ref type="table" target="#tab_1">II</ref> in which the best results over the same benchmark are highlighted in boldface. In addition, the symbols "+," "-," and "=" denote whether the CCR of the proposed algorithm upon the corresponding benchmarks are statistically better than, worse than, and equal to that of the associated peer competitors, respectively, with the employed rank-sum test 6 . Furthermore, the summarizations, how many times over the considered benchmarks the proposed EUDNN are better than, worse than, and equal to the corresponding peer competitor, are listed in the last row of Table <ref type="table" target="#tab_1">II</ref>.</p><p>In Table <ref type="table" target="#tab_1">III</ref>, the first column shows the names of the chosen benchmark datasets, the second column provides the corresponding best CCRs obtained, while the third column presents the numbers of neurons of the deep models (excluding the the classifier layer) with which the best CCRs are reached on the corresponding benchmark dataset. As we have claimed in Subsection IV-D that the maximum number of building blocks investigated in this paper is set to be five. Therefore, the number of layers, which include the input layer and hidden layers, shown in Table <ref type="table" target="#tab_1">III</ref> for each benchmark dataset does not exceed six. For the first row in Table <ref type="table" target="#tab_1">III</ref> as an example, it indicates that the best CCR of 98.85% on the MNIST benchmark dataset is achieved with only four building blocks where the input layer is with 784 neurons, and hidden layers are with 400, 202, 106, and 88 neurons, respectively.</p><p>It is clearly shown in Table <ref type="table" target="#tab_1">II</ref> 7 that the proposed EUDNN/AE obtains the best mean values upon the MNISTrot, the MNIST-rot-back-image, the Convex, and the Cifar10bw benchmarks, and the best rank-sum results upon the MNIST-rot, the Convex, and the Cifar10-bw benchmarks. Moreover, the proposed EUDNN/RBM wins both the best mean values and the rank-sum results upon the MNIST, and the MNIST-back-image benchmarks. Although the best result of the proposed EUDNN (obtained by the EUDNN/AE) over the MNIST-basic benchmark is a little worse than that of the SAE, which is the winner of the best mean value and rank-sum results, EUDNN/AE outperforms all the other peer competitors. Furthermore, the SAE obtains the best mean values upon the MNIST-basic and the MNIST-back-rand benchmarks, but the best result of the proposed algorithm (obtained by the EUDNN/AE) is statistically equal to that of the SAE upon the MNIST-back-rand benchmark, and also outperforms other competing algorithms. Upon the Rectanglesimage benchmarks, the best result of the proposed algorithm 6 To do this statistically test, we first select the better CCR generated by EUDNN/AE and EUDNN/RBM with the same benchmark, then the selected results are used to do the rank-sum test. 7 In this paper, the statistical results biases the results generated by the statistical significance toolkit, i.e., the Mann-Whitney-Wilcoxon rank-sum test <ref type="bibr" target="#b66">[67]</ref> with a 5% significant level.</p><p>(obtained by the EUDNN/RBM) is worse than that of the CAE and the SAE, while the EUDNN/RBM and CAE have the same results statistically. In addition, the best results of the proposed algorithm upon the MNIST-rot-back-image (obtained by the EUDNN/AE) and the Rectangles (obtained by the EUDNN/RBM) benchmarks are all statistically equivalent to that of the DBN, while the best mean values upon these two benchmarks are obtained by the EUDNN/AE and the EUDNN/RBM, respectively. Note here that the MNIST is a widely used classification benchmark for quantifying the performance of deep learning models, and the best results are frequently obtained by supervised models, which require sufficient labeled training data during their training phases. To our best knowledge, the CCR with 98.85% obtained by the proposed algorithm (EUDNN/RBM), which is an unsupervised approach is a very promising result among unsupervised deep learning models. In summary, the proposed algorithm totally wins 34 times over the 40 comparisons against the selected peer competitors, which reveals the superior performance of the proposed algorithm in learning meaningful representations with unsupervised neural network models.</p><p>2) Performance Analysis Regarding the First Stage Since we have claimed that the first stage of the proposed algorithm helps the unsupervised NN-based models learn optimal architectures and better-initialized parameter values, componentwise experiments over the optimal architectures and the initialized parameter values should be performed to investigate their respective effects to justify our designs. However, the initialized parameter values are dependent on the architectures. This leads to the specific experiment by varying only the architecture configurations on investigating how the learned architectures solely affect the performance is difficult to design. Hence, the performance regarding the initialized parameter values is mainly investigated here.</p><p>To this end, we first record the architecture configurations (see Table <ref type="table" target="#tab_1">III</ref>) with which the proposed algorithm presents the promising performance in best mean values of EUDNN/AE and EUDNN/RBM upon each benchmark from Table <ref type="table" target="#tab_1">II</ref>. Then experiments are re-performed by peer competitors with the recorded architecture configurations and randomly initialized parameter values. Finally, the learned representations are fed to the considered performance metric to measure whether these representations are meaningful. Specifically, experimental results are depicted in Fig. <ref type="figure" target="#fig_11">9</ref> in which the vertical axis denote the CCR while A-J in the horizontal axis represent the benchmarks MNIST, MNIST-basic, MNIST-rot, MNISTback-rand, MNIST-back-image, MNIST-rot-back-image, Rectangles, Rectangles-image, Convex, and Cifar10-bw, respectively.</p><p>It is shown in Fig. <ref type="figure" target="#fig_11">9</ref> that most of the peer competitors employing the chosen architecture configurations listed in Table <ref type="table" target="#tab_1">III</ref> obtain worse CCR upon the considered benchmarks compared to the proposed algorithm. Specifically, the proposed algorithm shows these best CCR upon MNIST, MNISTrot, MNIST-back-image, MNIST-rot-back-image, Convex, and Cifar10-bw benchmarks, which is consistent with the findings listed in Table <ref type="table" target="#tab_1">II</ref>. In addition, the proposed algorithm wins the best CCR upon MNIST-basic and MNIST-back-rand benchmarks as well, with these architecture configurations. In addition to the proposed algorithm in which the initialized parameter values are set by the proposed evolutionary approach, all the results illustrated in Fig. <ref type="figure" target="#fig_11">9</ref> are obtained by the compared algorithms with the same architecture configurations and commonly used parameter initializing methods for the second stage. As we all know that the performance of local search strategies is strongly rely on their starting position, therefore, it is reasonable to conclude that the evolutionary scheme employed by the first stage of the proposed algorithm has substantially helped the learned representations to be meaningful.  strategy (i.e., the results obtained by the proposed algorithm during the first stage). Finally, these results are illustrated in Fig. <ref type="figure" target="#fig_12">10</ref> for quantitative comparisons. Specifically in Fig. <ref type="figure" target="#fig_12">10</ref> the vertical axis denotes the CCR, while A-J in the horizontal axis represent the benchmarks MNIST, MNIST-basic, MNISTrot, MNIST-back-rand, MNIST-back-image, MNIST-rot-backimage, Rectangles, Rectangles-image, Convex, and Cifar10bw, respectively, and the bars in blue denote the results obtained by the proposed algorithm without the second stage, while the bars in red refer to that with the second stage.</p><p>It is clearly shown in Fig. <ref type="figure" target="#fig_12">10</ref> that the performance has been improved with the second stage of the proposed EUDNN over all the considered benchmarks compared to the algorithm that only the first stage is employed. Especially, the CCR have been significantly improved by about 20% upon the MNIST-rot, MNIST-back-rand, MNIST-back-image, MNISTrot-back-image, and Cifar10-bw benchmarks and 12.83% on the MNIST benchmark. In summary, it is concluded from these experimental results that the local search strategy utilized in the second stage helps the performance of the proposed algorithm to be improved much further, which promotes the  learned representations to be meaningful and satisfies our of this design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Visualizations of Learned Representations</head><p>In Subsection IV-E, a series of quantitative experiments has been given to highlight the performance of the proposed algorithm in learning meaningful representations with unsupervised deep NN-based models. Here, a qualitative experiment is provided for comprehensively understanding what the representations are learned from the proposed algorithm via visualizations, which is a common approach employed by related works <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref> to intuitively investigate the learning representations. For this purpose, the activation maximization approach <ref type="bibr" target="#b67">[68]</ref> is utilized to visualize the learned representations of the proposed algorithm over MNIST dataset and a number of 100 randomly selected visualizations of the patches are illustrated 8 in Fig. <ref type="figure" target="#fig_13">11</ref>. Furthermore, the SGD is employed during the optimization of the activation maximization with 10, 000 iterations and a fixed learning rate of 0.1. To be specific, Fig. <ref type="figure" target="#fig_13">11a</ref> shows the learned representations on depth 1 in which the visualization is commonly describable <ref type="bibr" target="#b67">[68]</ref>. It is clear in Fig. <ref type="figure" target="#fig_13">11a</ref> that some strokes are learned in most patches and a part of the representations is similar to that of the RBM <ref type="bibr" target="#b67">[68]</ref>, which can be viewed as the effectiveness of the proposed algorithm, because these similar representations over MNIST dataset have been reported in multiple kinds of literature <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. The visualizations of the representations on depths 2 and 3 are depicted in Figs. <ref type="figure" target="#fig_13">11b</ref> and<ref type="figure" target="#fig_13">11c</ref>, respectively. However, these representations are difficult to understand intuitively and be interpretable due to the high-level hierarchical nature <ref type="bibr" target="#b67">[68]</ref>. But it still can be concluded that the proposed algorithm has learned the meaningful representations by comparing them to the experiments simulated in <ref type="bibr" target="#b67">[68]</ref> that learned representations herein resemble those of the DAE to some extent. Noting that multiple learned features shown in Fig. <ref type="figure" target="#fig_13">11a</ref> seem to be random. The reason is that not all the neurons in the corresponding hidden layer have learned the meaningful features. Specifically, the visualization of features is from the 100 neurons randomly selected from the 8 Because visualizations of representations learned from the depth larger than one are difficult to interpret, and that from the depth larger than three have no reference for comparisons, only representations with depths 1, 2, and 3 are visualized here.</p><p>313,600 (this number can be calculated from Table <ref type="table" target="#tab_1">III</ref>), and it is not necessary that all the 313,600 neurons have learned the meaningful features. In summary, these visualizations give a qualitative observation to highlight that the meaningful representations have been effectively learned by the proposed algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In order to warrant the representations learned by unsupervised deep neural networks to be meaningful, the existing approaches for learning them need optimal combinations of hyper-parameters, appropriate parameter values, and sufficient labeled data as the training data. These approaches generally employ the exhaustive grid search method to directly optimize hyper-parameters due to their unavailable gradient information, which give an unaffordable computational complexity that increases with an order of magnitude as the number of hyperparameter grows. Furthermore, the gradient-based training algorithms in these existing algorithms are easy to be trapped into the local minima, which cannot guarantee them the best performance. In addition, in the current era of Big Data, the volume of labeled data is limited and obtaining sufficient data with labels is expensive, if not impossible. To address these concerning issues, we have proposed an evolving unsupervised deep neural networks method which heuristically searches for the best hyper-parameter settings and the global minima to learn the meaningful representations without sufficient labeled data. To be specific, two stages are composed in the proposed algorithm. In the first stage, all the information regarding hyper-parameter and parameter settings are encoded into the individual chromosome and the best one is selected when they go through a series of crossover, mutation, and selection operations. Furthermore, the activation functions that provide the nonlinear ability to the learning algorithm are also incorporated into the individual chromosome to go through the evolutions of obtaining the promising performance. In addition, the orthogonal complementary techniques are employed in the proposed algorithm to reduce the computational complexity for effectively learning the deep representations. Specifically, only a limited number of labeled data is needed in the proposed algorithm to direct the search to learn representations with meaningfulness. For further improving the performance, the second stage is introduced with a local search strategy to complement with the ability of the exploitation search for training the proposed algorithm with the architecture and the activation function optimized in the first stage. These two stages collectively promote the proposed algorithm effectively learning the meaningful representations with unsupervised deep neural network-based models. To evaluate the meaningfulness of the learned representations, a series of experiments are given against peer competitors over multiple benchmarks related classification tasks. The results measured by the softmax regression show the considerable competitiveness of the proposed algorithm in learning meaningful representations. In near future, we will place more focus on the efficient encoding methods as well as the way measuring the quality of the representation during the evolution of a larger scale and higher dimensional data. In addition, we would also investigate how to effectively evolve deep supervised neural networks, such as CNNs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. An example to illustrate a general flowchart of deep representation learning and its relationship to machine learning tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The training process of unsupervised deep neural networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .Algorithm 1 :</head><label>41</label><figDesc>Fig.4. The flowchart of the proposed algorithm that is composed of two distinct stages. Especially, the first stage is for finding optimal architectures as well as desirable initializations of the connection weight parameter values. The second stage is to fine-tune them for a potentially better performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Algorithm 2 : 4 Q 5 Q 6 S 7 P</head><label>24567</label><figDesc>Obtain the Optimal Connection Weight and Activation Function Input: Input data; size of population m; probability of crossover ρ; probability of mutation µ. Output: Optimal connection weight W ; activation function f (•). 1 Initialize the population P with the size m; 2 while stopping criteria are not satisfied do 3 Evaluate the fitness of individuals in P ; ← Generate new offspring with the probability ρ from two parents selected with binary tournament selection; ← Mutate all the individuals in Q with the probability µ; ← Select the individual with the best fitness from P ∪ Q; ← S ∪ Select (m -1) individuals from (P ∪ Q) \ S with binary tournament selection; 8 end 9 Evaluate the fitness of the individuals in P ; 10 ind best ← Select the individual with the best fitness from P ; 11 Return W and f (•) represented by ind best .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Fig.5. A flowchart describes the process of encoding the potential connection weight and activation function. First, a set of basis vectors S is given in the original space with n-dimension. Then, a set of coefficients b is generated to represent the vector a 1 by linear combining the basis vectors. Then, the orthogonal complements {a 2 , • • • , an} of a 1 are computed. Finally, all the information of computing a 1 , indicating whether the basis from {a 2 , • • • , an} is selected, and the activation functions are encoded into the chromosomes that are used to evolve to obtain the optimal connection weight and activation function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>First, a set of basis S = [s 1 , • • • , s n ] which can span a n-dimensional space is given, e.g., any n linear independent n-dimensional vectors. Then the vector a 1 is linearly combined by the bases in S with the coefficients b = [b 1 , • • • , b n ] that are randomly specified. Next, the orthogonal complements {a 2 , • • • , a n } of a 1 are computed by</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. The flowchart of the second stage in the proposed EUDNN. Especially, the predicted label is computed with the connection weights and activation functions for the input data. Then the loss of the classifier is formulated between the predicted label and the true label. Next, the error is back propagated and the parameter values of the connection weights are updated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Correct classification rates of training data and test data as training process continues.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. A group of digit samples (0 -9) from the MNIST benchmark test dataset.Briefly, these benchmark test datasets are categorized into three different classes based on the object types that they intend to recognize. The first one is about the hand-written digits and covers the MNIST, MNIST-basic, MNIST-rot, MNISTback-rand, MNIST-back-image, and MNIST-rot-back-image benchmarks. Examples from the MNIST benchmark are depicted in Fig.8for reference. The second one is to classify the geometries and the rectangles, such as the Rectangles, Rectangles-image, and the Convex benchmarks. The last one is to identify the natural objects in Cifar10-bw. Different variants in MNIST and rectangles datasets present the algorithms dissimilar difficulties from the aspects of perturbations, the small number of training dataset, and the large testing dataset size. Furthermore, the dimensions, number of classes, and the sizes of training set and test set of the chosen benchmark datasets are shown in TableI.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. The performance of the proposed algorithm against DAE, CAE, SAE, and DBN with the configurations on which the proposed algorithm obtains the best correct classification rates over benchmarks measured by softmax regression. Especially, A-J denote the benchmarks MNIST, MNIST-basic, MNIST-rot, MNIST-back-rand, MNIST-back-image, MNIST-rot-back-image, Rectangles, Rectangles-image, Convex, and Cifar10-bw, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Correct classification rate (CCR) comparisons of the proposed algorithm without (denoted by blue bars) and with (denoted by red bars) the second stage upon the MNIST, MNIST-basic, MNIST-rot, MNIST-back-rand, MNIST-back-image, MNIST-rot-back-image, Rectangles, Rectangles-image, Convex, and Cifar10-bw benchmarks, which are denoted by A-J, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 11 .</head><label>11</label><figDesc>Fig.11. Visualizations of the proposed algorithm over MNIST dataset with depths 1 (Fig.11a), 2 (Fig.11b), and 3 (Fig.11c) by activation maximization method.</figDesc><graphic coords="14,96.62,66.30,107.69,107.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I THE</head><label>I</label><figDesc>CONFIGURATIONS OF THE CHOSEN BENCHMARK DATASETS.</figDesc><table><row><cell>Benchmark</cell><cell>Dimension</cell><cell># of class</cell><cell>Size of training set</cell><cell>test set</cell></row><row><cell>MNIST</cell><cell>28 × 28</cell><cell>10</cell><cell>50,000</cell><cell>10,000</cell></row><row><cell>MNIST-basic</cell><cell>28 × 28</cell><cell>10</cell><cell>12,000</cell><cell>50,000</cell></row><row><cell>MNIST-rot</cell><cell>28 × 28</cell><cell>10</cell><cell>12,000</cell><cell>50,000</cell></row><row><cell>MNIST-back-rand</cell><cell>28 × 28</cell><cell>10</cell><cell>12,000</cell><cell>50,000</cell></row><row><cell>MNIST-back-image</cell><cell>28 × 28</cell><cell>10</cell><cell>12,000</cell><cell>50,000</cell></row><row><cell>MNIST-rot-back-image</cell><cell>28 × 28</cell><cell>10</cell><cell>12,000</cell><cell>50,000</cell></row><row><cell>Rectangles</cell><cell>28 × 28</cell><cell>2</cell><cell>1,200</cell><cell>50,000</cell></row><row><cell>Rectangles-image</cell><cell>28 × 28</cell><cell>2</cell><cell>12,000</cell><cell>50,000</cell></row><row><cell>Convex</cell><cell>28 × 28</cell><cell>2</cell><cell>8,000</cell><cell>50,000</cell></row><row><cell>Cifar10-bw</cell><cell>32 × 32</cell><cell>10</cell><cell>50,000</cell><cell>10,000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II THE</head><label>II</label><figDesc>CORRECT CLASSIFICATION RATE OF THE PROPOSED EUDNN (EUDNN/AE AND EUDNN/RBM) UPON MNIST, MNIST-BASIC, MNIST-ROT, MNIST-BACK-RAND, MNIST-BACK-IMAGE, MNIST-ROT-BACK-IMAGE, RECTANGLES, RECTANGLES-IMAGE, CONVEX, AND CIFAR10-BW BENCHMARKS AGAINST STACKED DENOISING AUTO-ENCODER (DAE), STACKED CONTRACTIVE AUTO-ENCODER (CAE), STACKED SPARSE AUTO-ENCODER (SAE), AND THE DEEP BELIEF NETWORK (DBN). BEST MEAN VALUES ARE HIGHLIGHTED IN BOLDFACE. THE SYMBOLS "+," "-," AND "=" DENOTE WHETHER THE PROPOSED ALGORITHM STATISTICALLY ARE BETTER THAN, WORSE THAN, AND EQUAL TO THAT OF THE CORRESPONDING PEER COMPETITORS, RESPECTIVELY, WITH THE EMPLOYED RANK-SUM TEST.</figDesc><table><row><cell>Benchmark</cell><cell>AE</cell><cell cols="2">EUDNN</cell><cell>RBM</cell><cell>DAE</cell><cell>CAE</cell><cell>SAE</cell><cell>DBN</cell></row><row><cell>MNIST</cell><cell cols="2">0.9878(0.00751)</cell><cell cols="2">0.9885(0.00255)</cell><cell>0.9820(0.00506)(+)</cell><cell>0.9843(0.00699)(+)</cell><cell>0.9832(0.00891)(+)</cell><cell>0.9771(0.00959)(+)</cell></row><row><cell>MNIST-basic</cell><cell cols="2">0.9674(0.00616)</cell><cell cols="2">0.9633(0.00473)</cell><cell>0.9580(0.00352)(+)</cell><cell>0.9635(0.00831)(+)</cell><cell>0.9776(0.00585)(-)</cell><cell>0.9658(0.00550)(+)</cell></row><row><cell>MNIST-rot</cell><cell cols="2">0.7952(0.00917)</cell><cell cols="2">0.7549(0.00286)</cell><cell>0.7274(0.00757)(+)</cell><cell>0.7706(0.00754)(+)</cell><cell>0.7852(0.00380)(+)</cell><cell>0.7639(0.00568)(+)</cell></row><row><cell>MNIST-back-rand</cell><cell cols="2">0.8843(0.00076)</cell><cell cols="2">0.8386(0.00054)</cell><cell>0.7725(0.00531)(+)</cell><cell>0.5741(0.00779)(+)</cell><cell>0.8851(0.00934)(=)</cell><cell>0.8221(0.00130)(+)</cell></row><row><cell>MNIST-back-image</cell><cell cols="2">0.4325(0.00569)</cell><cell cols="2">0.4830(0.00469)</cell><cell>0.4022(0.00012)(+)</cell><cell>0.4010(0.00337(+)</cell><cell>0.4638(0.00162)(+)</cell><cell>0.4587(0.00794)(+)</cell></row><row><cell>MNIST-rot-back-image</cell><cell cols="2">0.8925(0.00906)</cell><cell cols="2">0.8879(0.00815)</cell><cell>0.8691(0.00127)(+)</cell><cell>0.6574(0.00913)(+)</cell><cell>0.8733(0.00632)(+)</cell><cell>0.8830(0.00098)(=)</cell></row><row><cell>Rectangles</cell><cell cols="2">0.9627(0.00311)</cell><cell cols="2">0.9681(0.00829)</cell><cell>0.9232(0.00166)(+)</cell><cell>0.6275(0.00602)(+)</cell><cell>0.9408(0.00263)(+)</cell><cell>0.9622(0.00154)(=)</cell></row><row><cell>Rectangles-image</cell><cell cols="2">0.7521(0.00689)</cell><cell cols="2">0.7716(0.00048)</cell><cell>0.7598(0.00451)(+)</cell><cell>0.7810(0.00784)(=)</cell><cell>0.7725(0.00002)(-)</cell><cell>0.7628(0.00913)(+)</cell></row><row><cell>Convex</cell><cell cols="2">0.8113(0.00052)</cell><cell cols="2">0.8085(0.00826)</cell><cell>0.7930(0.00538(+)</cell><cell>0.8016(0.00996)(+)</cell><cell>0.8053(0.00878)(+)</cell><cell>0.7895(0.00443)(+)</cell></row><row><cell>Cifar10-bw</cell><cell cols="2">0.4798(0.00107)</cell><cell cols="2">0.4331(0.00962)</cell><cell>0.4309(0.00005)(+)</cell><cell>0.4860(0.00775)(+)</cell><cell>0.4423(0.00817)(+)</cell><cell>0.4598(0.00869)(+)</cell></row><row><cell></cell><cell></cell><cell cols="2">+/-/=</cell><cell></cell><cell>10/0/0</cell><cell>9/0/1</cell><cell>7/2/1</cell><cell>8/0/2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Performance Analysis Regarding the Second StageIn this experiment, we mainly investigate whether the local search strategy employed in the second stage promotes the integral performance of the proposed algorithm compared to only the evolutionary methods used in the first stage. For this purpose, we first pick up the promising CCR obtained by the proposed algorithm from Table II in which the results of the proposed algorithm are collectively achieved by the evolutionary method employed in the first stage and the local search strategy employed in the second stage. Then we select the corresponding results performed without the local search</figDesc><table><row><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.5</cell><cell cols="2">EUDNN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.4</cell><cell cols="2">DAE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">CAE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.3</cell><cell>SAE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">DBN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>TABLE III THE BEST CORRECT CLASSIFICATION RATE (CCR) OF THE PROPOSED ALGORITHM UPON MNIST, MNIST-BASIC, MNIST-ROT, MNIST-BACK-RAND, MNIST-BACK-IMAGE, MNIST-ROT-BACK-IMAGE, RECTANGLES, RECTANGLES-IMAGE, CONVEX, CIFAR10-BW BENCHMARKS AND THE CORRESPONDING ARCHITECTURE CONFIGURATIONS. Benchmark Best CCR Architecture configurations MNIST 0.9885 784, 400, 202, 106, 88 MNIST-basic 0.9674 784, 400, 211, 120 MNIST-rot 0.7952 784, 400, 233, 133, 100, 81 MNIST-back-rand 0.8843 784, 397, 202, 123 MNIST-back-image 0.4830 784, 386, 191, 1088, 100 MNIST-rot-back-image 0.8925 784, 378, 205, 106 Rectangles 0.9681 784, 397, 205, 113, 100, 75 Rectangles-image 0.7716 784, 402, 214, 122, 89 Convex 0.8113 784, 394, 200, 110, 55, 49 Cifar10-bw 0.4798 1024, 502, 253, 141, 130 3) A 0.2</cell><cell>B</cell><cell>C</cell><cell>D</cell><cell>E</cell><cell>F</cell><cell>G</cell><cell>H</cell><cell>I</cell><cell>J</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>1089-778X (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TEVC.2018.2808689, IEEE Transactions on Evolutionary Computation</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>13</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Without the second stage</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">With the second stage</cell></row><row><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell>A</cell><cell>B</cell><cell>C</cell><cell>D</cell><cell>E</cell><cell>F</cell><cell>G</cell><cell>H</cell><cell>I</cell><cell>J</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Even data is abundant in the Big Data era, most raw data collected is unlabeled for a classification task, e.g., the ImageNet classification benchmark that has been discussed in Section I.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Bias terms, which are another kind of connection weights widely existing in NNs, are incorporated into W and W ′ here for simplicity.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>The works in<ref type="bibr" target="#b35">[36]</ref>-<ref type="bibr" target="#b41">[42]</ref> were proposed two decades ago and cannot be applied for deep NNs, the work in<ref type="bibr" target="#b47">[48]</ref> concerned only the weight pruning, and the work in<ref type="bibr" target="#b48">[49]</ref> employed a direct way for evolving and did not have a general meaning.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>Theoretically, multiple solutions could be found in computing the bases of the null space. In practice, we only accept the orthonormal basis for the corresponding null space obtained from the singular value decomposition.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>Generally, the VC dimension can be viewed as an indicator measuring the complexity of multiple models which are capable of solving one particular task<ref type="bibr" target="#b57">[58]</ref>. The smaller the VC dimension, the more simplicity is the corresponding model, and a more simplicity model is with better generalization<ref type="bibr" target="#b58">[59]</ref>. Commonly, a large number and magnitude of elements in the transformation matrixes are positive to the VC dimension.</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work is supported in part by the China Scholarship Council under Grant 201506240048; in part by the Miaozi Project in Science and Technology Innovation Program of Sichuan Province under Grant 16-YCG061, China; in part by the National Natural Science Foundation of China for Distinguished Young Scholar under Grant 61622504; and in part by the National Natural Science Foundation of China under Grant 61432012 and Grant U1435213.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning hierarchical features for scene labeling</title>
		<author>
			<persName><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lecun</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1915" to="1929" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1799" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Shallow vs. deep sum-product networks</title>
		<author>
			<persName><forename type="first">O</forename><surname>Delalleau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="666" to="674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Receptive fields, binocular interaction and functional architecture in the cat&apos;s visual cortex</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Hubel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Wiesel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of physiology</title>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="106" to="154" />
			<date type="published" when="1962">1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning deep architectures for ai</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Foundations and Trends in Machine Learning</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning a good representation with unsymmetrical auto-encoder</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Computing and Applications</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Explicit guiding auto-encoders for learning meaningful representation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Computing and Applications</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Modeling</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Two problems with backpropagation and other steepestdescent learning procedures for networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 8th annual conf. cognitive science society</title>
		<meeting>8th annual conf. cognitive science society</meeting>
		<imprint>
			<date type="published" when="1986">1986</date>
			<biblScope unit="page" from="823" to="831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07">Jul. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">"</forename><surname>Rmsprop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">COURSERA: Lecture</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adadelta: an adaptive learning rate method</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Algorithms for hyper-parameter optimization</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bardenet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kégl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2546" to="2554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML (1)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="115" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fitting segmented regression models by grid search</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Statistics</title>
		<imprint>
			<biblScope unit="page" from="77" to="84" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Scaling learning algorithms towards ai</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Large-scale kernel machines</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Big data analytics: a survey</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Vasilakos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Big Data</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">21</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009">2009. 2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Auto-association by multilayer perceptrons and singular value decomposition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bourlard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kamp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Cybernetics</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">4-5</biblScope>
			<biblScope unit="page" from="291" to="294" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Autoencoders, minimum description length, and helmholtz free energy</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="3" to="3" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sparse coding with an overcomplete basis set: A strategy employed by v1?</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Olshausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Field</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="3311" to="3325" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sparse deep belief net model for visual area v2</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ekanadham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="873" to="880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Greedy layer-wise training of deep networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">153</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning</title>
		<meeting>the 25th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1096" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Contractive auto-encoders: Explicit invariance during feature extraction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rifai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning</title>
		<meeting>the 28th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="833" to="840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Evolving artificial neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1423" to="1447" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Genetic algorithms and neural networks: Optimizing connections and connectivity</title>
		<author>
			<persName><forename type="first">D</forename><surname>Whitley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Starkweather</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bogart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Parallel Computing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="347" to="361" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The genitor algorithm and selection pressure: Why rankbased allocation of reproductive trials is best</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Whitley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICGA</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="116" to="123" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Training feedforward neural networks using genetic algorithms</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Montana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="762" to="767" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The cascade-correlation learning architecture</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Christian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lebiere</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 2. Citeseer</title>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The upstart algorithm: A method for constructing and training feedforward neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Frean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="198" to="209" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Creating artificial neural networks that generalize</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sietsma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Dow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="67" to="79" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Improvement of real-valued genetic algorithm and performance study [j]</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zi-Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Electronica Sinica</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Evolving neural networks through augmenting topologies</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">O</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Miikkulainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="127" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Compositional pattern producing networks: A novel abstraction of development</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">O</forename><surname>Stanley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genetic Programming and Evolvable Machines</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="131" to="162" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A multiobjective sparse feature learning model for deep neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3263" to="3277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Differential evolution a simple and efficient heuristic for global optimization over continuous spaces</title>
		<author>
			<persName><forename type="first">R</forename><surname>Storn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Price</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Global Optimization</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="341" to="359" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Structure learning for deep neural networks based on multiobjective optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Large-scale evolution of image classifiers</title>
		<author>
			<persName><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Selle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L</forename><surname>Suematsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01041</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Information processing in dynamical systems: Foundations of harmony theory</title>
		<author>
			<persName><forename type="first">P</forename><surname>Smolensky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DTIC Document, Tech. Rep</title>
		<imprint>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Exploiting regularity without development</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">O</forename><surname>Stanley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Fall Symposium on Developmental Systems</title>
		<meeting>the AAAI Fall Symposium on Developmental Systems<address><addrLine>Menlo Park, CA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page">37</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Kpca plus lda: a complete kernel fisher discriminant framework for feature extraction and recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>-Y. Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="230" to="244" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Aistats</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">106</biblScope>
			<biblScope unit="page">275</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Support-vector networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="273" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A direct evolutionary feature extraction algorithm for classifying high dimensional data</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Conference on Artificial Intelligence</title>
		<meeting>the National Conference on Artificial Intelligence<address><addrLine>Menlo Park, CA; Cambridge, MA; London</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press; MIT Press</publisher>
			<date type="published" when="1999">1999. 2006</date>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">561</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">The nature of statistical learning theory</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1564" to="1564" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Statistical learning theory</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">An empirical evaluation of deep architectures on problems with many factors of variation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on Machine learning</title>
		<meeting>the 24th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="473" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Polytomous logistic regression</title>
		<author>
			<persName><forename type="first">J</forename><surname>Engel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistica Neerlandica</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="233" to="252" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">A practical guide to training restricted boltzmann machines</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Momentum</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">926</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Principles and procedures of statistics a biometrical approach</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Steel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dickey</surname></persName>
		</author>
		<idno>no. 519.5 S813</idno>
		<imprint>
			<date type="published" when="1997">1997. 1997</date>
			<publisher>WCB/McGraw-Hill</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">On contrastive divergence learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Carreira-Perpinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AISTATS</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="33" to="40" />
			<date type="published" when="2005">2005</date>
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Aistats</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="256" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Multi-objective optimization using evolutionary algorithms</title>
		<author>
			<persName><forename type="first">K</forename><surname>Deb</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>John Wiley &amp; Sons</publisher>
			<biblScope unit="volume">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">he is a jointly Ph.D. student financed by the China Scholarship Council in the School of Electrical and Computer Engineering, Oklahoma State University (OSU), USA. He is currently a</title>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<idno>2015.08-2017.02</idno>
		<imprint>
			<date type="published" when="2009">2009. 2017</date>
			<biblScope unit="volume">1341</biblScope>
			<pubPlace>Chengdu, China; New Zealand</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Montreal ; Postdoc Research Fellow in the School of Engineering and Computer Science, Victoria University of Wellington, Wellington</orgName>
		</respStmt>
	</monogr>
	<note>Yanan Sun (S&apos;15-M&apos;18) received a Ph.D. degree in engineering from the Sichuan University. His research topics are manyobjective optimization and deep learning</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">His research interest includes intelligent control, computational intelligence, conditional health monitoring, signal processing and their industrial/defense applications. Dr. Yen was an associate editor of the IEEE Control Systems Magazine</title>
		<author>
			<persName><forename type="first">G</forename><surname>Gary</surname></persName>
		</author>
		<author>
			<persName><surname>Yen</surname></persName>
		</author>
		<idno>S&apos;87-M&apos;88-SM&apos;97-F&apos;09</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Control Systems Technology, Automatica, Mechantronics, IEEE Transactions on Systems, Man and Cybernetics, Parts A and B and IEEE Transactions on Neural Networks. He is currently serving as an associate editor for the IEEE Transactions on Evolutionary Computation and the IEEE Transactions on Cybernetics. He served as the General Chair for the 2003 IEEE International Symposium on Intelligent Control</title>
		<title level="s">and Subspace Learning of Neural Networks</title>
		<editor>
			<persName><forename type="first">Canada</forename><surname>Vancouver</surname></persName>
		</editor>
		<editor>
			<persName><surname>Dr</surname></persName>
		</editor>
		<meeting><address><addrLine>Houston, TX; Beijing, China; Chengdu, China</addrLine></address></meeting>
		<imprint>
			<publisher>CRC Press</publisher>
			<date type="published" when="1992">1992. 2006. 2006-2009. 1994. 2004. 2007. 2010. 2009 2012. 2014</date>
		</imprint>
		<respStmt>
			<orgName>University of Notre Dame ; Electrical and Computer Engineering, Oklahoma State University (OSU) ; U.S. Air Force Research Laboratory in Albuquerque ; College of Computer Science, Sichuan University</orgName>
		</respStmt>
	</monogr>
	<note>His current research interests include Neural Networks and Big Data. He is a fellow of IEEE</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
