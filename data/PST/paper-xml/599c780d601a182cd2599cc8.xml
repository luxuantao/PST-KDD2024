<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Underwater Image Enhancement via Extended Multi-Scale Retinex</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shu</forename><surname>Zhang</surname></persName>
							<email>shu.zhang@port.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">Ocean University of China</orgName>
								<address>
									<addrLine>238 Songling Rd</addrLine>
									<postCode>266100</postCode>
									<settlement>Qingdao</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">University of Portsmouth</orgName>
								<address>
									<addrLine>Eldon Building, Winston Churchill Ave</addrLine>
									<postCode>PO1 2DJ</postCode>
									<settlement>Portsmouth</settlement>
									<region>Hampshire, UK</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ting</forename><surname>Wang</surname></persName>
							<email>qdwangting@ouc.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Shandong University of Science &amp; Technology</orgName>
								<address>
									<addrLine>579 Qianwangang Rd</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Huangdao Distr</orgName>
								<address>
									<postCode>266590</postCode>
									<settlement>Qingdao</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junyu</forename><surname>Dong</surname></persName>
							<email>dongjunyu@ouc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Ocean University of China</orgName>
								<address>
									<addrLine>238 Songling Rd</addrLine>
									<postCode>266100</postCode>
									<settlement>Qingdao</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hui</forename><surname>Yu</surname></persName>
							<email>hui.yu@port.ac.uk</email>
							<affiliation key="aff3">
								<orgName type="institution">University of Portsmouth</orgName>
								<address>
									<addrLine>Eldon Building, Winston Churchill Ave</addrLine>
									<postCode>PO1 2DJ</postCode>
									<settlement>Portsmouth</settlement>
									<region>Hampshire, UK</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Underwater Image Enhancement via Extended Multi-Scale Retinex</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2D62C5C865B8682880D0DC83CAA99212</idno>
					<idno type="DOI">10.1016/j.neucom.2017.03.029</idno>
					<note type="submission">Received date: 17 January 2017 Revised date: 3 March 2017 Accepted date: 9 March 2017</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>underwater image</term>
					<term>degradation</term>
					<term>enhancement</term>
					<term>color constancy</term>
					<term>multi-scale retinex</term>
					<term>hybrid filter</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Underwater exploration has become an active research area over the past few decades. The image enhancement is one of the challenges for those computer vision based underwater researches because of the degradation of the images in the underwater environment. The scattering and absorption are the main causes in the underwater environment to make the images decrease their visibility, for example, blurry, low contrast, and reducing visual ranges. To tackle aforementioned problems, this paper presents a novel method for underwater image enhancement inspired by the Retinex framework, which simulates the human visual system. The term Retinex is created by the combinations of "Retina" and "Cortex". The proposed method, namely LAB-MSR, is achieved by modifying the original Retinex algorithm. It utilizes the combination of the Bilateral Filter and Trilateral Filter on the three channels of the image in CIELAB color space according to the characteristics of each channel. With real world data, experiments are carried out to demonstrate both the degradation characteristics of the underwater images in different turbidities, and the competitive performance of the proposed method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Underwater exploration has become more active in recent years with the increasing application demands, such as the studies of the marine species <ref type="bibr" target="#b0">[1]</ref>, wreckage exploration, inspection of the underwater cables and pipelines, underwater scene analysis, search and rescue, and mapping of the offshore seabed among others. There is also a strong interest in applying computer vision based algorithms to these research works. However, due to the scattering and absorption effect in the underwater environment, visual images often suffer from low visibilities, such as low contrast, blur, and color variations <ref type="bibr" target="#b1">[2]</ref>. This is a real challenge for computer vision based underwater tasks, which require detailed information about the image for further operations, such as feature extraction.</p><p>The light rays suffer from the scattering and absorption effects in the underwater environment. The scattering effect is brought by the suspended particles in the water that reflect the light rays into other directions, which makes the image blurry. The absorption is caused by the medium of water that degrades the energies of light rays according to their wavelengths, which makes the image visually losing its contrast and reduce the visible ranges. The absorption depends on the density and turbidity of the water. Studies have been conducted into this field, but most of them require a dedicated underwater imaging device. Although there are some computer vision based post-processing methods for underwater image enhancement, the performances are still limited. To tackle aforementioned problems, this paper presents a novel method that can process the underwater image to make it less affected by the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T 2 underwater environment. The processed image has a clearer visibility and a higher dynamic range. The visual range is also improved. These factors are important for other computer vision tasks, which require observing underwater scenes. Two sets of the experiments included in this paper demonstrate the competitive performance of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Many methods have been proposed during recent years to de-noise and enhance the underwater images. These methods include the ones based on the deliberately designed hardware and the ones that are built on the computer vision algorithms. Schechner et al. <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> designed a dedicated hardware for underwater image enhancement. It utilized a polarized filter for the lens of the visual imaging device in the underwater environment. The scattering components in the medium of water are mostly associated with partial polarization of light. Since the main illumination in the ocean is the sun from above, a carefully placed polarized filter with a certain angle can remove most of the scattering components before they reach the lens. However, their method required to be prepared before the underwater images being taken, which leads to some limitations in certain applications. Iqbal et al. <ref type="bibr" target="#b4">[5]</ref> proposed an Unsupervised Color Correction Method (UCM) to deal with the reduced contrast and non-uniform color cast in the underwater images brought by the scattering and absorption effects underwater. They utilized two sets of different histogram stretching strategies for Red color channel and Blue color channel respectively according to the characters of the underwater environment. And the HSI color model was also employed at the end of the process for color correction. However, their method was based upon the assumption that the color cast in the underwater environment was only dominated by a specific color spectrum, which was blue. They deliberately compressed the blue channel of the image, which might damage the perceptions of the original blue components in some scenes with different color casts.</p><p>The method presented in this paper is inspired by the framework of the Retinex <ref type="bibr" target="#b5">[6]</ref>. The Retinex theory simulates the mechanism of the human vision system that perceives the world. The term of Retinex is created by the combination of the "retina" and "cortex". It attempts to achieve the color constancy when the scene is dominated by a certain illumination, which has a similar situation in the underwater environment. In the human visual neural system, the color constancy is fulfilled by both retinal and cortical mechanisms to discount changes in illuminations <ref type="bibr" target="#b6">[7]</ref>. Some researchers believe that the local contrasts between adjacent cones contribute significantly to the color constancy <ref type="bibr" target="#b7">[8]</ref>. Similarly, the traditional Retinex applies convolution process on local windows using Gaussian Filter to approximate the illumination component. The methods based on Retinex have been applied to the areas such as color image enhancement <ref type="bibr" target="#b8">[9]</ref>, foggy image filtering <ref type="bibr" target="#b10">[10]</ref>, and aero image process among others. Hurlbert <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b12">12]</ref> studied the properties of the retinex framework and other lightness theories. He treated them as a learning problem for artificial neural networks, and found that the solution had a center/surround spatial form <ref type="bibr" target="#b13">[13]</ref>. Wang et al. <ref type="bibr" target="#b14">[14]</ref> employed the Retinex framework on the Y channel of the image in YCbCr color space for image enhancement. Cheolkon et al. <ref type="bibr" target="#b15">[15]</ref> applied Retinex for illumination effects removal for eye detection under varying lighting conditions. There are also some researchers employing the Retinex to enhance the underwater images. Joshi et al. <ref type="bibr" target="#b16">[16]</ref> conducted the experiments with Retinex to enhance the images degraded by weathers, which has the similar properties as the ones degraded by underwater environment. They applied the method of the traditional Multi-Scale Retinex with Color Restoration to the images in the weather conditions such as haze, poor light, rain, smoke, and as well as the underwater scene. In their experiments, the underwater image quality was improved. However, the enhancement was still limited compared to the results for the images that were captured in the air since that the medium of water did more damages to the image visibilities than the mediums in the air did. Alex Raj et al. <ref type="bibr" target="#b17">[17]</ref> presented a Single-Scale Retinex based method for underwater image enhancement. They converted the image to YCbCr color space and used the Gaussian surround function for the process of convolution in the Retinex framework. The experiment results were positive. However, the visibilities of the processed underwater images were still polluted by the medium of water. Fu et al. <ref type="bibr" target="#b18">[18]</ref> also presented a Retinex based enhancement approach for the single underwater image. They transferred the underwater image from RGB color space to LAB color space and applied</p><formula xml:id="formula_0">A C C E P T E D M A N U S C R I P T 3</formula><p>Retinex only on L component. They utilized the Retinex theory to decompose the L channel of the underwater image for reflectance and illumination. However, their method involved 4-6 iterations of the process, and took several seconds to process one image. The method proposed in this paper extends the Retinex framework for underwater image enhancement. The Bilateral Filter and Trilateral Filter are utilized for different color channels of the underwater images to process the pixels according to different constraints. These two filters hold advantages over the classic Gaussian Filter employed in the traditional Retinex when applied on underwater images.</p><p>The rest of the paper is organized as following: Sec. 3 discusses the characters of the underwater image degradation; the proposed method is introduced in Sec. <ref type="bibr" target="#b3">4</ref>; the experiments that demonstrate both the visual degradation of the underwater image and the encouraging performance of the proposed method are shown in Sec. 5; the paper is then concluded in Sec. 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Characteristics of the underwater image degradation and problem formulation</head><p>The visibility of the image is limited in the underwater environment. The colors in the image can be washed out in a distance about 12 meters in clear water, and about 5 meters or less in turbid water <ref type="bibr" target="#b19">[19]</ref>. The higher the turbidity of the water is, the more the underwater images are attenuated. The main causes of this attenuation are the scattering and absorbing properties of the underwater environment. During the imaging processes, the scattering effect changes the directions of the light rays, while absorption effect reduces the energy of the light ray with respect to different wavelengths. The scattering effect is caused by the floating particles in the water that reflect the light rays into other directions. It follows the behaviors that are described by the Point Spread Function (PSF) <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b21">21]</ref>. There are two parts for scattering effect, the forward-scattering and the back-scattering. The forward-scattering reflects the original light rays of the scene into multiple directions, which make the scene seem to be blurry. The back-scattering reflects the ambient light into multiple directions, and parts of them can go directly towards the imaging device. This interferes with the sensations of the original light signals from the scene, and casts the ambient illumination on the whole underwater scene. The underwater irradiance of the light ray can be expressed as following:</p><formula xml:id="formula_1">( ) (0) cr E r E e   (1)</formula><p>where E(0) is the original irradiance of a target point in the water, E(r) is the observed irradiance of the target point with the distance of r from it. And c is the attenuation coefficient of the medium, which is affected by the scattering and absorbing effect of the medium together. Thus, Eq. ( <ref type="formula" target="#formula_8">1</ref>) can be further expressed as below, where a and b are the absorbing coefficient and scattering coefficient respectively.</p><formula xml:id="formula_2">( ) (0) ar br E r E e e  <label>(2)</label></formula><p>Different regions of the underwater image may have different depths, which refer to different r values in Eq. ( <ref type="formula" target="#formula_2">2</ref>). This leads to different levels of degradation in different regions of one underwater image. Moreover, the coefficient b is also spatial sensitive. This is because that the scattering effect is caused by the suspended particles in the water, and there is a high probability that the distribution of these Additionally, the absorption effect in the underwater environment, which refers to the coefficient a in the Eq. ( <ref type="formula" target="#formula_2">2</ref>), varies with respect to the different wavelengths of the light rays. As pointed out by Sahu et al. <ref type="bibr" target="#b22">[22]</ref>, Li et al. <ref type="bibr" target="#b23">[23]</ref> and Li et al. <ref type="bibr" target="#b24">[24]</ref>, the light rays with the wavelength below around 380 nm or above around 620 nm are more likely to be affected by underwater absorption effect. Figure <ref type="figure" target="#fig_1">2</ref> shows the relationship between the absorption coefficient and the wavelength <ref type="bibr" target="#b25">[25]</ref>. Thus, the attenuation of the underwater image varies not only in spatial domain, but also in color range domain.</p><p>As a result, we find out that the underwater image degradation following uneven distributions in multiple domains. To tackle this problem, this paper presents a novel method for underwater image enhancement that is built on the framework of the biologically inspired Retinex. The proposed method utilize the multiple filters to preserve the original features in multiple domains during the enhancement process, which demonstrates the competitive results in the experiments with real world data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">The proposed underwater image enhancement method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Retinex applied on underwater images</head><p>Many approaches have been proposed for underwater image enhancement, as described in Sec. 2. The methods are based on Retinex, which are biologically inspired ones that utilize the color constancy theory. In the human vision system, the color constancy is automatically applied to enable humans to perceive the world in different illumination conditions <ref type="bibr" target="#b16">[16]</ref>. The first theory on color constancy is the Retinex presented by <ref type="bibr">Land and McCann [6]</ref>. Recently, an algorithm named Multi-Scale Retinex with Color Restoration (MSRCR) has emerged <ref type="bibr" target="#b26">[26]</ref> to make the algorithm more reliable. A correction step is added for each color channel to suppress the desaturation of the image processed by Multi-Scale Retinex (MSR) <ref type="bibr" target="#b13">[13]</ref>. The proposed method follows the framework of the Retinex that estimates the illumination component from the observed image.</p><p>According to the lightness theory <ref type="bibr" target="#b27">[27]</ref>, the observed image can be decomposed into two components, which are the luminance and the reflectance, as shown in Eq. <ref type="bibr" target="#b2">(3)</ref>.</p><formula xml:id="formula_3">log[ ( , )] log[ ( , )] log[ ( , )] S x y L x y R x y  <label>(3)</label></formula><p>where S(x, y) is the observed pixel in the image at the location of (x, y). L(x, y) and R(x, y) denote the components of luminance and reflectance respectively. L(x, y) varies according to the different illuminations of the scene, while R(x, y) remains still since it is related to the nature property of a scene itself. The goal of the color constancy is to obtain the R(x, y) component from observed S(x, y). In the original theory of the MSR, the L(x, y) component can be approximated by applying multiple individual convolutions with different Gaussian Kernels to the original S(x, y) by different weights, as shown in Eq. ( <ref type="formula">4</ref>), where σ i is the Gaussian Kernel coefficient, w i is the weight, and n is the number of the scales. The sum of all w i should equal 1.0.</p><formula xml:id="formula_4">1 2 2 2 2 ( , ) {log[ ( , )] log[F( )* ( , )]} [( ) ( ) ] 1 F( ) exp 2 2 n MSR i i i win win i i i R x y w S x y S x y x x_center y y_center                           (4)</formula><p>Equation ( <ref type="formula">4</ref>) demonstrates the process of MSR with one Gaussian Kernel by one weight for each iteration. Differing from the Single-Scale Retinex (SSR) <ref type="bibr" target="#b28">[28]</ref>, the multiple scales can handle the different details at multiple levels in the image. Experimentally, three scales are appropriate enough to represent a small scale, a large scale and an intermediate scale respectively <ref type="bibr" target="#b13">[13]</ref>. And generally three equal weights are used for these three scales.</p><p>For multi-channel images, RGB images for example, MSR is performed for each channel respectively. Then a color correction step is employed to make the color tone closer to the original one as much as possible, as shown in Eq. ( <ref type="formula">5</ref>):</p><formula xml:id="formula_5">{ , , } { , , } ( , ) ( , ) ( , ) log[ ] ( , ) i MSRCR i MSR i R G B i i j j R G B R x y Merge C R x y S x y C S x y                     (5)</formula><p>where C i is the coefficient for each channel. It stands for the proportions of three channels within an image. It is better to adjust the coefficient C i after applying the MSR to the image to make the color tone approximate to the original one. The function of Merge() stands for the process of merging three singlechannel images into a multi-channel one. As the Retinex algorithm transfers the image into the log domain, a global adjustment to the final result is conducted including the Gain and the Offset. As shown in Eq. ( <ref type="formula" target="#formula_6">6</ref>), G and b are the gain and offset values respectively. </p><formula xml:id="formula_6">FINAL MSRCR R x y G R x y b  <label>(6)</label></formula><p>Many researchers are trying to apply Retinex theory to the underwater image enhancement or restoration. For example, Jobson et al. <ref type="bibr" target="#b29">[29]</ref> discussed the applications of MSR based Visual Servo on the underwater image with moderate turbidity, and the enhanced image was improved about 470% than the Fig. <ref type="figure">3</ref>. The example of the underwater images processed by retinex. The images in the left column are the original images; the images in the right column are the images processed with MSR. It can be observed that the halo artifacts exist in the processed images. The edges are obvious within these images, which lead to higher contrasts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T 6 unenhanced image with VCM Aggregate Score <ref type="bibr" target="#b30">[30]</ref>. As described in Sec. 3, the method built on Retinex is suitable for underwater enhancement since the process of estimating the luminance in Retinex is also spatial various. The convolution using small scale Gaussian Filter on the image plane only considers the nearby intensities, which ignores the intensities far from the Kernel center that might have different attenuation situations. However, the method in the form of the original Retinex only takes into account the spatial difference with a Gaussian distribution, excluding the degradation various in color range domain, which is also an important part of the characters for underwater image attenuation as discussed in Sec. 3. Thus, if a local region in the image has a wider spectrum of color, then the Retinex will only process this region with the Gaussian average of the colors within the filter region. Some colors may drift far from the original ones after the process, which might be unacceptable.</p><p>Another aspect of the improvement that is required for the Retinex is the Halo Artifacts around the edges in the image <ref type="bibr" target="#b31">[31]</ref>. For example, as shown in the right column of the Fig. <ref type="figure">3</ref>, the halo artifacts can be observed clearly around the edges between the diver's body and the water in the processed image. The intensity contrast is relatively high near these edges, which leads to larger gradients. This is caused by the Gaussian Filter applied on the edges that makes brighter side of the edge (the water) overenhanced and darker side of the edge (the diver's body) insufficiently enhanced. The over-enhanced area is called positive halo and the insufficiently enhanced area is called negative halo.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Filter combination based LAB-MSR for underwater image</head><p>In this paper, we present a novel method for underwater image enhancement by proposing the LAB-MSR algorithm. It is inspired by the original MSR framework and processes the underwater image in the CIELAB color space. As discussed in the Sec. 3, the degradations of the underwater image vary in multiple domains, while traditional Retinex with the Gaussian Filter only considers such variations in different spatial regions. Therefore, the proposed method utilizes a combination of Bilateral Filter and Trilateral Filter according to different color channels to process the underwater image. It takes into account the uneven distribution of the underwater image degradation in multiple domains.</p><p>The workflow of the proposed method is demonstrated in Fig. <ref type="figure" target="#fig_3">4</ref>, which can be divided into three main parts: (a) the pre-processing in the left, (b) the multi-channel enhancement with a combination of filters in the middle, and (c) the post-processing in the right. The whole process of the method consists of the following steps:</p><p>1) Calculate the original proportions of all three channels of the underwater image in RGB color space. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D</head><formula xml:id="formula_7">M A N U S C R I P T<label>7</label></formula><p>2) Convert the underwater image from RGB color space to CIELAB color space.</p><p>3) As described in Eq. ( <ref type="formula" target="#formula_7">7</ref>), apply convolutions with Bilateral Filter to the L channel of the underwater image to estimate and remove the luminance component in this channel of the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4)</head><p>As described in Eq. ( <ref type="formula">8</ref>), apply convolutions with Trilateral Filter to the A and the B channels of the image respectively to estimate and remove the luminance components in these two channels of the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5)</head><p>Convert the underwater image back from CIELAB to RGB color space. Then adjust the proportions of three channels to make them close to the original ones, as described in Eq. ( <ref type="formula">5</ref>).</p><p>The CIELAB color space uses a three-dimensional lookup table for color definitions. The three dimensions are L, A and B channel respectively. L channel is related to the lightness. A and B channels are the coordinates of two axes for a predefined chromatic table. The color encoding mechanism for CIELAB color space is similar to the human visual system, which also consists of one luminance channel and two chromatic channels <ref type="bibr" target="#b33">[32,</ref><ref type="bibr" target="#b34">33]</ref>. In the human visual neural system, the color information are conveyed from eye to the brain by three channels <ref type="bibr" target="#b6">[7]</ref>: (a) a luminance channel represented by the signals from L-cones and M-cones to compute the intensity of a stimulus (L+M); (b) the signals from Lcone and M-cone are subtracted from each other to compute the red-green component of the color stimulus (L-M); (c) the signals from L-cone and M-cone are subtracted from the S-cone signal to compute the blue-yellow component of the color stimulus (S-(L+M)). The CIELAB color space can better describe how human perceive a color. Moreover, The shadow gradients seem to be stronger in the L channel <ref type="bibr" target="#b35">[34]</ref>. Research has shown that the colors appear to have a bluer shade in shadows <ref type="bibr" target="#b34">[33]</ref>. This is coincidently similar with the underwater environment, where the surrounding seems to be in a bluish tone due to the absorption and scattering effects. Therefore, in the proposed method, the underwater images are converted from RGB color space to LAB color space for better analysis.</p><p>The Bilateral Filter and the Trilateral Filter are introduced as the edge-preserving filters, which are both treated as an alternative of the classic Gaussian Filter in some areas. The Bilateral Filter, which was first presented by Tomasi and Manduchi <ref type="bibr" target="#b37">[35]</ref>, considers the uneven distribution not only in the spatial domain, but also in the intensity values. Therefore, the Bilateral Filter is included in the proposed method for illumination estimation. This process is expressed as Eq. ( <ref type="formula" target="#formula_7">7</ref>).  </p><formula xml:id="formula_9">                   (7)</formula><p>where F(σ i ) shares the same function as in Eq. ( <ref type="formula">4</ref>), which stands for the Gaussian Distribution in a spatial domain. H(σ' i ) denotes the Gaussian Distribution in the domain of intensity values within the filter range. The Bilateral Filter has the advantages over the classic Gaussian Filter when applied in Retinex. It can alleviate the over-smooth of the edges in the enhanced images to a certain degree according to the similarity of the intensity values.</p><p>Following some drawbacks of anisotropic diffusion for contrast reduction in Bilateral Filter, a new nonlinear filter named Trilateral Filter was proposed by Choudhury and Tumblin <ref type="bibr" target="#b38">[36]</ref>. Compared to the Bilateral Filter, Trilateral Filter includes an additional analysis that involves the distribution of the intensity gradients within the filter range. Thus, the filter considers more on the pixels that hold larger similarities to the filter's central pixel with respect to their spatial locations, intensity values and local gradients altogether. Equation <ref type="bibr" target="#b7">(8)</ref> shows the process of the proposed method that utilizes the Trilateral Filter.</p><formula xml:id="formula_10">_ 1 2 2 2 ( , ) {log[ ( , )] log[F( ) H( ' ) G( " ) ( , )]} {Grad[ ( , )] Grad[ ( , )]} 1 G( " ) exp 2 " 2 " n TF MSR i i i i i win win i i i</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R x y w S x y S x y S x y S x_center y_center</head><formula xml:id="formula_11">                          (8) A C C E P T E D M A N U S C R I P T 8</formula><p>where F(σ i ) and H(σ' i ) hold the same functions as the ones in Eq. ( <ref type="formula" target="#formula_7">7</ref>), and G(σ" i ) stands for the Gaussian Filter on local gradients of the pixels. The Trilateral Filter can suppress the halo artifacts to a certain extent since it processes the pixels only with the neighboring pixels that have higher similarities in gradients.</p><p>The proposed method utilizes Bilateral Filter and Trilateral Filter on different channels of the underwater image for MSR processing according to the different characteristics of these channels. As described in Sec. 3, the underwater image degradation holds variations both in spatial domain and intensity value range domain. Therefore, the proposed method employs the Bilateral Filter and Trilateral Filter instead of classic Gaussian Filter due to that the Bilateral and Trilateral Filter both take into the considerations of the similarities in spatial domain and intensity value range simultaneously. The A and B channels together represent the chromatic characters of the underwater image, which hold the color information of the scene. The L channel represents the luminance component of the image. Therefore, the proposed method tends to process more pixels in the L channel by applying the Bilateral Filter with less constraints, and applies Trilateral Filter to the A and B channels to further preserve the edge signals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments and evaluations</head><p>In this paper, we evaluate the proposed method by two types of the experiments including both the subjective tests and the objective tests. The first type is about the psychophysical experiments related to the underwater image perceptions. This type of the experiments demonstrates both the psychophysical characters of the degraded underwater images, and the performance of the proposed method which improves the visibility of the underwater images. In the second type of the experiments, the proposed method is further tested with the undersea images in the real world.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Subjective tests</head><p>In this part, the experiments are conducted with 30 individual subjects observing images of two pads with 16 different testing color patches on each. These images are captured when the camera and the pads are both placed in the water with different turbidity conditions. Moreover, a pad with reference color patches is also captured in the air. The subjects are instructed to find the most similar reference color patch from each testing color patch. All the pads are printed by the same printer. All the images are captured by the same camera, and are displayed to the subjects by the same monitor. To demonstrate the performance of the proposed method, the experiments are divided into two sections. In the first section, the original images of the underwater testing color patches are provided to the subjects; in the second section, the processed underwater images containing the testing color patches are provided to the subjects. The processed images are all enhanced by the proposed method.   In the experiments, the underwater turbidity conditions are simulated by using a mixture of whole milk and grape juice in the water. Existing research has demonstrated that the milk can simulate the scattering effect, while the grape juice provides the absorbing effect <ref type="bibr" target="#b39">[37]</ref>. With different proportions of milk and juice to water, 4 different turbidity conditions are applied during the experiments. The pads that contain color patches are shown in Fig. <ref type="figure" target="#fig_5">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(a) Experiment with Original Underwater Image</head><p>In the first section of the experiment, the 30 subjects have been recruited to recognize the testing patches in the images captured underwater with 4 different turbidity conditions and 2 different distances. Examples of the images are shown in Fig. <ref type="figure" target="#fig_6">6</ref>. For the convenience of the experimental result analysis, the reference color patches are grouped into 7 groups according to 3 channels in RGB color space, as shown in Fig. <ref type="figure" target="#fig_5">5 (c)</ref>, where each row is a group: (A) In the color group 1, colors only vary in the green and blue channels with the values in the red channel fixed at the maximum. (B) In the color group 2, colors only vary in the red and blue channels with the values in the green channel fixed at the maximum. (C) In the color group 3, colors only vary in the red and green channels with the values in a blue channel fixed at the maximum. (D) In the color group 4, colors only vary in a green channel with the values in the red and blue channels fixed at maximum ones. (E) In the color group 5, colors only vary in blue channel with the values in the red and green channels fixed at maximum ones. (F) In the color group 6, vary colors only in a red channel with the values in the green and blue channels fixed at maximum one. And (G) the colors in the color group 7 are composed of grayscale, which are varying from black to white.</p><p>The experiment results are shown in Table <ref type="table" target="#tab_1">I</ref>, which demonstrates the percentage when perceiving one color group as a certain color group. As it can be observed that: (A) the color groups 1 and 4 are most likely to be confused with each other. Beside this confusion, the colors in group 1 are most likely to be confused with the ones in group 5. (B) The color groups 2 and 6 are most likely to be confused with  The percentages with underline belong to the color group that is perceived by most participants.</p><p>The percentages with dashed underline belong to the second most perceived color group. As it can be observed in the results of the first section of the experiments, the visual images can be highly affected in the underwater environment. The degradation of the images increases when turbidity condition of the water increases, as shown in Fig. <ref type="figure" target="#fig_8">7</ref>. With the turbidity in the water, which mostly causes the scattering and absorption, the perception of the blue and green components of the color becomes insensitive. This is likely due to the loss of the information in the red channel of the color, which makes the ambient lights seem to be dominated by green-blue. This demonstrates that the underwater image degradation follows the uneven distribution in color range domain. At the same time, the image seems to be blurrier with the increase of the turbidity and the object's distance. It can also be noted in the Fig. <ref type="figure" target="#fig_6">6</ref> that the attenuation of the underwater image is spatially different.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(b) Experiment with Underwater Image Enhanced by Proposed Method</head><p>In the second section of the experiment, the underwater images are processed using the proposed method, and then provided to subjects for testing. The proposed method is implemented using C++ with OpenCV libs. Three scales are applied with equal weights for them. Bilateral Filter and Trilateral Filter share the same σ values for the spatial domain, which are 15, 50 and 120 for three scales. The σ' for the distribution of color range domain equals σ/2, and the σ" is the same value as the σ'. Those parameters affect the sensitive ranges of the filters in three domains. The smaller σ values lead to tight constraints when evaluating the similarities of the pixels. This will make the processed image de-saturated. The larger σ values let the filters accept more pixels with larger weights in the convolution process, which results in over-saturated with more halo artifact.</p><p>The processed underwater images are shown in Fig. <ref type="figure" target="#fig_10">8</ref>, and the experiment results are demonstrated in Table <ref type="table" target="#tab_3">II</ref>. As it can be observed in the results, the visibilities of the underwater images enhanced by the proposed method are much better than the original ones. According to the Table <ref type="table" target="#tab_3">II</ref>, the accuracies of the color perceptions by the subjects are also well improved compared to the ones seated on the diagonal elements in Table <ref type="table" target="#tab_1">I</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The original images</head><p>The processed images The Method in <ref type="bibr" target="#b16">[16]</ref> The proposed method Fig. <ref type="figure">9</ref>. The comparison of the enhanced underwater images using MSRCR and the proposed method respectively. The left column shows the original images; the middle column shows the enhanced images by MSRCR; the right column shows the enhanced images by the proposed method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Performance comparison of the proposed method and the existing one</head><p>In this part, experiments are conducted to demonstrate the performance of the proposed method using real-world undersea images. The parameter settings of the proposed method are the same as the ones in the Sec 6.A.2. For comparison, we apply the method in the form of the Multi-Scale Retinex <ref type="bibr" target="#b16">[16]</ref> to the underwater images. All the images are scaled to VGA size for the convenience of the comparison. The results are demonstrated in Fig. <ref type="figure">9</ref>, where the first, the second and the third columns contain the original undersea images, the undersea images processed using the MSR in <ref type="bibr" target="#b16">[16]</ref>, and the enhancement results using the proposed methods respectively. The comparisons of the image quality measurement are also demonstrated in Table <ref type="table" target="#tab_4">III</ref>, which demonstrates the better performance of the proposed method for underwater image enhancement.</p><p>As it can be observed from the experiment results, the images enhanced by the proposed method are significantly improved from the original ones. With the proposed method, the contents in the dark areas of the original underwater images are brightened. The decayed colors in the underwater environment are corrected. Compared to the method of MSRCR, the proposed method alleviates the halo artifacts in the enhanced images to a great extent. The halo artifacts are obviously noticeable in the processed image by the MSRCR, especially where edges are sharp. The proposed method suppresses these effects due to the utilization of the filters with the consideration of the distributions in all three aspects. Furthermore, the excessively high contrast is avoided for the enhanced images with the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper we propose a novel method for underwater image enhancement. The proposed method, LAB-MSR, is inspired by the original MSR framework. It processes the underwater images in CIELAB color space. The combination of the Bilateral Filter and Trilateral Filter is applied to the three channels according to the characteristics of each channel. Compared to the Gaussian Filter employed in the original Retinex, these filters consider not only the degradation distribution in the spatial domain, but also the ones in terms of neighboring intensity values and neighboring gradients. This makes the proposed method more suitable for the situations in the underwater environment. Moreover, these filters enable the proposed method to suppress the halo artifacts that are found severely in the traditional Retinex when applied to images with sharp edges. This paper also discusses the degradation characteristics of the underwater images, which supports the construction of the proposed method. The experiments carried out in this paper includes two aspects: (a) demonstrate the degradation of the underwater images with real-world data and statistics; (b) demonstrate the competitive performance of the proposed method. In future, we will further explore the temporal information carried in the LAB channels of the underwater image with the wavelet filters <ref type="bibr" target="#b40">[38]</ref>. The potential applications of the proposed method include the underwater robot control and manipulation <ref type="bibr" target="#b41">[39]</ref>, the underwater scene analysis <ref type="bibr" target="#b42">[40]</ref>, etc.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Demonstration of the degradation of the underwater image. Different regions in the images have different attenuations. The left one is an encrusted Cargo container under the Red Sea; the right one is a locomotive wreckage on the sea floor.</figDesc><graphic coords="4,70.78,609.03,203.78,114.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The demonstration of variation of the seawater absorption coefficient according to different wavelengths.</figDesc><graphic coords="5,113.17,77.67,344.75,149.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The workflow of the proposed method.</figDesc><graphic coords="7,68.37,74.46,434.58,206.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>The pads with multiple color patches on each: (a) and (b) are the pads that contain testing color patches, which are placed underwater in the experiments; (c) is the pad that contains the reference color patches, which is placed in the air. For the convenience of the experimental results analysis, the color patches in (c) are grouped into 7 groups represented by each row. The color patches in (a) and (b) are randomly selected from (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. The demonstration of the original underwater images containing the testing color patches with different turbidity conditions. The images in the bottom row are captured a little further than the ones in the top row.</figDesc><graphic coords="9,276.93,626.17,78.30,58.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. The underwater color perception accuracy.</figDesc><graphic coords="10,165.43,631.10,239.88,109.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Beside this confusion, the colors in group 2 are most likely to be confused with the ones in group 7. (C) A few subjects recognize the colors in group 3 as the colors in group 7, but a majority of subjects recognizes the colors in group 7 as the colors in group 3. (D) The colors in group 5 are recognized correctly at most time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. The processed underwater images corresponding to the ones in Fig. 5. The images in the top row are the original underwater images; the images in the bottom row are the enhanced images. The images are enhanced by the proposed method using 3 scales for MSR with equal weights. Bilateral and Trilateral Filters apply 80, 150 and 250 to σ for each scale respectively.</figDesc><graphic coords="11,66.74,535.43,68.94,51.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table I</head><label>I</label><figDesc>The statistics of the underwater color perceptions</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>The Percentage of the Color Groups that Being Perceived as Group 1 Group 2 Group 3 Group 4 Group 5 Group 6 Group 7 Testing Color</head><label></label><figDesc></figDesc><table><row><cell>Group 1</cell><cell>45.72%</cell><cell>00.00%</cell><cell>00.00%</cell><cell>48.29%</cell><cell>05.98%</cell><cell>00.00%</cell><cell>00.00%</cell></row><row><cell>Group 2</cell><cell>00.00%</cell><cell>73.86%</cell><cell>01.01%</cell><cell>0.00%</cell><cell>01.01%</cell><cell>22.63%</cell><cell>01.50%</cell></row><row><cell>Group 3</cell><cell>00.00%</cell><cell>00.00%</cell><cell>88.71%</cell><cell>01.02%</cell><cell>00.51%</cell><cell>03.58%</cell><cell>06.15%</cell></row><row><cell>Group 4</cell><cell>28.75%</cell><cell>00.00%</cell><cell>00.00%</cell><cell>70.00%</cell><cell>01.25%</cell><cell>00.00%</cell><cell>00.00%</cell></row><row><cell>Group 5</cell><cell>00.00%</cell><cell>00.52%</cell><cell>00.00%</cell><cell>00.00%</cell><cell>99.47%</cell><cell>00.00%</cell><cell>00.00%</cell></row><row><cell>Group 6</cell><cell>00.00%</cell><cell>18.12%</cell><cell>01.87%</cell><cell>00.00%</cell><cell>01.25%</cell><cell>78.75%</cell><cell>00.00%</cell></row><row><cell>Group 7</cell><cell>0.66%</cell><cell>00.66%</cell><cell>54.00%</cell><cell>00.66%</cell><cell>00.66%</cell><cell>02.66%</cell><cell>28.66%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table II The</head><label>II</label><figDesc>accuracy of the perceptions for enhanced underwater images using proposed method</figDesc><table><row><cell>12</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Color Groups</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Group 1</cell><cell>Group 2</cell><cell>Group 3</cell><cell>Group 4</cell><cell>Group 5</cell><cell>Group 6</cell><cell>Group 7</cell></row><row><cell>Accuracy</cell><cell>67.39%</cell><cell>86.45%</cell><cell>89.67%</cell><cell>79.87%</cell><cell>99.52%</cell><cell>87.89%</cell><cell>30.33%</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">M A N U S C R I P T</cell></row><row><cell cols="4">A C C E P T E D</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table III</head><label>III</label><figDesc>The comparison of the quality measurements of the experiment in Fig.9The MSE is calculated using the Matlab function of immse(im, ref). The smaller MSE indicates closer the enhanced image to the original image.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Quality Measurement</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Row 1</cell><cell>Row 2</cell><cell>Row 3</cell><cell>Row 4</cell><cell>Row 5</cell></row><row><cell></cell><cell>The method in [16]</cell><cell>736.76</cell><cell>455.96</cell><cell>332.25</cell><cell>490.00</cell><cell>389.91</cell></row><row><cell>MSE</cell><cell>The proposed method</cell><cell>455.00</cell><cell>316.27</cell><cell>87.84</cell><cell>312.20</cell><cell>231.36</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported in part by National Natural Science Foundation of China (NSFC) (No. 41576011); International Science &amp; Technology Cooperation Program of China (ISTCP) (N0. 2014DFA10410); Fundamental Research Projects of Qingdao Science and Technology Plan (N0. 12-1-4-1-(8)-jch); Shandong Science and Technology Development Plan Projects (N0. 2012GHY11524); and the Engineering and Physical Sciences Research Council Project (EPSRC), UK (No. EP/N025849/1).</p><p>The authors would like to thank Amanuel Hirpa and Hina Saeeda for their proof-readings of the paper.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">DeepFish: Accurate underwater live fish recognition with a deep architecture</title>
		<author>
			<persName><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">187</biblScope>
			<biblScope unit="page" from="49" to="58" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A Survey on Underwater Image Enhancement Techniques</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sahu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Applications</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="19" to="23" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Recovery of underwater visibility and structure by polarization analysis, Oceanic Engineering</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Schechner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Karpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="570" to="587" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Clear underwater vision, Computer Vision and Pattern Recognition</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Schechner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Karpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2004. Proceedings of the 2004 IEEE Computer Society Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="volume">531</biblScope>
			<biblScope unit="page" from="536" to="543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Enhancing the low quality images using Unsupervised Colour Correction Method</title>
		<author>
			<persName><forename type="first">K</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Odetayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Salam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Z H</forename><surname>Talib</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Systems Man and Cybernetics (SMC), 2010 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1703" to="1709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The retinex</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Land</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Am. Sci</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="247" to="264" />
			<date type="published" when="1964-06">Jun.1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cortical mechanisms of colour vision</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Gegenfurtner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Neuroscience</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="563" to="572" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Relational colour constancy from invariant cone-excitation ratios</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Nascimento</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the Royal Society of London B: Biological Sciences</title>
		<imprint>
			<biblScope unit="volume">257</biblScope>
			<biblScope unit="page" from="115" to="121" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Z.-U</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-scale retinex for color image enhancement</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Jobson</surname></persName>
		</author>
		<author>
			<persName><surname>Woodell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings., International Conference on</title>
		<meeting>International Conference on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1996">1996. 1996</date>
			<biblScope unit="page" from="1003" to="1006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Marazzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Sparavigna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.08715</idno>
		<title level="m">Retinex filtering of foggy images: generation of a bulk set with selection and ranking</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Formal connections between lightness algorithms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hurlbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Optical Society of America A: Optics, Image Science, and Vision</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1684" to="1693" />
			<date type="published" when="1986-10">October 1986. 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Synthesizing a Color Algorithm from Examples</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Hurlbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">239</biblScope>
			<biblScope unit="page">482</biblScope>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A multiscale retinex for bridging the gap between color images and the human observation of scenes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Jobson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Woodell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Processing IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="965" to="976" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biologically inspired image enhancement based on Retinex</title>
		<imprint>
			<biblScope unit="volume">177</biblScope>
			<biblScope unit="page" from="373" to="384" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Neurocomputing</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Eye detection under varying illumination using the retinex theory</title>
		<author>
			<persName><forename type="first">C</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page" from="130" to="137" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Quantification of retinex in enhancement of weather degraded images</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Kamathe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICALIP2008)</title>
		<meeting>ICALIP2008)</meeting>
		<imprint>
			<biblScope unit="page" from="1229" to="1233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Underwater image enhancement using single scale retinex on a reconfigurable hardware</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Sm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Supriya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 International Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
	<note>Ocean Electronics (SYMPOL</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A retinex-based enhancing approach for single underwater image</title>
		<author>
			<persName><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="4572" to="4576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Underwater image processing: state of the art of restoration and image enhancement methods</title>
		<author>
			<persName><forename type="first">R</forename><surname>Schettini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Corchs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Advances in Signal Processing</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A computer model for underwater camera systems, Ocean Optics VI</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mcglamery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Society for Optics and Photonics1980)</title>
		<imprint>
			<biblScope unit="page" from="221" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Vision and the atmosphere</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="233" to="254" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A Survey on Underwater Image Enhancement Techniques</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sahu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Applications</title>
		<imprint>
			<biblScope unit="page">87</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Underwater image sharpness assessment based on selective attenuation of color in the water, OCEANS 2016-Shanghai</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Single underwater image restoration by blue-green channels dehazing and red channel correction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Quo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="1731" to="1735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Short range underwater optical communication links</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Chancey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">An automated multi Scale Retinex with Color Restoration for image enhancement</title>
		<author>
			<persName><forename type="first">S</forename><surname>Parthasarathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sankaran</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Land</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Mccann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lightness and retinex theory</title>
		<imprint>
			<date type="published" when="1971">1971</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Properties and performance of a center/surround retinex</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Jobson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>-U. Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Woodell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="451" to="462" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A comparison of visual statistics for the image enhancement of FORESITE aerial images with those of major image classes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Jobson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>-U. Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Woodell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Hines</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Defense and Security Symposium</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page">624608</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Feature visibility limits in the nonlinear enhancement of turbid images</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Jobson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Woodell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SPIE -The International Society for Optical Engineering</title>
		<meeting>SPIE -The International Society for Optical Engineering</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page">5108</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Halo artifacts reduction method for variational based realtime retinex image enhancement</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tsutsui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yoshikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Okuhata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Onoye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal &amp; Information Processing Association Annual Summit and Conference</title>
		<imprint>
			<biblScope unit="page">2012</biblScope>
		</imprint>
		<respStmt>
			<orgName>APSIPA ASC</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Asia-Pacific</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Palmer</surname></persName>
		</author>
		<title level="m">Vision science: Photons to phenomenology</title>
		<imprint>
			<publisher>MIT press</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Evaluation of color spaces for edge classification in outdoor scenes</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Reinhard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing 2005</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="952" to="955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">J.-F</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Estimating natural illumination from a single outdoor image</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Lalonde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><surname>Narasimhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="183" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Bilateral filtering for gray and color images, Computer Vision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Manduchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1998">1998. 1998</date>
			<biblScope unit="page" from="839" to="846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The Trilateral Filter for High Contrast Images and Meshes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tumblin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurographics Workshop on Rendering Techniques</title>
		<meeting><address><addrLine>Leuven, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">June2003</date>
			<biblScope unit="page" from="186" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Photometric Stereo in a Scattering Medium</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Murez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Treibitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision2015)</title>
		<meeting>the IEEE International Conference on Computer Vision2015)</meeting>
		<imprint>
			<biblScope unit="page" from="3415" to="3423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Data-driven multi-scale non-local wavelet frame construction and image recovery</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Scientific Computing</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="307" to="329" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Neural-Learning-Based Telerobot Control With Guaranteed Performance</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Classifying dynamic textures via spatiotemporal fractal analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="3239" to="3248" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
