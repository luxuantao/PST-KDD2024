<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DSANet: Dual Self-Attention Network for Multivariate Time Series Forecasting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Siteng</forename><surname>Huang</surname></persName>
							<email>huangsiteng@westlake.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Donglin</forename><surname>Wang</surname></persName>
							<email>wangdonglin@westlake.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Xuehan</forename><surname>Wu</surname></persName>
							<email>wuxuehan2@huawei.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Westlake University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Westlake University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Huawei Technologies Co</orgName>
								<address>
									<settlement>Ltd</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">WeCar Technology Co</orgName>
								<address>
									<settlement>Ltd</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DSANet: Dual Self-Attention Network for Multivariate Time Series Forecasting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2DC722F8C56BD6C7BB70A56EEE8073E3</idno>
					<idno type="DOI">10.1145/3357384.3358132</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Time Series Forecasting</term>
					<term>Deep Learning</term>
					<term>Self-Attention</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multivariate time series forecasting has attracted wide attention in areas, such as system, traffic, and finance. The difficulty of the task lies in that traditional methods fail to capture complicated nonlinear dependencies between time steps and between multiple time series. Recently, recurrent neural network and attention mechanism have been used to model periodic temporal patterns across multiple time steps. However, these models fit not well for time series with dynamic-period patterns or nonperiodic patterns. In this paper, we propose a dual self-attention network (DSANet) for highly efficient multivariate time series forecasting, especially for dynamic-period or nonperiodic series. DSANet completely dispenses with recurrence and utilizes two parallel convolutional components, called global temporal convolution and local temporal convolution, to capture complex mixtures of global and local temporal patterns. Moreover, DSANet employs a self-attention module to model dependencies between multiple series. To further improve the robustness, DSANet also integrates a traditional autoregressive linear model in parallel to the non-linear neural network. Experiments on realworld multivariate time series data show that the proposed model is effective and outperforms baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>• Mathematics of computing → Time series analysis;</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>As multivariate time series are ubiquitous in our daily life, research on multivariate time series forecasting has been carried out in many areas, such as sensor networks <ref type="bibr" target="#b10">[11]</ref>, road occupancy rates forecasting <ref type="bibr" target="#b9">[10]</ref>, and financial market prediction <ref type="bibr" target="#b15">[16]</ref>. However, complex and non-linear dependencies exist not only between time steps but also in a variety of variables. Furthermore, the dependencies may change dynamically with time, which significantly increases the difficulty of analysis. Therefore, a major challenge of multivariate time series forecasting is how to capture dynamic dependencies between time steps and multiple variables.</p><p>Advanced statistical methods have been proposed for time series forecasting, such as vector autoregression (VAR) <ref type="bibr" target="#b3">[4]</ref> and Gaussian process (GP) <ref type="bibr" target="#b10">[11]</ref>. However, they usually assume certain distribution or function form of time series, which makes them unable to capture complicated underlying non-linear relationships and reflect reality. In addition, most of them ignore the dependencies between variables when addressing multivariate time series, which reduces the accuracy of forecasting. Recently, deep neural networks have drawn great attention in related domains due to their flexibility in capturing nonlinearity. In particular, recurrent neural network (RNN) <ref type="bibr" target="#b11">[12]</ref> has been considered as the default starting point for sequence modeling. However, traditional RNNs have difficulty in capturing long-range dependencies due to gradient vanishing <ref type="bibr" target="#b2">[3]</ref>. As its variants, long short-term memory (LSTM) <ref type="bibr" target="#b6">[7]</ref> and gated recurrent unit (GRU) <ref type="bibr" target="#b4">[5]</ref>, have overcome the limitation. Attention mechanism <ref type="bibr" target="#b1">[2]</ref> also helps RNN to model temporal patterns, which allows modeling on dependencies of the input and output by focusing on the selective parts of the input sequence. Models based on LSTM or GRU with attention mechanism have been proposed for time series forecasting and show good performances in exploiting long-term dependencies and handling non-linear dynamics <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b12">13]</ref>. However, due to the unsatisfactory performance, the structure might not be suitable for those data with dynamic-period patterns or nonperiodic patterns, which is common in a complex environment.</p><p>To enable accurate and robust forecasting for multivariate time series, we propose a dual self-attention network (DSANet) for highly efficient multivariate time series forecasting without exogenous information. In DSANet, we first feed each of the univariate time series independently into two parallel convolutional components, called global temporal convolution and local temporal convolution, to model complex mixtures of global and local temporal patterns. Next, the learned time series representations from each convolutional component are fed into an individual self-attention module, with the aim of learning the dependencies among different series. To further improve the robustness, an autoregressive linear model is integrated in parallel to the non-linear attention network of DSANet. To the best of our knowledge, this is the first work to apply self-attention mechanism in time series forecasting with the help of well-designed dual branches architecture. Experiments on a real-world time series data set demonstrate the accuracy and robustness of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>We first consider statistical linear methods for multivariate time series. Here, the vector autoregression (VAR) <ref type="bibr" target="#b3">[4]</ref> model is widely considered as a baseline method, which generalizes the univariate autoregressive (AR) model by allowing for more than one evolving variable. To model non-linear relationships, some variants of the autoregressive model are used, such as LRidge <ref type="bibr" target="#b7">[8]</ref>, LSVR <ref type="bibr" target="#b13">[14]</ref> and Gaussian process (GP) <ref type="bibr" target="#b10">[11]</ref>. However, they assume certain distribution or function form of time series and fail to capture different forms of nonlinearity.</p><p>Due to the ability to flexibly model various non-linear relationships, neural networks are often applied to enable non-linear forecasting models. For example, recurrent neural network models using LSTM or GRU are often used to provide non-linear time series forecasting. To predict more accurately, complex structures such as recurrent-skip layer (LSTNet-S), temporal attention layer (LSTNet-A) <ref type="bibr" target="#b9">[10]</ref>, and a novel temporal pattern attention mechanism (TPA) <ref type="bibr" target="#b12">[13]</ref> have been proposed. However, when working on data with dynamic-period patterns or nonperiodic patterns, their performance drops significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARIES</head><p>A time series</p><formula xml:id="formula_0">X (i) = ⟨x (i) 1 , x (i) 2 , . . . , x (i)</formula><p>T ⟩ is a fully observed timeordered sequence of measurements, where measurement x</p><formula xml:id="formula_1">(i)</formula><p>T is recorded at time stamp T . Usually, the time interval between two consecutive measurements is constant. A multivariate time series is denoted as X = ⟨X (1) , X (2) , . . . , X (D) ⟩, where time series in X are correlated with each other, and measurements X T ∈ R D are recorded at time stamp T . Problem Statement: Given a set of multivariate time series X = ⟨X (1) , X (2) , . . . , X (D) ⟩, where D is the number of univariate time series and X (D) ∈ R T with T being the length of the input window size, we aim at predicting in a rolling forecasting fashion. That being said, we predict X T +h based on the known ⟨X 1 , X 2 , . . . , X T ⟩, where h is the desirable horizon ahead of the current time stamp. Likewise, we predict the future value of</p><formula xml:id="formula_2">X T +h+k based on ⟨X 1+k , X 2+k , • • • , X T +k ⟩, k ∈ R + ,</formula><p>with an assumption that the information within the window is sufficient for prediction and the window size is fixed. We hence formulate that for the the forecasting target X T +h ∈ R D , the input matrix at time stamp T is X = ⟨X (1) , X (2) , . . . , X (D) ⟩ ∈ R D×T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHODOLOGY</head><p>Figure <ref type="figure" target="#fig_0">1</ref> presents an overview of our proposed DSANet. DSANet utilizes two convolutional structures, namely global temporal convolution and local temporal convolution, to embed each univariate series in X into two representation vectors with temporal information of different scale. Each vector forms a matrix and then enter an elaborate self-attention module to capture the dependencies between multiple series. In the end, the model generates the final prediction by summing up the output of both self-attentional network and the AR component. The details of the building blocks are introduced in the following paragraphs. Global Temporal Convolution: Deep learning methods in previous work mainly use RNNs to capture temporal patterns. However, due to the inherently sequential nature, it is difficult for RNNs to model long sequences and compute in parallel, which ultimately damages the computing speed and forecasting effect. As convolutional structure has demonstrated its power in capturing features as well as parallel computing, we use it together with multiple T × 1 filters, called global temporal convolution, to extract time-invariant patterns of all time steps for univariate time series.</p><p>Each filter of global temporal convolution module sweeps through the input matrix X and produces a vector with size of D × 1, where the activation function is the ReLU function. Merged by the vectors, The convolutional structure finally obtains an output matrix H G of size D × n G , where n G is the number of filters in global temporal convolution. Note that each row of the matrix can be considered as a learned representation of a univariate series. Local Temporal Convolution: Considering that time steps with a shorter relative distance have a larger impact on each other, DSANet also utilizes a convolutional structure in parallel to global temporal convolution, which is called local temporal convolution. While global temporal convolution captures long-term dependencies between time steps, local temporal convolution focuses on modeling local temporal patterns, which can be more helpful for forecasting.</p><p>Different from global temporal convolution, the length of filters used in local temporal convolution is l, where l &lt; T is a hyperparameter. The k-th filter of local temporal convolution slides along the time dimension and produces a matrix M L k . In order to map local temporal relations in each univariate time series to a vector representation, DSANet uses a 1-D max-pooling layer over each column of the matrix M L k to capture the most representative features. Thus, we obtain an output matrix H L of size D × n L , where n L is the number of filters in local temporal convolution. Self-Attention Module: Due to the strong feature-extraction capability of self-attentional networks, we apply a self-attention module inspired by the Transformer <ref type="bibr" target="#b14">[15]</ref> to capture the dependencies between different series. For each learned representation of a univariate series, the self-attention module learns its relationship with other learned representations including itself. As shown in Figure <ref type="figure" target="#fig_0">1</ref>, the self-attention module is composed of a stack of N identical layers, and each layer has two sub-layers: a self-attention layer and a position-wise feed-forward layer.</p><p>In general, an attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and the output are all vectors. The output is computed as a weighted sum of the values, where the weight for each position is computed as the inner product between the query and keys at every other position in time series. In the self-attention module following the global temporal convolution, a set of queries, keys, and values are packed together into matrices Q G , K G , and V G , obtained by applying projections to the input H G . Mathematically, the scaled dot product self-attention computation can be expressed as</p><formula xml:id="formula_3">Z G = softmax Q G (K G ) T d k V G ,<label>(1)</label></formula><p>where d k is the dimension of keys. A multi-head attention is utilized to allow the model to jointly deal with information from different representation subspaces at different position. The resulting weighted representations are concatenated and linearly projected to obtain the final representation Z G O . The position-wise feed-forward layer consists of two linear transformations with a ReLU activation in between, which can be expressed as</p><formula xml:id="formula_4">F G = ReLU(Z G O W 1 + b 1 )W 2 + b 2 .<label>(2)</label></formula><p>While the linear transformation is the same across different positions, they use different parameters. Followed by layer normalization <ref type="bibr" target="#b0">[1]</ref>, residual connections around each of the sub-layers make training easier and improve generalization.</p><p>We have a similar procedure for the self-attention module following the local temporal convolution, where we input H L into the module and finally get the output F L . Autoregressive Component: Due to the nonlinearity of both convolutional and self-attention components, the scale of neural network output is not sensitive to that of input. To address the drawback, we consider the final prediction of DSANet as a mixture of a linear component and a non-linear component. Apart from the non-linear component introduced above, the classical AR model <ref type="bibr" target="#b5">[6]</ref> is taken as the linear component. The forecasting of the AR component is expressed as X L T +h ∈ R D . Generation of Prediction: In the forecasting stage, we first use a dense layer to combine the outputs of two self-attention modules and get the self-attention based prediction X D T +h ∈ R D . The final prediction of DSANet XT +h is then obtained by summing the selfattention based prediction X D T +h and the AR prediction X L T +h .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>Data Sets: We use a large time series data set provided by a gas station service company. The data set contains the daily revenue of five gas stations ranging from Dec.1, 2015 to Dec.1, 2018. The stations are geographically close, which means a complex mix of revenue promotion and mutual exclusion exists between them. Thus we consider the five time series of each gas station as a multivariate time series. Data visualization analysis is performed to ensure that the data set does not contain distinct repetitive patterns.</p><p>In our experiments, the data set is chronologically split into training (60%), validation (20%) and test (20%) sets. In each set, we further segment the data into multiple cases using sliding windows, which means in each segment, we use a multivariate time series of T length as the input data to the models and use the measurements of the time stamp T + h as the ground truth data. Comparison Methods: In our comparative evaluations, we consider 8 baselines: VAR, LRidge, LSVR, GP, GRU, LSTNet-S, LSTNet-A, TPA. All the methods are covered in Section 2. Experimental Settings: All neural network models are optimized by performing mini-batch stochastic gradient descent (SGD) with the Adam optimizer <ref type="bibr" target="#b8">[9]</ref>, and the loss is calculated by the mean square error (MSE). We conduct a grid search over all tunable hyperparameters on the validation set for each method. Specifically, for LRidge and LSVR, the regularization coefficient λ is chosen from {2 -10 , 2 -8 , • • • , 2 8 , 2 10 }. For GP, the RBF kernel bandwidth σ and the noise level α are chosen from {2 -10 , 2 -8 , • • • , 2 8 , 2 10 }. For all neural network models, the hidden dimension size of recurrent layers and convolutional layers are chosen from {32, 50, 100}. For LSTNet-S, we conduct a grid search over {20, 50, 100} for recurrent-skip layers. For DSANet, the length of filters used in local temporal convolution is chosen from {3, 5, 7}. We perform dropout and the rate is set as 0.1. Implementation Details: All methods are implemented in Python 3.6, where the deep learning methods are implemented using Py-Torch 1.0. A computer with Intel i7-8700 CPU, GTX1060 GPU, 6 cores, 32 GB RAM is used to conduct all experiments. Main Results: To measure the effectiveness of various methods for multivariate time series forecasting, we use root relative squared error (RRSE), mean absolute error (MAE) and empirical correlation coefficient (CORR) as evaluation metrics. A lower value is better for RRSE and MAE while a higher value is better for CORR. We set the problem parameter window = {32, 64, 128} and horizon = {3, 6, 12, 24}, respectively, which means the window length is set from 32 to 128 days and the horizon is set from 3 to 24 days over the dataset. Due to space limitation, we report on results only based on RRSE and MAE with window = 32. More experimental results and code are available online <ref type="foot" target="#foot_0">1</ref> .</p><p>Table <ref type="table" target="#tab_0">1</ref> summarizes the evaluation results of all the methods on the test set. Each row in Table <ref type="table" target="#tab_0">1</ref> compares the results of all methods in a particular metric with a specific window-horizon pair, and each column shows the results of a specific method in all cases. Boldface indicates the best result of each row in a particular metric.</p><p>From Table <ref type="table" target="#tab_0">1</ref>, a common phenomenon is that when the horizon increases, both RRSE and MAE of the same method increase on the whole, which shows that the larger the horizon, the harder the forecasting task. Note that GRU, LSTNet-S, LSTNet-A, TPA and DSANet often achieve a better performance in comparison to others, which shows that due to the ability to learn complicated non-linear dependencies between time steps and between multiple time series, deep learning methods can solve complex forecasting tasks better than traditional methods. However, it is observed that compared to other methods, DSANet achieves better results in all cases, indicating that taking advantage of the well-designed architecture, DSANet is more robust to deal with multivariate time series with dynamic-period patterns or nonperiodic patterns. Ablation Study: To justify the efficiency of our architecture design, a careful ablation study is conducted. Specifically, we remove each of the global temporal convolution branch, the local temporal convolution branch, and the AR component one at a time in our DSANet model, and each new model is named DSAwoGlobal, DSAwoLocal, and DSAwoAR. The test results measured using RRSE and MAE with window = 32 are shown in Figure <ref type="figure" target="#fig_1">2</ref>, from which several observations are worth highlighting: <ref type="bibr" target="#b0">(1)</ref> The best result on each window-horizon pair is obtained by complete DSANet, showing all components have contributed to the effectiveness and robustness of the whole model; <ref type="bibr" target="#b1">(2)</ref> The performance of DSAwoAR significantly drops, showing that the AR component plays a crucial role. The reason is that AR is generally robust to the scale changing in data according to <ref type="bibr" target="#b9">[10]</ref>; (3) DSAwoGlobal and DSAwoLocal also suffer from performance loss but less than removing the AR component. This is because features learned by the two branches coincide. In other words, when one branch is removed, some of the lost features can be obtained from the other branch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We present a novel deep learning framework, dual self-attention network (DSANet), for the task of multivariate time series forecasting, especially for those data with dynamic-period or nonperiodic patterns. Experiments on a large real-world dataset show promising results.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Dual Self-Attention Network (DSANet)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Ablation Test Results of DSANet</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Evaluation Results of Multivariate Time Series Forecasting</figDesc><table><row><cell>Methods</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/bighuang624/DSANet Session: Short -Time Sequences &amp; Dynamics CIKM '19, November 3-7,</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>2019, Beijing, China</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Layer Normalization</title>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>CoRR abs/1607.06450</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrice</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994">1994. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">LIBSVM: A library for support vector machines</title>
		<author>
			<persName><forename type="first">Chih-Chung</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chih-Jen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TIST</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</title>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Çaglar</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multivariate autoregressive modeling of fMRI time series</title>
		<author>
			<persName><forename type="first">Lanette</forename><surname>Moreau Harrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">D</forename><surname>Penny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><forename type="middle">J</forename><surname>Friston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1477" to="1491" />
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Ridge Regression: Biased Estimation for Nonorthogonal Problems</title>
		<author>
			<persName><forename type="first">Arthur</forename><forename type="middle">E</forename><surname>Hoerl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">W</forename><surname>Kennard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="80" to="86" />
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Modeling Long-and Short-Term Temporal Patterns with Deep Neural Networks</title>
		<author>
			<persName><forename type="first">Guokun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Cheng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Gaussian processes for time-series modelling</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Osborne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Ebden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Reece</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neal</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suzanne</forename><surname>Aigrain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical transactions. Series A, Mathematical, physical, and engineering sciences</title>
		<imprint>
			<biblScope unit="volume">371</biblScope>
			<biblScope unit="page">1984</biblScope>
			<date type="published" when="2013">2013. 2013. 20110550</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>David E Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="page">533</biblScope>
			<date type="published" when="1986">1986. 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Temporal Pattern Attention for Multivariate Time Series Forecasting</title>
		<author>
			<persName><forename type="first">Shun-Yao</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan-Keng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
		<idno>CoRR abs/1809.04206</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Support Vector Method for Function Approximation, Regression Estimation and Signal Processing</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">E</forename><surname>Golowich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dynamic Covariance Models for Multivariate Financial Time Series</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">José</forename><surname>Miguel Hernández-Lobato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
