<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DEBERTA: DECODING-ENHANCED BERT WITH DIS-ENTANGLED ATTENTION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<title level="a" type="main">DEBERTA: DECODING-ENHANCED BERT WITH DIS-ENTANGLED ATTENTION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent progress in pre-trained neural language models has significantly improved the performance of many natural language processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT with disentangled attention) that improves the BERT and RoBERTa models using two novel techniques. The first is the disentangled attention mechanism, where each word is represented using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangled matrices on their contents and relative positions. Second, an enhanced mask decoder is used to incorporate absolute positions in the decoding layer to predict the masked tokens in model pre-training. We show that these two techniques significantly improve the efficiency of model pre-training and the performance of both natural language understand(NLU) and natural langauge generation(NLG) tasks. Compared to RoBERTa-Large, a DeBERTa model trained on half of the training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%), on SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The Transformer has become the most effective neural network architecture for neural language modeling. Unlike recurrent neural networks (RNNs) that process text in sequence, Transformers apply self-attention to compute in parallel every word from the input text an attention weight that gauges the influence each word has on another, thus allowing for much more parallelization than RNNs for large-scale model training <ref type="bibr" target="#b44">(Vaswani et al., 2017)</ref>. Since 2018, we have seen the rise of a set of large-scale Transformer-based Pre-trained Language Models (PLMs), such as GPT <ref type="bibr" target="#b33">(Radford et al., 2019;</ref><ref type="bibr" target="#b3">Brown et al., 2020)</ref>, BERT <ref type="bibr" target="#b10">(Devlin et al., 2019)</ref>, RoBERTa <ref type="bibr" target="#b29">(Liu et al., 2019c)</ref>, XLNet <ref type="bibr" target="#b49">(Yang et al., 2019)</ref>, UniLM <ref type="bibr" target="#b12">(Dong et al., 2019a)</ref>, ELECTRA <ref type="bibr" target="#b7">(Clark et al., 2020)</ref>, T5 <ref type="bibr" target="#b34">(Raffel et al., 2019)</ref>, ALUM <ref type="bibr" target="#b28">(Liu et al., 2020)</ref>, StructBERT <ref type="bibr" target="#b46">(Wang et al., 2019)</ref> and ERINE <ref type="bibr" target="#b42">(Sun et al., 2019)</ref> . These PLMs have been fine-tuned using task-specific labels and created new state-of-the-art in many downstream natural language processing (NLP) tasks <ref type="bibr" target="#b27">(Liu et al., 2019b;</ref><ref type="bibr" target="#b32">Minaee et al., 2020;</ref><ref type="bibr" target="#b19">Jiang et al., 2019;</ref><ref type="bibr" target="#b16">He et al., 2019a;</ref><ref type="bibr">b;</ref><ref type="bibr">Shen et al., 2020)</ref>.</p><p>In this paper, we propose a new Transformer-based neural language model DeBERTa (Decodingenhanced BERT with disentangled attention) which has been proven to be more effective than RoBERTa and BERT and after fine-tuning leads to better results on a wide range of NLP tasks.</p><p>DEBERTa proposes a disentangled self-attention mechanism. Unlike BERT where each word in the input layer is represented using a vector which is the sum of its word (content) embedding and position embedding, each word in DeBERTa is represented using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangled matrices based on their contents and relative positions, respectively. This is motivated by the observation that the attention weight of a word pair depends on not only their contents but their relative positions. For example, the dependency between the words "deep" and "learning" is much stronger when they occur next to each other than when they occur in different sentences.</p><p>As an extension to disentangled attention, we enhance the output layer of BERT for pre-training to address a limitation of relative positions. We observe in some situations, it is challenging for the relative positions only mechanism to accurately predict masking tokens. For example, considering a sentence "A new store opened near the new mall" with the words store and mall masked for prediction, only using the relative positions is not sufficient for the model to accurately predict store and mall in this sentence, since they are exchangeable in syntax although with a different meaning, also right after the word new with an exact relative position to it. To address this limitation, we propose to introduce absolute positions back in the output layer of BERT, as a complement to the relative positions.</p><p>We show through a comprehensive empirical study that the disentangle attentions with its extensions substantially improve the efficiency of pre-training and the performance of downstream tasks. In the NLU tasks, compared to RoBERTa-Large, a DeBERTa model trained on half the training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%), on SQuAD v2.0 by +2.3%(88.4% vs. 90.7%), and RACE by +3.6% (83.2% vs. 86.8%). In the NLG tasks, DeBERTa improves the perplexity from 21.6 to 19.5 on the Wikitext-103 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">TRANSFORMER STRUCTURE</head><p>A Transformer-based language model is composed of stacked Transformer blocks <ref type="bibr" target="#b44">(Vaswani et al., 2017)</ref>. Each block contains a multi-head self-attention layer followed by a fully connected positional feed-forward network. The standard self-attention mechanism lacks a natural way to encode word position information. Thus, existing approaches add a positional bias to each input word embedding so that each input word is represented by a vector whose value depends on its content and position. The positional bias can be implemented using absolute position embedding <ref type="bibr" target="#b44">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b33">Radford et al., 2019;</ref><ref type="bibr" target="#b10">Devlin et al., 2019)</ref> or relative position embedding <ref type="bibr" target="#b18">(Huang et al., 2018;</ref><ref type="bibr" target="#b49">Yang et al., 2019)</ref>. It has been shown that relative position representations are more effective for natural language understanding and generation tasks <ref type="bibr" target="#b9">(Dai et al., 2019;</ref><ref type="bibr" target="#b38">Shaw et al., 2018)</ref>. The proposed Disentangled Attention mechanism differs from all existing approaches in that we represent each input word using two separate vectors that encode a word's content and position respectively, and attention weights among words are computed using disentangled matrices on their contents and relative positions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">MASKED LANGUAGE MODEL</head><p>Large-scale Transformer-based PLMs <ref type="bibr" target="#b10">(Devlin et al., 2019;</ref><ref type="bibr" target="#b29">Liu et al., 2019c;</ref><ref type="bibr" target="#b24">Lan et al., 2019)</ref> are typically pre-trained on large amounts of text to learn contextual word representations using a selfsupervision objective, known as Masked Language Model (MLM). Specifically, given a sequence X " tx i u, we corrupt it into X by masking 15% of its tokens at random and then train a language model parameterized by θ to reconstruct X by predicting the masked tokens x conditioned on X:</p><formula xml:id="formula_0">max θ log p θ pX| Xq " max θ ÿ iPC log p θ px i " x i | Xq (1)</formula><p>where C is the index set of the masked tokens in the sequence. The authors of BERT propose to keep 10% of the masked tokens unchanged, another 10% replaced with randomly picked tokens and the rest replaced with the [MASK] token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">APPROACH</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">DISENTANGLED ATTENTION</head><p>For a token at position i in a sequence, we represent it using two vectors, tH i u and tP i|j u, which represent its content and relative position with the token at position j, respectively. The calculation of the cross attention score between tokens i and j can be decomposed into four components as</p><formula xml:id="formula_1">A i,j " tH i , P i|j u ˆtH j , P j|i u " H i H j `Hi P j|i `Pi|j H j `Pi|j P j|i (2)</formula><p>That is, the attention weight of a word pair can be computed as a sum of four attention scores using disentangled matrices on their contents and positions as content-to-content, content-to-position, position-to-content, and position-to-position<ref type="foot" target="#foot_0">1</ref> .</p><p>Existing approaches <ref type="bibr" target="#b38">(Shaw et al., 2018;</ref><ref type="bibr" target="#b18">Huang et al., 2018)</ref> to relative position encoding use a separate embedding matrix to compute the relative position bias in computing attention weights. This is equivalent to computing the attention weights using only the content-to-content and content-toposition terms in equation 2. We argue that the position-to-content term is also important since the attention weight of a word pair depends not only on their contents but on their relative positions, which can only be fully modeled using both the content-to-position and position-to-content terms.</p><p>Since we use relative position embedding, the position-to-position term does not provide much additional information and is removed from equation 2 in our implementation.</p><p>Taking single-head attention as an example, the standard self-attention <ref type="bibr" target="#b44">(Vaswani et al., 2017)</ref> can be formulated as:</p><formula xml:id="formula_2">Q " HW q , K " HW k , V " HW v , A " QK ? d H o " softmaxpAqV</formula><p>where H P R N ˆd represents the input hidden vectors, H o P R N ˆd the output of self-attention, W q , W k , W v P R dˆd the projection matrices, A P R N ˆN the attention matrix, N the length of input sequence, and d the dimension of hidden state.</p><p>Denote k as the maximum relative distance, δpi, jq P r0, 2kq as the relative distance from token i to token j, which is defined as:</p><formula xml:id="formula_3">δpi, jq " # 0 for i ´j ď ´k 2k ´1 for i ´j ě k i ´j `k others (3)</formula><p>We can represent the disentangled self-attention with relative position bias as equation 4, where Q c , K c and V c are the projected content vectors generated using projection matrices W q,c , W k,c , W v,c P R dˆd respectively, P P R 2kˆd represents the relative position embedding vectors shared across all layers (i.e., staying fixed during forward propagation), and Q r and K r are projected relative position vectors generated using projection matrices W q,r , W k,r P R dˆd , respectively.</p><formula xml:id="formula_4">Q c " HW q,c , K c " HW k,c , V c " HW v,c , Q r " P W q,r , K r " P W k,r Ãi,j " Q c i K c j looomooon (a) content-to-content `Qc i K r δpi,jq looooomooooon (b) content-to-position `Kc j Q r δpj,iq looooomooooon (c) position-to-content H o " softmaxp Ã ? 3d qV c (4)</formula><p>Ãi,j is the element of attention matrix Ã, representing the attention score from token i to token j.</p><formula xml:id="formula_5">Q c i is the i-th row of Q c . K c j is the j-th row of K c . K r δpi,jq</formula><p>is the δpi, jq-th row of K r with regarding to relative distance δpi, jq. Q r δpj,iq is the δpj, iq-th row of Q r with regarding to relative distance δpj, iq. Note that we use δpj, iq rather than δpi, jq here. This is because for a given position i, position-to-content computes the attention weight of the key content at j with respect to the query position at i, thus the relative distance is δpj, iq. The position-to-content term is calculated as K c j Q r δpj,iq . The content-to-position term is calculated in a similar way.</p><p>Finally, we apply a scaling factor of 1 ? 3d on Ã which is important for stabilizing model training <ref type="bibr" target="#b44">Vaswani et al. (2017)</ref>, especially for large-scale PLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">EFFICIENT IMPLEMENTATION</head><p>For an input sequence of length N , it requires a space complexity of OpN 2 dq <ref type="bibr" target="#b38">(Shaw et al., 2018;</ref><ref type="bibr" target="#b18">Huang et al., 2018;</ref><ref type="bibr" target="#b9">Dai et al., 2019)</ref> to store the relative position embedding for each token. However, taking content-to-position as an example, we note that since δpi, jq P r0, 2kq and thus the embedding of all possible relative positions are always a subset of K r P R 2kˆd , then we can reuse K r in the attention calculation for all the queries. In experiments, we set the maximum relative distance k to 512 for pre-training. The disentangled attention weights can be computed efficiently using Algorithm 1. Let δ be the relative position matrix according to equation 3, i.e., δri, js " δpi, jq. Instead of allocating a different relative position embedding matrix for each query, we multiply each query vector Q c ri, :s by K r P R dˆ2k , as in line 3 ´5. Then, we extract the attention weight using the relative position matrix δ as the index, as in line 6 ´10. To compute the position-to-content attention score, we calculate ÃpÑc r:, js, i.e., the column vector of the attention matrix ÃpÑc , by multiplying each key vector K c rj, :s by Q r , as in line 11 ´13. Finally, we extract the corresponding attention score via the relative position matrix δ as the index, as in line 14 ´18. In this way, we do not need to allocate memory to store a relative position embedding for each query and thus reduce the space complexity to Opkdq (for storing K r and Q r ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Disentangled Attention</head><p>Input: Hidden state H, relative distance embedding P , relative distance matrix δ. Content projec-</p><formula xml:id="formula_6">tion matrix W k,c , W q,c , W v,c , position projection matrix W k,r , W q,r . 1: K c " HW k,c , Q c " HW q,c , V c " HW v,c , K r " P W k,r , Q r " P W q,r 2: A cÑc " Q c K c 3: for i " 0, ..., N ´1 do 4:</formula><p>ÃcÑp ri, :s " Q c ri, :sK r 5: end for 6: for i " 0, ..., N ´1 do 7:</p><p>for j " 0, ..., N ´1 do 8:</p><p>A cÑp ri, js " ÃcÑp ri, δri, jss 9: end for 10: end for 11: for j " 0, ..., N ´1 do 12:</p><p>ÃpÑc r:, js " K c rj, :sQ r 13: end for 14: for j " 0, ..., N ´1 do 15:</p><p>for i " 0, ..., N ´1 do 16:</p><p>A pÑc ri, js " ÃpÑc rδrj, is, js</p><formula xml:id="formula_7">17: end for 18: end for 19: Ã " A cÑc `AcÑp `ApÑc 20: H o " softmaxp Ã ? 3d qV c Output: H o</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">TWO EXTENSIONS OF THE DISENTANGLED ATTENTION</head><p>The DeBERTa model has two additional extensions. One is to address a limitation of the relative positions which have been fully captured by the disentangled attentions. The other is to enable generation tasks and a multi-task learning objective.</p><p>Given a sentence "A new store opened near the new mall" with the words store and mall masked for prediction, only using the relative positions is hard for the model to distinguish store and mall in this sentence, since both of them are right after the word new with the exact relative positions. To address this limitation, we propose to reconsider the introduction of absolute positions in the model, as a complement to the relative positions. There are at least two ways to introduce the absolution positions. The BERT model incorporates the absolute positions in the input layer. In DeBERTa, we propose an alternative to consider it right after all the Transformer layers but right before the softmax for masked token decoding, as shown in Figure <ref type="figure" target="#fig_2">2</ref>. In this way, DeBERTa captures the relative positions in all the Transformer layers and only character the absolute position as a complementary in the softmax decoding layer. We call this new approach as Enhanced Mask Decoder(EMD). In our empirical studies, we compare these two approaches to incorporate the absolute positions and observe that the new approach in DeBERTa is much better. We conjecture the early introduction of the absolute position will undesirably hamper the model from learning accurate relative positions information. In addition, this new design will enable us to introduce additional information besides positions to the pre-training, which is out of the scope of this paper and will be explored in future.</p><p>Besides natural language understanding(NLU), we further extend DeBERTa for natural language generation (NLG) to verify the impacts of the disentangled attention thoroughly in both settings. To enable the autoregressive generation, we follow <ref type="bibr" target="#b13">(Dong et al., 2019b</ref>) by using a triangular matrix for self-attention and set the upper triangular part of the self-attention mask to ´8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENT</head><p>This section evaluates DeBERTa on various NLP tasks for both NLU and NLG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">MAIN RESULTS ON NLU TASKS</head><p>Following previous papers on BERT, RoBERTa and XLNet, we report results using large and base models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">PERFORMANCE ON LARGE MODELS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>CoLA We pre-train our large models following the setting of BERT <ref type="bibr" target="#b10">(Devlin et al., 2019)</ref>, except that we use the BPE vocabulary as <ref type="bibr" target="#b33">(Radford et al., 2019;</ref><ref type="bibr" target="#b29">Liu et al., 2019c)</ref>. For training data, we use Wikipedia (English Wikipedia dump<ref type="foot" target="#foot_2">2</ref> ; 12GB), BookCorpus <ref type="bibr" target="#b52">(Zhu et al., 2015)</ref>  We evaluate DeBERTa on additional benchmarks: (1) Question Answering: SQuAD v1.1 <ref type="bibr" target="#b35">(Rajpurkar et al., 2016)</ref>, SQuAD v2.0 <ref type="bibr" target="#b36">(Rajpurkar et al., 2018)</ref>, RACE <ref type="bibr" target="#b23">(Lai et al., 2017)</ref>, ReCoRD <ref type="bibr" target="#b51">(Zhang et al., 2018)</ref> and SWAG <ref type="bibr" target="#b50">(Zellers et al., 2018)</ref>; (2) Natural Language Inference: MNLI <ref type="bibr" target="#b48">(Williams et al., 2018)</ref>; and (3) NER: CoNLL-2003. For comparison, we also include Megatron <ref type="bibr" target="#b39">(Shoeybi et al., 2019)</ref> with three different model sizes: Megatron 336M , Megatron 1.3B and Megatron 3.9B , which are trained using the same dataset as RoBERTa. Note that Megatron 336M has a similar model size as other models mentioned above<ref type="foot" target="#foot_5">5</ref> .</p><p>We summarize the results in Table <ref type="table" target="#tab_1">2</ref>. Compared to the previous SOTA models with similar sizes, including BERT, RoBERTa, XLNet and Megatron 336M , DeBERTa consistently outperforms them in all the 7 tasks. Taking RACE as an example, DeBERTa is significantly better than previous SOTA XLNet with an improvement of 1.4% (86.8% vs. 85.4%). Although Megatron 1.3B is 3 times larger than DeBERTa, we observe that DeBERTa can still outperform Megatron 1.3B in three of the four benchmarks. All the results show the superior performance of DeBERTa in various downstream tasks.</p><p>We are confident that DeBERTa can perform even better with a larger model size -we leave it to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">PERFORMANCE ON BASE MODELS</head><p>The setting for base model pre-training is similar to that for large models. The base model structure follows that of the BERT base model, i.e., L " 12, H " 768, A " 12. We use 4 DGX-2 with 64 V100 GPUs to train the base model and it takes about 10 days to finish a single pre-training of 1M training steps with batch size 2048. We train DeBERTa with the same 78G text data, and compare it with RoBERTa and XLNet trained using their 160G text data. For detailed comparison of datasets for pre-training, please refer the Appendix A.2.</p><p>We summarize the results in Table <ref type="table" target="#tab_2">3</ref>. Across all three tasks, DeBERTa consistently surpasses RoBERTa and XLNet, with more improvements than that in large models. For example, on the MNLI in-domain setting (MNLI-m), DeBERTa base obtains 1.2% (88.8% vs. 87.6%) over RoBERTa base , and 2% (88.8% vs. 86.8%) over XLNet base .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">MAIN RESULTS ON GENERATION TASKS</head><p>We further evaluate the DeBERTa model with auto-regressive language model (ARLM) using Wikitext-103 <ref type="bibr" target="#b31">(Merity et al., 2016</ref>  we show that the DeBERTa approach via injecting the absolute positions in the decoder layer is better than the RoBERTa approach (i.e., DeBERTa + AP) on the absolute positions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">MODEL ANALYSIS</head><p>In this section, we first present an ablation study to quantify the relative contributions of different components introduced in DeBERTa. Then, we study the convergence property to characterize the model training efficiency. Due to space limit, we illustrate the difference in attention patterns between DeBERTa and its counterpart RoBERTa in Appendix A.7. We run experiments for analysis using the base model setting where the Wikipedia + Bookcorpus data is used for model pre-training and a model can be pre-trained for 1M steps with batch size 256 in 7 days on a DGX-2 machine with 16 V-100 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">ABLATION STUDY</head><p>To verify our experimental setting, we pre-train the RoBERTa base model from scratch. We call the re-pre-trained RoBERTa RoBERTa-ReImp base . To investigate the relative contributions of different components in DeBERTa, we design three variations:</p><p>• -EMD is the DeBERTa base model without EMD.</p><p>• -C2P is the DeBERTa base model without the content-to-position term ((c) in Eq. 4).</p><p>• -P2C is the DeBERTa base model without the position-to-content term ((b) in Eq. 4). As XLNet also used relative position bias, this model is close to XLNet plus EMD.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>This paper introduces a new model called DeBERTa for large-scale language model pre-training. DeBERTa first proposes the disentangled attention mechanism that represents each word using two vectors that encode its content and position, respectively, to thoroughly capture both contents and relative positions. As an extension to the disentangled attention, DeBERTa incorporates the absolute positions in the decoding layer as a complement to the relative positions. Compare to the strong RoBERTa and XLNet models, the DeBERTa model shows both better pre-training efficient and downstream NLU and NLG task accuracy consistently. For future work, we will explore alternative approaches to combine both relative and absolute position information in pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX</head><p>A ‚ GLUE. The General Language Understanding Evaluation (GLUE) benchmark is a collection of nine natural language understanding (NLU) tasks. As shown in Table <ref type="table" target="#tab_7">6</ref>, it includes question answering <ref type="bibr" target="#b35">(Rajpurkar et al., 2016)</ref>, linguistic acceptability <ref type="bibr" target="#b47">(Warstadt et al., 2018)</ref>, sentiment analysis <ref type="bibr" target="#b41">(Socher et al., 2013</ref>), text similarity <ref type="bibr" target="#b4">(Cer et al., 2017)</ref>, paraphrase detection <ref type="bibr" target="#b11">(Dolan &amp; Brockett, 2005)</ref>, and natural language inference (NLI) <ref type="bibr" target="#b8">(Dagan et al., 2006;</ref><ref type="bibr" target="#b0">Bar-Haim et al., 2006;</ref><ref type="bibr" target="#b14">Giampiccolo et al., 2007;</ref><ref type="bibr" target="#b2">Bentivogli et al., 2009;</ref><ref type="bibr" target="#b25">Levesque et al., 2012;</ref><ref type="bibr" target="#b48">Williams et al., 2018)</ref>. The diversity of the tasks makes GLUE very suitable for evaluating the generalization and robustness of NLU models.</p><p>‚ ReCoRD is a commonsense Question Answering dataset. Each example consists of a news article, drawn from CNN and DailyMail, and a Cloze-style question about the article in which one entity is masked out <ref type="bibr" target="#b51">(Zhang et al., 2018)</ref>.</p><p>‚ RACE is a large-scale machine reading comprehension dataset, collected from English examinations in China, which are designed for middle school and high school students <ref type="bibr" target="#b23">(Lai et al., 2017)</ref>.</p><p>‚ SQuAD v1.1/v2.0 is the Stanford Question Answering Dataset (SQuAD) v1.1 and v2.0 <ref type="bibr" target="#b35">(Rajpurkar et al., 2016;</ref><ref type="bibr">2018)</ref> are popular machine reading comprehension benchmarks. Their passages come from approximately 500 Wikipedia articles and the questions and answers are obtained by crowdsourcing. The SQuAD v2.0 dataset includes unanswerable questions about the same paragraphs.</p><p>‚ SWAG is a large-scale adversarial dataset for the task of grounded commonsense inference, which unifies natural language inference and physically grounded reasoning <ref type="bibr" target="#b50">(Zellers et al., 2018)</ref>. SWAG consists of 113k multiple choice questions about grounded situations.  <ref type="bibr" target="#b29">(Liu et al., 2019c)</ref>, we adopted dynamic data batching. We also include span masking <ref type="bibr" target="#b20">(Joshi et al., 2019)</ref> as the additional masking strategy with the span size up to three. We list the detailed hyperparameters of pre-training in Table <ref type="table">8</ref>. For pre-training, we all use Adam <ref type="bibr" target="#b21">(Kingma &amp; Ba, 2014)</ref> as the optimizer with weight decay <ref type="bibr" target="#b30">(Loshchilov &amp; Hutter, 2018)</ref>. For fine-tuning, even though we can get better and robust results with RAdam <ref type="bibr" target="#b26">(Liu et al., 2019a)</ref> on some tasks, e.g. CoLA, RTE and RACE, we all use Adam <ref type="bibr" target="#b21">(Kingma &amp; Ba, 2014)</ref> as the optimizer for a fair comparison. For fine-tuning, we trained each task with a hyper-parameter search procedure, each run will take about 1-2 hours on a DGX-2 node. All the hyperparameters are presented in Table <ref type="table">9</ref>. The model selection is based on the performance on the task-specific development sets.</p><formula xml:id="formula_8">‚</formula><p>Our code is implemented based on Huggingface Transformers<ref type="foot" target="#foot_9">9</ref> , FairSeq<ref type="foot" target="#foot_10">10</ref> and Megatron shoeybi2019megatron<ref type="foot" target="#foot_11">11</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 HANDLING LONG SEQUENCE INPUT</head><p>With relative position bias, we choose to truncate the maximum relative distance to k as in equation 3. Thus in each layer, each token can attend directly to at most 2pk ´1q tokens and itself. By stacking Transformer layers, each token in the l´th layer can attend to at most p2k ´1ql tokens implicitly. Taking DeBERTa large as an example, where k " 512, L " 24, in theory, the maximum sequence length that can be handled is 24528. This is a byproduct benefit of our design choice and we found it is beneficial for the RACE task. A comparison of long sequence effect on the RACE task is shown in table <ref type="table" target="#tab_0">10</ref>.</p><p>Long sequence handling is an active research area as of late, there are a lot of works built on the Transformer to optimize its performance on long sequence handling <ref type="bibr" target="#b1">(Beltagy et al., 2020;</ref><ref type="bibr" target="#b22">Kitaev et al., 2020;</ref><ref type="bibr" target="#b6">Child et al., 2019;</ref><ref type="bibr" target="#b9">Dai et al., 2019)</ref>. One of the potential future works is to extend DeBERTa to deal with extremely long sequences and compare it with existing approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 MODEL COMPLEXITY</head><p>With Disentangled Attention, we introduced three additional parameters W q,r , W k,r P R dˆd and P P R 2kˆd . The total increase in parameter is 2L ˆd2 `2k ˆd. For the large model  duces about a 30% increase in computation. Compared with XLNet which also uses relative position embedding, the actual computation cost is about 15%. A further optimization by fusing the attention computation kernel could significantly reduce this additional cost. For EM D, since the decoder in pre-training only reconstructs the masked tokens, it does not introduce additional computation for unmasked tokens. In the situation where 15% tokens are masked and we use only two decoder layers, the additional cost is 0.15 ˆ2{L which results in an additional computational cost of only 3% for base model(L " 12) and 2% for large model(L " 24) in EM D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 DETAIL OF ENHANCED MASK DECODER</head><p>The structure of EM D is shown in figure <ref type="figure" target="#fig_2">2b</ref>. There are two inputs for EM D, i.e. I, H. H denotes the hidden states from the previous transformer layer, and I indicates the input for decoding which can be any necessary information for decoding, e.g., H, absolute position embedding or output from previous EM D layer. n denotes n stacked layers of EM D where the output of each EM D layer will be the input I for next EM D layer and the output of last EM D layer will be feed to language model head directly. The n layers can share the same weight. In our experiment we share the same weight for n " 2 layers to save parameters and use absolute position embedding as I of the first EM D layer. When I " H and n " 1, EM D is the same as BERT decoder layer. However, EM D is more general and flexible as it can take more input information for the decoding task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7 ATTENTION PATTERNS</head><p>To understand why DeBERTa performs differently from RoBERTa, we present their attention patterns in the last self-attention layer in Figure <ref type="figure" target="#fig_3">3</ref>, where we also depict the attention patterns of the three DeBERTa variants for comparison. Comparing RoBERTa with DeBERTa, we observe two obvious differences. First, RoBERTa has a clear diagonal line effect for a token to attend to itself, which is not observed in DeBERTa. This could be attributed to the use of EMD, in which the vectors of the masked but unchanged tokens are replaced with their position embeddings. This seems to be verified by examining the attention pattern of DeBERTa-EMD, where the diagonal line effect is brighter than the original DeBERTa. Second, there are vertical strips in the attention patterns of RoBERTa, which are mainly caused by high-frequent functional tokens (e.g., "a", "the", or punctuation). For DeBERTa, the strip appears in the first column, which represents the [CLS] token. We conjecture that a dominant emphasis on the [CLS] token is desirable for a good pre-trained model since the vector of this token is often used as a contextual representation of the entire input sequence in downstream tasks. We also observe that the vertical strip effect is quite obvious in the patterns of the three DeBERTa variants.</p><p>We provide three more examples to illustrate the difference in attention patterns between DeBERTa and RoBERTa as shown Figure <ref type="figure" target="#fig_4">4</ref>, 5.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>4.3.2 PRE-TRAINING EFFICIENCYTo investigate the convergence of model pre-training, we plot the performance of fine-tuned downstream tasks as a function of the number of pre-training steps. As shown in Figure1, for the RoBERTa-ReImp base model and the DeBERTa base model, we dump a checkpoint every 150K pre-training steps, and then fine-tune the checkpoint on two representative downstream tasks (MNLI and SQuAD v2.0) and then report the accuracy and F1 score, respectively. As a reference, we copy the final model performance of both the original RoBERTa base models<ref type="bibr" target="#b29">(Liu et al., 2019c)</ref> and XLNet base models<ref type="bibr" target="#b49">(Yang et al., 2019)</ref> and plot them as flat dot lines. The results show that DeBERTa consistently outperforms RoBERTa-ReImp during the course of pre-training, and converges faster to the performance of RoBERTa.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 1: Pre-training performance curve between DeBERTa and its counterparts on the MNLI and SQuAD v2.0 development set.</figDesc><graphic url="image-1.png" coords="8,110.49,417.86,194.04,122.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Comparison of the decoding layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Comparison of attention patterns of the last layer among DeBERTa, RoBERTa and DeBERTa variants (i.e., DeBERTa without EMD, C2P and P2C respectively).</figDesc><graphic url="image-3.png" coords="17,108.00,81.86,396.01,84.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Comparison on attention patterns of the last layer between DeBERTa and RoBERTa.</figDesc><graphic url="image-6.png" coords="18,177.30,464.87,257.40,105.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Comparison on attention patterns of last layer between DeBERTa and its variants (i.e. DeBERTa without EMD, C2P and P2C respectively).</figDesc><graphic url="image-9.png" coords="19,111.96,330.73,388.09,102.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison results on the GLUE development set.</figDesc><table><row><cell></cell><cell>Mcc F1/Acc</cell><cell>Acc</cell><cell>Acc</cell><cell>Corr</cell><cell>Acc Acc</cell><cell>Acc</cell></row><row><cell>BERT large</cell><cell>60.6 91.3</cell><cell>86.6/-</cell><cell>93.2</cell><cell>90.0</cell><cell cols="2">92.3 70.4 88.0 84.05</cell></row><row><cell>RoBERTa large</cell><cell>68.0 92.2</cell><cell>90.2/90.2</cell><cell>96.4</cell><cell>92.4</cell><cell cols="2">93.9 86.6 90.9 88.82</cell></row><row><cell>XLNet large</cell><cell>69.0 92.3</cell><cell>90.8/90.8</cell><cell>97.0</cell><cell>92.5</cell><cell cols="2">94.9 85.9 90.8 89.15</cell></row><row><cell cols="2">ALBERT xxlarge 71.4 92.2</cell><cell>90.8/-</cell><cell>96.9</cell><cell>93.0</cell><cell cols="2">95.3 89.2 90.9 89.96</cell></row><row><cell>ELECTRA large</cell><cell>69.1 92.4</cell><cell>90.9/-</cell><cell>96.9</cell><cell>92.6</cell><cell cols="2">95.0 88.0 90.8 89.46</cell></row><row><cell>DeBERTa large</cell><cell>70.5 92.3</cell><cell>91.1/91.1</cell><cell>96.8</cell><cell>92.8</cell><cell cols="2">95.3 88.3 91.9 90.00</cell></row></table><note>QQP MNLI-m/mm SST-2 STS-B QNLI RTE MRPC Avg.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>(6GB), OPENWEBTEXT (public Reddit content<ref type="bibr" target="#b15">(Gokaslan &amp; Cohen, 2019)</ref>; 38GB), and STORIES (a subset of CommonCrawl<ref type="bibr" target="#b43">(Trinh &amp; Le, 2018</ref>); 31GB). The total data size after data deduplication<ref type="bibr" target="#b39">(Shoeybi et al., 2019)</ref> is about 78GB. We report the details of pre-trained dataset in Appendix A.2. We use 6 DGX-2 machines with 96 V100 GPUs to train the model. A single model trained with 2K batch size and 1M steps takes about 20 days. Refer to Appendix A for the detailed hyperparamters.We summarize the results on eight GLUE<ref type="bibr" target="#b45">(Wang et al., 2018)</ref> tasks in Table 1, which compares DeBERTa with previous models with around 350M parameters: BERT, RoBERTa, XLNet, ALBERT and ELECTRA. Note that RoBERTa, XLNet, ALBERT 3 and ELECTRA use 160G training data while DeBERTa uses 78G training data. RoBERTa and XLNet are trained for 500K steps with 8K samples in a step, which amounts to four billion passes over training samples. We train DeBERTa for one million steps with 2K samples in each step. This amounts to two billion passes of its training samples, approximately half of either RoBERTa or XLNet. Table 1 shows that compared to BERT and ALBERT xxlarge 4 and ELECTRA large , DeBERTa still outperforms them in term of the average "GLUE" score. Note that MNLI is often used as an indicative task to monitor the progress of pre-training. DeBERTa significantly outperforms all existing models of similar size on MNLI and creates a new state-of-the-art (SOTA). Results on MNLI in/out-domain, SQuAD v1.1, SQuAD v2.0, RACE, ReCoRD, SWAG, CoNLL 2003 NER development set. Note that missing results in literature are signified by "-".</figDesc><table><row><cell>Model</cell><cell cols="7">MNLI-m/mm SQuAD v1.1 SQuAD v2.0 RACE ReCoRD SWAG NER Acc F1/EM F1/EM Acc F1/EM Acc F1</cell></row><row><cell>BERT large</cell><cell>86.6/-</cell><cell>90.9/84.1</cell><cell>81.8/79.0</cell><cell>72.0</cell><cell>-</cell><cell cols="2">86.6 92.8</cell></row><row><cell cols="2">RoBERTa large 90.2/90.2</cell><cell>94.6/88.9</cell><cell>89.4/86.5</cell><cell cols="4">83.2 90.6/90.0 89.9 93.4</cell></row><row><cell>XLNet large</cell><cell>90.8/90.8</cell><cell>95.1/89.7</cell><cell>90.6/87.9</cell><cell>85.4</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Megatron 336M</cell><cell>89.7/90.0</cell><cell>94.2/88.0</cell><cell>88.1/84.8</cell><cell>83.0</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">DeBERTa large 91.1/91.1</cell><cell>95.5/90.1</cell><cell>90.7/88.0</cell><cell cols="4">86.8 91.4/91.0 90.8 93.8</cell></row><row><cell>Megatron 1.3B</cell><cell>90.9/91.0</cell><cell>94.9/89.1</cell><cell>90.2/87.1</cell><cell>87.3</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Megatron 3.9B</cell><cell>91.4/91.4</cell><cell>95.5/90.0</cell><cell>91.2/88.5</cell><cell>89.5</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table><note>RoBERTa, DeBERTa is consistently better across all the tasks. Meanwhile, DeBERTa outperforms XLNet in six out of eight tasks. Particularly, the improvements on MRPC (1.1% over XLNet and 1.0% over RoBERTa), RTE (2.4% over XLNet and 1.7% over RoBERTa) and CoLA (1.5% over XLNet and 2.5% over RoBERTa) are significant. Even compared to the SOTA pre-trained models,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results on MNLI in/out-domain (m/mm), SQuAD v1.1 and v2.0 development set. and ARLM as in UniLM (Dong et al., 2019b). The training hyper-parameters are the same as DeBERTa base except we use less training steps (200k). For fair comparison, we use RoBERTa as our baseline in the same setting. At last, we include GPT-2 and Transformer-XL for references. All of those models use the base model settings. DeBERTa+AP denotes the DeBERTa model trained without EM D but adding the absolute position embedding into the input layer as RoBERTa.</figDesc><table><row><cell>Model</cell><cell></cell><cell cols="5">MNLI-m/mm (Acc) SQuAD v1.1 (F1/EM) SQuAD v2.0 (F1/EM)</cell></row><row><cell cols="2">RoBERTa base</cell><cell>87.6/-</cell><cell cols="2">91.5/84.6</cell><cell></cell><cell>83.7/80.5</cell></row><row><cell cols="2">XLNet base</cell><cell>86.8/-</cell><cell>-/-</cell><cell></cell><cell></cell><cell>-/80.2</cell></row><row><cell cols="2">DeBERTa base</cell><cell>88.8/88.5</cell><cell cols="2">93.1/87.2</cell><cell></cell><cell>86.2/83.1</cell></row><row><cell>Model</cell><cell cols="6">RoBERTa DeBERTa+AP DeBERTa DeBERTa-MT GPT-2 Transformer-XL</cell></row><row><cell>Dev PPL</cell><cell>21.6</cell><cell>20.7</cell><cell>20.5</cell><cell>19.5</cell><cell>-</cell><cell>23.1</cell></row><row><cell>Test PPL</cell><cell>21.6</cell><cell>20.0</cell><cell>19.9</cell><cell>19.5</cell><cell>37.50</cell><cell>24</cell></row></table><note>). DeBERTa-MT denotes our model trained jointly with MLM</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Language model results on Wikitext-103 .</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc>reports the results on Wikitext-103. Note that DeBERTa base obtains a better PPL on both dev and test, and the joint training of MLM and ARLM reduces PPL further, showing the effectiveness of DeBERTa. Moreover, as a comparison between different places to incorporate the absolute positions,</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5</head><label>5</label><figDesc>summarizes the results on four benchmark datasets. First, comparing RoBERTa with RoBERTa-ReImp, we observe that they perform similarly across all the four benchmark datasets. Thus, we can confidently treat RoBERTa-ReImp as a solid baseline for comparison. Second, we see that removing any one component in DeBERTa results in a sheer performance drop in all the</figDesc><table><row><cell>Model</cell><cell cols="4">MNLI-m/mm SQuAD v1.1 SQuAD v2.0 RACE Acc F1/EM F1/EM Acc</cell></row><row><cell>BERT base Devlin et al. (2019)</cell><cell>84.3/84.7</cell><cell>88.5/81.0</cell><cell>76.3/73.7</cell><cell>65.0</cell></row><row><cell>RoBERTa base Liu et al. (2019c)</cell><cell>84.7/-</cell><cell>90.6/-</cell><cell>79.7/-</cell><cell>65.6</cell></row><row><cell>XLNet base Yang et al. (2019)</cell><cell>85.8/85.4</cell><cell>-/-</cell><cell>81.3/78.5</cell><cell>66.7</cell></row><row><cell>RoBERTa-ReImp base</cell><cell>84.9/85.1</cell><cell>91.1/84.8</cell><cell>79.5/76.0</cell><cell>66.8</cell></row><row><cell>DeBERTa base</cell><cell>86.3/86.2</cell><cell>92.1/86.1</cell><cell>82.5/79.3</cell><cell>71.7</cell></row><row><cell>-EMD</cell><cell>86.1/86.1</cell><cell>91.8/85.8</cell><cell>81.3/78.0</cell><cell>70.3</cell></row><row><cell>-C2P</cell><cell>85.9/85.7</cell><cell>91.6/85.8</cell><cell>81.3/78.3</cell><cell>69.3</cell></row><row><cell>-P2C</cell><cell>86.0/85.8</cell><cell>91.7/85.7</cell><cell>80.8/77.6</cell><cell>69.6</cell></row><row><cell>-(EMD+C2P)</cell><cell>85.8/85.9</cell><cell>91.5/85.3</cell><cell>80.3/77.2</cell><cell>68.1</cell></row><row><cell>-(EMD+P2C)</cell><cell>85.8/85.8</cell><cell>91.3/85.1</cell><cell>80.2/77.1</cell><cell>68.5</cell></row></table><note>benchmarks. For instance, removing EMD (-EMD) results in a loss of 1.4% (71.7% vs. 70.3%) on RACE, 0.3% (92.1% vs. 91.8%) on SQuAD v1.1, 1.2% (82.5% vs. 81.3%) on SQuAD v2.0, 0.2% (86.3% vs. 86.1%) and 0.1% (86.2% vs. 86.1%) on MNLI-m/mm, respectively. Similarly, removing either content-to-position or position-to-content leads to consistent performance drops in all the benchmarks. As expected, removing two components results in even more significant deterioration in performance.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Ablation study of the DeBERTa base model.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>.1 DATASET Summary information of the NLP application benchmarks.</figDesc><table><row><cell>Corpus</cell><cell>Task</cell><cell cols="4">#Train #Dev #Test #Label</cell><cell>Metrics</cell></row><row><cell></cell><cell cols="5">General Language Understanding Evaluation (GLUE)</cell><cell></cell></row><row><cell>CoLA</cell><cell>Acceptability</cell><cell>8.5k</cell><cell>1k</cell><cell>1k</cell><cell>2</cell><cell>Matthews corr</cell></row><row><cell>SST</cell><cell>Sentiment</cell><cell>67k</cell><cell>872</cell><cell>1.8k</cell><cell>2</cell><cell>Accuracy</cell></row><row><cell>MNLI</cell><cell>NLI</cell><cell>393k</cell><cell>20k</cell><cell>20k</cell><cell>3</cell><cell>Accuracy</cell></row><row><cell>RTE</cell><cell>NLI</cell><cell>2.5k</cell><cell>276</cell><cell>3k</cell><cell>2</cell><cell>Accuracy</cell></row><row><cell>WNLI</cell><cell>NLI</cell><cell>634</cell><cell>71</cell><cell>146</cell><cell>2</cell><cell>Accuracy</cell></row><row><cell>QQP</cell><cell>Paraphrase</cell><cell>364k</cell><cell>40k</cell><cell>391k</cell><cell>2</cell><cell>Accuracy/F1</cell></row><row><cell>MRPC</cell><cell>Paraphrase</cell><cell>3.7k</cell><cell>408</cell><cell>1.7k</cell><cell>2</cell><cell>Accuracy/F1</cell></row><row><cell>QNLI</cell><cell>QA/NLI</cell><cell>108k</cell><cell>5.7k</cell><cell>5.7k</cell><cell>2</cell><cell>Accuracy</cell></row><row><cell>STS-B</cell><cell>Similarity</cell><cell>7k</cell><cell>1.5k</cell><cell>1.4k</cell><cell>1</cell><cell>Pearson/Spearman corr</cell></row><row><cell></cell><cell></cell><cell cols="2">Question Answering</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">SQuAD v1.1 MRC</cell><cell cols="3">87.6k 10.5k 9.5k</cell><cell>-</cell><cell>Exact Match (EM)/F1</cell></row><row><cell cols="2">SQuAD v2.0 MRC</cell><cell cols="3">130.3k 11.9k 8.9k</cell><cell>-</cell><cell>Exact Match (EM)/F1</cell></row><row><cell>ReCoRD</cell><cell>MRC</cell><cell>101k</cell><cell>10k</cell><cell>10k</cell><cell>-</cell><cell>Exact Match (EM)/F1</cell></row><row><cell>RACE</cell><cell>MRC</cell><cell cols="3">87,866 4,887 4,934</cell><cell>4</cell><cell>Accuracy</cell></row><row><cell>SWAG</cell><cell cols="2">Multiple choice 73.5k</cell><cell>20k</cell><cell>20k</cell><cell>4</cell><cell>Accuracy</cell></row><row><cell></cell><cell></cell><cell cols="2">Token Classification</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">CoNLL 2003 NER</cell><cell cols="3">14,987 3,466 3,684</cell><cell>8</cell><cell>F1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>CoNLL 2003 is an English dataset consisting of text from a wide variety of sources. It has 4 types of named entity.A.2 PRE-TRAINING DATASETFor DeBERTa pre-training, we use Wikipedia (English Wikipedia dump 6 ; 12GB), BookCorpus<ref type="bibr" target="#b52">(Zhu et al., 2015)</ref> 7 (6GB), OPENWEBTEXT (public Reddit content<ref type="bibr" target="#b15">(Gokaslan &amp; Cohen, 2019)</ref>; 38GB) and STORIES 8 (a subset of CommonCrawl (Trinh &amp; Le, 2018); 31GB). The total data size after data deduplication<ref type="bibr" target="#b39">(Shoeybi et al., 2019)</ref> is about 78GB. For pre-training, we also sample 5% training data as the validation set to monitor the training process. Table 7 compares datasets used in different pre-trained models. Comparison of the pre-training data.</figDesc><table><row><cell>Model</cell><cell cols="5">Wiki+Book OpenWebText Stories CC-News Giga5 ClueWeb Common Crawl</cell></row><row><cell></cell><cell>16GB</cell><cell>38GB</cell><cell>31GB</cell><cell>76GB 16GB 19GB</cell><cell>110GB</cell></row><row><cell>BERT</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>XLNet</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RoBERTa</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DeBERTa</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">A.3 IMPLEMENTATION DETAILS</cell><cell></cell><cell></cell></row><row><cell cols="2">Following RoBERTa</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 11 :</head><label>11</label><figDesc>Abluation study of the additional parameters in the DeBERTa base model.</figDesc><table><row><cell>Model</cell><cell></cell><cell></cell><cell cols="3">Parameters MNLI-m/mm SQuAD v1.1 SQuAD v2.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Acc</cell><cell>F1/EM</cell><cell>F1/EM</cell></row><row><cell cols="2">RoBERTa-ReImp base</cell><cell></cell><cell>120M</cell><cell>84.9/85.1</cell><cell>91.1/84.8</cell><cell>79.5/76.0</cell></row><row><cell cols="2">DeBERTa base</cell><cell></cell><cell>134M</cell><cell>86.3/86.2</cell><cell>92.1/86.1</cell><cell>82.5/79.3</cell></row><row><cell cols="3">DeBERTa base +ShareProjection</cell><cell>120M</cell><cell>86.3/86.3</cell><cell>92.2/86.2</cell><cell>82.3/79.5</cell></row><row><cell cols="2">Language Model Head</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Transformer Layer</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Q</cell><cell>K</cell><cell>V</cell><cell></cell><cell></cell></row><row><cell></cell><cell>H</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">In this sense, our model shares some similarity to Tensor Product Representation<ref type="bibr" target="#b40">(Smolensky, 1990;</ref><ref type="bibr" target="#b37">Schlag et al.,</ref> </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2019" xml:id="foot_1">;<ref type="bibr" target="#b5">Chen et al., 2019)</ref> where a word is represented using a tensor product of its filler (content) vector and its role (position) vector.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2">https://dumps.wikimedia.org/enwiki/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3">ALBERT includes an additional sentence-order prediction task.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4">The hidden dimension of ALBERTxxlarge is 4 times of DeBERTa and the computation cost is about 4 times of DeBERTa.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_5">Although T5<ref type="bibr" target="#b34">(Raffel et al., 2019)</ref> has more parameters (11B), it only reports the test results and it is not comparable with other models.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_6">https://dumps.wikimedia.org/enwiki/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_7">https://github.com/butsugiri/homemade_bookcorpus</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_8">https://github.com/tensorflow/models/tree/master/research/lm_commonsense</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_9">https://github.com/huggingface/transformers</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_10">https://github.com/pytorch/fairseq</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_11">https://github.com/NVIDIA/Megatron-LM</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>pd " 1024, L " 24, k " 512q, this introduces about 49M additional parameters, which is an increment of 13%. For the base modelpd " 768, L " 12, k " 512q, this introduces about 14M additional parameters, which is an increment of 12%. However, by sharing the projection matrix between content and position embedding, i.e. W q,r " W q,c , W k,r " W k,c , the number of parameters of DeBERTa will be the same as RoBERTa. Our experiment on base model shows that the results are almost the same. The results are shown in table 11. Due to computation resource limitation, we didn't run this setting with large model and we plan to re-run it in the future with this setting.</p><p>The additional computational complexity is OpN kdq due to the calculation of the additional positionto-content and content-to-position attention scores. Compared with BERT or RoBERTa, this intro-</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The second PASCAL recognising textual entailment challenge</title>
		<author>
			<persName><forename type="first">Roy</forename><surname>Bar-Haim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment</title>
				<meeting>the Second PASCAL Challenges Workshop on Recognising Textual Entailment</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">01</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Longformer: The long-document transformer</title>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The fifth pascal recognizing textual entailment challenge</title>
		<author>
			<persName><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoa</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Trang</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
		<idno>TAC&apos;09</idno>
	</analytic>
	<monogr>
		<title level="m">Proc Text Analysis Conference</title>
				<meeting>Text Analysis Conference</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inigo</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.00055</idno>
		<title level="m">Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Kezhen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiuyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamid</forename><surname>Palangi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Smolensky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><forename type="middle">D</forename><surname>Forbus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.02339</idno>
		<title level="m">Natural-to formal-language generation using tensor product representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><surname>Electra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10555</idno>
		<title level="m">Pre-training text encoders as discriminators rather than generators</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The pascal recognising textual entailment challenge</title>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
		<idno type="DOI">10.1007/11736790_9</idno>
		<ptr target="http://dx.doi.org/10.1007/11736790_9" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First International Conference on Machine Learning Challenges: Evaluating Predictive Uncertainty Visual Object Classification, and Recognizing Textual Entailment, MLCW&apos;05</title>
				<meeting>the First International Conference on Machine Learning Challenges: Evaluating Predictive Uncertainty Visual Object Classification, and Recognizing Textual Entailment, MLCW&apos;05<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="177" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2978" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automatically constructing a corpus of sentential paraphrases</title>
		<author>
			<persName><forename type="first">B</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><surname>Brockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Workshop on Paraphrasing (IWP2005)</title>
				<meeting>the Third International Workshop on Paraphrasing (IWP2005)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unified language model pre-training for natural language understanding and generation</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiao-Wuen</forename><surname>Hon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019a</date>
			<biblScope unit="page" from="13042" to="13054" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Unified language model pre-training for natural language understanding and generation</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiao-Wuen</forename><surname>Hon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.03197</idno>
		<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The third PASCAL recognizing textual entailment challenge</title>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/W07-1401" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing</title>
				<meeting>the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing<address><addrLine>Prague</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007-06">June 2007</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Gokaslan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vanya</forename><surname>Cohen</surname></persName>
		</author>
		<ptr target="http://Skylion007.github.io/OpenWebTextCorpus" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Openwebtext corpus</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11983</idno>
		<title level="m">A hybrid neural network model for commonsense reasoning</title>
				<imprint>
			<date type="published" when="2019">2019a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">X-sql: reinforce schema representation with context</title>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaushik</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08113</idno>
		<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Music transformer: Generating music with long-term structure</title>
		<author>
			<persName><forename type="first">Cheng-Zhi Anna</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Curtis</forename><surname>Hawthorne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monica</forename><surname>Dinculescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Eck</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Smart: Robust and efficient fine-tuning for pre-trained natural language models through principled regularized optimization</title>
		<author>
			<persName><forename type="first">Haoming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuo</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.03437</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Spanbert: Improving pre-training by representing and predicting spans</title>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Daniel S Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><surname>Levy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10529</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.04451</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Race: Large-scale reading comprehension dataset from examinations</title>
		<author>
			<persName><forename type="first">Guokun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11942</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The winograd schema challenge</title>
		<author>
			<persName><forename type="first">Hector</forename><surname>Levesque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ernest</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leora</forename><surname>Morgenstern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03265</idno>
		<title level="m">On the variance of the adaptive learning rate and beyond</title>
				<imprint>
			<date type="published" when="2019">2019a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-task deep neural networks for natural language understanding</title>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/P19-1441" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-07">July 2019b</date>
			<biblScope unit="page" from="4487" to="4496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08994</idno>
		<title level="m">Adversarial training for large neural language models</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
				<imprint>
			<date type="published" when="2019">2019c</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Fixing weight decay regularization in adam</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<title level="m">Pointer sentinel mixture models. arXiv</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">1609</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Shervin</forename><surname>Minaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Narjes</forename><surname>Nikzad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meysam</forename><surname>Chenaghlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.03705</idno>
		<title level="m">Deep learning based text classification: A comprehensive review</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI Blog</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1264</idno>
		<ptr target="https://www.aclweb.org/anthology/D16-1264" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016-11">November 2016</date>
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Know what you don&apos;t know: Unanswerable questions for squad</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03822</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Enhancing the transformer with explicit relational encoding for math problem solving</title>
		<author>
			<persName><forename type="first">Imanol</forename><surname>Schlag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Smolensky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nebojsa</forename><surname>Jojic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.06611</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Self-attention with relative position representations</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.14224</idno>
	</analytic>
	<monogr>
		<title level="m">Exploiting structured knowledge in text via graph-guided representation learning</title>
		<title level="s">the Association for Computational Linguistics: Human Language Technologies</title>
		<editor>
			<persName><forename type="first">Tao</forename><surname>Shen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yi</forename><surname>Mao</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2018">2018. 2020</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="464" to="468" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of the 2018 Conference of the North American Chapter</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08053</idno>
		<title level="m">Megatron-lm: Training multi-billion parameter language models using gpu model parallelism</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Tensor product variable binding and the representation of symbolic structures in connectionist systems</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Smolensky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="159" to="216" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 conference on empirical methods in natural language processing</title>
				<meeting>the 2013 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shikun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danxiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Hao Tian</surname></persName>
		</author>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09223</idno>
		<title level="m">Ernie: Enhanced representation through knowledge integration</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">A simple method for commonsense reasoning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Trieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.02847</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><surname>Samuel R Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07461</idno>
		<title level="m">Glue: A multi-task benchmark and analysis platform for natural language understanding</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Structbert: Incorporating language structures into pre-training for deep language understanding</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zuyi</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.04577</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Alex</forename><surname>Warstadt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><surname>Samuel R Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.12471</idno>
		<title level="m">Neural network acceptability judgments</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/N18-1101" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1112" to="1122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5754" to="5764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Swag: A large-scale adversarial dataset for grounded commonsense inference</title>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="93" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.12885</idno>
		<title level="m">Record: Bridging the gap between human and machine commonsense reading comprehension</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
				<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
