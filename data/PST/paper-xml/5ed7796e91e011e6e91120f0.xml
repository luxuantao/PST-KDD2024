<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TIMME: Twitter Ideology-detection via Multi-task Multi-relational Embedding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-06-18">18 Jun 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhiping</forename><surname>Xiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">CS Department</orgName>
								<orgName type="institution">UCLA</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Weiping</forename><surname>Song</surname></persName>
							<email>weiping.song@pku.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">CS Department</orgName>
								<orgName type="department" key="dep2">School of EECS</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haoyan</forename><surname>Xu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">College of Control Science and Engineering</orgName>
								<orgName type="institution">Zhejiang University Hangzhou</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhicheng</forename><surname>Ren</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">CS Department</orgName>
								<orgName type="institution">UCLA</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
							<email>yzsun@cs.ucla.edu</email>
							<affiliation key="aff4">
								<orgName type="department">CS Department</orgName>
								<orgName type="institution">UCLA</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TIMME: Twitter Ideology-detection via Multi-task Multi-relational Embedding</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-06-18">18 Jun 2020</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3394486.3403275</idno>
					<idno type="arXiv">arXiv:2006.01321v3[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>multi-task learning</term>
					<term>ideology detection</term>
					<term>heterogeneous information network</term>
					<term>social network analysis</term>
					<term>graph convolutional networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We aim at solving the problem of predicting people's ideology, or political tendency. We estimate it by using Twitter data, and formalize it as a classification problem. Ideology-detection has long been a challenging yet important problem. Certain groups, such as the policy makers, rely on it to make wise decisions. Back in the old days when labor-intensive survey-studies were needed to collect public opinions, analyzing ordinary citizens' political tendencies was uneasy. The rise of social medias, such as Twitter, has enabled us to gather ordinary citizen's data easily. However, the incompleteness of the labels and the features in social network datasets is tricky, not to mention the enormous data size and the heterogeneousity. The data differ dramatically from many commonly-used datasets, thus brings unique challenges. In our work, first we built our own datasets from Twitter. Next, we proposed TIMME, a multitask multi-relational embedding model, that works efficiently on sparsely-labeled heterogeneous real-world dataset. It could also handle the incompleteness of the input features. Experimental results showed that TIMME is overall better than the state-of-the-art models for ideology detection on Twitter. Our findings include: links can lead to good classification outcomes without text; conservative voice is under-represented on Twitter; follow is the most important relation to predict ideology; retweet and mention enhance a higher chance of like, etc. Last but not least, TIMME could be extended to other datasets and tasks in theory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>? Computing methodologies ? Multi-task learning; Neural networks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Studies on ideology never fails to attract people's interests. Ideology here refers to the political stance or tendency of people, often reflected as left-or right-leaning. Measuring the politicians' ideology helps predict some important decisions' final outcomes, but it does not provide more insights into ordinary citizens' views, which are also of decisive significance. Decades ago, social scientists have already started using probabilistic models to study the voting behaviors of the politicians. But seldom did they study the mass population's opinions, for the survey-based study is extremely labor-intensive and hard-to-scale <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b27">27]</ref>. The booming development of social networks in the recent years shed light on detecting ordinary people's ideology. In social networks, people are more relaxed than in an offline interview, and behave naturally. Social networks, in return, has shaped people's habits, giving rise to opinion leaders, encouraging youngsters' political involvement <ref type="bibr" target="#b25">[25]</ref>.</p><p>Most existing approaches of ideology detection on social networks focus on text <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref>. Most of their methodologies based on probabilistic models, following the long-lasting tradition started by social scientists. Some others <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b29">29]</ref> noticed the advantages of neural networks, but seldom do they focus on links. We will show that the social-network links' contribution to ideology detection has been under-estimated.</p><p>An intuitive explanation of how links could be telling is illustrated in Figure <ref type="figure">1</ref>. Different types of links come into being for different reasons. We have five relation types among users on Twitter today: follow, retweet, reply, mention, like, and the relations affect each other. For instance, after Rosa retweet from Derica and mention her, Derica reply to her; when Isabel mention some politicians in her posts, the politician's followers might come to interact with her. One might mention or reply to debate, but like always stands for agreement. The relations could reflect some opinions that a user would never tell you verbally. Words could be easily disguised, and there is always a problem called "the silent majority", for most people are unwilling to express. Yet there are some uniqueness of Twitter dataset, bringing about many challenges. It is especially the case when existing approaches are mostly dealing with smaller datasets with much sparser links than ours, such as academic graphs, text-word graphs, and knowledgegraphs. First, our Twitter dataset is large and the links are relatively dense (Section 4). Some models such as GraphSAGE <ref type="bibr" target="#b13">[14]</ref> will be super slow sampling our graph. Second, labels are extremely sparse, less than 1%. Most approaches will suffer from severe over-fitting, and the lack of reliable evaluation. Third, features are always incomplete, for in real-life datasets like Twitter, many accounts are removed or blocked. Fourth, modeling the heterogeneity is nontrivial. Many existing methods designed for homogeneous networks tend to ignore the information brought by the types of links.</p><p>Existing works can not address the above challenges well. Even though some realized the importance of links <ref type="bibr">[9,</ref><ref type="bibr" target="#b12">13]</ref>, they failed to provide an embedding. Most people learn an embedding by separating the heterogeneous graph into different homogeneous views entirely, and combine them in the very end.</p><p>We propose to solve the above-listed problems by TIMME (Twitter Ideology-detection via Multi-task Multi-relational Embedding), a model good at handling sparsely-labeled large graph, utilizing multiple relation types, and optionally dealing with missing features. Our code with data is released on Github at https://github. com/PatriciaXiao/TIMME. Our major contributions are: ? We propose TIMME for ideology detection on Twitter, whose encoder captures the interactions between different relations, and decoder treats different relations separately while measuring the importance of each relation to ideology detection. ? The experimental results have proved that TIMME outperforms the state-of-the-art models. Case studies showed that conservative voice is typically under-represented on Twitter. There are also many findings on the relations' interactions. ? The large-scale dataset we crawled, cleaned, and labeled (Appendix A) provides a new benchmark to study heterogeneous information networks.</p><p>In this paper, we will walk through the related work in Section 2, introduce the preliminaries and the definition of the problem we are working on in Section 3, followed by the details of the model we propose in Section 4, experimental results and discussions in Section 5, and Section 6 for conclusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Ideology Detection</head><p>Ideology detection in general could be naturally divided into two directions, based on the targets to predict: of the politicians <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b28">28]</ref>, and of the ordinary citizens <ref type="bibr">[1, 2, 5, 8, 13, 15-17, 20, 23, 29]</ref>. The work conducted on ordinary citizens could also be categorized into two types according to the source of data being used: intentionally collected via strategies like survey <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b20">20]</ref>, and directly collected such as from news articles <ref type="bibr" target="#b1">[2]</ref> or from social networks <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17]</ref>. Some studies take advantages from both sides, asking self-reported responses from a group of users selected from social networks <ref type="bibr" target="#b29">[29]</ref>, and some researchers admitted the limitations of survey experiments <ref type="bibr" target="#b23">[23]</ref>. Emerging from social science, probabilistic models have been widely used for such kinds of analysis since the early 1980s <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b28">28]</ref>. On the other hand, on social network datasets, it is quite intuitive trying to extract information from text data to do ideology-detection <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref>, only a few paid attention to links <ref type="bibr">[9,</ref><ref type="bibr" target="#b12">13]</ref>. Our work differs from them all, since: (1) unlike probabilistic models, we use GNN approaches to solve this problem, so that we take advantage of the high-efficient computational resources, and we have the embeddings for further analysis; (2) we focus on relations among users, and proved how telling those relations are.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph Neural Networks (GNN)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Graph Convolutional Networks (GCN).</head><p>Inspired by the great success of convolutional neural networks (CNN), researchers have been seeking for its extension onto information networks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19]</ref> to learn the entities' embeddings. The Graph Convolutional Networks (GCN) <ref type="bibr" target="#b18">[19]</ref> could be regarded as an approximation of spectraldomain convolution of the graph signals. A deeper insight <ref type="bibr" target="#b21">[21]</ref> shows that the key reason why GCN works so well on classification tasks is that its operation is a form of Laplacian smoothing, and concludes the potential over-smoothing problem, as well as emphasizes the harm of the lack of labels.</p><p>GCN convolutional operation could also be viewed as sampling and aggregating of the neighborhood information, such as Graph-SAGE <ref type="bibr" target="#b13">[14]</ref> and FastGCN <ref type="bibr" target="#b3">[4]</ref>, enabling training in batches. To improve GraphSAGE's expressiveness, GIN <ref type="bibr" target="#b40">[40]</ref> is developed, enabling more complex forms of aggregation. In practice, due to the sampling time cost brought by our links' high density, GIN, GraphSAGE and its extension onto heterogeneous information network such as HetGNN <ref type="bibr" target="#b43">[43]</ref> and GATNE <ref type="bibr" target="#b2">[3]</ref> are not very suitable on our datasets.</p><p>The relational-GCN (r-GCN) <ref type="bibr" target="#b32">[32]</ref> extends GCN onto heterogeneous information networks. A very large number of relation-types |R| ends up in overwhelming parameters, thus they put some constraints on the weight matrices, referred to as weight-matrix decomposition. GEM <ref type="bibr" target="#b22">[22]</ref> is almost a special case of r-GCN. Unfortunately, their code is kept confidential. According to the descriptions in their paper, they have a component of similar use as the attention weights ? in our encoder, but it is treated as a free parameter.</p><p>Another way of dealing with multiple link types is well-represented by SHINE <ref type="bibr" target="#b38">[38]</ref>, who treats the heterogeneous types of links as separated homogeneous links, and combines embeddings from all relations in the end. SHINE did not make good use of the multiple relations to its full potential, modeling the relations without allowing complex interactions among them. GTN <ref type="bibr" target="#b42">[42]</ref> is similar with SHINE in splitting the graph into separate views and combining the output at the very end. Besides, GTN uses meta-path, thus is potentially more expressive than SHINE, but would rely heavily on the quality and quantity of the meta-paths being used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Graph Attention</head><p>Networks. Graph Attention Networks (GAT) <ref type="bibr" target="#b36">[36]</ref> is another nontrivial direction to go under the topic of graph neural networks. It incorporates attention into propagation by applying self-attention on the neighbors. Multi-head mechanism is often used to ensure stability.</p><p>An extension of GAT on heterogeneous information networks is Heterogeneous Graph Attention Network, HAN <ref type="bibr" target="#b39">[39]</ref>. Beside inheriting the node-level attention from GAT, it considers different relation types by sampling its neighbors from different meta-paths. It first conducts type-specific transformation and compute the importance of neighbors of each node. After that, it aggregates the coefficients of all neighbor nodes to update the current node's representation. In addition, to obtain more comprehensive information, it conducts semantic-level attention, which takes the result of node-level attention as input and computes the importance of each meta-path. We use HAN as an important baseline in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Multi-Task Learning (MTL)</head><p>In multi-task learning (MTL) settings, there are multiple tasks sharing the same inductive bias jointly trained. Ideally, the performance of every task should benefit from leveraging auxiliary knowledge from each other. As is concluded in an overview <ref type="bibr" target="#b31">[31]</ref>, MTL could be applied with or without neural network structure. On neural network structure, the most common approach is to do hard parameter-sharing, where the tasks share some hidden layers. The most common way of optimizing an MTL problem is to solve it by joint-training fashion, with joint loss computed as a weighted combination of losses from different tasks <ref type="bibr" target="#b17">[18]</ref>. It has a very wide range of applications, such as the DMT-Demographic Models <ref type="bibr" target="#b37">[37]</ref> where multiple aspects of Twitter data (e.g. text, images) are fed into different tasks and trained jointly. Aron and Nirmal et al. <ref type="bibr" target="#b9">[10]</ref> also apply MTL on Twitter, separating the tasks by user categories. Our multi-task design differs from theirs, and treat node classification and link prediction on different relation types as different tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROBLEM DEFINITION</head><p>Our goal is to predict Twitter users' ideologies, by learning the ideology embedding of users in a political-centered social network. Definition 3.1. (Heterogeneous Information Network) Following previous work <ref type="bibr" target="#b34">[34]</ref>, we say that an information network G = {V, E}, where number of vertices is |V | = N , is a heterogeneous information network, when there are |T | = T types of vertices, |R| = R types of edges, and max(T , R) &gt; 1. G could be represented as G = {{V 1 , V 2 , . . . V T }, {E 1 , E 2 , . . . , E R }} Each possible edge from the i th node to the j th , represented as e i j ? E has a weight value w i j &gt; 0 associated to it, where w i j = 0 representing e i j E. In our case, G is a directed graph. In general, we have ?v i , v j ? ?v j , v i ? and w i j w ji .</p><p>Twitter data G T wit t er contains T = 1 type of entities (users), and R = 5 different types of edges (relations) among the entities, namely follow, retweet, like, mention, reply.</p><formula xml:id="formula_0">G T wit t er = {V, {E 1 , E 2 , E 3 , E 4 , E 5 }}</formula><p>Detailed description about Twitter data is included in Appendix A, and we call the subgraph we selected from Twitter-network a political-centered social network, which is defined as follows: Definition 3.2. (Political-Centered Social Network) The politicalcentered social network is a special case of directed heterogeneous information network. With a pre-defined politicians set P, in our selected heterogeneous network G T wit t er , ?e = ?v i , v j ? ? E r where r ? {1, 2, . . . , R}, there has to be either v i ? P or v j ? P. All the politicians in this dataset have ground-truth labels indicating their political stance. The political-centered social networks are represented as G p .</p><p>We would like to leverage the information we have to learn the representation of the users, which could help us reveal their ideologies. Due to the lack of Independent representatives (only two in total), we consider the binary-set labels only: { liberal, conservative }. Democratic on liberal side, Republican for conservative.</p><formula xml:id="formula_1">Definition 3.3. (Multi-task Multi-relational Network Embed- ding) Given a network G p = {V, {E 1 , E 2 , E 3 , E 4 , E 5 }}</formula><p>where the number of nodes is |V | = N , the goal of TIMME is to learn such a representation h i ? R d where d ? N for ?v i ? V, that captures the categorical information of nodes, such as their ideology tendencies. As a measurement, we want the representation H ? R N ?d , to success on both node-classification and link-prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHODOLOGY</head><p>The general architecture of our proposed model is illustrated in Figure <ref type="figure" target="#fig_0">2</ref>. It contains two components: encoder and decoder. The encoder contains two multi-relational convolutional layers. The output of the encoder is passed on to the decoder, who handles the downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Multi-Relation Encoder</head><p>As mentioned before in Section 1, the challenges faced by the encoder part are the large data scale, the heterogeneous link types, and the missing features.</p><p>GCN is very effective in learning the nodes' embeddings, especially good at classification tasks. Meanwhile, it is also naturally efficient, in terms of handling the large amount of vertices N .</p><p>Random-walk-based approaches such as node2vec <ref type="bibr" target="#b11">[12]</ref> with time complexity O(a 2 N ), where a is the average degree of the graph, suffer from the relatively-high degree in our dataset. On the other hand, GCN-based approaches are naturally efficient here. Like is analyzed in Cluster-GCN <ref type="bibr" target="#b5">[6]</ref>, the time complexity of the standard GCN model is O(L?A? 0 F + LN F 2 ), where L is the number of layers, ?A? 0 the number of non-zeros in the adjacency matrix, F the number of features. Note that the time complexity increases</p><formula xml:id="formula_2">?? N Features Multi-relational GCN 2|R| Adjacency matrixes 1 Identical matrix ?? W 1 W 2 Encoder Decoder ? Layer 1 Layer 2 W 2|R| W 2|R|+1 ? 1 W 2|R|+1 ? 2 ? 2|R| ? 2|R|+1 ?? N Embeddings ?? W 1 W 2 ? W 2|R| ? 1 ? 2 ? 2|R| ? 2|R|+1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Entity Classification</head><p>Link Prediction linearly when N increases. A GCN model's layer-wise propagation could be written as:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>??</head><formula xml:id="formula_3">H (l +1) = ? ?H (l ) W (l ) ? = D 1 2 (A + I N ) D 1 2</formula><p>, where D is defined as the diagonal matrix and A the adjacency matrix. D ii , the diagonal element d i , is equal to the sum of all the edges attached to v i ;</p><formula xml:id="formula_4">H (l ) ? R N ?d (l )</formula><p>is the d (l ) -dimensional representation of the N nodes at the l th layer;</p><formula xml:id="formula_5">W (l ) ? R d (l ) ?d (l +1)</formula><p>is the weight parameters at layer l which is similar with that of an ordinary MLP model 1 . In a certain way, ? could be viewed as A after being normalized.</p><p>We propose to model the heterogeneous types of links and their interactions in the encoder. Otherwise, if we split the views like many others did, the model will never be expressive enough to capture the interactions among relations. For any given politicalcentered graph G P , let's denote the total number of nodes |V | = N , the number of relations |R| = R, the set of nodes V, the set of relations R, and E r being the set of links under relation r ? R. Representation being learned after layer l (l ? {1, 2}) is represented as</p><formula xml:id="formula_6">H (l ) ? R N ?d (l )</formula><p>, and the input features form the matrix</p><formula xml:id="formula_7">H (0) ? R N ?d (0)</formula><p>. R where | R| = 2R+1 represents all relations in the original direction (R), the relations in reversed direction (R), and an identicalmatrix relation <ref type="bibr" target="#b0">(1)</ref>. Our dataset has |R| = R = 5, so it should be fine not to conduct a weight-matrix decomposition like r-GCN <ref type="bibr" target="#b32">[32]</ref>. We 1 MLP here refers to Multi-layer Perceptron. model the layer-wise propagation at Layer l + 1 as:</p><formula xml:id="formula_8">H (l +1) = ? r ? R ? r ?r H (l ) W (l ) r where H (l ) ? R N ?d (l )</formula><p>is used to denote the representation of the nodes after the l th encoder layer, and the initial input feature is</p><formula xml:id="formula_9">H (0) . ?r = D 1 2 r (A r + I N ) D 1 2</formula><p>r is defined in similar way as ? in GCN, but it is calculated per relation. The activation function ? we use is ReLU. By default, ? == [? 1 , . . . ? r . . . ] T ? R 2R+1 is calculated by scaled dot-product self-attention over the outputs of H</p><formula xml:id="formula_10">(l +1) r = ?r H (l ) W (l ) r : A = Attention(Q, K, V ) = so f tmax QK T ? d V ? R (2R+1)?d</formula><p>where</p><formula xml:id="formula_11">Q = K = V ? R (2R+1)?d comes from the 2R + 1 matrices H (l +1) r ? R N ?d , stacking up as O ? R (2R+1)</formula><p>?N ?d , taking an average over the N entities. We calculate an attention to apply to the 2R + 1 outputs as:</p><formula xml:id="formula_12">? = so f tmax sum col QK T ? d ? R 2R+1</formula><p>where sum col (X ) takes the sum of each column in X ? R d 1 ?d 2 and ends up in a vector ? R d 2 .</p><p>The last problem to solve is that the initial features H (0) is often incomplete in real life. In most cases, people would go by onehot features or randomized features. But we want to enable our model to use the real features, even if the real-features are incomplete. Inspired by graph representation learning strategies such as LINE <ref type="bibr" target="#b35">[35]</ref>, we proposed to treat the unknown features as trainable parameters. That is, for a graph G p whose vertice set is V, V f eatur ed V f eatur el ess = and V f eatur ed V f eatur el ess = V, for any node with valid feature ?v i ? V f eatur ed , the node's feature vector H (0) i is known and fixed. For ?v j ? V f eatur el ess , the corresponding row vector H (0) j is unknown and treated as a trainable parameter. The generation of the features will be discussed in the Appendix A. In brief, TIMME can handle any missing input feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Multi-Task Decoder</head><p>We propose TIMME as a multi-task learning model such that the sparsity of the labels could be overcome with the help of the link information. As is shown in Figure <ref type="figure" target="#fig_1">3</ref>, we propose two architectures of the multi-task decoder. When we test it on a single-task i, we simply disable the remaining losses but a single L i , and name our model in single-task mode TIMME-single.</p><p>L 0 is defined the same way as was proposed in <ref type="bibr" target="#b18">[19]</ref>, in our case a binary cross-entropy loss:</p><formula xml:id="formula_13">L 0 = - y ?Y t r ain y log(y) + (1 -y) log(1 -y)</formula><p>where Y t r ain contains the labels in the training set we have.</p><p>L 1 , . . . L R are link-prediction losses, calculated by binary crossentropy loss between link-labels and the predicted link scores' logits. To keep the links asymmetric, we used Neural Tensor Network (NTN) structure <ref type="bibr" target="#b33">[33]</ref>, with simplification inspired by DistMult <ref type="bibr" target="#b41">[41]</ref>. We set the number of slices be k = 1 for W r ? R d ?d ?k , omitting  the linear transformer U , and restricting the weight matrices W r each being a diagonal matrix. For convenience, we refer to this link-prediction cell as TIMME-NTN. Consider triplet (v i , r , v j ), and denote the encoder output of v i , v j ? V as h i , h j ? R d , the score function of the link is calculated as:</p><formula xml:id="formula_14">s(i, r , j) = h i W r h j + V h i h j + b</formula><p>where W r ? R d ?d is a diagonal matrix for any ?r ? R. W r , V ? R 2d and b ? R are all parameters to be learned. Group-truth label of a positive (existing) link is 1, otherwise 0. The first decoder-architecture TIMME sums all R + 1 losses as L = R i=0 L i . Without average, each task's loss is directly proportional to the amount of data points sampled at the current batch. Low-resource tasks will take a smaller portion. This is the most straightforward design of a MTL decoder.</p><p>The second, TIMME-hierarchical, has ? = [? 1 , . . . , ? | R | ] T being computed via self-attention on the average embedding over the R link-prediction task-specific embeddings. Here, L = R i=0 L i is the same with TIMME. TIMME-hierarchical essentially derives the node-label information from the link relations, thus provides some insights on each relation's importance to ideology prediction. TIMME, TIMME-hierarchical, TIMME-single models share exactly the same encoder architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, we introduce the dataset we crawled, cleaned and labeled, together with our experimental results and analysis. (3) For every candidate c ? C, we also crawl their most-recent s followers to make the follow relation more complete. (4) For every user u ? P ?C, crawl their tweets as much as possible, until we hit the limit (? 3, 200) set by Twitter API. (5) From the followers &amp; followees we collect follow relation, from the tweets we extract: retweet, mention, reply, like. (6) Select different groups of users from C, based on how many connections they have with members in P, and making those groups into the 4 subsets, as is shown in Table <ref type="table" target="#tab_2">1</ref>. <ref type="bibr" target="#b6">(7)</ref> We filter the relations within any selected group so that if a relation e = ?v i , v j ? ? G p , there must be v i ? G p and v j ? G p .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data Preparation</head><p>Our four datasets represent different user groups. PureP contains only the politicians. P50 contains politicians and users keen on political affairs. P20?50 is politicians with the group of users who are of moderate interests on politics. P+all is a union set of the three, plus some randomly-selected outliers of politics. P+all is the most challenging subset to all models. More details on the dataset, including how we generated features and how we tried to get more labels, are all described in details in Appendix A. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Performance Evaluation</head><p>In practice, we found that we do not need any features for nodes, and use one-hot encoding vector as initial feature. We split the train, validation, and test set of node labels by 8:1:1, keep it the same across all datasets and throughout all models, measuring the labels' prediction quality by F1-score and accuracy. For link-prediction tasks, we split all positive links into training, validation, and testing sets by 85:5:10, keeping same portion across all datasets and all models, evaluating by ROC-AUC and PR-AUC. <ref type="foot" target="#foot_0">2</ref>5.2.1 Baseline Methods. We have explored a lot of possible baseline models. Some methods we mentioned in section 2, HetGNN <ref type="bibr" target="#b43">[43]</ref>, GATNE <ref type="bibr" target="#b2">[3]</ref> and GTN <ref type="bibr" target="#b42">[42]</ref> generally converge ? 10 ? 100 times slower than our model on any task. GraphSAGE <ref type="bibr" target="#b13">[14]</ref> is not very suitable on our dataset. Moreover, other well-designed models such as GIN <ref type="bibr" target="#b40">[40]</ref> are way too different from our approach at a very fundamental level, thus are not considered as baselines. Some other methods such as GEM <ref type="bibr" target="#b22">[22]</ref> and SHINE <ref type="bibr" target="#b38">[38]</ref> should be capable of handling the dataset at this scale, but they are not releasing their code to the public, and we can not easily guarantee reproduction.</p><p>We decided to use the three baselines: GCN, r-GCN and HAN. They are closely-related to our model, open-sourced, and efficient. We understand that none of them were specifically designed for social-networks. Early explorations without tuning them resulted in terrible outcomes. To make the comparisons fair, we did a lot of work in hyper-parameter optimization, so that their performances are significantly improved. The GCN baseline treats all links as the same type and put them into one adjacency matrix. We also extend the baseline models to new tasks that were not mentioned in their original papers. We refer to GCN+ and HAN+ as the GCNbase-model or HAN-base-model with TIMME-NTN attached to it. By comparing with GCN/GCN+, we show that reserving heterogeneousity is beneficial. Comparing with r-GCN, we prove that their design is not as suitable for social networks as ours. With HAN/HAN+ we show that, although their model is potentially more expressive, our model still outperforms theirs in most cases, even after we carefully improved it to its highest potential (Appendix C). We did not have to tune the hyper-parameters of TIMME models closely as hard, thanks to its robustness.</p><p>HAN+ has an expressive and flexible structure that helps it achieve high in some tasks. The downsides of HAN/HAN+ are also obvious: it easily gets over-fitting, and is extremely sensitive to dataset statistics, with large memory consumption that typically more than 32G to run tasks on P+all, where TIMME models takes less than 4G space with the same hidden size and embedding dimensions as the baseline model's settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">TIMME.</head><p>To stabilize training, we would have to use the step-decay learning rate scheduler, the same with that for ResNet. The optimizer we use is Adam, kept consistent with GCN and r-GCN. We do not need input features for nodes, thus our encoder utilizes one-hot embedding by default. One of the many advantages of TIMME is how robust it is to the hyper-parameters and all other settings, reflected by that the same default parameter settings serve all experiments well. Like many others have done before, to avoid information leakage, whenever we run tasks involving link-prediction, we will remove all link-prediction test-set links from our adjacency matrices.</p><p>It is shown in Table <ref type="table" target="#tab_3">2</ref> and 3 that multi-task models TIMME and TIMME-hierarchical are generally better than TIMME-single on most tasks. Even TIMME-single is superior to the baseline models most of the times. TIMME models are stable and scalable. The classification task, despite the many labels we manually added, easily over-estimating the models. Models trained on single nodeclassification task will easily get over-fitted. If we force them to keep training after convergence, only multi-task TIMME models keep stable. The baselines and TIMME-single suffer from dramatic performance-drop, especially HAN/HAN+.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Case Studies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Selection of Input Features. To justify the reason why we do not need any features for nodes, we show the node-classification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reply Weights</head><p>Friend Weights Encoder Output </p><p>r ), and the encoder output embeddings (H (2) ). Red for ground-truth republican nodes, blue for democratic.  training-curves of TIMME-single with one-hot features, randomized features, partly-known-partly-randomized features, and with partly-known-partly-trainable features. The results are collected from P50 dataset. To make it easier to compare, we have fixed training epochs 300 for node-classification, and 200 for follow-relation link-prediction. It is shown that text feature is significantly better than randomized feature, and treating the missing part of the text-generated feature as trainable is better than treat it as fixed randomized feature. However, one-hot feature always outperforms them all, essentially means that relations are more reliable and less noisy than text information in training our network embedding. We have proved in Appendix B that the 2R + 1 weight matrices at the first convolutional layer captures the nodes' learned features when using one-hot features. Experimental evidence is shown in Figure <ref type="figure" target="#fig_2">4</ref>. It shows that although worse than the encoder output, the first embedding layer also captured the features of nodes. The embedding comes from epoch 300, node-classification task on PureP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Performance Measurement on News Agency.</head><p>A good measurement of our prediction's quality would be on some users with ground-truth tendency, but unlabeled in our dataset. News agents' accounts are typically such users, as is shown in Figure <ref type="figure">8</ref>. Among them we select some of the agencies believed to have clear tendencies. 3 The continuous scores we have for prediction come from the softmax of the last-layer output of our node-classification task, which is in the format of (prob l e f t , prob r i?ht ). Right in the middle represents (prob l e f t , prob r i?ht ) = (0.5, 0.5), left-most being (1.0, 0.0), right-most (0.0, 1.0). For most cases, our model's predictions agree with people's common belief. But CNN News is an 3 We fetch most of the ground-truth labels of the news agents from the public voting results on https://www.allsides.com/media-bias/media-bias-ratings, got them after the prediction results are ready. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN (@CNN)</head><p>CBC News (@cbcnews)</p><p>Guardian News (@guardiannews)</p><p>New York Times (@nytimes)</p><p>Christian Science Monitor (@csmonitor)</p><p>The American Spectator (@amspectator) Fox News Opinion (@FoxNewsOpinion) National Review (@NRO)</p><p>Figure <ref type="figure">8</ref>: The News Agencies' Ideologies. Text colors come from the public's voting online, blue for left and red for right, black for middle (centrist). Length represents the value from the last layer, reflecting the extent.</p><p>interesting case. It is believed to be extremely left, but predicted as slightly-left-leaning centrist. Some others have findings supporting our results: CNN is actually only a little bit left-leaning. <ref type="foot" target="#foot_1">4</ref> Although the public tends to believe that CNN is extremely liberal, it is more reasonable to consider it as centrist biased towards left-side. People's opinion on news agencies' tendencies might be polarized. Besides, although there are significantly more famous news agencies on the liberal side, those right-leaning ones tend to support their side more firmly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Geography Distribution.</head><p>Consider results from the largest dataset (P+all), and with predictions coming out from TIMMEhierarchical. We predict each Twitter user's ideology as either liberal or conservative. Then we calculate the percentage of the users on both sides, and depict it in Figure <ref type="figure">6</ref>. Darkest red represents p ? [0, 1  8 ] of users in that area are liberal, remaining [ 7  8 , 1] are conservative; darkest blue areas have [ 7  8 , 1] users being liberal, [0, 1  8 ] conservative. The intermediate colors represent the evenlydivided ranges in between. The users' locations are collected from the public information in their account profile. From our observation, conservative people are typically under-represented. 56 For instance, as a well-known firmly-conservative state, Utah (UT) is only shown as slightly right-leaning on our map. This is intuitively reasonable, since Twitter users are also biased. Typically biased towards youngsters and urban citizens. Although we are able to solve the problem of silent-majority by utilizing their link relations instead of text expressions, we know nothing about offline ideology. We suppose that some areas are silent on Twitter, and this guess is supported by the county-level results at Florida, shown in Figure <ref type="figure">7</ref>. This time the color-code represents evenlydivided seven ranges from [0, 1  7 ] to [ 6  7 , 1], because of the necessity of reserving one color for representing silent areas (denoted as white for N/A). The silent counties, typically some rural areas, have no user in our dataset, inferring that people living there do not use Twitter very often. The remaining parts of the graph makes complete sense, demonstrating a typical swing state. <ref type="foot" target="#foot_4">7</ref>5.3.4 Correlated Relations. When we train TIMME-single with only one relation type, some other relations' predictions benefit from it, and are becoming more and more accurate. We assume that, if by training on relation r i we achieve a good performance on relation r j , then we say relation r i probably leads to r j . As is shown in Figure <ref type="figure" target="#fig_6">9</ref>, relations among politicians are relatively independent except that all other relations might stimulate like. In more ordinary user groups, reply is the one that significantly benefit from all other relations. It is also interesting to observe that the highly-political P50 shows that like leads to retweet, while from more ordinary users' perspective once they liked they are less likely to retweet. The relations among the relations are asymmetric.  [0.99, 2.01], but still has some common trends, as is shown in Figure <ref type="figure" target="#fig_8">10</ref>. Despite that reply pops out rather than follow on PureP, we still insist that follow is the most important relation. That is because we only crawled the most recent about 5000 followers / followees. If a follow happened long time ago, we would not capture it. The follow relation is especially incomplete on PureP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>The TIMME models we proposed handles multiple relations, with a multi-relational encoder, and multi-task decoder. We step aside the silent-majority problem by relying mostly on the relations, instead of the text information. Optionally, we accept incomplete input features, but we showed that links are able to do well on generating the ideology embedding without additional text information. From our observation, links help much more than naively-processed text in ideology-detection problem, and follow is the most important relation to ideology detection. We also concluded from visualizing the state-level overall ideology map that conservative voices tend to be under-represented on Twitter. Meanwhile we confirmed that public opinions on news agencies' ideology could be polarized, with very obvious tendencies. Our model could be easily extended to any other social network embedding problem, such as on any other dataset like Facebook as long as the dataset is legally available, and of course it works on predicting other tendencies like preferring Superman or Batman. We also believe that our dataset would be beneficial to the community. Our team also received some external help from Yupeng Gu. He offered us his crawler code and his old dataset as references.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">ACKNOWLEDGEMENT</head><p>Sentence-BERT <ref type="bibr" target="#b30">[30]</ref> found that BERT / XLNet embeddings are generally performing worse than GloVe <ref type="bibr" target="#b26">[26]</ref> average on sentence-level tasks. Not to mention the computational cost of transformers. We therefore use GloVe-average of the words as features, Wikipedia 2014 + Gigaword 5 (300d) pre-trained version. When we apply the average-GloVe embedding on tweet-level, and want to tell the ideology behind the tweets, we could easily achieve ? 72.84% accuracy, using a 2-layers MLP, after only 200 epochs of training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Label Preparation</head><p>If we are to use only the 583 labels from the politicians, the evaluation will always be untrustworthy. To overcome this issue, we manually expand the labels. We first crawled the users' profiles of ?v i ? P ? C, getting their information such as location and account description. Next, using the descriptions, searching for the words democratic, republican, conservative, liberal, their correct spell and variations, we have a large group of candidates. Then we do manual filtering to get rid of the uncertain users, reading their descriptions and recent tweets. We successfully included 2, 976 high-quality new labels in the end. Those labels make the node-classification task significantly more stable and reliable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B PROOF OF WEIGHT BEING FEATURE</head><p>Starting from our layer-wise propagation formula, we have that, at the first convolutional layer (notations in Section 4):</p><formula xml:id="formula_16">H (1) = ? r ? R ? r ?r H (0) W (0) r</formula><p>where H (0) ? N ? d (0) is the input feature-matrix. When using one-hot embedding of features, H (0) = I and d (0) = N , thus the right-hand-side is equivalent with ?</p><formula xml:id="formula_17">r ? R ? r ?r W (0) r . Now, W<label>(0)</label></formula><p>r on its own plays the role of H (0) W</p><p>r when H (0) I . Previously, relation r 's propagation could be viewed as aggregation of a linear transformation (W (0) r ) done on H (0) , from the neighborhood ( ?r ) of each node under relation r . Now, it could simply be viewed as the propagation of W</p><formula xml:id="formula_19">(0) r ? R N ?d (1)</formula><p>. From another point of view, it is equivalent as having input features being H</p><formula xml:id="formula_20">(0) = W (0) r ? R N ?d (1)</formula><p>, and set W (0) r ? R d (1) ?d (1) = I d (1) being fixed identical matrix not to be updated. That's the reason why we believe that W (0) r captures the nodes' learned features under relation r .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C BASELINE HYPER-PARAMETER AND ARCHITECTURAL OPTIMIZATIONS C.1 Applying GCN model Directly</head><p>As is discussed in Section 2, due to the uniqueness of the politicalcentered social network dataset, most of the existing models won't work well under our problem settings. We want to examine how well could GCN do when treating all relations as the same, ignoring the heterogeneous types. Very interestingly, without much work on hyper-parameter optimization, we only increased the hidden size and added the learning rate scheduler, it works pretty well. This phenomenon could potentially be an indirect evidence that relations are correlated, in addition to the discussions in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Missing-Task Completion</head><p>We compare our model's performance on each task with the baselines. Ideally, we want models working on heterogeneous information networks with both node-classification task and link-prediction task as our baselines, so that we could compare with them directly. However, the situation we faced was not as easy as such. For instance, GCN and HAN never considered applying themselves directly on link-prediction tasks. But we all know that once we have the embeddings of the nodes, link prediction is doable.</p><p>Therefore, we decided that whenever a baseline originally couldn't handle a task, we lend it our decoder's task-specific cells. This decision brings about some significant improvements on the link prediction performances of NTN+ and GCN+, since TIMME-NTN is powerful and efficient for link-prediction. Just in case, we also decide that when a node-classification task is missing, we should add a linear transformation layer with output units 2, the same as what we did, and apply a simple cross-entropy loss. From this perspective, it is no longer fair to compare them with r-GCN directly. To distinguish them from others' standard models, we add a plus sign "+" to the names, indicating that "we lend it our cells".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Optimizing r-GCN</head><p>The most important contribution of r-GCN is the weight-matrix decomposition methods. This mechanism would be very helpful in reducing the parameters, especially when the number of relations R is super high. However, in our case where R is small, the weightdecomposition operation is counter-effective. The first option, basis decomposition, the number of basis b is easily being larger than R. In the second option, block-diagonal decomposition, reduces the parameter size too dramatically, and harms the model's performance.</p><p>Reviewing the experiments reported in the r-GCN paper, seeing how they chose these hyper-parameters across datasets, we found that when R is small, they often chose basis-decomposition with b = 0. We go by the same option, which works well in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Optimizing HAN</head><p>HAN/HAN+, in general, because of the complex structure with a lot of parameters, gets easily over-fitting. What makes things worse, its training curve is never stable, and our early tryouts on using validation set to automatically stop it at an optimal point did not work well. We had do it manually, by verifying when its best result appears on the validation set and when over-fitting starts, finding the right time to stop training. By default, we set learning rate 0.005, regularization parameter 0.001, the semantic-level attention-vector dimension 128, multi-head-attention cell's number of heads K = 8. We set the hyper-parameters in the TIMME-NTN component of HAN+ the same with ours. Optimizing HAN was a tough work to do, for it requires re-adapting every choices we made on every dataset for every task. Adding more meta-path would potentially boosting its performance, but the computational cost will be overwhelming. Another observation is that, TIMME models are significantly better than HAN/HAN+ in handling imperfect features. When using GloVe-average features, TIMME models typically perform about 1% worse than using one-hot features, while HAN/HAN+ experience performance-drop up to around 10%.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The general architecture of our model, with the encoder shown in details. Grey blocks represent missing features. Our model can either handle them by treating them as learnable parameters, or use one-hot features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The two types of decoder in our multi-task framework, referred to as TIMME and TIMME-hierarchical.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: t-SNE of matrices onto 2D space. Showing reply (and reversed), friend (and reversed) weight matrices of the first convolutional layer (W</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a) Features' Impact on Node-Classi cation Task (b) Features' Impact on "Follow" Link-Prediction Task epoch epoch ROC-AUC score F1-score</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Illustration of impact of features. Random features in blue, partly-know partly randomized (and fixed) in yellow, partly-known partly-trainable in green, one-hot in red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Figure 6: Overall ideology on Twitter in each state.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: The impact of training on single-link-prediction tasks, on Pure-P (left), P50 (middle), P+all (right) dataset respectively.</figDesc><graphic url="image-1.png" coords="8,73.12,94.19,116.38,116.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>5. 3 . 5</head><label>35</label><figDesc>Relation's Contributions to Ideology Detection. The importance of each relation to ideology prediction could be measured by the value of the corresponding ? r values in the decoder of TIMME-hierarchical. All the values are close to 0.2 in practice, in retweet mention follow reply like PureP P50 P20~50 P+all</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Illustration of ? value in decoder on each dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>An example of different relation types on Twitter. Derica is on liberal (left) side while Rosa is on the conservative (right) side. Isabel does not have significant tendency.</figDesc><table><row><cell></cell><cell></cell><cell>Tweet</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">Democratic is the</cell></row><row><cell cols="2">some_democratician</cell><cell cols="3">best party I believe.</cell><cell>Retweet</cell><cell>some_republican</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(with comment)</cell></row><row><cell cols="2">Follow</cell><cell></cell><cell></cell><cell cols="2">I STRONGLY disagree.</cell><cell>Follow</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Mention</cell><cell>@Derica</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">I prefer Republican.</cell></row><row><cell>RT @Isabel I agree w...</cell><cell cols="2">Derica Like</cell><cell>Reply</cell><cell cols="2">Okay. Whatever you say.</cell><cell>Rosa Like</cell></row><row><cell></cell><cell></cell><cell cols="2">Mention</cell><cell></cell><cell>Mention</cell></row><row><cell>Retweet</cell><cell cols="2">I agree with what</cell><cell></cell><cell></cell><cell>But @some_republican</cell></row><row><cell></cell><cell cols="3">@some_democratician</cell><cell></cell><cell>proposed something</cell></row><row><cell></cell><cell cols="2">said because ......</cell><cell></cell><cell>Isabel</cell><cell>very interesting......</cell></row><row><cell>Figure 1:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>5.1.1 Data Crawling. The statics of the political-centered social network datasets we have are listed in Table1. Data prepared is described in Appendix A, ready by April, 2019. In brief, we did: (1) Collecting some Twitter accounts of the politicians P;(2) For every politician ?p ? P, crawl her/his most-recent s followers and s followees, putting them in a candidate set C. Descriptive statistics of the three selected subsets of our dataset.</figDesc><table><row><cell></cell><cell>PureP</cell><cell>P50</cell><cell>P20?50</cell><cell>P+all</cell></row><row><cell># User</cell><cell>583</cell><cell>5,435</cell><cell>12,103</cell><cell>20,811</cell></row><row><cell># Link</cell><cell cols="4">122,347 1,593,721 1,976,985 6,496,107</cell></row><row><cell># Labeled User</cell><cell>581</cell><cell>759</cell><cell>961</cell><cell>1,206</cell></row><row><cell># Featured User</cell><cell>579</cell><cell>5,149</cell><cell>11,725</cell><cell>19,418</cell></row><row><cell># Follow-Link</cell><cell>59,073</cell><cell>529,448</cell><cell>158,746</cell><cell>915,438</cell></row><row><cell># Reply-Link</cell><cell>1,451</cell><cell>96,757</cell><cell>121,133</cell><cell>530,598</cell></row><row><cell># Retweet-Link</cell><cell>19,760</cell><cell>311,359</cell><cell cols="2">595,030 1,684,023</cell></row><row><cell># Like-Link</cell><cell>14,381</cell><cell>302,571</cell><cell cols="2">562,496 1,794,111</cell></row><row><cell cols="2"># Mention-Link 27,682</cell><cell>353,586</cell><cell cols="2">539,580 1,571,937</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Node classification measured by F1-score/accuracy.</figDesc><table><row><cell>Model</cell><cell>PureP</cell><cell>P50</cell><cell>P20?50</cell><cell>P+all</cell></row><row><cell>GCN</cell><cell>1.0000/1.0000</cell><cell>0.9600/0.9600</cell><cell>0.9895/0.9895</cell><cell>0.9076/0.9083</cell></row><row><cell>r-GCN</cell><cell>1.0000/1.0000</cell><cell>0.9733/0.9733</cell><cell>0.9895/0.9895</cell><cell>0.9327/0.9333</cell></row><row><cell>HAN</cell><cell>0.9825/0.9824</cell><cell>0.9466/0.9467</cell><cell>0.9789/0.9789</cell><cell>0.9238/0.9250</cell></row><row><cell>TIMME-single</cell><cell>1.0000/1.0000</cell><cell>0.9733/0.9733</cell><cell>0.9895/0.9895</cell><cell>0.9333/0.9324</cell></row><row><cell>TIMME</cell><cell>0.9825/0.9824</cell><cell>0.9867/0.9867</cell><cell>1.0000/1.0000</cell><cell>0.9495/0.9500</cell></row><row><cell>TIMME-hierarchical</cell><cell>1.0000/1.0000</cell><cell>0.9733/0.9780</cell><cell>0.9895/0.9895</cell><cell>0.9580/0.9583</cell></row><row><cell>Model</cell><cell>PureP</cell><cell>P50</cell><cell>P20?50</cell><cell>P+all</cell></row><row><cell></cell><cell></cell><cell>Follow Relation</cell><cell></cell><cell></cell></row><row><cell>GCN+</cell><cell>0.8696/0.6167</cell><cell>0.9593/0.8308</cell><cell>0.9870/0.9576</cell><cell>0.9855/0.9329</cell></row><row><cell>r-GCN</cell><cell>0.8596/0.6091</cell><cell>0.9488/0.8023</cell><cell>0.9872/0.9537</cell><cell>0.9685/0.9201</cell></row><row><cell>HAN+</cell><cell>0.8891/0.7267</cell><cell>0.9598/0.8642</cell><cell>0.9620/0.8850</cell><cell>0.9723/0.9256</cell></row><row><cell>TIMME-single</cell><cell>0.8809/0.6325</cell><cell>0.9717/0.8792</cell><cell>0.9920/0.9709</cell><cell>0.9936/0.9696</cell></row><row><cell>TIMME</cell><cell>0.8763/0.6324</cell><cell>0.9811/0.9154</cell><cell>0.9945/0.9799</cell><cell>0.9943/0.9736</cell></row><row><cell>TIMME-hierarchical</cell><cell>0.8812/0.6409</cell><cell>0.9809/0.9145</cell><cell>0.9984/0.9813</cell><cell>0.9944/0.9739</cell></row><row><cell></cell><cell></cell><cell>Reply Relation</cell><cell></cell><cell></cell></row><row><cell>GCN+</cell><cell>0.8602/0.7306</cell><cell>0.9625/0.9022</cell><cell>0.9381/0.8665</cell><cell>0.9705/0.9154</cell></row><row><cell>r-GCN</cell><cell>0.7962/0.6279</cell><cell>0.9421/0.8714</cell><cell>0.8868/0.7815</cell><cell>0.9640/0.9085</cell></row><row><cell>HAN+</cell><cell>0.8445/0.6359</cell><cell>0.9598/0.8616</cell><cell>0.9495/0.8664</cell><cell>0.9757/0.9210</cell></row><row><cell>TIMME-single</cell><cell>0.8685/0.7018</cell><cell>0.9695/0.9307</cell><cell>0.9593/0.9070</cell><cell>0.9775/0.9508</cell></row><row><cell>TIMME</cell><cell>0.9077/0.8004</cell><cell>0.9781/0.9417</cell><cell>0.9747/0.9347</cell><cell>0.9849/0.9612</cell></row><row><cell>TIMME-hierarchical</cell><cell>0.9224/0.8152</cell><cell>0.9766/0.9409</cell><cell>0.9737/0.9341</cell><cell>0.9854/0.9629</cell></row><row><cell></cell><cell cols="2">Retweet Relation</cell><cell></cell><cell></cell></row><row><cell>GCN+</cell><cell>0.8955/0.7145</cell><cell>0.9574/0.8493</cell><cell>0.9351/0.8408</cell><cell>0.9724/0.9303</cell></row><row><cell>r-GCN</cell><cell>0.8865/0.6895</cell><cell>0.9411/0.8084</cell><cell>0.9063/0.7728</cell><cell>0.9735/0.9326</cell></row><row><cell>HAN+</cell><cell>0.7646/0.6139</cell><cell>0.9658/0.9213</cell><cell>0.9478/0.8962</cell><cell>0.9750/0.9424</cell></row><row><cell>TIMME-single</cell><cell>0.9015/ 0.7202</cell><cell>0.9754/0.9127</cell><cell>0.9673/0.9073</cell><cell>0.9824/0.9424</cell></row><row><cell>TIMME</cell><cell>0.9094/0.7285</cell><cell>0.9779/0.9181</cell><cell>0.9772/0.9291</cell><cell>0.9858/0.9511</cell></row><row><cell>TIMME-hierarchical</cell><cell>0.9105/0.7344</cell><cell>0.9780/0.9190</cell><cell>0.9766/0.9275</cell><cell>0.9869/0.9543</cell></row><row><cell></cell><cell></cell><cell>Like Relation</cell><cell></cell><cell></cell></row><row><cell>GCN+</cell><cell>0.9007/0.7259</cell><cell>0.9527/0.8499</cell><cell>0.9349/0.8400</cell><cell>0.9690/0.9032</cell></row><row><cell>r-GCN</cell><cell>0.8924/0.7161</cell><cell>0.9343/0.7966</cell><cell>0.9038/0.7681</cell><cell>0.9510/0.8945</cell></row><row><cell>HAN+</cell><cell>0.8606/0.6176</cell><cell>0.9733/0.8851</cell><cell>0.9611/0.9062</cell><cell>0.9894/0.9481</cell></row><row><cell>TIMME-single</cell><cell>0.9113/0.7654</cell><cell>0.9725/0.9119</cell><cell>0.9655/0.9069</cell><cell>0.9796/0.9374</cell></row><row><cell>TIMME</cell><cell>0.9249/0.7926</cell><cell>0.9753/0.9171</cell><cell>0.9759/0.9292</cell><cell>0.9846/0.9504</cell></row><row><cell>TIMME-hierarchical</cell><cell>0.9278/0.7945</cell><cell>0.9752/0.9175</cell><cell>0.9752/0.9271</cell><cell>0.9851/0.9518</cell></row><row><cell></cell><cell cols="2">Mention Relation</cell><cell></cell><cell></cell></row><row><cell>GCN+</cell><cell>0.8480/0.6233</cell><cell>0.9602/0.8617</cell><cell>0.9261/0.8170</cell><cell>0.9665/0.8910</cell></row><row><cell>r-GCN</cell><cell>0.8312/0.6023</cell><cell>0.9382/0.7963</cell><cell>0.8938/0.7563</cell><cell>0.9640/0.8902</cell></row><row><cell>HAN+</cell><cell>0.9000/0.7206</cell><cell>0.9573/0.8616</cell><cell>0.9574/0.8891</cell><cell>0.9724/0.9119</cell></row><row><cell>TIMME-single</cell><cell>0.8587/0.6502</cell><cell>0.9713/0.8981</cell><cell>0.9614/0.8923</cell><cell>0.9725/0.9096</cell></row><row><cell>TIMME</cell><cell>0.8684/0.6689</cell><cell>0.9730/0.9035</cell><cell>0.9730/0.9185</cell><cell>0.9839/0.9446</cell></row><row><cell>TIMME-hierarchical</cell><cell>0.8643/0.6597</cell><cell>0.9732/0.9046</cell><cell>0.9723/0.9166</cell><cell>0.9846/0.9463</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note><p>Link-prediction measured by ROC-AUC/PR-AUC.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>This work is partially supported by NSF III-1705169, NSF CAREER Award 1741634, NSF #1937599, DARPA HR00112090027, Okawa Foundation Grant, and Amazon Research Award. Weiping Song is supported by National Key Research and Development Program of China with Grant No. 2018AAA0101900/ 2018AAA0101902 as well as the National Natural Science Foundation of China (NSFC Grant No. 61772039 and No. 91646202). At the early stage of this work, Haoran Wang 8 contributed a lot to a nicely-implemented first version of the model, benefiting the rest of our work. Meanwhile, Zhiwen Hu 9 explored the related methods' efficiencies, and his works shed light on our way.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>AUC refers to Area Under Curve, PR for precision-recall curve, ROC for receiver operating characteristic curve.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>https://libguides.com.edu/c.php?g=649909&amp;p=4556556</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>National General Election Polls data partly available at https://www.realclearpolitics. com/epolls/2020/president/National.html.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3"><p>Compare with the visualization of previous election at https://en.wikipedia.org/wiki/ Political_party_strength_in_U.S._states.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4"><p>The ground-truth election outcome in Florida at 2016 is at https://en.wikipedia.org/ wiki/2016_United_States_presidential_election_in_Florida.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15" xml:id="foot_5"><p>https://developer.twitter.com/en/docs/basics/rate-limiting</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A DATA PREPARATION</head><p>We target at building a dataset representing the political-centered social network (Section 3), a selected subset from the giant Twitter network. Handling this dataset would be challenging. For example, for GraphSAGE, neighborhood-sampling can not be easily done both effectively and efficiently. Our dataset reaches the blind spots of many existing models.</p><p>The tools we used to crawl politicians' name lists from the government website, and their potential Twitter accounts from Google, is Scrapy. 10 To legally and reliably crawl from Twitter data, we first applied for Developer API from Twitter 11 , and then used Tweepy 12 for crawling. We set very strict rate limits for our crawlers so as not to harm any server. Our dataset is released at https://github.com/PatriciaXiao/TIMME. Raw data was collected by April, 2019.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Twitter IDs Preparation</head><p>Let us take the same notation as in Section 3, describing the process as: to construct G p = {V, {E 1 , E 2 , E 3 , E 4 , E 5 }}, we first select the users to be included V, then we include the links among vertices in V under each relation r ? R = {1, 2, 3, 4, 5} into E r accordingly.</p><p>A.1.1 Politicians Twitter IDs. As is described briefly in Section 5.1, we need to start from a set of politicians P, which we treat as seeds for further crawling.</p><p>To start with, we first get the name list of the recently-active politicians, consists of: ? The union-set of 115 t h and 116 t h US congress members, where we observe a lot of overlap between the two groups; 13 ? Recent-years' presidents and their cabinets; 14  ? Additional politicians must be included: Hilary Clinton, who was running for the president of the United States not long ago; Michelle Obama, who was the former First Lady.</p><p>Next, with the help of Google, we crawled the most-likely Twitter names and IDs of the politicians. We do so automatically, by providing Google a politician's name and the keyword "twitter", and parsing the first response. Then after manual filtering, we have 583 politicians' Twitter accounts available, who make up our politicians set P. Anyone else to be included in our dataset must be in the 1-hop neighborhood of a politician (Section 3).</p><p>A.1.2 Candidate Non-Politicians Twitter IDs. With the help of Twitter Developer API, we are able to get the full followers and followees list of any Twitter user.</p><p>However, it is not affordable to include all followers and followees of the politicians, thus we set a limit on window size s when crawling the candidate non-politicians list, only accepting the mostrecent s = 5, 000 followers or followees of any politician. These followers and followees we collected form a raw candidate set C r aw . Then we remove the politicians from this set, resulting in the final 10 https://scrapy.org/ 11 https://developer.twitter.com/ 12 https://www.tweepy.org/ 13 Congress members' name list with party information is publicly available at https: //www.congress.gov/members . 14 Obama and Trump's cabinet is publicly available at https://obamawhitehouse. archives.gov/administration/cabinet and https://www.whitehouse.gov/the-trumpadministration/the-cabinet/ respectively candidates set C = C r aw -P. ?v i ? C, we apply the same window size s = 5, 000 and crawled their most recent s followers, s followees. All follower-followee pairs are stored into a database for the convenience of the following steps.</p><p>A.1.3 Selecting Subgroups from Candidates. C is still too large a user set, and chaotic, as we don't know anything about its components. To conduct meaningful analysis, we need to select some meaningful subgroups from it, such as a very-political subgroup, and a political-outliers subgroup, etc.</p><p>The criteria we used to select the desired subgroups of users is some thresholds. We define a political-measurement t i for each user v i ? C, who is followed by t i,1 politicians p ? P, and meanwhile following t i,2 politicians, thus t i is computed by</p><p>Then we set a threshold range t, set upon each t i , used for filtering the groups of users. Considering we set t as threshold range for graph G p , ?v i ? V, if t i ? t, then v i ? G p , otherwise v i G p . By having t = {?}, we select a minimum subgraph containing purely politicians, resulting in our PureP dataset. t ? [50, ?) allows us to select a small group of users who are keen on political topics, together with the politicians, being our P50 dataset. t ? [20, 50) for less-political users, plus the politicians, being our P20?50 dataset. t ? [20, ?) includes all nodes v i whose t i ? 20. We want to have a dataset representing more general users, containing some users from each group. Therefore, we include another 3, 000 users randomly selected from the group t ? <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5)</ref>. Adding these random political-outlier users will make the dataset resembles the real network even more. Putting together the politicians, t ? [20, ?) group, and the 3, 000 random outliers from t ? [0, 5) group, we form the dataset P+all. Ideally, P+all has representatives of all groups of users on Twitter. The statistics are concluded in Table <ref type="table">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Relation Preparation</head><p>Only the follow relation is directly observed and already wellprepared at this stage (stored in a database, as we mentioned before). Other Twitter relations: retweet, mention, like, reply, must be concluded from tweets. We distinguish the different relation types from the tweets by the tweets' fields in responded JSON from API. For example, there are some fields indicating if an "" mark is a mention, a retweet, or it links to nothing. According to our observation, the fields in the Json file responded from Twitter API might change across time. We don't know when will it be the next update, so there's no ground-truth solution for this part. We suggest whoever want to do so test the crawler first on her/his own account, trying all behaviors to conclude some patterns. Note: rate limit applies. 15  Due to the Twitter official API limits, the maximum amount of tweets we could crawl for each user along the timeline is around 3, 200. Therefore, all relations are incomplete. All links we have only reflect some recent interactions among the users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Feature Preparation</head><p>We get feature from text, using a user's tweets posted to generate her/his feature. Although there has been some recent advances in NLP with transformer-based structures, such as BERT and XLNet,</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mass political attitudes and the survey response</title>
		<author>
			<persName><surname>Christopher H Achen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Political Science Review</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="1218" to="1231" />
			<date type="published" when="1975">1975. 1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Ramy</forename><surname>Baly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgi</forename><surname>Karadzhov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrhman</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.00542</idno>
		<title level="m">Multi-task ordinal regression for jointly predicting the trustworthiness and the leading political ideology of news media</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Representation learning for attributed multiplex heterogeneous network</title>
		<author>
			<persName><forename type="first">Yukuo</forename><surname>Cen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1358" to="1368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Fastgcn: fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cao</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.10247</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Opinionaware Knowledge Graph for Political Ideology Detection</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengjiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3647" to="3653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks</title>
		<author>
			<persName><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The statistical analysis of roll call data</title>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Clinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Jackman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Rivers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Political Science Review</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="355" to="370" />
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Predicting the political alignment of twitter users</title>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Michael D Conover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Gon?alves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Ratkiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filippo</forename><surname>Flammini</surname></persName>
		</author>
		<author>
			<persName><surname>Menczer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE third international conference on privacy, security, risk and trust and 2011 IEEE third international conference on social computing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="192" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Political polarization on twitter</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Michael D Conover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Ratkiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Francisco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filippo</forename><surname>Gon?alves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Menczer</surname></persName>
		</author>
		<author>
			<persName><surname>Flammini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifth international AAAI conference on weblogs and social media</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Predicting the Demographics of Twitter Users from Website Traffic Data</title>
		<author>
			<persName><forename type="first">Aron</forename><surname>Culotta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Cutler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<meeting><address><addrLine>Austin, TX</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="72" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">Micha?l</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Ideology detection for twitter users with heterogeneous types of links</title>
		<author>
			<persName><forename type="first">Yupeng</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingyu</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.08207</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Political ideology detection using recursive neural networks</title>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Enns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1113" to="1122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Identifying stance by analyzing political discourse on twitter</title>
		<author>
			<persName><forename type="first">Kristen</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Goldwasser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on NLP and Computational Social Science</title>
		<meeting>the First Workshop on NLP and Computational Social Science</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="66" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mining twitter for fine-grained political opinion polarity classification, ideology detection and sarcasm detection</title>
		<author>
			<persName><forename type="first">Sandeepa</forename><surname>Kannangara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Eleventh ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="751" to="752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-task learning using uncertainty to weigh losses for scene geometry and semantics</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7482" to="7491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m">huzhiwen@g.ucla.edu, currently working at PayPal</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The national boundaries of solidarity: a survey experiment on solidarity with unemployed people in the European Union</title>
		<author>
			<persName><forename type="first">Theresa</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Kamm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Political Science Review</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="179" to="195" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Heterogeneous graph neural networks for malicious account detection</title>
		<author>
			<persName><forename type="first">Ziqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaochao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinxing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 27th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2077" to="2085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Trust across political conflicts: Evidence from a survey experiment in divided societies</title>
		<author>
			<persName><forename type="first">Sergio</forename><surname>Martini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariano</forename><surname>Torcal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Party Politics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="126" to="139" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Tea party in the house: A hierarchical ideal point topic model and its application to republican legislators in the 112th congress</title>
		<author>
			<persName><surname>Viet-An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Resnik</surname></persName>
		</author>
		<author>
			<persName><surname>Miler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1438" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Does Twitter motivate involvement in politics? Tweeting, opinion leadership, and political engagement</title>
		<author>
			<persName><forename type="first">Chang</forename><surname>Sup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Park</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers in Human Behavior</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1641" to="1648" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Populism, ideology and contradiction: mapping young people&apos;s political views</title>
		<author>
			<persName><forename type="first">Gary</forename><surname>Pollock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellison</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Sociological Review</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="141" to="166" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A spatial model for legislative roll call analysis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Keith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Howard</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><surname>Rosenthal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Political Science</title>
		<imprint>
			<biblScope unit="page" from="357" to="384" />
			<date type="published" when="1985">1985. 1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Beyond binary labels: political ideology prediction of twitter users</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Preo?iuc-Pietro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Hopkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lyle</forename><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="729" to="740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.10084</idno>
		<title level="m">Sentence-bert: Sentence embeddings using siamese bert-networks</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">An overview of multi-task learning in deep neural networks</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05098</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="926" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mining heterogeneous information networks: principles and methodologies</title>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="159" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on world wide web</title>
		<meeting>the 24th international conference on world wide web</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Graph attention networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Twitter demographic classification using deep multi-modal multi-task learning</title>
		<author>
			<persName><forename type="first">Prashanth</forename><surname>Vijayaraghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soroush</forename><surname>Vosoughi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deb</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="478" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Shine: Signed heterogeneous information network embedding for sentiment link prediction</title>
		<author>
			<persName><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minyi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Eleventh ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="592" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Heterogeneous graph attention network</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houye</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanfang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2022" to="2032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<title level="m">How powerful are graph neural networks? arXiv preprint</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6575</idno>
		<title level="m">Embedding entities and relations for learning and inference in knowledge bases</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Graph Transformer Networks</title>
		<author>
			<persName><forename type="first">Seongjun</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minbyul</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raehyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunwoo J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="11960" to="11970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Heterogeneous graph neural network</title>
		<author>
			<persName><forename type="first">Chuxu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongjin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitesh V</forename><surname>Chawla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="793" to="803" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
