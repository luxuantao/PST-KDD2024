<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A game-based abstraction-refinement framework for Markov decision processes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2010-08-03">3 August 2010</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mark</forename><surname>Kattenbelt</surname></persName>
							<email>mark.kattenbelt@comlab.ox.ac.uk</email>
						</author>
						<author>
							<persName><forename type="first">Marta</forename><surname>Kwiatkowska</surname></persName>
							<email>marta.kwiatkowska@comlab.ox.ac.uk</email>
						</author>
						<author>
							<persName><forename type="first">Gethin</forename><surname>Norman</surname></persName>
							<email>gethin@dcs.gla.ac.uk</email>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Parker</surname></persName>
							<email>david.parker@comlab.ox.ac.uk</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Oxford University Computing Laboratory</orgName>
								<address>
									<addrLine>Wolfson Building, Parks Road</addrLine>
									<postCode>OX1 3QD</postCode>
									<settlement>Oxford</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computing Science</orgName>
								<orgName type="institution">University of Glasgow</orgName>
								<address>
									<addrLine>18 Lilybank Gardens</addrLine>
									<postCode>G12 8RZ</postCode>
									<settlement>Glasgow</settlement>
									<country key="GB">Scotland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A game-based abstraction-refinement framework for Markov decision processes</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2010-08-03">3 August 2010</date>
						</imprint>
					</monogr>
					<idno type="MD5">FFD505A18991B400C3779F6D880AF11C</idno>
					<idno type="DOI">10.1007/s10703-010-0097-6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T03:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Probabilistic verification</term>
					<term>Markov decision processes</term>
					<term>Abstraction</term>
					<term>Abstraction refinement</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In the field of model checking, abstraction refinement has proved to be an extremely successful methodology for combating the state-space explosion problem. However, little practical progress has been made in the setting of probabilistic verification. In this paper we present a novel abstraction-refinement framework for Markov decision processes (MDPs), which are widely used for modelling and verifying systems that exhibit both probabilistic and nondeterministic behaviour. Our framework comprises an abstraction approach based on stochastic two-player games, two refinement methods and an efficient algorithm for an abstraction-refinement loop. The key idea behind the abstraction approach is to maintain a separation between nondeterminism present in the original MDP and nondeterminism introduced during the abstraction process, each type being represented by a different player in the game. Crucially, this allows lower and upper bounds to be computed for the values of reachability properties of the MDP. These give a quantitative measure of the quality of the abstraction and form the basis of the corresponding refinement methods. We describe a prototype implementation of our framework and present experimental results demonstrating automatic generation of compact, yet precise, abstractions for a large selection of real-world case studies.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Numerous real-life systems from a wide range of application domains, including communication and network protocols, security protocols and distributed algorithms, exhibit both probabilistic and nondeterministic behaviour, and therefore developing techniques to verify the correctness of such systems is an important research topic. Markov decision processes (MDPs) are a natural and widely used model for this purpose and the automatic verification of MDPs using probabilistic model checking has proved successful for their analysis. Despite improvements in implementations and tool support in this area, the state-space explosion problem remains a major hurdle for the practical application of these methods.</p><p>This paper is motivated by the success of abstraction-refinement techniques <ref type="bibr" target="#b7">[8]</ref>, which have been established as one of the most effective ways of attacking the state-space explosion problem in non-probabilistic model checking. The basic idea is to construct a smaller abstract model, by removing details from the concrete system not relevant to the property of interest, which is consequently easier to analyse. This is done in such a way that when the property is verified true in the abstraction it also holds in the concrete system. On the other hand, if the property does not hold in the abstraction, information from the model checking process (typically a counterexample) is used either to show that the property is false in the concrete system or to refine the abstraction. This process forms the basis of a loop which refines the abstraction until the property is shown to be true or false in the concrete system.</p><p>In the probabilistic setting, it is typically necessary to consider quantitative properties, in which case the actual probability or expectation of some event must be determined, e.g. "the probability of reaching an error state within T time units" or "the expected power consumption before termination". We therefore adopt a quantitative approach when defining a correspondence between properties of the concrete and abstract models. One example would be the case where quantitative results computed from the abstraction constitute conservative bounds on the actual values for the concrete model. In fact, due to the presence of nondeterminism in an MDP there is not necessarily a single value corresponding to a given quantitative measure. Instead, best-case and worst-case scenarios are analysed. More specifically, model checking of MDPs typically reduces to computation of probabilistic reachability and expected reachability properties, namely the minimum or maximum probability of reaching a set of states, and the minimum or maximum expected reward or cost incurred when reaching a set of states.</p><p>When constructing an abstraction of an MDP, the resulting model will invariably exhibit a greater degree of nondeterminism caused by the uncertainty with regards to the precise behaviour of the system that the abstraction process introduces. The key idea in our abstraction approach is to maintain a distinction between the nondeterminism from the original MDP and the nondeterminism introduced during the abstraction process. To achieve this, we model abstractions of MDPs as stochastic two-player games <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b38">40]</ref>, where the two players correspond to the two different forms of nondeterminism. We can then analyse these models using techniques developed for such games <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b13">15]</ref>.</p><p>Our analysis of these abstract models results in a separate lower and upper bound for each of the minimum and maximum probabilities (or expected rewards) of reaching a set of states. This approach is particularly appealing since this information provides both a quantitative measure of the quality (or preciseness) of the abstraction and an indication of how to improve it. By comparison, if no discrimination between the two forms of nondeterminism is made, a single lower and upper bound would be obtained. Therefore, in the (common) situation where the minimum and maximum probabilities (or expected rewards) are notably different, the quantitative measure and corresponding basis for refinement would be lost.</p><p>Consider, for example, the extreme case where the two-player game approach reveals that the minimum probability of reaching some set of states is in the interval [0, ε min ] and the maximum probability is in the interval [1-ε max , 1]. In this case, a single pair of bounds could at best establish that both the minimum and maximum probability lie within the interval [0, 1], effectively yielding no information about the utility of the abstraction or how to refine it in order to improve the bounds.</p><p>Based on the separate bounds obtained through this approach, we present two methods for automatically refining our game-based abstractions of MDPs and an efficient algorithm for an abstraction-refinement loop. In addition, using a prototype implementation of the framework, we give experimental results that demonstrate automatic generation of compact, yet precise, abstractions for a large selection of MDP case studies. For convenience, the current prototype performs model-level abstractions, first building an MDP in the probabilistic model checker PRISM <ref type="bibr" target="#b20">[22,</ref><ref type="bibr" target="#b35">37]</ref> and then reducing it to a stochastic two-player game. Our framework is also designed to function with higher-level abstraction methods such as predicate abstraction <ref type="bibr" target="#b17">[19]</ref>, which has been adapted to probabilistic models <ref type="bibr" target="#b23">[25,</ref><ref type="bibr" target="#b42">44]</ref>. In fact, the techniques presented in this paper have subsequently been applied to build abstractions of probabilistic models using both predicate abstraction <ref type="bibr" target="#b24">[26]</ref> and convex polyhedra <ref type="bibr" target="#b28">[30]</ref>.</p><p>A preliminary version of this paper, introducing the game-based abstraction approach, was published in conference proceedings as <ref type="bibr" target="#b26">[28]</ref>.</p><p>Outline of the paper In the next section, we present background material on Markov decision processes and stochastic two-player games required in the remainder of the paper. Section 3 describes our abstraction technique and shows its correctness. Based on this, Sect. 4 introduces a corresponding abstraction-refinement framework. In Sect. 5, we describe a prototype implementation of the techniques from Sects. 3 and 4, and present experimental results for a large selection of real-world case studies. Section 6 discusses related work and Sect. 7 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Let R ≥0 denote the set of non-negative reals. For a finite set Q, we denote by Dist(Q) the set of discrete probability distributions over Q, i.e. the set of functions μ : Q → [0, 1] such that q∈Q μ(q) = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Probability measures and spaces</head><p>We begin by briefly introducing some relevant notions from probability and measure theory. For further details, the interested reader is referred to, for example, <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b25">27]</ref>. Definition 1 Let be an arbitrary non-empty set and F a family of subsets of . We say that F is a σ -field on if:</p><p>1. the empty set ∅ is in F ; 2. whenever A is an element of F , the complement \A is also in F ; 3. whenever A i is an element of F for each i ∈ N, the union i∈N A i is also in F .</p><p>The elements of F are called measurable sets, and ( , F ) is called a measurable space. Definition 2 (Probability measure and space) Let ( , F ) be a measurable space. A function Prob : F → [0, 1] is a probability measure on ( , F ) and ( , F , Prob) a probability space, if the following conditions hold:</p><formula xml:id="formula_0">1. Prob( ) = 1 2. if A 1 , A 2 , . . . is a disjoint sequence of elements of F , then Prob( i A i ) = i Prob(A i ).</formula><p>For any family A of subsets of , there exists a unique smallest σ -field containing A <ref type="bibr" target="#b25">[27]</ref>, which we call the σ -field generated by A. Furthermore, given that A satisfies certain properties (see e.g. <ref type="bibr" target="#b25">[27]</ref> for details), and we define the probability for each element of A with a function Prob : A → [0, 1], then the function is guaranteed to extend uniquely to a probability measure on the σ -field generated by A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Discrete-time Markov chains</head><p>We now introduce discrete-time Markov chains (DTMCs), a model for systems with only probabilistic behaviour. A DTMC can be thought of as a state transition system, where transitions are annotated with probabilities indicating the likelihood of their occurrence. Definition 3 A DTMC is a tuple D = (S, s init , P) where:</p><p>-S is a set of states; s init ∈ S is an initial state; -P : S × S → [0, 1] is a transition probability matrix, with s ∈S P(s, s ) = 1 for all s ∈ S.</p><p>For any states s, s ∈ S, the transition probability matrix gives the probability P(s, s ) of making a transition from s to s . A path of a DTMC represents an execution of the system that it is modelling. Formally, a path of a DTMC is a non-empty, finite or infinite sequence of states π = s 0 s 1 s 2 . . . such that P(s i , s i+1 ) &gt; 0 for all i ≥ 0. We denote by π(i) the state s i . For a finite path π fin , we let |π fin | denote the length of π fin (i.e. number of transitions), and let last(π fin ) be its final state. Finally, π (i) denotes the prefix of length i of π , i.e. the finite path π(0) . . . π(i). The sets of all finite and infinite paths starting in state s are denoted Path fin (s) and Path(s), respectively.</p><p>To reason about the probabilistic behaviour of the DTMC, we need to determine the probability that certain paths are taken. This is achieved by defining, for each state s ∈ S, a probability space (Path(s), F s , Prob s ) over the (infinite) paths from s. Below, we give an outline of this construction. For further details, see <ref type="bibr" target="#b25">[27]</ref>.</p><p>The probability measure Prob s is induced by the transition probability matrix P as follows. First, for any finite path π fin ∈ Path fin (s) of length n, we define the probability P s (π fin ):</p><formula xml:id="formula_1">P s (π fin ) def = 1 i f n = 0 P(π fin (0), π fin (1)) • • • P(π fin (n -1), π fin (n)) otherwise.</formula><p>Next, we define the cylinder set of π fin , denoted cyl(π fin ), as follows:</p><formula xml:id="formula_2">cyl(π fin ) def = {π ∈ Path(s) | π fin is a prefix of π}</formula><p>i.e. the cylinder set cyl(π fin ) is the set of all infinite paths which have π fin as a prefix. We denote by Cyl(s) the family of cylinder sets cyl(π fin ), where π fin ranges over the elements of Path fin (s). Then, let F s be the smallest σ -field on Path(s) that contains the family of sets Cyl(s). Due to properties of Cyl(s), we can then define the probability measure on F s as the unique measure which extends the function Prob s : Cyl(s) → [0, 1] where Prob s (cyl(π fin )) = P s (π fin ) for all π fin ∈ Path fin (s).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Markov decision processes</head><p>Markov decision processes (MDPs) are a natural representation for the modelling and analysis of systems with both probabilistic and nondeterministic behaviour.</p><formula xml:id="formula_3">Definition 4 A Markov decision process is a tuple M = (S, s init , Steps, r), where -S is a set of states; -s init ∈ S is an initial state; -Steps : S → 2 Dist(S) is a probabilistic transition function; -r : S × Dist(S) → R ≥0 is a reward function.</formula><p>A probabilistic transition s μ -→ s is made from a state s by first nondeterministically selecting a distribution μ ∈ Steps(s) and then making a probabilistic choice of target state s according to the distribution μ. As for a DTMC, a path of an MDP corresponds to an execution of the system that it is modelling. It represents a particular resolution of both nondeterminism and probability. Formally, a path of an MDP is a non-empty, finite or infinite sequence of probabilistic transitions:</p><formula xml:id="formula_4">π = s 0 μ 0 -→ s 1 μ 1 -→ s 2 μ 2 -→ • • •</formula><p>such that μ i (s i+1 )&gt;0 for all i≥0. We denote by π(i) the state s i and by step(π, i) the distribution μ i . We extend the notions of length, final state and prefix of paths from DTMCs to MDPs in the standard manner.</p><p>In contrast to a path, an adversary (sometimes also known as a policy, strategy or scheduler) represents a particular resolution of nondeterminism only. An adversary resolves the nondeterministic choice in a state of an MDP, based on the history of its execution up to that point. More precisely, an adversary is a function mapping every finite path π fin to a distribution μ ∈ Steps(last(π fin )). For any state s ∈ S and adversary A, let Path A fin (s) and Path A (s) denote the sets of finite and infinite paths starting in s that correspond to A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 5</head><p>An adversary A is called memoryless (or simple) if, for any finite paths π fin and π fin for which last(π fin ) = last(π fin ), we have A(π fin ) = A(π fin ).</p><p>The behaviour of an MDP from a state s, under a given adversary A, is purely probabilistic and can be viewed as a DTMC whose states are given by the set of finite paths Path A fin (s). Formally, the behaviour can be described by the infinite-state DTMC D A s = (S A s , s, P A s ), where:</p><p>-S A s = Path A fin (s); -for any two finite paths π fin , π fin ∈ S A s we have</p><formula xml:id="formula_5">P A s (π fin , π fin ) = μ(s ) if π fin is of the form π fin a,μ -→ s and A(π fin ) = (a, μ) 0 otherwise.</formula><p>There is a one-to-one correspondence between the infinite paths of the DTMC D A s and Path A (s). It therefore follows that, using the probability measure construction for DTMC given in Sect. 2.2, we can construct a probability space (Path A (s), F A s , Prob A s ). Based on this construction, we now introduce two quantitative measures for MDPs which together form the basis for probabilistic model checking of MDPs <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13]</ref>.</p><p>The first measure is probabilistic reachability, which refers to the probability of reaching a set of target states. For adversary A, the probability of reaching the target set F ⊆ S from state s is given by:</p><formula xml:id="formula_6">p A s (F ) def = Prob A s {π ∈ Path A (s) | ∃i ∈ N.π(i) ∈ F }.</formula><p>The second measure we consider is expected reachability, which refers to the expected reward accumulated before reaching a set of target states. For an adversary A, the expected reward of reaching the target set F from state s, denoted e A s (F ), is defined as the expectation <ref type="bibr" target="#b2">[3]</ref>, with respect to the probability measure Prob A s , of the random variable r(F, •) : Path A (s) → R which returns, for a given path, the total reward accumulated until a state in F is reached along the path. Formally we have:</p><formula xml:id="formula_7">e A s (F ) def = π ∈Path A (s) r(F, π) dProb A s</formula><p>where for any path π ∈ Path A (s):</p><formula xml:id="formula_8">r(F, π) def = min{j |π(j )∈F } i=1 r(π(i-1), step(π, i-1)) if ∃j ∈ N.π(j ) ∈ F ∞ otherwise.</formula><p>For simplicity, we have defined the reward of a path that does not reach F to be ∞, even though the total reward of the path may not be infinite. Essentially, this means that the expected reward of reaching F from s under A is finite if and only if, under the adversary A, a state in F is reached from s with probability 1.</p><p>A natural way to reason about the behaviour of an MDP is to consider the minimum and maximum values of these measures, quantifying over all adversaries. Definition 6 For an MDP M = (S, s init , Steps, r), set of target states F ⊆ S and state s ∈ S, the minimum and maximum reachability probabilities of reaching F from s equal:</p><formula xml:id="formula_9">p min s (F ) = inf A p A s (F ) and p max s (F ) = sup A p A s (F )</formula><p>and the minimum and maximum expected rewards of reaching F from s equal:</p><formula xml:id="formula_10">e min s (F ) = inf A e A s (F ) and e max s (F ) = sup A e A s (F ).</formula><p>We write, for example, p min (F ) to denote the vector of values p min s (F ) over all states s ∈ S. More generally, for any vector x over S, we write x s to denote the element for state s ∈ S.</p><p>Computing values for probabilistic and expected reachability reduces to the stochastic shortest path problem for Markov decision processes; see for example <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12]</ref>. A key result in this respect is that optimality with respect to probabilistic and expected reachability can always be achieved with memoryless adversaries (see <ref type="bibr">Definition 5)</ref>. A consequence of this is that these quantities can be computed through a method known as value iteration <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12]</ref>, which iteratively computes increasingly precise approximations to the required values. The lemma below forms the basis for value iteration.</p><p>Lemma 1 Consider an MDP M = (S, s init , Steps, r) and set of target states F ⊆ S.</p><p>-The sequences of vectors p min, (n)  n∈N and p max,(n) n∈N converge to the vectors p min (F ) and p max (F ), where for any state s ∈ S:</p><formula xml:id="formula_11">-if s ∈ F , then p min,(n) s = p max,(n) s = 1 for all n ∈ N; -if s ∈ F , then: p min,(n) s = 0 if n = 0 min μ∈Steps(s) s ∈S μ(s ) • p min,(n-1) s otherwise p max,(n) s = 0 if n = 0 max μ∈Steps(s) s ∈S μ(s ) • p max,(n-1) s otherwise.</formula><p>-The sequences of vectors e min, (n)  n∈N and e max,(n) n∈N converge to the vectors e min (F ) and e max (F ), where for any state s ∈ S:</p><formula xml:id="formula_12">-if s ∈ F , then e min,(n) s = e max,(n) s = 0 for all n ∈ N; -if s ∈ F , then: e min,(n) s = ⎧ ⎪ ⎨ ⎪ ⎩ ∞ if p max s &lt; 1 0 if p max s = 1 and n = 0 min μ∈Steps(s) (r(s, μ) + s ∈S μ(s ) • e min,(n-1) s ) otherwise e max,(n) s = ⎧ ⎪ ⎨ ⎪ ⎩ ∞ if p min s &lt; 1 0 if p min s =<label>1</label></formula><p>and n = 0 max μ∈Steps(s) (r(s, μ) + s ∈S μ(s ) • e max,(n-1) s ) otherwise.</p><p>In practice, value iteration is carried out by computing the vector of required values for increasingly large values of n, until a pre-specified convergence criterion is met. One common approach is to check that the maximum difference between the corresponding elements of successive vectors is below some fixed threshold δ. Another is to use the maximum relative difference of vector elements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Stochastic two-player games</head><p>Finally in this section, we describe (simple) stochastic games <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b38">40]</ref>, which are turn-based games involving two players and chance.</p><formula xml:id="formula_13">Definition 7 A stochastic two-player game is a tuple G = ((V , E), v init , (V 1 , V 2 , V p ), δ, r) where: -(V , E) is a finite directed graph; -v init ∈ V is an initial vertex; -(V 1 , V 2 , V p ) is a partition of V ; -δ : V p → Dist(V ) is a probabilistic transition function; -r : E → R ≥0 is a reward function over edges.</formula><p>Vertices in V 1 , V 2 and V p are called 'player 1', 'player 2' and 'probabilistic' vertices, respectively. The game proceeds in steps, moving from a vertex v to one of its neighbouring vertices v in the game graph. The choice of v depends on the type of the vertex v. If v ∈ V 1 then player 1 chooses v , if v ∈ V 2 then player 2 makes the choice, and if v ∈ V p then v is selected randomly according to the distribution δ(v). A Markov decision process can thus be thought of as a stochastic game in which there are no player 2 vertices and where there is a strict alternation between player 1 and probabilistic vertices.</p><p>A play in a game G is a sequence of vertices ω = v 0 v 1 v 2 . . . such that (v i , v i+1 ) ∈ E for all i ∈ N. We denote by ω(i) the vertex v i and, for a finite play ω fin , we write last(ω fin ) for the final vertex of ω fin and |ω fin | for its length (the number of transitions). The prefix of length i of play ω is denoted ω (i) .</p><p>A strategy for player 1 is a function</p><formula xml:id="formula_14">σ 1 : V * V 1 → Dist(V ), i.</formula><p>e. a function from the set of finite plays ending in a player 1 vertex to the set of distributions over vertices, such that for any</p><formula xml:id="formula_15">ω fin ∈ V * V 1 and v ∈ V , if σ 1 (ω fin )(v)&gt;0, then (last(ω fin ), v) ∈ E.</formula><p>Strategies for player 2, denoted by σ 2 , are defined analogously. For a fixed pair of strategies (σ 1 , σ 2 ) we denote by Play σ 1 ,σ 2 fin (v) and Play σ 1 ,σ 2 (v) the set of finite and infinite plays starting in vertex v that correspond to these strategies. For strategy pair (σ 1 , σ 2 ), the behaviour of the game is completely random and, for any vertex v, we can construct a probability space</p><formula xml:id="formula_16">(Play σ 1 ,σ 2 (v), F σ 1 ,σ 2 v , Prob σ 1 ,σ 2 v</formula><p>). This construction proceeds in similar fashion to the one described for MDPs in the previous section.</p><p>We can then, as for MDPs, define the notions of probabilistic reachability and expected reachability. For a fixed strategy pair (σ 1 , σ 2 ), vertex v ∈ V and a set of target vertices F ⊆ V , we define the corresponding probability of reaching F as:</p><formula xml:id="formula_17">p σ 1 ,σ 2 v (F ) def = Prob σ 1 ,σ 2 v {ω ∈ Play σ 1 ,σ 2 (v) | ∃i ∈ N ∧ ω(i) ∈ F }</formula><p>and the expected reward of reaching F as:</p><formula xml:id="formula_18">e σ 1 ,σ 2 v (F ) def = ω∈Play σ 1 ,σ 2 (v) r(F , ω) dProb σ 1 ,σ 2 v</formula><p>where for any play ω ∈ Play σ 1 ,σ 2 (v):</p><formula xml:id="formula_19">r(F , ω) def = min{j |ω(j )∈F } i=1 r(ω(i-1), ω(i)) if ∃j ∈ N.ω(j ) ∈ F ∞ otherwise. Definition 8 For a game G = ((V , E), v init , (V 1 , V 2 , V p ), δ, r), set of target vertices F ⊆ V</formula><p>and vertex v ∈ V , the optimal probabilities of the game for player 1 and player 2, with respect to F and v, are defined as follows:</p><p>sup</p><formula xml:id="formula_20">σ 1 inf σ 2 p σ 1 ,σ 2 v (F ) and sup σ 2 inf σ 1 p σ 1 ,σ 2 v (F )</formula><p>and the optimal expected rewards are:</p><p>sup</p><formula xml:id="formula_21">σ 1 inf σ 2 e σ 1 ,σ 2 v (F ) and sup σ 2 inf σ 1 e σ 1 ,σ 2 v (F ).</formula><p>A player 1 strategy σ 1 is optimal from vertex v with respect to the probability of reaching target set F if:</p><formula xml:id="formula_22">inf σ 2 p σ 1 ,σ 2 v (F ) = sup σ 1 inf σ 2 p σ 1 ,σ 2 v (F ).</formula><p>(</p><formula xml:id="formula_23">)<label>1</label></formula><p>The optimal strategies for player 2 and for expected rewards can be defined analogously. We now summarise a number of results from <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 9</head><p>A strategy σ i is pure if it does not use randomisation, that is, for any finite play ω fin such that last(ω fin ) ∈ V i , there exists v ∈ V such that σ i (ω fin )(v ) = 1. A strategy σ i is memoryless if its choice depends only on the current vertex, that is, σ i (ω fin ) = σ i (ω fin ) for any finite plays ω fin and ω fin such that last(ω fin ) = last(ω fin ).</p><p>As is the case with MDPs, for any stochastic game, the family of pure memoryless strategies suffices for optimality with respect to probabilistic and expected reachability. Moreover, these values can be computed through an iterative processes known as value iteration <ref type="bibr" target="#b9">[10]</ref>.</p><formula xml:id="formula_24">Lemma 2 Consider a stochastic game G = ((V , E), v init , (V 1 , V 2 , V p ), δ, r) and set of target vertices F ⊆ V .</formula><p>-The sequence of vectors p (n)  n∈N converges to the optimal probabilities for player 1 with respect to the target set F , where for any vertex v ∈ V :</p><formula xml:id="formula_25">-if v ∈ F , then p (n) v =</formula><p>1 for all n ∈ N; -and otherwise:</p><formula xml:id="formula_26">p (n) v = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ 0 if n = 0 max (v,v )∈E p (n-1) v if n &gt; 0 and v ∈ V 1 min (v,v )∈E p (n-1) v if n &gt; 0 and v ∈ V 2 v ∈V δ(v)(v ) • p (n-1) v if n &gt; 0 and v ∈ V p .</formula><p>-The sequence of vectors e (n)  n∈N converges to the optimal expected rewards for player 1 with respect to the target set F , where for any vertex v ∈ V :</p><formula xml:id="formula_27">-if v ∈ F , then e (n) v = 0 for all n ∈ N; -if sup σ 2 inf σ 1 p σ 1 ,σ 2 v (F ) &lt; 1, then e (n) v =</formula><p>∞ for all n ∈ N; -and otherwise:</p><formula xml:id="formula_28">e (n) v = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ 0 if n = 0 max (v,v )∈E (r(v, v ) + e (n-1) v ) if n &gt; 0 and v ∈ V 1 min (v,v )∈E (r(v, v ) + e (n-1) v ) if n &gt; 0 and v ∈ V 2 v ∈V (r(v, v ) + δ(v)(v ) • e (n-1) v ) if n &gt; 0 and v ∈ V p .</formula><p>Lemma 2 forms the basis of an iterative method to compute the vector of optimal values for a game. Note that, although this concerns only the optimal probability for player 1, similar results hold for player 2. Observe the similarity between this and the value iteration method for MDP solution described in Lemma 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Abstraction for Markov decision processes</head><p>We now present our notion of abstraction for MDPs, which is based on stochastic twoplayer games. The key idea is to maintain a distinction between the nondeterminism in an MDP and the nondeterminism introduced by abstracting it. The abstraction of an MDP is a stochastic two-player game in which the choices made by player 1 correspond to the nondeterminism from the process of abstraction and the choices made by player 2 correspond to the nondeterminism in the original MDP.</p><p>For an MDP M = (S, s init , Steps, r), which we refer to as the concrete model, we construct an abstraction based on a partition P = {S 1 , S 2 , . . . , S n } of its state space S. The elements of P are referred to as abstract states, each comprising a set of concrete states. Intuitively, in the stochastic game representing the abstraction, player 1 vertices are abstract states and player 2 vertices correspond to the sets of nondeterministic choices in individual concrete states.</p><p>In order to formally define the abstraction process, we need a few preliminary definitions. For any distribution μ over S, we denote by μ the probability distribution over P lifted from μ, i.e. μ(S i ) = s∈S i μ(s) for all S i ∈ P. In addition, for any state s, we denote by Steps(s) the set of reward-distribution pairs {(r(s, μ), μ) | μ ∈ Steps(s)}. The inclusion of rewards in the definition of Steps(s) ensures that the reward values associated with probability distributions in the concrete model are preserved, even in the case where multiple (concrete) distributions are lifted to the same distribution over abstract states. This differs from <ref type="bibr" target="#b26">[28]</ref> where, to simplify the presentation, we assumed that distributions that were lifted to the same abstract distribution had identical reward values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 10</head><p>Given an MDP M = (S, s init , Steps, r) and a partition P = {S 1 , . . . , S n } of the state space S, we define the corresponding abstraction G P M as the stochastic game:</p><formula xml:id="formula_29">G P M = ((V , E), v init , (V 1 , V 2 , V p ), δ, r)</formula><p>in which:</p><p>-</p><formula xml:id="formula_30">V 1 = P; -V 2 = {v 2 ⊆ R ≥0 ×Dist(P) | v 2 = Steps(s) for some s ∈ S}; -V p = {v p ∈ R ≥0 ×Dist(P) | v p ∈ Steps(s) for some s ∈ S}; -v init = S i where s init ∈ S i ; -(v, v ) ∈ E if</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>and only if one of the following conditions holds:</head><p>- shows the stochastic game representing the corresponding abstraction. Player 1 vertices are drawn as grey rectangles, each corresponding to an element of the partition P. The player 2 vertices that are successors of each player 1 vertex are drawn as white shapes, enclosed within it. The concrete states corresponding to each player 2 vertex are also depicted inside the player 2 vertices. These are states from the MDP whose outgoing choices are identical after lifting. Probabilistic vertices are not drawn explicitly, but rather shown as groups of probabilistic transitions emanating from each player 2 vertex, in the same style as the MDP. For completeness, we also show, in Fig. <ref type="figure" target="#fig_0">1</ref>(d), the same stochastic game with all vertices illustrated explicitly. Player 1, player 2 and probabilistic vertices are coloured grey, white and black, respectively.</p><formula xml:id="formula_31">v ∈ V 1 , v ∈ V 2 and v = Steps(s) for some s ∈ v; -v ∈ V 2 , v ∈ V p and v ∈ v; -v ∈ V p , v ∈ V 1 and v = (r, μ) such that μ(v )&gt;0; -δ : V p → Dist(V ) projects (r, μ) ∈ V p onto μ; -r : E → R ≥0 is the reward function such that for any (v, v ) ∈ V ×V : r(v, v ) = r if (v, v ) ∈ V 2 ×V p ,</formula><p>Intuitively, the roles of the vertices and players in the abstraction can be understood as follows. A player 1 vertex corresponds to an abstract state: an element of the partition P of the states from the original MDP. Player 1 chooses a concrete state from this set and then player 2 chooses a probability distribution from those available in the concrete state (which is now a distribution over abstract states rather than concrete states).</p><p>This description perhaps misleadingly gives the impression that the stochastic game representing the abstraction is no smaller than the original concrete model. This is not the case. Firstly, note that V 2 vertices are actually sets of reward-distribution pairs that correspond to concrete states, not the concrete states themselves. Hence, concrete states with the same reward values and outgoing distributions (after lifting) are collapsed into one player 2 vertex (see, for example, s 1 and s 2 in Fig. <ref type="figure" target="#fig_1">1(c)</ref>). Furthermore, in practice there is no need to store the entire vertex set V of the stochastic game. Since we have a strict alternation between V 1 , V 2 and V p vertices, we need only store the vertices in V 1 , the outgoing transitions comprising each reward-distribution pair from V 1 , and how these pairs are grouped (into elements of V 2 ). Later, in Example 3 and Sect. 5 we will show how, on all the case studies considered, the abstraction process brings a significant reduction in model size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Analysis of the abstraction</head><p>In this section we describe how, for an MDP M and partition P, the stochastic game G P M representing the abstraction of M yields lower and upper bounds for probabilistic reachability and expected reachability properties of M. To ease notation we let, for x ∈ {p, e} and F ⊆ V :</p><formula xml:id="formula_32">x lb,min v (F ) def = inf σ 1 ,σ 2 x σ 1 ,σ 2 v (F ) x ub,min v (F ) def = sup σ 1 inf σ 2 x σ 1 ,σ 2 v (F ) x lb,max v (F ) def = sup σ 2 inf σ 1 x σ 1 ,σ 2 v (F ) x ub,max v (F ) def = sup σ 1 ,σ 2 x σ 1 ,σ 2 v (F ).</formula><p>The values x ub,min v (F ) and x lb,max v (F ) are the optimal reachability probabilities (if x = p) and expected rewards (if x = e) for players 1 and 2, respectively. As we have seen in Lemma 2, these optimal values can be generated using a simple iterative computation. This process can also be used to construct a pair of (memoryless) strategies that result in these optimal values <ref type="bibr" target="#b9">[10]</ref>. The other values, x lb,min v (F ) and x ub,max v (F ), although not usually considered for stochastic games (because the players co-operate), can be computed similarly by considering the game as an MDP <ref type="bibr" target="#b1">[2]</ref> (see <ref type="bibr">Lemma 1)</ref>. The following theorem shows how these four values represent bounds on the corresponding properties of the MDP.</p><p>Theorem 1 Let M = (S, s init , Steps, r) be an MDP, P a partition of S and F ∈ P a set of target states.</p><formula xml:id="formula_33">If G P M = ((V , E), v init , (V 1 , V 2 , V p ), δ, r)</formula><p>is the stochastic game constructed from M and P according to Definition 10, then for any x ∈ {e, p} and s ∈ S:</p><formula xml:id="formula_34">x lb,min v (F ) ≤ x min s (F ) ≤ x ub,min v (F ) x lb,max v (F ) ≤ x max s (F ) ≤ x ub,max v (F )</formula><p>where v is the unique vertex of V 1 such that s ∈ v and F = {F }.</p><p>We also show that using a finer partition results in a more precise abstraction, i.e. a stochastic game providing tighter lower and upper bounds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 11</head><p>If P and P are partitions of a state space S, then P is finer than P (denoted P P ) if, for any v ∈ P, there exists v ∈ P such that v ⊆ v . Equivalently, we say that P is coarser than P.</p><p>Theorem 2 Let M = (S, s init , Steps, r) be an MDP, P and P partitions of S such that P P and F ∈ P ∩ P a set of target states.</p><formula xml:id="formula_35">If G P M = ((V , E), v init , (V 1 , V 2 , V p ), δ, r)</formula><p>and</p><formula xml:id="formula_36">G P M = ((V , E ), v init , (V 1 , V 2 , V p ), δ , r</formula><p>) are the stochastic games constructed from P and P according to Definition 10, then for any x ∈ {e, p} and v ∈ P:</p><formula xml:id="formula_37">x lb,min v (F ) ≤ x lb,min v (F ) and x ub,min v (F ) ≤ x ub,min v (F ) x lb,max v (F ) ≤ x lb,max v (F ) and x ub,max v (F ) ≤ x ub,max v (F )</formula><p>where v is the unique element of P such that v ⊆ v and F = {F }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Examples</head><p>In this section, we present examples illustrating the application of the results given above. Then, in the following section, we provide proofs of Theorems 1 and 2.</p><p>Example 2 Let us return to the previous simple example (see Example 1 and Fig. <ref type="figure" target="#fig_0">1</ref>). Suppose that we are interested in the minimum probability in the original MDP (Fig. <ref type="figure" target="#fig_1">1(a)</ref>) of, starting from state s 0 , reaching the target set {s 6 }. This can be computed as 0.15. From the abstraction shown in Fig. <ref type="figure" target="#fig_1">1(c</ref>) and the results of Theorem 1, we can establish that the minimum probability lies within the interval [0, 0.5]. If, on the other hand, the abstract model had instead been constructed as an MDP, i.e. with no discrimination between the two forms of nondeterminism, we would only have been able to determine that the minimum reachability probability lay in the interval [0, 1].</p><p>Example 3 To illustrate the application of our abstraction to a larger MDP, we consider a more complex example: a model of the Zeroconf protocol <ref type="bibr" target="#b6">[7]</ref> for dynamic self-configuration of local IP addresses within a local network. Zeroconf provides a distributed, 'plug and play' approach to IP address configuration, managed by the individual devices of the network. The model concerns the situation where a new device joins a network of N existing hosts, in which there are a total of M IP addresses available. For full details of the MDP model of the protocol and partition used in the abstraction process, see <ref type="bibr" target="#b26">[28]</ref>.</p><p>Table <ref type="table" target="#tab_0">1</ref> shows the size of the concrete model (MDP) and its abstraction (game) for a range of values of N and M. For each model, we compute the minimum and maximum expected time for a host to complete the protocol (i.e. to configure and start using an IP address). Table <ref type="table" target="#tab_0">1</ref> shows the exact results, obtained from the MDP, and the lower and upper bounds, obtained from the abstraction. In addition, Fig. <ref type="figure">2</ref> illustrates results for the maximum probability that the new host has not configured successfully by time T .</p><p>As stated previously, an advantage of our approach is the ability to quantify the utility of the abstraction, based on the difference between the lower and upper bounds obtained. In the case of the plots in Fig. <ref type="figure">2</ref>, for a particular time bound T this difference is indicated by the vertical distance between the curves for the lower and upper bounds at the point T on the horizontal axis. Examining these differences between bounds for the presented results, it can be seen that our abstraction approach yields tight approximations for the performance characteristics of the protocol while at the same time producing a significant reduction in both the number of states and transitions. Observe also that the size of the abstract model increases linearly, rather than exponentially, in N and is independent of M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Correctness of the abstraction</head><p>We now prove Theorems 1 and 2, presented in Sect. 3.1. Before doing so, we require a number of preliminary concepts. For the remainder of this section we fix an MDP M = (S, s init , Steps, r), partitions P and P of S such that P P and set of target states F ∈ P ∩ P .</p><formula xml:id="formula_38">Let G P M = ((V , E), v init , (V 1 , V 2 , V p ), δ, r) and G P M = ((V , E ), v init , (V 1 , V 2 , V p )</formula><p>, δ , r ) be the stochastic games obtained from P and P according to Definition 10 and let F = {F }.</p><p>To simplify notation, we will add subscripts to distributions μ and sets of rewarddistributions pairs Steps(s), indicating the partition that was used to lift them from the concrete to the abstract state space, e.g. for partition P, μ ∈ Dist(S), v ∈ P and s ∈ S, we have μ P (v) = s∈v μ(s) and Steps(s</p><formula xml:id="formula_39">) P = {(r(s, μ), μ P ) | μ ∈ Steps(s)}.</formula><p>We next define a mapping from vertices of G P M to the vertices of</p><formula xml:id="formula_40">G P M . Definition 12 Let [•] : V → V be the mapping from vertices of G P M to vertices of G P M such that for any v ∈ V : -if v ∈ P, then [v] = v where v is the unique element of P such that v ⊆ v (such a v</formula><p>exists from the fact that P P ); -if v = Steps(s) P for some s ∈ S, then [v] = Steps(s) P ; -if v = (r, μ P ) for some r ∈ R ≥0 and μ ∈ Dist(S), then [v] = (r, μ P ).</p><p>By construction, this mapping extends naturally to plays, i.e. for any play ω</p><formula xml:id="formula_41">= v 0 v 1 v 2 . . . of G P M we have [ω] = [v 0 ][v 1 ][v 2 ] . . . is a play of G P M .</formula><p>We denote by [•] -1 the inverse mapping from plays of G P M to sets of plays of G P M , i.e. for any finite play ω of</p><formula xml:id="formula_42">G P M we have [ω ] -1 = {ω ∈ V * | [ω] = ω }.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 3 For any play ω of</head><formula xml:id="formula_43">G P M , it holds that r(F , ω) = r (F , [ω]).</formula><p>Proof The proof follows directly from Definition 12 and since we assume that F is an element of both P and P .</p><p>Using this mapping between plays, for any player 1 vertex v and strategy pair σ = (σ 1 , σ 2 ) of the game G P M , we now construct a (randomised) strategy pair</p><formula xml:id="formula_44">[σ ] v = ([σ 1 ] v , [σ 2 ] v ) of the game G P</formula><p>M such that, under σ and [σ ] v , when starting at v and [v] respectively, the probability of reaching F is equal. For any finite play ω of G P M such that ω (0) = [v] and last(ω ) ∈ V i , let the probability of [σ i ] v selecting vertex v ∈ V after play ω equal:</p><formula xml:id="formula_45">[σ i ] v (ω )(v ) def = Prob σ v ([ω v ] -1 v ) Prob σ v ([ω ] -1 v )</formula><p>where</p><formula xml:id="formula_46">[ ω ] -1 v = {ω ∈ [ ω ] -1 | ω(0) = v} for any play ω of G P M and Prob σ v ( ) = Prob σ v {ω ∈ Play σ (v) | ω (i) ∈ for some i ∈ N}</formula><p>for any set of finite plays of G P M .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposition 1 For any vertex v and strategy pair</head><formula xml:id="formula_47">σ = (σ 1 , σ 2 ) of G P M , if is a measurable set of plays of Play [ σ ]v ([v]), then Prob [σ ]v [v] ( ) = Prob σ v ([ ] -1 v ).</formula><p>Proof From the measure construction (see Sect. 2) it is sufficient to show that:</p><formula xml:id="formula_48">Prob [σ ]v [v] (ω ) = Prob σ v [ω ] -1 v</formula><p>for any finite play ω of G P M and vertex v ∈ [ω (0)] -1 , which we now prove by induction on the length of the play ω . If |ω | = 1, then ω comprises a single vertex and since [ω ] -1 v = {v}, by the measure construction:</p><formula xml:id="formula_49">Prob [σ ]v [v] (ω ) = Prob σ v [ω ] -1 v = 1.</formula><p>Next, suppose by induction that the lemma holds for all plays of length n. Consider any play ω of length n+1 of G P M . By definition ω is of the form ω ṽ for some play ω of length n and vertex ṽ ∈ V . If last( ω ) ∈ V i (for i = 1, 2), then from the measure construction:</p><formula xml:id="formula_50">Prob [σ ]v [v] (ω ) = Prob [σ ]v [v] ( ω ) • [σ i ] v ( ω )( ṽ ) = Prob σ v ([ ω ] -1 v ) • [σ i ] v ( ω )( ṽ ) by induction = Prob σ v ([ ω ] -1 v ) • Prob σ v ([ ω ṽ ] -1 v ) Prob σ v ([ ω ] -1 v ) by definition of [σ i ] v = Prob σ v ([ ω ṽ ] -1 v ) rearranging = Prob σ v ([ω ] -1 v )</formula><p>since ω = ω ṽ .</p><p>On the other hand, if last( ω ) ∈ V p , then last( ω ) = (r, μ P ) for some r ∈ R ≥0 and μ ∈ Dist(S), and hence in this case, again from the measure construction, we have:</p><formula xml:id="formula_51">Prob [σ ]v [v] (ω ) = Prob [σ ]v [v] ( ω ) • μ P ( ṽ ) = Prob σ v ([ ω ] -1 v ) • μ P ( ṽ ) by induction = Prob σ v ([ ω ] -1 v ) • ṽ∈[ ṽ ] -1 μ P ( ṽ) by Definition 12 = ṽ∈[ ṽ ] -1 Prob σ v ([ ω ] -1 v ) • μ P ( ṽ) rearranging = ṽ∈[ ṽ ] -1 Prob σ v ([ ω ] -1 v ṽ) by definition of Prob σ v = Prob σ v ([ω ] -1 v )</formula><p>by Definition 12 and since ω = ω ṽ .</p><p>Since these are all the cases to consider, the lemma holds by induction.</p><p>Before we give the proofs of Theorems 1 and 2 we require the following lemma, a classical result from measure theory and two propositions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 4 For any vertex v and strategy pair</head><formula xml:id="formula_52">σ = (σ 1 , σ 2 ) of G P M , the mapping [•] of Defini- tion 12 is a measurable function between (Play σ (v), F σ v ) and (Play [σ ]v ([v]), F [σ ]v [v] ).</formula><p>Proof The proof follows from measure construction (see Sect. 2).</p><p>Theorem 3 <ref type="bibr" target="#b2">[3]</ref> Let ( , F ) and ( , F ) be measurable spaces, and suppose that Pr is a measure on ( , F ) and the function T : → is measurable. If f is a real non-negative measurable function on ( , F ), then:</p><formula xml:id="formula_53">ω∈ f (T ω) d Pr = ω ∈ f (ω ) d Pr T -1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposition 2 For any vertex v and strategy pair σ of</head><formula xml:id="formula_54">G P M we have p σ v (F ) = p [ σ ]v [v] (F ) and e σ v (F ) = e [ σ ]v [v] (F ).</formula><p>Proof The first equation follows from Proposition 1 using standard results from measure theory. For the second equation, by definition of the expected rewards for games (see Sect. 2.4), we have:</p><formula xml:id="formula_55">e σ v (F ) = ω∈Play σ (v) r(F , ω) dProb σ v = ω∈Play σ (v) r (F , [ω]) dProb σ v by Lemma 3 = ω ∈Play [σ ]v ([v]) r(F , ω ) dProb σ v ([•] -1</formula><p>) by Theorem 3 and Lemma 4</p><formula xml:id="formula_56">= ω ∈Play [σ ]v ([v]) r(F , ω ) dProb [σ ]v [v]</formula><p>by Proposition 1</p><formula xml:id="formula_57">= e [σ ]v [v] (F )</formula><p>as required.</p><p>Proposition 3 For any vertex v and player 2 strategy σ 2 of G P M there exists a player 2 strategy σ 2 of G P M such that</p><formula xml:id="formula_58">sup σ 1 x σ 1 ,σ 2 v (F ) ≥ sup σ 1 x σ 1 ,σ 2 v (F ) inf σ 1 x σ 1 ,σ 2 v (F ) ≤ inf σ 1 x σ 1 ,σ 2 v (F )</formula><p>for any vertex v of G P M such that [v] = v and x ∈ {e, p}.</p><p>Proof Let v be a vertex and σ 2 a player 2 strategy of G P M and let v be any vertex of</p><formula xml:id="formula_59">G P M such that [v] = v .</formula><p>In the first step of the proof we construct the player 2 strategy σ 2 . Therefore consider any finite play ω of G P M that ends in a player 2 vertex. By Definition 10 we have last(ω) = Steps(s) P for some s ∈ S and [ω] is a finite play of G P M ending in the player 2 vertex Steps(s) P . Now σ 2 ([ω]) = (r(s, μ), μ P ) for some μ ∈ Steps(s) and we let σ 2 (ω) = (r(s, μ), μ P ) which by Definition 10 is an element of Steps(s) P , and hence is a valid player 2 choice in G P M . Although the μ meeting the above requirements is not necessarily unique, for the purposes of the proof, it is sufficient to choose any such distribution. From Definition 12 it then follows that:</p><formula xml:id="formula_60">[ω(σ 2 (ω))] = [ω]σ 2 ([ω]) for all finite plays ω of G P M that end in a player 2 vertex.<label>(2)</label></formula><p>In the next step of the proof we will show that, for any player 1 strategy</p><formula xml:id="formula_61">σ 1 of G P M , if ([σ 1 ] v , [σ 2 ] v )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>is the strategy pair of G P</head><p>M constructed from the strategy pair (σ 1 , σ 2 ) of G P M following the method described above, then [σ 2 ] v (ω) = σ 2 (ω) for all finite plays ω of G P M such that ω (0) = [v] and last(ω ) is a player 2 vertex. Therefore, consider any such play ω of G P M . Now, by construction of ([σ 1 ] v , [σ 2 ] v ) we have that for any v ∈ V :</p><formula xml:id="formula_62">[σ 2 ] v (ω )(v ) = Prob σ v ([ω v ] -1 v ) Prob σ v ([ω ] -1 v )</formula><p>where</p><formula xml:id="formula_63">[ ω ] -1 v = {ω ∈ [ ω ] -1 | ω(0) = v} for any play ω of G P M and Prob σ 1 ,σ 2 v ( ) = Prob σ 1 ,σ 2 v {ω ∈ Play σ 1 ,σ 2 (v) | ω (i) ∈ for some i ∈ N}</formula><p>for any set of finite plays of G P M . Now, from (2) we have that:</p><formula xml:id="formula_64">{ω ∈ Play σ 1 ,σ 2 (v) | [ω] = ω v } = {ω(σ 2 (ω)) ∈ Play σ 1 ,σ 2 (v) | [ω] = ω } if v = σ 2 ([ω]) ∅ otherwise.</formula><p>Combining this with the definition of [σ 2 ] v , it follows that:</p><formula xml:id="formula_65">[σ 2 ] v (ω )(v ) = 1 if v = σ 2 ([ω]) 0 otherwise,</formula><p>and hence, since ω was arbitrary, we have that [σ 2 ] v (ω) = σ 2 (ω) for all finite play ω of G P M such that ω (0) = [v] and last(ω ) is a player 2 vertex.</p><p>In the final step of the proof, we will use these results to show that the proposition holds. Since v was arbitrary, using Proposition 2 and the above correspondence between σ 2 and [σ 2 ], we have that for any player 1 strategy σ 1 of G P M :</p><p>x</p><formula xml:id="formula_66">[σ 1 ],σ 2 v (F ) = x σ 1 ,σ 2 v (F )</formula><p>for all vertices v of G P M such that [v] = v and x ∈ {e, p}. Hence, since σ 1 was arbitrary, it follows that:</p><formula xml:id="formula_67">sup σ 1 x σ 1 ,σ 2 v (F ) ≥ sup σ 1 x σ 1 ,σ 2 v (F ) inf σ 1 x σ 1 ,σ 2 v (F ) ≤ inf σ 1 x σ 1 ,σ 2 v (F )</formula><p>for any vertex v of G P M such that [v] = v and x ∈ {e, p} as required.</p><p>We are now in a position to give the proofs of Theorems 1 and 2.</p><p>Proof of Theorem 2 Consider any x ∈ {e, p} and v ∈ P. From Proposition 2 it follows that: inf</p><formula xml:id="formula_68">σ 1 ,σ 2 x σ 1 ,σ 2 [v] (F ) ≤ inf σ 1 ,σ 2 x σ 1 ,σ 2 v (F ) sup σ 1 ,σ 2 x σ 1 ,σ 2 [v] (F ) ≥ sup σ 1 ,σ 2 x σ 1 ,σ 2 v (F )</formula><p>and hence x lb,min</p><formula xml:id="formula_69">[v] (F ) ≤ x lb,min v (F ) and x ub,max v (F ) ≤ x ub,max [v]</formula><p>(F ). On the other hand, by definition we have:</p><formula xml:id="formula_70">x ub,min v (F ) = sup σ 1 inf σ 2 x σ 1 ,σ 2 [v] (F ) = inf σ 2 sup σ 1 x σ 1 ,σ 2 [v] (F ) by [9, 40] ≥ inf σ 2 sup σ 1 x σ 1 ,σ 2 v (F ) by Proposition 3 = sup σ 1 inf σ 2 x σ 1 ,σ 2 v (F ) by [9, 40] = x ub,min [v] (F ) by definition.</formula><p>Similarly, by definition and <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b38">40]</ref>:</p><formula xml:id="formula_71">x lb,max [v] (F ) = sup σ 2 inf σ 1 x σ 1 ,σ 2 [v] (F ) ≥ sup σ 2 inf σ 1 x σ 1 ,σ 2 v (F ) by Proposition 3 = x lb,max v (F )</formula><p>by definition and <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b38">40]</ref> which completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Theorem 1</head><p>The proof follows from Theorem 2 together with the following facts.</p><p>-The partition P = {{s} | s ∈ S} is finer than all other partitions.</p><p>-If G P M is constructed from partition P = {{s} | s ∈ S} according to Definition 10, then:</p><formula xml:id="formula_72">x lb,min {s} (F ) = x min s (F ) = x ub,min {s} (F ) x lb,max {s} (F ) = x max s (F ) = x ub,max {s} (F )</formula><p>for all s ∈ S and x ∈ {e, p}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">An abstraction-refinement framework for Markov decision processes</head><p>We now present a framework for automatically generating abstractions of MDPs, which uses the game-based abstraction method of Sect. 3. This is inspired by the success of counterexample-guided abstraction-refinement (CEGAR) techniques, developed for nonprobabilistic verification, which iteratively refine an abstraction based on results from model checking. Probabilistic verification, however, is typically quantitative in nature: probabilistic model checking of MDPs essentially involves computing minimum or maximum probabilistic or expected reachability measures. Thus, we propose a quantitative abstractionrefinement process.</p><p>The construction and analysis of a game-based abstraction of an MDP yields lower and upper bounds for these values (plus corresponding strategies). The difference between these bounds can be seen as a measure of the quality of the abstraction. If this difference is too high (say, above some error ε), then it is desirable to refine the abstraction to reduce the difference. This provides the basis for a quantitative abstraction-refinement loop, illustrated in Fig. <ref type="figure">3</ref>: starting with a simple, coarse abstraction, we refine the abstraction until the difference between the bounds is below ε. In fact we can consider different criteria for termination of the process: for example checking that the difference is below ε for a single abstract state or set of abstract states, such as the abstract states containing an initial state.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Refinement strategies</head><p>In our context, refinement corresponds to replacing the partition used to build the abstraction with a finer partition. We propose two methods, called strategy-based and value-based refinement. The first examines the difference between strategy pairs that yield the lower and upper bounds. The motivation for this is that, since the bounds are different and the actual Fig. <ref type="figure">3</ref> A quantitative abstraction-refinement framework for MDPs value for the concrete model falls between the bounds, one (or both) of the strategy pairs must make choices that are not valid in the concrete model (this can be seen as analogous to non-probabilistic abstraction refinement, which is based on a counterexample trace). The second method differs by considering all strategy pairs yielding the bounds.</p><p>For both methods, we demonstrate that the refinement results in a strictly finer partition. For finite-state models, this guarantees that, for any user-specified error bound ε, the abstraction-refinement loop eventually terminates. In addition, if a partition is coarser than probabilistic bisimulation <ref type="bibr" target="#b29">[31]</ref>, i.e. bisimilar states are in the same element of the partition, then so is the partition after refinement. This follows from the fact that, under such a partition, bisimilar states are represented by the same player 2 vertex and, as will be seen, neither refinement method separates such states. Consequently, if the initial partition is coarser than probabilistic bisimulation, then any subsequent refinement will also be coarser. This observation guarantees that the refinement process also terminates when it is applied to infinitestate MDPs that have a finite bisimulation quotient.</p><p>To simplify presentation, we describe the refinement step when computing minimum reachability probabilities, i.e. we assume that we have MDP M = (S, s init , Steps, r), partition P, target set F ∈ P and corresponding game</p><formula xml:id="formula_73">G P M = ((V , E), v init , (V 1 , V 2 , V p ), δ, r) in which p lb,min v (F ) &lt; p ub,min v (F ) for some v ∈ V 1 .</formula><p>The cases for maximum probabilistic and expected reachability follow in identical fashion.</p><p>Strategy-based refinement As mentioned in Sect. 2, when computing p lb,min v (F ) and p ub,min v (F ), a pair of strategies that obtain these values is also generated. Since a player 1 strategy's choice can be considered as a set of concrete states (the states whose choices are abstracted to the chosen player 2 vertex), the player 1 strategy from each pair provides a method for refinement. More precisely, we refine P as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Find memoryless strategy pairs (σ lb</head><p>1 , σ lb 2 ), (σ ub 1 , σ ub 2 ) such that for any v ∈ V :</p><formula xml:id="formula_74">p σ lb 1 ,σ lb 2 v (F ) = p lb,min v (F ) and p σ ub 1 ,σ ub 2 v (F ) = p ub,min v (F ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">For each player 1 vertex</head><formula xml:id="formula_75">v ∈ V 1 such that σ lb 1 (v) = σ ub 1 (v) replace v in the partition P with v σ 1 lb , v σ 1 ub and v \ (v σ 1 lb ∪ v σ 1 ub ) where: v σ 1 lb = s ∈ v | σ lb 1 (v) = Steps(s) and v σ 1 ub = s ∈ v | σ ub 1 (v) = Steps(s) .</formula><p>The following states that there always exists a vertex v such that σ lb 1 (v) = σ ub 1 (v), and hence applying this refinement will always (strictly) refine the partition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 5 If there exists a vertex</head><formula xml:id="formula_76">v ∈ V 1 such that p lb,min v (F ) &lt; p ub,min v (F ) and (σ lb 1 , σ lb 2 ) and (σ ub 1 , σ ub 2 )</formula><p>are corresponding memoryless optimal strategy pairs, then there also exists</p><formula xml:id="formula_77">a vertex v ∈ V 1 such that σ lb 1 (v) = σ ub 1 (v).</formula><p>Value-based refinement The second refinement method is again based on the choices made by optimal strategy pairs. However, rather than looking at a single strategy pair for each of the bounds, we use all strategies that achieve one of the bounds when refining the partition. More precisely, letting v s 2 = Steps(s), we refine each element v ∈ P as follows.</p><p>1. Construct the following subsets of v:</p><formula xml:id="formula_78">v min lb = {s ∈ v | p lb,min v s 2 (F ) = p lb,min v (F )} v min ub = {s ∈ v | p ub,min v 2 (s) (F ) = p ub,min v (F )}.</formula><p>2. Replace v with v min lb \v min ub , v min ub \v min lb , v min lb ∩ v min ub and v\(v min lb ∪ v min ub ). The following states that this refinement method will always lead to a (strictly) finer partition. Recall that the family of pure memoryless strategies suffices for optimality with respect to probabilistic and expected reachability in stochastic games.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 6 If there exists</head><formula xml:id="formula_79">v ∈ V 1 such that p lb,min v (F ) &lt; p ub,min v (F ), then there exists v ∈ V 1 such that v min lb = v min ub .</formula><p>Proof The proof is by contradiction. Therefore, suppose that there </p><formula xml:id="formula_80">exists v ∈ V 1 such that p lb,min v (F ) &lt; p ub,min v (F ) and v min lb = v min ub for all v ∈ V 1 .</formula><formula xml:id="formula_81">p σ lb 1 ,σ lb 2 v (F ) = p lb,min v (F ) and p σ ub 1 ,σ ub 2 v (F ) = p ub,min v (F ) for all v ∈ V</formula><p>whose existence follows from <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b38">40]</ref>. Now, for any player 1 vertex v ∈ V 1 , by definition:</p><formula xml:id="formula_82">v min lb = {s ∈ v | p lb,min v 2 (s) (F ) = p lb,min v (F )} v min ub = {s ∈ v | p ub,min v 2 (s) (F ) = p ub,min v (F )}</formula><p>where v 2 (s) = Steps(s). Now, consider any v ∈ V 1 , by construction we have σ ub</p><formula xml:id="formula_83">1 (v) = v s ub 2</formula><p>for some s ub ∈ v min ub , and since by the hypothesis v min lb = v min ub , we also have that s ub ∈ v min lb . Therefore, since σ lb 2 is optimal and v ∈ V 1 was arbitrary, it follows that:</p><formula xml:id="formula_84">p lb,min v (F ) = p σ ub 1 ,σ lb 2 v (F ) for all v ∈ V 1 .</formula><p>In particular, since v is a player 1 vertex, we have: </p><formula xml:id="formula_85">p lb,min v (F ) = p σ ub 1 ,σ lb 2 v (F ) ≥ inf σ 2 p σ ub 1 ,σ 2 v (F ) since σ lb 2 is a player 2 strategy = sup σ 1 inf σ 2 p σ 1 ,σ 2 v (F ) since σ ub 1 is an optimal strategy = p ub,min</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example 4</head><p>To illustrate the refinement strategies, we consider the simple MDP given in Fig. <ref type="figure" target="#fig_3">4</ref>(a). We assume that a reward of 1 is associated with each transition and compute the maximum expected reward of reaching the target set {s 4 }. We start by partitioning the state space into the target set ({s 4 }) and the remaining states, resulting in the stochastic two-player Calculating the lower and upper bounds on the maximum expected reward to reach {s 4 }, using the game in Fig. <ref type="figure" target="#fig_3">4(b)</ref>, yields the interval [2, ∞) for the vertex containing states s 0 -s 3 (and [0, 0] for the one containing s 4 ). There is only one memoryless strategy resulting in each bound: the one that selects {s 3 } (for the lower bound) and the one that selects {s 0 , s 1 , s 2 } (for the upper bound). Consequently, both the strategy-based and value-based refinement techniques refine the partition from {s 0 , s 1 , s 2 , s 3 }, {s 4 } to {s 0 , s 1 , s 2 }, {s 3 }, {s 4 }. The new abstraction is shown in Fig. <ref type="figure" target="#fig_3">4(c</ref>). Note that the player 1 vertex containing {s 0 , s 1 , s 2 } now leads to three distinct player 2 vertices, since the (lifted) distributions for MDP states s 0 , s 1 and s 2 are no longer equivalent under the new partition. Calculating bounds with the new game now gives [4, ∞) and [3, ∞) for the vertices {s 0 , s 1 , s 2 } and {s 3 }. From the vertex {s 0 , s 1 , s 2 }, the optimal strategy for the lower bound chooses {s 2 } and for the upper bound can choose either {s 0 } or {s 1 }. With strategy-based refinement, using either strategy for the upper bound gives the partition {s 0 }, {s 1 }, {s 2 }, {s 3 }, {s 4 }. For value-based refinement, s 0 and s 1 are kept together, giving rise instead to the partition {s 0 , s 1 }, {s 2 }, {s 3 }, {s 4 }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The abstraction-refinement algorithm</head><p>With the refinement strategies of the previous section, we can implement the abstractionrefinement loop shown in Fig. <ref type="figure">3</ref>. Starting with a coarse partition, we iteratively: construct an abstraction (according to Definition 10); compute lower and upper bounds (and corresponding strategies); and refine using either the strategy-based or value-based method of Sect. 4.1.</p><p>In this section, we present an optimised algorithm for the refinement loop, illustrated in Fig. <ref type="figure" target="#fig_4">5(a)</ref>. At the ith refinement step, we construct the abstraction G P i M of the MDP using partition P i . Using G P i M , we then compute vectors of lower and upper bounds x lb,(i) and x ub,(i) (indexed over P i , i.e. over player 1 vertices of G P i M ) for the minimum probability of reaching F in the MDP, using value iteration. If the bounds are sufficiently close (within ε), the loop terminates; otherwise, we refine the partition. The algorithm includes two optimisations, designed to improve the convergence rate of numerical solution:</p><p>-wherever possible, we re-use existing numerical results: when calculating lower bounds (line 5), we use the same vector from the previous step as the initial solution; for upper bounds (line 6), we re-use the lower bound from the current step; both are guaranteed lower bounds on the solution. -we store (in set V done ) vertices for which lower and upper bounds coincide (and we thus have the exact solution for their concrete states); again, we use these values in initial solutions for value iteration. Numerical solution of the game at each step is performed with an improved version of the standard value iteration algorithm <ref type="bibr" target="#b9">[10]</ref>, illustrated in Fig. <ref type="figure" target="#fig_4">5</ref>(b), which includes several optimisations. Firstly, we employ the qualitative algorithms of <ref type="bibr" target="#b14">[16]</ref> to efficiently find vertices for which the solution is exactly 0 or 1 (lines 2 and 4). Secondly, we only compute new values for vertices where the answer is unknown, i.e. those not in the set V done , mentioned above, and not identified by the qualitative algorithms (lines 9 and 14). Thirdly, the value iteration algorithm is given an initial vector x init for the fixpoint computation (normally this is a vector of zeros). This is used, as described above, to improve convergence. The correctness of the value iteration scheme is given in Sect. 4.3 below. As in the previous section, the code presented in Fig. <ref type="figure" target="#fig_4">5</ref> is for computation of minimum probabilities. The case for maximum probabilities and for expected rewards require only trivial changes to the value iteration algorithm. The REFINE function referenced in line 16 of Fig. <ref type="figure" target="#fig_4">5</ref>(a) can be either the strategy-based or value-based refinement from Sect. 4.1. For the latter, we require that lines 11 and 16 of the VALITER algorithm in Fig. <ref type="figure" target="#fig_4">5</ref>(b) return all (rather than just one) player 1 strategy choice which achieves the lower/upper bound.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Correctness of the abstraction-refinement algorithm</head><p>We now demonstrate that the value iteration scheme of Fig. <ref type="figure" target="#fig_4">5</ref> converges. For simplicity, we only consider computing bounds on minimum reachability probabilities and assume a fixed MDP M = (S, s init , Steps, r) and target set F . The cases for maximum probabilistic and expected reachability follow in identical fashion.</p><p>We begin with a number of preliminary concepts. For a set of vertices V , let be the partial order over (</p><formula xml:id="formula_86">V →[0, 1]) × (V →[0, 1]) where x x if and only if x v ≤ x v for all v ∈ V . For any stochastic game G = ((V , E), v init , (V 1 , V 2 , V p ), δ, r), subset of player 1 vertices V done and target set F , let F lb V done : (V 1 →[0, 1]) → (V 1 →[0,<label>1</label></formula><p>]) be the function:</p><formula xml:id="formula_87">F lb V done (x)(v) def = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ 1 i f v ∈ V lb 1 0 i f v ∈ V lb 0 inf σ 1 inf σ 2 p σ 1 ,σ 2 v (F ) if v ∈ V done \ (V lb 1 ∪ V lb 0 ) min v 2 ←v min vp←v 2 v∈V δ(v p )(v) • x v otherwise</formula><p>where the notation v ← v denotes the set {v ∈ V | (v, v ) ∈ E} and:</p><formula xml:id="formula_88">V lb 1 = {v ∈ V 1 | inf σ 1 inf σ 2 p σ 1 ,σ 2 v (F ) = 1} V lb 0 = {v ∈ V 1 | inf σ 1 inf σ 2 p σ 1 ,σ 2 v (F ) = 0}. Furthermore, let F ub V done : (V 1 →[0, 1]) → (V 1 →[0,<label>1</label></formula><p>]) be the function:</p><formula xml:id="formula_89">F ub V done (x)(v) def = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ 1 i f v ∈ V ub 1 0 i f v ∈ V ub 0 sup σ 1 inf σ 2 p σ 1 ,σ 2 v (F ) if v ∈ V done \ (V ub 1 ∪ V ub 0 ) max v 2 ←v min vp←v 2 v∈V δ(v p )(v) • x v otherwise</formula><p>where:</p><formula xml:id="formula_90">V ub 1 = {v ∈ V 1 | sup σ 1 inf σ 2 p σ 1 ,σ 2 v (F ) = 1} V ub 0 = {v ∈ V 1 | sup σ 1 inf σ 2 p σ 1 ,σ 2 v (F ) = 0}.</formula><p>The following results are adapted from <ref type="bibr" target="#b8">[9]</ref> to the notation used in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 7</head><p>For any stochastic game G, target set F and subset of player 1 vertices V done , the function F lb V done is monotonic with respect to .</p><p>Theorem 4 For any stochastic game G, target set F and subset of player 1 vertices V done , the function F lb V done has a unique fixed point u lb and u lb v is the optimal value for the probability of reaching F when player 1 and player 2 cooperate in order to minimise it, that is</p><formula xml:id="formula_91">u lb v = inf σ 1 inf σ 2 p σ 1 ,σ 2 v (F ).</formula><p>Lemma 8 For any stochastic game G, target set F and subset of player 1 vertices V done , the function F ub V done is monotonic with respect to .</p><p>Theorem 5 For any stochastic game G, target set F and subset of player 1 vertices V done , the function F ub V done has a unique fixed point u ub and u ub v is the optimal value for the probability of reaching F for player 1, that is</p><formula xml:id="formula_92">u ub v = sup σ 1 inf σ 2 p σ 1 ,σ 2 v (F ).</formula><p>Using these results and the Knaster-Tarski theorem, to prove the convergence of the value iteration scheme of Fig. <ref type="figure" target="#fig_4">5</ref>, supposing x lb,init and x ub,init are the initial values for computing u lb and u ub respectively, it is sufficient to show that:</p><p>x ,init u and x ,init F V done (x ,init ) for ∈ {lb, ub}.</p><p>(</p><p>Therefore, suppose that P and P old are the partitions from the current and previous refinement steps, G P M and G P old M are the corresponding abstractions and V done is set of vertices of</p><formula xml:id="formula_94">G P old M</formula><p>for which the lower and upper bounds are equal. Note that, by construction we have that P P old . We now prove that (3) holds by considering the cases where = lb and = ub in turn.</p><p>-If = lb then by construction, the initial vector x lb,init is given by:</p><formula xml:id="formula_95">x lb,init v = ⎧ ⎨ ⎩ 1 i fv ∈ V lb 1 0 i fv ∈ V lb 0 p lb,min v old (F ) otherwise</formula><p>where v old is the unique element of P old such that v ⊆ v old . Now, for any v ∈ V 1 , by Theorem 2 we have that:</p><formula xml:id="formula_96">p lb,min v old (F ) ≤ p lb,min v (F )(= u lb v )<label>(4)</label></formula><p>and hence x lb,init u lb .</p><p>Therefore, it remains to show that x lb,init F lb V done (x lb,init ). Now consider any v ∈ V 1 . First, by construction we have:</p><formula xml:id="formula_97">p lb,min v old (F ) ≤ x lb,init v . (<label>5</label></formula><formula xml:id="formula_98">) Next, if v ∈ V lb 1 ∪ V lb 0 ∪ V done ,</formula><p>then by definition of F lb V done we have F lb V done (x lb,init )(v) = p lb,min v (F ), and hence using (4) we have Since these are all the possible cases to consider, we have x lb,init F lb V done (x lb,init ) as required.</p><formula xml:id="formula_99">F lb V done (x lb,init )(v) ≥ x lb,init v . On the other hand, if v ∈ V lb 1 ∪ V lb 0 ∪ V done , then by definition of F lb V done : F lb V done (x lb,init )(v) = min v 2 ←v min vp←v 2 v 1 ∈V 1 δ(v p )(v 1 ) • x lb,init v 1 ≥ min v 2 ←v min vp←v 2 v 1 ∈V 1 δ(v p )(v 1 ) • p lb,min v old 1 (F ) by (5) = min v 2 ←v min vp←v 2 v old 1 ∈V old 1 v 1 ⊆v old 1 δ(v p )(v 1 ) • p lb,</formula><p>-If = ub, then by construction:</p><formula xml:id="formula_100">x ub,init v = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ 1 i fv ∈ V ub 1 0 i fv ∈ V ub 0 p ub,min v (F ) if v ∈ V done \ (V ub 1 ∪ V ub 0 ) p lb,min v (F ) otherwise.</formula><p>Now for any v ∈ V 1 by definition of p lb,min v (F ) and p ub,min v (F ) and construction respectively, we have:</p><formula xml:id="formula_101">p lb,min v (F ) ≤ p ub,min v (F )(= u ub v )<label>(6)</label></formula><p>and hence x ub,init u ub .</p><p>Therefore, it remains to show that x ub,init F ub V done (x ub,init ). Now consider any v ∈ V 1 . First by construction, we have</p><formula xml:id="formula_102">p lb,min v (F ) ≤ x lb,init v . (<label>7</label></formula><formula xml:id="formula_103">) Next, if v ∈ V lb 1 ∪ V lb 1 ∪ V done then by construction F ub V done (x ub,init )(v) = p ub,min</formula><p>v (F ), and hence using <ref type="bibr" target="#b5">(6)</ref> we have</p><formula xml:id="formula_104">F ub V done (x ub,init )(v) ≥ x ub,init v . On the other hand, if v ∈ V lb 1 ∪ V lb 1 ∪ V done then by definition of F ub V done : F ub V done (x ub,init )(v) = max v 2 ←v min vp←v 2 v 1 ∈V 1 δ(v p )(v 1 ) • x ub,init v 1 ≥ max v 2 ←v min vp←v 2 v 1 ∈V 1 δ(v p )(v 1 ) • p lb,min v 1</formula><p>(F ) by ( <ref type="formula" target="#formula_102">7</ref>) Since these are all the cases to consider, we have x ub,init F ub V done (x ub,init ) as required. This completes the proof of (3), and hence that the value iteration scheme of Fig. <ref type="figure" target="#fig_4">5</ref> converges.</p><formula xml:id="formula_105">≥ min v 2 ←v min vp←v 2 v 1 ∈V 1 δ(v p )(v 1 ) • p lb,min</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental results</head><p>We have implemented our abstraction-refinement framework using a prototype Java implementation. This section presents experimental results for the performance of our techniques on case studies from the repository of the probabilistic model checker PRISM <ref type="bibr" target="#b35">[37]</ref>. Below are listed the case studies and (in parentheses) the reachability properties used. <ref type="foot" target="#foot_0">1</ref>-the Zeroconf network configuration protocol for N configured hosts and M IP addresses ("the minimum probability that the host configures correctly"); -the IEEE 802.11 WLAN and IEEE 802.3 CSMA/CD protocols using a backoff counter maximum of bc and a maximum packet send time of 500 µs ("the minimum probability that a station's backoff counter reaches bc"). -the FireWire root contention protocol for a network transmission delay of d ns ("the maximum expected time to elect a leader"); -the randomised consensus shared coin protocol of Aspnes and Herlihy for N processes and parameter K ("the maximum expected time until termination").</p><p>Several of the models (FireWire, WLAN and CSMA/CD) were modelled using probabilistic timed automata and translated into MDPs using the digital clocks semantics of <ref type="bibr" target="#b27">[29]</ref>. For all experiments, the initial partition P contained: (1) the initial state(s); (2) the target states; and</p><p>(3) all remaining states. The abstraction was refined until the maximum relative difference between the bounds for the initial state(s) was below ε = 10 -4 , i.e. when max</p><formula xml:id="formula_106">v∈V init x ub v -x lb v x ub v &lt; 10 -4</formula><p>where V init is the set of abstract states that contain at least one concrete initial state. The value iteration algorithm used the default convergence criteria of PRISM (maximum relative difference less than 10 -6 ). Table <ref type="table" target="#tab_3">2</ref> presents statistics for the performance of our implementation. For each example, we give the size of the original MDP (i.e. number of concrete states) and, for each of our two refinement techniques, the size of the final abstraction (i.e. number of abstract states/player 1 vertices), the number of refinement steps and the total time required for the entire sequence of refinements and value iteration computations.</p><p>The abstractions The first, and most important, conclusion that we draw from these results is the quality of the automatically generated abstractions. For results of a relatively high precision (ε = 10 -4 , i.e. the bounds differ by less than 4 significant figures), the abstractions yielded state-space reductions of up to four orders of magnitude. We find it very promising that this holds for such a wide range of models.</p><p>It is also interesting to compare the abstractions we have obtained with manually developed abstractions derived for the same case studies in the literature. For the FireWire example, employing the manual abstraction process of <ref type="bibr" target="#b40">[42]</ref> (4 refinement steps and a number of complex proofs) and the digital clocks semantics of <ref type="bibr" target="#b27">[29]</ref> yields an abstract model with 1,212 states for d = 30. For the Zeroconf example, the manual abstraction of <ref type="bibr" target="#b26">[28]</ref> has 737 (N = 4, M = 64) and 881 (N = 8, M = 64) states. For both models we achieve smaller abstractions, in a fully automatic fashion, than those obtained manually. Furthermore, there is scope to combine the approaches, e.g. start with a human-derived abstraction and then refine mechanically. The refinement techniques Value-based refinement mostly outperforms the strategy-based variant, both in terms of the number of refinement steps and the size of the final abstraction. The faster convergence for the value-based approach is likely to be caused by the splitting of partition elements into four, rather than three, at each step. The fact that this is not at the expense of generating larger abstract models suggests that value-based refinement, as intended, avoids splitting states which should stay in the same partition element. However, on some case studies-FireWire for example-strategy-based refinement requires much fewer steps and is thus quicker. Despite this, the final abstractions produced are slightly larger. Similar results are observed for the consensus case study but, for the largest models, the increased size of the abstraction makes the process slower overall.</p><p>Performance The motivations for this work are to establish the viability of the stochastic two-player games as an abstract model for MDPs and to explore the benefits of our abstraction-refinement framework. For convenience, the implementation used in this paper performs model-level abstraction using a prototype abstraction tool which first builds the full concrete MDP (using PRISM) and then reduces it to a game based on a partition of the state space. As a result, this is currently the most time-intensive part of the process. This is illustrated by Table <ref type="table" target="#tab_4">3</ref> which shows, for the largest models in each case study, a breakdown of the timing for each part of the best-performing refinement technique: abstraction construction ("Build"), refinement ("Refine") and numerical solution using value iteration ("Solve"). Despite this, the overall performance of the framework is very encouraging. Table <ref type="table" target="#tab_4">3</ref> also gives the time-savings obtained by using the optimisations from Sect. 4.2 which reuse earlier numerical results to improve convergence. Comparing the time spent on numerical solution (column "Solve") with the time savings achieved (column "Optimisation Saving"), we see that the optimisations result in substantial improvements in performance.</p><p>Lastly, Table <ref type="table" target="#tab_4">3</ref> shows the time required to verify each model in PRISM (using the fastest available engine in the tool). We see that, perhaps unsurprisingly, verification with PRISM is currently faster than using the abstraction-refinement approach. We emphasise that the main goals of this are work are not to improve solution time for instances where it is feasible Fig. <ref type="figure">6</ref> Illustrations of convergence for the CSMA and Consensus case studies to verify the full concrete model, but rather to develop techniques that will scale to larger models. This will be achieved by building abstractions directly from high-level languages, e.g. using predicate abstraction, and bypassing the construction of the concrete model. Even without this, in some cases (e.g. Zeroconf), the time for abstraction refinement is much faster than PRISM, when the time for explicit construction of the abstraction is disregarded.</p><p>Convergence Finally, we look at the convergence of the refinement process, i.e., the changes in difference between the bounds at each refinement step. Previously, we argued that the difference between the bounds provided a useful quantitative measure of the quality of the abstraction. To illustrate this fact, Fig. <ref type="figure">6</ref> presents, for two of the CSMA/CD models, the lower and upper bounds on the corresponding probabilistic reachability property after each refinement step when using the value-based refinement method and, for two of the consensus models, the lower bounds for the expected reachability property after each refinement step for both refinement methods. Although there are several sudden drops (or increases) in these bounds, the overall trends show a gradual convergence in the quantitative results. One reason for the sudden jumps is the fact that the values shown in the graphs in Fig. <ref type="figure">6</ref> are for the initial state of each model, whereas the refinement scheme attempts to improve lower and upper the bounds for all states. This means that not every refinement will improve the bounds for the initial state.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related work</head><p>Below, we compare our approach with the closest work in the field, namely implementations of abstraction and refinement methods for MDPs. General issues relating to abstraction in the field of probabilistic model checking are discussed in <ref type="bibr" target="#b22">[24,</ref><ref type="bibr" target="#b32">34]</ref>. D'Argenio et al. <ref type="bibr" target="#b10">[11]</ref> introduce an approach for verifying reachability properties of MDPs based on probabilistic simulation <ref type="bibr" target="#b36">[38]</ref> and a corresponding tool RAPTURE. Properties are analysed on abstractions obtained through successive refinements, starting from a coarse partition derived from the property under study. Since the abstractions used are themselves MDPs, this approach only produces a lower (upper) bound for the minimum (maximum) reachability probability, and hence appears more suited to analysing Markov chains (which contain no nondeterminism and thus the minimum and maximum probabilities coincide). In <ref type="bibr" target="#b10">[11]</ref>, refinement is repeated until the probability (not the error bound) obtained falls below a given threshold.</p><p>Based on <ref type="bibr" target="#b10">[11]</ref> and using predicate abstraction <ref type="bibr" target="#b17">[19]</ref>, Hermanns et al. <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b42">44]</ref> propose a CEGAR framework for MDPs described in the PRISM language and implement it in a tool called PASS <ref type="bibr" target="#b33">[35]</ref>. The CEGAR framework is used for verifying or refuting properties of the form "the maximum probability of error is at most p" for a given probability threshold p. Since abstractions produce only upper bounds on maximum probabilities, to refute a property, probabilistic counterexamples <ref type="bibr" target="#b18">[20]</ref> (comprising multiple paths whose combined probability exceeds p) are generated. If these paths are spurious, then they are used to generate further predicates using interpolation.</p><p>Perhaps the most important distinguishing feature of our work is the use of two-player games as abstractions. These provide lower and upper bounds, avoiding enumeration of a set of paths for a counterexample (which may be large or even infinite, resulting in nontermination). In addition, the bounds enable a quantitative approach: we target properties without thresholds such as "what is the maximum probability of failure?". Recent extensions to the PASS tool <ref type="bibr" target="#b41">[43]</ref> use the game-based abstraction approach introduced in this paper and demonstrate that this is faster and yields smaller abstractions. Furthermore, the abstraction-refinement framework presented in this paper has been successfully applied in several other contexts. In <ref type="bibr" target="#b23">[25]</ref>, predicate abstraction is used to construct stochastic game abstractions of PRISM models. In <ref type="bibr" target="#b24">[26]</ref> and <ref type="bibr" target="#b28">[30]</ref>, game-based abstraction and refinement are used to develop verification techniques for probabilistic software and probabilistic timed automata, respectively. In the former, the techniques are used to verify ANSI-C code for network utilities whose complexity is significantly beyond the scope of existing probabilistic verification tools. In the latter, methods to analyse infinite-state real-time systems are presented that outperform all existing approaches.</p><p>In [14], de Alfaro et al. propose a technique called 'magnifying-lens abstraction' (MLA) which computes lower and upper bounds on reachability probabilities of MDPs. The approach is based on, first, partitioning the state space into regions and then analysing ('magnifying') the states of each region separately by applying value iteration locally. Regions are refined adaptively until the difference between the bounds for all regions is within some specified accuracy. MLA is suited to models where there is some notion of 'distance' between states (states close together have similar reachability probabilities) which is used during refinement. Since probabilities are computed for each state of each region and then a minimum/maximum is stored for each region, analysing either small numbers of large regions or large number of small regions is expensive in time and memory. Furthermore, the greatest possible reduction in state-space N is O( √ N). Comparing results in <ref type="bibr">[14]</ref> for Zeroconf (N = 4, M = 32), which has 26,121 concrete states, using the same accuracy of ε = 10 -3 , the MLA approach gives 131 regions and a maximum region size of 1,005, whereas our value-based refinement yields 699 abstract states. Note that, as Fig. <ref type="figure">2</ref> demonstrates, our abstract model is smaller for larger values of M while, for MLA, increasing M will increase either the number of regions or maximum region size (or both). Similarly, our framework is applicable to infinite-state systems, whereas [14] is not.</p><p>An alternative approach to the abstraction of MDPs is <ref type="bibr" target="#b3">[4]</ref>, which proposes a CEGAR framework for verifying a fragment of the logic PCTL using MDPs as abstract models (as in <ref type="bibr" target="#b10">[11]</ref>). The focus of the paper is on suitable notions of counterexamples for this process, observing that existing proposals such as those used in <ref type="bibr" target="#b19">[21]</ref> are not sufficiently expressive for general PCTL formulae. Instead, the paper proposes that counterexamples take the form of "small" MDPs that simulate the concrete model. Algorithms to implement the proposed CEGAR framework are given, but no implementation is attempted.</p><p>Elsewhere, abstraction techniques have been considered for discrete-time Markov chains <ref type="bibr" target="#b16">[18,</ref><ref type="bibr" target="#b37">39]</ref>, which can be seen as MDPs without nondeterminism. These techniques abstract probabilities to intervals; however, practical applicability has yet to be demonstrated. Also relevant is the work of Chatterjee et al. <ref type="bibr" target="#b5">[6]</ref> who propose a counterexample-based abstractionrefinement technique for planning problems on stochastic games. Again, an implementation to test this on practical examples has not yet been developed.</p><p>Another important direction for abstraction and refinement of probabilistic systems is the extension of abstract interpretation to the probabilistic setting <ref type="bibr" target="#b31">[33,</ref><ref type="bibr" target="#b34">36,</ref><ref type="bibr" target="#b39">41]</ref>, although these approaches have yet to be combined with refinement. For details on the relationship between the game-based abstraction approach and abstract interpretation, see <ref type="bibr" target="#b41">[43]</ref>.</p><p>In <ref type="bibr" target="#b15">[17]</ref> a method for approximating continuous-state (and hence infinite-state) Markov processes by a family of finite-state Markov chains is presented. It is shown that, for simple quantitative modal logic, if the continuous Markov process satisfies a formula, then one of the approximations also satisfies the formula. Finally, McIver and Morgan have developed a framework for the refinement and abstraction of probabilistic programs using expectation transformers <ref type="bibr" target="#b30">[32]</ref>. The proof techniques developed in this work have been implemented in the HOL theorem-proving environment <ref type="bibr" target="#b21">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>We have presented a novel abstraction-refinement framework for Markov decision processes. Our abstraction technique is based on the use of stochastic two-player games, in which one player corresponds to nondeterministic choices from the MDP and the other corresponds to the nondeterminism introduced through abstraction. Using existing results and algorithms from the stochastic games literature, we have shown how the abstraction can be used to compute both lower and upper bounds on the minimum and maximum probability or expected reward of reaching a set of states in the original MDP.</p><p>In addition to providing quantitative results for properties of the MDP, the stochastic game yields a measure of the utility of the abstraction, thus giving a basis for refining it to improve its accuracy. Using this abstraction approach, we have introduced two refinement techniques and an optimised algorithm for a quantitative abstraction-refinement loop. Our experimental results illustrate how this framework provides efficient and fully automatic generation of precise yet compact abstractions for large MDPs from a wide selection of complex case studies. This demonstrates that stochastic two-player games are indeed a good choice of model for representing abstractions of MDPs.</p><p>There are many possibilities for future work on this topic. An important direction is to investigate the application of the framework at the level of MDP modelling formalisms. In fact, these techniques have subsequently been employed to develop probabilistic verification methods for imperative languages such as C <ref type="bibr" target="#b24">[26]</ref>, using predicate abstraction, and for probabilistic timed automata <ref type="bibr" target="#b28">[30]</ref>, using convex polyhedra <ref type="bibr" target="#b28">[30]</ref>. In each case, a key challenge is developing efficient techniques and heuristics to guide refinement; this represents an important area of future work. We also plan to extend our abstraction and refinement methods to other types of models, for example, continuous-time models and hybrid automata.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Example 1</head><label>1</label><figDesc>and v = (r, μ) for some μ ∈ Dist(P ) 0 otherwise. We illustrate the abstraction process on a simple example, shown in Fig.1. The concrete model (the MDP) is given in Fig.1(a): states are drawn as circles and probability distributions as labelled arrows, grouped with arcs. For clarity, we omit rewards in this case. We use the partition P = {{s 0 }, {s 1 , s 2 , s 3 }, {s 4 , s 5 }, {s 6 }}. The abstraction process is illustrated in two steps. Firstly, Fig.1(b) shows the partition P, as dotted lines around groups of states, and the result of lifting each distribution μ in the MDP to μ. Secondly, Fig.1(c)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1</head><label>1</label><figDesc>Fig. 1 Simple example of the abstraction process for Markov decision processes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4</head><label>4</label><figDesc>Fig. 4 Example of the abstraction-refinement process</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5</head><label>5</label><figDesc>Fig. 5 Abstraction-refinement loop algorithm (minimum reachability probabilities)</figDesc><graphic coords="23,48.92,318.91,341.52,276.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Zeroconf protocol (Example 3): model statistics and numerical results (expected completion time)</figDesc><table><row><cell cols="3">N M States (transitions)</cell><cell></cell><cell cols="3">Minimum expected time</cell><cell cols="2">Maximum expected time</cell></row><row><cell></cell><cell cols="2">Concrete model</cell><cell>Abstraction</cell><cell>lb</cell><cell>Exact</cell><cell>ub</cell><cell>lb</cell><cell>Exact</cell><cell>ub</cell></row><row><cell>4</cell><cell>32 26,121</cell><cell>(50,624)</cell><cell cols="6">737 (1,594) 8.1088 8.1572 8.2190 8.1231 8.2465 8.3047</cell></row><row><cell>5</cell><cell>58,497</cell><cell>(139,104)</cell><cell cols="6">785 (1,678) 8.1410 8.2035 8.2839 8.1595 8.3183 8.3950</cell></row><row><cell>6</cell><cell cols="2">145,801 (432,944)</cell><cell cols="6">833 (1,762) 8.1758 8.2533 8.3538 8.1988 8.3956 8.4922</cell></row><row><cell>7</cell><cell cols="2">220,513 (614,976)</cell><cell cols="6">857 (1,806) 8.2131 8.2939 8.4293 8.2412 8.4579 8.5972</cell></row><row><cell>8</cell><cell cols="8">432,185 (1,254,480) 881 (1,850) 8.2538 8.3379 8.5110 8.2871 8.5254 8.7109</cell></row><row><cell>4</cell><cell>64 50,377</cell><cell>(98,080)</cell><cell cols="6">737 (1,594) 8.0508 8.0733 8.1022 8.0574 8.1150 8.1422</cell></row><row><cell>5</cell><cell cols="2">113,217 (270,272)</cell><cell cols="6">785 (1,678) 8.0645 8.0931 8.1299 8.0730 8.1457 8.1808</cell></row><row><cell>6</cell><cell cols="2">282,185 (839,824)</cell><cell cols="6">833 (1,762) 8.0788 8.1136 8.1586 8.0891 8.1774 8.2206</cell></row><row><cell>7</cell><cell cols="8">426,529 (1,189,792) 857 (1,806) 8.0935 8.1289 8.1883 8.1058 8.2008 8.2619</cell></row><row><cell>8</cell><cell cols="8">838,905 (2,439,600) 881 (1,850) 8.1088 8.1448 8.2190 8.1231 8.2252 8.3047</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc>Performance statistics for the abstraction-refinement implementation</figDesc><table><row><cell>Case study (parameter)</cell><cell></cell><cell>States in</cell><cell cols="3">Strategy-based refinement</cell><cell cols="3">Value-based refinement</cell></row><row><cell></cell><cell></cell><cell>MDP</cell><cell cols="3">States Steps Time (s)</cell><cell cols="2">States Steps</cell><cell>Time (s)</cell></row><row><cell>IPv4 Zeroconf</cell><cell>4 32</cell><cell>26,121</cell><cell cols="2">1,041 206</cell><cell>51.66</cell><cell>699</cell><cell>112</cell><cell>28.27</cell></row><row><cell>protocol (N M)</cell><cell>4 64</cell><cell>50,377</cell><cell cols="2">1,041 189</cell><cell>76.30</cell><cell>676</cell><cell>112</cell><cell>49.86</cell></row><row><cell></cell><cell>4 128</cell><cell>98,889</cell><cell cols="2">917 179</cell><cell>127.3</cell><cell>680</cell><cell>112</cell><cell>95.80</cell></row><row><cell></cell><cell>8 32</cell><cell>552,097</cell><cell cols="2">2,277 208</cell><cell>1,184</cell><cell>883</cell><cell>128</cell><cell>791.7</cell></row><row><cell></cell><cell>8 64</cell><cell>1,065,569</cell><cell cols="2">2,277 188</cell><cell>2,412</cell><cell>877</cell><cell>128</cell><cell>1,584</cell></row><row><cell></cell><cell cols="2">8 128 2,092,513</cell><cell cols="2">1,720 217</cell><cell>4,293</cell><cell>855</cell><cell>128</cell><cell>3,300</cell></row><row><cell>IEEE 802.11</cell><cell>2</cell><cell>28,480</cell><cell>399</cell><cell>70</cell><cell>13.98</cell><cell>253</cell><cell>70</cell><cell>14.50</cell></row><row><cell>WLAN (bc)</cell><cell>3 4</cell><cell>96,302 345,000</cell><cell cols="2">1,141 118 2,619 214</cell><cell>84.77 571.9</cell><cell>704 1,584</cell><cell>118 214</cell><cell>83.00 557.6</cell></row><row><cell></cell><cell>5</cell><cell>1,295,218</cell><cell cols="2">5,569 406</cell><cell>4,512</cell><cell>3,605</cell><cell>406</cell><cell>4,625</cell></row><row><cell>IEEE 802.3</cell><cell>4</cell><cell>92,978</cell><cell>419</cell><cell>53</cell><cell>30.25</cell><cell>410</cell><cell>53</cell><cell>30.2</cell></row><row><cell>CSMA/CD (bc)</cell><cell>5 6</cell><cell>277,493 793,110</cell><cell cols="2">846 1,671 138 78</cell><cell>129.8 678.6</cell><cell>839 1,663</cell><cell>77 138</cell><cell>126.9 654.3</cell></row><row><cell></cell><cell>7</cell><cell>2,221,189</cell><cell cols="2">3,297 266</cell><cell>3,687</cell><cell>3,176</cell><cell>267</cell><cell>3,580</cell></row><row><cell>IEEE 1394</cell><cell>30</cell><cell>4,093</cell><cell cols="2">1,274 320</cell><cell>106.7</cell><cell>910</cell><cell>860</cell><cell>307.1</cell></row><row><cell>FireWire root contention (d)</cell><cell>60 120</cell><cell>8,618 22,852</cell><cell cols="2">2,141 244 3,957 183</cell><cell>181.9 276.7</cell><cell cols="2">1,495 2,746 1,404 994</cell><cell>588.0 1,576</cell></row><row><cell></cell><cell>240</cell><cell>84,152</cell><cell cols="2">7,956 203</cell><cell>833.3</cell><cell cols="2">5,572 2,379</cell><cell>7,681</cell></row><row><cell>Randomised</cell><cell>5 2</cell><cell>173,056</cell><cell>1,298</cell><cell>47</cell><cell>177.5</cell><cell>566</cell><cell>236</cell><cell>690.1</cell></row><row><cell>consensus protocol (N K)</cell><cell>5 4 5 8</cell><cell>327,936 637,696</cell><cell cols="2">2,529 5,008 155 76</cell><cell>801.7 5,548</cell><cell>1,111 2,074</cell><cell>346 557</cell><cell>1,940 7,368</cell></row><row><cell></cell><cell>5 16</cell><cell>1,257,216</cell><cell cols="2">9,987 282</cell><cell>39,803</cell><cell cols="3">4,144 1,027 34,133</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc>More detailed timing statistics for the abstraction-refinement implementation</figDesc><table><row><cell cols="5">Case study (parameter) Abstraction-refinement time (s)</cell><cell></cell><cell></cell><cell>PRISM</cell></row><row><cell></cell><cell></cell><cell>Total</cell><cell>Breakdown</cell><cell></cell><cell></cell><cell>Optimisation</cell><cell>time (s)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Build</cell><cell>Refine</cell><cell>Solve</cell><cell>saving</cell><cell></cell></row><row><cell>Zeroconf</cell><cell>8 64</cell><cell>1,584</cell><cell>1,574</cell><cell>2.91</cell><cell>6.41</cell><cell>18.83</cell><cell>149.5</cell></row><row><cell>(N M)</cell><cell>8 128</cell><cell>3,300</cell><cell>3,287</cell><cell>5.78</cell><cell>7.36</cell><cell>52.47</cell><cell>269.8</cell></row><row><cell>WLAN</cell><cell>4</cell><cell>557.6</cell><cell>527.8</cell><cell>0.33</cell><cell>29.02</cell><cell>153.2</cell><cell>19.34</cell></row><row><cell>(bc)</cell><cell>5</cell><cell>4,625</cell><cell>4,300</cell><cell>1.763</cell><cell>320.9</cell><cell>158.1</cell><cell>89.80</cell></row><row><cell>CSMA</cell><cell>6</cell><cell>654.3</cell><cell>616.3</cell><cell>3.63</cell><cell>33.91</cell><cell>61.79</cell><cell>41.39</cell></row><row><cell>(bc)</cell><cell>7</cell><cell>3,580</cell><cell>3,322</cell><cell>15.42</cell><cell>240.6</cell><cell>394.6</cell><cell>134.3</cell></row><row><cell>FireWire</cell><cell>120</cell><cell>276.7</cell><cell>32.35</cell><cell>0.10</cell><cell>244.0</cell><cell>267.0</cell><cell>8.71</cell></row><row><cell>(d)</cell><cell>240</cell><cell>833.3</cell><cell>151.4</cell><cell>0.23</cell><cell>680.9</cell><cell>841.0</cell><cell>25.37</cell></row><row><cell>Consensus</cell><cell>5 8</cell><cell>7,368</cell><cell>5,637</cell><cell>4.33</cell><cell>1,726</cell><cell>4,027</cell><cell>2,379</cell></row><row><cell>(N K)</cell><cell>5 16</cell><cell>34,133</cell><cell>21,837</cell><cell>13.28</cell><cell>12,278</cell><cell>41,878</cell><cell>14,956</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Supporting files for the models and properties used in the experimental results are available from: http:// www.prismmodelchecker.org/files/fmsd-games/.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements This work was supported in part by EPSRC grants EP/D07956X and EP/D076625. The authors would like to thank the anonymous referees for their helpful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Model checking for a probabilistic branching time logic with fairness</title>
		<author>
			<persName><forename type="first">C</forename><surname>Baier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kwiatkowska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Distrib Comput</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="125" to="155" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An analysis of stochastic shortest path problems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bertsekas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tsitsiklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math Oper Res</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="580" to="595" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Probability and measure</title>
		<author>
			<persName><forename type="first">P</forename><surname>Billingsley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979">1979</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A counterexample guided abstraction-refinement framework for Markov decision processes</title>
		<author>
			<persName><forename type="first">R</forename><surname>Chadha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Viswanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans Comput Logic</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Trading memory for randomness</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>De Alfaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Henzinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1st int. conf. quantitative evaluation of systems (QEST&apos;04)</title>
		<meeting>1st int. conf. quantitative evaluation of systems (QEST&apos;04)<address><addrLine>Los Alamitos</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Comput. Soc</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="206" to="217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Counterexample-guided planning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Henzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jhala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Majumdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 21st conference in uncertainty in artificial intelligence (UAI&apos;05)</title>
		<meeting>21st conference in uncertainty in artificial intelligence (UAI&apos;05)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="104" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dynamic configuration of IPv4 link-local addresses</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cheshire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Adoba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Guttman</surname></persName>
		</author>
		<ptr target="www.zeroconf.org" />
	</analytic>
	<monogr>
		<title level="m">Zeroconf Working Group of the Internet Engineering Task Force</title>
		<imprint>
			<date type="published" when="2002-08">2002. August 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Counterexample-guided abstraction refinement</title>
		<author>
			<persName><forename type="first">E</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Grumberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Veith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 12th int. conf. computer aided verification (CAV&apos;00)</title>
		<title level="s">Lecture notes in computer science</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Emerson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Sistla</surname></persName>
		</editor>
		<meeting>12th int. conf. computer aided verification (CAV&apos;00)<address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="154" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The complexity of stochastic games</title>
		<author>
			<persName><forename type="first">A</forename><surname>Condon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf Comput</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="224" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Advances in computational complexity theory</title>
		<author>
			<persName><forename type="first">A</forename><surname>Condon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DIMACS Ser Discrete Math Theor Comput Sci</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="51" to="73" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
	<note>On algorithms for simple stochastic games</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Proc. 1st joint int workshop process algebra and probabilistic methods, performance modelling and verification (PAPM/PROBMIV&apos;01)</title>
		<author>
			<persName><forename type="first">P</forename><surname>D'argenio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jeannet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Larsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture notes in computer science</title>
		<editor>
			<persName><forename type="first">L</forename><surname>De Alfaro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Gilmore</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page" from="39" to="56" />
			<date type="published" when="2001">2001</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
	<note>Reachability analysis of probabilistic systems by successive refinements</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Computing minimum and maximum reachability times in probabilistic systems</title>
		<author>
			<persName><forename type="first">L</forename><surname>De Alfaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 10th int. conf. concurrency theory (CONCUR&apos;99)</title>
		<title level="s">Lecture notes in computer science</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Baeten</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Mauw</surname></persName>
		</editor>
		<meeting>10th int. conf. concurrency theory (CONCUR&apos;99)<address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="66" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Magnifying-lens abstraction for Markov decision processes</title>
		<author>
			<persName><forename type="first">L ;</forename><surname>De Alfaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>De Alfaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 19th int. conf. computer aided verification (CAV&apos;07)</title>
		<title level="s">Lecture notes in computer science</title>
		<editor>
			<persName><forename type="first">W</forename><surname>Damm</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Hermanns</surname></persName>
		</editor>
		<meeting>19th int. conf. computer aided verification (CAV&apos;07)<address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1997">1997. 2007</date>
			<biblScope unit="volume">4590</biblScope>
			<biblScope unit="page" from="325" to="338" />
		</imprint>
		<respStmt>
			<orgName>Stanford University 14</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
	<note>Formal verification of probabilistic systems</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Concurrent reachability games</title>
		<author>
			<persName><forename type="first">L</forename><surname>De Alfaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Henzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kupferman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 39th symp. foundations of computer science (FOCS&apos;98)</title>
		<meeting>39th symp. foundations of computer science (FOCS&apos;98)<address><addrLine>Los Alamitos</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Comput. Soc</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="564" to="575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<author>
			<persName><forename type="first">L</forename><surname>De Alfaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Henzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kupferman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Concurrent reachability games</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">386</biblScope>
			<biblScope unit="page" from="188" to="217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Approximating labelled Markov processes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Desharnais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jagadeesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Panangaden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf Comput</title>
		<imprint>
			<biblScope unit="volume">184</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="160" to="200" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Don&apos;t know in probabilistic systems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fecher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Leucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 13th int. spin workshop on model checking of software (SPIN&apos;06)</title>
		<title level="s">Lecture notes in computer science</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Valmari</surname></persName>
		</editor>
		<meeting>13th int. spin workshop on model checking of software (SPIN&apos;06)<address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">3925</biblScope>
			<biblScope unit="page" from="71" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Construction of abstract state graphs with PVS</title>
		<author>
			<persName><forename type="first">S</forename><surname>Graf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Saidi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 9th int. conf. computer aided verification (CAV&apos;97)</title>
		<title level="s">Lecture notes in computer science</title>
		<editor>
			<persName><forename type="first">O</forename><surname>Grumberg</surname></persName>
		</editor>
		<meeting>9th int. conf. computer aided verification (CAV&apos;97)<address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="72" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Counterexample generation in probabilistic model checking</title>
		<author>
			<persName><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Katoen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Damman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Softw Eng</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="241" to="257" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Probabilistic CEGAR</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hermanns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wachter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 20th int. conf. computer aided verification (CAV&apos;08)</title>
		<title level="s">Lecture notes in computer science</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Malik</surname></persName>
		</editor>
		<meeting>20th int. conf. computer aided verification (CAV&apos;08)<address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">5123</biblScope>
			<biblScope unit="page" from="162" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">PRISM: A tool for automatic verification of probabilistic systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kwiatkowska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Norman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 12th int. conf. tools and algorithms for the construction and analysis of systems (TACAS&apos;06)</title>
		<title level="s">Lecture notes in computer science</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Hermanns</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Palsberg</surname></persName>
		</editor>
		<meeting>12th int. conf. tools and algorithms for the construction and analysis of systems (TACAS&apos;06)<address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">3920</biblScope>
			<biblScope unit="page" from="441" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Probabilistic guarded commands mechanized in HOL</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hurd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mciver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Morgan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theor Comput Sci</title>
		<imprint>
			<biblScope unit="volume">346</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="96" to="112" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An abstraction framework for mixed nondeterministic and probabilistic systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Huth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Validation of stochastic systems. Lecture notes in computer science</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Baier</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Haverkort</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Hermanns</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Katoen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Siegle</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="419" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Game-based probabilistic predicate abstraction in PRISM</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kattenbelt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kwiatkowska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Norman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 6th workshop quantitative aspects of programming languages</title>
		<meeting>6th workshop quantitative aspects of programming languages</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>QAPL&apos;08</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Abstraction refinement for probabilistic software</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kattenbelt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kwiatkowska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Norman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 10th int. conf. verification, model checking and abstract interpretation (VMCAI&apos;09)</title>
		<title level="s">Lecture notes in computer science</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Muller-Olm</surname></persName>
		</editor>
		<meeting>10th int. conf. verification, model checking and abstract interpretation (VMCAI&apos;09)<address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">5403</biblScope>
			<biblScope unit="page" from="182" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Denumerable Markov chains, 2nd edn</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kemeny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Knapp</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1976">1976</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Game-based abstraction for Markov decision processes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kwiatkowska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Norman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3th int. conf. quantitative evaluation of systems (QEST&apos;06)</title>
		<meeting>3th int. conf. quantitative evaluation of systems (QEST&apos;06)<address><addrLine>Los Alamitos</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Comput. Soc</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="157" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Performance analysis of probabilistic timed automata using digital clocks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kwiatkowska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Norman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sproston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Form Methods Syst Des</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="33" to="78" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Stochastic games for verification of probabilistic timed automata</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kwiatkowska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Norman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 7th international conference on formal modelling and analysis of timed systems (FORMATS&apos;09)</title>
		<title level="s">Lecture notes in computer science</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Ouaknine</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Vaandrager</surname></persName>
		</editor>
		<meeting>7th international conference on formal modelling and analysis of timed systems (FORMATS&apos;09)<address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">5813</biblScope>
			<biblScope unit="page" from="212" to="227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bisimulation through probabilistic testing</title>
		<author>
			<persName><forename type="first">K</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Skou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf Comput</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Abstraction, refinement and proof for probabilistic systems. Monographs in computer science</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mciver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Morgan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Abstract interpretation of programs as Markov decision processes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Monniaux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci Comput Program</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="179" to="205" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Analysing randomized distributed algorithms</title>
		<author>
			<persName><forename type="first">G</forename><surname>Norman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture notes in computer science</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Baier</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Haverkort</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Hermanns</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Katoen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Siegle</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page" from="384" to="418" />
			<date type="published" when="2004">2004</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
	<note>Validation of stochastic systems</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<ptr target="http://depend.cs.uni-sb.de/PASS/" />
		<title level="m">PASS tool homepage</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Program analysis and compilation, theory and practice, essays dedicated to Reinhard Wilhelm on the occasion of his 60th birthday</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Pierro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hankin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wiklicky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture notes in computer science</title>
		<editor>
			<persName><forename type="first">T</forename><surname>Reps</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sagiv</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Bauer</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">4444</biblScope>
			<biblScope unit="page" from="160" to="174" />
			<date type="published" when="2006">2006</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
	<note>Abstract interpretation for worst and average case analysis</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<ptr target="http://www.prismmodelchecker.org/" />
		<title level="m">PRISM web site</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Modelling and verification of randomized distributed real time systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Segala</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Model-checking Markov chains in the presence of uncertainties</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Viswanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Agha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 12th int. conf. tools and algorithms for the construction and analysis of systems (TACAS&apos;06)</title>
		<title level="s">Lecture notes in computer science</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Hermanns</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Palsberg</surname></persName>
		</editor>
		<meeting>12th int. conf. tools and algorithms for the construction and analysis of systems (TACAS&apos;06)<address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">3920</biblScope>
			<biblScope unit="page" from="394" to="410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Stochastic games</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shapley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. national academy of science</title>
		<meeting>national academy of science</meeting>
		<imprint>
			<date type="published" when="1953">1953</date>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1095" to="1100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Probabilistic abstract interpretation of imperative programs using truncated normal distributions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 6th workshop on quantitative aspects of programming languages (QAPL&apos;08)</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Aldini</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Baier</surname></persName>
		</editor>
		<meeting>6th workshop on quantitative aspects of programming languages (QAPL&apos;08)<address><addrLine>Dordrecht</addrLine></address></meeting>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">220</biblScope>
			<biblScope unit="page" from="43" to="59" />
		</imprint>
	</monogr>
	<note>Electronic notes in theoretical computer science</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Root contention in IEEE 1394</title>
		<author>
			<persName><forename type="first">M</forename><surname>Stoelinga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Vaandrager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th int. AMAST workshop real-time and probabilistic systems (ARTS&apos;99)</title>
		<title level="s">Lecture notes in computer science</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Katoen</surname></persName>
		</editor>
		<meeting>5th int. AMAST workshop real-time and probabilistic systems (ARTS&apos;99)<address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="53" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Best probabilistic transformers</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wachter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th int. conf. verification, model checking and abstract interpretation (VMCAI&apos;10)</title>
		<title level="s">Lecture notes in computer science</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Barthe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Hermenegildo</surname></persName>
		</editor>
		<meeting>11th int. conf. verification, model checking and abstract interpretation (VMCAI&apos;10)<address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">5944</biblScope>
			<biblScope unit="page" from="362" to="379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Probabilistic model checking modulo theories</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wachter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hermanns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th int. conf. quantitative evaluation of systems (QEST&apos;07)</title>
		<meeting>4th int. conf. quantitative evaluation of systems (QEST&apos;07)<address><addrLine>Los Alamitos</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Comput. Soc</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="129" to="138" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
