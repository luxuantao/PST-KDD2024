<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Knowle dge-Base d Systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Haiyun</forename><surname>Peng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yukun</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Erik</forename><surname>Cambria</surname></persName>
							<email>cambria@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Knowle dge-Base d Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1ADD988D1BF05031474A2D5FFEC49407</idno>
					<idno type="DOI">10.1016/j.knosys.2018.02.034</idno>
					<note type="submission">Received 14 November 2017 Revised 21 January 2018 Accepted 22 February 2018</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T04:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Aspect-based sentiment analysis Chinese NLP Multi-grained Chinese text representation</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Aspect-based sentiment analysis aims at identifying sentiment polarity towards aspect targets in a sentence. Previously, the task was modeled as a sentence-level sentiment classification problem that treated aspect targets as a hint. Such approaches oversimplify the problem by averaging word embeddings when the aspect target is a multi-word sequence. In this paper, we formalize the problem from a different perspective, i.e., that sentiment at aspect target level should be the main focus. Due to the fact that written Chinese is very rich and complex, Chinese aspect targets can be studied at three different levels of granularity: radical, character and word. Thus, we propose to explicitly model the aspect target and conduct sentiment classification directly at the aspect target level via three granularities. Moreover, we study two fusion methods for such granularities in the task of Chinese aspect-level sentiment analysis. Experimental results on a multi-word aspect target subset from SemEval2014 and four Chinese review datasets validate our claims and show the improved performance of our model over the state of the art.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years, sentiment analysis has become increasingly popular for processing social media data on online communities, blogs, wikis, microblogging platforms, and other online collaborative media <ref type="bibr">[1]</ref> . Sentiment analysis is a branch of affective computing research <ref type="bibr" target="#b2">[2]</ref> that aims to classify text -but sometimes also audio and video <ref type="bibr" target="#b3">[3]</ref> -into either positive or negative -but sometimes also neutral <ref type="bibr" target="#b4">[4]</ref> . Most of the literature is on English language but recently an increasing number of publications is tackling the multilinguality issue <ref type="bibr" target="#b5">[5]</ref> .</p><p>Sentiment analysis techniques can be broadly categorized into symbolic and sub-symbolic approaches: the former include the use of lexicons <ref type="bibr" target="#b6">[6]</ref> , ontologies <ref type="bibr" target="#b7">[7]</ref> , and semantic networks <ref type="bibr" target="#b8">[8]</ref> to encode the polarity associated with words and multi-word expressions; the latter consist of supervised <ref type="bibr" target="#b9">[9]</ref> , semi-supervised <ref type="bibr" target="#b10">[10]</ref> and unsupervised <ref type="bibr" target="#b11">[11]</ref> machine learning techniques that perform sentiment classification based on word co-occurrence frequencies. There are also a few hybrid approaches <ref type="bibr" target="#b12">[12]</ref> that leverage an ensemble of symbolic and sub-symbolic techniques for polarity detection.</p><p>Sentiment analysis has raised growing interest both within the scientific community, leading to many exciting open challenges, as well as in the business world, due to the remarkable benefits to be had from financial <ref type="bibr" target="#b13">[13]</ref> and political <ref type="bibr" target="#b14">[14]</ref> forecasting, e-health <ref type="bibr" target="#b15">[15]</ref> and e-tourism <ref type="bibr" target="#b16">[16]</ref> , user profiling <ref type="bibr" target="#b17">[17]</ref> and community detection <ref type="bibr" target="#b18">[18]</ref> , manufacturing and supply chain applications <ref type="bibr" target="#b19">[19]</ref> , human communication comprehension <ref type="bibr" target="#b20">[20]</ref> and dialogue systems <ref type="bibr" target="#b21">[21]</ref> , etc.</p><p>While most works approach it as a simple categorization problem, sentiment analysis is actually a suitcase research problem <ref type="bibr" target="#b22">[22]</ref> that requires tackling many NLP tasks, including named-entity recognition <ref type="bibr" target="#b23">[23]</ref> , word polarity disambiguation <ref type="bibr" target="#b24">[24]</ref> , concept extraction <ref type="bibr" target="#b25">[25]</ref> , subjectivity detection <ref type="bibr" target="#b26">[26]</ref> , personality recognition <ref type="bibr" target="#b27">[27]</ref> , sarcasm detection <ref type="bibr" target="#b28">[28]</ref> , and especially aspect extraction <ref type="bibr" target="#b29">[29]</ref> . Aspect-based sentiment analysis (ABSA) proposes a finer-grained polarity detection that extracts aspects first and then classifies them as either positive or negative. For example, in the sentence "The size of the room was smaller than our expectation but the view from the room would not make you disappointed. ", sentiments expressed towards "room size " and "room view " are negative and positive, respectively. Those two terms are called aspect terms and ABSA associates a polarity to each aspect term. Another similar yet different sub-task of ABSA is sentiment analysis towards aspect category. For example, both "room size " and "room view " in the previous example belong to "ROOM_FACILITY". Other aspect categories in this domain are like "PRICE", "SERVICE" and so on. The work of Wang et al. <ref type="bibr" target="#b30">[30]</ref> belongs to this sub-task.</p><p>In this paper, we focus on aspect term sentiment classification, which is a finer grained study compared to the work of Wang et al. We define aspect term as aspect target. If an aspect term contains multiple words, we call this type of aspect term as aspect target sequence. In aspect target sentiment classification, Tang et al. <ref type="bibr" target="#b31">[31]</ref> used target dependent a long short-term memory (LSTM) network. In particular, they use a Bi-LSTM model to encode the sequential information in TC-LSTM. They later appended each word with target embedding to reinforce the extraction of correlation between target and context words in the sentence. In <ref type="bibr" target="#b32">[32]</ref> , they designed a pure attention-based memory network to explicitly learn the correlation between context words and aspect target. Nevertheless, they simply used average aspect word embedding to represent aspect term, which failed to consider the aspect target sequence information. Wang et al. <ref type="bibr" target="#b30">[30]</ref> employed an attention mechanism upon the sequential output from a LSTM layer. Their work treated the sentence sequential information as equal importance to aspect target sequential information.</p><p>All the previous work modeled ABSA as a sentence-level sentiment classification problem that treated aspect target/term as a hint. Such design will result in a dilemma when there appear two aspect targets with opposite sentiment polarities in the same sentence. All state-of-the-art works only focused on one aspect target at a time. They cannot process two aspect targets at the same time, due to the assumption that sentiment of a sentence is equivalent to the sentiment of aspect target/term. Moreover, little attention is paid to aspect target itself, especially when aspect target is a sequence of words, namely multi-word aspect. Almost all literature took the average word embeddings to represent aspect target sequence, which ignored aspect target sequential information. In the English language, their models work well in situations where aspect target has a single word, but not in multiple words. In other cases, although they employed a sentence-level sequence encoder, the importance of aspect target sequence is treated with no emphasis compared with non-aspect word sequence. To this end, we propose two versions of an aspect target sequence model (ATSM), namely: ATSM-S, where -S stands for single granularity, and ATSM-F, where -F stands for fusion. The model is available for download at http://github.com/senticnet/aspect-target-sequence-model .</p><p>ATSM-S explicitly addresses to the multi-word aspect target case. The model includes two crucial modules: adaptive embedding learning and aspect target sequence learning. The first module aims at appending sentence context meaning to general word embeddings for each of the aspect target words. Thus, an accurate vector representation which encoded sentence context will be obtained for aspect target words. Specifically, we extract sentence context with a LSTM encoder. Each aspect target word was attended by the encoded context to form an adaptive word embedding. The second module links each adaptive aspect word embedding with a sequence learning. In the experimental comparison, our ATSM-S outperforms the state of the art on an English multiword aspect subset filtered from SemEval 2014 and four Chinese review datasets.</p><p>Although ATSM-S only solves part of the problems (multi-word aspect scenario) in English ABSA, it becomes comprehensive in addressing Chinese ABSA if considering the multi-granularity representation of Chinese text.</p><p>Chinese is a pictogram language whose text originates from images. Chinese text originates from simple symbols. The symbols gradually evolved to fixed types (named radicals). Through a geometric composition, those fixed types build up characters. Then a concatenation of characters creates the word. Unlike English, each Chinese sub-word granular representation still encodes semantics, shown in Table <ref type="table" target="#tab_0">1</ref> . Whereas in English, only partial character Ngrams encode semantics. This motivates us to explore each granularity of Chinese text in ABSA. In addition, the surface form of Chinese text is at the character level. This guaranteed that even the smallest aspect target, such as a single Chinese character, can be broken down into a sequence of aspect targets at the radical level. Thus, we proposed ATSM-F as an upgraded version of ATSM-S. Specifically, ATSM-S was conducted at each Chinese granularity and ATSM-F fused their results together. In the design of fusion, we tested both early fusion (hierarchical structure) and late fusion (flat structure). Finally, ATSM-F with late fusion prevails all other methods on three out of four Chinese review datasets. To round up, we made the following contributions:</p><p>1. We view aspect-level sentiment analysis from a new perspective, in which aspect target sequence dominates the final result. Whereas in recent literature using deep learning, sentence-level classification is the popular solution <ref type="bibr" target="#b30">[30]</ref><ref type="bibr" target="#b31">[31]</ref><ref type="bibr" target="#b32">[32]</ref> . 2. Starting from our perspective, we propose the adaptive embedding learning to append sentence context to aspect targets. Followed by an explicit modeling of aspect target sequence. Results on English multi-word aspect subset from SemEval2014 and four Chinese review datasets validate the superiority of our model. 3. We take advantage of the multi-grained representation nature of Chinese text and improve the final performance further, which suggests a broader application scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Aspect-based sentiment analysis</head><p>In ABSA, there are three research directions. The first direction is aspect term extraction, such as <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b34">34]</ref> . The second direction aims at categorizing the given aspect term to different categories <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b36">36]</ref> . Wang et al. <ref type="bibr" target="#b30">[30]</ref> employed an attention mechanism upon the sequential output from a LSTM layer. Their work aims at predicting sentiment polarity of the category, such as "FOOD" and "PRICE", rather than any particular aspect terms.</p><p>The third branch works on aspect term sentiment classification. Aspect term is usually marked in a sentence and the goal is to determine the sentiment polarity towards the aspect term. Early works used dictionary-based methods <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b38">38]</ref> . Recent works employed machine learning-based feature engineering and classification <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b40">40]</ref> .</p><p>Most state-of-the-art works used a LSTM network <ref type="bibr" target="#b41">[41]</ref> and attention mechanism as the basic modules in their methods <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b32">32]</ref> . Tang et al. used target dependent Bi-LSTM model to encode the sequential information in TC-LSTM. They later appended each word with target embedding to reinforce the extraction of correlation between target and context words in the sentence. In MemNet <ref type="bibr" target="#b32">[32]</ref> , they designed a pure attention-based memory network to explicitly learn the correlation between context words and aspect words.</p><p>Previous works on aspect-term sentiment analysis suffered from two main drawbacks. Firstly, ABSA is modeled as a sentencelevel sentiment classification problem that treated aspect target/term as a hint. Such design will result in a dilemma when there appear two aspect targets with opposite sentiment polarities. All state-of-the-art works only focused on one aspect target at a time. They cannot process two aspect targets at the same time, due to the assumption that sentiment of a sentence is equivalent to the sentiment of aspect target/term. Secondly, little attention is paid to aspect target itself, namely aspect target sequence information. In this paper, we aim to address these two drawbacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Chinese text representation</head><p>Contemporary Chinese text processing mostly relies on Chinese word embeddings <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b43">43]</ref> . However, Chinese word consists subelements like characters that contain semantics. Xu et al. <ref type="bibr" target="#b44">[44]</ref> studied characters within a word can enrich semantics for Chinese word and character embeddings. Chinese text has one level below character level, which is radical level. It has been demonstrated that radical level representation also encodes certain extent of semantics <ref type="bibr" target="#b45">[45]</ref> . Chen et al. <ref type="bibr" target="#b46">[46]</ref> started to decompose Chinese words into characters and proposed a character-enhanced word embedding model (CWE). Sun et al. <ref type="bibr" target="#b47">[47]</ref> started to decompose Chinese characters to radicals and developed a radicalenhanced Chinese character embedding. Shi et al. <ref type="bibr" target="#b45">[45]</ref> began to train pure radical-based embedding for short-text categorization, Chinese word segmentation, and web search ranking. Li et al. <ref type="bibr" target="#b48">[48]</ref> proposed component-enhanced Chinese character embedding models by incorporating internal compositions and the external contexts of Chinese characters. Yin et al. <ref type="bibr" target="#b49">[49]</ref> extended the pure radical embedding in <ref type="bibr" target="#b50">[50]</ref> by introducing multi-granularity Chinese word embeddings. Peng et al. <ref type="bibr" target="#b51">[51]</ref> developed radical-based hierarchical Chinese embeddings specifically for sentiment analysis. However, none of them has exploited the multi-grained characteristic of Chinese text in complex NLP problems, such as ABSA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method overview</head><p>In this section, we first define our task and then present an overview of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Aspect target sequence</head><p>Aspect is a concept that contains various interpretations, such as aspect target/term, aspect word, aspect category, aspect sentiment etc. For instance, a sentence " (This cuisine has a good flavor.)" has an aspect target/term " (flavor)". The aspect target contains only one aspect word, which is " (flavor)". The aspect target belongs to an aspect category of "FOOD". Other aspect categories in the domain of restaurant are like "PRICE", "SERVICE" and so on. The sentiment of the aspect target " (flavor)"is positive in the sentence. However, in the context of this paper, we define aspect as aspect target sequence. As Chinese text can be decomposed to three granularities, a single unit of higher level representation can be decomposed to a sequence of units of lower level representation. For instance, the single-word aspect target " (flavor)" in the previous example can be decomposed to a sequence of Chinese characters: " " and " ". Moreover, the characters can be further decomposed to a sequence of Chinese radicals: " ", " "," " and " ". As <ref type="bibr" target="#b44">[44,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b51">51]</ref> suggested, various granularities contain exclusive semantics. In the above example, " " at word level simply means 'flavor'. " " and " " at character level mean 'thinking of the flavor'. " ", " "," " and " " at radical level mean 'to taste the unknown and brainstorm the flavor'. It is apparent to see from the example that sub-component semantics provide complementary explanations to the word and, hence, enrich its meaning. We reconstruct an aspect target as three sequences at three granularities. Methods were developed to work on these sequences in order to determine the sentiment polarity of the aspect target.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Task definition</head><p>A sentence s of n units (the unit could be radical, character or word) in the format s = { u 1 , u 2 , . . . , u j , u j+1 , . . . , u j+ L , . . . , u n -1 , u n } is marked out with aspect target (comprising multiple units) { u j , u j+1 , . . . , u j+ L } . The u j+ L stands for the ( j + L ) th unit in the sentence and the L th unit in the aspect target. L indicates that the aspect target contains L consecutive units. The goal is to predict the sentiment polarity of the aspect target.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Overview of the algorithm</head><p>In all previous works of ABSA <ref type="bibr" target="#b30">[30]</ref><ref type="bibr" target="#b31">[31]</ref><ref type="bibr" target="#b32">[32]</ref> , if an aspect target contains multiple words, they treated the multi-word aspect as one unified target by averaging their word embeddings. This is disadvantageous in two ways. Firstly, word embeddings of aspect target that were trained from general corpus might mislead the meaning of aspect target in the sentence. Secondly, sequential information within the aspect target is lost. For instance, a sentence "The red apple released in California last year was a disappointment. " contains an aspect target "red apple ". Based on the occurrence of "released in California", we can understand that "red apple " stands for iPhone. If general word embeddings of "red " and "apple " were used in the task, it will deviate from the symbolic meaning of iPhone to fruit apple. To make it worse, by averaging the word embeddings of "red " and "apple ", sequential information is lost and the averaged word embedding will result in a new/irrelevant meaning in the word vector space.</p><p>In order to address the above two issues, we propose a threestep model. The first step is adaptive embedding learning, which essentially aims at learning intra-sentence context for each unit in the aspect target sequence. It was designed to embed intrasentence contexts to the general embeddings of units in aspect target sequence, which will resolve the first issue aforementioned. The second step is simply a sequence learning process of aspect target, which has never been addressed before. Last but not the least, Chinese text has three granularities of representation (radical, character and word) so that we apply the first two steps at each of the three granularities and glue them together with fusion mechanisms. This is particular for Chinese text, as even single word aspect target can be decomposed to up to three sequences of representation. Fig. <ref type="figure" target="#fig_0">1</ref> presents a graphical illustration of ATSM-F in late fusion. In English, however, our model only applies to cases where aspect target contains multiple words. We will illustrate each of the three steps below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Adaptive embedding learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Sentence sequence learning</head><p>Sequential information is crucial in determining aspect term sentiment polarity. For example, there are two sentences: "The movie supposed to be amazing but I find it just so-so. " and "The movie supposed to be just so so but I find it amazing. " These two sentences have exactly same words but arranged in a different order. This results in the opposite sentiment polarity of the aspect target "The movie ". In order to extract sentence sequential information like the above, we use a LSTM to encode the sentence sequential information. The output of the LSTM is a sequence of cell hidden outputs which has the same length of the sentence. Mathematically, a sentence and its corresponding LSTM sequence output are denoted as { w 1 , w 2 , . . . , w j , . . . , w n -1 , w n } and { h 1 , h 2 , . . . , h j , . . . , h n -1 , h n } , respectively, where w n ∈ R 1 ×d and</p><formula xml:id="formula_0">h n ∈ R 1 ×d lstm .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Aspect target unit learning</head><p>As we discussed before, the meaning of aspect target word may be shifted due to the sentence context, such as the example of "red apple ". Thus, we decide to embed the intra-sentence contexts to each unit in the aspect target. To this end, we employ an attention mechanism to realize the learning. As we know from Bahdanau et al. <ref type="bibr" target="#b52">[52]</ref> , the attention mechanism can be understood as a weighted memory of lower-level elements. Conceptually, the output attention vector extracts the correlation between query (in our case which is the unit in aspect target) with each element. In our model, we compute an attention for each unit in aspect target with LSTM sequence hidden output from sentence sequence learning and name it adaptive vector. Thus, the adaptive vector extracts the most relevant correlation with intra-sentence context.</p><p>Specifically, for an aspect target unit u i and its word embed-</p><formula xml:id="formula_1">ding v i ∈ R 1 ×d in a sentence of length n , its adaptive vector V adapt ∈ R 1 ×(d+ d lstm )</formula><p>is given as below:</p><formula xml:id="formula_2">V adapt = n j=1 α j • v i h j (<label>1</label></formula><formula xml:id="formula_3">)</formula><p>where h j ∈ R 1 ×d lstm is the j th output from LSTM hidden output sequence. α j is the weight for the j th memory in the sentence and n j=1 α j = 1 . It depicts how much the semantic influence of the j th unit imposed on the aspect target unit u i . It is a weight com-puted from softmax below:</p><formula xml:id="formula_4">α j = e g j n m =1 e g m (<label>2</label></formula><formula xml:id="formula_5">)</formula><p>where g j is a score obtained from a feed-forward neural network attention model and its formula is given as:</p><formula xml:id="formula_6">g j = tanh W j • v i h j + b (<label>3</label></formula><formula xml:id="formula_7">)</formula><p>where W ∈ R (d+ d lstm ) ×1 and b ∈ R 1 ×1 . We compute the adaptive vector for each unit in aspect target. In the end, we will get as many adaptive vectors as the number of aspect target units. Each of these adaptive vectors concentrates the influence that sentence context imposed on aspect target, namely it enriches the semantic meaning of aspect target by extracting correlation from intra-sentence context. For instance, the meaning of "apple " in our previous example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Sequence learning of aspect target</head><p>Having obtained the adaptive vector of each aspect target unit, we will extract the sequential information in aspect target sequence. We find that sequential information within aspect target sequence is crucial in representing the meaning of an aspect target. Recall the previous example of "red apple ". Only by connecting "red " and "apple " will we have a complete impression of the new iPhone 7 in red coating. Isolating the two aspect words will be harmful. Therefore, we employ the second LSTM neural network <ref type="bibr" target="#b41">[41]</ref> to encode such sequential information.</p><p>Specifically, we concatenate the adaptive vector of each aspect target unit to form an aspect target sequence. This sequence will be fed to a LSTM as input. In the end, we take out the hidden output H L from the last LSTM cell as the representation of aspect target sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Fusion of multi-granularity representation</head><p>Unlike Latin languages such as English, Chinese written language is a type of pictogram, where its primitive forms symbolize certain meanings, such as characters ' (sun)' and ' (moon)'. With time went by, more complex meanings need to be represented in text. Thus, certain simple characters cluster together to form complex characters. For instance, ' (shining)' composed of two sub-element characters ' (sun)' and ' (moon)'. The semantic relation between is both ' (sun)' and ' (moon)' emit light and bring brightness. Simple characters like ' ' and ' ' were thus called 'radical' when they appear in constructing complex characters. In order to represent abstract meanings, certain complex characters were clustered to form words. For instance, word ' (celebrity)' composed of character ' (shining)' and ' (star)'. Celebrities are shining stars in a sense.</p><p>Due to the above reasons, we understand modern Chinese text can be represented at three different granularities: radical, character and word. Inspired by Peng et al. <ref type="bibr" target="#b51">[51]</ref> , we represent Chinese text at three different granularities in our model and study the possible outcomes by fusion any of them.</p><p>In order to fit Chinese text into our deep learning framework, we represent Chinese text with embedding vectors. Particularly, we use the skip-gram model <ref type="bibr" target="#b53">[53]</ref> to learn the embedding vectors at different granularities. Our training corpus contains about 8 million Chinese words, which is equal to 38 million Chinese characters or 150 million Chinese radicals. For word embedding vectors, we conduct word segmentation on the corpus using ICTCLAS <ref type="bibr" target="#b54">[54]</ref> segmenter and then train with the skip-gram model. For character embedding vectors, we split each word in the corpus to individual characters, in which we still keep the order of characters. For radical embedding vectors, we decompose each character into radicals and concatenate them in the order from left to right. The decomposition was based on a Chinese character-radical look-up table we built using the Chinese character parser 'HanziCraft<ref type="foot" target="#foot_0">1</ref> '.</p><p>We design two fusion mechanisms (early and late) to merge three granularities. Early fusion concatenates different granularities of each aspect unit before aspect target sequence learning. Specifically, each aspect target word was represented by a concatenation of its sub-granular representations before it was sent to aspect target sequence learning. The output from aspect target sequence learning step will be fed to a softmax classifier.</p><p>Late fusion concatenates different granularities after aspect target sequence learning. Thus, for each granularity, an aspect target sequence representation will be obtained first. These representations will be concatenated and fed to a softmax classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Early fusion</head><p>We have already proposed the fundamental model ATSM-S for ABSA. However, the performance of the model should largely depend on the representation of text because the embedding vectors are the initial input to the model. To this end, we plan to incorporate the multi-level representation of Chinese text. ATSM-S emphasizes the role of word level of representation. We incorporate multi-level representation for aspect word to further improve the accuracy of aspect words. Instead of using only word level representation in ATSM-S, we explore using either two or three level of representation, namely radical, character and word level.</p><p>Specifically for each sentence, we construct three types of sentence strings: a word string, a character string, and a radical string. In each of the string, aspect words are decomposed into the corresponding level. For each unit in the decomposed string of aspect words, it will learn an attention vector between the whole sentence string. For example, given an aspect word ' (craftsmanship)'. One word attention vector will be learned from the word string. Two character attention vectors will be learned from the character string because the aspect word contains two characters, ' ' and ' '. Three radical attention vectors will be learned due to the aspect word can be decomposed into three radicals, ' ', ' ' and ' '. Then, we compute an average attention vector for each representation level. Three resulting average attention vectors will be concatenated and will be treated as a fusion of multi-level representation. As this fusion is a feature level fusion for aspect term, we call this fusion the early fusion. The fusion attention vector will be fed to a LSTM like in ATSM-S. The final output from the LSTM will be fed to a softmax classifier. Graphical illustration is in Fig. <ref type="figure" target="#fig_1">2</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Late fusion</head><p>Unlike the early fusion, where the fusion takes place at the feature level, in late fusion, the fusion of multi-level representation happens at classification step.</p><p>In late fusion, our ATSM-S is used intact at three different levels independently. As shown in Fig. <ref type="figure" target="#fig_1">2 (b)</ref>, the green break line box stands for ATSM-S working at the word level. While the purple box stands for working at the character level and the blue box stands for working at the radical level. We take out the last LSTM hidden output from each level and concatenate them. The resulting concatenated vector will be fed to a softmax classifier.</p><p>Late fusion differs from early fusion in assuming that semantics within a sentence should be unified at representation level. In other words, the semantics of aspect terms at a single level can hardly help extract semantics at other representation levels. Thus, in late fusion, the ATSM-S works merely on one level at a time and combines at final classification step only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Evaluation</head><p>In this section, we present our evaluations in three steps. The first step will conduct experimental evaluations of various methods to model aspect target sequence, as well as adaptive embedding learning. The second step will compare the proposed ATSM-S with the state of the art. The last step will evaluate the improvement by fusions of granularities. We used Tensorflow and Keras to implement our model. All models used AdagradOptimizer with a learning rate of 0.1 and a L2-norm regularizer of 0.01. Each minibatch contains 50 samples. We report the average testing results of each model for 50 epochs in 5-fold cross validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Datasets</head><p>Four Chinese datasets from four domains were used in our experiments <ref type="foot" target="#foot_1">2</ref> . They are Chinese aspect datasets we collect from <ref type="bibr" target="#b55">[55]</ref> . They contains reviews in four domains: notebook, car, camera, and phone. Aspect targets were originally tagged by Zhao et al. <ref type="bibr" target="#b56">[56]</ref> . Then, we manually labeled the sentiment polarity towards each aspect target as either positive or negative. The metadata of the dataset was displayed in Table <ref type="table" target="#tab_1">2</ref> . English dataset used in our experiments was a subset from the SemEval2014 <ref type="bibr" target="#b57">[57]</ref> , which contains reviews from two different domains: restaurant and laptop. We select the reviews which contain multi-word aspect target only and results in a subset of 2309 reviews (30% of the original dataset).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Comparison methods</head><p>In our experiments, we include three types of baseline comparisons. The first type includes the self-variants of ATSM-S, which examines the validity of each module of our model. The second type is the state-of-the-art methods in ABSA task, which tests the overall performance. The last type explores how the fusion of multigrained representation of Chinese will affect the ABSA task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.1.">Variants of ATSM-S</head><p>As there are two major modules of ATSM-S, namely adaptive embedding learning and sequence learning of aspect target, we design different variants for either of the modules to validate its superiority. ATSM-v1 and ATSM-v2 were designed to examine adaptive embedding module. ATSM-v3, ATSM-v4 and ATSM-v5 were designed to examine the module of sequence learning of aspect target.  5. ATSM-v5 : The last variant of ATSM-S. Unlike ATSM-v4 which models the aspect target sequence with a CNN, ATSM-v5 concatenates their embeddings and feeds to a nonlinear neural layer. 6. ATSM-S : There are three sub-categories of this type. The first one is ATSM-S working on the word level. The other two are ATSM-S models working on character and radical level, respectively. These three variants do not have any fusion of representation levels and, hence, serve as baselines towards our fusion mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.2.">State of the art</head><p>We include several state-of-the-art methods: SVM, LSTM, Bi-L STM, TD-L STM, TC-L STM, MemNets and ATSM-S.</p><p>1. SVM : SVM classifier trained on the surface and parse features, such as unigram, bigram, POS tags. Aspect target features were concatenated to sentence features. 2. LSTM : The typical sequence modeling method that unveils the sequential information from the head to the tail of the sentence. It does not pay special attention to aspect term in the sentence. For long sentences, this method leverages more on the ending words in the sentence than the beginning words. Thus, for the case when aspect term appears in the head of the sentence, it may not work well. 3. Bi-LSTM : It adds a reverse sequential learning step to LSTM. Bi-LSTM models both head to tail and tail to head sequential information, however, it does not distinguish the aspect term with context words in ABSA. 4. TD-LSTM : Instead of attending to the complete length of a sentence like LSTM, TD-LSTM <ref type="bibr" target="#b31">[31]</ref> used a forward and a backward sequence that ends immediately after including aspect term. It extracts sentence semantics prior and after aspect term separately. 5. TC-LSTM : In addition to TD-LSTM, TC-LSTM appended the sentence word embedding with aspect target embedding. It hopes this procedure could explicitly capture the interaction between aspect word and sentence context word. Nevertheless, this method treated the sequential information from aspect target sequence and sentence word sequence with equal importance. They did not model the aspect target sequence. 6. MemNet : This method took out the aspect word and looked for correlation with sentence context words. The problem of this method is it did not use the sequential information within aspect target sequence. In our experiments on both English and Chinese datasets, we conducted various experiments using hop number from one to nine of this model and reported the best results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.3.">Fusion comparison</head><p>1. ATSM-F : Based on ATSM-S, it fuses not only three representation granularities but also any two representation granularities, in both early and late manner. There are 11 different settings in this experiment. It evaluates whether fusion will improve from single granularity and which combination benefits the final result most.</p><p>Unlike all the previous methods, the novelties of our model are three folds. Firstly, we adapt the general word embedding to ABSA task by encoding sentence context. Secondly, we explicitly model the aspect target sequence. Finally, we design fusion mechanism on ATSM-S to take advantage of three granularities of Chinese representations. The experimental results are shown in Tables <ref type="table" target="#tab_2">3 ,</ref><ref type="table">4</ref> and 6 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Result analysis 7.3.1. Self comparison</head><p>In this section, we compare different variants of ATSM-S with experiments on Chinese datasets. The experimental results were shown in Table <ref type="table" target="#tab_2">3</ref> .</p><p>We can observe that ATSM-S achieves the highest accuracy in all the datasets and highest F-score in three datasets. It generally demonstrates the validity of our model design. In order to elaborate more details, we conduct the comparisons with model variants.</p><p>The only difference between ATSM-v1 and ATSM-S is that the former omits sentence sequential information. The decrease of performance from ATSM-v1 proves that ATSM-S has successfully encoded sentence sequence. Even if the sentence sequential information is correctly learned, the overall performance cannot be guaranteed. This is illustrated by ATSM-v2, which encodes sentence sequence but does not learn adaptive embedding. Since ATSM-S learns adaptive embedding on top of ATSM-v2, a more accurate aspect target representation is learned and, hence, contributes to the final performance. ATSM-v3, -v4 and -v5 distinguish with each other in the way they model aspect target sequence. -v3 takes the average of aspect target word embeddings, which ignores aspect target sequential information. -v4 models the aspect target sequence with a CNN. -v5 models the sequence with the middle layer from an MLP. In comparison, ATSM-S models the sequence with a LSTM. From the table, we can conclude that LSTM achieves the best results compared with other variants. It further proves our assumption that the aspect target sequential information plays a significant role in ABSA task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.2.">Peer comparison</head><p>From Table <ref type="table">4</ref> , we can see that ATSM-S beats other state-of-theart methods by around 1-4% in all four datasets and in one overall dataset.</p><p>We believe the first reason why ATSM-S wins over other methods is that we explicitly learned the adaptive meaning of each aspect target unit. The adaptive embedding of each aspect target unit not only carries semantics from general word embedding but also encodes semantics within the sentence. In comparison, the baseline model ATSM-v2 eliminates sentence sequence learning step and, hence, results in a poor adaptive embedding.</p><p>The second reason is that we explicitly modeled the aspect target sequence. Other state-of-the-art works either ignored the aspect target sequence <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b32">32]</ref> or treated aspect target sequence as equal importance as sentence sequence <ref type="bibr" target="#b31">[31]</ref> . Both of the approaches did not render enough emphasis on aspect target sequence. To validate its importance, we designed the second baseline variant of ATSM-S, which is ATSM-v3. It differs from ATSM-S only in ignoring target sequence information. The sharp decrease of performance from ATSM-S to ATSM-v3 validated our assumption.</p><p>The difference between ATSM-S with the popular attention model where the aspect is embedded by a LSTM layer are twofold. Firstly, ATSM-S specifically encodes aspect target sequential information. Whereas attention model treated aspect target as an averaged embedding vectors. Secondly, aspect target sequential information was given higher importance than sentence sequential information in ATSM-S. Whereas attention model treated the two sequences of equal importance.</p><p>Since ATSM-S specializes in modeling aspect target sequence, we conduct further experiments to test whether it is language independent. Thus, we removed reviews from English SemEval2014 dataset that had single-word aspect target only (eg. pasta) and collected the remaining reviews that all had multi-word aspect target (eg. build quality) to form a multi-word aspect target subset. Meanwhile, we also collected the removed reviews to form a single-word aspect target subset.</p><p>Experimental results on these two subsets were shown in Table <ref type="table" target="#tab_3">5</ref> in comparison with the top few state-of-the-art works, namely MemNet, TC-L STM, TD-L STM, and Bi-LSTM. In the singleword case, the proposed ATSM-S achieved the highest accuracy. It is beyond our expectation because the module of sequence learning of aspect target from ATSM-S would not work on single-word aspect target. On the other hand, it validates the contribution of adaptive embedding learning module, which learns an accurate presentation of aspect target. In the other case, the table shows ATSM-S is the best at predicting multi-word aspect sentiment polarity in English dataset than the state of the art. The main reason is that our model explicitly learns adaptive embedding and aspect target sequence, where the latter is crucial. A visual analysis is provided in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.">Visual case study</head><p>We visualize the difference between ATSM-S and a typical baseline model (MemNet) via a case study from English SemEval 2014 dataset.</p><p>As shown in Fig. <ref type="figure" target="#fig_3">3</ref> , we plot the heatmap of attention weights. The deeper the color is, the heavier weight the word has. Our ATSM-S has two heatmaps due to we explicitly learn adaptive embedding for each aspect target word ('Korean' and 'dishes').</p><p>Whereas MemNet only has one, because it averages the word embeddings of aspect target and learns a sentence-level attention. It is apparent that either of our aspect unit adaptive embedding captures a key opinion word in the sentence, which are 'affordable' and 'yummy'. In the later step of aspect target sequence learning, both of the opinion words will be captured and reflected in our final model output. Nevertheless, the heatmap from MemNet is the final model output, which unfortunately misses a crucial part of the opinionated content. The case study provides an intuitive explanation of why our ATSM-S prevails.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5.">Granularity and fusion analysis</head><p>In the last set of experiments, we evaluated if multiple granularities in Chinese text representation will improve the performance of our model further. As shown in Table <ref type="table" target="#tab_4">6</ref> , we performed ATSM-S at each of the three granularities as baselines. We also applied ATSM-F in both early fusion mode and late fusion mode. The ATSM-F in late fusion of word and character level achieved high results in four out of five datasets. It beat ATSM-S in almost any single granularity situation (except word level on Car dataset. However, it is close to the performance of ATSM-S at word level.), which proved that a fusion of multiple granularities promoted the sentiment inference over single granularity.</p><p>Generally, we could see ATSM-S at character level produces the top few results in all single granularity cases. However, we found that word level performed better than character level on notebook and car datasets, a deeper look into those two datasets revealed the possible cause of biased data distribution. After computing variances of experimental results for each dataset, we found that the average variance of Notebook and Car dataset is 1.7 times bigger than the average variance of all five datasets at the word level, and 1.29 times larger than the average at the character level. This indicates that our model performances in these two datasets were less robust than the other three datasets. Furthermore, we found that the number of unique aspect target in these two datasets were relatively higher compared with their dataset size. This explains why our model did not generalize well on these two datasets. Moreover, due to their smaller size compared, we believe all the above caused the character level performed worse than word level. In comparison, in the other three datasets, whose variances were well below average, character level outperformed   word level with an obvious advantage. This is consistent with our expectation, as working at character level will wipe out the negative effects brought by word segmentation. It also explains why 'W + C' achieved the top few results. Sentiment information from the character level is effectively extracted and properly maintained with the help of effective character embeddings and ATSM-S. Fused with the word level information, the character level sentiment information improved the overall performance. However, working at radical level did not improve the performance much, if not exacerbating the situation. Thus, it drove us to analyze the reason. We studied the aspect target distribution for each of the three representation granularities with our experimental datasets as examples. As shown in Fig. <ref type="figure" target="#fig_4">4</ref> , we plot the percentage of token types (i.e, unique tokens) of three different granularities appearing less than 10 times in the whole dataset. It is obvious to find that the representation of character level largely reduces the percentage of token types occurring only once in the word level. That is to say, character level representation signifi-cantly reduces the data sparsity of rare words by decomposing the words into characters. This explains why character level representation could greatly improve from word level. Radical level, on the contrary, does not reduce much the percentage from character level. One possible reason could be the ineffectiveness of our radical embedding vectors. In training the radical embeddings, we did not distinguish the radicals by phoneme and morpheme. This may introduce errors to radical embeddings, as phonemes do not carry semantics. These radical embeddings could affect the final results drastically. That being said, the radical level representation is still comparable to other baseline models. It indicates the potential of introducing radical level representation in the task of Chinese ABSA.</p><p>We elaborate why ATSM-F in late fusion mode achieves the top few performances as below. Our fusion mechanisms experimented on possible combinations of three granularities extract multi-granular semantics in the task of ABSA. In comparison, late fusion has a flat structure, while early fusion has a hierarchical structure. Using a flat structure means the semantic encoded by each granularity is relatively independent. Whereas using a hierarchical structure breaks down the semantic flow along each granularity. Because it fuses the representations of multi-granularity for each aspect target word, semantics at character and radical level were cut off by the boundary of words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>In this paper, we investigated the problem of aspect-level sentiment analysis from a new perspective, in which the aspect target sequence dominates the final result. Accordingly, we proposed ATSM-S, which prevailed the state of the art in multi-word aspect sentiment analysis on SemEval2014. Moreover, our model specifically catered to the multi-granularity representations of Chinese text. By designing a late fusion method, ATSM-F outperformed all state-of-the-art works in three Chinese review datasets. As one of the first attempts to exploiting multi-granularity nature of Chinese ABSA, this work paves the path for potentially wider application scenarios in Chinese natural language processing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. ATSM-F late fusion framework. RNN-1,-2,-3 are at word, character and radical level, respectively. Green RNNs are for adaptive embedding learning. Grey RNNs are sequence learning of aspect target. Aspect target is highlighted in red. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Fusion mechanisms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1.</head><label></label><figDesc>ATSM-v1 : The first variant of ATSM-S. It eliminates sentence sequence learning step in ATSM-S. In the following steps, it replaces sentence level LSTM hidden state outputs with initial word embeddings. 2. ATSM-v2 : The second variant of ATSM-S. It removes the adaptive embedding learning module. Instead, it takes the sentence level LSTM hidden state outputs of each aspect target word as the input to aspect target sequence learning module. 3. ATSM-v3 : The third variant of ATSM-S. It replaces the module of aspect target sequence learning with an average of aspect target word embeddings. It does not extract the aspect target sequence information. 4. ATSM-v4 : The fourth variant of ATSM-S. It opts for different modeling of aspect target sequence. Specifically, it replaces the LSTM at aspect target sequence level in ATSM-S with a CNN (convolutional neural network).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Visual attention weights of each word in the example. (a) is from ATSM-S. (b) is from baseline model.</figDesc><graphic coords="9,122.84,57.00,360.00,87.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Percentage of number of terms in corresponding to from 1 to 10 occurrences in three-level representation.</figDesc><graphic coords="9,42.84,333.51,250.80,180.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Comparison between English and Chinese text in composition.</figDesc><table><row><cell>English</cell><cell></cell><cell></cell><cell>Chinese</cell><cell></cell><cell></cell></row><row><cell>Hierarchy</cell><cell>Example</cell><cell>Encodes semantics</cell><cell>Hierarchy</cell><cell>Example</cell><cell>Encodes semantics</cell></row><row><cell>Character</cell><cell>a, b, c, ...</cell><cell>N</cell><cell>Radical</cell><cell>, ,</cell><cell>Y</cell></row><row><cell>Character N-gram</cell><cell>pre, sub</cell><cell>partial Y</cell><cell>Character</cell><cell>, ,</cell><cell>Y</cell></row><row><cell>Word</cell><cell>awake, cheer</cell><cell>Y</cell><cell>Single-character word</cell><cell>,</cell><cell>Y</cell></row><row><cell>Phrase</cell><cell>kick off, put on</cell><cell>Y</cell><cell>Multi-character word</cell><cell>,</cell><cell>Y</cell></row><row><cell>Sentence</cell><cell>Nice to meet you.</cell><cell>Y</cell><cell>Sentence</cell><cell></cell><cell>Y</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Metadata of Chinese dataset.</figDesc><table><row><cell></cell><cell>Notebook</cell><cell>Car</cell><cell>Camera</cell><cell>Phone</cell><cell>Overall</cell></row><row><cell>Positive</cell><cell>417</cell><cell>886</cell><cell>1558</cell><cell>1713</cell><cell>4574</cell></row><row><cell>Negative</cell><cell>206</cell><cell>286</cell><cell>673</cell><cell>843</cell><cell>2008</cell></row><row><cell>Percentage of multi-word aspect</cell><cell>38.20</cell><cell>36.95</cell><cell>44.55</cell><cell>40.49</cell><cell>41.02</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>Variants of ATSM-S on Chinese datasets at word level.</figDesc><table><row><cell>Notebook</cell><cell></cell><cell>Car</cell><cell></cell><cell>Camera</cell><cell></cell><cell>Phone</cell><cell></cell><cell>Overall</cell><cell></cell></row><row><cell>Acc</cell><cell>F1</cell><cell>Acc</cell><cell>F1</cell><cell>Acc</cell><cell>F1</cell><cell>Acc</cell><cell>F1</cell><cell>Acc</cell><cell>F1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5</head><label>5</label><figDesc>Accuracy and Macro-F1 results on single-word/multi-word aspect target subset from SemEval2014.</figDesc><table><row><cell></cell><cell cols="2">ATSM-S (word)</cell><cell>MemNet</cell><cell></cell><cell>TC-LSTM</cell><cell></cell><cell>TD-LSTM</cell><cell></cell><cell>Bi-LSTM</cell><cell></cell></row><row><cell></cell><cell>Acc</cell><cell>F1</cell><cell>Acc</cell><cell>F1</cell><cell>Acc</cell><cell>F1</cell><cell>Acc</cell><cell>F1</cell><cell>Acc</cell><cell>F1</cell></row><row><cell>Multi-word aspect English dataset</cell><cell>65.37</cell><cell>36.54</cell><cell>58.54</cell><cell>42.16</cell><cell>63.58</cell><cell>43.87</cell><cell>63.48</cell><cell>47.16</cell><cell>62.19</cell><cell>45.02</cell></row><row><cell>Single-word aspect English dataset</cell><cell>75.39</cell><cell>54.12</cell><cell>67.83</cell><cell>52.70</cell><cell>59.33</cell><cell>49.58</cell><cell>68.38</cell><cell>52.95</cell><cell>72.80</cell><cell>54.35</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6</head><label>6</label><figDesc>Accuracy results of multi-granularity with and without fusion mechanisms. (W, C, R stands for word, character and radical level, respectively. + means a fusion operation.)</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Notebook</cell><cell>Car</cell><cell>Camera</cell><cell>Phone</cell><cell>Overall</cell></row><row><cell>ATSM-S</cell><cell></cell><cell>W</cell><cell>75.59</cell><cell>82.94</cell><cell>82.88</cell><cell>84.86</cell><cell>85.95</cell></row><row><cell></cell><cell></cell><cell>C</cell><cell>74.32</cell><cell>81.56</cell><cell>87.98</cell><cell>88.34</cell><cell>88.50</cell></row><row><cell></cell><cell></cell><cell>R</cell><cell>69.92</cell><cell>75.68</cell><cell>77.19</cell><cell>78.09</cell><cell>79.87</cell></row><row><cell>ATSM-F</cell><cell>Early fusion</cell><cell>W + C</cell><cell>77.52</cell><cell>82.16</cell><cell>86.55</cell><cell>87.13</cell><cell>89.38</cell></row><row><cell></cell><cell></cell><cell>W + R</cell><cell>68.38</cell><cell>76.61</cell><cell>77.73</cell><cell>78.29</cell><cell>83.64</cell></row><row><cell></cell><cell></cell><cell>C + R</cell><cell>69.99</cell><cell>77.81</cell><cell>80.73</cell><cell>80.90</cell><cell>87.41</cell></row><row><cell></cell><cell></cell><cell>W + C + R</cell><cell>69.99</cell><cell>77.55</cell><cell>78.76</cell><cell>78.91</cell><cell>84.94</cell></row><row><cell></cell><cell>Late fusion</cell><cell>W + C</cell><cell>73.67</cell><cell>82.93</cell><cell>88.30</cell><cell>88.46</cell><cell>89.33</cell></row><row><cell></cell><cell></cell><cell>W + R</cell><cell>67.26</cell><cell>78.23</cell><cell>80.68</cell><cell>84.94</cell><cell>86.43</cell></row><row><cell></cell><cell></cell><cell>C + R</cell><cell>67.58</cell><cell>79.00</cell><cell>87.63</cell><cell>88.14</cell><cell>88.50</cell></row><row><cell></cell><cell></cell><cell>W + C + R</cell><cell>67.91</cell><cell>78.15</cell><cell>87.98</cell><cell>88.07</cell><cell>89.30</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://hanzicraft.com .</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Datasets download at: http://sentic.net/chinese-review-datasets.zip .</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Table 4 Accuracy and Macro-F1 results on Chinese datasets at word level. Notebook Car Camera Phone</title>
	</analytic>
	<monogr>
		<title level="j">Overall Acc F1 Acc F1 Acc F1 Acc F1 Acc F1 SVM</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page">92</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A Practical Guide to Sentiment Analysis</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bandyopadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Feraco</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Springer</publisher>
			<pubPlace>Cham, Switzerland</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A review of affective computing: From unimodal analysis to multimodal fusion</title>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bajpai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hussain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="98" to="125" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Context-dependent sentiment analysis in user-generated videos</title>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mazumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL</title>
		<meeting>the ACL</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="873" to="883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Distinguishing between facts and opinions for sentiment analysis: survey and challenges</title>
		<author>
			<persName><forename type="first">I</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Welsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="65" to="77" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multilingual sentiment analysis: From formal to informal and scarce resource languages</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cornforth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell. Rev</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="499" to="527" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Lexicon generation for emotion analysis of text</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bandhakavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wiratunga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Massie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Deepak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intell. Syst</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="102" to="108" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">OntoSenticNet: a commonsense ontology for sentiment analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dragoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intell. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">SenticNet 5: discovering conceptual primitives for sentiment analysis by means of context embeddings</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI</title>
		<meeting>the AAAI</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Statistical learning theory and ELM for big social data analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Oneto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bisio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comput. Intell. Mag</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="45" to="55" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semi-supervised learning for big social data analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">275</biblScope>
			<biblScope unit="page" from="1662" to="1673" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning word representations for sentiment analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognit. Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="843" to="851" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Sentic Computing: A Common-Sense-Based Framework for Concept-Level Sentiment Analysis</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hussain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Springer</publisher>
			<pubPlace>Cham, Switzerland</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Natural language based financial forecasting: a survey</title>
		<author>
			<persName><forename type="first">F</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Welsch</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10462-017-9588-9</idno>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell. Rev</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Challenges of sentiment analysis for dynamic events</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hossein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sheth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intell. Syst</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="70" to="75" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sentic computing for patient centered application</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Durrani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Havasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Eckl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Munro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICSP</title>
		<meeting>the ICSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1279" to="1282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sentiment analysis in tripadvisor</title>
		<author>
			<persName><forename type="first">A</forename><surname>Valdivia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Luzon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intell. Syst</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="72" to="77" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">What men say, what women hear: finding gender-specific meaning shades</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garimella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intell. Syst</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="62" to="67" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning community embedding with community detection and node embedding on graphs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cavallari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CIKM</title>
		<meeting>the CIKM</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="377" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adaptive two-stage feature selection for sentiment classification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SMC</title>
		<meeting>the SMC</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1238" to="1243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-attention recurrent network for human communication comprehension</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI</title>
		<meeting>the AAAI</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Augmenting end-to-end dialog systems with commonsense knowledge</title>
		<author>
			<persName><forename type="first">T</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI</title>
		<meeting>the AAAI</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sentiment analysis is a big suitcase</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gelbukh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Thelwall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intell. Syst</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="74" to="80" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Label embedding for zero-shot fine-grained named entity typing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the COLING</title>
		<meeting>the COLING</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="171" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Word polarity disambiguation using Bayesian model and opinion-level features</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognit. Comput</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="369" to="380" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">SenticNet 4: a semantic resource for sentiment analysis based on conceptual primitives</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bajpai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the COLING</title>
		<meeting>the COLING</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2666" to="2677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bayesian network based extreme learning machine for subjectivity detection</title>
		<author>
			<persName><forename type="first">I</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ragusa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gastaldo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zunino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jfranklin.2017.06.007</idno>
	</analytic>
	<monogr>
		<title level="j">J. Frankl. Inst</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep learning-based document modeling for personality detection from text</title>
		<author>
			<persName><forename type="first">N</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gelbukh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intell. Syst</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="74" to="79" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A deeper look into sarcastic tweets using deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vij</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the COLING</title>
		<meeting>the COLING</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1601" to="1612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Targeted aspect-based sentiment analysis via embedding commonsense knowledge into an attentive LSTM</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI</title>
		<meeting>the AAAI</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Attention-based LSTM for aspect-level sentiment classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on EMNLP</title>
		<meeting>the Conference on EMNLP</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="606" to="615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.01100</idno>
		<title level="m">Effective LSTMs for target-dependent sentiment classification</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Aspect level sentiment classification with deep memory network</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EMNLP</title>
		<meeting>the EMNLP</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="214" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Opinion word expansion and target extraction through double propagation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="27" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Aspect extraction for opinion mining with a deep convolutional neural network</title>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gelbukh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Based Syst</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="42" to="49" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Aspect ranking: identifying important product aspects from online consumer reviews</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Forty-Ninth ACL: Human Language Technologies</title>
		<meeting>the Forty-Ninth ACL: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1496" to="1505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Coupled multi-layer attentions for co-extraction of aspect and opinion terms</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI</title>
		<meeting>the AAAI</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3316" to="3322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Survey on aspect-level sentiment analysis</title>
		<author>
			<persName><forename type="first">K</forename><surname>Schouten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Frasincar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="813" to="830" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multi-aspect opinion polling from textual reviews</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Tsou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth ACM Conference on Information and Knowledge Management</title>
		<meeting>the Eighteenth ACM Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1799" to="1802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Sentiment analysis using support vector machines with diverse information sources</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mullen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Collier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EMNLP</title>
		<meeting>the EMNLP</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="412" to="418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Nrc-canada-2014: detecting aspects and sentiment in customer reviews</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kiritchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohammad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Workshop on Semantic Evaluation (SemEval)</title>
		<meeting>the Eighth International Workshop on Semantic Evaluation (SemEval)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="437" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Bilingual word embeddings for phrase-based machine translation</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EMNLP</title>
		<meeting>the EMNLP</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1393" to="1398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep learning for chinese word segmentation and pos tagging</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EMNLP</title>
		<meeting>the EMNLP</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="647" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Improve chinese word embeddings by exploiting internal structure</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL-HLT</title>
		<meeting>the NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1041" to="1050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Radical embedding: delving deeper to chinese radicals</title>
		<author>
			<persName><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL2</title>
		<meeting>the ACL2</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="594" to="598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Joint learning of character and word embeddings</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-B</forename><surname>Luan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IJCAI</title>
		<meeting>the IJCAI</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1236" to="1242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Radical-enhanced chinese character embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICNIP</title>
		<meeting>the ICNIP</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="279" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Component-enhanced chinese character embeddings</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EMNLP</title>
		<meeting>the EMNLP</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="829" to="834" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Multi-granularity chinese word embedding</title>
		<author>
			<persName><forename type="first">R</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EMNLP</title>
		<meeting>the EMNLP</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="981" to="986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Radical-based hierarchical embeddings for chinese sentiment analysis at sentence level</title>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the FLAIRS</title>
		<meeting>the FLAIRS</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="347" to="352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<title level="m">Neural machine translation by jointly learning to align and translate</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<title level="m">Efficient estimation of word representations in vector space</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">HHMM-based chinese lexical analyzer ICTCLAS</title>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second SIGHAN Workshop on Chinese Language Processing</title>
		<meeting>the Second SIGHAN Workshop on Chinese Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="184" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Sentence compression for aspect-based sentiment analysis</title>
		<author>
			<persName><forename type="first">W</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio Speech Lang. Process</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2111" to="2124" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Creating a fine-grained corpus for chinese sentiment analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intell. Syst</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="36" to="43" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Manandhar</surname></persName>
		</author>
		<title level="m">Semeval-2014 task 4: aspect based sentiment analysis</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="27" to="35" />
		</imprint>
	</monogr>
	<note>Proceedings of the SemEval</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
