<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Learning for Human Part Discovery in Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Gabriel</forename><forename type="middle">L</forename><surname>Oliveira</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Abhinav</forename><surname>Valada</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Claas</forename><surname>Bollen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Wolfram</forename><surname>Burgard</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
						</author>
						<title level="a" type="main">Deep Learning for Human Part Discovery in Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1A5AE39236FF5A95ACEC8BF92E4A2EEF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T10:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper addresses the problem of human body part segmentation in conventional RGB images, which has several applications in robotics, such as learning from demonstration and human-robot handovers. The proposed solution is based on Convolutional Neural Networks (CNNs). We present a network architecture that assigns each pixel to one of a predefined set of human body part classes, such as head, torso, arms, legs. After initializing weights with a very deep convolutional network for image classification, the network can be trained end-to-end and yields precise class predictions at the original input resolution. Our architecture particularly improves on over-fitting issues in the up-convolutional part of the network. Relying only on RGB rather than RGB-D images also allows us to apply the approach outdoors. The network achieves state-of-the-art performance on the PASCAL Parts dataset. Moreover, we introduce two new part segmentation datasets, the Freiburg sitting people dataset and the Freiburg people in disaster dataset. We also present results obtained with a ground robot and an unmanned aerial vehicle.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Convolutional Neural Networks (CNNs) have recently achieved unprecedented results in multiple visual perception tasks, such as image classification <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b23">[24]</ref> and object detection <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. CNNs have the ability to learn effective hierarchical feature representations that characterize the typical variations observed in visual data, which makes them very well-suited for all visual classification tasks. Feature descriptors extracted from CNNs can be transferred also to related tasks. The features are generic and work well even with simple classifiers <ref type="bibr" target="#b24">[25]</ref>.</p><p>In this paper, we are not just interested in predicting a single class label per image, but in predicting a highresolution semantic segmentation output, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. Straightforward pixel-wise classification is suboptimal for two reasons: first, it runs in a dilemma between localization accuracy and using large receptive fields. Second, standard implementations of pixel-wise classification are inefficient computationally. Therefore, we build upon very recent work on so-called up-convolutional networks <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b15">[16]</ref>. In contrast to usual classification CNNs, which contract the high-resolution input to a low-resolution output, these networks can take an abstract, low-resolution input and predict a high-resolution output, such as a full-size image <ref type="bibr" target="#b3">[4]</ref>. In Long et al. <ref type="bibr" target="#b15">[16]</ref>, an up-convolutional network was attached to a classification network, which resolves the above-mentioned dilemma: the contractive network part includes large receptive fields, while the up-convolutional part provides high localization accuracy.</p><p>All authors are with the Department of Computer Science at the University of Freiburg, 79110 Freiburg, Germany. This work has partly been supported by the European Commission under ERC-StG-PE7-279401-VideoLearn, ERC-AG-PE7-267686-LIFENAV, and FP7-610603-EUROPA2. In this paper, we technically refine the architecture of Long et al. and apply it to human body part segmentation, where we focus especially on the usability in a robotics context. Apart from architectural changes, we identify data augmentation strategies that substantially increase performance. For robotics, human body part segmentation can be a very valuable tool, especially when it can be applied both indoors and outdoors. For persons who cannot move their upper body, some of the most basic actions such as drinking water is rendered impossible without assistance. Robots could identify human body parts, such as hands, and interact with them to perform some of these tasks. Other applications such as learning from demonstration and human robot handovers can also benefit from accurate human part segmentation. For a learning-from-demonstration task, one could take advantage of the high level description of human parts. Each part could be used as an explicit mapping between the human and joints of the robot for learning control actions. Tasks such as humanrobot handovers could also benefit. A robot that needs to hand a tool to its human counterpart must be able to detect where the hands are to perform the task.</p><p>Human body part segmentation has been considered a very challenging task in computer vision due to the wide variability of the body parts' appearance. There is large variation due to pose and viewpoint, self-occlusion, and clothing. Good results have been achieved in the past in conjunction with depth sensors <ref type="bibr" target="#b21">[22]</ref>. We show that CNNs can handle this variation very well even with regular RGB cameras, which can be used also outdoors. The proposed network architecture yields correct body part labels and also localizes them precisely. We outperform the baseline by Long et al. <ref type="bibr" target="#b15">[16]</ref> by a large margin on the standard PASCAL parts dataset.</p><p>To evaluate the approach directly in a robotics setting, we introduce two new datasets for human body part segmentation: Freiburg Sitting People and Freiburg People in Disaster. They provide high resolution data for experiments on ground and aerial robot segmentation applications.</p><p>The paper is organized as follows. We first discuss related work in Section II. In Section III, we present our methodology for human part segmentation including the proposed architecture. Experimental results are described in Section IV. Ongoing work and possible future research directions are discussed in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In the context of semantic segmentation, there are several approaches that encode segmentation relations using Conditional Random Fields (CRFs) <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b17">[18]</ref>- <ref type="bibr" target="#b19">[20]</ref>. Plath et al. <ref type="bibr" target="#b19">[20]</ref> present an approach that couples local image features with a CRF and an image classification approach to combine global image classification with local segmentation. Another branch of CRFs called Hierarchical Conditional Random Fields (HCRF) has been introduced by Boix et al. <ref type="bibr" target="#b0">[1]</ref>. They propose a technique called harmony potential to overcome the problem of classical HCRFs, that they do not allow multiple classes to be assigned to a single region. Maire et al. <ref type="bibr" target="#b18">[19]</ref> use an alternative people detection and segmentation approach, in which they merge the outputs of a top-down part detector in a generalized eigen problem, producing pixel groupings. Lucchi et al. <ref type="bibr" target="#b17">[18]</ref> present an analysis of the importance of spatial and global constraints in CRFs when such features have already extracted information from the whole image.</p><p>For semantic segmentation, it is also popular to make use of pre or post-processing methods, such as superpixels <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b10">[11]</ref> and region proposals <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. Farabet et al. <ref type="bibr" target="#b5">[6]</ref> classify superpixels using a CNN. Classification results are combined to obtain pixel-wise labeling. Gupta et al. <ref type="bibr" target="#b8">[9]</ref> sample region proposals for detection and semantic segmentation. Hariharan et al. <ref type="bibr" target="#b9">[10]</ref> introduce an approach that makes use of region proposals for detection and coarse segmentation. They use CNN features to describe the proposals and Support Vector Machines (SVM) to classify them. The results produced from the SVM are coarse masks and in order to improve it, a superpixel classification method is used to refine the initial coarse prediction. Their more recent hypercolumn representation makes use of an additional description of each pixel in the network <ref type="bibr" target="#b10">[11]</ref>. A similar approach was applied to segmentation in a robotics context by Liu et al. <ref type="bibr" target="#b14">[15]</ref>. All these approaches are based on CNN features, but due to the preprocessing, the task of semantic segmentation cannot be trained end-to-end, but requires some engineering for CNNs to be applicable.</p><p>In contrast, the so-called fully convolutional network (FCN) developed by Long et al. <ref type="bibr" target="#b15">[16]</ref> allows training the network end-to-end for the semantic segmentation task. This more elegant approach also led to better performance and provides the state-of-the-art performance in semantic segmentation. The approach replaces the fully connected layers of a deep classification network, e.g. VGG <ref type="bibr" target="#b23">[24]</ref>, by convolution layers that produce coarse score maps. A successive up-convolutional network allows them to increase the resolution of these score maps. There have been some recent extensions of Long et al. <ref type="bibr" target="#b15">[16]</ref>. Chen et al. <ref type="bibr" target="#b1">[2]</ref> use a fully connected CRF to refine the segmentation maps obtained from <ref type="bibr" target="#b15">[16]</ref>. Ronneberger et al. <ref type="bibr" target="#b20">[21]</ref> applied the approach to cell segmentation in the biomedical context and proposed several technical improvements that allow training from few images and predicting higher resolution outputs. None of these approaches has been applied to human body part segmentation.</p><p>Literature includes several works on human or animal part segmentation and person keypoint prediction <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>. Zhang et al. <ref type="bibr" target="#b27">[28]</ref> perform part detection based on region proposals that are classified using a CNN. The approach was demonstrated on a bird part segmentation dataset. Tompson et al. <ref type="bibr" target="#b25">[26]</ref> developed an approach that learns an end-to-end human keypoint detector for pose estimation using a CNN. Zhang et al. <ref type="bibr" target="#b28">[29]</ref> use poselets for part discovery and calculate features for each region using a CNN. Jain et al. <ref type="bibr" target="#b11">[12]</ref> present a sliding window approach for part localization with CNNs. They employ a CNN at each position of the window to detect human body parts. This requires thousands of CNN evaluations and considering that the time for each evaluation is not negligible, the method yields long run times. Simon et al. <ref type="bibr" target="#b22">[23]</ref> introduced a CNN approach called part detector discovery, which detects and localizes bird parts without training on the specific dataset. The method is based on analyzing the gradient maps of the network and finding the spatial regions related to the annotated parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY A. Problem Definition</head><p>Semantic segmentation associates to each pixel of an input image exactly one out of N cl pre-defined class labels. In this paper, the class labels correspond to human body parts at two different granularity levels. In a coarser task, we consider four labels (head, torso, arms, legs). In the finer task, we have 14 labels and distinguish also between the left and right side of the person (head, torso, upper right arm, lower right arm, right hand, upper left arm, lower left arm, left hand, upper right leg, lower right leg, right foot, upper left leg, lower left leg and left foot.</p><p>We approach the problem with a CNN that is trained endto-end to predict the class labels. Training minimizes the usual cross-entropy (softmax) loss. The softmax function converts a score a K for class K into a posterior class probability P K ∈ [0, 1]:</p><formula xml:id="formula_0">P K = exp(a K ) N cl l=1 exp(a l )<label>(1)</label></formula><p>At test time, the softmax is replaced by the argmax function to yield a single class label per pixel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Architecture</head><p>The architecture is based on the network from Long et al. <ref type="bibr" target="#b15">[16]</ref>, where we replaced their up-convolutional part with our own refinement architecture. The whole network architecture is shown in Fig. <ref type="figure" target="#fig_1">2</ref>. Like in Long et al., the parameters of the contracting part of the network is initialized with the parameters of the VGG classification network <ref type="bibr" target="#b23">[24]</ref>.</p><p>The proposed refinement architecture is composed of multiple layers, where each layer combines the upsampled output of its previous layer with the pooled features of the corresponding layer of the contracting network part. The first provides the preliminary class scores at the coarse resolution, whereas the second contributes information for refining the resolution. The combination of both is detailed in Fig. <ref type="figure" target="#fig_2">3</ref>. The coarse score map is fed into an up-convolutional layer, i.e., it is upsampled by a factor 2 via bilinear interpolation followed by a convolution. We use a ReLU activation function after each up-convolutional operation to better deal with the vanishing gradient problem. The feature map from the contracting network part is fed into a convolutional layer followed by dropout to improve the robustness to over-fitting. The effect of applying dropout to all refinement layers is analyzed in Section IV. Finally, the output of both streams are summed element-wise to yield the output of the refinement layer. This output is again the input for the next refinement layer. Each layer increase the resolution of the segmentation by a factor 2.</p><p>With this refinement architecture we manage to obtain a high quality output at the resolution of the input image. This is in contrast to Long et al. <ref type="bibr" target="#b15">[16]</ref>, who stopped their refinement after three layers, because they did not observe any improvement afterwards. A full description of the architecture is presented at Table <ref type="table" target="#tab_0">I</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Feature Map Dropout</head><p>Another attribute of our proposed approach is to make dropout more robust. To this end, we implemented a new feature map dropout. We expand the random dropout to the entire feature map, which is based on the Spatial Dropout method <ref type="bibr" target="#b25">[26]</ref>. One singular characteristic of human body part segmentation is strong spatial correlation, resulting in features that are likely correlated across the map. Hence, dropout must also be correlated. Feature map dropout performs a Bernoulli Given a network with L hidden layers and l ∈ {1, ..., L}. Let z l be the vector of inputs into layer l and let y l denote the vector of outputs from layer l. W l and b l are the weights and biases at layer l. * denote the element-wise product and r (l) is the vector of independent Bernoulli random variables that has probability p of being 1. Feature Map Dropout is then expressed as</p><formula xml:id="formula_1">r (l) z ∼ Bernoulli(p) ∼ y (l) = r (l) * y (l) z (l+1) i = w (l+1) i ∼ y (l) + b (l+1) i y (l+1) i = f (z (l+1) i ) The variable ∼ y (l)</formula><p>is called thinned vector of outputs. This sets apart the feature map dropout from the standard dropout. The resulting thinner network</p><formula xml:id="formula_2">∼ y (l)</formula><p>has the entire feature maps zeroed. For instance, in a convolution layer of size <ref type="bibr" target="#b0">(1,</ref><ref type="bibr">64,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b19">20)</ref>, and a dropout of 0.5, approximately 32 of the 64 feature channels will be zeroed after the input passes the dropout layer.  </p><formula xml:id="formula_3">- - - 300 × 300 × 3 conv1_1 3 × 3 1 100 498 × 498 × 64 conv1_2 3 × 3 1 1 498 × 498 × 64 pool1 2 × 2 2 0 249 × 249 × 64 conv2_1 3 × 3 1 1 249 × 249 × 128 conv2_2 3 × 3 1 1 249 × 249 × 128 pool2 2 × 2 2 0 125 × 125 × 128 conv3_1 3 × 3 1 1 125 × 125 × 256 conv3_2 3 × 3 1 1 125 × 125 × 256 conv3_3 3 × 3 1 1 125 × 125 × 256 pool3 2 × 2 2 0 63 × 63 × 256 conv4_1 3 × 3 1 1 63 × 63 × 512 conv4_2 3 × 3 1 1 63 × 63 × 512 conv4_3 3 × 3 1 1 63 × 63 × 512 pool4 2 × 2 2 0 32 × 32 × 512 conv5_1 3 × 3 1 1 32 × 32 × 512 conv5_2 3 × 3 1 1 32 × 32 × 512 conv5_3 3 × 3 1 1 32 × 32 × 512 pool5 2 × 2 2 0 16 × 16 × 512 fc6-conv 7 × 7 1 0 10 × 10 × 4096 fc7-conv 1 × 1 1 0 10 × 10 × 4096 Up-conv1 4 × 4 2 0 22 × 22 × N cl Up-conv2 4 × 4 2 0 46 × 46 × N cl Up-conv3 4 × 4 2 0 94 × 94 × N cl Up-conv4 4 × 4 2 0 190 × 190 × N cl Up-conv5 4 × 4 2 0 382 × 382 × N cl output - - - 300 × 300 × N cl</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Data Augmentation</head><p>We augment the training data by randomly mirroring and cropping the images. Inspired by the data augmentation suggested in Dosovitskiy et al. <ref type="bibr" target="#b4">[5]</ref>, we additionally apply geometry and color transformations to increase the amount of training data and the robustness of our network to overfitting. In particular, we implemented the following set of transformations:</p><p>• Scaling: Scale the image by a factor between 0.7 and 1.4; • Rotation: Rotate the image by an angle of up to 30 degrees; • Color: Add a value between -0.1 and 0.1 to the hue channel of the HSV representation. Unlike in the setting of Dosovitskiy et al. <ref type="bibr" target="#b4">[5]</ref>, where rotation and scaling had the lowest impact among all transformations, these spatial augmentation strategies are very important for the task and data considered here, as shown in Section IV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Network Training</head><p>Training is performed in a multi-stage process in order to save time. We initialize the contracting part of the network with the 16 layer version of the VGG architecture <ref type="bibr" target="#b23">[24]</ref>, which is the same as used by Long et al. <ref type="bibr" target="#b15">[16]</ref>. The base network has small convolution filters (3 × 3) and 1 pixel stride. The network also has 5 max-pooling layers with 2 × 2 pixel windows with stride 2. We also considered training the whole network from scratch without initializing it with weights from the VGG network. However, this was inconvenient in terms of time, as the base network approximately takes four weeks to train on a multi-GPU cluster.</p><p>The overall network is then trained by backpropagation using Stochastic Gradient Descent (SGD) with momentum. Each minibatch consists of just one image. The learning rate and momentum are fixed to 1e -10 and 0.99, respectively. We train the refinement layer by layer, which takes two days per refinement layer. Thus, the overall training starting from the pre-trained VGG network took 10 days on a single GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>We evaluated the performance of our network on the PASCAL Parts dataset, a new Freiburg Sitting People dataset, and a new Freiburg People in Disaster dataset. On all three datasets we report quantitative results and compare to results obtained with the state-of-the-art FCN baseline <ref type="bibr" target="#b15">[16]</ref>. We finetuned the FCN for each dataset on the same training data that was used for training our network. Moreover, we conducted experiments in a direct robotics context with a ground robot and an unmanned aerial vehicle. The implementation was based on the publicly available Caffe <ref type="bibr" target="#b12">[13]</ref> deep learning toolbox, and all experiments were carried out with a system containing an NVIDIA Titan X GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. PASCAL Parts dataset</head><p>The PASCAL Parts dataset <ref type="bibr" target="#b2">[3]</ref> includes annotations for 20 PASCAL super classes and part annotations for each of them. We focused on the person subset of this dataset, which consists of 3539 images. The annotations even include eyes and ears, which may not seem relevant in a robotics context for now. Therefore we merged labels to two granularity levels, one with just 4 body parts and one with 14 body parts; see Section III-A. Our experiments were based on a fairly recent release, so there are not many works reporting part segmentation results. To the best of our knowledge, the only works reported so far are <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b26">[27]</ref>, though none of them have reported results on the person category. Therefore, we present the first quantitative results on person part segmentation for the PASCAL Parts dataset.</p><p>As metrics, we chose pixel accuracy and intersection over union. Let n ij be the number of pixels of class i predicted to belong to class j, where t i = j n ij be the total number of pixels of class i. The pixel accuracy Acc = i n ii / i t i takes into account also the prediction of background pixels. Background prediction is important to avoid false positives.</p><p>The downside of pixel accuracy as a sole measure, however, is the dominance of the background in the metric. More than three quarters of the images are background. Therefore, along with pixel accuracy, we also report the intersection over union (IOU), which is a popular metric for computer vision datasets. It is defined as IOU = (1/N ) i n ii /(t i + j n ji -n ii ). Unlike pixel accuracy, IOU does not take the background detection into account and solely measures the semantic segmentation of the parts. However, it does penalize false positive pixel assignments.</p><p>1) Coarse body parts: We first predicted the coarse segmentation with four body part classes. We randomly divided the dataset into 70% training and 30% testing. Table <ref type="table" target="#tab_1">II</ref> shows the results. There is a 5% percentage points improvement over the state of the art in both metrics. Additionally, we also perform experiments without the feature map dropout at the refinement part of the network. Table <ref type="table" target="#tab_1">II</ref> presents our results for the network without feature map dropout at the expansive part of the network and when dropout is included. The addition of the dropout layer brings a considerable gain, in terms of better mean pixel accuracy and IOU. This result confirm that a spatial correlated dropout can benefit from the strong spatial correlation of human body parts. Based on the obtained results all the following experiments will report the proposed approach with the inclusion of the feature map dropout. 2) Detailed body parts: When predicting all 14 body parts, we randomly divided the dataset into 80% training and 20% testing. Fig. <ref type="figure">4</ref> shows a set of results obtained by our network. The results are organized column-wise, where each column is an example and the rows correspond to input image, ground truth and results obtained using the FCN of Long et al. The last row constitutes the results using our network. The results of our approach are closer to the ground truth than the FCN baseline. Table III contains the corresponding quantitative numbers. We outperform the FCN baseline <ref type="bibr" target="#b15">[16]</ref> by 1% percentage point in both metrics. The smaller improvement on the more complex task indicates that there was not enough training data to exploit the larger capacity of our network. For the experiments reported so far, we did not make use of any data augmentation. We shall see in the next section that the latter is important, especially for more complex tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Effect of Data Augmentation</head><p>Apart from the usual mirroring and cropping, we applied two types of augmentations to our training data: spatial augmentations and color augmentation; see the detailed Fig. <ref type="figure">4</ref>: Qualitative results on the PASCAL dataset (task with 14 body parts). First row: Input image. Second row: Ground truth. Third row: Result predicted with FCN <ref type="bibr" target="#b15">[16]</ref>. Fourth row: Result predicted by our network. Our approach produces more accurate segmentation masks, not only for single person segmentation but also when there are multiple persons in the image.</p><p>description in Section III-D. Table <ref type="table" target="#tab_3">IV</ref> shows the impact of these types of data augmentation on the result. Table ?? and Table VI summarize the IOU along with the pixel accuracy for the two granularity levels. Clearly, both types of data augmentation improved results significantly. These results emphasize the importance of a solid data augmentation technique when approaching relatively complex tasks with limited training set sizes. The relative improvement of data augmentation was bigger on the more difficult task with 14 classes, which can be attributed to the fact that, a more difficult task requires more training data.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Freiburg Sitting People Part Segmentation Dataset</head><p>To evaluate the proposed part segmentation approach in a robotics task, we created a new dataset 1 that provides high resolution segmentation masks for people in sitting position, specifically people in wheelchairs. Fig. <ref type="figure" target="#fig_4">5a</ref>, presents the input sample image and Fig. <ref type="figure" target="#fig_4">5b</ref> its groundtruth, while Fig. <ref type="figure" target="#fig_4">5c</ref> shows the segmentation prediction. The dataset has 200 images of six different people in multiple viewpoints and in a wide range of orientations. The ground truth annotation contains the 14 body part classes as used for the PASCAL parts dataset. Due to the unavailability of a large amount of data, we chose two different testing scenarios. First we trained our network on the PASCAL parts dataset, and used the full sitting people dataset for testing. Alternatively, we randomly chose two people from the dataset for training (along with the data from PASCAL parts) and the remaining four as the test set. Results are shown in Table <ref type="table" target="#tab_6">VII</ref>. Obviously, the network generalized well to new datasets. The improvement over the FCN baseline was much larger than the difference between the network that had seen sitting people for training and the one that had not. Nonetheless, providing training data that is specific to the task helped improve the performance.</p><p>Another aspect of the network which is of great interest for robotic tasks is time performance. Our network can process a single frame in 229 ms, so providing more than 4 frames 1 http://www2.informatik.uni-freiburg.de/~oliveira/dataset.html per second. Long et al. <ref type="bibr" target="#b15">[16]</ref> provides inference times ranging from 150 to 175 ms. Our approach having a deeper refinement architecture presents a higher computational load. For an output smaller than the 300 × 300 used in our experiments, higher frames can be expected. Our approach yields more details; see the arm detection and the line between torso and lower limbs. This is because the architecture yields the full resolution of the input image also for the output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Microsoft COCO</head><p>Microsoft COCO constitutes a very large dataset for semantic segmentation. However, its focus is on whole objects and part annotations are not provided. Therefore, we cannot report quantitative results. Fig. <ref type="figure" target="#fig_5">6</ref> shows a sample result obtained on this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Real-World Robot Experiments</head><p>In this section we present experimental results performed using real robots. First, we performed experiments with a ground robot and measured how robust the technique is to scale changes. The ground robot used for the experiments is the Obelix robot, Figure <ref type="figure" target="#fig_7">7a</ref>. Obelix is a robot designed for autonomous navigation in pedestrian environments and it is useful to mimic the human perspective for perception tasks.</p><p>In our experiments, we obtained data from a Bumblebee stereo camera. The main goal of the experiments performed with Obelix was to measure the response of the system to    dataset. The dataset consists of images and corresponding segmentation masks for a set of 4 people in an environment that mimics a disaster scenario, with clutter and heavy occlusion around. Figure <ref type="figure" target="#fig_9">10</ref> shows an example with the groundtruth and the results obtained using our approach. As the dataset is rather small, we did not use it for training the network but just for testing. We used the network trained on the PASCAL dataset with 4 body parts. Table <ref type="table" target="#tab_7">VIII</ref> presents the results for this dataset. Our approach performed 28% better than the state of the art FCN. While the FCN performance would be too weak for a robot to rely on, the results obtained with our network and data augmentation can already be useful in practice. V. CONCLUSION AND FUTURE WORK We presented a deep learning methodology for human part segmentation that uses refinements based on a stack of upconvolutional layers. It yielded semantically accurate results and well-localized segmentation maps. We identified augmentation strategies that substantially increased performance of the network. We also demonstrated that adding feature map dropout to each refinement step boosts the overall system performance. In addition, we presented results on the PASCAL Parts, Microsoft COCO datasets and on two new robotics segmentation datasets: Freiburg Sitting People and Freiburg People in Disaster. Our approach advances the state of the art on all the above datasets. To the best of our knowledge our approach also is the first to tackle human part segmentation at this level of granularity (14 parts) with a single RGB camera.</p><p>Future work will include investigating the potential of our architecture for human keypoint prediction. A method that can accurately find body joints will have direct applications in human pose estimation and activity recognition. There are also many aspects of the method that we intend to refine, such as having multiple filters per class in the coarse refinement modules of the network. We also intend to work on simplifying the architecture for real-time part segmentation on smaller hardware. Another future line of research will be performing human part segmentation in videos while exploiting the temporal context.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Input image (left) and the corresponding mask (right) predicted by our network on various standard datasets.</figDesc><graphic coords="1,435.60,236.13,57.53,57.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Proposed architecture. Only convolutional, pooling, and up-convolutional layers are visualized. Up-convolutional layers have size N = N cl . We call the network part up to fc7-conv the contractive network part, whereas the part after fc7-conv is called the expansive network part.</figDesc><graphic coords="3,313.20,244.55,244.79,89.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Description of the first refinement layer. Successive refinement layers have the same architecture, but take different inputs. The upper stream takes the output from the contractive network (fc7) or from the previous refinement layer as input. It applies an up-convolution followed by a ReLU. The lower stream takes high-resolution features from the corresponding layer in the contractive network as input. It applies a convolution followed by dropout (only during training). trial per output feature during training and propagates the dropout value across the entire feature map.Given a network with L hidden layers and l ∈ {1, ..., L}. Let z l be the vector of inputs into layer l and let y l denote the vector of outputs from layer l. W l and b l are the weights and biases at layer l. * denote the element-wise product and r (l) is the vector of independent Bernoulli random variables that has probability p of being 1. Feature Map Dropout is then expressed as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>name kernel size stride pad output size data</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Results on the Freiburg Sitting People dataset.</figDesc><graphic coords="6,54.00,410.14,80.79,80.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Segmentation results on the Microsoft COCO dataset.Our approach yields more details; see the arm detection and the line between torso and lower limbs. This is because the architecture yields the full resolution of the input image also for the output.</figDesc><graphic coords="6,222.54,410.15,80.78,80.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(a) Obelix ground robot. (b) AR.DRONE 2.0 aerial platform.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Robotics platforms used in our tests.</figDesc><graphic coords="7,54.00,272.70,58.75,58.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 :Fig. 9 :</head><label>89</label><figDesc>Fig.8: Qualitative results of the range experiment with the Obelix robot. The lower resolution at one point does not allow detection of small body parts. However, the larger parts, such as the torso and head, are still detected correctly even at 6m distance.</figDesc><graphic coords="7,54.00,436.02,58.75,58.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 :</head><label>10</label><figDesc>Fig. 10: Prediction of our network for the Freiburg People in Disaster Dataset.</figDesc><graphic coords="7,240.72,354.36,58.75,58.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Our architecture in more detail. The Up-conv layers refer to each refinement step. For brevity reasons ReLUs, dropout and some layers from the up-convolution step are omitted from the table.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>Results on PASCAL dataset with 4 body parts. Table also includes the addition of dropout.</figDesc><table><row><cell>Method</cell><cell>Accuracy</cell><cell>IOU</cell></row><row><cell>FCN [16]</cell><cell>71.30</cell><cell>57.35</cell></row><row><cell>Ours -No dropout</cell><cell>74.60</cell><cell>61.20</cell></row><row><cell>Ours -With dropout</cell><cell>76.58</cell><cell>63.03</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III :</head><label>III</label><figDesc>Results on the PASCAL dataset with 14 body parts.</figDesc><table><row><cell>Method</cell><cell cols="2">Accuracy IOU</cell></row><row><cell>FCN [16]</cell><cell>75.60</cell><cell>53.12</cell></row><row><cell>Ours</cell><cell>77.00</cell><cell>54.18</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV :</head><label>IV</label><figDesc>Augmentation results Accuracy and IOU on the PASCAL dataset with 4 body parts.</figDesc><table><row><cell></cell><cell>Acc.</cell><cell>IOU</cell></row><row><cell>Method</cell><cell></cell><cell>Head Torso Arms Legs All</cell></row><row><cell>FCN [16]</cell><cell>71.30</cell><cell>70.74 60.62 48.44 50.38 57.35</cell></row><row><cell>Ours</cell><cell>76.58</cell><cell>75.08 64.81 55.61 56.72 63.03</cell></row><row><cell>Ours (Spatial)</cell><cell>82.18</cell><cell>80.49 74.39 67.17 70.39 73.00</cell></row><row><cell cols="3">Ours (Spatial + Color) 85.51 83.24 79.41 73.73 76.52 78.23</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V :</head><label>V</label><figDesc>Augmentation results (IOU) on the PASCAL dataset with 14 body parts.</figDesc><table><row><cell>Method</cell><cell>Head Torso L U</cell><cell>L LW</cell><cell>L</cell><cell>R U</cell><cell>R LW</cell><cell>R</cell><cell>R U</cell><cell>R LW</cell><cell>R</cell><cell>L U</cell><cell>L LW</cell><cell>L</cell><cell>Mean</cell></row><row><cell></cell><cell>arm</cell><cell>arm</cell><cell>hand</cell><cell>hand</cell><cell>arm</cell><cell>hand</cell><cell>leg</cell><cell>leg</cell><cell>foot</cell><cell>leg</cell><cell>leg</cell><cell>foot</cell><cell></cell></row><row><cell>FCN [16]</cell><cell cols="2">74.0 66.2 56.6 46.0</cell><cell cols="3">34.1 58.9 44.1</cell><cell cols="3">31.0 49.3 44.5</cell><cell cols="3">40.8 48.5 47.6</cell><cell cols="2">41.2 53.1</cell></row><row><cell>Ours (Spatial)</cell><cell cols="2">81.8 78.0 69.5 63.1</cell><cell cols="3">59.0 71.2 63.0</cell><cell cols="3">58.7 65.4 60.6</cell><cell cols="3">52.0 67.9 60.3</cell><cell cols="2">50.0 66.9</cell></row><row><cell cols="3">Ours (Spatial+Color) 84.0 81.5 74.1 68.0</cell><cell cols="3">64.0 75.4 67.4</cell><cell cols="3">61.9 72.4 67.1</cell><cell cols="3">56.9 73.0 66.1</cell><cell cols="2">57.7 71.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">R = right, L = left, U = upper, LW = lower.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VI :</head><label>VI</label><figDesc>Augmentation summary on the PASCAL dataset with 14 body parts.</figDesc><table><row><cell>Method</cell><cell cols="2">Accuracy IOU</cell></row><row><cell>FCN [16]</cell><cell>75.60</cell><cell>53.12</cell></row><row><cell>Ours</cell><cell>77.00</cell><cell>54.18</cell></row><row><cell>Ours (Spatial)</cell><cell>84.19</cell><cell>66.93</cell></row><row><cell>Ours (Spatial + Color)</cell><cell>88.20</cell><cell>71.71</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VII :</head><label>VII</label><figDesc>Results with and without training on the Freiburg people dataset.</figDesc><table><row><cell>Method</cell><cell cols="2">Accuracy IOU</cell></row><row><cell>FCN [16]</cell><cell>59.69</cell><cell>43.17</cell></row><row><cell>Ours (Trained on PASCAL)</cell><cell>78.04</cell><cell>59.84</cell></row><row><cell>Ours (Training with 2 people, Test-</cell><cell>81.78</cell><cell>64.10</cell></row><row><cell>ing with 4)</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VIII :</head><label>VIII</label><figDesc>Results for Freiburg People in Disaster dataset.</figDesc><table><row><cell>Method</cell><cell>Head</cell><cell>Torso</cell><cell>Arms</cell><cell>Legs</cell><cell>IOU</cell></row><row><cell>FCN [16]</cell><cell>52.71</cell><cell>62.49</cell><cell>35.04</cell><cell>43.25</cell><cell>43.20</cell></row><row><cell>Ours</cell><cell cols="5">80.56 79.45 63.93 64.91 71.99</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Boix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Josep</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joost</forename><surname>Gonfaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">D</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordi</forename><surname>Serrat</surname></persName>
		</author>
		<author>
			<persName><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Harmony potentials. IJCV</title>
		<imprint>
			<biblScope unit="page" from="83" to="102" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected CRFs</title>
		<author>
			<persName><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Detect what you can: Detecting and representing objects using holistic models and body parts</title>
		<author>
			<persName><forename type="first">Xianjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaobai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning to generate chairs with convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jost</forename><surname>Tobias Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jost</forename><surname>Tobias Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">A</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="766" to="774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning hierarchical features for scene labeling</title>
		<author>
			<persName><forename type="first">Clement</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Camille</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1915" to="1929" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R-Cnn</forename><surname>Fast</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.08083</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning rich features from RGB-D images for object detection and segmentation</title>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><forename type="middle">Andrés</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="345" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Simultaneous detection and segmentation</title>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><forename type="middle">Andrés</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning human pose estimation features with convolutional networks</title>
		<author>
			<persName><forename type="first">Ajrun</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><surname>Caffe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Layered interpretation of street view images</title>
		<author>
			<persName><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuoxin</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Srikumar Ramalingam, and Oncel Tuzel</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>Robotics: Science and Systems</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2015-11">nov 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Parsing semantic parts of cars using graphical models and segment appearance consistency</title>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaochen</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Are spatial and global constraints really necessary for segmentation?</title>
		<author>
			<persName><forename type="first">Aurelien</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Boix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Object detection and segmentation from joint embedding of parts and pixels</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-class image segmentation using conditional random fields and global classification</title>
		<author>
			<persName><forename type="first">Nils</forename><surname>Plath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Toussaint</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shinichi</forename><surname>Nakajima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="817" to="824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Realtime human pose recognition in parts from a single depth image</title>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mat</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toby</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Finocchio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Kipman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Part detector discovery in deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Marcel</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Rodner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joachim</forename><surname>Denzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="162" to="177" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Place recognition with convnet landmarks: Viewpoint-robust, condition-robust, trainingfree</title>
		<author>
			<persName><forename type="first">Niko</forename><surname>Suenderhauf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sareh</forename><surname>Shirazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Jacobson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feras</forename><surname>Dayoub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Pepperell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Upcroft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Milford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arjun</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Semantic part segmentation with deep learning</title>
		<author>
			<persName><forename type="first">Stavros</forename><surname>Tsogkas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno>CoRR, abs/1505.02438</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Partbased R-CNNs for fine-grained category detection</title>
		<author>
			<persName><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">PANDA: pose aligned networks for deep attribute modeling</title>
		<author>
			<persName><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc'aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lubomir</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1637" to="1644" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
