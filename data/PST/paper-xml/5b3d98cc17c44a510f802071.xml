<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GANomaly: Semi-supervised Anomaly Detection via Adversarial Training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Samet</forename><surname>Akcay</surname></persName>
							<email>samet.akcay@durham.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Durham University</orgName>
								<address>
									<settlement>Durham</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Amir</forename><surname>Atapour-Abarghouei</surname></persName>
							<email>amir.atapour-abarghouei@durham.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Durham University</orgName>
								<address>
									<settlement>Durham</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Toby</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
							<email>toby.breckon@durham.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Durham University</orgName>
								<address>
									<settlement>Durham</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Engineering</orgName>
								<orgName type="institution">Durham University</orgName>
								<address>
									<settlement>Durham</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GANomaly: Semi-supervised Anomaly Detection via Adversarial Training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">256603B57FD06C3DB0F37E0FFC7B9E6B</idno>
					<idno type="DOI">10.1007/978-3-030-20893-6_39</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Anomaly detection</term>
					<term>Semi-supervised learning</term>
					<term>Generative Adversarial Networks</term>
					<term>X-ray security imagery</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Anomaly detection is a classical problem in computer vision, namely the determination of the normal from the abnormal when datasets are highly biased towards one class (normal) due to the insufficient sample size of the other class (abnormal). While this can be addressed as a supervised learning problem, a significantly more challenging problem is that of detecting the unknown/unseen anomaly case that takes us instead into the space of a one-class, semi-supervised learning paradigm. We introduce such a novel anomaly detection model, by using a conditional generative adversarial network that jointly learns the generation of high-dimensional image space and the inference of latent space. Employing encoder-decoder-encoder sub-networks in the generator network enables the model to map the input image to a lower dimension vector, which is then used to reconstruct the generated output image. The use of the additional encoder network maps this generated image to its latent representation. Minimizing the distance between these images and the latent vectors during training aids in learning the data distribution for the normal samples. As a result, a larger distance metric from this learned data distribution at inference time is indicative of an outlier from that distribution-an anomaly. Experimentation over several benchmark datasets, from varying domains, shows the model efficacy and superiority over previous state-of-the-art approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Despite yielding encouraging performance over various computer vision tasks, supervised approaches heavily depend on large, labeled datasets. In many of the real world problems, however, samples from the more unusual classes of interest are of insufficient sizes to be effectively modeled. Instead, the task of anomaly detection is to be able to identify such cases, by training only on samples considered to be normal and then identifying these unusual, insufficiently available samples (abnormal ) that differ from the learned sample distribution Fig. <ref type="figure">1</ref>. Overview of our anomaly detection approach within the context of an X-ray security screening problem. Our model is trained on normal samples (a), and tested on normal and abnormal samples (b). Anomalies are detected when the output of the model is greater than a certain threshold A(x) &gt; φ. of normality. For example a tangible application, that is considered here within our evaluation, is that of X-ray screening for aviation or border security-where anomalous items posing a security threat are not commonly encountered, exemplary data of such can be difficult to obtain in any quantity, and the nature of any anomaly posing a potential threat may evolve due to a range of external factors. However, within this challenging context, human security operators are still competent and adaptable anomaly detectors against new and emerging anomalous threat signatures.</p><p>As illustrated in Fig. <ref type="figure">1</ref>, a formal problem definition of the anomaly detection task is as follows: given a dataset D containing a large number of normal samples X for training, and relatively few abnormal examples X for the test, a model f is optimized over its parameters θ. f learns the data distribution p X of the normal samples during training while identifying abnormal samples as outliers during testing by outputting an anomaly score A(x), where x is a given test example. A Larger A(x) indicates possible abnormalities within the test image since f learns to minimize the output score during training. A(x) is general in that it can detect unseen anomalies as being non-conforming to p X .</p><p>There is a large volume of studies proposing anomaly detection models within various application domains <ref type="bibr" target="#b0">[2]</ref><ref type="bibr" target="#b1">[3]</ref><ref type="bibr" target="#b2">[4]</ref><ref type="bibr" target="#b21">23,</ref><ref type="bibr" target="#b37">39]</ref>. Besides, a considerable amount of work taxonomized the approaches within the literature <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b17">19,</ref><ref type="bibr" target="#b26">28,</ref><ref type="bibr" target="#b27">29,</ref><ref type="bibr" target="#b31">33]</ref>. In parallel to the recent advances in this field, Generative Adversarial Networks (GAN) have emerged as a leading methodology across both unsupervised and semi-supervised problems. Goodfellow et al. <ref type="bibr" target="#b14">[16]</ref> first proposed this approach by co-training a pair networks (generator and discriminator). The former network models high dimensional data from a latent vector to resemble the source data, while the latter distinguishes the modeled (i.e., approximated) and original data samples.</p><p>Several approaches followed this work to improve the training and inference stages <ref type="bibr" target="#b6">[8,</ref><ref type="bibr" target="#b15">17]</ref>. As reviewed in <ref type="bibr" target="#b21">[23]</ref>, adversarial training has also been adopted by recent work within anomaly detection.</p><p>Schlegl et al. <ref type="bibr" target="#b37">[39]</ref> hypothesize that the latent vector of a GAN represents the true distribution of the data and remap to the latent vector by optimizing a pre-trained GAN based on the latent vector. The limitation is the enormous computational complexity of remapping to this latent vector space. In a follow-up study, Zenati et al. <ref type="bibr" target="#b38">[40]</ref> train a BiGAN model <ref type="bibr" target="#b12">[14]</ref>, which maps from image space to latent space jointly, and report statistically and computationally superior results albeit on the simplistic MNIST benchmark dataset <ref type="bibr" target="#b23">[25]</ref>.</p><p>Motivated by <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b37">39,</ref><ref type="bibr" target="#b38">40]</ref>, here we propose a generic anomaly detection architecture comprising an adversarial training framework. In a similar vein to <ref type="bibr" target="#b37">[39]</ref>, we use single color images as the input to our approach drawn only from an example set of normal (non-anomalous) training examples. However, in contrast, our approach does not require two-stage training and is both efficient for model training and later inference (run-time testing). As with <ref type="bibr" target="#b38">[40]</ref>, we also learn image and latent vector spaces jointly. Our key novelty comes from the fact that we employ adversarial autoencoder within an encoder-decoder-encoder pipeline, capturing the training data distribution within both image and latent vector space. An adversarial training architecture such as this, practically based on only normal training data examples, produces superior performance over challenging benchmark problems. The main contributions of this paper are as follows:</p><p>semi-supervised anomaly detection-a novel adversarial autoencoder within an encoder-decoder-encoder pipeline, capturing the training data distribution within both image and latent vector space, yielding superior results to contemporary GAN-based and traditional autoencoder-based approaches. -efficacy-an efficient and novel approach to anomaly detection that yields both statistically and computationally better performance. -reproducibility-simple and effective algorithm such that the results could be reproduced via the code 1 made publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Anomaly detection has long been a question of great interest in a wide range of domains including but not limited to biomedical <ref type="bibr" target="#b37">[39]</ref>, financial <ref type="bibr" target="#b1">[3]</ref> and security such as video surveillance <ref type="bibr" target="#b21">[23]</ref>, network systems <ref type="bibr" target="#b2">[4]</ref> and fraud detection <ref type="bibr" target="#b0">[2]</ref>. Besides, a considerable amount of work has been published to taxonomize the approaches in the literature <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b17">19,</ref><ref type="bibr" target="#b26">28,</ref><ref type="bibr" target="#b27">29,</ref><ref type="bibr" target="#b31">33]</ref>. The narrower scope of the review is primarily focused on reconstruction-based anomaly techniques. The vast majority of the reconstruction-based approaches have been employed to investigate anomalies in video sequences. Sabokrou et al. <ref type="bibr" target="#b35">[37]</ref> investigate the use of Gaussian classifiers on top of autoencoders (global) and nearest 1 The code is available on https://github.com/samet-akcay/ganomaly. neighbor similarity (local) feature descriptors to model non-overlapping video patches. A study by Medel and Savakis <ref type="bibr" target="#b28">[30]</ref> employs convolutional long shortterm memory networks for anomaly detection. Trained on normal samples only, the model predicts the future frame of possible standard example, which distinguishes the abnormality during the inference. In another study on the same task, Hasan et al. <ref type="bibr" target="#b16">[18]</ref> considers a two-stage approach, using local features and fully connected autoencoder first, followed by fully convolutional autoencoder for end-to-end feature extraction and classification. Experiments yield competitive results on anomaly detection benchmarks. To determine the effects of adversarial training in anomaly detection in videos, Dimokranitou <ref type="bibr" target="#b11">[13]</ref> uses adversarial autoencoders, producing a comparable performance on benchmarks.</p><p>More recent attention in the literature has been focused on the provision of adversarial training. The seminal work of Ravanbakhsh et al. <ref type="bibr" target="#b33">[35]</ref> utilizes image to image translation <ref type="bibr" target="#b19">[21]</ref> to examine the abnormality detection problem in crowded scenes and achieves state-of-the-art on the benchmarks. The approach is to train two conditional GANs. The first generator produces optical flow from frames, while the second generates frames from optical-flow.</p><p>The generalisability of the approach mentioned above is problematic since in many cases datasets do not have temporal features. One of the most influential accounts of anomaly detection using adversarial training comes from Schlegl et al. <ref type="bibr" target="#b37">[39]</ref>. The authors hypothesize that the latent vector of the GAN represents the distribution of the data. However, mapping to the vector space of the GAN is not straightforward. To achieve this, the authors first train a generator and discriminator using only normal images. In the next stage, they utilize the pretrained generator and discriminator by freezing the weights and remap to the latent vector by optimizing the GAN based on the z vector. During inference, the model pinpoints an anomaly by outputting a high anomaly score, reporting significant improvement over the previous work. The main limitation of this work is its computational complexity since the model employs a two-stage approach, and remapping the latent vector is extremely expensive. In a follow-up study, Zenati et al. <ref type="bibr" target="#b38">[40]</ref> investigate the use of BiGAN <ref type="bibr" target="#b12">[14]</ref> in an anomaly detection task, examining joint training to map from image space to latent space simultaneously, and vice-versa. Training the model via <ref type="bibr" target="#b37">[39]</ref> yields superior results on the MNIST <ref type="bibr" target="#b23">[25]</ref> dataset.</p><p>Overall prior work strongly supports the hypothesis that the use of autoencoders and GAN demonstrate promise in anomaly detection problems <ref type="bibr" target="#b21">[23,</ref><ref type="bibr" target="#b37">39,</ref><ref type="bibr" target="#b38">40]</ref>. Motivated by the idea of GAN with inference studied in <ref type="bibr" target="#b37">[39]</ref> and <ref type="bibr" target="#b38">[40]</ref>, we introduce a conditional adversarial network such that generator comprises encoderdecoder-encoder sub-networks, learning representations in both image and latent vector space jointly, and achieving state-of-the-art performance both statistically and computationally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Approach: GANomaly</head><p>To explain our approach in detail, it is essential to briefly introduce the background of GAN. Generative Adversarial Networks (GAN) are an unsupervised machine learning algorithm that was initially introduced by Goodfellow et al. <ref type="bibr" target="#b14">[16]</ref>. The original primary goal of the work is to generate realistic images. The idea being that two networks (generator and discriminator) compete with each other during training such that the former tries to generate an image, while the latter decides whether the generated image is a real or a fake. The generator is a decoderalike network that learns the distribution of input data from a latent space. The primary objective here is to model high dimensional data that captures the original real data distribution. The discriminator network usually has a classical classification architecture, reading an input image, and determining its validity (i.e., real vs. fake).</p><p>GAN have been intensively investigated recently due to their future potential <ref type="bibr" target="#b10">[12]</ref>. To address training instability issues, several empirical methodologies have been proposed <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b36">38]</ref>. One well-known study that receives attention in the literature is Deep Convolutional GAN (DCGAN) by Radford and Chintala <ref type="bibr" target="#b32">[34]</ref>, who introduce a fully convolutional generative network by removing fully connected layers and using convolutional layers and batch-normalization <ref type="bibr" target="#b18">[20]</ref> throughout the network. The training performance of GAN is improved further via the use of Wasserstein loss <ref type="bibr" target="#b6">[8,</ref><ref type="bibr" target="#b15">17]</ref>.</p><p>Adversarial Auto-Encoders (AAE) consist of two sub-networks, namely an encoder and a decoder. This structure maps the input to latent space and remaps back to input data space, known as reconstruction. Training autoencoders with adversarial setting enable not only better reconstruction but also control over latent space <ref type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b25">27,</ref><ref type="bibr" target="#b29">31]</ref>.</p><p>GAN with Inference are also used within discrimination tasks by exploiting latent space variables <ref type="bibr" target="#b8">[10]</ref>. For instance, the research by <ref type="bibr" target="#b9">[11]</ref> suggests that networks are capable of generating a similar latent representation for related high-dimensional image data. <ref type="bibr">Lipton and Tripathi [26]</ref> also investigate the idea of inverse mapping by introducing a gradient-based approach, mapping images back to the latent space. This has also been explored in <ref type="bibr" target="#b13">[15]</ref> with a specific focus on joint training of generator and inference networks. The former network maps from latent space to high-dimensional image space, while the latter maps from image to latent space. Another study by Donahue et al. <ref type="bibr" target="#b12">[14]</ref> suggests that with the additional use of an encoder network mapping from image space to latent space, a vanilla GAN network is capable of learning inverse mapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Proposed Approach</head><p>Problem Definition. Our objective is to train an unsupervised network that detects anomalies using a dataset that is highly biased towards a particular class -i.e., comprising normal non-anomalous occurrences only for training. The formal definition of this problem is as follows:</p><p>We are given a large training dataset D comprising only M normal images, D = {X 1 , . . . , X M }, and a smaller testing dataset D of N normal and abnormal images, D = {( X1 , y 1 ), . . . , ( XN , y N )}, where y i ∈ [0, 1] denotes the image label. In the practical setting, the training set is significantly larger than the test set such that M N . Given the dataset, our goal is first to model D to learn its manifold, then detect the abnormal samples in D as outliers during the inference stage. The model f learns both the normal data distribution and minimizes the output anomaly score A(x). For a given test image x, a high anomaly score of A(x)) indicates possible anomalies within the image. The evaluation criteria for this is to threshold (φ) the score, where A(x) &gt; φ indicates anomaly.</p><p>Ganomaly Pipeline. Figure <ref type="figure" target="#fig_0">2</ref> illustrates the overview of our approach, which contains two encoders, a decoder, and discriminator networks, employed within three sub-networks.</p><p>First sub-network is a bow tie autoencoder network behaving as the generator part of the model. The generator learns the input data representation and reconstructs the input image via the use of an encoder and a decoder network, respectively. The formal principle of the sub-network is the following: The generator G first reads an input image x, where x ∈ R w×h×c , and forward-passes it to its encoder network G E . With the use of convolutional layers followed by batch-norm and leaky ReLU () activation, respectively, G E downscales x by compressing it to a vector z, where z ∈ R d . z is also known as the bottleneck features of G and hypothesized to have the smallest dimension containing the best representation of x. The decoder part G D of the generator network G adopts the architecture of a DCGAN generator <ref type="bibr" target="#b32">[34]</ref>, using convolutional transpose layers, ReLU () activation and batch-norm together with a tanh layer at the end. This approach upscales the vector z to reconstruct the image x as x. Based on these, the generator network G generates image x via x = G D (z), where z = G E (x).</p><p>The second sub-network is the encoder network E that compresses the image x that is reconstructed by the network G. With different parametrization, it has the same architectural details as G E . E downscales x to find its feature representation ẑ = E(x). The dimension of the vector ẑ is the same as that of z for consistent comparison. This sub-network is one of the unique parts of the proposed approach. Unlike the prior autoencoder-based approaches, in which the minimization of the latent vectors is achieved via the bottleneck features, this sub-network E explicitly learns to minimize the distance with its parametrization. During the test time, moreover, the anomaly detection is performed with this minimization.</p><p>The third sub-network is the discriminator network D whose objective is to classify the input x and the output x as real or fake, respectively. This subnetwork is the standard discriminator network introduced in DCGAN <ref type="bibr" target="#b32">[34]</ref>.</p><p>Having defined our overall multi-network architecture, as depicted in Fig. <ref type="figure" target="#fig_0">2</ref>, we now move on to discuss how we formulate our objective for learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Training</head><p>We hypothesize that when an abnormal image is forward-passed into the network G, G D is not able to reconstruct the abnormalities even though G E manages to map the input X to the latent vector z. This is because the network is modeled only on normal samples during training and its parametrization is not suitable for generating abnormal samples. An output X that has missed abnormalities can lead to the encoder network E mapping X to a vector ẑ that has also missed abnormal feature representation, causing dissimilarity between z and ẑ. When there is such dissimilarity within latent vector space for an input image X, the model classifies X as an anomalous image. To validate this hypothesis, we formulate our objective function by combining three loss functions, each of which optimizes individual sub-networks. Adversarial Loss. Following the current trend within the new anomaly detection approaches <ref type="bibr" target="#b37">[39,</ref><ref type="bibr" target="#b38">40]</ref>, we also use feature matching loss for adversarial learning. Proposed by Salimans et al. <ref type="bibr" target="#b36">[38]</ref>, feature matching is shown to reduce the instability of GAN training. Unlike the vanilla GAN where G is updated based on the output of D (real/fake), here we update G based on the internal representation of D. Formally, let f be a function that outputs an intermediate layer of the discriminator D for a given input x drawn from the input data distribution p X , feature matching computes the L 2 distance between the feature representation of the original and the generated images, respectively. Hence, our adversarial loss L adv is defined as:</p><formula xml:id="formula_0">L adv = E x∼p X f (x) -E x∼p X f (G(x) 2 .</formula><p>(1)</p><p>Contextual Loss. The adversarial loss L adv is adequate to fool the discriminator D with generated samples. However, with only an adversarial loss, the generator is not optimized towards learning contextual information about the input data. It has been shown that penalizing the generator by measuring the distance between the input and the generated images remedies this issue <ref type="bibr" target="#b19">[21]</ref>. Isola et al. <ref type="bibr" target="#b19">[21]</ref> show that the use of L 1 yields less blurry results than L 2 . Hence, we also penalize G by measuring the L 1 distance between the original x and the generated images (x = G(x)) using a contextual loss L con defined as:</p><formula xml:id="formula_1">L con = E x∼p X x -G(x) 1 . (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>Encoder Loss. The two losses introduced above can enforce the generator to produce images that are not only realistic but also contextually sound. Moreover, we employ an additional encoder loss L enc to minimize the distance between the bottleneck features of the input (z = G E (x)) and the encoded features of the generated image (ẑ = E(G(x))). L enc is formally defined as</p><formula xml:id="formula_3">L enc = E x∼p X G E (x) -E(G(x)) 2 . (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>In so doing, the generator learns how to encode features of the generated image for normal samples. For anomalous inputs, however, it will fail to minimize the distance between the input and the generated images in the feature space since both G and E networks are optimized towards normal samples only. Overall, our objective function for the generator becomes the following:</p><formula xml:id="formula_5">L = w adv L adv + w con L con + w enc L enc (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>where w adv , w adv and w adv are the weighting parameters adjusting the impact of individual losses to the overall objective function. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model Testing</head><p>During the test stage, the model uses L enc given in Eq. 3 for scoring the abnormality of a given image. Hence, for a test sample x, our anomaly score A(x) or s x is defined as</p><formula xml:id="formula_7">A(x) = G E (x) -E(G(x)) 1 . (<label>5</label></formula><formula xml:id="formula_8">)</formula><p>To evaluate the overall anomaly performance, we compute the anomaly score for individual test sample x within the test set D, which in turn yields us a set of anomaly scores S = {s i : A( xi ), xi ∈ D}. We then apply feature scaling to have the anomaly scores within the probabilistic range of [0, 1].</p><formula xml:id="formula_9">s i = s i -min(S) max(S) -min(S)<label>(6)</label></formula><p>The use of Eq. 6 ultimately yields an anomaly score vector S for the final evaluation of the test set D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head><p>To evaluate our anomaly detection framework, we use three types of dataset ranging from the simplistic benchmark of MNIST <ref type="bibr" target="#b23">[25]</ref>, the reference benchmark of CIFAR <ref type="bibr" target="#b22">[24]</ref> and the operational context of anomaly detection within X-ray security screening <ref type="bibr" target="#b3">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MNIST.</head><p>To replicate the results presented in <ref type="bibr" target="#b38">[40]</ref>, we first experiment on MNIST data <ref type="bibr" target="#b23">[25]</ref> by treating one class being an anomaly, while the rest of the classes are considered as the normal class. In total, we have ten sets of data, each of which consider individual digits as the anomaly. CIFAR10. Within our use of the CIFAR dataset, we again treat one class as abnormal and the rest as normal. We then detect the outlier anomalies as instances drawn from the former class by training the model on the latter labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>University Baggage Anomaly Dataset-(UBA)</head><p>. This sliding window patched-based dataset comprises 230,275 image patches. Normal samples are extracted via an overlapping sliding window from a full X-ray image, constructed using single conventional X-ray imagery with associated false color materials mapping from dual-energy <ref type="bibr" target="#b34">[36]</ref>. Abnormal classes (122, 803) are of 3 sub-classes-knife (63, 496), gun (45, 855) and gun component (13, 452)-contain manually cropped threat objects together with sliding window patches whose intersection over union with the ground truth is greater than 0.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Full Firearm vs. Operational Benign-(FFOB).</head><p>In addition to these datasets, we also use the UK government evaluation dataset [1], comprising both expertly concealed firearm (threat) items and operational benign (non-threat) imagery from commercial X-ray security screening operations (baggage/parcels). Denoted as FFOB, this dataset comprises 4, 680 firearm full-weapons as full abnormal and 67, 672 operational benign as full normal images, respectively.</p><p>The procedure for train and test set split for the above datasets is as follows: we split the normal samples such that 80% and 20% of the samples are considered as part of the train and test sets, respectively. We then resize MNIST to 32 × 32, DBA and FFOB to 64 × 64, respectively.</p><p>Following Schlegl et al. <ref type="bibr" target="#b37">[39]</ref> (AnoGAN) and Zenati et al. <ref type="bibr" target="#b38">[40]</ref> (EGBAD), our adversarial training is also based on the standard DCGAN approach <ref type="bibr" target="#b32">[34]</ref> for a consistent comparison. As such, we aim to show the superiority of our multinetwork architecture regardless of using any tricks to improve the GAN training. In addition, we also compare our method against the traditional variational autoencoder architecture <ref type="bibr" target="#b4">[6]</ref> (VAE) to show the advantage of our multi-network architecture. We implement our approach in PyTorch <ref type="bibr" target="#b30">[32]</ref> (v0.4.0 with Python 3.6.5) by optimizing the networks using Adam <ref type="bibr" target="#b20">[22]</ref> with an initial learning rate lr = 2e -3 , and momentums β 1 = 0.5, β 2 = 0.999. Our model is optimized based on the weighted loss L (defined in Eq. 4) using the weight values w bce = 1, w rec = 50 and w enc = 1, which were empirically chosen to yield optimum results. (Figure <ref type="figure" target="#fig_5">5(b)</ref>). We train the model for 15, 25, 25 epochs for MNIST, UBA and FFOB datasets, respectively. Experimentation is performed using a dual-core Intel Xeon E5-2630 v4 processor and NVIDIA GTX Titan X GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>We report results based on the area under the curve (AUC) of the Receiver Operating Characteristic (ROC), true positive rate (TPR) as a function of false positive rate (FPR) for different points, each of which is a TPR-FPR value for different thresholds.</p><p>Figure <ref type="figure" target="#fig_2">4</ref>(a) presents the results obtained on MNIST data using 3 different random seeds, where we observe the clear superiority of our approach over previous contemporary models <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b37">39,</ref><ref type="bibr" target="#b38">40]</ref>. For each digit chosen as anomalous, our model achieves higher AUC than EGBAD <ref type="bibr" target="#b38">[40]</ref>, AnoGAN <ref type="bibr" target="#b37">[39]</ref> and variational autoencoder pipeline VAE <ref type="bibr" target="#b4">[6]</ref>. Due to showing its poor performance within relatively unchallenging dataset, we do not include VAE in the rest of experiments. Figure <ref type="figure" target="#fig_2">4</ref>(b) shows the performance of the models trained on the CIFAR10 dataset. We see that our model achieves the best AUC performance for any of the class chosen as anomalous. The reason for getting relatively lower quantitative results within this dataset is that for a selected abnormal category, there exists a normal class that is similar to the abnormal (plane vs. bird, cat vs. dog, horse vs. deer and car vs. truck).  For UBA and FFOB datasets shown in Table <ref type="table" target="#tab_0">1</ref>, our model again outperforms other approaches excluding the case of the knife. In fact, the performance of the models for knife is comparable. Relatively lower performance of this class is its shape simplicity, causing an overfit and hence high false positives. For the overall performance, however, our approach surpasses the other models, yielding AUC of 0.666 and 0.882 on the UBA and FFOB datasets, respectively.</p><p>Figure <ref type="figure" target="#fig_3">5</ref> depicts how the choice of hyper-parameters ultimately affect the overall performance of the model. In Fig. <ref type="figure" target="#fig_3">5</ref>(a), we see that the optimal performance is achieved when the size of the latent vector z is 100 for the MNIST   Figure <ref type="figure" target="#fig_4">6</ref> provides the histogram of the anomaly scores during the inference stage (a) and t-SNE visualization of the features extracted from the last convolutional layer of the discriminator network (b). Both of the figures demonstrate a clear separation within the latent vector z and feature f (.) spaces.</p><p>Table <ref type="table" target="#tab_1">2</ref> illustrates the runtime performance of the GAN-based models. Compared to the rest of the approaches, AnoGAN <ref type="bibr" target="#b37">[39]</ref> is computationally rather expensive since optimization of the latent vector is needed for each example. For EGBAD <ref type="bibr" target="#b38">[40]</ref>, we report similar runtime performance to that of the original paper. Our approach, on the other hand, achieves the highest runtime performance. Runtime performance of both UBA and FFOB datasets are comparable to MNIST even though their image and network size are double than that of MNIST.  A set of examples in Fig. <ref type="figure" target="#fig_6">7</ref> depict real and fake images that are respectively the input and output of our model. We expect the model to fail when generating anomalous samples. As can be seen in Fig. <ref type="figure" target="#fig_6">7</ref>(a), this is not the case for the class of 2 in the MNIST data. This stems from the fact that MNIST dataset is relatively unchallenging, and the model learns sufficient information to be able to generate samples not seen during training. Another conclusion that could be drawn is that distance in the latent vector space provides adequate details for detecting anomalies even though the model cannot distinguish abnormalities in the image space. On the contrary to the MNIST experiments, this is not the case. Figures <ref type="figure" target="#fig_6">7(b-c</ref>) illustrate that model is unable to produce abnormal objects.</p><p>Overall these results purport that our approach yields both statistically and computationally superior results than leading state-of-the-art approaches <ref type="bibr" target="#b37">[39,</ref><ref type="bibr" target="#b38">40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We introduce a novel encoder-decoder-encoder architectural model for general anomaly detection enabled by an adversarial training framework. Experimentation across dataset benchmarks of varying complexity, and within the operational anomaly detection context of X-ray security screening, shows that the proposed method outperforms both contemporary state-of-the-art GAN-based and traditional autoencoder-based anomaly detection approaches with generalization ability to any anomaly detection task. Future work will consider employing emerging contemporary GAN optimizations <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b15">17,</ref><ref type="bibr" target="#b36">38]</ref>, known to improve generalized adversarial training.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Pipeline of the proposed approach for anomaly detection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Comparison of the three models. (A) AnoGAN [39], (B) Efficient-GAN-Anomaly [40], (C) Our Approach: GANomaly</figDesc><graphic coords="8,66.51,400.67,289.12,63.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Results for MNIST (a) and CIFAR (b) datasets. Variations due to the use of 3 different random seeds are depicted via error bars. All but GANomaly results in (a) were obtained from [40].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. (a) Overall performance of the model based on varying size of the latent vector z. (b) Impact of weighting the losses on the overall performance. Model is trained on MNIST dataset with an abnormal digit-2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. (a) Histogram of the scores for both normal and abnormal test samples. (b) t-SNE visualization of the features extracted from the last conv. layer f (.) of the discriminator</figDesc><graphic coords="12,56.10,54.11,338.80,135.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 (</head><label>5</label><figDesc>b) demonstrates the impact of tuning the loss function in Eq. 4 on the overall performance. The model achieves the highest AUC when w bce = 1, w rec = 50 and w enc = 1. We empirically observe the same tuning-pattern for the rest of datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Exemplar real and generated samples containing normal and abnormal objects in each dataset. The model fails to generate abnormal samples not being trained on.</figDesc><graphic coords="13,96.30,65.84,268.21,500.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>AUC results for UBA and FFOB datasets</figDesc><table><row><cell>Method</cell><cell>UBA</cell><cell></cell><cell>FFOB</cell></row><row><cell></cell><cell cols="3">Gun Gun-parts Knife Overall Full-weapon</cell></row><row><cell cols="2">AnoGAN [39] 0.598 0.511</cell><cell>0.599 0.569</cell><cell>0.703</cell></row><row><cell cols="2">EGBAD [40] 0.614 0.591</cell><cell>0.587 0.597</cell><cell>0.712</cell></row><row><cell>GANomaly</cell><cell>0.747 0.662</cell><cell cols="2">0.520 0.643 0.882</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Computational performance of the approaches. (Runtime in terms of millisecond)</figDesc><table><row><cell>Model</cell><cell cols="3">MNIST CIFAR DBA FFOB</cell></row><row><cell cols="2">AnoGAN [39] 7120</cell><cell>7120</cell><cell>7110 7223</cell></row><row><cell cols="2">EGBAD [40] 8.92</cell><cell>8.71</cell><cell>8.88 8.87</cell></row><row><cell>GANomaly</cell><cell>2.79</cell><cell>2.21</cell><cell>2.66 2.53</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fraud detection system: a survey</title>
		<author>
			<persName><forename type="first">A</forename><surname>Abdallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Maarof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zainal</surname></persName>
		</author>
		<idno type="DOI">10.1016/J.JNCA.2016.04.007</idno>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S1084804516300571" />
	</analytic>
	<monogr>
		<title level="j">J. Netw. Comput</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="90" to="113" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A survey of anomaly detection techniques in financial domain</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Islam</surname></persName>
		</author>
		<idno type="DOI">10.1016/J.FUTURE.2015.01.001</idno>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S0167739X15000023" />
	</analytic>
	<monogr>
		<title level="j">Future Gener. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="278" to="288" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A survey of network anomaly detection techniques</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Naser Mahmood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<idno type="DOI">10.1016/J.JNCA.2015.11.016</idno>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S1084804515002891" />
	</analytic>
	<monogr>
		<title level="j">J. Netw. Comput. Appl</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="19" to="31" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Using deep convolutional neural network architectures for object classification and detection within X-ray baggage security imagery</title>
		<author>
			<persName><forename type="first">S</forename><surname>Akcay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Kundegorski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Willcocks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIFS.2018.2812196</idno>
		<ptr target="https://doi.org/10.1109/TIFS.2018.2812196" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Forensics Secur</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2203" to="2215" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Variational autoencoder based anomaly detection using reconstruction probability</title>
		<author>
			<persName><forename type="first">J</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Spec. Lect. IE</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Towards principled methods for training generative adversarial networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1701.04862" />
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2017-04">2017. April 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v70/arjovsky17a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08">August 2017</date>
			<biblScope unit="page" from="6" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Anomaly detection</title>
		<author>
			<persName><forename type="first">V</forename><surname>Chandola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="DOI">10.1145/1541880.1541882</idno>
		<ptr target="https://doi.org/10.1145/1541880.1541882" />
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="58" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">InfoGAN: interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2172" to="2180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Inverting the generator of a generative adversarial network (II)</title>
		<author>
			<persName><forename type="first">A</forename><surname>Creswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Bharath</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05701</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generative adversarial networks: an overview</title>
		<author>
			<persName><forename type="first">A</forename><surname>Creswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Arulkumaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Bharath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="65" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adversarial autoencoders for anomalous event detection in images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dimokranitou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
		<respStmt>
			<orgName>Purdue University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adversarial feature learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1605.09782" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04">April 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adversarially learned inference</title>
		<author>
			<persName><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improved training of Wasserstein GANs</title>
		<author>
			<persName><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5767" to="5777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning temporal regularity in video sequences</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="733" to="742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A survey of outlier detection methodologies</title>
		<author>
			<persName><forename type="first">V</forename><surname>Hodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Austin</surname></persName>
		</author>
		<idno type="DOI">10.1023/B:AIRE.0000045502.10941.a9</idno>
		<ptr target="https://doi.org/10.1023/B:AIRE.0000045502.10941.a9" />
	</analytic>
	<monogr>
		<title level="j">Artif. Intell. Rev</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="85" to="126" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Batch normalization: accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v37/ioffe15.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07">July 2015</date>
			<biblScope unit="page" from="7" to="09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.632</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2017.632" />
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07">July 2017</date>
			<biblScope unit="page" from="5967" to="5976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adam: a method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kinga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An overview of deep learning based methods for unsupervised and semi-supervised anomaly detection in videos</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Kiran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Parakkal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Imaging</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">36</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/" />
		<title level="m">MNIST handwritten digit database</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Precise recovery of latent vectors from generative adversarial networks</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tripathi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adversarial autoencoders</title>
		<author>
			<persName><forename type="first">A</forename><surname>Makhzani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Frey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Novelty detection: a review-part 1: statistical approaches. Signal Process</title>
		<author>
			<persName><forename type="first">M</forename><surname>Markou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<idno type="DOI">10.1016/J.SIGPRO.2003.07.018</idno>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S016516840300" />
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Novelty detection: a review-part 2: neural network based approaches. Signal Process</title>
		<author>
			<persName><forename type="first">M</forename><surname>Markou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<idno type="DOI">10.1016/J.SIGPRO.2003.07.019</idno>
		<idno>SIGPRO.2003.07.019</idno>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S0165" />
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page">168403002032</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Anomaly detection in video using predictive convolutional long short-term memory networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Medel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Savakis</surname></persName>
		</author>
		<idno>CoRR abs/1612.0</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<title level="m">Automatic differentiation in PyTorch</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">A review of novelty detection. Signal Process</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Pimentel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Clifton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Clifton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tarassenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="215" to="249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Training adversarial discriminators for cross-channel abnormal event detection in crowds</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<idno>CoRR abs/1706.0</idno>
		<ptr target="http://arxiv.org/abs/1706.07680" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Automated X-ray image analysis for cargo security: critical review and future promise</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaccard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Morton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Griffin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. X-Ray Sci. Technol. (Prepr.)</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Real-time anomaly detection and localization in crowded scenes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sabokrou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hoseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Klette</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPRW.2015.7301284</idno>
		<ptr target="http://ieeexplore.ieee.org/document/7301284/" />
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="56" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Improved techniques for training GANs</title>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unsupervised anomaly detection with generative adversarial networks to guide marker discovery</title>
		<author>
			<persName><forename type="first">T</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Seeböck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Waldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Langs</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-59050-9_12</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-59050-912" />
	</analytic>
	<monogr>
		<title level="m">IPMI 2017</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Niethammer</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10265</biblScope>
			<biblScope unit="page" from="146" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Zenati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Foo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lecouat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Manek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">R</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.06222</idno>
		<title level="m">Efficient GAN-based anomaly detection</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
