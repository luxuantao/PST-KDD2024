<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DADNet: Dilated-Attention-Deformable ConvNet for Crowd Counting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Dan</forename><surname>Guo</surname></persName>
							<email>guodan@hfut.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Information Engineering</orgName>
								<orgName type="institution">Hefei University of Technology</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Information Engineering</orgName>
								<orgName type="institution">Hefei University of Technology</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kun</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Information Engineering</orgName>
								<orgName type="institution">Hefei University of Technology</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Information Engineering</orgName>
								<orgName type="institution">Hefei University of Technology</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Information Engineering</orgName>
								<orgName type="institution">Hefei University of Technology</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Information Engineering</orgName>
								<orgName type="institution">Hefei University of Technology</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<addrLine>10 pages</addrLine>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DADNet: Dilated-Attention-Deformable ConvNet for Crowd Counting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E6635E3561DE95E6931685B4E27759D1</idno>
					<idno type="DOI">10.1145/3343031.3350881</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T05:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Crowd counting</term>
					<term>density map estimation</term>
					<term>scale-aware attention</term>
					<term>dilated convolution</term>
					<term>deformable convolution</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most existing CNN-based methods for crowd counting always suffer from large scale variation in objects of interest, leading to density maps of low quality. In this paper, we propose a novel deep model called Dilated-Attention-Deformable ConvNet (DADNet), which consists of two schemes: multiscale dilated attention and deformable convolutional DME (Density Map Estimation). The proposed model explores a scale-aware attention fusion with various dilation rates to capture different visual granularities of crowd regions of interest, and utilizes deformable convolutions to generate a high-quality density map. There are two merits as follows:</p><p>(1) varying dilation rates can effectively identify discriminative regions by enlarging the receptive fields of convolutional kernels upon surrounding region cues, and (2) deformable CNN operations promote the accuracy of object localization in the density map by augmenting the spatial object location sampling with adaptive offsets and scalars. DADNet not only excels at capturing rich spatial context of salient and tiny regions of interest simultaneously, but also keeps a robustness to background noises, such as partially occluded objects. Extensive experiments on benchmark datasets verify that DADNet achieves the state-of-the-art performance. Visualization results of the multi-scale attention maps further validate the remarkable interpretability achieved by our solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>â€¢ Computing methodologies â†’ Computer vision; Machine learning; Interest point and salient region detections; Object detection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Crowd counting aiming at estimating the number of people in images, has attracted extensive attention due to the potential important applications in the real world, such as public security, traffic monitoring and video surveillance. This task still remains challenging due to the existence of scale variations, object occlusions, and background noises in the crowd scene.</p><p>Recently, convolutional neural network (CNN) based approaches have been widely explored for crowd counting. Some works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b39">40]</ref> have achieved significant successes by adopting the multi-column network architecture. For example, MCNN <ref type="bibr" target="#b39">[40]</ref> is a typical multi-column architecture, which showed strong adaptability to discover variable-size objects (heads) using convolutions with different kernel sizes in each column. Based on the multi-column architecture, Sam et al. <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b25">26]</ref> designed an auxiliary switch-CNN layer to vote the best appropriate CNN regressor column for each image patch for the density map generation. Moreover, Sindagi et al. <ref type="bibr" target="#b30">[31]</ref> proposed the Global Context Estimator (GCE), Local Context Estimator (LCE), and Fusion-CNN modules to explicitly incorporate global and local contextual information of crowd images. Besides, a single-column structure network based on dilated convolutions is proposed <ref type="bibr" target="#b17">[18]</ref>, which deployed dilated convolutional layers for progressively extracting deeper contextual information of saliency. The CSRNet model <ref type="bibr" target="#b17">[18]</ref> expanded the receptive field without losing resolution by the dilated convolutions.</p><p>In addition, a close task to crowd counting is the problem of vehicle counting <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22]</ref>. Vehicle counting aims at precisely estimate the number of vehicles in traffic congestion scenes <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b38">39]</ref>. Onoro-Rubio et. al. <ref type="bibr" target="#b21">[22]</ref> proposed an MCNN-like network called Hydra-3s to generate the density map. Zhang et al. <ref type="bibr" target="#b38">[39]</ref> used a combination of Fully Convolutional Neural network (FCN) and Long Short-Term Memory network (LSTM) to estimate the vehicle density and count number jointly. Besides, the CSRNet model <ref type="bibr" target="#b17">[18]</ref> for crowd counting has also verified its effectiveness on the vehicle counting dataset TRANCOS <ref type="bibr" target="#b7">[8]</ref>. Although these methods achieve remarkable progress, there are remains two defects: (1) they ignore the attention map differences among multi-scale feature maps; and (2) even they consider and fuse all these multi-scale feature maps, but always concatenate them directly <ref type="bibr" target="#b39">[40]</ref>. This simple fusion strategy can not adapt to the complex issues of scale variations, object occlusions, and background noises in crowd scenes. In the paper, we propose a novel network framework called Dilated-Attention-Deformable ConvNet (DADNet), which designs adaptive dilated-CNN attention fusion and adaptive deformable-CNN DME (Density Map Estimation) modules to address the above issues.</p><p>As an illustration example shown in Figure <ref type="figure">1</ref>, under the dilated scale 1 (dilation rate ğ‘Ÿ=1), the 2D attention map highlights the big heads of interest, e.g., the nearby red box subregion; while the dilated scale 4 (dilation rate ğ‘Ÿ=9) highlights the small heads of interest, e.g., the distant green box subregion. The attention maps at dilated scales 2 (dilation rate ğ‘Ÿ=3) and 3 (dilation rate ğ‘Ÿ=6) reflect the pervasive head coverages and the contour information of heads, respectively. The latter two dilated scales alleviate the noise issue in occlusion situations (e.g., the blue box subregions). DADNet jointly learns the context variety under different dilated scale views and delivers a three-layer deformable DME to generate a high-quality density map.</p><p>In detail, as shown in Figure <ref type="figure" target="#fig_2">2</ref>, the DADNet is realized by three steps. First, low-level feature maps of an image are extracted by a modified VGG-16 backbone network <ref type="bibr" target="#b28">[29]</ref>. Secondly, a scale-aware attention fusion using dilated convolution is designed to discover the visual responses of big objects, tiny objects, and contour profile of objects in crowd or traffic scenes effectively. Finally, the accuracy of object localization is further promoted by utilizing a deformable convolution with adaptive receptive fields on object locations, which promises high-quality density maps. The main contributions of this paper are summarized as follows: </p><formula xml:id="formula_0">â€¢</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Traditional methods. Early approaches usually counted the number of people by detecting heads or bodies in the images. Most of them focused on extracting hand-crafted features (e.g., Haar wavelets <ref type="bibr" target="#b33">[34]</ref> and HOG <ref type="bibr" target="#b4">[5]</ref>) from the human body or particular body parts <ref type="bibr" target="#b32">[33]</ref>. Dollar et al. <ref type="bibr" target="#b5">[6]</ref> used a slide window detector over the image to count the person. All these detection-based approaches are not applicable when crowds are extremely congested. Meanwhile, some researchers explored regression-based approaches for crowd counting, which aimed at training regression models to directly map the visual features to the number of people. Idress et al. <ref type="bibr" target="#b11">[12]</ref> proposed a regression model to learn multiply features (i.e., head detection and SIFT <ref type="bibr" target="#b20">[21]</ref> interest points) for extremely dense crowd images. They adopted the object counting value for optimization training, while neglecting the location information of objects. This training process needed massive image samples. Furthermore, to solve the sparse and imbalanced training data, Chen et al. <ref type="bibr" target="#b1">[2]</ref> proposed a novel cumulative attribute-based regression model to effectively capture the scalar variation of objects, which showed a promising performance on sparse data and imbalanced training data yet suffering an expensive computational cost. By contrast, Lempitsky et al. <ref type="bibr" target="#b15">[16]</ref> first introduced a density map to estimate the counting number accurately. They adopted a linear model to map the input image to the density map. Pham et al. <ref type="bibr" target="#b23">[24]</ref> used a random forest regression to learn the non-linear mapping instead of the linear mapping.</p><p>CNN-based methods. Inspired by the success of deep CNN in the computer vision community, a variety of CNNbased approaches have been proposed for crowd counting. Wang et al. <ref type="bibr" target="#b34">[35]</ref> designed a CNN-based regression network Dilated convolutions. The dilated convolution operation is widely used in a variety of vision tasks, such as semantic segmentation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b36">37]</ref>, image de-raining <ref type="bibr" target="#b16">[17]</ref>, and object detection <ref type="bibr" target="#b31">[32]</ref>. Both Yu et al. <ref type="bibr" target="#b36">[37]</ref> and Chen et al. <ref type="bibr" target="#b2">[3]</ref> designed dilated-CNN based convolution networks to capture contextual features to segment objects robustly. Li et al. <ref type="bibr" target="#b16">[17]</ref> proposed a recurrent feature aggregation module involving dilation convolution operations for image de-raining. Recently, Song et al. <ref type="bibr" target="#b31">[32]</ref> utilized a pyramid dilated convolution module to extract multi-scale spatial features for the salient object detection simultaneously. The most related to ours is <ref type="bibr" target="#b17">[18]</ref> for crowd counting. The CSRNet model <ref type="bibr" target="#b17">[18]</ref> based on progressively dilated convolutions expanded contextual information in the back-end DME module. Specifically, it <ref type="bibr" target="#b17">[18]</ref> adopted a fixed dilation rate.</p><p>Contrary to previous approaches, the paper proposes a dilated-CNN based multi-column (scale) architecture to generate a high-quality density map. The dilated-CNN with various dilation rates contributes to learning the rich contextual information for the front-end feature representation. Moreover, an adaptive deformable convolution is introduced in the back-end DME module to precisely locate objects' positions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED METHOD</head><p>In this paper, we propose a Dilated-Attention-Deformable ConvNet (DADNet) for crowd counting. As illustrated in Figure <ref type="figure" target="#fig_2">2</ref>, DADNet mainly consists of two modules: a scale-aware attention module and a deformable density map estimation module. We first employ the first 10 layers of VGG-16 as the backbone which generates the VGG feature maps of each image. Based on the VGG feature maps, DADNet captures various visual attention granularities of crowd regions of interest with different dilation scale convolutions, and implements a scale-aware attention fusion. Finally, it feeds the fused feature maps into the deformable Density Map Estimation (DME) module, which utilizes deformable convolutions by augmenting the spatial sampling locations with learnable offsets and scalars to generate high-quality density maps. Specifically, DADNet is an attention-injective deformable ConvNet, which addresses the large variations in object sizes with various dilation scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Scale-aware Attention Fusion</head><p>Current top-down CNN-based methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b39">40]</ref> usually focus on discriminative object regions and ignore non-discriminative areas. In this paper, we devote to realizing the dense object location correlation by transferring spatial information from discriminative regions to adjacent non-discriminative regions. To this end, the dilated convolution, also called atrous convolution, is adopted, which provides a larger receptive field than normal convolution operations and remains the unchanged number of model parameters. Given a kernel size ğ‘˜ Ã— ğ‘˜ and a dilation rate ğ‘Ÿ, the receptive field of a dilated convolution operation is enlarged to ğ‘˜ + (ğ‘˜ -1)(ğ‘Ÿ -1). In other words, it can capture border surrounding areas and richer context information than normal convolution units. The head areas of people in an image always vary drastically in the crowd scenes. A unitary receptive field can not adapt to handle the crowd density variation.</p><p>To address the problem of crowd density variation, we design a multi-scale dilated convolution attention module as shown in Figure <ref type="figure" target="#fig_2">2</ref>. We use different dilation scales to discover visual context cues. The core idea is to enable different visual context cues to perform the spatial referring on non-discriminative areas. In this paper, we set the number of varying scales ğ‘† = 4 with corresponding dilation rate ğ‘Ÿ âˆˆ {1, 3, 6, 9}. At each dilated rate (ğ‘Ÿğ‘–, ğ‘– âˆˆ [1, ğ‘†]), we calculate dilated feature maps by Fğ‘Ÿ ğ‘– = â„±ğ‘Ÿ(Fğ‘£ğ‘”ğ‘”), where Fğ‘£ğ‘”ğ‘” denotes the VGG feature maps and â„±ğ‘Ÿ denotes the dilated convolutional operation on Fğ‘£ğ‘”ğ‘”.</p><p>Based on feature maps {Fğ‘Ÿ ğ‘– }(ğ‘– âˆˆ [1, ğ‘†]), we conduct a scaleaware fusion based on an attention weighting mechanism to fuse different feature branches. The fusion measures the contribution of the spatial context under different dilation scales. The corresponding 2D attention map Iğ‘Ÿ ğ‘– of feature map Fğ‘Ÿ ğ‘– âˆˆ R ğ»Ã—ğ‘Š Ã—#ğ‘â„ is formulated as:</p><formula xml:id="formula_1">Iğ‘Ÿ ğ‘– = ğ‘†ğ‘–ğ‘”ğ‘šğ‘œğ‘–ğ‘‘(â„± {1Ã—1} (Fğ‘Ÿ ğ‘– , Î˜â„± )) âˆˆ R ğ»Ã—ğ‘Š ,<label>(1)</label></formula><p>where ğ» Ã— ğ‘Š is the dimension of feature maps, #ğ‘â„ is the channel number, ğ‘†ğ‘–ğ‘”ğ‘šğ‘œğ‘–ğ‘‘ denotes the sigmoid activation function, â„± {1Ã—1} denotes the 1 Ã— 1 convolution operation, and Î˜â„± is the model parameters of â„± {1Ã—1} .</p><p>To make a scale-aware attention fusion, we normalize the attention maps [I1, . . . , Iğ‘†] at each scale. At dilation rate ğ‘Ÿğ‘–, the normalized 2D attention map Wğ‘Ÿ ğ‘– is defined as follow:</p><formula xml:id="formula_2">Wğ‘Ÿ ğ‘– = Iğ‘Ÿ ğ‘– ./ âˆ‘ï¸ ğ‘† ğ‘Ÿ ğ‘– =1 Iğ‘Ÿ ğ‘– âˆˆ R ğ»Ã—ğ‘Š ,<label>(2)</label></formula><p>where "./" is the element-wise division operation. Finally, we employ the scale-aware attention maps [Wğ‘Ÿ 1 , . . ., Wğ‘Ÿ ğ‘† ] to generate the fused feature maps F ğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘› :</p><formula xml:id="formula_3">F ğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘› = ğ‘† âˆ‘ï¸ ğ‘–=1 F ğ‘ğ‘¡ğ‘¡ ğ‘– = ğ‘† âˆ‘ï¸ ğ‘–=1 Fğ‘Ÿ ğ‘– âŠ™ Wğ‘Ÿ ğ‘– âˆˆ R ğ»Ã—ğ‘Š Ã—#ğ‘â„ ,<label>(3)</label></formula><p>where âŠ™ means element-wise product operation, and F ğ‘ğ‘¡ğ‘¡ ğ‘– denotes the scale-aware feature map at dilation scale ğ‘–. Note that the feature dimensions of F ğ‘ğ‘¡ğ‘¡ ğ‘– and F ğ‘“ ğ‘¢ğ‘ ğ‘–ğ‘œğ‘› are the same as Fğ‘£ğ‘”ğ‘”.</p><p>Figure <ref type="figure" target="#fig_3">3</ref> depicts the normalized multi-scale dilated attention maps {Wğ‘Ÿ ğ‘– }|ğ‘Ÿğ‘– âˆˆ {1, 3, 6, 9} of three image samples for crowd counting. We can see that DADNet captures different attention responses effectively, which is adaptive to tackle diversified crowd distributions, complex backgrounds, and various occlusions in crowd scenes. From a theoretical view, the solution of scale-aware dilated attention is flexible, which can be extended to arbitrary branches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Deformable Density Map Estimation</head><p>After adaptively fusing feature maps in Section 3.1, in this subsection, a deformable DME is proposed to modulate the accurate spatial transformation of the object locations in the to-be-generated density map. As illustrated in Figure <ref type="figure" target="#fig_2">2</ref>, the DME module consists of a three-layer deformable convolution (ğ‘‘ğ‘’ğ‘“ -ğ‘ğ‘œğ‘›ğ‘£) network, in which the sampling location weight w(p), scalar âˆ†s and offset âˆ†p on each ğ‘‘ğ‘’ğ‘“ -ğ‘ğ‘œğ‘›ğ‘£ layer are all the learning parameters. With the help of these parameters, the convolutional grid can self-adapt to obtain useful location cues on the fused feature map to generate the high-quality density map.</p><p>As a basic convolution, the sampling location p ğ‘˜ with a convolution kernel of 3 Ã— 3 can be expressed as p ğ‘˜ âˆˆ ğ’¦ = {(-1, -1), (0, -1), . . . , (1, 0), (1, 1)}. For a location p in the input feature map x, the output feature map y(p) is formulated as:</p><formula xml:id="formula_4">y(p) = ğ’¦ âˆ‘ï¸ ğ‘˜=1 w(p ğ‘˜ ) â€¢ x(p + p ğ‘˜ )<label>(4)</label></formula><p>where w(p) weighted the summation of sampled values <ref type="bibr" target="#b3">[4]</ref>. Different from the uniform sampling with the fixed p ğ‘˜ in normal convolutions, the adaptive learnable offset âˆ†p and modulation scalar âˆ†s are added. Both âˆ†s and âˆ†p in the deformable convolution can be optimized via training <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b40">41]</ref>. In the paper, we adopt the deformable convolution in <ref type="bibr" target="#b40">[41]</ref>. With the same sampling location set ğ’¦, the output feature map y(p) in the deformable convolution is expressed as follow:</p><formula xml:id="formula_5">y(p) = ğ’¦ âˆ‘ï¸ ğ‘˜=1 w(p ğ‘˜ ) â€¢ x(p + p ğ‘˜ + âˆ†p ğ‘˜ ) â€¢ âˆ†s ğ‘˜ ,<label>(5)</label></formula><p>where âˆ†p ğ‘˜ and âˆ†s ğ‘˜ denote the offset and the modulation scalar at the ğ‘˜-th location in ğ’¦, respectively. Technically speaking, the deformable DME is realized as follows. We use three consecutive ğ‘‘ğ‘’ğ‘“ -ğ‘ğ‘œğ‘£ğ‘› layers with the same kernel size of 3 Ã— 3 and add the ReLU activation <ref type="bibr" target="#b14">[15]</ref> function after each ğ‘‘ğ‘’ğ‘“ -ğ‘ğ‘œğ‘£ğ‘› layer to continuously refine the to-be-generated density map. Finally, we use 1Ã—1 convolution on the ğ‘‘ğ‘’ğ‘“ -ğ‘ğ‘œğ‘£ğ‘› layer to generate the density maps. This kind of dynamic sampling scheme is appropriate for crowd counting, especially for crowd scenes with some congested noises. The proposed DME adjusts the sampling locations in the image automatically, which promotes the accuracy of object location in the to=be-generated density map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Loss Optimization</head><p>We adopt the pixel-wise mean square error (MSE) loss as the optimization objective, which measures the difference between a predicted density map and the groundtruth. Given an image ğ¼ğ‘–, the model parameter Î˜ of DADNet is optimized as follow:</p><formula xml:id="formula_6">ğ¿ğ‘œğ‘ ğ‘  = 1 2ğµ ğµ âˆ‘ï¸ ğ‘–=1 â€–ğ¹ (ğ¼ğ‘–, Î˜) -ğ‘Œğ‘–â€– 2 2 , (<label>6</label></formula><formula xml:id="formula_7">)</formula><p>where ğµ is the batch size, ğ¹ (ğ¼ğ‘–, Î˜) denotes the predicted density map, and ğ‘Œğ‘– is the groundtruth. UCF CC 50 <ref type="bibr" target="#b11">[12]</ref> contains only 50 images collected from the Internet. The number of people ranges from 94 to 4,543 in each image, and the average count is 1,280. The limited training data and a wide margin of the crowd density variation make the dataset extremely challenging. For a fair comparison, experiments on this dataset are conducted with the standard settings in <ref type="bibr" target="#b11">[12]</ref>, i.e., a 5-fold cross-validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS 4.1 Experiment Setup</head><p>UCF-QRNF <ref type="bibr" target="#b12">[13]</ref> is a new and the largest dataset for crowd counting, which contains 1,535 images. The minimum and maximum numbers of people are 49 and 12,865, respectively. And the average count is 815. The UCF-QRNF dataset has the largest crowd density variation.</p><p>TRANCOS <ref type="bibr" target="#b7">[8]</ref> is a vehicle counting dataset collected from different traffic congested scenes with various viewpoints. It consists of 1,244 images captured by surveillance cameras. The regions of interest (ROI) in each image are annotated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Evaluation metrics. We adopt five metrics as follows:</head><p>Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index (SSIM) <ref type="bibr" target="#b35">[36]</ref>, and Grid Average Mean Absolute Error (GAME) <ref type="bibr" target="#b7">[8]</ref>.</p><p>Given an image ğ¼, the groundtruth density map ğ‘Œ is obtained by the method in <ref type="bibr" target="#b30">[31]</ref>. ğ‘Œ is calculated by convolving each pixel with a Gaussian kernel, which is formalized as follows:</p><formula xml:id="formula_8">ğ‘Œ = âˆ‘ï¸ ğ‘¥ ğ‘– âˆˆğ‘„ ğ›¿(ğ‘¥ -ğ‘¥ğ‘–) Ã— ğº ğœ‡,ğœ 2 (ğ‘¥) ,<label>(7)</label></formula><p>where ğ‘¥ is a pixel position in image, ğ‘¥ğ‘– is the ğ‘–-th head position in the annotation set ğ‘„. ğœ‡ and ğœ represent the kernel size and standard deviation parameters of Gaussian kernel (ğº ğœ‡,ğœ 2 ), respectively. We set ğœ‡ = 15 and ğœ = 4 for all datasets. Besides, as the size of feature maps is cut down to 1/8 of the input image, the same is the density map. We utilize bilinear interpolation <ref type="bibr" target="#b17">[18]</ref> to resize it to the size of the groundtruth.</p><p>The most commonly used metrics MAE and RMSE are calculated as follows:</p><formula xml:id="formula_9">ğ‘€ ğ´ğ¸ = 1 ğ‘ ğ‘ âˆ‘ï¸ ğ‘–=1 | á»¸ğ‘– -Å¶ğ‘–| , ğ‘…ğ‘€ ğ‘†ğ¸ = â¯ â¸ â¸ â· 1 ğ‘ ğ‘ âˆ‘ï¸ ğ‘–=1 | á»¸ğ‘– -Å¶ğ‘–| 2 , (<label>8</label></formula><formula xml:id="formula_10">)</formula><p>where ğ‘ is the number of test images, á»¸ğ‘– and Å¶ğ‘– denote the predicted and the groundtruth counts of the ğ‘–-th test image, respectively. á»¸ğ‘– is calculated by summing over the predicted density map.</p><p>Besides, the GAME metric is used to evaluate the TRAN-COS dataset. GAME(ğ¿) splits the density map into 4 ğ¿ nonoverlapping subregions and calculates the MAE value in each subregion, where ğ¿ is the split level. The sum of the MAE values in all these subregions is the GAME value. GAME is equal to the MAE metric when ğ¿ = 0.</p><formula xml:id="formula_11">ğºğ´ğ‘€ ğ¸(ğ¿) = 1 ğ‘ ğ‘ âˆ‘ï¸ ğ‘–=1 ( 4 ğ¿ âˆ‘ï¸ ğ‘™=1 | á»¸ ğ‘™ ğ‘– -Å¶ ğ‘™ ğ‘– |) ,<label>(9)</label></formula><p>where á»¸ ğ‘™ ğ‘– represents the predicted count of the ğ‘–-th test image within region ğ‘™, while Å¶ ğ‘™ ğ‘– is the groundtruth count. The higher ğ¿, the more restrictive the GAME metric will be <ref type="bibr" target="#b7">[8]</ref>. In this paper, we experiment with ğ¿ = 0, 1, 2, and 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Implementation details.</head><p>The proposed model is implemented on the platform of PyTorch <ref type="bibr" target="#b22">[23]</ref>. To promote the diversity and variety of training data, we crop four patches with a fixed size (256 Ã— 256) at some random positions in each training image. The training patches are horizontally flipped with a probability of 0.5 for data augmentation. In the proposed model, we adopt the Adam optimizer <ref type="bibr" target="#b13">[14]</ref> and set the batch size to 50, the initialized learning rate to 1ğ‘’ -5 , and the dropout rate to 0.5 over every 50 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Study</head><p>Here we experiment and discuss the ablation study to investigate the effectiveness of each component within the proposed model.</p><p>â€¢ DADNet w/o dil-cov replaces the dilated convolutional operation by the normal convolutional operation in the scale-aware dilated attention module. To be fair, the kernel sizes of these convolutional layers are equivalently transformed to {3, 7, 13, 19}, respectively. â€¢ DADNet w/o def-cov replaces the deformable convolution operation by the normal convolution operation in the DME module. â€¢ DADNet w/o scale-fusion removes the scale-aware attention fusion in DADNet. Under this condition, we directly sum the feature maps of each dilated convolution branch.</p><p>As the experimental results are shown in Table <ref type="table" target="#tab_0">1</ref> on the ShanghaiTech Part A dataset, the overall DADNet has obvious performance improvement compared with DADNet w/o dil-cov and DADNet w/o def-cov. We observe obvious increases at the MAE and RMSE values of DADNet w/o dil-cov. It verifies that the dialed convolution can increase the receptive field and context information, which improves the accuracy of the density map estimation. Compared with DADNet, the MAE value of DADNet w/o def-cov rises from 64.2 to a much bigger value 67.5. The reason is that the learnable parameters (offset âˆ†p and scalar âˆ†s) in the deformable convolution are helpful to generate density maps. The parameters ensure the adaptive object location sampling in the surrounding spatial context. Besides, even with both the dilated and the deformable convolutions, DADNet w/o scale-fusion still has a performance drop On the UCF CC 50 dataset, our approach achieves comparable performances with the MAE of 285.5 and the RMSE of 389.7, which is better than most of the comparative methods except <ref type="bibr" target="#b24">[25]</ref>and <ref type="bibr" target="#b30">[31]</ref>. UCF CC 50 is challenging due to its sparse training data, extremely dense crowds, and noise backgrounds. To solve the issues, some models have some additional data-processing. <ref type="bibr" target="#b24">[25]</ref> used the original image and a low-resolution density map to optimize a high-resolution density map. <ref type="bibr" target="#b30">[31]</ref> adopted both local and global context estimators to optimize the feature extraction. Our model is just a simple end-to-end network based on VGG features without additional data-processing.</p><p>By observing Figure <ref type="figure" target="#fig_5">4</ref>, there are two interesting conclusions. (1) Figure <ref type="figure" target="#fig_5">4</ref> (a). The persons in boxed regions are surrounded with many background noises. For examples, the color of clothes is similar to head, and the color of hand is the same to the face. It is difficult to locate the real head Table <ref type="table" target="#tab_2">3</ref>: Performance evaluation on the UCF-QNRF dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model MAEâ†“ RMSEâ†“</head><p>Idrees et al. <ref type="bibr" target="#b11">[12]</ref> 315.0 508.0 MCNN <ref type="bibr" target="#b39">[40]</ref> 277.0 426.0 ResNet101 <ref type="bibr" target="#b8">[9]</ref> 190.0 277.0 CMTL <ref type="bibr" target="#b29">[30]</ref> 252.0 514.0 Switching-CNN <ref type="bibr" target="#b25">[26]</ref> 228.0 445.0 DenseNet201 <ref type="bibr" target="#b10">[11]</ref> 163.0 226.0 Idrees et al. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Comparison results on vehicle counting dataset.</head><p>We make comparison with the state-of-the-art methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b38">39]</ref>. As shown in Table <ref type="table" target="#tab_4">4</ref>, our approach achieves the best performance, especially with a great improvement on GAME3. The higher ğ¿ = 3, the more restrictive the GAME metric will be. Compared with the results of CSRNet <ref type="bibr" target="#b17">[18]</ref>, our approach is 21.6% lower on GAME0 (ğ¿ =0), 19.7% lower    proposed DADNet has great robustness and generalization, which applies to both crowd and vehicle counting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Density Map Quality</head><p>To evaluate the image quality of the predicted density map intuitively, we adopt the PSNR and SSIM metrics to assess our model. The higher the PSNR score is, the higher-quality the predict density map is. The SSIM metric is used to measure the similarity between the predict density map and the groundtruth. Table <ref type="table" target="#tab_5">5</ref> shows that DADNet achieves the highest PSNR and SSIM values (24.16 and 0.81) for crowd counting, respectively. As shown in Figure <ref type="figure" target="#fig_5">4</ref>, when we zoom two red boxes, DADNet generates much more accurate head position and clearer density map than CSRNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>The paper proposes a Dilated-Attention-Deformable ConvNet (DADNet) framework for both crowd and vehicle counting, which generates high-quality multi-scale attention maps. To capture multi-scale contextual cues in the image, the proposed DADNet designs a scale-aware dilated attention mechanism to effectively discover different visual appearances of nearby objects, distant objects and contour profile of the objects in  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2 Figure 1 :</head><label>21</label><figDesc>Figure 1: Illustration of scale-aware dilated attention fusion in the proposed DADNet. The screenshot of each cropped image subregion (i.e., red, blue and green subregions) responses different attention maps under various dilation scales. Utilizing the complementary of all these attention maps is helpful to exactly locate objects in the crowd scenes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>The proposed DADNet generates high-quality density maps by effectively learning visual context cues of multiscale features, which shows strong adaptability to resist scale variations, object occlusions, and background noises in crowd image. â€¢ DADNet consists of a scale-aware attention fusion and a deformable DME. The former utilizes an adaptive 2D attention map mechanism on multi-scale features for exact visual representation, while the latter augments the flexibility of spatial sampling locations of objects with learnable offsets and scalars. â€¢ Extensive experiments on three crowd counting benchmark datasets (i.e., ShanghaiTech, UCF CC 50, UCF-QNRF) and one vehicle counting dataset (TRANCOS) achieve the state-of-the-art performance. Ablation studies demonstrate the effectiveness of each module within the proposed model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The architecture of the proposed DADNet. The convolutional parameter is denoted as "(kernel size)Ã—(kernel size)@(filter size)-(dilation rate)" in the dilated convolutions, while "Def-conv-(kernel size)Ã—(kernel size)@(filter size)" in the deformable convolutions. to extract convolutional features and compared it with traditional hand-crafted features. Zhang et al. [38] designed a deep CNN network with two alternative loss objectives on both density map and counting number evaluations to address a novel cross-scene counting problem. With the alternative training, the proposed model was fine-tuned and adapted to new target scenarios. To handle the scale variance of crowds, researchers further employed a CNN-based multi-column architecture with different convolutional kernel sizes in multiple branches. Zhang et al. [40] proposed a Multi-column Convolutional Neural Network (MCNN) to tackle arbitrary image size inputs. Based on the MCNN framework, Sam et al. [1, 26] proposed a Switch-CNN layer to vote the best appropriate CNN regressor column for each image patch for the density map generation. More recently, many advanced methods have been used to improve density map quality. Shen et al. [27] explored a Generative Adversarial Network (GAN) model to constrain the cross-scale density consistency in the crowd estimation. Shi et al. [28] designed a Negative Correlation Learning (NCL) based ensemble network for the density map generation. Liu et al. [20] proposed a self-supervised approach by incorporating unlabelled crowd images during training. As the attention mechanism has been widely used in various computer vision tasks, Liu et al. [19] designed a framework named DecideNet to introduce visual attention for crowd counting. MA Hossain et al. [10] proposed a joint attention network to model both global and local visual contextual information.Dilated convolutions. The dilated convolution operation is widely used in a variety of vision tasks, such as semantic segmentation<ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b36">37]</ref>, image de-raining<ref type="bibr" target="#b16">[17]</ref>, and object detection<ref type="bibr" target="#b31">[32]</ref>. Both Yu et al.<ref type="bibr" target="#b36">[37]</ref> and Chen et al.<ref type="bibr" target="#b2">[3]</ref> designed dilated-CNN based convolution networks to capture contextual features to segment objects robustly. Li et al.<ref type="bibr" target="#b16">[17]</ref> proposed a recurrent feature aggregation module involving dilation convolution operations for image de-raining. Recently, Song et al.<ref type="bibr" target="#b31">[32]</ref> utilized a pyramid dilated convolution module to extract multi-scale spatial features for the salient object detection simultaneously. The most related to ours</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Visualization of multi-scale dilated attention maps of three images in the ShanghaiTech Part A dataset. As for crowd counting, Figures (a), (b), (c), and (d) reflect different attention appearances on the big objects of interest (nearby people's heads, ğ‘Ÿ=1), pervasive areas of objects (ğ‘Ÿ=3), contour profiles of objects (ğ‘Ÿ=6), and tiny objects of interest (distant people's heads, ğ‘Ÿ=9), respectively.</figDesc><graphic coords="4,66.99,219.53,477.87,63.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>4. 1 . 1</head><label>11</label><figDesc>Datasets. In this paper, we test and verify the proposed model on both crowd counting and vehicle counting datasets. ShanghaiTech [40] is divided into Part A and Part B. Part A includes 482 images downloaded from the Internet, while Part B contains 716 images which are shot from streets in Shanghai. Images in Part A are more crowded than in Part B. Parts A and B contain 300/182 and 400/316 images for training/testing sets, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Comparison of the proposed DADNet model to CSRNet [18] and the groundtruth on the generated density maps. GT denotes the goundtruth. (a) Local subregion comparison. (b) Global dentistry map comparison.</figDesc><graphic coords="7,82.70,216.38,91.15,50.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>density maps have high-quality, which are approximate to the groundtruth. Experimental results indicate that DADNet has good robustness to generate clear density maps. (2) Figure 4 (b). Detecting tiny regions is tough.Some algorithms pay much more attention to context and have high response regions overly, resulting in the overestimation of human count. For example, CSRNet overestimates the real count, which adopted a single scale dilated convolution. It verifies that DADNet adapts to the scale variability. Our multi-scale dilated attention can extract richer visual context cues. In addition, Figure5demonstrates that DADNet has close count number to the groundtruth too. The predicted density map of the proposed model exhibits good clarity and accuracy in both crowded and sparse areas.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Visualization of density maps of four examples in the ShanghaiTech Part A dataset for crowd counting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Visualization of multi-scale dilated attention maps of four images in the TRANCOS dataset. As for vehicle counting, Figures (a), (b), (c), and (d) reflect different attention appearances on the tiny objects of interest (distant cars, ğ‘Ÿ=1), big objects of interest (nearby cars, ğ‘Ÿ=3), contour profiles of tiny objects (ğ‘Ÿ=6) and big objects (ğ‘Ÿ=9), respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Visualization of density maps of four examples in the TRANCOS dataset for vehicle counting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Ablation study on the ShanghaiTech Part A dataset.</figDesc><table><row><cell>Model</cell><cell cols="2">MAEâ†“ RMSEâ†“</cell></row><row><cell>DADNet w/o dil-cov</cell><cell>65.5</cell><cell>105.5</cell></row><row><cell>DADNet w/o def-cov</cell><cell>67.5</cell><cell>106.5</cell></row><row><cell>DADNet w/o scale-fusion</cell><cell>66.7</cell><cell>104.2</cell></row><row><cell>DADNet</cell><cell>64.2</cell><cell>99.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance evaluation on the Shang-haiTech and UCF CC 50 datasets.</figDesc><table><row><cell>Method</cell><cell>Part A</cell><cell cols="2">Part B</cell><cell>UCF CC 50</cell></row><row><cell></cell><cell cols="4">MAEâ†“ RMSEâ†“ MAEâ†“ RMSEâ†“ MAEâ†“ RMSEâ†“</cell></row><row><cell>Zhang et al. [38]</cell><cell cols="2">181.8 277.7 32.0</cell><cell cols="2">49.8 467.0 498.5</cell></row><row><cell>MCNN [40]</cell><cell cols="2">110.2 173.2 26.4</cell><cell cols="2">41.3 377.6 509.1</cell></row><row><cell>CMTL [30]</cell><cell cols="2">101.3 152.4 20.0</cell><cell cols="2">31.1 322.8 341.1</cell></row><row><cell cols="3">Switching-CNN [26] 90.4 135.0 21.6</cell><cell cols="2">33.4 318.1 439.2</cell></row><row><cell>CP-CNN [31]</cell><cell cols="2">73.6 106.4 20.1</cell><cell cols="2">30.1 295.8 320.9</cell></row><row><cell>ACSCP [27]</cell><cell cols="2">75.7 102.7 17.2</cell><cell cols="2">27.4 291.0 404.6</cell></row><row><cell>Liu et al. [20]</cell><cell cols="2">73.6 112.0 13.7</cell><cell cols="2">21.4 337.6 434.3</cell></row><row><cell>D-ConvNet [28]</cell><cell cols="2">73.5 112.3 18.7</cell><cell cols="2">26.0 288.4 404.7</cell></row><row><cell>IG-CNN [1]</cell><cell cols="2">72.5 118.2 13.6</cell><cell cols="2">21.1 291.4 349.4</cell></row><row><cell>ic-CNN [25]</cell><cell cols="2">68.5 116.2 10.7</cell><cell cols="2">16.0 260.9 365.5</cell></row><row><cell>CSRNet [18]</cell><cell cols="2">68.2 115.0 10.6</cell><cell cols="2">16.0 266.1 397.5</cell></row><row><cell>DADNet</cell><cell>64.2 99.9</cell><cell>8.8</cell><cell cols="2">13.5 285.5 389.7</cell></row><row><cell cols="5">compared to DADNet. It verifies that fusion effectively inte-</cell></row><row><cell cols="5">grates different visual cues in the crowd scenes.</cell></row><row><cell cols="2">4.3 Main Comparison</cell><cell></cell><cell></cell></row><row><cell cols="5">4.3.1 Comparison results on crowd counting datasets. As shown</cell></row><row><cell cols="5">in Table 2, our approach achieves the best MAE and RMSE</cell></row><row><cell cols="5">values on the ShanghaiTech dataset, especially for Part A</cell></row><row><cell cols="5">which have a great scale view variance in the crowd scenes.</cell></row><row><cell cols="5">As for Part B, our approach achieves the best MAE of 8.8 and</cell></row><row><cell cols="5">RMSE of 13.5 too. Experimental results on the UCF-QNRF</cell></row><row><cell cols="2">dataset are summarized in</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>DADNet achieves the lowest MAE and RMSE, as well as a 14.2% improvement of MAE. These results indicate that DADNet adapts to the scale variability and has good robustness.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>GT Count: 361 GT Count: 441 GT Count: 93 DADNet Est Count: 96.36 DADNet Est Count: 396.26 DADNet Est Count: 439.86 DADNet Est Count: 362.90 GT Count: 398</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Performance evaluation on the TRANCOS dataset.</figDesc><table><row><cell>Model</cell><cell cols="4">GAME0â†“ GAME1â†“ GAME2â†“ GAME3â†“</cell></row><row><cell>Fiaschi et al. [7]</cell><cell>17.77</cell><cell>20.14</cell><cell>23.65</cell><cell>25.99</cell></row><row><cell cols="2">Lempitsky et al. [16] 13.76</cell><cell>16.72</cell><cell>20.72</cell><cell>24.36</cell></row><row><cell>Hydra-3s [22]</cell><cell>10.99</cell><cell>13.75</cell><cell>16.69</cell><cell>19.32</cell></row><row><cell>FCN-HA [39]</cell><cell>4.21</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CSRNet [18]</cell><cell>3.56</cell><cell>5.49</cell><cell>8.57</cell><cell>15.04</cell></row><row><cell>DADNet</cell><cell>2.79</cell><cell>4.41</cell><cell>6.43</cell><cell>9.27</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Qualitative evaluation on the ShanghaiTech Part A dataset.crowd or traffic scenes. In addition, a deformable convolution module with adaptive receptive fields is used to further promote the accuracy of object localization, which promises high-quality density maps. Extensive experiments are conducted on the crowd datasets ShanghaiTech, UCF CC 50, and UCF-QNRF, and the vehicle dataset TRANCOS. Experimental results show that the proposed DADNet achieves superior performance than the state-of-the-art approaches.</figDesc><table><row><cell>Model</cell><cell cols="2">PSNRâ†‘ SSIMâ†‘</cell></row><row><cell>MCNN [40]</cell><cell>21.40</cell><cell>0.52</cell></row><row><cell>CP-CNN [31]</cell><cell>21.72</cell><cell>0.72</cell></row><row><cell>CSRNet [18]</cell><cell>23.79</cell><cell>0.76</cell></row><row><cell>DADNet</cell><cell>24.16</cell><cell>0.81</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work is supported by the National Natural Science Foundation of China (NSFC) under grants 61725203, 61732008, 61876058, and 61632007.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Divide and Grow: Capturing Huge Diversity in Crowd Images With Incrementally Growing CNN</title>
		<author>
			<persName><forename type="first">Babu</forename><surname>Deepak</surname></persName>
		</author>
		<author>
			<persName><surname>Sam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Neeraj N Sajjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mukundhan</forename><surname>Venkatesh Babu</surname></persName>
		</author>
		<author>
			<persName><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3618" to="3626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cumulative attribute space for age and crowd density estimation</title>
		<author>
			<persName><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2467" to="2474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</title>
		<author>
			<persName><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI)</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">Navneet</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pedestrian detection: An evaluation of the state of the art</title>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="743" to="761" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning to count with regression forest and structured labels</title>
		<author>
			<persName><forename type="first">Luca</forename><surname>Fiaschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ullrich</forename><surname>KÃ¶the</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fred</forename><forename type="middle">A</forename><surname>Hamprecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Pattern Recognition (ICPR)</title>
		<meeting>the 21st International Conference on Pattern Recognition (ICPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2685" to="2688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Extremely overlapping vehicle counting</title>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Guerrero-GÃ³mez-Olmedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beatriz</forename><surname>Torre-JimÃ©nez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>LÃ³pez-Sastre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saturnino</forename><surname>Maldonado-BascÃ³n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Onoro-Rubio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Iberian Conference on Pattern Recognition and Image Analysis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="423" to="431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Crowd Counting Using Scale-Aware Attention Networks</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehrdad</forename><surname>Hosseinzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omit</forename><surname>Chanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1280" to="1288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-source multi-scale counting in extremely dense crowd images</title>
		<author>
			<persName><forename type="first">Haroon</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Imran</forename><surname>Saleemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cody</forename><surname>Seibert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2547" to="2554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Composition loss for counting, density map estimation and localization in dense crowds</title>
		<author>
			<persName><forename type="first">Haroon</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhmmad</forename><surname>Tayyab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kishan</forename><surname>Athrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somaya</forename><surname>Al-Maadeed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nasir</forename><surname>Rajpoot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="532" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<title level="m">Adam: A method for stochastic optimization. International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning to count objects in images</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1324" to="1332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recurrent squeeze-and-excitation context aggregation net for single image deraining</title>
		<author>
			<persName><forename type="first">Xia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianlong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongbin</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="254" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Csrnet: Dilated convolutional neural networks for understanding the highly congested scenes</title>
		<author>
			<persName><forename type="first">Yuhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deming</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1091" to="1100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Decidenet: Counting varying density crowds through attention guided detection and density estimation</title>
		<author>
			<persName><forename type="first">Jiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenqiang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5197" to="5206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Leveraging unlabeled data for crowd counting by learning to rank</title>
		<author>
			<persName><forename type="first">Xialei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joost</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">D</forename><surname>Bagdanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7661" to="7669" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Object recognition from local scale-invariant features</title>
		<author>
			<persName><surname>David G Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the Seventh IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="1150" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Towards perspective-free object counting with deep learning</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Onoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-</forename><surname>Rubio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto J LÃ³pez-Sastre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="615" to="629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Autodiff Workshop</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Count forest: Co-voting uncertain number of targets using random forest for crowd density estimation</title>
		<author>
			<persName><forename type="first">Viet-Quoc</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsuo</forename><surname>Kozakaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Osamu</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryuzo</forename><surname>Okada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3253" to="3261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Iterative crowd counting</title>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Viresh Ranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Hoai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="270" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Switching convolutional neural network for crowd counting</title>
		<author>
			<persName><forename type="first">Babu</forename><surname>Deepak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiv</forename><surname>Sam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Surya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Babu</forename><surname>Venkatesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4031" to="4039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Crowd Counting via Adversarial Cross-Scale Consistency Pursuit</title>
		<author>
			<persName><forename type="first">Zan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minsi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianguo</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5245" to="5254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Crowd Counting With Deep Negative Correlation Learning</title>
		<author>
			<persName><forename type="first">Zenglin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofeng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangdong</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoyan</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5382" to="5390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">CNN-Based cascaded multi-task learning of high-level prior and density estimation for crowd counting</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vishwanath</surname></persName>
		</author>
		<author>
			<persName><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName><surname>Patel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Generating highquality crowd density maps using contextual pyramid cnns</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vishwanath</surname></persName>
		</author>
		<author>
			<persName><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1861" to="1870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pyramid dilated deeper convlstm for video salient object detection</title>
		<author>
			<persName><forename type="first">Hongmei</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanyuan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kin-Man</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="715" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Counting people in the crowd using a generic head detector</title>
		<author>
			<persName><forename type="first">Adrien</forename><surname>Venkatesh Bala Subburaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cyril</forename><surname>Descamps</surname></persName>
		</author>
		<author>
			<persName><surname>Carincotte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Ninth International Conference on Advanced Video and Signal-Based Surveillance (AVSS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="470" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Robust real-time face detection</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep people counting in extremely dense crowds</title>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaochun</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM International Conference on Multimedia</title>
		<meeting>the 23rd ACM International Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1299" to="1302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eero</forename><forename type="middle">P</forename><surname>Hamid R Sheikh</surname></persName>
		</author>
		<author>
			<persName><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multi-Scale Context Aggregation by Dilated Convolutions</title>
		<author>
			<persName><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Cross-scene crowd counting via deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Cong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="833" to="841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Fcn-rlstm: Deep spatio-temporal neural networks for vehicle counting in city cameras</title>
		<author>
			<persName><forename type="first">Shanghang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanhang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joao</forename><forename type="middle">P</forename><surname>Costeira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">JosÃ© Mf</forename><surname>Moura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3667" to="3676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Single-image crowd counting via multi-column convolutional neural network</title>
		<author>
			<persName><forename type="first">Yingying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Desen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siqin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="589" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deformable convnets v2: More deformable, better results</title>
		<author>
			<persName><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9308" to="9316" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
