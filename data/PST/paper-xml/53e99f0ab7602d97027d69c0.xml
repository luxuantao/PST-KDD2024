<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Comparison of Classification Methods for Predicting Deception in Computer-Mediated Communication</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2015-04-13">13 April 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lina</forename><surname>Zhou</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Information Systems</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>Baltimore County</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Judee</forename><forename type="middle">K</forename><surname>Burgoon</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Center for the Management of Information</orgName>
								<orgName type="institution">University of Arizona</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Douglas</forename><forename type="middle">P</forename><surname>Twitchell</surname></persName>
							<affiliation key="aff5">
								<orgName type="department">Management Information Systems</orgName>
								<orgName type="institution">University of Arizona</orgName>
							</affiliation>
						</author>
						<author>
							<persName><roleName>AND</roleName><forename type="first">Tiantian</forename><surname>Qin</surname></persName>
						</author>
						<author>
							<persName><surname>Nunamaker</surname><genName>Jr</genName></persName>
							<affiliation key="aff7">
								<orgName type="department">Center for the Management of Information</orgName>
								<orgName type="institution">University of Arizona</orgName>
								<address>
									<settlement>Tucson</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jay</forename><forename type="middle">F Lina</forename><surname>Nunamaker</surname><genName>JR</genName></persName>
						</author>
						<author>
							<persName><surname>Zhou</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Nebraska</orgName>
								<address>
									<settlement>Lincoln]</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<addrLine>37-41 Mortimer Street</addrLine>
									<postCode>W1T 3JH</postCode>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">JAY F</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="department">Management Information Systems</orgName>
								<orgName type="institution">University of Arizona</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff8">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>Baltimore</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Comparison of Classification Methods for Predicting Deception in Computer-Mediated Communication</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2015-04-13">13 April 2015</date>
						</imprint>
					</monogr>
					<idno type="MD5">4515954AF89FA5FA2069BA69F9E75D4F</idno>
					<idno type="DOI">10.1080/07421222.2004.11045779</idno>
					<note type="submission">On: 13 April 2015, At: 14:12 Publisher: Routledge Informa Ltd Registered in England and Wales Registered Number: 1072954 Registered office: Mortimer House,</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>for research</term>
					<term>teaching</term>
					<term>and private study purposes. Any substantial or systematic reproduction</term>
					<term>redistribution</term>
					<term>reselling</term>
					<term>loan</term>
					<term>sub-licensing</term>
					<term>systematic supply</term>
					<term>or distribution in any County. Dr. Zhou&apos;s research interest includes text mining</term>
					<term>ontology</term>
					<term>deception detection</term>
					<term>machine learning</term>
					<term>information retrieval</term>
					<term>and</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Taylor &amp; Francis makes every effort to ensure the accuracy of all the information (the "Content") contained in the publications on our platform. However, Taylor &amp; Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor &amp; Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>140 ZHOU ET AL.</p><p>2002. This award is given for a lifetime of exceptional achievement in information systems. He was elected a fellow of the Association of Information Systems in 2000. He has over 40 years of experience in examining, analyzing, designing, testing, evaluating, and developing information systems. He has served as a test engineer at the Shippingport Atomic Power facility, as a member of the ISDOS team at the University of Michigan, and as a member of the faculty at Purdue University prior to joining the faculty at the University of Arizona in 1974. His research on group support systems addresses behavioral as well as engineering issues and focuses on theory as well as implementation. Dr. Nunamaker received his Ph.D. in systems engineering and operations research from Case Institute of Technology, an M.S. and B.S. in engineering from the University of Pittsburgh, and a B.S. from Carnegie Mellon University. He has been a licensed professional engineer since 1965.</p><p>ABSTRACT: The increased chance of deception in computer-mediated communication and the potential risk of taking action based on deceptive information calls for automatic detection of deception. To achieve the ultimate goal of automatic prediction of deception, we selected four common classification methods and empirically compared their performance in predicting deception. The deception and truth data were collected during two experimental studies. The results suggest that all of the four methods were promising for predicting deception with cues to deception. Among them, neural networks exhibited consistent performance and were robust across test settings. The comparisons also highlighted the importance of selecting important input variables and removing noise in an attempt to enhance the performance of classification methods. The selected cues offer both methodological and theoretical contributions to the body of deception and information systems research. KEY WORDS AND PHRASES: classification methods, deception, deception detection, linguistic cues. DECEPTION IN HUMAN COMMUNICATION occurs when information senders attempt to create a false impression in receivers. Most people have experienced deception of one form or another from outright lies and fabrications to little "white" lies <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b20">22]</ref>. Deception may result in receivers taking actions unfavorable to themselves on behalf of the information senders. If the actions are critical to a person's life, an organization's survival, or even a nation's stability, neglecting deception may lead to immeasurable losses. Therefore, the need to improve deception detection is of longstanding concern and strong practical relevance to research communities, practitioners, and government agencies.</p><p>Extensive research has been conducted on cues that can be used to detect deception <ref type="bibr" target="#b15">[17]</ref>. A close review of the literature reveals, however, that most studies mainly focus on deception in rich media channels (e.g., face-to-face interactions), with some exceptions in the area of textual and computer-mediated communication <ref type="bibr" target="#b43">[45]</ref>. Increasing reliance on computers in support of human-to-human communication poses at least two challenges to deception research. The first is information overload: the pace of information Downloaded by [University of Nebraska, Lincoln] at 14:12 <ref type="bibr" target="#b12">13</ref> April 2015 growth has far exceeded our capability to process it. The second is humans' poor deception detection capability, which is seldom better than chance <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b15">17]</ref>.</p><p>One obvious solution to address both issues is to automate the deception detection process. To achieve the automatic prediction of deception, we envision a three-step course of action: (1) identify significant cues to deception, (2) automatically derive the cues from various media, and (3) build classification models for predicting deception from new messages. Previous research has identified promising indicators of deceit based on language features <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b43">45]</ref>. For example, deceptive messages have been found to include higher informality and expressivity, and lower lexical diversity and complexity. Focusing on language behaviors, rather than specific content, has the advantage that indicators derived from language behaviors may be relatively independent of context and are more amenable to simple parsing approaches (e.g., the very challenging task of semantic parsing can be bypassed). Moreover, deceivers may have control over the content of their messages, but deceptive intent may still be "betrayed" through one's language use. Some progress has been made in identifying and automatically deriving deception indicators from text by integrating findings and methods from multiple relevant disciplines, including natural language processing, linguistics <ref type="bibr" target="#b36">[38]</ref>, and stylistic research <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b25">27]</ref>. The majority of deception studies have relied on human coders manually rating behavioral indicators of deception <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b35">37]</ref>, and these have stopped short of prediction, instead simply reporting verbal and nonverbal cues associated with truth or deception.</p><p>The current investigation addresses the third objective of building classification models for predicting deceit by evaluating four popular classification approaches for their ability to discriminate truthful from deceptive text-based messages. Data from two empirical studies were subjected to classification with discriminant analysis, logit regression, neural networks, and decision tree analysis. Because a single message from a deceiver may present different predictive power than a series of messages from the same deceiver, the classification approaches were evaluated with both messages and individuals as the unit of analysis. The first consisted of individual messages. The second comprised all messages from individual subjects. These comparisons also shed light on the features of deceptive messages.</p><p>Discriminant analysis has been applied in detecting deception with some limited success (e.g., <ref type="bibr">[15]</ref>). Compared with discriminant analysis, logistic regression places less stringent requirements on the underlying data features and may therefore be well suited to the task of predicting deception. In contrast with statistical methods, machine learning methods are not tied to the characteristics of the population distribution; however, they are usually more computationally intensive. Many common machine learning approaches, such as decision trees and neural networks, can automatically build classification models from the existing data and then predict the outcome for the new data. Neural networks have been found to provide better prediction than discriminant analysis in some applications <ref type="bibr" target="#b40">[42,</ref><ref type="bibr" target="#b41">43]</ref>. There has also been an initial attempt at applying decision trees in grouping messages into deceptive and truthful classes <ref type="bibr" target="#b2">[3]</ref>. With the scattered reports using one classification method for deception detection, there is a great need to compare the performance of difference classification methods in the Downloaded by [University of Nebraska, Lincoln] at 14:12 13 April 2015 context of deception detection and identify the most appropriate ones. Hence, this paper extends prior work on cues to deception by investigating four classification methods-discriminant analysis, logistic regression, decision trees, and neural networks-for their predictive power in discriminating truth from deception. We begin by reviewing theories and prior empirical findings related to cues to deception, based on which cues were identified for inclusion into the classification models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Theories in Support of Detecting Deception from Lean Media COMMUNICATION AND OTHER RELEVANT RESEARCH has long studied how and why individuals deceive. Within this broad area of investigation, there have been several theories proposed for rich media. Media richness is measured on a continuum and is determined by four criteria: feedback (asking questions or making corrections), multiple cues (transmitting voice inflection, body language, and so on), language variety (range of meaning that can be encoded in language symbols), and personal focus (transmitting feelings and emotions) <ref type="bibr" target="#b12">[13]</ref>. According to these criteria, computer-mediated communication (CMC) is generally leaner than face-to-face communication, especially if it is text-based. Despite the lack of theories specifically directed toward text-based CMC, some theories that have been frequently used to guide deception research in media-rich channels may be extended to less rich channels.</p><p>Interpersonal deception theory (IDT) attempts to explain deception from an interpersonal and conversational perspective, rather than an individual and psychological perspective <ref type="bibr" target="#b1">[2]</ref>. IDT posits that within the context and relationship of the sender and receivers of deception, deceivers will display strategic modifications of behavior in response to a receiver's suspicions, but may also display nonstrategic (inadvertent) behavior, or leakage cues, indicating that deception is occurring. The theory is not confined to any one modality, nor does it focus solely on physiological or nonverbal indicators. Therefore, it is also applicable to leaner mediated channels.</p><p>Channel expansion theory <ref type="bibr" target="#b7">[8]</ref> expands media richness theory <ref type="bibr" target="#b12">[13]</ref> by including the experience senders or receivers have with a channel or medium, the topic of the communication, the organizational context, and the other parties in the communication. The more experience the senders and the receivers have with each of these domains, the richer they find the media they are using. The heightened perceived richness may have one of two results: those more experienced with text-based CMC will find it richer and, in the process of using it, transmit more deception cues, or they may have a greater ability to strategically hide possible deception cues. People hold expectations about the discourse of others and assume that others will satisfy the interaction demands of exchanging messages that are sufficiently complete, truthful, clear, and relevant to the current topics <ref type="bibr" target="#b19">[21]</ref>. To be successful in evading detection, deceivers have to make their messages appear complete, truthful, clear, and relevant. Therefore, some aspects of deceivers' linguistic patterns should be similar to those displayed in Downloaded by [University of Nebraska, Lincoln] at 14:12 13 April 2015 face-to-face situations. Others may take advantage of the unique properties of written, rather than spoken, language, and of the response delays in text communication that afford them more time to plan, monitor, and edit what they say.</p><p>Focusing on the dynamics in interpersonal interaction, interaction adaptation theory <ref type="bibr" target="#b5">[6]</ref> describes and predicts patterns of reciprocity and compensation between communicators. The theory holds that there may be a fair amount of reciprocal behavior display (including language behavior) between deceivers and truth-tellers. This makes it easy for deceivers to get away with their deception because it is difficult to distinguish deceivers from truth-tellers within a given pair of communication. Conversely, it may be easier to differentiate deceivers from truth-tellers who are involved in different interactions. Interpersonal adaptation theory also implies that deception is likely to be a continuous event that unfolds over time <ref type="bibr" target="#b37">[39]</ref>. Deceivers may manage to embed their deception intentions in other truthful messages to increase their chances of success. Therefore, aggregating all the messages from an information sender may provide a holistic view and, consequently, a better indication of deception than examining individual messages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Linguistic Cues to Deception</head><p>The above-mentioned theories are concerned with how deception occurs, but do not supply guidelines on how to detect it. In addition, textual messages lack facial expressions, gestures, and conventions of body posture and distance, so the text itself is the only source for inferring personal opinions and attitudes and verifying message credibility. Two disciplines that have produced methods for detecting deceit from text are criminal justice and linguistics. In criminal investigations, the validity of statements made by suspects has been evaluated using criteria-based content analysis <ref type="bibr" target="#b33">[35]</ref>, reality monitoring <ref type="bibr" target="#b22">[24]</ref>, and scientific content analysis <ref type="bibr" target="#b32">[34]</ref>. Each of these techniques includes a set of criteria against which susceptible statements is compared. The presence or absence of a criterion (e.g., contextual embedding) affects the judged truthfulness of an account. Such criteria are usually quite complex and mostly used by trained experts. Verbal immediacy research <ref type="bibr" target="#b27">[29]</ref> offers detailed criteria for scoring the degree to which language creates psychological closeness or distance (nonimmediacy). Non-immediate language is thought to signal both a distancing by deceivers from their messages, which reduces their accountability and responsibility for what they say, and an indication of negative feelings associated with the act of deceiving. Even though none of these systems for coding deceptive discourse was developed specifically for CMC, they provide the theoretical and evidentiary foundation for the investigation of deception in CMC.</p><p>Several prior investigations that have sought to determine the viability of using linguistics-based cues to distinguish truthful from deceptive messages have produced promising results <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b43">45]</ref>. The majority of the classes of linguistic features studied received significant support. In particular, deceivers' messages were more expressive than their partners and they appeared more informal, as they had more typographical Downloaded by [University of Nebraska, Lincoln] at 14:12 13 April 2015 errors than truth-tellers. Deceivers' messages were less complex, which was manifested in less punctuation, fewer long sentences, and fewer syllables per word. Deceptive subjects displayed less diversity at both the lexical and content level than did truth-tellers. They also used more non-immediate and uncertain language in the form of fewer self-references and more modal verbs. However, the same cues could conceivably show opposite effects under deception in different modalities and task settings. A case in point: whereas deception has routinely been associated with shorter messages, in one of our experiments, the deceivers who performed a decision-making task with a partner via e-mail displayed higher quantity-of words, verbs, noun phrases, and sentences-and used more affective language <ref type="bibr" target="#b43">[45]</ref>. Yet, in another experiment, during an interview conducted either via text-chat or audio-conferencing, deceivers tended to use briefer messages and less language referring to emotions and feelings than did truth-tellers <ref type="bibr" target="#b2">[3]</ref>. This suggests that deception behavior is moderated by contextual factors. Therefore, the classification models built on top of cues identified in one type of deception context may not be suitable to other contexts. Consequently, the test data used in the current study to evaluate the classification models were collected under the same context as the training data.</p><p>Based on the aforementioned research results, we reevaluated the classification of linguistics features in our prior investigations (see <ref type="bibr" target="#b43">[45]</ref>) and made several deletions, additions, and regroupings. Deletion was based on correlation analysis and the results of the prior studies. For cues that were highly correlated, all but one was removed. For example, words, sentences, and noun phrases from the quantity group were deleted because they were so highly correlated with verbs. Indicators that did not find support in any of the previous studies were also eliminated. As more resources supporting the automatic derivation of cues became available, several new cues were also added to the list. For example, Whissel et al.'s <ref type="bibr" target="#b36">[38]</ref> affect dictionary made possible the inclusion of measures of pleasantness, activation, and imagery. If deception is indeed an unpleasant act, or if deceivers attempt to divert attention from themselves by becoming less expressive, their language should exhibit less pleasantness and be devoid of imagery or language conveying high arousal. The regrouping included merging two or more cues into one (e.g., individual references combined both firstperson and second-person personal pronouns) and dividing one group into two subgroups (e.g., a new uncertainty grouping was created by extracting some cues from the original non-immediacy group). The resultant updated list of cues, as shown in Table <ref type="table" target="#tab_0">1</ref>, was used to create classification models.</p><p>Motivated by Whissel et al.'s affect dictionary <ref type="bibr" target="#b36">[38]</ref> and the need to distinguish between positive and negative forms of affect, we generated six additional cues: positive pleasantness, positive activation, positive imagery, highly positive pleasantness, highly positive activation, and negative activation. Positive (or negative) affect cues are those with scalar values at least one standard deviation higher (or lower) than the mean of the corresponding affect, whereas those designated as highly positive (or negative) cues have values at least two standard deviations from the mean of the corresponding affect. Downloaded by [University of Nebraska, Lincoln] at 14:12 13 April 2015  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Automated Generation of Cues to Deception</head><p>In each of the studies reported in this paper, the messages were collected automatically via an online system. A Web-based e-mail messaging system automatically captured all of the textual data for the experimental task and stored it in a MS SQL 2000 database from which we retrieved the data.</p><p>To process the messages and extract each of the individual cues, we used GATE (general architecture for text engineering), created at the University of Sheffield. GATE 2.0 is a Java-based, object-oriented framework, architecture, and development environment for creating programs for analyzing, processing, or generating natural language <ref type="bibr" target="#b9">[10]</ref>. It has been employed on many projects (see <ref type="bibr" target="#b26">[28]</ref>) including, for example, creation of the American National Corpus (americannationalcorpus.org) and text summarization <ref type="bibr" target="#b24">[26]</ref>. GATE is a component-based architecture based on two main components: language resources (LR) and processing resources (PR). LRs are data-only resources, such as single documents (or messages in our case), corpora, ontologies, and lexicons. PRs are programmatic or algorithmic resources that either use or process LRs, such as parsers, part-of-speech taggers, and cue derivation modules <ref type="bibr" target="#b10">[11]</ref>. For example, to count all of the verbs in a document, one would create an LR that contains or represents the document. Next, two PRs, a part-of-speech tagger and a verb counter, are created. Last, an application, or pipeline, is created wherein the PRs are assigned to process the document LR and the number of verbs is counted. Figure <ref type="figure" target="#fig_1">1</ref> shows one of the messages from the experimental task in GATE with its verb phrases marked, which are counted to create the verb quantity cue.</p><p>To accomplish the goal of extracting and deriving cues from messages, we created a set of PRs, each of which extracted a cue or set of cues from the document. For example, we built a PR using the GATE-provided Java annotations processing engine (JAPE) <ref type="bibr" target="#b10">[11]</ref> that recognized and counted group references such as we, us, and ours. A transformation-based part-of-speech tagger shipped with GATE was used to tag each word according to its part of speech. These tags were then used to extract cues, such as verb quantity. In addition, dictionaries were incorporated to enhance the abilities Downloaded by [University of Nebraska, Lincoln] at 14:12 13 April 2015 of GATE. Based on an affect dictionary <ref type="bibr" target="#b36">[38]</ref>, we were able derive the values of pleasantness and several other cues from messages. All of the work described above lays the solid foundation for the automatic prediction of deception. It allows us to focus on investigating the performance of classification approaches in deception detection. The reviewed theories suggest observing deceptive behavior over the course of an interaction rather than from individual messages may yield the best predictive models. On the one hand, using individual messages as the unit of analysis has the potential advantage of providing greater precision with regard to deceivers' one-time behavior, but has the disadvantage that messages from the same individual are not independent of one another, thereby violating the assumptions of some statistical models. Moreover, people who produce numerous messages are overrepresented in the corpus and have greater influence on the resulting classification models. Furthermore, if some of the messages within the deception condition are truthful, this can introduce noise and lead to biased results. On the other hand, data that are aggregated over all the messages from a given person should benefit from being a more reliable summary of a given individual's linguistic behavior. But this approach yields smaller sample sizes with concomitant reductions in statistical power, and it averages estimates across variables in ways that may disguise the importance of a given cue. For example, if a sender uses high emotiveness in one message but not in another, the relationship between the predictor of emotiveness and the criterion measure of the message, even though not highly reliable in the given sample, may go undiscovered and not be investigated further. Therefore, as a secondary objective, we compared the classification methods on a new dimension: units of analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overview of Classification Approaches</head><p>IN THIS SECTION, WE GIVE A BRIEF OVERVIEW of the four classification methods under investigation: discriminant analysis, logistic regression, decision trees, and neural networks. Among many variants of each approach, we select one that is appropriate for the task under investigation. The mechanisms for selecting important cues (inputs or attributes) in each method are also discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discriminant Analysis</head><p>Linear discriminant analysis (simplified as discriminant analysis hereafter) <ref type="bibr" target="#b17">[19]</ref> is a popular statistics method for the classification task. It constructs a linear function β′X by maximizing the univariate between-groups variability relative to within-groups variability, which is stated below <ref type="bibr" target="#b40">[42]</ref>:</p><formula xml:id="formula_0">( ) 2 1 2 , β µ -β µ ′ ′ β Σβ ′<label>(1)</label></formula><p>where Σ, µ 1 , and µ 2 are the common covariance matrix and mean vectors of two groups π 1 and π 2 , respectively. Classification models derived from the above procedure are usually optimal in minimizing the expected cost of misclassification <ref type="bibr" target="#b34">[36]</ref>. Fisher's linear discriminant function was developed under the assumption that different groups from the underlying populations have equal covariance structures <ref type="bibr" target="#b21">[23]</ref>. In applied research, data are seldom compatible with the underlying assumption. Nonetheless, discriminant analysis has been found to be very robust to deviations from the above ideal condition when the data are substantially linear. Since it is still unclear whether deception data are "well behaved," discriminant analysis may work well in the current investigation.</p><p>When using discriminant analysis, it is common practice to remove independent variables that are not significant in a stepwise manner. We used both the forced entry and forward stepwise methods available in SPSS (statistical package for the social sciences) for the discriminant analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Logistic Regression</head><p>In contrast to the linear relationships between the decision variable and the independent variables in linear regression models, logistic regression methods apply an additional logistic function and transform the linear probabilities into logit ones. The logit distribution constrains the estimated probabilities lying between zero and one, which can be represented as follows: Downloaded by [University of Nebraska, Lincoln] at 14:12 13 April 2015</p><formula xml:id="formula_1">PREDICTING DECEPTION IN COMPUTER-MEDIATED COMMUNICATION 149 x p e 1 , 1 -α-β′ = + (2)</formula><p>where x is a vector of input variables, β is a vector of coefficients, and α is a constant term. In our study, the logistic maximum likelihood procedure was utilized to estimate the coefficients of the classification model corresponding to each of the 19 variables listed in Table <ref type="table" target="#tab_0">1</ref>. The estimated coefficients β can be interpreted as the effect of the independent variable on the probability of the event (e.g., deception) divided by the probability of nonevent (e.g., truth) <ref type="bibr" target="#b8">[9]</ref>. β values reflect the significance of input variables and can thereby help us identify important cues to deception from a candidate list. As with the discriminant analysis, we applied both the forced entry and forward stepwise methods in the logistic regression analysis using SPSS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Decision Trees</head><p>The C4.5 algorithm <ref type="bibr" target="#b30">[32]</ref> for inducing classification models of decision trees is an extension of the basic ID3 algorithm. It uses a greedy, top-down algorithm to produce the tree. If all data items belong to the same class, C4.5 keeps the decision tree as a leaf. Otherwise, it will recursively try to find the best attribute to split the data items into sub-leaves. Information gain after splitting relative to before splitting is the common criterion for selecting the best attribute. If no gains can be obtained by splitting, the set of mixed data items is made into a leaf, which is labeled as the most frequent class of data items in the set. From a decision tree, rules can be derived by following attributes and decision criteria on each path from the root to the leaf. An example of a decision tree created with data from the experimental task is shown in Figure <ref type="figure" target="#fig_2">2</ref>. The values in boxes and associated arrows indicate the path taken by a single message to its classification. Since the lower parts of a decision tree are often based on relatively few samples and can be inaccurate, it is desirable to have some way of pruning trees <ref type="bibr" target="#b28">[30]</ref>. C4.5 can determine how deeply to grow a decision tree and reduce errors through pruning <ref type="bibr" target="#b38">[40]</ref>. Pruning decision trees is a process of replacing a sub-tree with a branch or leaf node when the replacement can result in reduction of expected errors. A common problem with decision trees is overfitting the data, which may lower generalizability of the models. Pruning decision trees not only helps reduce errors but also avoid the overfitting problem <ref type="bibr" target="#b28">[30]</ref>. In this study, we used the C4.5 revision 8 for decision trees (called J48) and the neural network implementation (discussed next) from WEKA <ref type="bibr" target="#b39">[41]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Neural Networks</head><p>Neural networks are structured in layers, which generally consist of at least one input layer, one output layer, and a number of hidden layers existing in between. Each layer can have one or more nodes and there are weights to connect the nodes in different layers. Neural networks have a number of variations in terms of possible algorithms. We chose the most commonly and widely used back-propagation (BP) networks for Downloaded by [University of Nebraska, Lincoln] at 14: <ref type="bibr">12 13</ref> April 2015 this study. The BP network is a supervised learning network, aiming to train the network to map input vectors to a desired output vector. During training, the training input variables are first fed to the input layer of the network and passed to the output layer gradually. Training is an iterative process of minimizing the differences between current actual output of the network and the desired output in the output layer. The desired output in the current study is the label of deceptive or truthful assigned to each data sample beforehand. The actual output is the output label automatically generated by the neural network after performing some kind of transformations such as sigmoid. The difference between the actual output and the desired output is calculated and back-propagated to the previous layer(s), which causes a series of adjustments of connection weights in such a way so as to reduce the observed output errors <ref type="bibr" target="#b31">[33]</ref>. The above process is repeated for each sample in the training data set, and then repeated for the whole data set, until prespecified stopping conditions are met. The stopping condition could be either the maximum acceptable error rate or an arbitrary number of repetitions on the whole data set.</p><p>In the construction of neural networks, the number of hidden layers and the number of nodes in each layer are important decisions. Since increasing the number of hidden layers greatly increases the complexity of neural networks and requires larger data samples, the use of one hidden layer is very common. It is also a common practice to start the number of nodes in the hidden layer with (I + O)/2, where I and O are the numbers of input and output variables, respectively, and then adjust the number gradually to find the optimal one.</p><p>In the neural network, each input is indirectly connected to each output via all the units in the hidden layer. Due to the highly nonlinear structure of the neural network, there is no well-defined method to easily interpret the relative strength of each input to each output in the network. The sensitivity of each output to the small perturbation in each input may provide a good estimation of the relationship between the two. Based on the algorithm in Engelbrecht and Cloete <ref type="bibr" target="#b16">[18]</ref>, we developed an application to estimate the relative strength of each input variable to the output by calculating the partial derivatives of the output unit o with respect to each input unit x i (i = 1, . . . , I) while holding fixed the values of all other input variables x k , k ≠ i.</p><formula xml:id="formula_2">i i N N L j i p p j i j i y o o s N x N y x 1 1 1 1 1 , = = = ∂ ∂ ∂ = = ∂ ∂ ∂ ∑ ∑∑ (<label>3</label></formula><formula xml:id="formula_3">)</formula><p>where N is the number of input samples and L is the number of nodes in the hidden layer.</p><p>The partial derivative terms in formula (3) can be replaced with the multiplication of connection weights and derivatives of the activation function such as the sigmoid function. Thus, given the same perturbation in x i , greater sensitivity is achieved when the change in derivatives of the activation function is greater.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Empirical Comparisons Data Set Collection</head><p>EXPERIMENTAL DATA WERE COLLECTED from subjects performing a common decision-making task, the desert survival problem (DSP) <ref type="bibr" target="#b23">[25]</ref>, which we updated and modified for our program of research. The problem focused on being stranded in the Kuwaiti desert. The primary goal for participants was to achieve a consensus ranking of the items to be salvaged from their overturned jeep in order of their usefulness to survival. The participants were recruited from a management information systems course at a large southwestern university who received extra credit for their participation. The participants were randomly paired in two-person groups and communicated with each other about the ranking via an e-mail system developed by the research team. The e-mail messages exchanged between a pair of partners were the major source of data for analysis.</p><p>The groups were randomly assigned to one of the experimental conditions: deception or truth. In the deception condition, the participants who logged in first (randomly) were instructed to deceive the other partner; in the truth condition, no special instructions were given. In both conditions, the participants who sent messages first were called senders, and the other participants in dyads were called receivers. No participant was aware of the condition of his or her partner. Two different data sets were collected. The procedures for each were similar in terms of task design, communication tools, and duration of the experiment (three days in total), with a few exceptions. In the first experiment, DSP1, participants were given a half-day time slot to communicate with their partners. For example, if participants A and B were partners and A sent messages between midnight and midday on the first, second, and third days, B responded between midday and midnight on each of the three days. In other Downloaded by [University of Nebraska, Lincoln] at 14:12 13 April 2015 words, each participant was to compose at most three messages. In the second experiment, DSP2, the restriction on the number of message exchanges was relaxed. Thus, a participant was allowed to send any number of messages. Moreover, in DSP1, salvageable items were removed from consideration on the second and third days, so that the task was altered in succeeding days; with DSP2, the task remained constant.</p><p>Table <ref type="table" target="#tab_2">2</ref> presents a descriptive summary of the data sets from DSP1 and DSP2. For purposes of the current analyses, only data from those members of each pair who were randomly designated as "senders" were included. The analyses thus constitute between-groups comparisons between senders in the respective deception and truth conditions. Data from receivers (partners) are not included.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis Method</head><p>Twenty-four (eighteen original plus six additional) cues indicative of deception were used as input variables to train the classification models. Since the cue selection was mainly based on the analysis of DSP1 data (see <ref type="bibr" target="#b43">[45]</ref>) and because of the similarity in the experimental design between DSP1 and DSP2, it was of interest to see whether the same cues remained effective for predicting deception in DSP2 data. When developing classification models, deceivers' data were used as positive samples, and truthful senders' data were selected as negative samples. Moreover, in order to determine which data unit (message or subject) could provide better predictive power, the classification methods were tested using both the collection of individual messages and the averages of each subject's messages. The first data set is called message data, and the second is labeled subject data. In the latter case, the estimate of each cue in individual messages was averaged by subject.</p><p>It is informative to train a classification method and measure its performance on the same data set. Testing classification models with the training data can reflect the ability to capture the underlying relationships between the input and output. It is more important, however, to evaluate the performance of classification methods on similar new data <ref type="bibr" target="#b13">[14]</ref> because of the interest in the level of generalization and predictive power of the models. Therefore, each data set was randomly split into ten subsets. Nine of the subsets were selected to train the classification models, and the remaining subset was used to assess the performance of the models. In order to avoid the bias from a single split of data and get an objective measure of performance of classification methods, the split procedure is repeated ten times by choosing one of the subsets as the testing data and the other nine as the training data. Finally, the classification performance was measured by averaging the performances in the ten tests. The entire procedure is called tenfold cross-validation. This procedure was applied to testing all of the classification methods except discriminant analysis. For discriminant analysis, however, cross-validation was conducted by classifying each case with the functions derived from all cases other than that case in a process called leave-one-out crossvalidation. Logistic regression models were evaluated using both tenfold and leaveone-out cross-validation approaches. On the one hand, logistic regression is as common a statistical approach as discriminant analysis. On the other hand, the logistic function and sigmoid function in BP neural networks are identical. In order to compare different methods on an equal footing, we applied both validation approaches to logistic regression methods.</p><p>The classification performance of the four methods was compared on three measures: (1) overall performance-overall percentage of accurate classification; (2) deception performance-percentage of deceptive messages accurately classified, and (3) truth performance-percentage of truthful messages accurately classified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Architecture of Network and Size of Decision Tree</head><p>CONSIDERING THE SMALL-SIZED DATA SET and relatively large number of cues, the neural network was configured with one hidden layer. As stated before, determining the number of nodes in the hidden layer is an important issue in configuring neural networks. A small number of nodes in the hidden layer not only reduces the computation complexity but also prevents overfitting the data. Based on the testing, three and one appeared to be a good choice for the number of nodes in the hidden layer for message and subject data of DSP2, respectively, and three for both message and subject data of DSP1.</p><p>The mean size of decision trees induced from message data of DSP2 included 18 leaves, 13 layers, and 35 nodes. After pruning, the tree size was reduced to 4 leaves, 3 layers, and 7 nodes. Nonetheless, the issue was greatly mitigated in the subject data of DSP2 because the size of the decision tree for subject data was reduced from 7 nodes to 3 nodes by pruning. A similar pattern occurred in the DSP1 data. The size of a decision tree suggests the difficulty of classification and the number of target classes. If the tree size is greatly reduced after pruning, it may indicate that the data are less statistically reliable <ref type="bibr" target="#b11">[12]</ref>. Message data had a large initial tree and a much smaller pruned one, which revealed that there was a high degree of residual variation in these data sets. The small reduction in the size of trees for subject data suggested that subject data may be more statistically reliable. Downloaded by [University of Nebraska, Lincoln] at 14:12 13 April 2015</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classification Performance of the Four Methods</head><p>The performances of the four classification methods are shown in Table <ref type="table">3</ref>. To reflect the level of degradation in classification performance with holdout samples, we list the results on the training data along with the cross-validation results. In order to gain a deeper understanding of the classification performance on deception data as opposed to truth data, Table <ref type="table">3</ref> also shows the deception performance and truth performance with each classification method.</p><p>Overall, it can be observed that there were obvious reductions in the classification performance for the cross-validation compared with that for training. Three out of four methods could differentiate between deceptive subjects and truthful subjects from the training data nearly perfectly. However, when they were tested on the holdout data in cross-validations, the performances degraded substantially. It highlighted the great variability in the data set, pointing to the difficulty of predicting deception.</p><p>In comparing the training performance of classification models between subject and message data, it is notable that the former was superior to the latter. It implies that there may be more noise in the message data, which is difficult for classification models to capture. Despite the smaller size of subject data, the aggregate nature of subject data may have contributed to its lower variability.</p><p>The generalizability of classification models is reflected in the cross-validation performance. The results showed that the overall performance on subject data varied greatly for DSP2, ranging from a respectable 61.5 percent for neural network methods to 46.2 percent for decision trees, which is no better than chance, and ranging from a respectable 76.7 percent by neural network methods to 53.3 percent by both decision tree and discriminant analysis methods for DSP1. However, there were relatively smaller differences in the predictability power for message data, ranging from 66 percent by neural networks to 55.3 percent by decision trees for DSP2, and from 79.2 percent by neural networks to 66.7 percent by discriminant analysis for DSP1. The overall low performances across classification methods indicated that some noise existing in the data may have prevented the classification methods from achieving optimal performance. It underscored the need for screening the input variables by eliminating those variables that have little influence on the classification results.</p><p>The side-by-side comparison between the performance on deception data and truth data revealed that, in general, deception was more accurately identified than truth with few exceptions. It means that the percent of deception data predicted by the classification methods that were truly deceptive was higher than the percent of truth data that were actually truthful. This indicates a false-positive bias in these classification models toward declaring truthful messages or subjects as deceptive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Important Cues Selected by the Four Approaches</head><p>Identifying important cues has theoretical, practical, and methodological implications. There are a host of theories on social interaction that may be applicable to the deception research. Conversely, significant cues identified in this study may provide Downloaded by [University of Nebraska, Lincoln] at 14:12 13 April 2015  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Notes:</head><p>For cross-validation in logistic regression, for example, 53.8/57.7, the first number was based on the leave-one-out cross-validation, and the second number was based on tenfold cross-validation. Downloaded by [University of Nebraska, Lincoln] at 14:12 13 April 2015 new evidence in support of existing theories. Moreover, the discovered important cues may be applied to detecting deception in practice. Furthermore, reducing the size of input variables may improve the performance of classification methods.</p><p>All of the four methods have some built-in mechanisms to assist selecting important cues from the original cue list. For example, the β coefficient and/or p-value from the discriminant analysis and logistic regression models are directly suggestive of the discriminatory power of a cue. The attributes included in the pruned decision trees allow for more information gains to make classification decisions. The input variables of neural network models to which the output is more sensitive are indicative of their importance to predicting the output. Based on the above cue-selection criteria, important cues were selected for each data set, as shown in Table <ref type="table">4</ref>.</p><p>It is notable from Table <ref type="table">4</ref> that there is considerable overlap on the selected cues between discriminant analysis and logistic regression. For example, verbs, content diversity, and highly positive pleasantness were identified as significant cues for subject data by the two methods. Since both methods essentially use maximum likelihood estimation in selecting significant cues, it is not surprising that the selected cues were similar to each other. Decision trees and neural networks use different criteria in choosing important cues, consequently assigning higher weights to different sets of cues. Among the cues resulting from the four methods, those from decision trees were most distinct. For example, redundancy and perceptual information were considered as important for subject data only by the decision tree model. The most information gains required by decision trees resulted in the most reduction in uncertainty after splitting the data sets, which was divergent from the criteria employed in other approaches.</p><p>It is common in deception research to select cues to deception by examining their statistical significance. It is a good practice if we use statistical approaches for prediction. However, it should be cautioned that important cues identified by statistical methods might not be appropriate for machine learning models. On the other hand, cues that are important for machine learning models may not have statistical power. For example, spatiotemporal information and positive pleasantness selected by the two machine learning approaches for message data were not statistically significant. It highlights the difference between the highly nonlinear model of neural networks and the linear model of discriminant analysis.</p><p>There was a small overlap on the selected cues between the two data sets, as shown in Table <ref type="table">4</ref>. It is argued that deception is moderated by many factors, such as communication medium <ref type="bibr" target="#b18">[20]</ref>, interactivity <ref type="bibr" target="#b3">[4]</ref>, context of relationship and motivations [15], and so on. Due to the slight difference in experimental design between DSP2 and DSP1, the significance of a cue varied from one to the other. For example, content diversity was very important for DSP2 only, but pausality was important for DSP1 only. Nonetheless, some cues consistently emerged as important for both data sets, such as verbs, modal verbs, typo ratio, and highly positive pleasantness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classification Performance with the Important Cues Only</head><p>Reducing a large set of cues to a small set of important cues may bring multifold benefits to the classification models. complexity. Second, it may reduce the error rate and improve the classification performance. Third, it avoids overfitting the data and increases generalizability of the resulting models. The performances of classification models using important cues are listed in Table <ref type="table" target="#tab_4">5</ref>. To gain a better understanding of the change in classification performance before and after adopting important cues, the cross-validation results for DSP1 data sets using all the original cues and using only important cues are displayed in Figure <ref type="figure" target="#fig_3">3</ref>. The DSP2 data set shows a similar pattern.</p><p>As shown in Table <ref type="table" target="#tab_4">5</ref> and Figure <ref type="figure" target="#fig_3">3</ref>, cross-validation results were consistently improved with the reduced set of cues. They illustrate the advantage of selecting cues that increase the generalizability of the classification models. A smaller set of important cues was superior to the larger set of cues in predicting deception.</p><p>With important input variables only, the neural network outperformed the other methods in predicting deception for DSP2 subject data with 88.5 percent precision, while the decision tree with 65.4 percent was the worse. The classification performances for DSP1 subject data were similar across different methods, with logistic regression and neural network methods achieving 83.3 and 80 percent accuracy, respectively. The performances for message data were also similar across the classification methods despite the data set, with only one exception. Decision tree performed significantly poorer on the DSP2 data set with 57.4 percent precision. Comparison of the two statistical methods suggested that the logistic regression and discriminant analysis methods performed equally well on DSP2 data, but the former was relatively better on the DSP1 data. Between the two machine learning methods, neural networks achieved similar performances on the DSP1 data to decision trees, but the former was far better on the DSP2 data.</p><p>To sum up, all the classification methods appeared to be on par on predicting deception from DSP1 data. For DSP2 data, three methods showed similar performance, the exception being decision trees. In addition, the pattern that deception performance was higher than truth performance was consistently exhibited in the results of logistic regression and neural network models across data sets. However, the pattern received mixed support for discriminant analysis and decision tree approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>NO SINGLE METHOD EMERGED AS SUPERIOR for predicting deception. All of the four methods under investigation are potentially good alternatives if only important cues are included in the model. The average performances on DSP1 subject and message data were 77.9 percent and 70.5 percent, respectively, and the average performances on DSP2 subject and message data were 79.2 and 78.1, respectively. The results indicate that the methods not only were able to capture the underlying relationships in the deception data but also showed good generalizability. The performances of discriminant analysis, logistic regression, and neural networks were consistent across data sets, while decision trees showed poorer performance on DSP2 data. Their low performance indicated that there was great variability within the data set. However, the performance of Downloaded by [University of Nebraska, Lincoln] at 14:12 13 April 2015 decision trees may be improved by improving tree pruning techniques <ref type="bibr" target="#b29">[31]</ref>. Thus, we are not in a position to conclude that decision trees are inferior based on one case. Neural networks are generally good at representing a high degree of nonlinear relationships between input and output. The close call between the neural network and other approaches implies that the level of nonlinearity in the experimental data were low. It is further confirmed by referring to the size of neural network models for both data sets, including either one or three nodes in the hidden layer. The size of neural networks indicates the level of interaction between the input variables <ref type="bibr" target="#b11">[12]</ref>. Thus, it was not surprising that methods favoring linear or one-dimensional relationships were still effective. The near linearity might have resulted from our removing highly correlated data when refining the original list of cues.</p><p>With small margins, neural networks exhibited better performance than the other methods. The significance of the difference will be tested with future data sets. More importantly, neural networks were consistently reliable across all of test settings. As shown in Figure <ref type="figure" target="#fig_3">3</ref>, the performances of neural networks on two data sets, either before or after selecting important cues were close to one another. Although both discriminant analysis and logistic regression performed equally well as neural networks on training data, their performances in various test settings were much more dispersed. For example, before selecting important cues, the overall performance of discriminant analysis on subject data of DSP1 was 53.3 percent, which jumped to 76.7 percent afterward. Furthermore, as statistical methods, both discriminant analysis and logistic regression methods assume that the underlying data possess certain features; otherwise, they may not fit the data. An advantage of neural networks is that they are quite robust across applications and data sets.</p><p>The improvement of the cross-validation results after pruning input variables underlines the importance of identifying important cues. For example, the overall performance of discriminant analysis and on DSP2 subject data climbed 38.5 percentage points with the pruned input. Selecting a smaller set of important cues as input not only directly reduced the computational complexity of classification models but also removed noise that is negatively associated with classification accuracy.</p><p>Compared with the classification performances on message data, those on subject data were generally better or as good, as shown in Table <ref type="table" target="#tab_4">5</ref>. It suggests that subjects (or an aggregated estimation of subjects' behavior over the entire course of an interaction) instead of messages may be a better unit for deception analysis. This argument is supported by the propositions in IDT that deceivers may strategically manage their behavior by occasionally taking a low-key stance and making their behavior look like truth tellers'. It is also supported by interaction adaption theory's claim that communication partners may reciprocate or compensate their behavior over the course of interaction. Thus, an individual message from a deceiver may not always exhibit deceptive behavior. On the contrary, if we combine a series of messages from a deceiver, it may bring deceptive behavior to light that is otherwise hidden in individual messages. Deception may not be reflected in one-point behavior, but is more likely to be unveiled from a series of behaviors <ref type="bibr" target="#b42">[44]</ref>. One shortcoming of the current analysis is that the sample sizes of the subject data were much smaller than those of the message data. Both machine learning and statistical methods favor large data sets. With increases in sample size, the advantage of using subject data rather than message data should be more pronounced. Nonetheless, when there are only a few messages exchanged by a deceiver, individual messages may also be able to provide good indications of deception.</p><p>The mixed patterns of higher deception performance for some of the methods did not conform to the findings from a prior study showing that the detection accuracy was much higher on truth than on deception <ref type="bibr" target="#b6">[7]</ref>. The test results even displayed a tendency toward an opposite pattern. Compared with the prior study that was conducted in face-to-face settings, the current investigation involved a different context and availability of cues to deception. To reach the end of predicting deception, deception performance may be more important than truth performance. Therefore, the linguistic cues accessible in CMC, as examined in this study, may be helpful for improving Downloaded by [University of Nebraska, Lincoln] at 14:12 13 April 2015 the accuracy of detecting deception. As a caution, however, methods that do well in detecting deception may also yield too many false alarms. Thus, there is potential risk in favoring such approaches. Of course, firm conclusions should not be drawn on the basis of a single study, but the results at least show that deception performance may be better than truth performance in some contexts.</p><p>In order to enrich and provide new evidence for existing cues to deception, we also identified the directions of important cues. As predicted in the prior research, deceivers used more modal verbs and fewer individual references, making their utterances more tentative and nonspecific <ref type="bibr" target="#b27">[29]</ref>. Due to the negative experience associated with deception, deceivers tend to disassociate themselves from their messages and display higher non-immediacy in their language. Moreover, average word length, an indication of complexity, was lower for deceivers than for truth tellers. Deception is known as a cognitively taxing process. As a result of greater cognitive demands when fabricating plausible and consistent messages, deceivers tend to resort to less complex language in communication to save some effort. In contrast with the prior findings that the quantity of deceptive messages is less than that of truthful messages, we found the opposite. This is in line with the patterns from a prior study using a similar decision-making task <ref type="bibr" target="#b43">[45]</ref>. Deceivers in the current study may have given more elaborate reasons for their rankings in order to gain credibility from communication partners and to achieve their deception goals. In this case, deceivers intent on being persuasive are likely to use more verbs and modifiers, and more expressive language. Most of the literature, however, focuses on statements of fact or recollections, such as criminal statement analysis, when deceivers do not have the same details to put into messages as truth tellers do. Under such circumstances, different linguistic patterns were expected. Moreover, in the current study, deceivers tended to include more sensory, spatial, and temporal details than truth tellers, which is opposite to the propositions in reality monitoring <ref type="bibr" target="#b22">[24]</ref> and criteria-based content analysis <ref type="bibr" target="#b33">[35]</ref>. Since the two criteria were based on the assumption that there is a real event or past memory for participants to resort to, the hypothetical context of desert survival may have caused a complete reversal of the above pattern. Therefore, when there is lack of support of real experience, more sensory and spatial details may indicate deception rather than truth. Imagery is a newly added cue in this study. On average, deceivers (M = 1.58) showed lower imagery ratio than truth tellers (M = 1.69) in DSP2. Compared with the previous finding on content diversity <ref type="bibr" target="#b43">[45]</ref>, DSP2 revealed an opposite direction. Instead of being lower, the content diversity of deceivers was slightly higher than truth tellers on DSP2. As stated before, we relaxed the restriction on the number of message exchanges in DSP2, which might have accounted for the discrepancy. When deceivers are given the opportunity to interact as many times as they want to, they tended to use different words to enhance their persuasiveness. The high content diversity may also be a result of deceivers' changing the subject to avoid being suspected, or it may be a result of creating shorter messages. Driving toward the goal of reaching consensus on the assigned task, truth tellers were likely to repeat some topic words in continuous communication.</p><p>The power of back-propagation neural networks lies in their ability to represent Downloaded by [University of Nebraska, Lincoln] at 14:12 13 April 2015 complicated, and highly nonlinear, relationships. They therefore have great potential. Whether or not this potential is realized depends, to a large extent, on the quality of data used to train them. It is premature to conclude the effectiveness of any classification methods at this point. Further investigation with larger data sets will give us deeper insight into the intra-relations of cues as well as the robustness of different classification methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Downloaded by [University of Nebraska, Lincoln] at 14:12 13 April 2015</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Screen Shot of GATE with Verb Phrases Highlighted on a Message from the Experimental Task.</figDesc><graphic coords="10,138.00,113.00,336.00,241.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Example Decision Tree Created with Data from Experimental Task. Boxes and arrows show path taken by a single example message to its classification.</figDesc><graphic coords="13,138.00,113.00,336.00,189.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Table 3 .</head><label>3</label><figDesc>Summary of the Performances of Classification Methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>First, it can directly lower the computation Downloaded by [University of Nebraska, Lincoln] at 14:12 13 April 2015 Table 4. Important Cues Selected by the Classification Models. and "2" indicate that the cue was selected for DSP1 and DSP2, respectively. Downloaded by [University of Nebraska, Lincoln] at 14:12 13 April 2015</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Cross-Validation Performance on DSP1 Data Sets. Note: (a) subject_before: performance on subject data before selecting important cues; message_after: performance on message data after selecting important cues leave-one-out cross-validation results for logistic regression models are included.</figDesc><graphic coords="23,312.00,287.00,162.00,128.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Summary of Linguistic Constructs and Their Component DependentVariables and Measures (Adapted from<ref type="bibr" target="#b43">[45]</ref>).</figDesc><table><row><cell>Quantity</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">1. Verbs: words that are the grammatical center of a predicate and express act,</cell></row><row><cell cols="5">occurrence, or mode of being.</cell></row><row><cell cols="5">2. Modifiers: adjectives and adverbs that describe words or makes the meaning of the</cell></row><row><cell cols="3">words more specific.</cell><cell></cell></row><row><cell>Complexity</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>total number of words</cell></row><row><cell cols="4">3. Average sentence length:</cell><cell>total number of sentences</cell><cell>.</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">total number of characters</cell></row><row><cell cols="3">4. Average word length:</cell><cell cols="2">total number of words</cell><cell>.</cell></row><row><cell>5. Pausality:</cell><cell cols="4">total number of punctuation marks total number of sentences</cell><cell>.</cell></row><row><cell>Expressivity</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">10. Emotiveness:</cell><cell cols="3">total number of adjectives + total number of adverbs total number of nouns + total number of verbs</cell><cell>.</cell></row><row><cell>Diversity</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">total number of different content words or terms</cell></row><row><cell cols="3">11. Content diversity:</cell><cell cols="2">total number of content words or terms</cell><cell>,</cell><cell>where content</cell></row><row><cell cols="5">words or terms primarily express lexical meaning.</cell></row><row><cell cols="2">12. Redundancy:</cell><cell cols="3">total number of function words total number of sentences</cell><cell>,</cell><cell>where function words express primarily</cell></row><row><cell cols="4">grammatical relationships.</cell></row><row><cell>Informality</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">13. Typographical error ratio:</cell><cell>total number of misspelled words total number of words</cell><cell>.</cell></row></table><note><p><p><p>Uncertainty 6. Modal verbs: auxiliary verbs that are used with a verb of predication and express a modal modification. 7. Passive voice: a form of the verb used when the subject is being acted upon rather than doing something.</p>Non-immediacy 8. Individual references: singular first and second personal pronoun.</p>9. Group references: first personal plural pronoun.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>(Continued)    </figDesc><table><row><cell>Specificity</cell></row><row><cell>14. Spatio-temporal information: information about locations of people or objects, or</cell></row><row><cell>information about when the even happened or explicitly describes a sequence of</cell></row><row><cell>events.</cell></row><row><cell>15. Perceptual information: indicates sensorial experiences, such as sounds, smells,</cell></row><row><cell>physical sensations, and visual details</cell></row><row><cell>Affect</cell></row><row><cell>16. Affect: conscious subjective aspect of a emotion apart from bodily changes</cell></row><row><cell>17. Pleasantness: positive or negative feelings associated with the emotional state.</cell></row><row><cell>18. Activation: the dynamics of emotional state.</cell></row></table><note><p>19. Imagery: words that provide a clear mental picture.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Summary of Data from the DSP Experiments.</figDesc><table><row><cell>Total number of</cell><cell>DSP1</cell><cell>DSP2</cell></row><row><cell>Messages</cell><cell>180</cell><cell>204</cell></row><row><cell>Subjects</cell><cell>60</cell><cell>52</cell></row><row><cell>Deceptive messages</cell><cell>48</cell><cell>55</cell></row><row><cell>Deceptive subjects</cell><cell>16</cell><cell>13</cell></row><row><cell>Truthful messages (senders)</cell><cell>48</cell><cell>39</cell></row><row><cell>Truthful subjects (senders)</cell><cell>16</cell><cell>13</cell></row><row><cell>Female:Male ratio</cell><cell>1.36:1</cell><cell>1:2.25</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Summary of Classification Performance with Important Cues Only.</figDesc><table><row><cell></cell><cell>Neural</cell><cell>networks</cell><cell>Cross-</cell><cell>Training validation</cell><cell></cell><cell></cell><cell>100 80</cell><cell>92.7 80.2</cell><cell></cell><cell>100 71.4</cell><cell>95.3 79.1</cell><cell></cell><cell>100 87.5</cell><cell>90.6 81.1</cell><cell></cell><cell></cell><cell>100 88.5</cell><cell>79.8 74.5</cell><cell></cell><cell>100 85.7</cell><cell>70.8 69.2</cell><cell></cell><cell>100 91.7</cell><cell>89.1 78.2</cell></row><row><cell>Downloaded by [University of Nebraska, Lincoln] at 14:12 13 April 2015</cell><cell>Discriminant Logistic Decision</cell><cell>analysis regression trees</cell><cell>Cross-Cross-Cross-</cell><cell>Training validation Training validation Training validation</cell><cell></cell><cell></cell><cell>86.7 76.7 100 83.3/83.3 96.7 76.7</cell><cell>76 74 82.3 78.1/79.2 95.8 79.1</cell><cell></cell><cell>78.6 64.3 100 78.6/78.6 92.9 71.4</cell><cell>76.7 74.4 74.4 69.8/72.1 90.7 79.1</cell><cell></cell><cell>93.8 87.5 100 87.5/87.5 100 81.3</cell><cell>75.5 73.6 88.7 84.9/84.9 100 79.2</cell><cell></cell><cell></cell><cell>84.6 80.8 84.6 76.9/76.9 96.1 65.4</cell><cell>75.7 72.3 75.5 72.3/74.4 76.6 57.4</cell><cell></cell><cell>84.6 76.9 84.6 76.9/76.9 100 66.7</cell><cell>76.9 74.4 73.5 69.7/72.7 73 48.1</cell><cell></cell><cell>84.6 84.6 84.6 76.9/76.9 92.9 64.3</cell><cell>74.5 70.9 76.7 73.8/75.4 78.9 61.2</cell></row><row><cell></cell><cell>Classification</cell><cell>methods</cell><cell>Test</cell><cell>methods</cell><cell>(a) DSP1</cell><cell>Overall performance</cell><cell>Subject</cell><cell>Message</cell><cell>Truth performance</cell><cell>Subject</cell><cell>Message</cell><cell>Deception performance</cell><cell>Subject</cell><cell>Message</cell><cell>(b) DSP2</cell><cell>Overall performance</cell><cell>Subject</cell><cell>Message</cell><cell>Truth performance</cell><cell>Subject</cell><cell>Message</cell><cell>Deception performance</cell><cell>Subject</cell><cell>Message</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Downloaded by [University of Nebraska, Lincoln] at 14:12 13 April 2015</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>Downloaded by [University of Nebraska, Lincoln] at 14:12 13 April 2015 Downloaded by [University of Nebraska, Lincoln] at 14:12 13 April 2015</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments: The authors thank Fang Chen for her help in collecting the experimental data for DSP2. Portions of this research were supported by funding from the U.S. Air Force Office of Scientific Research under the U.S. Department of Defense University Research Initiative (Grant F49620-01-1-0394). The views, opinions, and findings in this report are those of the authors' and should not be construed as an official Department of Defense position, policy, or decision.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Detecting deception: The relationship of available information to judgmental accuracy in initial encounter</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Bauchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human Communication Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="253" to="264" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Interpersonal deception theory</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Buller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Burgoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communication Theory</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="203" to="242" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Detecting deception through linguistic analysis</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Burgoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Blair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Nunamaker</surname></persName>
		</author>
		<author>
			<persName><surname>Jr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First NSF/NIJ Symposium on Intelligence and Security Informatics</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Miranda</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Zeng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Demchak</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Schroeder</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Madjusudan</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="91" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Does participation affect deception success? A test of the interactivity principle</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Burgoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Buller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Floyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human Communication Research</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="503" to="534" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Writing style as predictor of newspaper readership, satisfaction and image</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Burgoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Burgoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wilkinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journalism Quarterly</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="225" to="231" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adaptation in dyadic interaction: defining and operationalizing patterns of reciprocity and compensation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Burgoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dillman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Stern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communication Theory</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="295" to="316" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Interpersonal deception V: Accuracy in deception detection</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Burgoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Buller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Ebesu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rockwell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communication Monographs</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="303" to="325" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Channel expansion theory and the experiential nature of media richness perceptions</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Zmud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Academy of Management Journal</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="153" to="170" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The analysis of relationships involving dichotomous dependent variables</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Cleary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Angel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Health and Social Behavior</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="334" to="348" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A general architecture for text engineering</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cunningham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computers and the Humanities</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="223" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A framework and graphical development environment for robust NLP tools and applications</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Maynard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bontcheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Tablan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Proceedings of the Fortieth Anniversary Meeting of the Association for Computational Linguistics</title>
		<meeting>the Fortieth Anniversary Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-07">July 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural networks, decision tree induction and discriminant analysis: An empirical comparison</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Curram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mingers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Operational Research Society</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="440" to="450" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Organizational information, message richness and structural design</title>
		<author>
			<persName><forename type="first">R</forename><surname>Daft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Science</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="554" to="571" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Large automatic learning, rule extraction, and generalization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wittner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Solla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jackel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hopfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Depaulo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">I</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Lassiter</surname></persName>
		</author>
		<idno>14:12 13</idno>
	</analytic>
	<monogr>
		<title level="j">Complex Systems</title>
		<editor>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Schlenker</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="323" to="370" />
			<date type="published" when="1985">1987. April 2015 15. 1985</date>
			<publisher>McGraw-Hill</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note>Deceiving and detecting deceit</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Lying in everyday life</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Depaulo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Kashy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Kirkendol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Wyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Epstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="979" to="995" />
			<date type="published" when="1996-05">May 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cues to deception</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Depaulo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Lindsay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Malone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Muhlenbruck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Charlton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cooper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page" from="74" to="112" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A sensitivity analysis algorithm for pruning feedforward neural networks</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Engelbrecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Cloete</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Neural Networks</title>
		<meeting>IEEE International Conference on Neural Networks<address><addrLine>Los Alamitos, CA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="1274" to="1277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The use of multiple measurements in taxonomic problems</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals Eugenics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="188" />
			<date type="published" when="1936">1936</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Electronic lies: Lying to others and detecting lies using electronic media</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Carlson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Fifth Americas Conference on Information Systems. Atlanta, GA: AIS</title>
		<meeting>Fifth Americas Conference on Information Systems. Atlanta, GA: AIS</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="612" to="614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Logic and conversation</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Grice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Syntax and Semantics</title>
		<title level="s">Speech Acts</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Cole</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Morgan</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1975">1975</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="41" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Broadening the deception construct</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Bell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quarterly Journal of Speech</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="288" to="302" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Applied Multivariate Statistical Analysis, 5th ed</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Wichern</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Prentice Hall</publisher>
			<pubPlace>Upper Saddle River, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The cognitive interview and the assessment of the credibility of adults&apos; statements</title>
		<author>
			<persName><forename type="first">G</forename><surname>Köhnken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Schimossek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Aschermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Höfer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Psychology</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="671" to="684" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">The Desert Survival Problem</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Eady</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1974">1974</date>
			<publisher>Experimental Learning Methods</publisher>
			<pubPlace>Plymouth, MI</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Text summarisation. Masters dissertation, Department of Computer Science, Imperial College</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Stylistic analysis</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Lynch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Methods of Research in Communication</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Emmert</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">D</forename><surname>Brooks</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1970">1970</date>
			<biblScope unit="page" from="315" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">A survey of uses of GATE</title>
		<author>
			<persName><forename type="first">D</forename><surname>Maynard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cummingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bontcheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Catizone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Demetriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Hamza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hepple</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Herring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Oakes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Setzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stevenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Tablan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ursu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wilks</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<pubPlace>University of Sheffield, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Non-immediacy between communicator and object of communication in a verbal message: Application to the inference of attitudes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mehrabian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wiener</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Consulting Psychology</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="420" to="425" />
			<date type="published" when="1966">1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An empirical comparison of pruning methods for decision-tree induction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mingers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="227" to="243" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An exploratory study on promising cues in deception detection and application of decision tree</title>
		<author>
			<persName><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Burgoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Nunamaker</surname></persName>
		</author>
		<author>
			<persName><surname>Jr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Seventh Annual Hawaii International Conference of System Sciences</title>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Sprague</surname><genName>Jr</genName></persName>
		</editor>
		<meeting>the Thirty-Seventh Annual Hawaii International Conference of System Sciences<address><addrLine>Los Alamitos, CA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>available at www.hicss.hawaii.edu/HICSS37/ apahome37.html</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
		<title level="m">C4.5: Programs for Machine Learning</title>
		<meeting><address><addrLine>San Mateo, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers</publisher>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumerlhart</surname></persName>
		</author>
		<author>
			<persName><surname>Mcclelland</surname></persName>
		</author>
		<title level="m">Parallel and Distributed Processing: Exploration in the Microstructure of Cognition</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">L</forename></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The less travelled road to truth: Verbal cues in deception detection in accounts of fabricated and self-experienced events</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Sporer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="373" to="397" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Criteria-based content analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Steller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Köhnken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Psychological Methods in Criminal Investigation and Evidence</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Raskin</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="217" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Managerial applications of neural networks: The case of bank failure predictions</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Y</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Kiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Science</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="926" to="947" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Detecting deceit via analysis of verbal and nonverbal behavior</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vrij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Nonverbal Behavior</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="239" to="263" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A dictionary of affect in language: IV. Reliability, validity, and applications</title>
		<author>
			<persName><forename type="first">C</forename><surname>Whissell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fournier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pelland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Makarec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perceptual and Motor Skills</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="875" to="888" />
			<date type="published" when="1986-06">June 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Adaptation and communicative design: Patterns of interaction in truthful and deceptive conversation</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Burgoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human Communication Research</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="9" to="37" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Winston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="1992">1992</date>
			<publisher>Addison-Wesley Longman</publisher>
			<pubPlace>Boston</pubPlace>
		</imprint>
	</monogr>
	<note>d ed</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<title level="m">Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations</title>
		<meeting><address><addrLine>San Francisco</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A comparison of discriminant analysis versus artificial neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>George Swales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Margavio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Operational Research Society</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="51" to="60" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A meta-analysis of financial applications of neural networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zahedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computational Intelligence and Organizations</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="164" to="178" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A longitude analysis of language behavior of deception in e-mail</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Burgoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Twitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First NSF/NIJ Symposium on Intelligence and Security Informatics</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Miranda</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Zeng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Demchak</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Schroeder</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Madhusudan</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="102" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">An exploratory study into deception detection in text-based computer-mediated communication</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Twitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Burgoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Nunamaker</surname></persName>
		</author>
		<author>
			<persName><surname>Jr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Thirty-Sixth Hawaii International Conference on System Sciences</title>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Sprague</surname><genName>Jr</genName></persName>
		</editor>
		<meeting>Thirty-Sixth Hawaii International Conference on System Sciences<address><addrLine>Los Alamitos, CA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note>available at www.hicss.hawaii.edu/HICSS36/apahome36.htm</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
