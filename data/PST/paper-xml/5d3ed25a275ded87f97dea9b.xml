<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mining Algorithm Roadmap in Scientific Publications</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hanwen</forename><surname>Zha</surname></persName>
							<email>hwzha@cs.ucsb.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
							<email>wenhuchen@cs.ucsb.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Keqian</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xifeng</forename><surname>Yan</surname></persName>
							<email>xyan@cs.ucsb.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Mining Algorithm Roadmap in Scientific Publications</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3292500.3330913</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Relation Extraction</term>
					<term>Taxonomy Construction</term>
					<term>Knowledge Base Construction</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The number of scientific publications is ever increasing. The long time to digest a scientific paper posts great challenges on the number of papers people can read, which impedes a quick grasp of major activities in new research areas especially for intelligence analysts and novice researchers. To accelerate such a process, we first define a new problem called mining algorithm roadmap in scientific publications, and then propose a new weakly supervised method to build the roadmap. The algorithm roadmap describes evolutionary relation between different algorithms, and sketches the undergoing research and the dynamics of the area. It is a tool for analysts and researchers to locate the successors and families of algorithms when analyzing and surveying a research field. We first propose abbreviated words as candidates for algorithms and then use tables as weak supervision to extract these candidates and labels. Next we propose a new method called Cross-sentence Attention NeTwork for cOmparative Relation (CANTOR) to extract comparative algorithms from text. Finally, we derive order for individual algorithm pairs with time and frequency to construct the algorithm roadmap. Through comprehensive experiments, our proposed algorithm shows its superiority over the baseline methods on the proposed task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The number of scientific publications is ever increasing. According to the prominent STM report <ref type="bibr" target="#b37">[38]</ref>, the number of journal articles published in 2014 alone approached 2.5 million and this number is still increasing on a yearly basis. The long time to digest a scientific paper posts great challenges on the number of papers a researcher can digest. Experienced researchers may be familiar to identify the demanded papers. However, the problem becomes much severe for intelligence analysts who need to browse papers and quickly grasp the major activities in new research areas. The novice researchers may have a similar obstacle in finding out papers related to their research. They usually take plenty of time to come up with keywords, retrieve and read relevant papers and iterate this process.</p><p>One step assisting with this process is taxonomy construction <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b41">42]</ref>, which extracts concepts from a collection of documents and builds a tree structure to describe the hierarchical relation between different concepts. Analysts and researchers can follow this concept hierarchy to quickly identify more desired keywords or documents. However, previous taxonomy construction methods mostly focus on isA relation. They either rely on pattern-based methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b31">32]</ref> which extract hierarchical relation leveraging linguistic features, or clustering-based methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b41">42]</ref>, which cluster concepts to induce an implicit hierarchy.</p><p>In this paper, we generate a graph called Algorithm Roadmap, focusing on a special type of concept -"algorithms", and its specific form -"abbreviations". Given a scientific corpus, we mine comparative algorithms (described in section 3), and construct a graph connecting mined algorithms. In Figure <ref type="figure">1</ref>, for example, a roadmap for algorithm Generative Adversarial Network (GAN) <ref type="bibr" target="#b8">[9]</ref>, describes its successors and competitors in the scientific literature. The generated algorithm roadmap captures the development of algorithms, sketches the undergoing research, and models the dynamics of an area. It serves as a tool for analysts and researchers to locate the successors and families of algorithms when doing analysis and survey.</p><p>Figure <ref type="figure">1:</ref> A pedagogical example of the algorithm roadmap for "GAN" algorithm.</p><p>Conclusively, there exist three major challenges for mining the algorithm roadmap in the scientific literature, corresponding to the label, entity, and relation respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation Type</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Instance</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Single Sentence</head><p>We train models using different GAN methods : WGAN-GP , WGAN with weight clipping and DCGAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Single Sentence</head><p>In almost all experiments BayesGAN outperforms DCGAN and W-DCGAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross Sentence</head><p>LapGAN proposed a Laplacian pyramid implementation of GANs. ... DCGAN used a deeper convolutional network. Cross Sentence GDL focuses on unsupervised learning. ... GAN and DCGAN show results for unsupervised learning and semi-supervised classification .</p><p>Table <ref type="table">1</ref>: Examples of comparative algorithms.</p><p>Label Scarcity: Collecting in-domain algorithm entities and relation labels in scientific publications are prohibitively expensive. Existing datasets or curated in-domain knowledge bases <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4]</ref> are rather small and frequently outdated with the development of science. Moreover, a newly invented algorithm probably only appears in a single paper. This scarcity raises a challenge for supervised and distantly supervised entity extraction methods like <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> or weakly supervised phrase extraction approaches relying on frequency <ref type="bibr" target="#b29">[30]</ref>. The low coverage of knowledge base can also influence the availability of relation labels when using distant supervision <ref type="bibr" target="#b21">[22]</ref>.</p><p>Entity: General entity recognition does not directly separate the algorithms with the others. Though using abbreviations as the representation of algorithms alleviates the problem of considering all types of entities, few types other than algorithm exists. In addition, the abbreviation, as a short form of text, is prone to ambiguity. Word sense disambiguation methods <ref type="bibr" target="#b22">[23]</ref> have been studied to disambiguate word senses, however, deciding the sense for the abbreviation in the scientific domain is still challenging when lack of labeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation:</head><p>The narrations of two comparative algorithms either lie in a single sentence or are distributed across sentences. For example, in Figure <ref type="figure">1</ref>, the comparative relation may be described in one sentence, e.g., "Algorithm A outperforms Algorithm B ...," or in multiple-sentences, e.g., "Algorithm A ... ; Algorithm B ...." Moreover, it is likely more than two algorithms are compared or more than two abbreviations appear in a paragraph. Additional abbreviations may convey a meaning related to comparative relation. Unsupervised pattern-based methods such as <ref type="bibr" target="#b13">[14]</ref> focus on isA relation, which are not suitable for finding compared algorithms. Most existing researches for the supervised relation extraction focus on single sentence relation extraction with an exception of <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b36">37]</ref>, which focus on general documents while not targeting on a specific narration of algorithm abbreviations and comparative relation. On the other hand, these supervised approaches require annotated corpora.</p><p>We propose a framework to mine the algorithm roadmap in scientific publications to tackle the previous raised challenges. It first extracts abbreviations with specific pattern as algorithm candidates. Then it leverages weak supervision from tables and text to create training data for comparative relation identification and entity typing. Next, it applies our proposed relation extraction method Cross-sentence Attention NeTwork for cOmparative Relation (CAN-TOR) to extract comparative algorithms in the text. It leverages words and abbreviations in the context, and jointly predicts the candidate types for addressing ambiguity during the roadmap construction. Finally, it connects the compared algorithms into a graph with time and frequency information.</p><p>Extensive experiments on three real-world datasets demonstrate our superior performance in finding the comparative relation. Our CANTOR model outperforms supervised and unsupervised baseline methods by a large margin. We perform case studies on the constructed algorithm roadmaps to further visualize the effectiveness of the construction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head><p>Knowledge base construction is a known technique for harvesting knowledge and storing facts in a structured format. The constructed knowledge base plays an important role in downstream applications such as information retrieval, question answering, and document analysis, etc. Most existing automatically constructed knowledge bases focus on general domain, which either extract facts from Wikipedia info-boxes <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b33">34]</ref> or harvest knowledge with specific linguistic patterns <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b38">39]</ref>. Taxonomy can be viewed as a tree-structure knowledge graph, where linked nodes have hierarchical relation. Plenty of methods have been proposed to extract these hierarchical relation, either leveraging linguistic patterns <ref type="bibr" target="#b13">[14]</ref> or hierarchical clustering of concepts which implicitly captures the hierarchical relation <ref type="bibr" target="#b41">[42]</ref>. These methods mainly focus on the general domain, harvesting common knowledge with pattern or statistics.</p><p>Many works focus on for mining scientific publications, for example, <ref type="bibr" target="#b0">[1]</ref> proposed a keyphrase and relation extraction competition for scientific publications, <ref type="bibr" target="#b3">[4]</ref> collected a dataset for scientific taxonomy construction, <ref type="bibr" target="#b12">[13]</ref> studied the evolution of scientific topics through dynamic topic models <ref type="bibr" target="#b20">[21]</ref> modeling implicit topics and obscure relations, and some technical reports 1 2 manually analyzed the development of areas such as artificial intelligence. Some of these works collected datasets for scientific publications, but the process is known to be expensive and the collected datasets are normally small in size.</p><p>Word sense disambiguation <ref type="bibr" target="#b22">[23]</ref> is a type of technique used to distinguish ambiguous word senses. They either disambiguate word senses with a sense inventory or distinguish super senses by clustering words. Inspired by methods using super senses, we use types as evidence to distinguish abbreviations. To leverage the constraint of abbreviations, we use predefined types as super senses for abbreviations.</p><p>Another line of work related to ours is relation extraction, which has attracted much attention from the community, while most of the works focus on news and web data <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b28">29]</ref>. Recent neural network based methods have achieved great success in relation extraction, including CNN-based approaches <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref> and LSTMbased approaches <ref type="bibr" target="#b30">[31]</ref>. These approaches all consider relations lying in a single sentence. On the other hand, most relation extraction works assume entities and relation sets are given in the datasets, while others apply distant supervision to link entity mentions <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b27">28]</ref> in the text to the knowledge base entities <ref type="bibr" target="#b18">[19]</ref> and acquire relation labels. Their weaknesses lie in the fact that they require either annotated corpora or well-covered knowledge bases.</p><p>Beside single-sentence relation approaches, some previous works exist on cross-sentence relation extraction. <ref type="bibr" target="#b25">[26]</ref> proposes to construct cross-sentence relation data for entities with minimal-span assumption. <ref type="bibr" target="#b24">[25]</ref> proposes to use a Graph-LSTM to encode the shortest path in the extracted dependency parse tree, where the tree roots of different sentences are linked together. <ref type="bibr" target="#b36">[37]</ref> proposes a method using self-attention <ref type="bibr" target="#b35">[36]</ref> and bi-affine scoring algorithm to predict biological relations between all mention pairs in the abstract simultaneously. Our work differs from them in three key ways. First, we leverage weak supervision from the paper rather than using annotated corpus or distant supervision from an external knowledge base. Second, we consider typing of entities for abbreviation ambiguity and roadmap construction. Third, we model both single-sentence and cross-sentence comparative relations with words and abbreviations in the context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARIES</head><p>In this paper, we mainly target at mining algorithm roadmap in scientific publications. In order to provide a better understanding of our paper, we first give definitions related to algorithm roadmap and then briefly overview our proposed method. Algorithm Roadmap. It is a directed acyclic graph G, where each node of the graph is an algorithm term in abbreviation form. Each directed edge e 1 → e 2 in the graph G represents a directed evolutionary relation between two algorithm nodes e 1 and e 2 . For example, in the computer science domain, there are algorithms such as GAN (Generative Adversarial Networks) <ref type="bibr" target="#b8">[9]</ref> and DCGAN (Deep Convolutional Generative Adversarial Networks) <ref type="bibr" target="#b26">[27]</ref>. A directed edge GAN → DCGAN represents "DCGAN" is a successor and is evolved from "GAN". Comparative Relation. It is a relation between two algorithms, which means two terms were compared with each other in some papers. For example, pair (GAN , DCGAN ) having comparative relation means "DCGAN" was compared to "GAN" in some papers, but there is no direction information implies which technique is a successor. Roadmap Construction. We are the first to mine algorithm pairs with comparative relation using weak supervision harvested from tables and texts. Moreover, we connect the compared algorithms into a directed graph G by deriving order with time and frequency information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXTRACTING COMPARATIVE RELATION</head><p>In this section, we present a framework to extract comparative algorithm pairs from papers. The framework consists of three steps: i) Extracting abbreviations as algorithm candidate mentions; ii) Applying weak supervision from tables and texts to create training data for comparative relation and typing; iii) Learning to predict the relation for candidate mention pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Candidate Mention Extraction</head><p>We use abbreviations as our algorithm candidates. The intuition of using abbreviations as algorithm candidates lies in two folds: entity and relation label availability.</p><p>Lack of annotated corpus and well-covered in-domain knowledge bases, general entity recognition methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> do not fit our candidate mention extraction. With low occurrence frequency, phrase extraction approaches do not satisfy the job as well.</p><p>We observed that abbreviation is a commonly used representation of algorithm terms. With a unified form, it is easy to harvest from the corpus. More importantly, using abbreviations as candidates provides a possibility to gather supervision from tables for comparative relation, which we will show in section 4.3.</p><p>Abbreviations follow specific patterns and may refer to several types of meanings. For example, Table <ref type="table" target="#tab_0">2</ref> shows algorithms such as CNN (Convolutional Neural Network), datasets such as MNIST (Modified National Institute of Standards and Technology dataset), and metrics such as AUC (Area under curve). Types of a few abbreviations can be distinguished by checking the signal words following the abbreviation. For example, an algorithm abbreviation may be followed by algorithm, method, model etc. in the text.</p><p>We use regular expression with pattern consists of capital letters, lowercase letters, numbers, and hypen, to unsupervisedly harvest abbreviations as algorithm mention candidates from the text. We extract type of a few abbreviations identified by signal words to provide weak supervision for entity typing in section 4.2.5, unidentified abbreviations are randomly sampled as type Others. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Cross-Sentence Relation Extraction</head><p>We designed our model to incorporate both single-sentence and cross-sentence information, and consider all abbreviations in a paragraph. To this end, our model consists of a single-sentence module with Piecewise CNN <ref type="bibr" target="#b39">[40]</ref>, and a cross-sentence module which leverages self-attention to attend to all words capturing the paragraphlevel relation information, and abbreviation-attention to attend to all abbreviations helping describe the relation of the candidate pair. Moreover, typing is jointly done on the attended candidates to assist downstream roadmap construction. Mention pair predictions are pooled on single-sentence module and cross-sentence module for the entity pair prediction. Finally, the predictions of the two modules are interpolated with weights learned simultaneously with other parameters. PCNN is a variation of CNN that adopts piecewise max pooling in relation extraction. It divides the sentence into three segments: part before first entity, part between two entities and part after second entity. Thus each convolutional filter q i is divided into three segments (q i1 , q i2 , q i3 ). The max-pooling is performed on three segments separately, which is defined as</p><formula xml:id="formula_0">p i j = max(q i j ) 1 ≤ i ≤ n, 1 ≤ j ≤ 3 (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where n is the number of convolutional filters, and p i is equal to the concatenation of p i j over all segments j, which aggregates information from different parts. A non-linear layer is added on top of the sentence relational encoding which is represented by all filters p 1:r , to get the relation prediction:</p><formula xml:id="formula_2">o 1 = W 1 tanh(p 1:r ) + b 1 .<label>(2)</label></formula><p>4.2.3 Cross-Sentence Module. Our cross-sentence module focuses on finding paragraph-level comparative relation, where two algorithm mention candidates lie across sentences. We base on recent Transformer architecture <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b35">36]</ref> to build this module, due to its better performance in encoding long-distance context compared to Long Short Term Memory Networks (LSTMs) <ref type="bibr" target="#b14">[15]</ref> and Convolutional neural networks (CNNs).</p><p>Self-Attention. We adapt Transformer <ref type="bibr" target="#b35">[36]</ref> to encode word sequences in a paragraph, where we calculate the self-attention of the words, and use a convolutional layer in self-attention blocks similar to <ref type="bibr" target="#b36">[37]</ref> to alleviate the burden on the model to attend to local features. We add residual connections <ref type="bibr" target="#b11">[12]</ref> to both multihead attention and convolutional layers. The Transformer contains stacked layers of Transformer block, which contains its own set of parameters. The token embedding X = {x 1 , ..., x N } is fed to the first-layer transformer block and the output of the kth-layer block A k is calculated by</p><formula xml:id="formula_3">A k = A k + A k × so f tmax( A T k A k d Ak )<label>(3)</label></formula><p>where so f tmax(•) is a column-wise normalizing function, and d Ak is the dimension of the input token embedding of kth transformer block used for self-attention. A convolutional layer Conv with residual connection follows the self-attention layer:</p><formula xml:id="formula_4">H Ak = A k + Conv( A k ).<label>(4)</label></formula><p>We follow BERT <ref type="bibr" target="#b6">[7]</ref> which recently achieves great success in multiple NLP tasks, to add a special &lt;CLS&gt; token at the start of the paragraph and a special &lt;SEP&gt; token at the end of each sentence in the paragraph. The representation of &lt;CLS&gt; is used for gathering relational information in a paragraph. With self-attention layer, all other tokens in a paragraph attend to this &lt;CLS&gt; token. &lt;SEP&gt; is a special token stands for the end of a sentence, which is used to incorporate the sentence boundary information in the model.</p><p>Abbreviation-Attention. The abbreviation-attention layer calculates attention over all abbreviations in the sentences. When additional algorithms are also compared or share a similar relation to two candidates, two candidate mentions may have a high probability to be comparative.</p><p>Different from the self-attention mechanism, the abbreviation attention is calculated based on all abbreviations in a paragraph. Denoting all token embeddings of abbreviations as B, transformer blocks with a new set of parameters are applied. Similar to selfattention, with kth-layer input embedding B k , the kth-layer output of abbreviation attention B k is calculated as</p><formula xml:id="formula_5">B k = B k + B k × so f tmax( B T k B k d Bk ).<label>(5)</label></formula><p>Similarly a convolutional layer with residual connection is applied to the output of abbreviation-attention layer:</p><formula xml:id="formula_6">H Bk = B k + Conv( B k ).<label>(6)</label></formula><p>With abbreviation-attention layer, all the abbreviations in the sentences are attending to the algorithm candidates. The final output H Bk,e1 and H Bk,e2 of the algorithm candidates in H Bk are selected as the entity representation, which fuses all abbreviation information in the paragraph.</p><p>Character Embedding. Some of the abbreviations are rarely mentioned in the text, which may result in an insufficiently trained word embedding. Since the abbreviation is often created by summarized text, similar abbreviations probably imply overlapped word sequences. To leverage this intuition, we use character embedding that describes character-level information of abbreviations and we apply a character-level convolutional layer followed by a maxpooling layer to get a character-level abbreviation representation.</p><p>For an abbreviation with corresponding character embedding sequences C =&lt; c 1 , c 2 , ..., c n &gt;, we apply a convolution kernel followed by a max-pooling layer.</p><formula xml:id="formula_7">H c = max(Conv(c i )) 1 ≤ i ≤ n<label>(7)</label></formula><p>Fusion Layer. Finally, We use a single layer on top of the encoded paragraph representation H Ak, &lt;C LS &gt; and the algorithm candidate representation E to model their interaction. The candidate representation E is constructed by concatenating original word embedding X e1 , X e2 , character embedding H c,e1 , H c,e2 , attended abbreviation embedding H Bk,e1 , H Bk,e2 . The final fusion layer predicts a final relational score for one instance.</p><formula xml:id="formula_8">E = [X e1 , X e2 , H c,e1 , H c,e2 , H Bk,e1 , H Bk,e2 ]<label>(8)</label></formula><formula xml:id="formula_9">o 2 = W 2 ([H Ak, &lt;C LS &gt; , E]) + b 2 (9)</formula><p>4.2.4 Combined Relation Extraction. Predicting whether an algorithm candidate pair is compared forms a multi-instance learning problem <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b34">35]</ref>. For each pair, a bag of instances may contain two candidates. The entity-level prediction is an aggregation over multiple mention pair instances. Based on different assumptions, different weighting strategies have been proposed such as maxpooling <ref type="bibr" target="#b34">[35]</ref> and selective attention <ref type="bibr" target="#b16">[17]</ref>. We follow at-least-one assumption, where a positive example has at least one instance implies the comparative relation, and use max-pooling to select the instance with the maximum score for an entity pair in both single-sentence and cross-sentence module.</p><p>The final score of an algorithm candidate pair (e 1 , e 2 ) is a interpolation of the aggregated prediction score O 1 (z|S) of the singlesentence module and O 2 (z|S) of the cross-sentence module. The trainable weights λ 1 and λ 2 are jointly learned from the data to reflect the importance of single-sentence and cross-sentence part. The weights are limited to be positive and have a total sum of 1.</p><formula xml:id="formula_10">O(z|S) = λ 1 O 1 (z|S) + λ 2 O 2 (z|S) λ 1 , λ 2 &gt; 0, λ 1 + λ 2 = 1<label>(10)</label></formula><p>Finally, we use softmax to normalize the scores to get a probability distribution p z = so f tmax(O(z|S)), and relation prediction loss is defined as a cross-entropy loss:</p><formula xml:id="formula_11">L RE = − 2 i=1 y i • loд(p z,i )</formula><p>, where each y i ∈ {0, 1} indicates algorithm candidate pair relation is true for which class (without/with relation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.5">Entity Typing. Previous relation extraction modules do not</head><p>distinguish the types of the abbreviations. Few types other than algorithm exists, even though using abbreviations as algorithm candidates addresses the problem of candidate recognition. In addition, introducing abbreviations may increase chance of ambiguity. For example, "GAN" could be an algorithm (Generative Adversarial Network) but also a gene in biology. "CNN" could be an algorithm Convolutional Neural Network but also a television channel (Cable News Network).</p><p>Inspired by word sense disambiguation methods that label super sense types for word clusters <ref type="bibr" target="#b22">[23]</ref>, we jointly predict the types for abbreviation candidates with relation extraction task to distinguish abbreviations for downstream roadmap construction. Considering the limited types of abbreviation, we pre-define a fixed type inventory instead of using clustering and labeling word clusters.</p><p>We use a projection matrix W 3 on top of the attended algorithm candidate representation after abbreviation attention to predict the type of the candidate abbreviation, and the scores are normalized with softmax function: p t = so f tmax(W t H Bke ).</p><p>The type prediction loss also applies the cross-entropy loss:</p><formula xml:id="formula_12">L T P = − T i=1 y t,i • loд(p t,i )</formula><p>, where there are total T types and each y t i ∈ {0, 1} indicates the correctness for ith type.</p><p>We add a type constraint to the loss function, considering that a comparative relation only holds for candidates with the same type. For a compared algorithm candidate pair e1, e2 in the ground truth, a type constraint loss is defined as a kl-divergence of two predicted types, where</p><formula xml:id="formula_13">L T C = D K L (p t,e1 , p t,e2 ).</formula><p>The final score is a weighted sum of all the loss functions, with weights as hyper parameters.</p><formula xml:id="formula_14">L = γ 1 • L RE + γ 2 • L T P + γ 3 • L T C (11)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Weakly Supervised Training Data</head><p>The labels for comparative relation is hardly available from existing datasets and curated knowledge bases. We propose a weakly supervised approach based on our observation that in the same row or same column of the We randomly sample other non-positive candidate pairs as negative examples in training. To reduce the huge number of unrelated and non-informative negative examples, we follow the minimumspan strategy in <ref type="bibr" target="#b25">[26]</ref>, and limit sampled negative candidate pairs to the co-occurred pairs shown in a limited length of continuous sentences. Intuitively, most compared algorithms are kept since authors tend to describe compared algorithms coherently in a short paragraph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">GENERATING ALGORITHM ROADMAP</head><p>Previous comparative relation extraction step produces a large set of compared abbreviation pairs, and each pair corresponds to an undirected edge in algorithm roadmap. Our goal is to derive direction for the edge and connecting individual pairs.</p><p>The evolutionary relation has a strong association with the comparative relation. The publication time is a strong indicator of evolution direction for compared algorithms. We use first occurrence time in the corpus of an abbreviation as an approximation. For those pairs identified with the same occurrence time, we expect usually low frequent algorithm is evolved from high frequent one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Algorithm Roadmap Construction</head><p>Input: Comparative algorithm list P, time dictionary T , and frequency dictionary F Output: Algorithm roadmap G 1: for all (e1, e2) ∈ P do</p><formula xml:id="formula_15">2: if T [e1] &lt; T [e2] or (T [e1] = T [e2] and F [e1] &gt; F [e2]) then 3:</formula><p>G.add(e1 → e2) G.add(e2 → e1)</p><p>6: return G Example. Pairs "GAN" and "DCGAN" are mined compared algorithms. We locate their first appearance time, and find that "GAN" was published first in 2014, and "DCGAN" was first published in 2015. GAN (2014) → DCGAN (2015) is predicted as the direction where "DCGAN" is a successor.</p><p>When connecting individual pairs, only candidates above certain probability threshold are kept. In addition, candidates with different types in different pairs except for type Other are considered as separated nodes for roadmap construction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENT</head><p>The following section is organized in this way, first we describe datasets and implementation details, second, we show held-out and manual evaluation results of different methods in comparative relation extraction task, third, we perform case studies to visualize the constructed algorithm roadmaps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Dataset</head><p>We crawled papers from scientific conferences in domains including machine learning, natural language processing, and database. The corpora include papers in NeurIPS/NIPS (Annual Conference on Neural Information Processing Systems) from 1987 to 2017 <ref type="foot" target="#foot_3">4</ref> , ACL (Annual Meeting of the Association for Computational Linguistics) from 1974 to 2017 <ref type="foot" target="#foot_4">5</ref> , and VLDB (The Proceedings of the VLDB Endowment) from 2008 to 2017 <ref type="foot" target="#foot_5">6</ref> . The statistics of each of datasets is shown in Table <ref type="table" target="#tab_3">3</ref>  From these datasets, we extract algorithm candidate mentions, apply weak supervision to extract types from texts and comparative relation labels from tables as described in section 4. We split train and test data with a ratio of 80% and 20%. Among training data, 10% is separated as validation data. Additional implementation details are included in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Results</head><p>We conduct both held-out evaluation and manual evaluation on our method and several baseline methods in the task of comparative relation extraction, where models predict whether given candidate pairs are comparative or not. Evaluated methods can be divided to unsupervised methods including co-occurrence based methods <ref type="bibr" target="#b9">[10]</ref>, word-similarity based methods <ref type="bibr" target="#b19">[20]</ref>, and supervised relation extraction methods <ref type="bibr" target="#b39">[40]</ref>. The pattern-based method <ref type="bibr" target="#b13">[14]</ref> is not compared due to its low recall in our task.</p><p>Test set from weakly supervised table data is used for held-out evaluation. The evaluation is harsh due to the limited number of positive examples, and noisy because of few table parsing errors. In the manual evaluation, for each supervised method, we randomly sample 100 examples from their positive predictions and ground truth positive set in test data, and combine those into a unified manual test set. We let human annotators to label the pairs, where we do not distinguish compared algorithms, datasets or metrics following the criteria of our weak supervision. In the following we introduce evaluated methods in detail.</p><p>PCNN_single: Piecewise CNN model <ref type="bibr" target="#b39">[40]</ref>, which is one of the state of art single-sentence relation extraction methods. PCNN_single only uses single-sentence instances for candidate pairs.</p><p>PCNN_cross: The same PCNN model as PCNN_single where cross-sentence instances are also used.</p><p>Sent_cooccur: A method similar to co-occurrence method used in hypernym detection <ref type="bibr" target="#b9">[10]</ref>. Sent_cooccur calculates the co-occurrence frequency of candidate pairs in one sentence. A threshold that decides a positive-negative ratio closest to the ground-truth test data is used.</p><p>Doc_cooccur: Similar to Sent_cooccur, where the co-occurrence frequency in one document is used instead.</p><p>Word_similarity: A method predicts comparative relation score based on word embedding similarity, where the embedding is pretrained with the Skip-Gram model <ref type="bibr" target="#b19">[20]</ref>    <ref type="table" target="#tab_4">4</ref> shows the manual evaluation for all different methods. For held-out evaluation, we draw the precision-recall curve of all methods, and for manual evaluation we calculate the weighted macro F1 and AUC(Area under the ROC Curve). AUC depicts the ranking correctness, where F1 does not take the rank into account.</p><p>Overall, due to a limited number of positive examples from weak supervision, the unsupervised methods show a low precision on the held-out evaluation. The manual test set looses the strict condition of positive examples, moreover, its construction filters most negative examples from the unsupervised methods. These two result in increased performance of unsupervised methods. However, neither co-occurrence based model or similarity is good at modeling the ranking of comparative pairs, thus result in a low AUC.</p><p>Co-occurrence is one indicator for comparative relation of abbreviations with good recall while suffering from low precision. This is because counting co-occurrence introduces non-comparative abbreviations into the results. Sentence-level co-occurrence model  has a better performance than the document-level model since compared candidates are more likely to appear in a short context. Word similarity model performs between two co-occurrence methods. The word embedding captures the context of type rather than comparative relation. On the other hand, a large number of candidates are rarely mentioned, which leads to insufficient training of word embedding.</p><p>The supervised relation extraction methods generally outperform the unsupervised methods. The relation extraction model PCNN_single that uses single-sentence works well, but its precision drops rapidly when recall increases. PCNN_cross considering cross-sentence instances further improves the performance of the model, which shows the importance of cross-sentence instances in finding comparative relation. Our CANTOR method outperforms all these methods, which implies better modeling of cross-sentence comparative relation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>We do an ablation study to show the performance of different components. We use the manual test data in NeurIPS dataset collected by random samples from positive predictions and held-out positive examples from supervised methods to evaluate the components. As shown in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Case Study</head><p>For each dataset, we mine compared algorithms from the entire corpus with our trained CANTOR model and connect the individual pairs with the approach described in section 5. In Figure <ref type="figure" target="#fig_6">4</ref>, we show parts of the algorithm roadmaps constructed from different datasets.</p><p>In each figure, each node contains its abbreviation name and its first occurrence time described in section 5 in its dataset. To be noticed, this time is not necessarily equal to the first publication time, as the algorithm is non-necessarily published in this conference. "GAN" (Generative Adversarial Networks) is a deep generative model <ref type="bibr" target="#b8">[9]</ref>, which has been extensively cited since proposed. Researchers even maintain a "GAN zoo"<ref type="foot" target="#foot_7">8</ref> to keep track of various kinds of "GAN" successors.</p><p>In NeurIPS dataset, our method mines its direct successors such as "DCGAN," "SteinGAN," "UnrolledGAN," "Reg-GAN" and "ALI." Then we keep identifying the successors of each successor. For example, "DCGAN" has successors including "W-DCGAN, " "Stein-GAN," and "Improved-GAN" etc. The comparison of our mined algorithms with algorithms in "GAN zoo" reveals a good precision in found successors. Our current method does not distinguish different forms of an abbreviation, thus "SteinGAN" and "SteinGan" are viewed as separated candidates. A minimum confidence score threshold can be used to control each level of the roadmap to trade off the precision and recall.</p><p>Similarly in ACL dataset, query "Word2Vec" usually stands for a word embedding method. Our method identifies its direct successor such as "Glove, " "GCCA, " "NetSize, " and "NetSime. " And Glove has successors including "HLBL," "SAC" and "vecDCS" etc. In VLDB dataset, query "MonteDB" is a database management system, our method finds its direct successor such as "VectorWise," "HyperR," "PostgreSQL. " And "MXQuery" has successors such as "BDB" and "MapReduce-RDF-3X." Among the results, "LLVM" is a compiler backend used by some database management system. This error comes from incorrect table parsing, and the pair is treated as a positive example in training data.</p><p>Overall our method mines compared algorithms with good quality, though it has potential drawbacks. Some errors come from the direction derivation, mostly because of the incorrect time information and the lack of entity linking. For example, in ACL dataset, "LSA-Wiki" is actually a baseline method compared with "Word2Vec" that uses Latent Semantic Analysis used on Wikipedia. However, this abbreviation as a whole first appears in 2015, resulting in an error in direction. On the other hand, the first appearance time of an algorithm in the dataset is non-necessarily the first time this algorithm was proposed since algorithms could firstly show up in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>We propose a new task of mining algorithm roadmap in scientific literature, and present a weakly-supervised method towards solving this problem. Our method automatically identifies candidate mentions and relation labels, then jointly predicts abbreviation Our current model mainly focuses on algorithms in the form of abbreviations. However, this could be extended to general forms of entities and relations by integrating our model with general phrase mining algorithms <ref type="bibr" target="#b29">[30]</ref>, entity linking <ref type="bibr" target="#b18">[19]</ref>, and general cross-sentence relations with corresponding supervision signal. We will leave these directions for the future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>4. 2 . 1 Figure 2 :</head><label>212</label><figDesc>Abbreviation Attention abbv</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3</head><label>3</label><figDesc>Figure3shows the held-out evaluation and Table4shows the manual evaluation for all different methods. For held-out evaluation, we draw the precision-recall curve of all methods, and for manual evaluation we calculate the weighted macro F1 and AUC(Area under the ROC Curve). AUC depicts the ranking correctness, where F1 does not take the rank into account.Overall, due to a limited number of positive examples from weak supervision, the unsupervised methods show a low precision on the held-out evaluation. The manual test set looses the strict condition of positive examples, moreover, its construction filters most negative examples from the unsupervised methods. These two result in increased performance of unsupervised methods. However, neither co-occurrence based model or similarity is good at modeling the ranking of comparative pairs, thus result in a low AUC.Co-occurrence is one indicator for comparative relation of abbreviations with good recall while suffering from low precision. This is because counting co-occurrence introduces non-comparative abbreviations into the results. Sentence-level co-occurrence model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Precision-Recall curve of different relation extraction methods for finding comparative relation on NeurIPS, ACL and VLDB dataset with held-out evaluation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(a) Partial roadmap for "GAN" in NeurIPS dataset. (b) Partial roadmap for "Word2Vec" in ACL dataset. (c) Partial roadmap for "MonetDB" in VLDB dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Examples of partial algorithm roadmaps for query "GAN" in NeurIPS dataset, query "Word2Vec" in ACL dataset, and query "MonetDB" in VLDB dataset.</figDesc><graphic url="image-79.png" coords="8,53.80,382.11,504.42,117.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>9 https://arxiv.org/ types and extracts comparative relation across sentences leveraging attention context of words and abbreviations, finally it connects individual pairs into a roadmap. Our model outperforms baseline methods on three real-world datasets and shows good mining results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>An example of different types of abbreviations.</figDesc><table><row><cell>Type</cell><cell>Abbreviations</cell><cell>Signal Word</cell></row><row><cell>Algorithm</cell><cell>CNN, LSTM, GAN</cell><cell>algorithm</cell></row><row><cell>Metric</cell><cell>AUC, MAP, MAE</cell><cell>metric</cell></row><row><cell>Dataset</cell><cell>MNIST, CIFAR10, SQuAD</cell><cell>dataset</cell></row><row><cell>Others</cell><cell>NP, VP, POS</cell><cell>/</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>table, mentioned abbreviations are often comparative, including compared algorithms, datasets, or metrics etc. This gives us an opportunity to create positive training examples from the table without human effort.We first used a table parsing tool<ref type="bibr" target="#b5">[6]</ref> 3 to extract tables from raw pdf files of papers. Then we processed the parsed results to identify abbreviations in the same row or column. We enumerated and labeled aligned abbreviation pairs as positive examples with comparative relation. The supervision from the table gives compared abbreviations of various types.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>.</figDesc><table><row><cell>Dataset</cell><cell cols="4">documents sentences positive pairs abbreviations</cell></row><row><cell>NeurIPS</cell><cell>7k</cell><cell>3144k</cell><cell>9k</cell><cell>66k</cell></row><row><cell>ACL</cell><cell>5k</cell><cell>2277k</cell><cell>22k</cell><cell>71k</cell></row><row><cell>VLDB</cell><cell>2k</cell><cell>1289k</cell><cell>5k</cell><cell>40k</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Dataset statistics of NeurIPS, ACL, VLDB dataset.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>implemented in Gensim7  for each corpus. The threshold is decided similarly to Sent_cooccur.CANTOR: Our proposed cross-sentence relation extraction method, which considers both single-sentence and cross-sentence instances, all abbreviations in the context, and jointly typing the candidates. Manual evaluation of different relation extraction methods for finding comparative relation on NeurIPS, ACL and VLDB datasets.</figDesc><table><row><cell>Method</cell><cell>NeurIPS AUC F1</cell><cell>ACL AUC</cell><cell>F1</cell><cell>VLDB AUC F1</cell></row><row><cell>sent_cooccur</cell><cell cols="4">0.68 0.71 0.66 0.71 0.70 0.67</cell></row><row><cell>doc_cooccur</cell><cell cols="4">0.57 0.69 0.46 0.68 0.62 0.68</cell></row><row><cell cols="5">word_similarity 0.62 0.70 0.67 0.72 0.66 0.72</cell></row><row><cell>PCNN_single</cell><cell cols="4">0.73 0.71 0.72 0.68 0.78 0.67</cell></row><row><cell>PCNN_cross</cell><cell cols="4">0.75 0.71 0.76 0.74 0.82 0.76</cell></row><row><cell cols="5">CANTOR (ours) 0.82 0.74 0.79 0.78 0.85 0.78</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 ,</head><label>5</label><figDesc>stacking self-attention, abbreviation-attention, typing and combined modeling improves the model performance.</figDesc><table><row><cell>Method</cell><cell>AUC</cell><cell>F1</cell></row><row><cell>Self-Attention</cell><cell>0.77</cell><cell>0.72</cell></row><row><cell cols="2">+Abbreviation-Attention 0.78</cell><cell>0.72</cell></row><row><cell>+Tpying</cell><cell>0.78</cell><cell>0.74</cell></row><row><cell>CANTOR (ours)</cell><cell cols="2">0.82 0.74</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Abalation study on different components in NeurIPS dataset.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://www.technologyreview.com/s/612768/we-analyzed-16625-papers-to-figureout-where-ai-is-headed-next/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">2 https://aiindex.org/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">https://github.com/allenai/pdffigures</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">https://nips.cc/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">http://aclweb.org/anthology/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5">https://www.vldb.org/pvldb/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6">https://radimrehurek.com/gensim/ Research Track Paper KDD '19, August 4-8, 2019, Anchorage, AK, USA</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7">https://github.com/hindupuravinash/the-gan-zoo</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors would like to thank the anonymous reviewers for their thoughtful comments. This research was sponsored in part by the Army Research Laboratory under cooperative agreements W911NF09-2-0053 and NSF IIS 1528175. The views and conclusions contained herein are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Laboratory or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notice herein.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A IMPLEMENTATION DETAILS A.1 Preprocessing</head><p>The paper pdf files are converted into plain text files by using Linux pdftotext tool, and non-ascii letters are removed. For each dataset, we keep a word vocabulary with all abbreviations and other words with minimum frequency threshold 5. The max paragraph length is set to 160 words, and the max number of continuous sentences considered is set to 20. Paragraphs longer than the threshold are cut off.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Training</head><p>The model is implemented in pytorch <ref type="bibr" target="#b23">[24]</ref> and trained on a single GeForce GTX 1080 GPU. The dimensions of word embedding, character embedding, and positional embedding are set to 100, 50 and 10 respectively. The word embedding is pre-trained in each scientific publication corpus with Skip-Gram model implemented in Gensim. The kernel size of the convolutional layer is set to 7. vfill We use 200 filters for the convolutional layer in the singlesentence module, and the same number of filters as input dimension for the convolutional layer in each Transformer block. We apply layer normalization <ref type="bibr" target="#b1">[2]</ref> to each component of transformer block, and adopt dropout <ref type="bibr" target="#b32">[33]</ref> to the input layer, piece-wise max-pooling and Transformer block with a dropout rate 0.3. The number of Transfomer block layer is set to 1, since we did not observe performance gain in increasing layers. We use Adam optimizer <ref type="bibr" target="#b15">[16]</ref> with a learning rate 0.001. In training, the batch size is set to be 32, and for each positive example, we sample 5 different negative examples. In validation and test, we use all examples. The maximum number of epochs is set to be 16, where the result with best positive-class validation F1 is kept.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Scienceie-extracting keyphrases and relations from scientific publications</title>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Augenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mrinal</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lakshmi</forename><surname>Vikraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 12th International Workshop on Semantic Evaluation</title>
				<meeting>The 12th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lei Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Ryan Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<title level="m">Layer normalization</title>
				<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGMOD international conference on Management of data</title>
				<meeting>the 2008 ACM SIGMOD international conference on Management of data</meeting>
		<imprint>
			<publisher>AcM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semeval-2016 task 13: Taxonomy extraction evaluation (texeval-2)</title>
		<author>
			<persName><forename type="first">Georgeta</forename><surname>Bordea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Els</forename><surname>Lefever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Buitelaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation</title>
				<meeting>the 10th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1081" to="1091" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Toward an architecture for never-ending language learning</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Betteridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Kisiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Burr</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName><surname>Estevam R Hruschka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">M</forename><surname>Jr</surname></persName>
		</author>
		<author>
			<persName><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<meeting><address><addrLine>Atlanta</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Looking beyond text: Extracting figures, tables and captions from computer science papers</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santosh</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><surname>Divvala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshops at the Twenty-Ninth AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The Automatic Content Extraction (ACE) Program-Tasks, Data, and Evaluation</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>George R Doddington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">A</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lance</forename><forename type="middle">A</forename><surname>Przybocki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><forename type="middle">M</forename><surname>Strassel</surname></persName>
		</author>
		<author>
			<persName><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
				<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">INRIASAC: Simple hypernym extraction methods</title>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Grefenstette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Workshop on Semantic Evaluation</title>
				<meeting>the 9th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hierarchical topic models and the nested chinese restaurant process</title>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Thomas L Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Detecting topic evolution in scientific literature: how can citations help</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baojun</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prasenjit</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM conference on Information and knowledge management</title>
				<meeting>the 18th ACM conference on Information and knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="957" to="966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automatic acquisition of hyponyms from large text corpora</title>
		<author>
			<persName><forename type="first">A</forename><surname>Marti</surname></persName>
		</author>
		<author>
			<persName><surname>Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th conference on Computational</title>
				<meeting>the 14th conference on Computational</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="539" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural relation extraction with selective attention over instances</title>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2124" to="2133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">End-to-end Sequence Labeling via Bidirectional LSTM-CNNs-CRF</title>
		<author>
			<persName><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1064" to="1074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">DBpedia spotlight: shedding light on the web of documents</title>
		<author>
			<persName><forename type="first">Pablo</forename><forename type="middle">N</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Jakob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrés</forename><surname>García-Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Bizer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th international conference on semantic systems</title>
				<meeting>the 7th international conference on semantic systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Gibbs sampling for logistic normal topic models with graph-based priors</title>
		<author>
			<persName><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanna</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Analyzing Graphs</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">61</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised does not mean uninterpretable: The case for word sense induction and disambiguation</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Panchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugen</forename><surname>Ruppert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Faralli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><surname>Paolo Ponzetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Biemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
				<meeting>the 15th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="86" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Cross-sentence n-ary relation extraction with graph lstms</title>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Distant Supervision for Relation Extraction beyond the Sentence Boundary</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
				<meeting>the 15th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1171" to="1182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno>ICLR 2016</idno>
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cotype: Joint extraction of typed entities and relations with knowledge bases</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeqiu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenqi</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clare</forename><forename type="middle">R</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Tarek F Abdelzaher</surname></persName>
		</author>
		<author>
			<persName><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee</title>
				<meeting>the 26th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1015" to="1024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Modeling relations and their mentions without labeled text</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="148" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Automated phrase mining from massive text corpora</title>
		<author>
			<persName><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clare</forename><forename type="middle">R</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1825" to="1837" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Improving hypernymy detection with an integrated path-based and distributional method</title>
		<author>
			<persName><forename type="first">Vered</forename><surname>Shwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning syntactic patterns for automatic hypernym discovery</title>
		<author>
			<persName><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1297" to="1304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Yago: a core of semantic knowledge</title>
		<author>
			<persName><forename type="first">Gjergji</forename><surname>Fabian M Suchanek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerhard</forename><surname>Kasneci</surname></persName>
		</author>
		<author>
			<persName><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th international conference on World Wide Web</title>
				<meeting>the 16th international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="697" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multi-instance multi-label learning for relation extraction</title>
		<author>
			<persName><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julie</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning</title>
				<meeting>the 2012 joint conference on empirical methods in natural language processing and computational natural language learning</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="455" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Simultaneously Self-Attending to All Mentions for Full-Abstract Biological Relation Extraction</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter</title>
				<meeting>the 2018 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">The STM report: An overview of scientific and scholarly journal publishing</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Ware</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Mabe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Probase: A probabilistic taxonomy for text understanding</title>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongsong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haixun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenny</forename><forename type="middle">Q</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data</title>
				<meeting>the 2012 ACM SIGMOD International Conference on Management of Data</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="481" to="492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Distant Supervision for Relation Extraction via Piecewise Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1753" to="1762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Relation Classification via Convolutional Deep Neural Network</title>
		<author>
			<persName><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computational Linguistics</title>
				<meeting>the International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">TaxoGen: Constructing Topical Concept Taxonomy by Adaptive Term Embedding and Clustering</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangbo</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiusi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Sadler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michelle</forename><surname>Vanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
