<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Person Re-identification: Clustering and Fine-tuning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hehe</forename><surname>Fan</surname></persName>
							<email>hehe.fan@student.uts.edu.au</email>
						</author>
						<author>
							<persName><forename type="first">Liang</forename><surname>Zheng</surname></persName>
							<email>liang.zheng@uts.edu.au@c.yan</email>
						</author>
						<author>
							<persName><forename type="first">Yan</forename><surname>Chenggang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Y</forename><forename type="middle">I</forename><surname>Yang</surname></persName>
							<email>yi.yang@uts.edu.au.</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Information and Control</orgName>
								<orgName type="department" key="dep2">Center for Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">Hangzhou Dianzi University</orgName>
								<orgName type="institution" key="instit2">University of Technology Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Center for Artificial Intelligence</orgName>
								<orgName type="institution">University of Technology Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Institute of Information and Control</orgName>
								<orgName type="institution">Hangzhou Dianzi University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Center for Artificial Intelligence</orgName>
								<orgName type="institution">University of Technology Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department" key="dep1">Institute of Information and Control</orgName>
								<orgName type="department" key="dep2">Center for Artificial Intelligence</orgName>
								<orgName type="institution">Hangzhou Dianzi University</orgName>
								<address>
									<addrLine>Baiyang Road</addrLine>
									<postCode>1158, 310018</postCode>
									<settlement>Hangzhou</settlement>
									<region>Zhejiang</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">University of Technology Sydney</orgName>
								<address>
									<addrLine>15 Broadway</addrLine>
									<postCode>2007</postCode>
									<settlement>Ultimo, Sydney</settlement>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="department">Center for Artificial Intelligence</orgName>
								<orgName type="institution">Uni-versity of Technology Sydney</orgName>
								<address>
									<addrLine>15 Broadway</addrLine>
									<postCode>2007</postCode>
									<settlement>Ultimo, Sydney</settlement>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="department">Institute of Information and Control</orgName>
								<orgName type="institution">Hangzhou Dianzi University</orgName>
								<address>
									<addrLine>Baiyang Road</addrLine>
									<postCode>1158, 310018</postCode>
									<settlement>Hangzhou</settlement>
									<region>Zhejiang</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff8">
								<orgName type="department">Center for Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">Y. Yang</orgName>
								<orgName type="institution" key="instit2">University of Technology Sydney</orgName>
								<address>
									<addrLine>15 Broadway</addrLine>
									<postCode>2007</postCode>
									<settlement>Ultimo, Sydney</settlement>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Person Re-identification: Clustering and Fine-tuning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DD3BC3D9E7CAAB1D5A01D9EE0BE9D95A</idno>
					<idno type="DOI">10.1145/3243316</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T04:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Large-scale person re-identification</term>
					<term>unsupervised learning</term>
					<term>convolutional neural network</term>
					<term>clustering</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The superiority of deeply learned pedestrian representations has been reported in very recent literature of person re-identification (re-ID). In this article, we consider the more pragmatic issue of learning a deep feature with no or only a few labels. We propose a progressive unsupervised learning (PUL) method to transfer pretrained deep representations to unseen domains. Our method is easy to implement and can be viewed as an effective baseline for unsupervised re-ID feature learning. Specifically, PUL iterates between (1) pedestrian clustering and (2) fine-tuning of the convolutional neural network (CNN) to improve the initialization model trained on the irrelevant labeled dataset. Since the clustering results can be very noisy, we add a selection operation between the clustering and fine-tuning. At the beginning, when the model is weak, CNN is fine-tuned on a small amount of reliable examples that locate near to cluster centroids in the feature space. As the model becomes stronger, in subsequent iterations, more images are being adaptively selected as CNN training samples. Progressively, pedestrian clustering and the CNN model are improved simultaneously until algorithm convergence. This process is naturally formulated as self-paced learning. We then point out promising directions that may lead to further improvement. Extensive experiments on three large-scale re-ID datasets demonstrate that PUL outputs discriminative features that improve the re-ID accuracy. Our code has been released at https://github.com/hehefan/Unsupervised-Person-Re-identification-Clustering-and-Fine-tuning. CCS Concepts: • Computing methodologies → Visual content-based indexing and retrieval; Image representations;</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>This article discusses person re-identification (re-ID), a task that provides critical insights whether a person re-appears in another camera after being spotted in one <ref type="bibr" target="#b61">[62]</ref>. Recent works view it as a search problem aiming to retrieving the relevant images to the top ranks <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b63">64]</ref>.</p><p>Large-scale learning methods based on the convolutional neural network (CNN) constitute the recent advances in this field. Broadly speaking, current deep learning methods can be categorized into two classes, i.e., similarity learning and representation learning. For the former, the training input can be image pairs <ref type="bibr" target="#b38">[39]</ref>, triplets <ref type="bibr" target="#b26">[27]</ref>, or quadruplets <ref type="bibr" target="#b7">[8]</ref>. These methods directly output the similarity score of two input images by focusing on their distinct body patches, without an explicit feature extraction process. This line of methods is less efficient during testing. However, when learning feature representations, contrastive loss or softmax loss are most commonly employed. For these methods, there is an explicit feature extraction process for the query and gallery images, which is learned to discriminate among a pool of identities or among image pairs/triplets <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18]</ref>. Then the re-ID procedure is rather like a retrieval problem under some commonly used similarity measurement such as Euclidean distance. Due to its efficiency in large-scale galleries and the potentials in further acceleration, the deep feature learning strategy has become more popular <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b63">64]</ref>. Therefore, we adopt the classification model proposed in References <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b61">62]</ref> for feature learning.</p><p>Despite the impressive progress achieved by the deep learning methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b63">64]</ref>, most of these methods focus on re-ID with sufficient training data in the same environment. In fact, the lack of labeled training data under a new environment has been addressed by many previous works by resorting to unsupervised or semi-supervised learning <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b45">[46]</ref><ref type="bibr" target="#b46">[47]</ref><ref type="bibr" target="#b47">[48]</ref><ref type="bibr" target="#b48">[49]</ref><ref type="bibr" target="#b49">[50]</ref>. These works typically deal with the relatively small datasets <ref type="bibr" target="#b40">[41]</ref>, and to our knowledge, none of these works are integrated into the deep learning framework that has been proven as the state of the art under the large-scale datasets <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b59">60]</ref>. In fact, the problem of high annotation cost is more severe for deep learning based methods due to the intensive demand of data. So, under these circumstances, we make initial attempts by developing a easy-to-implement method for unsupervised deep representation learning in the re-ID community. Our method can be easily extended to the semi-supervised setting by annotating a small amount of bounding boxes. We also notice some works on human-in-the-loop learning <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b39">40]</ref>, introducing labels by human feedback. We envision our work is complementary to this line of methods.</p><p>In this article, we aim at finding "better" feature representations without labeled training data to adapt to the unlabeled dataset. Named "progressive unsupervised learning" (PUL), our method is an iterative process and each iteration consists of two components (shown in Figure <ref type="figure" target="#fig_0">1</ref>). <ref type="bibr" target="#b0">(1)</ref> We deploy the original model pre-trained on some external labeled data to extract features of images from the unlabeled training set. Standard k-means clustering is performed on the CNN features, followed by a selection operation to choose reliable samples. (2) The original model is fine-tuned using the selected training samples. We then use this newborn model to extract features and begin another iteration of training. At the beginning, when the model is weak, the proposed approach learns from a small amount of reliable examples, which are near to cluster centroids in the feature space, to avoid getting stuck at a bad optimum or oscillating. As the model becomes stronger in subsequent iterations, more data instances are adaptively selected and exploited as training examples. By this progressive method, the model and clustering are trained and rectified alternately. This process can also be called self-paced learning <ref type="bibr" target="#b21">[22]</ref>.</p><p>The contributions of this article are threefold:</p><p>(1) Among the early efforts, we propose an easy-to-implement unsupervised deep learning framework for large-scale person re-ID. <ref type="bibr" target="#b1">(2)</ref> We are the first to discover the underlying relation between clustering and self-paced learning and integrate the self-paced learning paradigm into the unsupervised learning regime. This learning strategy facilitates PUL to extract faithful knowledge from highly unreliable clustering results in learning. (3) Extensive experiments on three large-scale datasets indicate that our method noticeably improves the re-ID accuracy in the cross-dataset evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Deeply Learned Person Representations</head><p>Deep learning models have been popular since Krizhevsky et al. <ref type="bibr" target="#b20">[21]</ref> won ILSVRC'12 by a large margin. Deeply learned person representations are producing state-of-the-art re-ID performance recently. Like vehicle re-identification <ref type="bibr" target="#b28">[29]</ref>, person re-ID can be seen as a type of instance-level retrieval <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b62">63]</ref>, in which given a query image depicting a particular object, the aim is to retrieve images containing the same object that may be captured under different views, illumination, or with occlusions. The first two works in re-ID to use deep learning were <ref type="bibr" target="#b51">[52]</ref> and <ref type="bibr" target="#b22">[23]</ref>. Generally speaking, from the perspective of model, classification models as used in image classification <ref type="bibr" target="#b20">[21]</ref> and siamese models that use image pairs <ref type="bibr" target="#b33">[34]</ref> or triplets <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b36">37]</ref> are two types of CNN models that are commonly employed in re-ID. At the beginning when the training datasets are not big enough, such as VIPeR <ref type="bibr" target="#b15">[16]</ref> that provides only two images for each identity, the siamese model dominates re-ID community. As the scale of re-ID dataset becomes large, such as 83:4</p><p>H. Fan et al.</p><p>Market-1501 <ref type="bibr" target="#b59">[60]</ref>, the classification model is widely employed. A survey of re-ID can be viewed in Reference <ref type="bibr" target="#b61">[62]</ref>. However, comparing with deep similarity learning methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b38">39]</ref>, the representation learning methods are more extendable for large galleries. For example, Hermans et al. <ref type="bibr" target="#b17">[18]</ref> used an efficient variant of the triplet loss based on the distance between samples. Another popular choice consists in training an identification network and extracting the intermediate output as a discriminative embedding <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b61">62]</ref>. For example, Xiao et al. <ref type="bibr" target="#b44">[45]</ref> propose an online instance matching loss to address the problem of having only few training samples each class. Lin et al. <ref type="bibr" target="#b24">[25]</ref> combine attribute classification losses and the re-ID loss for embedding learning. To exploit more training data, Zheng et al. <ref type="bibr" target="#b63">[64]</ref> employ the generative adversarial network to generate samples that are expected to generate uniform prediction probabilities in the softmax layer. In this article, we adopt the baseline identification model, named "ID-discriminative Embedding" (IDE) in Reference <ref type="bibr" target="#b57">[58]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Unsupervised Person Re-ID</head><p>Although deeply learned person representations have achieved significant progress by supervised learning methods, less attention is paid on unsupervised learning for re-ID. Kodirov et al. <ref type="bibr" target="#b19">[20]</ref> utilize a graph regularized dictionary learning to capture discriminative cues for cross-view identity matching. Yang et al. <ref type="bibr" target="#b49">[50]</ref> propose a weighted linear coding method to learn multi-level descriptors from raw pixel data in an unsupervised manner. Peng et al. <ref type="bibr" target="#b32">[33]</ref> propose a multi-task dictionary learning model to transfer a view-invariant representation from a number of existing labeled source datasets to an unlabeled target dataset. Ma et al. <ref type="bibr" target="#b30">[31]</ref> utilize image-sequence information to fuse different descriptions. These methods focus on small-scale datasets and do not adopt deep features.</p><p>Another choice in unsupervised re-ID consists in deploying hand-crafted features directly. Several effective descriptors have been developed in recent years. In the earlier works, Farenzena <ref type="bibr" target="#b12">[13]</ref> et al. use the weighted color histogram, the maximally stable color regions, and the recurrent high-structured patches to segment the pedestrian foreground from the background. Gray and Tao <ref type="bibr" target="#b15">[16]</ref> use 8 color channels and 21 texture filters on the luminance channel, and the pedestrian is partitioned into horizontal stripes. Recently, Zhao et al. <ref type="bibr" target="#b54">[55]</ref><ref type="bibr" target="#b55">[56]</ref><ref type="bibr" target="#b56">[57]</ref> use the 32-dim LAB color histogram and the 128-dim SIFT descriptor are extracted from each 10 × 10 patch densely sampled with a step size of 5 pixels. Liao et al. <ref type="bibr" target="#b23">[24]</ref> propose the local maximal occurrence (LOMO) descriptor, which includes the color and SILTP histograms. LOMO is later employed by <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref> and a similar set of features is used by Chen et al. <ref type="bibr" target="#b6">[7]</ref>. Zheng et al. <ref type="bibr" target="#b59">[60]</ref> propose extracting the 11-dim color names descriptor for each local patch, and aggregating them into a global vector through a Bag-of-Words model.</p><p>Person re-ID is a key component in long-term/cross-camera pedestrian tracking. Usually, cross-camera tracking has the following components: person detection, within-camera person association, and cross-camera person association. We usually refer cross-camera person association as person re-ID. Online learning methods are usually adopted in single object tracking (SOT) <ref type="bibr" target="#b1">[2]</ref>, where tracking is usually performed within a same camera. The online learning methods bootstrap themselves by using the current tracking state to mine positive and negative samples to improve the tracker. In spirit, this type of methods and our method are similar to self-paced learning. However, their difference is twofold. First, online learning in tracking aims to discriminate between the foreground object and the background, while person re-ID aims to discriminate between different persons. Second, online learning methods in tracking deal more with image changes caused by temporal changes, while person re-ID methods are usually based on static images and deal with image variations caused by different cameras. We therefore speculate that online learning methods in tracking might not be directly deployed in unsupervised person re-ID. Nevertheless, the spirit underlined in online learning methods can be borrowed into the domain of unsupervised person re-ID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Self-paced Learning</head><p>In the human learning process, knowledge is imparted in the form of curriculum and organized from easy to difficult. Inspired by this process, the theory of Curriculum Learning (CL) <ref type="bibr" target="#b5">[6]</ref> is proposed to accelerate the speed of convergence of the training process to a minimum. The main challenge of CL is that it requires a known separation to indicate whether a sample in a given training dataset is easy or hard. To alleviate this deficiency, self-paced learning <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b29">30]</ref> embeds easiness identification into model learning. SPL considers the samples with small losses as easy training instances. During learning, SPL uses easy training instances to train the model. The learning principle behind SPL is to prevent latent variable models from getting stuck in a bad local optimum or oscillating. Then SPL is widely used in weakly-supervised and semi-supervised learning <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b42">43]</ref>. For example, to automatically detect whether an event of interest happens in temporally untrimmed long videos, which usually consist of multiple video shots, Fan et al. <ref type="bibr" target="#b11">[12]</ref> propose a multi-instance learning by selecting reliable instances method, which simultaneously learns a linear SVM classifier and infers a binary indicator for each instance to select reliable training instances from.</p><p>In this article, we utilize SPL to select reliable samples to fine-tune the original model. If we consider the labels of the samples from the unlabeled dataset as latent variables, then our method also belongs to latent variable model. Therefore, it is necessary to avoid getting stuck in bad optimums or oscillating using SPL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROGRESSIVE UNSUPERVISED LEARNING</head><p>In this section, we present the progressive unsupervised learning framework for re-ID in detail. In this work, all the backbone CNN models, such as ResNet-50 <ref type="bibr" target="#b16">[17]</ref>, are pre-trained on ImageNet <ref type="bibr" target="#b35">[36]</ref>. In Step 0, we fine-tune the backbone model on an irrelevant labeled dataset, yielding the "original model." This step is also called model initialization. Then, the system goes into an iterative procedure. In each iteration, we use the current model to extract features for images from the unlabeled dataset. Then, these unlabeled data instances are clustered and selected to generate a reliable training set. Using the reliable training set, the original model is fine-tuned. The iterative procedure stops when the scale of the reliable training set becomes stable. The progressive unsupervised learning framework is demonstrated in Figure <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Formulation</head><p>Suppose the unlabeled dataset contains N cropped person images {x i } N i=1 belonging to K identities. Since the images are not labeled, we denote y i as the identity of image x i , where y i = 1, . . . , K, and i = 1, . . . , N . Here, y i can be regarded as a latent variable: the proposed PIL is a latent variable model.</p><p>Let v i be the indicator whether image x i is selected into the reliable training set. If v i = 1, then x i is selected as a reliable sample; otherwise, x i is discarded from the reliable training set, which is used to fine-tune the CNN model. We further denote We formulate our idea as optimizing the following three problems alternately:</p><formula xml:id="formula_0">v = [v 1 , . . . ,v N ]</formula><formula xml:id="formula_1">min y,c 1 , ...,c K K k=1 y i =k ϕ (x i ; θ ) -c k 2 ,<label>(1)</label></formula><formula xml:id="formula_2">min v K k=1 y i =k v i ϕ (x i ; θ ) -c k -λ v 1 , s.t . v i ∈ {0, 1}, y i =k v i ≥ 1, ∀k,<label>(2)</label></formula><formula xml:id="formula_3">min θ,w N i=1 v i L(y i , ϕ (x i ; θ ); w),<label>(3)</label></formula><p>where c k is the mean of points {ϕ (x i ; θ )} y i =k in the feature space, λ is a positive parameter controlling sample selection, and L denotes a loss function for classification. Equation ( <ref type="formula" target="#formula_1">1</ref>) infers the labels y = [y 1 , . . . ,y N ] of images {x i } N i=1 using the result of the standard k-means clustering. Since clustering results can be considerably noisy, it is inappropriate to use all of the inferred labels y to fine-tune the CNN model. To alleviate the above problem, rather than using all samples for fine-tuning, the proposed PUL method selects the training images according to how reliable their inferred identities are.</p><p>In our formulation, Equation ( <ref type="formula" target="#formula_2">2</ref>) serves this goal by selecting reliable data instances that are close enough to the cluster centroids. These samples are considered to be reliable in terms of their estimated labels. We formulate this operation as self-paced learning (SPL) <ref type="bibr" target="#b21">[22]</ref>. When the model is weak, the SPL selectively learns from a few easy and reliable images. These images will have a relatively small loss in the classification CNN. As the model becomes stronger in subsequent iterations, more samples are selected and exploited as reliable training instances for CNN finetuning. In PUL, samples close to their nearest centroid is considered as a reliable training instance. In the other words, the "reliability" of an sample depends on the Euclidean distance to the nearest centroid. The constraint y i =k v i ≥ 1 guarantees that each cluster contains at least one reliable sample. SPL is usually used in semi-supervised learning and weekly supervised learning, so our work makes initial attempt to integrate SPL into the unsupervised learning regime due to the unsupervised nature of k-means. Suppose each clustering contains i iterations, Equation (1), i.e., the k-means algorithm, can be exactly solved in time O (nki). For the reliable data instance selection, i.e., Equation (2), the time complexity is O (nk ). Therefore, compared to clustering, the time complexity of reliable instance selection can be ignored.</p><p>In Equation ( <ref type="formula" target="#formula_3">3</ref>), the CNN model is fine-tuned using the selected reliable samples defined in Equation (2). Generally speaking, three types of loss functions for CNN models are commonly employed in the person re-ID community. The first one is the classification method, in which a classifier is added on top of the model ϕ (x; θ ) as a fully connected layer. When v i equals to 0, i.e., the sample is not considered reliable, its classification loss will not be calculated. Equation (3) can also be replaced with the contrastive loss <ref type="bibr" target="#b33">[34]</ref> or triplet loss <ref type="bibr" target="#b36">[37]</ref>. In this article, we mainly focus on the classification model.</p><p>Equations  the same person will be located closer to cluster centroid. As a consequence, more selection indicators v i are expected to become 1 to minimize Equation (2). Therefore, more examples are selected into the reliable training set, until no more reliable data instances are added. Figure <ref type="figure">2</ref> illustrates how the training set changes during optimization. We also present two groups of visual examples of this process in Figure <ref type="figure" target="#fig_3">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Optimization Procedure</head><p>The optimization of Equations ( <ref type="formula" target="#formula_1">1</ref>), (2), and (3) can be solved in an alternate manner.</p><p>(1) Optimize y and c 1 , . . . , c K when θ and v are fixed. In this step, the optimization problem reduces to the classic k-means clustering and can be non-trivially solved.</p><p>(2) Optimize v when θ , y and c 1 , . . . , c K are fixed. The goal of this step is to select reliable data instances to fine-tune the CNN model with the clustering results y in Equation (1). To minimize the first component of Equation ( <ref type="formula" target="#formula_2">2</ref>), all selection indicators {v i } can be trivially set to be 0, which means no sample is selected. However, the second component l 1 -normv 1 monotonically increases 83:8 H. Fan et al.</p><p>as the number of selected samples decreases. So it encourages the selection indicators to be 1. By striking a balance between these two components, the objective function aims to select an appropriate number of reliable samples for CNN training. A sample will be selected if the distance of its feature to its corresponding cluster centroid is smaller than λ. As noted before, λ is a threshold to control the reliable sample selection.</p><p>(3) Optimize θ and w when v, y and c 1 , . . . , c K are fixed. Note that at the beginning of each iteration, w is initialized randomly.  In practice, we use cosine to measure the similarity between two feature vectors. The optimization algorithm of PUL is presented in Algorithm 1. To guarantee each cluster contains at least one reliable sample, we choose the nearest feature to the corresponding centroid as the center of the cluster. In the algorithm, samples with f i • f k &gt; λ will be selected (v i = 1) for fine-tuning; otherwise, sample will not be selected (v i = 0). When the number of selected reliable samples is saturated, the algorithm will converge.</p><formula xml:id="formula_4">{c k } K k=1 → {f k } K k=1 ; l 2 -normalize {f i } N i=1 ; for k = 1 to K do for i = 1 to N do calculate inner product δ i = f i • f k ; if δ i &gt; λ then v i ← 1; //</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Semi-supervised PUL</head><p>The PUL framework can be easily extended to semi-supervised learning. To integrate labeled data into the training process, we can directly add the labeled data to the selected reliable training set in every iteration.</p><p>The semi-supervised PUL is a more generalized case. When all the training images are labeled, since there is no need to infer labels or select reliable samples, the semi-supervised PUL is reduced to supervised learning; when no labeled data is available, the method has to rely on clustering and self-paced learning to obtain reliable training data, which is the case of unsupervised PUL.</p><p>Unsupervised Person Re-identification: Clustering and Fine-tuning 83:9 Note that we use the train/test protocol proposed in Reference <ref type="bibr" target="#b64">[65]</ref> for CUHK03. For market and CUHK03, the query and gallery share the same IDs. However, for Duke, except for the shared 702 IDs, the gallery contains another 408 IDs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Settings</head><p>We mainly evaluate the proposed method on DukeMTMC-reID <ref type="bibr" target="#b63">[64]</ref> and Market-1501 <ref type="bibr" target="#b59">[60]</ref>, because they are two relatively large datasets with multiple cameras. We also report results on CUHK03 <ref type="bibr" target="#b22">[23]</ref>, which is a relatively small dataset. We do not use MARS <ref type="bibr" target="#b57">[58]</ref>, another large-scale dataset, because it is an extension of Market-1501 and hence has a similar data distribution with Market-1501.</p><p>DukeMTMC-reID is a subset of the pedestrian tracking dataset DukeMTMC <ref type="bibr" target="#b34">[35]</ref>. This dataset contains 36,411 images with 1,812 identities captured from 8 different view points. Following Market-1501, the dataset is also split into three parts: 16,522 images with 702 identities for training, 17,661 images with 1,110 identities in gallery, and another 2,228 images with 702 identities in the gallery for query. Images in this dataset vary in size. For simplicity, we use "Duke" to represent this dataset.</p><p>Market-1501 contains 32,668 images of 1,501 identities in total, which are captured from 6 cameras placed in front of a campus supermarket. The images are detected and cropped using the Deformable Part Model (DPM) <ref type="bibr" target="#b13">[14]</ref>. The dataset is split into three parts: 12,936 images with 751 identities for training, 19,732 images with 750 identities for gallery, and another 3,368 hand-drawn images with the same 750 gallery identities for query. All the images are of the size 128 × 64. For simplicity, we use "Market" to represent this dataset.</p><p>CUHK03 contains 14,096 images of 1,467 identities. Each identity is captured from two cameras in the CUHK campus, and has an average of 4.8 images in each camera. The dataset provides both manually labeled images and DPM-detected images. We experiment on the DPM-detected images. Note that the original evaluation protocol of CUHK03 has 20 train/test splits. Nevertheless, to be in consistency with Market-1501 and Duke-reID, we use the train/test protocol proposed in <ref type="bibr" target="#b64">[65]</ref>: 7,365 images with 767 identities for training, 5,332 images with the remaining 700 identities for gallery, and 1,400 images with the same 700 gallery identities for query. Images in this dataset also vary in size. We list the statistics and some some samples of the these three datasets in Table <ref type="table" target="#tab_0">1</ref> and Figure <ref type="figure">4</ref>.</p><p>We report the rank-1, -5, -10, -20 accuracy and mean average precision (mAP) for all the three datasets. All experiments use single query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>We use ResNet-50 <ref type="bibr" target="#b16">[17]</ref> pre-trained on ImageNet as our basic CNN model. To fine-tune the model using the classification framework <ref type="bibr" target="#b61">[62]</ref>, we modify the fully connected layer to adapt to different datasets. We also insert a dropout layer before the fully connected layer and set the dropout rate to 0.5 for all datasets. All images are resized to 224 × 224. For data augmentation, we randomly We compare PUL with deploying the fine-tuned CNN model on an unseen testing dataset (baseline). Results are shown in Table <ref type="table">2</ref>. The reliability threshold λ is set to be 0.85 for all the three datasets. Since Duke, Market and CUHK03 have 702, 751, and 767 training IDs, respectively, the number of clusters K is set to be 700 for Duke and 750 for Market and CUHK03. When the fine-tuned model is directly deployed on other datasets, re-ID accuracy is far inferior to training/testing on the same dataset. For example, the CNN model fine-tuned and tested on Market achieves 76.2% in rank-1 accuracy, but drops to 21.9% when trained on Duke and tested on Market. This is because of the changes in data distribution between different datasets. In stark contrast, PUL effectively improves the re-ID accuracy over the baseline. For example, when using Duke to train the original CNN model, PUL leads to an improvement of +8.6% in rank-1 accuracy and +5.9% in mAP on Market-1501.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Evaluation of PUL 4.4.1 Ablation Study-PUL Without Sample Selection.</head><p>The selection of reliable samples for CNN fine-tuning is a key component in the PUL system. Here, we investigate how the system works without reliable sample selection. In our experiment, for each PUL iteration, we use all the samples as training data for CNN fine-tuning. That is, after k-means clustering, all the images in each cluster are viewed as a class and fed into CNN. The model is initialized on Duke and tested on Market-1501.</p><p>Results are presented in Table <ref type="table" target="#tab_3">3</ref> and Figures <ref type="figure">5(e</ref>) and 5(f). Comparing with the baseline results in Table <ref type="table">2</ref>, we hardly observe any noticeable improvement of using PUL without sample selection. When K = 1, 250, the largest gain over the baseline, we can obtain is +1.4% in rank-1 accuracy, and +1.2% in mAP. This indicates that the samples within each cluster are very noisy, and that if we use all the noisy samples for fine-tuning without any selection, the learned CNN model gets stuck in a bad optimum. These results suggest that reliable sample selection is a necessary component in PUL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Further</head><p>Understanding of PUL with Parameter Changes. Two important parameters are involved in this article, i.e., the reliability threshold λ and the number of clusters K. To explore these two parameters, we mainly deploy the fine-tuned ResNet-50 model on Duke described in Section 4.3. We apply PUL using this fine-tuned ResNet-50 model on Market-1501 without any labels. PUL is always trained for 25 iterations.</p><p>First, we evaluate the impact of K (we fix λ to 0.85), which is sampled from {250, 500, 750, 1,000, 1,250}, because Market has 751 training identities (not known in practice). Results are shown in Figures <ref type="figure">5(a</ref>) and 5(b). Since the Market dataset has 751 identities, K = 750 achieves the best performance as expected. Although an accurate K helps PUL achieve better performance, this experiment indicates that an approximate K, which is not too far away from its ground-truth value, can also obtain good accuracy. In practice, our method requires a rough estimation of the number of persons in the target domain, which we think would still be manpower saving, because the target domain can be label-free.</p><p>Second, we evaluate the impact of λ, keeping K = 750. Since we use the cosine distance to measure the similarity of two feature vectors and ReLU (rectified linear unit) as activation function (feature elements ≥ 0), the range of λ should be [0, 1]. To further narrow the test range for λ, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Semi-supervised Re-ID</head><p>The semi-supervised learning scheme is briefly introduced in Section 3.3. In this case, we initialize the CNN model with labeled images of L IDs. These L labeled images are also used in the subsequent CNN fine-tuning iterations. In our experiment, we set L = 25 and L = 50. We report the results in Table <ref type="table" target="#tab_4">4</ref>. Two major observations can be made. First, comparing with the unsupervised learning method (PUL), adding 25 or 50 IDs as supervised information notably improves the re-ID accuracy. For example, when testing on Market-1501 with 25 labeled IDs, Duke and CUHK03, we observe improvement of +2.2%, +2.7%, and +0.8% in rank-1 accuracy, respectively. The improvement on CUHK03 is smaller compared with Market and Duke. It is probably because of the severe illumination change and less diversified training samples in CUHK03: learning an invariant feature is extremely difficult.</p><p>Second, we observe that using more IDs as training samples is beneficial to the system. For example, when testing on Market-1501, using 50 IDs brings about +3.2% more improvement than using 25 IDs. On the other two testing sets, the additional improvement brought by the 50 IDs is +3.8% and +0.2% in rank-1 accuracy, respectively. These results suggest that using more labeled data will increase the model performance at the cost of higher labeling cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Comparison with Other Methods</head><p>We then compare the PUL method with competing unsupervised methods. As we mentioned in Section 2.2, limited attention has been paid to unsupervised learning for re-ID and fewer works have adopted deep features. Since these unsupervised methods mainly focus on re-ID on relatively small datasets, benchmarking results are lacking on larger datasets such as Market. In this section, we evaluate the performances of two hand-crafted features, i.e., Bag-of-Words (BoW) <ref type="bibr" target="#b59">[60]</ref>, local maximal occurrence (LOMO) <ref type="bibr" target="#b23">[24]</ref> and one transfer learning method, i.e., the Unsupervised Multitask Dictionary Learning (UMDL) model <ref type="bibr" target="#b32">[33]</ref> on the large-scale datasets. To our knowledge, except hand-crafted features, UMDL is the only work on unsupervised re-ID whose code is released.</p><p>Note that, for BoW and LOMO, there is no training process. We directly apply these two handcrafted features on the test datasets. After feature extraction, we normalize the feature vectors and use Euclidean distance to measure the similarity.</p><p>UMDL needs multiple datasets to learning a task-independent dictionary. To meet this requirement, we use two of the three datasets as labeled datasets and treat the remaining one as an unlabeled dataset. Since the released code of UMDL does not include feature extraction, we use the BoW feature proposed in Reference <ref type="bibr" target="#b59">[60]</ref> for this method. Since the computational complexity of UMDL is prohibitively high, we choose 2,500 images from each dataset in training. For PUL, we still fix the reliability threshold λ to 0.85 for all the three datasets. The number of clusters K is set to 750 for Market and CUHK03, and 700 for Duke. Results are presented in Table <ref type="table" target="#tab_4">4</ref>, from which two conclusions can be drawn.</p><p>First, when comparing Tables <ref type="table">2</ref> and<ref type="table" target="#tab_4">4</ref>, we find that initialization using more labeled datasets does not noticeably improve re-ID accuracy. Sometimes, multi-dataset initialization yields even worse results than single dataset initialization. For example, when tested on duke, PUL achieves 30.0% in rank-1 accuracy when initialized by both Market+CUHK03 but 30.4% in rank-1 accuracy when initialized only by Market. The baseline approach also exhibits similar phenomenon when tested on CUHK03. This indicates that using more labeled datasets may be not a best choice for unsupervised re-ID. In comparison, PUL with unlabeled data can always improve the re-ID accuracy, regardless of the datasets used for CNN initialization.</p><p>Second, both the CNN baseline and PUL outperform UMDL by a large margin. For example, when tested on three datasets, the CNN baseline outperforms UMDL by +5.5%, +4.4%, and +2.9% in rank-1 accuracy, respectively. When PUL is performed, we observe larger performance gaps, arriving at +8.7%, +11.5%, and +6.7% in rank-1 accuracy on the three datasets, respectively. Under the semi-supervised setting (Section 3.3), the superiority of PUL is even more significant. For example, when using 50 labeled IDs, PUL outperforms UMDL by +15.3%, +17.0%, and +7.5% in rank-1 accuracy on the three datasets, respectively. In fact, as more data is labeled, UMDL does not exhibit noticeable improvement, while PUL shows clear performance gain. Moreover, considering the training inefficiency of UMDL, we can thus validate the effectiveness of PUL.</p><p>We additionally compared our method with Person Transfer Generative Adversarial Network (PTGAN) <ref type="bibr" target="#b41">[42]</ref> in Table <ref type="table">2</ref>, which adopts a person transfer generative adversarial network to bridge domain gap for person re-Id. Note that, for CUHK03, PTGAN uses a different train/test protocol, which allows the model to exploit more unlabeled training data than ours. We therefore do not report the accuracy of PTGAN on CUHK03 in Table <ref type="table">2</ref>. Being easy to implement, our PUL achieves favorable accuracy to PTGAN. For example, when initialized on Duke and tested on Market, our method outperforms PTGAN by 6.1% in rank-1 accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this article, a simple progressive unsupervised learning (PUL) method is proposed for person re-ID, based on the iterations between k-means clustering and CNN fine-tuning. We show that reliable sample selection is a key component to its success. Progressively, the proposed method produces CNN models with high discriminative ability.</p><p>The proposed method is easy to implement and a number of further extensions can be made. For example, as mentioned in Reference <ref type="bibr" target="#b61">[62]</ref>, in video re-ID, frames within a tracklet can be viewed as belonging to the same ID, which can be used to initialize clustering and fine-tuning. Another promising idea is to integrate diversity into PUL, so that training samples from multiple cameras can be selected for fine-tuning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Illustration of the PUL framework. In step 0, we initialize CNN on an irrelevant labeled dataset. Then, we go into the iterations. During each iteration, we (1) extract CNN features for the unlabeled dataset and perform clustering and sample selection, and (2) fine-tune the CNN model using the selected samples. Each cluster denotes a person.</figDesc><graphic coords="3,56.07,93.76,374.44,166.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>as the selection indication vector and y = [y 1 , . . . ,y N ] ∈ {1, . . . , K } N as the identity vector for {x i } N i=1 . The backbone CNN model (without the classification layer) is as denoted ϕ (•; θ ), which is parameterized by θ . The output of ϕ (•; θ ) is a one-dimensional feature embedding, which is fed into a classifier parameterized by w. Note that a typical classification CNN model is composed of θ and w. During CNN training, θ and w are optimized simultaneously under a classification model.83:6 H. Fan et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>7 Fig. 2 .</head><label>72</label><figDesc>Fig. 2. Illustration of the learning process. Reliable samples covered in the blue disc are used for CNN finetuning. As the CNN model gets fine-tuned, more challenging training data can be mined (contained in the enlarged blue disc).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Two visual examples of the working mechanism of progressive unsupervised learning (PUL). Each color denotes a distinct ID and the red circles represent reliable areas. From training iteration 2 to training iteration 3, more samples are selected as reliable samples. In PUL, when the number of selected samples is saturated, the algorithm reaches convergence.</figDesc><graphic coords="7,52.44,282.07,380.56,117.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>ALGORITHM 1 :</head><label>1</label><figDesc>Progressive Unsupervised Learning Input: Unlabeled data {x i } N i=1 , reliability threshold λ, number of clusters K, original model ϕ (•, θ o ).</figDesc><table><row><cell>Output: Model ϕ (•, θ t ).</cell></row><row><cell>Initialization: θ o → θ t .</cell></row><row><cell>while not convergence do</cell></row><row><cell>// for each training iteration</cell></row><row><cell>randomly initialize w;</cell></row><row><cell>extract feature: f</cell></row></table><note><p>i = ϕ (x i , θ t ) for all i; k-means: update {y i } N i=1 and {c k } K k=1 ; select center features:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Statistics of Duke<ref type="bibr" target="#b63">[64]</ref>, Market<ref type="bibr" target="#b59">[60]</ref>, and CUHK03<ref type="bibr" target="#b22">[23]</ref> Datasets</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Training</cell><cell></cell><cell></cell><cell>Test</cell><cell></cell></row><row><cell cols="2">Datasets # cams</cell><cell cols="2"># IDs # images</cell><cell cols="4">Query # IDs # images # IDs # images Gallery</cell></row><row><cell>Duke</cell><cell>8</cell><cell>702</cell><cell>16,522</cell><cell>702</cell><cell>2,228</cell><cell>1,110</cell><cell>17,661</cell></row><row><cell>Market</cell><cell>6</cell><cell>751</cell><cell>12,936</cell><cell>750</cell><cell>3,368</cell><cell>750</cell><cell>19,732</cell></row><row><cell>CUHK03</cell><cell>2</cell><cell>767</cell><cell>7,365</cell><cell>700</cell><cell>1,400</cell><cell>700</cell><cell>5,332</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Re-ID Accuracy of PUL Without</figDesc><table><row><cell></cell><cell>Sample Selection</cell><cell></cell></row><row><cell cols="3"># clusters (K) 250 500 750 1,000 1,250</cell></row><row><cell>rank-1</cell><cell>32.2 34.6 37.3 36.7</cell><cell>37.5</cell></row><row><cell>mAP</cell><cell>12.8 14.5 15.7 14.6</cell><cell>15.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Person re-ID Accuracy (Rank-1, -5, -10, -20 Precision and mAP) of Five Models: BoW, LOMO, UMDL, The Baseline (Fine-Tuned ResNet-50 on Two of the Three Datasets and Tested on the Last One), and PUL (Both the Unsupervised and the Semi-supervised Versions are Shown)</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Duke, CUHK03 → Market</cell><cell></cell><cell cols="3">Market, CUHK03 → Duke</cell><cell></cell><cell></cell><cell cols="3">Duke, Market → CUHK03</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Methods</cell><cell cols="14">Rank-1 Rank-5 Rank-10 Rank-20 mAP Rank-1 Rank-5 Rank-10 Rank-20 mAP Rank-1 Rank-5 Rank-10 Rank-20 mAP</cell></row><row><cell>Unsupervised</cell><cell cols="3">BoW[60] LOMO[24] UMDL[33] Hand Deep feature (baseline) 40.0 35.8 27.2 34.5</cell><cell>52.4 41.6 52.6 56.5</cell><cell>60.3 49.1 59.6 63.7</cell><cell>67.6 57.2 68.0 70.2</cell><cell>14.8 17.1 8.0 12.3 12.4 18.5 17.0 22.9</cell><cell>28.8 21.3 31.4 37.3</cell><cell>34.9 26.6 37.6 44.2</cell><cell>41.2 33.3 44.7 50.8</cell><cell>8.3 4.8 7.3 11.3</cell><cell>2.1 0.6 1.4 4.3</cell><cell>4.6 1.9 5.0 10.1</cell><cell>7.0 3.6 7.1 15.1</cell><cell>11.2 5.4 9.7 19.4</cell><cell>1.9 0.7 1.3 3.9</cell></row><row><cell></cell><cell cols="2">Deep feature (PUL)</cell><cell>45.5</cell><cell>60.7</cell><cell>66.7</cell><cell>72.6</cell><cell>20.5 30.0</cell><cell>43.4</cell><cell>48.5</cell><cell>54.0</cell><cell>16.4</cell><cell>8.1</cell><cell>15.8</cell><cell>22.0</cell><cell>28.5</cell><cell>7.9</cell></row><row><cell>Semi-sup</cell><cell>25ID 50ID</cell><cell>UMDL[33] Deep feature (PUL) UMDL[33] Deep feature (PUL)</cell><cell>35.2 47.7 35.6 50.9</cell><cell>53.2 63.6 53.5 66.5</cell><cell>60.2 69.3 60.4 72.6</cell><cell>68.4 75.5 68.6 78.3</cell><cell>13.3 18.9 22.2 32.7 13.4 19.5 24.8 36.5</cell><cell>31.7 47.2 32.4 52.6</cell><cell>37.8 53.0 38.6 57.9</cell><cell>45.2 59.1 45.7 64.5</cell><cell>7.6 18.8 8.3 21.5</cell><cell>1.4 8.9 1.6 9.1</cell><cell>5.2 18.4 5.4 18.6</cell><cell>7.2 23.8 7.5 25.1</cell><cell>10.0 28.5 10.2 32.5</cell><cell>1.3 8.8 1.4 9.2</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>ACM Trans. Multimedia Comput. Commun. Appl., Vol. 14, No. 4, Article 83. Publication date: October 2018.</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Part of this work was done when Hehe Fan was visiting Hangzhou Dianzi University. This work is in part supported by National Nature Science Foundation of China (Grants No. 61671196, No. 61525206, and No. 61701149), in part supported by Zhejiang Province Nature Science Foundation of China LR17F030006, in part supported by National Key Research and Development Program of China (Grant No. 2017YFC0820600), 111 Project, No. D17019, in part supported by Data to Decisions CRC (D2D CRC) and in part supported by Cooperative Research Centres Programme. Liang Zheng is the recipient of a SIEF STEM+ Business fellowship.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>rotate images within 20 degrees and shift them horizontally and vertically within 45 pixels. Batch size is set to 16. We use stochastic gradient descent with a momentum of 0.9 and the learning rate is set to 0.001 without any decay during the whole training process. Unless otherwise specified, ResNet-50 is fine-tuned for 20 epochs within each PUL training iteration. During feature extraction, for a given image, we use the output of the average-pooling layer as the visual representation. The l 2 normalized representations are used in sample selection and retrieval, while clustering uses features without normalization (l 2 normalization in clustering yields inferior results in our preliminary experiments).</p><p>For the clustering step, we use k-means++ <ref type="bibr" target="#b0">[1]</ref> to select initial cluster centers for k-mean clustering to speed up convergence. The maximum number of iterations of the k-means algorithm within each PUL iteration is set to 300.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Performance of Baseline System</head><p>The basic ResNet-50 model is fine-tuned for 40 epochs on each training set. Note that our focus is not on how to improve the performance by modifying neural networks <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b63">64]</ref> or post-processing <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b64">65]</ref>. Therefore, results in Table <ref type="table">2</ref> do not represent the state of the art.  we present the proportions of selected samples under different λ in the first training iteration in Figure <ref type="figure">5(c</ref>). Note that each training iteration contains 20 fine-tuned epochs for the ResNet-50. When λ &lt; 0.7, nearly all of data instances are selected as being reliable, which is undesirable. Therefore, we choose λ from {0.70, 0.75, 0.80, 0.85, 0.90}. Figure <ref type="figure">5</ref>(d) demonstrates the change of the portion of selected samples during training. As training goes on, more and more samples are selected. Let us take λ = 0.85 for instance. Before the 9th training iteration, the proportion of selected samples increases fast from 31.8% to 45.3%. After that, the number oscillates between 45.3% and 47.4%. The curve becomes saturated after 20 epochs, which indicates that PUL training can be stopped when the number of selected samples converges.</p><p>An interesting phenomenon in Figure <ref type="figure">5</ref>(d) is that, when λ ≤ 0.85, the proportion of reliable samples in the 1st training iteration witnesses a sudden drop after the 2nd training iteration. This is because, in the 1st training iteration, the original model fine-tuned on Duke is too weak to discriminate the data from Market. As a result, too many samples are incorrectly selected as reliable samples. In the 2nd training iteration, the model has been fine-tuned on Market and can better separate reliable and unreliable samples on Market.</p><p>We report the re-ID performance with different λ in Figures 5(e) and 5(f). In general, performance improves with more iterations, indicating that the model gradually gets stronger. Specifically, λ = 0.85 yields superior accuracy.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">2007. k-means++: The advantages of careful seeding</title>
		<author>
			<persName><forename type="first">David</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergei</forename><surname>Vassilvitskii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA&apos;07</title>
		<meeting>the 18th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA&apos;07</meeting>
		<imprint>
			<biblScope unit="page" from="1027" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Visual tracking with online multiple instance learning</title>
		<author>
			<persName><forename type="first">Boris</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPRW.2009.5206737</idno>
		<ptr target="https://doi.org/10.1109/CVPRW.2009.5206737" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;09)</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;09)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="983" to="990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Scalable person re-identification on supervised smoothed manifold</title>
		<author>
			<persName><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.358</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2017.358" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR&apos;17)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR&apos;17)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3356" to="3365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Regularized diffusion process on bidirectional context for object retrieval</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Latecki</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2018.2828815</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2018.2828815" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Ensemble diffusion for retrieval</title>
		<author>
			<persName><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longin</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Latecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.90</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2017.90" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV&apos;17)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV&apos;17)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="774" to="783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jérôme</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="DOI">10.1145/1553374.1553380</idno>
		<ptr target="https://doi.org/10.1145/1553374.1553380" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning (ICML&apos;09)</title>
		<meeting>the 26th Annual International Conference on Machine Learning (ICML&apos;09)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Similarity learning with spatial constraints for person re-identification</title>
		<author>
			<persName><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zejian</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Badong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.142</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.142" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR&apos;16)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR&apos;16)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1268" to="1277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Beyond triplet loss: A deep quadruplet network for person re-identification</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaotang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianguo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.145</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2017.145" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR&apos;17)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR&apos;17)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1320" to="1329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Person re-identification by multichannel parts-based CNN with improved triplet loss function</title>
		<author>
			<persName><forename type="first">De</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yihong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanping</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinjun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.149</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.149" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR&apos;16)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR&apos;16)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1335" to="1344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Triplet-based deep hashing network for cross-modal retrieval</title>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaojia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianglong</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2018.2821921</idno>
		<ptr target="https://doi.org/10.1109/TIP.2018.2821921" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3893" to="3903" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note>Xinbo Gao, and Dacheng Tao</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Few-example object detection with model communication</title>
		<author>
			<persName><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2018.2844853</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2018.2844853" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Complex event detection by identifying reliable shots from untrimmed videos</title>
		<author>
			<persName><forename type="first">Hehe</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">De</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.86</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2017.86" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV&apos;17</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV&apos;17</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="736" to="744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Person reidentification by symmetry-driven accumulation of local features</title>
		<author>
			<persName><forename type="first">Michela</forename><surname>Farenzena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loris</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Perina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vittorio</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Cristani</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2010.5539926</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2010.5539926" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd IEEE Conference on Computer Vision and Pattern Recognition (CVPR&apos;10)</title>
		<meeting>the 23rd IEEE Conference on Computer Vision and Pattern Recognition (CVPR&apos;10)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2360" to="2367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName><forename type="first">Pedro</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2009.167</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2009.167" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deep transfer learning for person reidentification</title>
		<author>
			<persName><forename type="first">Mengyue</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaowei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghong</forename><surname>Tian</surname></persName>
		</author>
		<idno>arXiv abs/1611.05244</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Viewpoint invariant pedestrian recognition with an ensemble of localized features</title>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Tao</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-540-88682-2_21</idno>
		<ptr target="https://doi.org/10.1007/978-3-540-88682-2_21" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th European Conference on Computer Vision (ECCV&apos;08)</title>
		<meeting>the 10th European Conference on Computer Vision (ECCV&apos;08)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="262" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.90" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR&apos;16)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR&apos;16)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<idno>arXiv abs/1703.07737</idno>
		<title level="m">defense of the triplet loss for person re-identification</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Self-paced learning with diversity</title>
		<author>
			<persName><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shoou-I</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen-Zhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference on Neural Information Processing Systems</title>
		<meeting>the Annual Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2078" to="2086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Person re-identification by unsupervised l 1 graph learning</title>
		<author>
			<persName><forename type="first">Elyor</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen-Yong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46448-0_11</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46448-0_11" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th European Conference on Computer Vision (ECCV&apos;16)</title>
		<meeting>the 14th European Conference on Computer Vision (ECCV&apos;16)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="178" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual Conference on Neural Information Processing Systems</title>
		<meeting>the 26th Annual Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Self-paced learning for latent variable models</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Pawan</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th Annual Conference on Neural Information Processing Systems</title>
		<meeting>the 24th Annual Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="1189" to="1197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">DeepReID: Deep filter pairing neural network for person reidentification</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2014.27</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2014.27" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR&apos;14</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR&apos;14</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="152" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Person re-identification by local maximal occurrence representation and metric learning</title>
		<author>
			<persName><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2015.7298832</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2015.7298832" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR&apos;15)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR&apos;15)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2197" to="2206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Improving person re-identification by attribute and identity learning</title>
		<author>
			<persName><forename type="first">Yutian</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno>arXiv abs/1703.07220</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">POP: Person re-identification post-rank optimisation</title>
		<author>
			<persName><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guijin</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2013.62</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2013.62" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV&apos;13)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV&apos;13)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="441" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">End-to-end comparative attention networks for person re-identification</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meibin</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianguo</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2017.2700762</idno>
		<ptr target="https://doi.org/10.1109/TIP.2017.2700762" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3492" to="3506" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-scale triplet CNN for person re-identification</title>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">I</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<idno type="DOI">10.1145/2964284.2967209</idno>
		<ptr target="https://doi.org/10.1145/2964284.2967209" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM Conference on Multimedia Conference (MM&apos;16)</title>
		<meeting>the 2016 ACM Conference on Multimedia Conference (MM&apos;16)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="192" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">PROVID: Progressive and multimodal vehicle reidentification for large-scale urban surveillance</title>
		<author>
			<persName><forename type="first">Xinchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huadong</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMM.2017.2751966</idno>
		<ptr target="https://doi.org/10.1109/TMM.2017.2751966" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="645" to="658" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Self-paced co-training</title>
		<author>
			<persName><forename type="first">Fan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zina</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning (ICML&apos;17</title>
		<meeting>the 34th International Conference on Machine Learning (ICML&apos;17</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2275" to="2284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Person re-identification by unsupervised video matching</title>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xudong</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kin-Man</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yisheng</forename><surname>Zhong</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2016.11.018</idno>
		<ptr target="https://doi.org/10.1016/j.patcog.2016.11.018" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="197" to="210" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The many shades of negativity</title>
		<author>
			<persName><forename type="first">Zhigang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMM.2017.2659221</idno>
		<ptr target="https://doi.org/10.1109/TMM.2017.2659221" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1558" to="1568" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unsupervised cross-dataset transfer learning for person re-identification</title>
		<author>
			<persName><forename type="first">Peixi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaowei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimiliano</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiejun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghong</forename><surname>Tian</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.146</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.146" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR&apos;16)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR&apos;16)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1306" to="1315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">CNN image retrieval learns from BoW: Unsupervised finetuning with hard examples</title>
		<author>
			<persName><forename type="first">Filip</forename><surname>Radenovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giorgos</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ondrej</forename><surname>Chum</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46448-0_1</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46448-0_1" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th European Conference on Computer Vision (ECCV&apos;16)</title>
		<meeting>the 14th European Conference on Computer Vision (ECCV&apos;16)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Performance measures and a data set for multi-target, multi-camera tracking</title>
		<author>
			<persName><forename type="first">Ergys</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><forename type="middle">S</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlo</forename><surname>Tomasi</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-48881-3_2</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-48881-3_2" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV&apos;16</title>
		<meeting>the European Conference on Computer Vision (ECCV&apos;16</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="17" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">ImageNet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-015-0816-y</idno>
		<ptr target="https://doi.org/10.1007/s11263-015-0816-y" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">FaceNet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2015.7298682</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2015.7298682" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR&apos;15)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR&apos;15)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">SVDNet for pedestrian retrieval</title>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.410</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2017.410" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV&apos;17)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV&apos;17)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3820" to="3828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Gated siamese convolutional neural network architecture for human re-identification</title>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Rama Varior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mrinal</forename><surname>Haloi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46484-8_48</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46484-8_48" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th European Conference on Computer Vision (ECCV&apos;16)</title>
		<meeting>the 14th European Conference on Computer Vision (ECCV&apos;16)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="791" to="808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Human-in-the-loop person re-identification</title>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46493-0_25</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46493-0_25" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th European Conference on Computer Vision (ECCV&apos;16)</title>
		<meeting>the 14th European Conference on Computer Vision (ECCV&apos;16)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="405" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Person re-identification by video ranking</title>
		<author>
			<persName><forename type="first">Taiqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-10593-2_45</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-10593-2_45" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th European Conference on Computer Vision (ECCV&apos;14</title>
		<meeting>the 13th European Conference on Computer Vision (ECCV&apos;14</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="688" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Person transfer GAN to bridge domain gap for person re-identification</title>
		<author>
			<persName><forename type="first">Longhui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<idno>arXiv abs/1711.08565</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Exploit the unknown gradually: Oneshot video-based person re-identification by stepwise learning</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutian</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR&apos;18</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR&apos;18</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5177" to="5186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning deep feature representations with domain guided dropout for person re-identification</title>
		<author>
			<persName><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.140</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.140" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR&apos;16)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR&apos;16)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1249" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Joint detection and identification feature learning for person search</title>
		<author>
			<persName><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bochao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.360</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2017.360" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR&apos;17)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR&apos;17)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3376" to="3385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Effective uyghur language text detection in complex background images for traffic prompt identification</title>
		<author>
			<persName><forename type="first">Chenggang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongtao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Yin</surname></persName>
		</author>
		<idno type="DOI">10.1109/TITS.2017.2749977</idno>
		<ptr target="https://doi.org/10.1109/TITS.2017.2749977" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transport. Syst</title>
		<editor>
			<persName><forename type="first">Qionghai</forename><surname>Zhang</surname></persName>
		</editor>
		<editor>
			<persName><surname>Dai</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="220" to="229" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Supervised hash coding with deep neural network for environment perception of intelligent vehicles</title>
		<author>
			<persName><forename type="first">Chenggang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongtao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongbao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Yin</surname></persName>
		</author>
		<idno type="DOI">10.1109/TITS.2017.2749965</idno>
		<ptr target="https://doi.org/10.1109/TITS.2017.2749965" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transport. Syst</title>
		<editor>
			<persName><forename type="first">Qionghai</forename><surname>Zhang</surname></persName>
		</editor>
		<editor>
			<persName><surname>Dai</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="284" to="295" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Enhancing person re-identification in a selftrained subspace</title>
		<author>
			<persName><forename type="first">Xun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
		<idno type="DOI">10.1145/3089249</idno>
		<ptr target="https://doi.org/10.1145/3089249" />
	</analytic>
	<monogr>
		<title level="j">TOMCCAP</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Feature selection for multimedia analysis by sharing information among multiple tasks</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhigang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMM.2012.2237023</idno>
		<ptr target="https://doi.org/10.1109/TMM.2012.2237023" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="661" to="669" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Unsupervised learning of multi-level descriptors for person re-identification</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longyin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siwei</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st AAAI Conference on Artificial Intelligence</title>
		<meeting>the 21st AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4306" to="4312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Person reidentification via ranking aggregation of similarity pulling and dissimilarity pushing</title>
		<author>
			<persName><forename type="first">Mang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingming</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunxia</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruimin</forename><surname>Hu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMM.2016.2605058</idno>
		<ptr target="https://doi.org/10.1109/TMM.2016.2605058" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="2553" to="2566" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep metric learning for person re-identification</title>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICPR.2014.16</idno>
		<ptr target="https://doi.org/10.1109/ICPR.2014.16" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Pattern Recognition (ICPR&apos;14</title>
		<meeting>the 22nd International Conference on Pattern Recognition (ICPR&apos;14</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="34" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning a discriminative null space for person re-identification</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.139</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.139" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR&apos;16)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR&apos;16)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1239" to="1248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Sample-specific SVM learning for person re-identification</title>
		<author>
			<persName><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baohua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atshushi</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.143</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.143" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR&apos;16)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR&apos;16)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1278" to="1287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Person re-identification by salience matching</title>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2013.314</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2013.314" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV&apos;13)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV&apos;13)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2528" to="2535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Unsupervised salience learning for person re-identification</title>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2013.460</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2013.460" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3586" to="3593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning mid-level filters for person re-identification</title>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2014.26</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2014.26" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR&apos;14)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR&apos;14)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="144" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">MARS: A video benchmark for large-scale person re-identification</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Bie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46466-4_52</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46466-4_52" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th European Conference On Compuer Vision (ECCV&apos;16)</title>
		<meeting>the 14th European Conference On Compuer Vision (ECCV&apos;16)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="868" to="884" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Pose invariant embedding for deep person reidentification</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno>arXiv abs/1701.07732</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Scalable person reidentification: A benchmark</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.133</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2015.133" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV&apos;15)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV&apos;15)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1116" to="1124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Query-adaptive late fusion for image search and person re-identification</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqiong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2015.7298783</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2015.7298783" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR&apos;15)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR&apos;15)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1741" to="1750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<idno>arXiv abs/1610.02984</idno>
		<title level="m">Person re-identification: Past, present and future</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="volume">83</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">SIFT meets CNN: A decade survey of instance retrieval</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2017.2709749</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2017.2709749" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1224" to="1244" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Unlabeled samples generated by GAN improve the person reidentification baseline in vitro</title>
		<author>
			<persName><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.405</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2017.405" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV&apos;17)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV&apos;17)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3774" to="3782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Re-ranking person re-identification with k-reciprocal encoding</title>
		<author>
			<persName><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donglin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.389</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2017.389" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR&apos;17)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR&apos;17)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3652" to="3661" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
