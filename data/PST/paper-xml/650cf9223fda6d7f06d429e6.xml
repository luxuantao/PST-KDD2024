<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RMT: Retentive Networks Meet Vision Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-09-20">20 Sep 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Qihang</forename><surname>Fan</surname></persName>
							<email>fanqihang.159@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="institution">CASIA</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">CASIA</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huaibo</forename><surname>Huang</surname></persName>
							<email>huaibo.huang@cripac.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="institution">CASIA</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">CASIA</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mingrui</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="institution">CASIA</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">CASIA</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hongmin</forename><surname>Liu</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of Science and Technology Beijing</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ran</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="institution">CASIA</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">CASIA</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">RMT: Retentive Networks Meet Vision Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-09-20">20 Sep 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2309.11523v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transformer first appears in the field of natural language processing and is later migrated to the computer vision domain, where it demonstrates excellent performance in vision tasks. However, recently, Retentive Network (RetNet) has emerged as an architecture with the potential to replace Transformer, attracting widespread attention in the NLP community. Therefore, we raise the question of whether transferring RetNet's idea to vision can also bring outstanding performance to vision tasks. To address this, we combine RetNet and Transformer to propose RMT. Inspired by RetNet, RMT introduces explicit decay into the vision backbone, bringing prior knowledge related to spatial distances to the vision model. This distance-related spatial prior allows for explicit control of the range of tokens that each token can attend to. Additionally, to reduce the computational cost of global modeling, we decompose this modeling process along the two coordinate axes of the image. Abundant experiments have demonstrated that our RMT exhibits exceptional performance across various computer vision tasks. For example, RMT achieves 84.1% Top1-acc on ImageNet-1k using merely 4.5G FLOPs. To the best of our knowledge, among all models, RMT achieves the highest Top1-acc when models are of similar size and trained with the same strategy. Moreover, RMT significantly outperforms existing vision backbones in downstream tasks such as object detection, instance segmentation, and semantic segmentation. Our work is still in progress.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Since the Transformer was proposed in the field of NLP <ref type="bibr" target="#b43">[45]</ref>, it has achieved outstanding performance in many downstream tasks. Despite the modality gap between com-* Ran He is the corresponding author. Top1 Acc. MaxViT-T <ref type="bibr" target="#b26">[28]</ref> 31M 83.6 SMT-S <ref type="bibr" target="#b29">[31]</ref> 20M 83.7 BiFormer-S <ref type="bibr" target="#b64">[66]</ref> 26M 83.8 RMT-S (Ours) 27M 84.1 BiFormer-B <ref type="bibr" target="#b64">[66]</ref> 57M 84.3 MaxViT-S <ref type="bibr" target="#b24">[26]</ref> 69M 84.5 RMT-B (Ours) 54M 85.0 SMT-L <ref type="bibr" target="#b29">[31]</ref> 81M 84.6 MaxViT-B <ref type="bibr" target="#b42">[44]</ref> 120M 84.9 RMT-L (Ours) 95M 85.5 puter vision and natural language processing, researchers have successfully migrated this architecture to vision tasks, bringing us once again a huge surprise like in previous NLP tasks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b21">23,</ref><ref type="bibr" target="#b26">28]</ref>.</p><p>Recently, several powerful architectures <ref type="bibr" target="#b34">[36,</ref><ref type="bibr" target="#b39">41]</ref> have emerged in the NLP community, yet no work has attempted to transfer these NLP architectures to visual tasks. Compared to Transformer, Retentive Network (RetNet) <ref type="bibr" target="#b39">[41]</ref> has demonstrated stronger performance on a range of NLP tasks. Therefore, we hope to be able to transfer this robust NLP architecture, RetNet, to the domain of vision.</p><p>The fundamental operator in RetNet is retention. A significant difference between retention and the basic Self-Attention operator in Transformer is the introduction of decay coefficients, which explicitly control the attention weights of each token with respect to its neighboring tokens, ensuring that the attention weights decay as the distance between tokens increases. This decay effectively introduces prior knowledge about one-dimensional distance into the model, resulting in improved performance.</p><p>Based on the findings in RetNet, we attempt to further improve retention into a 2D form and introduce it to visual tasks. Specifically, the original version of retention undergoes unidirectional decay, which suits the causal properties of NLP. However, for images without causal properties, this unidirectional decay is not suitable. Therefore, we first expand retention from unidirectional to bidirectional. Additionally, the original version of retention, designed for onedimensional sequential information, is not appropriate for use in two-dimensional space. Hence, considering the spatial characteristics of two dimensions, we design a decay matrix based on 2D distance. Finally, to address the high computational load caused by a large number of tokens during the early stage of the vision backbone, we decompose the 2D computation process separately along the two axes of the image. We name this mechanism adapted to images as the Retentive Self-Attention (ReSA) mechanism. Based on the ReSA mechanism, we construct the RMT family.</p><p>We demonstrate the effectiveness of the proposed method through extensive experiments. As shown in Fig. <ref type="figure" target="#fig_1">1</ref>, our RMT significantly outperforms the state-of-theart (SOTA) models on image classification tasks. Additionally, our model exhibits more prominent advantages compared to other models in tasks such as object detection and instance segmentation. Our contributions can be summarized as follows:</p><p>? We extend the core mechanism of the Retentive Network, retention, to the two-dimensional scenario, introducing spatial prior knowledge related to distances into vision models. The new mechanism is called Retentive Self-Attention (ReSA). ? We decompose ReSA along two image axes, reducing computational complexity. This decomposition method effectively minimizes the computational burden while having minimal impact on the model's performance. ? Extensive experiments demonstrate the excellent performance of RMT. Particularly in downstream tasks such as object detection and instance segmentation, RMT exhibits significant advantages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Transformer. Transformer architecture was firstly proposed in <ref type="bibr" target="#b43">[45]</ref> to address the training limitation of recurrent model and then achieve massive success in many NLP tasks. By splitting the image into small, non-overlapped patches sequence, Vision Transformer (ViTs) <ref type="bibr" target="#b9">[10]</ref> also have attracted great attention and become widely used on vision tasks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">17,</ref><ref type="bibr" target="#b33">35,</ref><ref type="bibr" target="#b49">51,</ref><ref type="bibr" target="#b57">59]</ref>. Unlike in the past, where RNNs and CNNs have respectively dominated the NLP and CV fields, the transformer architecture has shined through in various modalities and fields <ref type="bibr" target="#b22">[24,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b35">37,</ref><ref type="bibr" target="#b51">53]</ref>.</p><p>Prior Knowledge in Transformer. Numerous attempts have been made to incorporate prior knowledge into the Transformer model to enhance its performance. The original Transformers <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b43">45]</ref> use trigonometric position encoding to provide positional information for each token. <ref type="bibr" target="#b30">[32]</ref> proposes the use of relative positional encoding as a replacement for the original absolute positional encoding. <ref type="bibr" target="#b4">[5]</ref> points out that zero padding in convolutional layers could also provide positional awareness for the Transformer, and this method of position encoding is highly efficient. In many studies, ConvFFN <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b45">47]</ref> has been employed to further enrich the positional information in the Transformer. Furthermore, in the recent Retentive Network <ref type="bibr" target="#b39">[41]</ref>, explicit attenuation has been introduced to provide the model with prior knowledge based on distance changes.</p><p>Retentive Network. Retentive Network <ref type="bibr" target="#b39">[41]</ref> proposes the retention mechanism for sequence modeling. Compared to traditional Transformer based models <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b21">23,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b43">45]</ref>, retention proposed in RetNet uses the explicit decay to model the prior of 1D distance. It includes three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent. In retention, it uses a decay matrix multiplied by a weight matrix to control the proportion of each token seeing its surrounding tokens based on distance priors. We attempt to extend this idea to 2D space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminary</head><p>Retentive Network. Retentive Network (RetNet) is a powerful architecture for language models. This work proposes the retention mechanism for sequence modeling. Retention brings the explicit decay to the language model, which Transformers do not have. Retention firstly considers a sequence modeling problem in a recurrent manner. It can be written as Eq. 1:</p><formula xml:id="formula_0">o n = n m=1 ? n-m (Q n e in? )(K m e im? ) ? v m<label>(1)</label></formula><p>During training, for a parallel training process, Eq. 1 is writed as Eq. 2:</p><formula xml:id="formula_1">Q = (XW Q ) ? ?, K = (XW K ) ? ?, V = XW V ? n = e in? , D nm = ? n-m , n ? m 0, n &lt; m Retention(X) = (QK ? ? D)V<label>(2</label></formula><p>) where ? is the complex conjugate of ?, and D ? R |x|?|x| contains both causal masking and exponential decay, which</p><formula xml:id="formula_2">Conv Stem Stage1 RMT Block ? ? 1 Conv 3 ? 3 Stride 2 Stage2 RMT Block ? ? 2 Conv 3 ? 3 Stride 2 Stage3 RMT Block ? ? 3 Conv 3 ? 3 Stride 2 Conv 3 ? 3 Stride 2 Stage4 RMT Block ? ? 4 LN DWConv 3 ? 3 ReSA LN FFN</formula><p>RMT Block Retentive Self-Attention symbolize the relative distance in one-dimensional sequence, which brings the prior knowledge. Based on the 1D explicit decay in the retention, we try to develop it to 2D and bring the spatial prior knowledge to the vision model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Retentive Self-Attention</head><p>Unidirectional to Bidirectional. Due to the causal nature of language tasks, the retention in RetNet is unidirectional, meaning that each token can only attend to the tokens preceding it and not those following it. This is not suitable for tasks without causal properties, such as image recognition tasks. Therefore, we first extend the retention to two dimensions, where for each token, its output becomes Eq. 3:</p><formula xml:id="formula_3">o n = N m=1 ? |n-m| (Q n e in? )(K m e im? ) ? v m (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>where N is the number of tokens. The equation can be rearranged into a parallel form, expressed as Eq. 4:</p><formula xml:id="formula_5">BiRetention(X) = (QK ? ? D Bi )V D Bi nm = ? |n-m|<label>(4)</label></formula><p>where BiRetention denotes the retention with bidirectional modeling ability.</p><p>1D to 2D. Although retention now has the ability for bidirectional modeling, this modeling capability remains limited to a one-dimensional level and is still not applicable to two-dimensional images. Therefore, we further extend the one-dimensional retention to two dimensions. For images, each token has a unique two-dimensional coordinate within the plane. For the nth token, we use (x n , y n ) to represent its two-dimensional coordinate. Based on the 2D coordinates of each token, we modify each element in the matrix D to be the Manhattan distance between the corresponding token pairs at their respective positions, completing the transformation from a 1D to a 2D decay coefficient. The matrix D transfers to Eq. 5:</p><formula xml:id="formula_6">D 2d nm = ? |xn-xm|+|yn-ym|<label>(5)</label></formula><p>In the retention <ref type="bibr" target="#b39">[41]</ref>, the Softmax is abandoned and replaced with a gating function to increase the nonlinearity of the operator. However, according to our experiments, this approach does not yield better results for vision models. Instead, it introduces additional parameters and computational complexity. Therefore, we still use Softmax to introduce nonlinearity to our model. Based on the steps mentioned above, our Retentive Self-Attention can be expressed as Eq. 6:</p><formula xml:id="formula_7">ReSA(X) = Softmax(QK ? ? D 2d )V D 2d nm = ? |xn-xm|+|yn-ym|<label>(6)</label></formula><p>Decomposed ReSA in Early Stage. The current ReSA is not entirely applicable to image recognition tasks. This is because, in the early stages of the vision backbone, there are a large number of tokens, resulting in excessive computational costs for Attention. This is also the problem that most variants of Vision Transformers strive to solve <ref type="bibr" target="#b37">[39,</ref><ref type="bibr" target="#b48">50,</ref><ref type="bibr" target="#b49">51,</ref><ref type="bibr" target="#b52">54,</ref><ref type="bibr" target="#b57">59,</ref><ref type="bibr" target="#b61">63]</ref>.</p><p>Our ReSA also encounters this problem. Therefore, we decompose ReSA into two axes of the image, as described in the specific process shown in Eq. 7:</p><formula xml:id="formula_8">Q H , K H = (Q, K) B,L,C-&gt;B,W,H,C Q W , K W = (Q, K) B,L,C-&gt;B,H,W,C Attn H = Softmax(Q H K ? H ? D H ) Attn W = Softmax(Q W K ? W ? D W ) D H nm = ? |yn-ym| , D W nm = ? |xn-xm| ReSA dec (X) = Attn H (Attn W V ) (7)</formula><p>Based on this decomposition of ReSA, the shape of the receptive field of each token is shown in Fig. <ref type="figure" target="#fig_3">3</ref>, which is identical to the shape of the complete ReSA's receptive field.</p><p>In order to further enhance the local expression capability of ReSA, we also introduce a local enhancement module using DWConv:</p><formula xml:id="formula_9">X out = ReSA(X) + LCE(X);<label>(8)</label></formula><p>Retentive Self-Attention (decomposed) : matrix multiplication </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Overall Architecture</head><p>The architecture of our entire model is shown in Fig Table <ref type="table">3</ref>. Comparison to other backbones using Mask R-CNN with "3 ? +MS" schedule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We conducted extensive experiments on multiple vision tasks, such as image classification on ImageNet-1K <ref type="bibr" target="#b7">[8]</ref>, object detection and instance segmentation on COCO 2017 <ref type="bibr" target="#b28">[30]</ref>, and semantic segmentation on ADE20K <ref type="bibr" target="#b63">[65]</ref>.</p><p>We also make ablation studies to validate the importance of each component in RMT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Image Classification</head><p>Settings. We train our models on ImageNet-1K <ref type="bibr" target="#b7">[8]</ref> from scratch. And we follow the same training strategy in DeiT <ref type="bibr" target="#b41">[43]</ref> for a fair comparison. The maximum rates of increasing stochastic depth <ref type="bibr" target="#b19">[21]</ref> are set to 0.1/0.15/0.4/0.5 for RMT-T/S/B/L <ref type="bibr" target="#b19">[21]</ref>, respectively. We use the AdamW optimizer with a cosine decay learning rate scheduler to train the models. We set the initial learning rate, weight decay, and batch size to 0.001, 0.05, and 1024, respectively. We adopt the strong data augmentation and regularization used in <ref type="bibr" target="#b30">[32]</ref>. Our settings are RandAugment <ref type="bibr" target="#b6">[7]</ref> (randm9-mstd0.5-inc1), Mixup <ref type="bibr" target="#b60">[62]</ref> (prob=0.8), CutMix <ref type="bibr" target="#b59">[61]</ref> (prob=1.0), Random Erasing <ref type="bibr" target="#b62">[64]</ref> (prob=0.25).</p><p>Results. We compare RMT against many state-of-the-art models in Tab. 1. Results in the table demonstrate that RMT consistently outperforms previous models across all set-tings. Specifically, RMT-S achieves 84.1% Top1-accuracy with only 4.5 GFLOPs. RMT-B also surpasses iFormer <ref type="bibr" target="#b38">[40]</ref> by 0.4% with similar FLOPs. Furthermore, our RMT-L model surpasses MaxViT-B <ref type="bibr" target="#b42">[44]</ref> in top1-accuracy by 0.6% while using fewer FLOPs. Our RMT-T has also outperformed many lightweight models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Object Detection and Instance Segmentation</head><p>Settings. We adopt MMDetection <ref type="bibr" target="#b2">[3]</ref> to implement Reti-naNet <ref type="bibr" target="#b27">[29]</ref> and Mask-RCNN <ref type="bibr" target="#b18">[20]</ref>. We use the commonly used "1?" (12 training epochs) setting for the two strategies and "3 ? +MS" for Mask-RCNN. Following <ref type="bibr" target="#b30">[32]</ref>, during training, images are resized to the shorter side of 800 pixels while the longer side is within 1333 pixels. We adopt the AdamW optimizer with a learning rate of 0.0001, weight decay of 0.05, and batch size of 16 to optimize the model.</p><p>For "1?" schedule, the learning rate declines with the decay rate of 0. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Semantic Segmentation</head><p>Settings. We adopt the Semantic FPN <ref type="bibr" target="#b23">[25]</ref> and Uper-Net <ref type="bibr" target="#b50">[52]</ref> based on MMSegmentation <ref type="bibr" target="#b5">[6]</ref>, apply RMTs which are pretrained on ImageNet-1K as backbone. We use the same setting of PVT <ref type="bibr" target="#b44">[46]</ref> to train the Semantic FPN, and we train the model for 80k iterations. All models are trained with the input resolution of 512 ? 512. When testing the model, we resize the shorter side of the image to 512 pixels. As for UperNet, we follow the default settings in Swin <ref type="bibr" target="#b30">[32]</ref>. We take AdamW with a weight decay of 0.01 as the optimizer to train the models for 160K iterations. The learning rate is set to 6?10 -5 with 1500 iterations warmup.</p><p>Results. The results of semantic segmentation can be found in Tab. 4 and Tab. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>? decay. We verify the impact of explicit decay on the model, as shown in the Tab. 6. Explicit decay improves the performance of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We propose RMT, a vision backbone that integrates retentive network and Vision Transformer. RMT introduces explicit decay related to distance, which brings spatial prior knowledge to visual models. The new mechanism is called Retentive Self-Attention (ReSA). To reduce the complexity of the model, RMT also employs a method that decomposes the ReSA into two axes. Extensive experiments validate the effectiveness of RMT, especially in downstream tasks such as object detection, where RMT demonstrates significant advantages. Our work is still in progress.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. FLOPs vs. Top-1 accuracy on ImageNet-1K with 224 ? 224 resolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Overall architecture of RMT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Illustration of decomposed ReSA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison</figDesc><table><row><cell cols="2">Cost</cell><cell>Model</cell><cell>Parmas (M)</cell><cell>FLOPs (G)</cell><cell>Top1-acc (%)</cell><cell cols="2">Cost</cell><cell>Model</cell><cell>Parmas (M)</cell><cell>FLOPs (G)</cell><cell>Top1-acc (%)</cell></row><row><cell></cell><cell></cell><cell>PVTv2-b1 [47]</cell><cell>13</cell><cell>2.1</cell><cell>78.7</cell><cell></cell><cell></cell><cell>Swin-S [32]</cell><cell>50</cell><cell>8.7</cell><cell>83.0</cell></row><row><cell></cell><cell></cell><cell>QuadTree-B-b1 [42]</cell><cell>14</cell><cell>2.3</cell><cell>80.0</cell><cell></cell><cell></cell><cell>ConvNeXt-S [33]</cell><cell>50</cell><cell>8.7</cell><cell>83.1</cell></row><row><cell></cell><cell></cell><cell>RegionViT-T [2]</cell><cell>14</cell><cell>2.4</cell><cell>80.4</cell><cell></cell><cell></cell><cell>CrossFormer-B [48]</cell><cell>52</cell><cell>9.2</cell><cell>83.4</cell></row><row><cell></cell><cell></cell><cell>MPViT-XS [26]</cell><cell>11</cell><cell>2.9</cell><cell>80.9</cell><cell></cell><cell></cell><cell>InceptionNeXt-S [60]</cell><cell>49</cell><cell>8.4</cell><cell>83.5</cell></row><row><cell>tiny model</cell><cell>? 2.5G</cell><cell>tiny-MOAT-2 [55] VAN-B1 [16] BiFormer-T [66] CrossFormer-T [48] NAT-M [18] QnA-T [1]</cell><cell>10 14 13 28 20 16</cell><cell>2.3 2.5 2.2 2.9 2.7 2.5</cell><cell>81.0 81.1 81.4 81.5 81.8 82.0</cell><cell>base model</cell><cell>? 9.0G</cell><cell>PVTv2-b4 [47] NAT-S [18] Quadtree-B-b4 [42] Ortho-B [22] ScaleViT-B [58] MOAT-1 [55]</cell><cell>63 51 64 50 81 42</cell><cell>10.0 7.8 11.5 8.6 8.6 9.1</cell><cell>83.6 83.7 84.0 84.0 84.1 84.2</cell></row><row><cell></cell><cell></cell><cell>SMT-T [31]</cell><cell>12</cell><cell>2.4</cell><cell>82.2</cell><cell></cell><cell></cell><cell>InternImage-S [49]</cell><cell>50</cell><cell>8.0</cell><cell>84.2</cell></row><row><cell></cell><cell></cell><cell>RMT-T</cell><cell>14</cell><cell>2.5</cell><cell>82.4</cell><cell></cell><cell></cell><cell>BiFormer-B [66]</cell><cell>57</cell><cell>9.8</cell><cell>84.3</cell></row><row><cell></cell><cell></cell><cell>DeiT-S [43] Swin-T [32] ConvNeXt-T [33] Focal-T [56]</cell><cell>22 29 29 29</cell><cell>4.6 4.5 4.5 4.9</cell><cell>79.9 81.3 82.1 82.2</cell><cell></cell><cell></cell><cell>MViTv2-B [28] CMT-B [15] iFormer-B [40] RMT-B</cell><cell>52 46 48 54</cell><cell>10.2 9.3 9.4 9.7</cell><cell>84.4 84.5 84.6 85.0</cell></row><row><cell></cell><cell></cell><cell>InceptionNeXt-T [60]</cell><cell>28</cell><cell>4.2</cell><cell>82.3</cell><cell></cell><cell></cell><cell>DeiT-B [43]</cell><cell>86</cell><cell>17.5</cell><cell>81.8</cell></row><row><cell></cell><cell></cell><cell>FocalNet-T [57]</cell><cell>29</cell><cell>4.5</cell><cell>82.3</cell><cell></cell><cell></cell><cell>Swin-B [32]</cell><cell>88</cell><cell>15.4</cell><cell>83.3</cell></row><row><cell></cell><cell></cell><cell>RegionViT-S [2]</cell><cell>31</cell><cell>5.3</cell><cell>82.6</cell><cell></cell><cell></cell><cell>LITv2 [35]</cell><cell>87</cell><cell>13.2</cell><cell>83.6</cell></row><row><cell>small model</cell><cell>? 4.5G</cell><cell>CSWin-T [9] MPViT-S [26] ScalableViT-S [58] MOAT-0 [55] Ortho-S [22] InternImage-T [49] CMT-S [15]</cell><cell>23 23 32 28 24 30 25</cell><cell>4.3 4.7 4.2 5.7 4.5 5.0 4.0</cell><cell>82.7 83.0 83.1 83.3 83.4 83.5 83.5</cell><cell>large model</cell><cell>? 18.0G</cell><cell>CrossFormer-L [48] Ortho-L [22] CSwin-B [9] MPViT-B [26] ScalableViT-L [58] SMT-L [31] MOAT-2 [55]</cell><cell>92 88 78 75 104 81 73</cell><cell>16.1 15.4 15.0 16.4 14.7 17.7 17.2</cell><cell>84.0 84.2 84.2 84.3 84.4 84.6 84.7</cell></row><row><cell></cell><cell></cell><cell>FAT-B3 [13]</cell><cell>29</cell><cell>4.4</cell><cell>83.6</cell><cell></cell><cell></cell><cell>iFormer-L [40]</cell><cell>87</cell><cell>14.0</cell><cell>84.8</cell></row><row><cell></cell><cell></cell><cell>MaxViT-T [44]</cell><cell>31</cell><cell>5.6</cell><cell>83.6</cell><cell></cell><cell></cell><cell>CMT-L [15]</cell><cell>75</cell><cell>19.5</cell><cell>84.8</cell></row><row><cell></cell><cell></cell><cell>SMT-S [31]</cell><cell>20</cell><cell>4.8</cell><cell>83.7</cell><cell></cell><cell></cell><cell>InterImage-B [49]</cell><cell>97</cell><cell>16.0</cell><cell>84.9</cell></row><row><cell></cell><cell></cell><cell>BiFormer-S [66]</cell><cell>26</cell><cell>4.5</cell><cell>83.8</cell><cell></cell><cell></cell><cell>MaxViT-B [44]</cell><cell>120</cell><cell>23.4</cell><cell>84.9</cell></row><row><cell></cell><cell></cell><cell>RMT-S</cell><cell>27</cell><cell>4.5</cell><cell>84.1</cell><cell></cell><cell></cell><cell>RMT-L</cell><cell>95</cell><cell>18.2</cell><cell>85.5</cell></row></table><note><p>with the state-of-the-art on ImageNet-1K classification.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison to other backbones using RetinaNet and Mask R-CNN on COCO val2017 object detection and instance segmentation.</figDesc><table><row><cell>2. Sim-</cell></row></table><note><p><p><p><p><p>The first three stages utilize the decomposed ReSA, while the last stage uses the original ReSA. Like many previous backbones</p><ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b25">27,</ref><ref type="bibr" target="#b64">66]</ref></p>, we incorporate CPE</p><ref type="bibr" target="#b4">[5]</ref> </p>into our model.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>1 at the epoch 8 and 11. While for "3 ? +MS" schedule, the learning rate declines with the decay rate of 0.1 at the epoch 27 and 33.</figDesc><table /><note><p><p><p>Results. Tab. 2 and Tab.</p><ref type="bibr" target="#b2">3</ref> </p>shows the results with Reti-naNet and Mask R-CNN. The results demonstrate that our RMT performs best in all comparisons. For the RetinaNet framework, our RMT-T outperforms FAT-B2 by +1.1 AP, while S/B/L also perform better than other methods. As for the Mask R-CNN with "1?" schedule, RMT-L outperforms the recent InternImage-B by +1.8 box AP and +1.9 mask AP. For "3 ? +MS" schedule, RMT-S outperforms InternImage-T for +1.6 box AP and +1.2 mask AP. All the above results tell that RMT outperforms its counterparts by evident margins.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Comparison with the state-of-the-art on ADE20K. The framework is SemanticFPN.</figDesc><table><row><cell>Backbone</cell><cell cols="3">Semantic FPN Params(M) FLOPs(G) mIoU(%)</cell></row><row><cell>DAT-T [51]</cell><cell>32</cell><cell>198</cell><cell>42.6</cell></row><row><cell>CrossFormer-S [48]</cell><cell>34</cell><cell>221</cell><cell>46.0</cell></row><row><cell>UniFormer-S [27]</cell><cell>25</cell><cell>247</cell><cell>46.6</cell></row><row><cell>CSWin-T [9]</cell><cell>26</cell><cell>202</cell><cell>48.2</cell></row><row><cell>Shuted-S [39]</cell><cell>26</cell><cell>183</cell><cell>48.2</cell></row><row><cell>RMT-S</cell><cell>30</cell><cell>180</cell><cell>49.4</cell></row><row><cell>DAT-S</cell><cell>53</cell><cell>320</cell><cell>46.1</cell></row><row><cell>UniFormer-B [27]</cell><cell>54</cell><cell>350</cell><cell>47.7</cell></row><row><cell>CrossFormer-B [48]</cell><cell>56</cell><cell>331</cell><cell>47.7</cell></row><row><cell>CSWin-S [9]</cell><cell>39</cell><cell>271</cell><cell>49.2</cell></row><row><cell>RMT-B</cell><cell>57</cell><cell>294</cell><cell>50.4</cell></row><row><cell>DAT-B</cell><cell>92</cell><cell>481</cell><cell>47.0</cell></row><row><cell>CrossFormer-L</cell><cell>95</cell><cell>497</cell><cell>48.7</cell></row><row><cell>CSWin-B [9]</cell><cell>81</cell><cell>464</cell><cell>49.9</cell></row><row><cell>RMT-L</cell><cell>98</cell><cell>482</cell><cell>51.4</cell></row><row><cell>Backbone</cell><cell cols="3">UperNet Params(M) FLOPs(G) mIoU(%)</cell></row><row><cell>DAT-T [51]</cell><cell>60</cell><cell>957</cell><cell>45.5</cell></row><row><cell>NAT-T [18]</cell><cell>58</cell><cell>934</cell><cell>47.1</cell></row><row><cell>InternImage-T [49]</cell><cell>59</cell><cell>944</cell><cell>47.9</cell></row><row><cell>HorNet-T [38]</cell><cell>55</cell><cell>924</cell><cell>49.2</cell></row><row><cell>SMT-S [31]</cell><cell>50</cell><cell>935</cell><cell>49.2</cell></row><row><cell>RMT-S</cell><cell>56</cell><cell>937</cell><cell>49.8</cell></row><row><cell>DAT-S [51]</cell><cell>81</cell><cell>1079</cell><cell>48.3</cell></row><row><cell>SMT-B [31]</cell><cell>62</cell><cell>1004</cell><cell>49.6</cell></row><row><cell>HorNet-S [38]</cell><cell>85</cell><cell>1027</cell><cell>50.0</cell></row><row><cell>InterImage-S [49]</cell><cell>80</cell><cell>1017</cell><cell>50.2</cell></row><row><cell>CSWin-S [9]</cell><cell>65</cell><cell>1027</cell><cell>50.4</cell></row><row><cell>RMT-B</cell><cell>83</cell><cell>1051</cell><cell>51.5</cell></row></table><note><p><p><ref type="bibr" target="#b4">5</ref></p>. All the FLOPs are measured with the resolution of 512 ? 2048. All our models achieve the best performance in all comparisons. Specifically, our RMT-S exceeds Shunted-S for +1.2 mIoU with Semantic FPN. Moreover, our RMT-B outperforms the re-</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Comparison with the state-of-the-art on ADE20K. The framework is UperNet.</figDesc><table><row><cell cols="4">Model Params(M) FLOPs(G) Top1-acc(%)</cell></row><row><cell>RMT-T</cell><cell>14</cell><cell>2.5</cell><cell>82.4</cell></row><row><cell>w/o ?</cell><cell>14</cell><cell>2.5</cell><cell>81.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Ablation study cent InternImage-S for +1.3 mIoU. All the above results demonstrate our model's superiority in dense prediction.</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learned queries for efficient local attention</title>
		<author>
			<persName><forename type="first">Moab</forename><surname>Arar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><forename type="middle">H</forename><surname>Bermano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">RegionViT: Regional-to-Local Attention for Vision Transformers</title>
		<author>
			<persName><forename type="first">Chun-Fu ;</forename><surname>Richard) Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rameswar</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">MMDetection: Open mmlab detection toolbox and benchmark</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Twins: Revisiting the design of spatial attention in vision transformers</title>
		<author>
			<persName><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaxia</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Conditional positional encodings for vision transformers</title>
		<author>
			<persName><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Mmsegmentation, an open source semantic segmentation toolbox</title>
		<author>
			<orgName type="collaboration">MMSegmentation Contributors</orgName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName><forename type="first">Barret</forename><surname>Ekin D Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPRW</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cswin transformer: A general vision transformer backbone with cross-shaped windows</title>
		<author>
			<persName><forename type="first">Xiaoyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006">2022. 2, 4, 5, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2021</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multiscale vision transformers</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Haoqi Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karttikeya</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghao</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Jiyang Guan, and Ran He. Rethinking local perception in lightweight vision transformer</title>
		<author>
			<persName><forename type="first">Qihang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaibo</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Lightweight vision transformer with bidirectional interaction</title>
		<author>
			<persName><forename type="first">Qihang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaibo</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ran</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2023. 2, 4, 5</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Doubly-fused vit: Fuse information from vision transformer doubly with local representation</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofeng</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cmt: Convolutional neural networks meet vision transformers</title>
		<author>
			<persName><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yehui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">; Meng-Hao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng-Ze</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng-Ning</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi-Min</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.09741</idno>
	</analytic>
	<monogr>
		<title level="m">Visual attention network</title>
		<imprint>
			<date type="published" when="2022">2022. 2, 4, 5 [16. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Transformer in transformer</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enhua</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neighborhood attention transformer</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Walton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiachen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Humphrey</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sun</forename><surname>Jian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Orthogonal transformer: An efficient vision transformer backbone with token orthogonalization</title>
		<author>
			<persName><forename type="first">Huaibo</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ran</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ran He, and Tieniu Tan. Vision transformer with super token sampling</title>
		<author>
			<persName><forename type="first">Huaibo</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Panoptic feature pyramid networks</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Mpvit: Multi-path vision transformer for dense prediction</title>
		<author>
			<persName><forename type="first">Youngwan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonghee</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Willette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005">2022. 1, 4, 5</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Uniformer: Unified transformer for efficient spatiotemporal representation learning</title>
		<author>
			<persName><forename type="first">Kunchang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanglu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Improved multiscale vision transformers for classification and detection</title>
		<author>
			<persName><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao-Yuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karttikeya</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Scale-aware modulation meet transformer</title>
		<author>
			<persName><forename type="first">Weifeng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianwen</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006">2023. 1, 4, 5, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006">2021. 2, 4, 5, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A convnet for the 2020s</title>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanzi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao-Yuan</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unified-io: A unified model for vision, language, and multi-modal tasks</title>
		<author>
			<persName><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>Roozbeh Mottaghi, and Aniruddha Kembhavi</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fast vision transformers with hilo attention</title>
		<author>
			<persName><forename type="first">Zizheng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bohan</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Rwkv: Reinventing rnns for the transformer era</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Alcaide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Anthony</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Hornet: Efficient high-order spatial interactions with recursive gated convolutions</title>
		<author>
			<persName><forename type="first">Yongming</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenliang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yansong</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ser-Lam</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Shunted self-attention via multi-scale token aggregation</title>
		<author>
			<persName><forename type="first">Daquan</forename><surname>Sucheng Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengfeng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinchao</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Xinchao Wang, and Shuicheng YAN. Inception transformer</title>
		<author>
			<persName><forename type="first">Chenyang</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Retentive network: A successor to Transformer for large language models</title>
		<author>
			<persName><forename type="first">Yutao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqing</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jilong</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<idno>ArXiv, abs/2307.08621</idno>
		<imprint>
			<date type="published" when="2023">2023. 1, 2, 3</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Quadtree attention for vision transformers</title>
		<author>
			<persName><forename type="first">Shitao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Maxvit: Multi-axis vision transformer</title>
		<author>
			<persName><forename type="first">Zhengzhong</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hossein</forename><surname>Talebi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peyman</forename><surname>Milanfar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinxiao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006">2022. 1, 4, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Pvtv2: Improved baselines with pyramid vision transformer</title>
		<author>
			<persName><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Visual Media</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2005">2022. 2, 4, 5</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Crossformer: A versatile vision transformer hinging on cross-scale attention</title>
		<author>
			<persName><forename type="first">Wenxiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binbin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2006">2022. 4, 5, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Internimage: Exploring large-scale vision foundation models with deformable convolutions</title>
		<author>
			<persName><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenhang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006">2023. 4, 5, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15808</idno>
		<title level="m">Cvt: Introducing convolutions to vision transformers</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Vision transformer with deformable attention</title>
		<author>
			<persName><forename type="first">Zhuofan</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuran</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiji</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><forename type="middle">Erran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingcheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A modularized multi-modal foundation model across text</title>
		<author>
			<persName><forename type="first">Haiyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaya</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiabo</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guohai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songfang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Lite vision transformer with enhanced self-attention</title>
		<author>
			<persName><forename type="first">Chenglin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yilin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianming</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Moat: Alternating mobile convolution and attention brings strong vision models</title>
		<author>
			<persName><forename type="first">Chenglin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qihang</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Focal selfattention for local-global interactions in vision transformers</title>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Focal modulation networks</title>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Scalablevit: Rethinking the context-oriented generalization of vision transformer</title>
		<author>
			<persName><forename type="first">Rui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hailong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yansong</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuefeng</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiu</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Wave-vit: Unifying wavelet and transformers for visual representation learning</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yehao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong-Wah</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Inceptionnext: when inception meets convnext</title>
		<author>
			<persName><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seong</forename><surname>Joon Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><surname>Yann N Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Multi-scale vision longformer: A new vision transformer for high-resolution image encoding</title>
		<author>
			<persName><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Scene parsing through ade20k dataset</title>
		<author>
			<persName><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Biformer: Vision transformer with bi-level routing attention</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinjiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanghan</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rynson</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005">2023. 1, 4, 5</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
