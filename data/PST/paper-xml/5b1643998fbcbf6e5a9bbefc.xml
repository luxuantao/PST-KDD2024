<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ASCERTAIN: Emotion and Personality Recognition using Commercial Sensors</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ramanathan</forename><surname>Subramanian</surname></persName>
						</author>
						<title level="a" type="main">ASCERTAIN: Emotion and Personality Recognition using Commercial Sensors</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">717A3D5426FD4F25F0F1EE2BB4FF0B15</idno>
					<idno type="DOI">10.1109/TAFFC.2016.2625250</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2625250, IEEE Transactions on Affective Computing IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, AUGUST 2016 1 This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2625250, IEEE Transactions on Affective Computing IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, AUGUST 2016 1949-3045 This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2625250, IEEE Transactions on Affective Computing This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2625250, IEEE Transactions on Affective Computing</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Emotion and Personality recognition</term>
					<term>Physiological signals</term>
					<term>Multimodal analysis</term>
					<term>Commercial sensors Self-reported ratings Arousal</term>
					<term>Valence</term>
					<term>Engagement Liking</term>
					<term>Familiarity Personality Scales Extraversion</term>
					<term>Agreeableness Conscientiousness</term>
					<term>Neuroticism</term>
					<term>Openness Physiological signals ECG</term>
					<term>GSR</term>
					<term>Frontal EEG</term>
					<term>Facial features</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present ASCERTAIN-a multimodal databaASe for impliCit pERsonaliTy and Affect recognitIoN using commercial physiological sensors. To our knowledge, ASCERTAIN is the first database to connect personality traits and emotional states via physiological responses. ASCERTAIN contains big-five personality scales and emotional self-ratings of 58 users along with their Electroencephalogram (EEG), Electrocardiogram (ECG), Galvanic Skin Response (GSR) and facial activity data, recorded using off-the-shelf sensors while viewing affective movie clips. We first examine relationships between users' affective ratings and personality scales in the context of prior observations, and then study linear and non-linear physiological correlates of emotion and personality. Our analysis suggests that the emotion-personality relationship is better captured by non-linear rather than linear statistics. We finally attempt binary emotion and personality trait recognition using physiological features. Experimental results cumulatively confirm that personality differences are better revealed while comparing user responses to emotionally homogeneous videos, and above-chance recognition is achieved for both affective and personality dimensions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>D Espite rapid advances in Human-computer Interaction (HCI) and relentless endeavors to improve user experience with computer systems, the need for agents to recognize and adapt to the affective state of users has been widely acknowledged. While being a critical component of human behavior, affect is nevertheless a highly subjective phenomenon influenced by a number of contextual and psychological factors including personality.</p><p>The personality-affect relationship has been actively studied ever since a correlation between the two was proposed in Eysenck's personality model <ref type="bibr" target="#b0">[1]</ref>. Eysenck posited that Extraversion, the personality dimension that describes a person as either talkative or reserved, is accompanied by low cortical arousal-i.e., extraverts require more external stimulations than introverts. His model also proposed that neurotics, characterized by negative feelings such as depression and anxiety, are more sensitive to external stimulation and become easily upset or nervous due to minor stressors.</p><p>Many affective studies have attempted to validate and extend Eyesenk's findings. Some have employed explicit user feedback in the form of affective self-ratings <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, while others have measured implicit user responses such as Electroencephalogram (EEG) activity <ref type="bibr" target="#b3">[4]</ref> and heart rate <ref type="bibr" target="#b4">[5]</ref> for their analyses. However, few works have investigated affective correlates of traits other than Extraversion and Neuroticism. Conversely, social psychology studies have examined personality mainly via non-verbal social behavioral cues (see <ref type="bibr" target="#b5">[6]</ref> for a review), but few works have modeled personality traits based on emotional behavior. Conducting studies to examine the personality-affect relationship is precluded by problems such as subject preparation time, invasiveness of sensing equipment and the paucity of reliable annotators for annotating emotional attributes. This work builds on <ref type="bibr" target="#b6">[7]</ref> and examines the influence of personality differences on users' affective behavior via the ASCERTAIN database 1 . We utilize ASCERTAIN to (i) understand the relation between emotional attributes and personality traits, and (ii) characterize both via users' physiological responses. ASCERTAIN contains personality scores and emotional self-ratings of 58 users in addition to their affective physiological responses. More specifically, ASCER-TAIN is used to model users' emotional states and bigfive personality traits via heart rate (Electrocardiogram or ECG), galvanic skin response (GSR), EEG and facial activity patterns observed while viewing 36 affective movie clips.</p><p>We specifically designed a study with movie scenes as they effectively evoke emotions <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, as typified by genres such as thriller, comedy or horror. Also, different from existing affective databases such as DEAP <ref type="bibr" target="#b9">[10]</ref>, MAHNOB <ref type="bibr" target="#b10">[11]</ref> and DECAF <ref type="bibr" target="#b8">[9]</ref>, ASCERTAIN comprises data recorded exclusively using commercial sensors to ensure ecological validity and scalability of the employed framework for largescale profiling applications.</p><p>Using the ASCERTAIN data, we first examine correla-tions among users' valence (V) and arousal (A) self-ratings and their personality dimensions. We then attempt to isolate physiological correlates of emotion and personality. Our analyses suggest that the relationships among emotional attributes and personality traits are better captured by nonlinear rather than linear statistics. Finally, we present singletrial (binary) recognition of A,V and the big-five traits considering physiological responses observed over (a) all, and (b) emotionally homogeneous (e.g., high A, high V) clips. Superior personality recognition is achieved for (b), implying that personality differences are better revealed by comparing responses to emotionally similar stimuli. The salient aspects of ASCERTAIN are: 1. To our knowledge, ASCERTAIN is the first physiological database that facilitates both emotion and personality recognition. In social psychology, personality traits are routinely modeled via questionnaires or social behavioral cues. Instead, this is one of the first works to assess personality traits via affective physiological responses (the only other work to this end is <ref type="bibr" target="#b11">[12]</ref>). 2. Different from the DEAP <ref type="bibr" target="#b9">[10]</ref>, MAHNOB <ref type="bibr" target="#b10">[11]</ref> and DE-CAF <ref type="bibr" target="#b8">[9]</ref> databases, we use wearable, off-the-shelf sensors for physiological recordings. This enhances the ecological validity of the ASCERTAIN framework, and abovechance recognition of emotion and personality affirms its utility and promise for commercial applications. 3. We present interesting insights concerning correlations among affective and personality attributes. Our analyses suggest that the emotion-personality relationship is better captured via non-linear statistics. Also, personality differences are better revealed by comparing user responses to emotionally similar videos (or more generally, under similar affect inducement). From here on, Section 2 reviews related literature to motivate the need for ASCERTAIN, while Section 3 details the materials and methods employed for data compilation. Section 4 presents descriptive statistics, while correlations among users' affective ratings and personality dimensions are analyzed in Section 5. Section 6 details physiological correlates of emotion and personality, while Section 7 presents recognition experiments. Section 8 discusses the correlation and recognition results, and Section 9 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>This section reviews related work focusing on (a) multimodal affect recognition, (b) personality assessment and (c) the personality-affect relationship.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Multimodal affect recognition</head><p>As emotions are conveyed by content creators using multiple means (audio, video), and expressed by humans in a number of ways (facial expressions, speech and physiological responses), many affect recognition (AR) methods employ a multimodal framework. Common content-based modalities employed for AR include audio <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b15">[16]</ref>, visual <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b18">[19]</ref> and audio-visual <ref type="bibr" target="#b19">[20]</ref>- <ref type="bibr" target="#b21">[22]</ref>. Recent AR methodologies have focused on monitoring user behavior via the use of physiological sensors (see <ref type="bibr" target="#b22">[23]</ref> for a review). Emotions induced by music clips are recognized via heart rate, muscle movements, skin conductivity and respiration changes in <ref type="bibr" target="#b23">[24]</ref>. Lisetti et al. <ref type="bibr" target="#b24">[25]</ref> use GSR, heart rate and temperature signals to recognize emotional states. As part of the HUMAINE project <ref type="bibr" target="#b12">[13]</ref>, three naturalistic and six induced affective databases containing multimodal data (including physiological signals) are compiled from 8-125 participants. Tavakoli et al. <ref type="bibr" target="#b25">[26]</ref> examine the utility of various eye fixation and saccade-based features for valence recognition, while Subramanian et al. <ref type="bibr" target="#b26">[27]</ref> correlate user responses with eye movement patterns to discuss the impact of emotions on visual attention and memory.</p><p>Koelstra et al. <ref type="bibr" target="#b9">[10]</ref> analyze blood volume pressure, respiration rate, skin temperature and Electrooculogram (EOG) patterns for recognizing emotional states induced by 40 music videos. MAHNOB-HCI <ref type="bibr" target="#b10">[11]</ref> is a multimodal database containing synchronized face video, speech, eye-gaze and physiological recordings from 27 users. Abadi et al. <ref type="bibr" target="#b8">[9]</ref> study Magnetoencephalogram (MEG), Electromyogram (EMG), EOG and ECG responses from users for music and movie clips, and conclude that better emotion elicitation and AR are achieved with movie clips.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Personality recognition</head><p>The big-five or five-factor model <ref type="bibr" target="#b27">[28]</ref> describes human personality in terms of five dimensions-Extraversion (sociable vs reserved), Neuroticism or the degree of emotional stability (nervous vs confident), Agreeableness (compassionate vs dispassionate), Conscientiousness (dutiful vs easy-going) and Openness (curious/creative vs cautious/conservative).</p><p>A comprehensive survey of personality computing approaches is presented in <ref type="bibr" target="#b5">[6]</ref>. The traditional means to model personality traits are questionnaires or self-reports. Argamon et al. <ref type="bibr" target="#b29">[29]</ref> use lexical cues from informal texts for recognizing Extraversion (Ex) and Neuroticism (Neu). Olguin et al. <ref type="bibr" target="#b30">[30]</ref> and Alameda-Pineda et al. <ref type="bibr" target="#b31">[31]</ref> show that non-verbal behavioral measures acquired using a sociometric badge such as the amount of speech and physical activity, number of face-to-face interactions and physical proximity to other objects is highly correlated with personality. Much work has since employed non-verbal behavioral cues in social settings for personality recognition including <ref type="bibr" target="#b32">[32]</ref>, where Ex is recognized using speech and social attention cues in round-table meetings, while <ref type="bibr" target="#b33">[33]</ref>, <ref type="bibr" target="#b34">[34]</ref> predict Ex and Neu from proxemic and attention cues in party settings.</p><p>Among works that have attempted recognition of all five personality factors, Mairesse et al. <ref type="bibr" target="#b35">[35]</ref> use acoustic and lexical features, while Staiano et al. <ref type="bibr" target="#b36">[36]</ref> analyze structural features of individuals' social networks. Srivastava et al. <ref type="bibr" target="#b37">[37]</ref> automatically complete personality questionnaires for 50 movie characters utilizing lexical, audio and visual behavioral cues. Brouwer et al. <ref type="bibr" target="#b38">[38]</ref> estimate personality traits via physiological measures, which are revealed sub-consciously and more genuinely (less prone to manipulation) than questionnaire answers. In a gaming-based study, they observe a negative correlation between (i) heart rate and Ex, and (ii) skin-conductance and Neu.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Personality-Affect relationship</head><p>The relationship between personality and affect has been extensively examined in social psychology <ref type="bibr" target="#b39">[39]</ref>, but not in a computational setting. Eysenck's seminal personality theory <ref type="bibr" target="#b0">[1]</ref> posits that extraverts require more external stimulation than introverts, and that neurotics are aroused more easily. Many studies have since studied the personalityaffect relationship by examining explicit or implicit user responses. Personality effects on brain activation related to valence (V) and arousal (A) is investigated in <ref type="bibr" target="#b2">[3]</ref>, which concludes that Neu correlates negatively with positive V, and positively with A. In an EEG-based study <ref type="bibr" target="#b3">[4]</ref>, a negative correlation is observed between Ex and A, while a positive correlation is noted between Neu and A especially for negative valence stimuli.</p><p>The impact of personality traits on affective user ratings is studied using path analysis in <ref type="bibr" target="#b40">[40]</ref>. Feedback scores from 133 students are analyzed in <ref type="bibr" target="#b1">[2]</ref> to conclude that neurotics experience positive emotions similar to emotionally stable counterparts in pleasant situations, even though they may experience negative emotions more strongly. Event-related potentials and heart rate changes are studied in <ref type="bibr" target="#b4">[5]</ref> to confirm a positive correlation between Neu and A for negative stimuli, while a signal-detection task is used in <ref type="bibr" target="#b41">[41]</ref> to suggest that extraverts are generally less aroused than introverts. Brumbaugh et al. <ref type="bibr" target="#b42">[42]</ref> examine correlations among the big-five traits, and find Ex and Neu to be associated with increased A while viewing negative videos. Abadi et al. <ref type="bibr" target="#b11">[12]</ref> attempt recognition of the big-five traits from affective physiological responses, and our work is most similar to theirs in this respect. Nevertheless, we consider more users and a larger stimulus set in this work (58 users and 36 clips vs 36 users and 16 clips in <ref type="bibr" target="#b11">[12]</ref>), and show superior personality trait recognition on comparing physiological responses to emotionally homogeneous clips.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Spotting the research gap</head><p>Examination of related literature reveals that AR methodologies are increasingly becoming user-centric instead of content-centric, suggesting that emotions better manifest via human behavioral cues rather than multimedia contentbased (typically audio, visual and speech-based) cues. Nevertheless, the influence of psychological factors such as personality on emotional behavior has hardly been examined, in spite of prior work suggesting that personality affects one's a) feelings <ref type="bibr" target="#b39">[39]</ref>, <ref type="bibr" target="#b43">[43]</ref>, b) emotional perception <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> and c) multimedia preferences <ref type="bibr" target="#b44">[44]</ref>, <ref type="bibr" target="#b45">[45]</ref>.</p><p>Motivated by the above findings and the lack of publicly available data sets positioned at the intersection of personality and affect, we introduce ASCERTAIN, a multimodal corpus containing physiological recordings of users viewing emotional videos. ASCERTAIN allows for inferring both personality traits and emotional states from physiological signals. We record GSR, EEG, ECG signals using wearable sensors, and facial landmark trajectories (EMO) using a web-camera. In the light of recent technological developments, these signals can be acquired and analyzed instantaneously. Also, Wang and Ji <ref type="bibr" target="#b22">[23]</ref> advocate the need for less-intrusive sensors to elicit natural emotional behavior from users. Use of wearable sensors is critical to ensure the ecological validity, repeatability and scalability of affective computing studies, which are typically conducted in controlled lab conditions and with small user groups.</p><p>Table <ref type="table" target="#tab_1">1</ref> presents an overview of publicly available usercentric AR datasets. Apart from being one of the largest datasets in terms of the number of participants and stimuli examined for analysis, ASCERTAIN is also the first database to facilitate study of the personality-affect relationship.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ASCERTAIN OVERVIEW</head><p>Fig. <ref type="figure" target="#fig_1">1</ref> presents an overview of the ASCERTAIN framework and a summary of the compiled data is provided in Table <ref type="table" target="#tab_2">2</ref>.</p><p>To study the personality-affect relationship, we recorded users' physiological responses as they viewed the affective movie clips used in <ref type="bibr" target="#b8">[9]</ref>. Additionally, their explicit feedback, in the form of arousal, valence, liking, engagement and familiarity ratings, were obtained on viewing each clip. Finally, personality measures for the big-five dimensions were also compiled using a big-five marker scale (BFMS) questionnaire <ref type="bibr" target="#b46">[46]</ref>. We now describe (1) the procedure adopted to compile users' emotional ratings, personality measures and physiological responses, and (2) the physiological features extracted to measure users' emotional responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Materials and Methods</head><p>Subjects: 58 university students (21 female, mean age = 30) participated in the study. All subjects were fluent in English and were habitual Hollywood movie watchers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Materials:</head><p>One PC with two monitors was used for the experiment. One monitor was used for video clip presentation at 1024 × 768 pixel resolution with 60 Hz screen refresh rate, and was placed roughly one meter before the user. The other monitor allowed the experimenter to verify the recorded sensor data. Following informed consent, physiological sensors were positioned on the user's body as shown in Fig. <ref type="figure" target="#fig_2">2(a)</ref>. The GSR sensor was tied  to the left wrist, and two electrodes were fixed to the index and middle finger phalanges. Two measuring electrodes for ECG were placed at each arm crook, with the reference electrode placed at the left foot. A single dry-electrode EEG device was placed on the head like a normal headset, with the EEG sensor touching the forehead and the reference electrode clipped to the left ear. EEG data samples were logged using the Lucid Scribe software, and all sensor data were recorded via bluetooth. A webcam was used to record facial activity. Synchronized data recording and preprocessing were performed using MATLAB Psychtoolbox (http://psychtoolbox.org/).</p><p>Protocol: Each user performed the experiment in a session lasting about 90 minutes. Viewing of each movie clip is denoted as a trial. After two practice trials involving clips that were not part of the actual study, users watched movie clips randomly shown in two blocks of 18 trials, with a short break in-between to avoid fatigue. In each trial (Fig. <ref type="figure" target="#fig_1">1(b</ref>)), a fixation cross was displayed for four seconds followed by clip presentation. After viewing each clip, users self-reported their emotional state in the form of affective ratings within a time limit of 30 seconds. They also completed a personality questionnaire after the experiment.</p><p>Stimuli: We adopted the 36 movie clips used in <ref type="bibr" target="#b8">[9]</ref> for our study. These clips are between 51-127 s long (µ =80, σ =20), and are shown to be uniformly distributed (9 clips per quadrant) over the arousal-valence (AV) plane.</p><p>Affective ratings: For each movie clip, we compiled valence (V) and arousal (A) ratings reflecting the user's affective impression. A 7-point scale was used with a -3 (very negative) to 3 (very positive) scale for V, and a 0 (very boring) to 6 (very exciting) scale for A. Likewise, ratings concerning engagement (Did not pay attention -Totally attentive), liking (I hated it -I loved it) and familiarity (Never seen it before -Remember it very well) were also acquired. Mean user V,A ratings for the 36 clips are plotted in Fig. <ref type="figure" target="#fig_2">2</ref>(b), and are color-coded based on the ground-truth ratings from <ref type="bibr" target="#b8">[9]</ref>. Ratings form a 'C'-shape in the AV plane, consistent with prior affective studies <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>.</p><p>Personality scores: Participants also completed the big-five marker scale (BFMS) questionnaire <ref type="bibr" target="#b46">[46]</ref> which has been used in many personality recognition works <ref type="bibr" target="#b32">[32]</ref>- <ref type="bibr" target="#b34">[34]</ref>. Scale distributions for the big-five traits are shown in Fig. <ref type="figure" target="#fig_2">2(c</ref>).</p><p>The most and least variance in personality scores are noted for the Extraversion and Openness traits respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Physiological feature extraction</head><p>We extracted physiological features corresponding to each trial over the final 50 seconds of stimulus presentation, owing to two reasons: (1) The clips used in <ref type="bibr" target="#b8">[9]</ref> are not  </p><formula xml:id="formula_0">(a) (b) (c)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EMO (72)</head><p>Statistics concerning horizontal and vertical movement of 12 motion units (MUs) specified in <ref type="bibr" target="#b47">[47]</ref>.</p><p>emotionally homogeneous, but are more emotional towards the end. ( <ref type="formula">2</ref>) Some employed features (see Table <ref type="table" target="#tab_3">3</ref>) are nonlinear functions of the input signal length, and fixed time-intervals needed to be considered as the movie clips were of varying lengths. Descriptions of the physiological signals examined in this work are as follows.</p><p>Galvanic Skin Response (GSR): GSR measures transpiration rate of the skin. When two electrodes are positioned on the middle and index finger phalanges and a small current is sent through the body, resistance to current flow changes with the skin transpiration rate. Most of the GSR information is contained in low-frequency components, and the signal is recorded at 100 Hz sampling frequency with a commercial bluetooth sensor. Following <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b23">[24]</ref>, we extracted 31 GSR features listed in Table <ref type="table" target="#tab_3">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Electroencephalography (EEG):</head><p>EEG measures small changes in the skull's electrical field produced by neural activity, and information is encoded in the EEG signal amplitude as well as in certain frequency components. We used a commercial, single dry-electrode EEG sensor 2 , which records eight information channels sampled at 32 Hz. The recorded information includes frontal lobe activity, level of facial activation, eye-blink rate and strength, which are relevant emotional responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Electrocardiogram (ECG):</head><p>Heart rate characteristics have been routinely used for user-centered emotion recognition. We performed R-peak detection on the ECG signal to compute users' inter-beat intervals (IBI), heart rate (HR), and the heart rate variability (HRV). We also extracted power spectral density (PSD) in low frequency bands as in <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b23">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Facial landmark trajectories (EMO):</head><p>A facial feature tracker <ref type="bibr" target="#b47">[47]</ref> was used to compute displacements of 12 interest points or motion units (MU) in each video frame. We calculated 6 statistical measures for each landmark to obtain a total of 72 features (Table <ref type="table" target="#tab_3">3</ref>).</p><p>2. www.neurosky.com</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Data Quality</head><p>A unique aspect of ASCERTAIN with respect to prior affective databases is that physiological signals are recorded using commercial and minimally invasive sensors that allow body movement of participants. However, it is well known that body movements can degrade quality of the recorded data, and such degradation may be difficult to detect using automated methods. Therefore, we plotted the recorded data for each modality and trial, and rated the data quality manually on a scale of 1 (good data)-5 (missing data). For ECG, we evaluated the raw signal from each arm as well as the R-peak amplitude. The presence/absence of facial tracks and correctness of the tracked facial locations were noted for EMO. For GSR, we examined the extent of data noise, and rated EEG (i) on the raw signal, (ii) by summarizing the quality of δ (&lt; 4 Hz), θ (4-7 Hz), α (8-15 Hz), β (16-31 Hz) and γ (&gt; 31 Hz) frequency bands, and (iii) on the pre-calculated attention and meditation channels available as part of the EEG data. Plots and tables with explanations on data quality are available with the dataset. Fig. <ref type="figure" target="#fig_3">3</ref> presents an overview of the data quality for the four considered modalities, with the proportion of trials for which the quality varies from 1-5 highlighted. About 70% of the recorded data is good (corresponding to levels 1-3) for all modalities except ECG, with GSR data being the cleanest. Maximum missing data is noted for EEG, reflecting the sensitivity of the EEG device to head movements. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DESCRIPTIVE STATISTICS</head><p>In this section, we present statistics relating to user selfreports and personality scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Analysis of Self-ratings</head><p>As mentioned previously, we selected 36 movie clips such that their emotional ratings were distributed uniformly over the AV plane as per ground-truth ratings in <ref type="bibr" target="#b8">[9]</ref>, with 9 clips each corresponding to the HAHV (high arousal-high valence), LAHV (low arousal-high valence), LALV (low arousal-low valence) and HALV (high arousal-low valence) quadrants 3 . The targeted affective state was mostly reached during the ASCERTAIN study as shown in Fig. <ref type="figure" target="#fig_2">2</ref>(b). A twosample t-test revealed significantly higher mean A ratings 3. For consistency's sake, quadrant-wise video labels derived based on ratings from <ref type="bibr" target="#b8">[9]</ref>  Overall, emotion elicitation was more consistent for valence as in prior works <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. We computed agreement among participants' A,V ratings using the Krippendorff's alpha metric-agreement for A and V were respectively found to be 0.12 and 0.58, implying more consensus for clip valence as above. We then computed the agreement between the ASCERTAIN and DECAF <ref type="bibr" target="#b8">[9]</ref> populations using the Cohen's Kappa (κ) measure. To this end, we computed κ between ground-truth (GT) labels from <ref type="bibr" target="#b8">[9]</ref> and each user's A,V labels assigned as high/low based on the mean rating-the mean agreement over all users for A and V was found to be 0.24 and 0.73 respectively. We also computed the κ measure between GT and the ASCERTAIN population based on the mean A,V rating of all users-here, an agreement of 0.39 was observed for A and 0.61 for V. Overall, these measures suggest that while individual-level differences exist in affective perception of the movie clips, there is moderate to substantial agreement between assessments of the ASCERTAIN and DECAF populations implying that the considered movie clips are effective for emotion elicitation.</p><p>Fig. <ref type="figure" target="#fig_4">4</ref> presents box-plots describing the distribution of the arousal (A), valence (V), engagement (E), liking (L) and familiarity (F) user ratings for (i) all, and (ii) quadrant-based videos. Clearly, low-arousal videos are perceived as more 'neutral' in terms of A and V, which leads to the 'C' shape in Fig. <ref type="figure" target="#fig_2">2(b</ref>). All videos are perceived as sufficiently engaging, while HV clips are evidently more liked than LV clips. Also, the presented movie clips were not very conversant to participants, suggesting that the ASCERTAIN findings are overall unlikely to be influenced by familiarity biases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Affective Ratings vs Personality Scales</head><p>To examine relationships between the different user ratings, we computed Pearson correlations among self-reported attributes as shown in Table <ref type="table" target="#tab_5">4</ref>. Since the analysis involves attribute ratings provided by 58 users for each of the 36 clips, we accounted for multiple comparisons by limiting the false discovery rate (FDR) to within 5% using the procedure outlined in <ref type="bibr" target="#b48">[48]</ref>. Highlighted numbers denote correlations found to be significant over at least 15 users (25% of the population) adopting the above methodology.</p><p>Focusing on significant correlations, A is moderately correlated with E, while V is found to correlate strongly with L mirroring the observations of Koelstra et al. <ref type="bibr" target="#b9">[10]</ref>. A moderate and significant correlation is noted between E and L implying that engaging videos are likely to appeal to viewers' senses, and similarly, between F and L confirming the mere exposure effect observed in <ref type="bibr" target="#b49">[49]</ref> attributing liking to familiarity. Nevertheless, different from <ref type="bibr" target="#b9">[10]</ref> with music videos where a moderate correlation is noted between A and V ratings, we notice that the A and V dimensions are uncorrelated for the ASCERTAIN study, which again reinforces the utility of movie clips as good control stimuli. To validate our experimental design, we tested for effects of video length on A,V ratings but did not find any.</p><p>Table <ref type="table" target="#tab_6">5</ref> presents Pearson correlations between personality dimensions. Again focusing on significant correla-   Partial correlations between emotional and personality attributes are tabulated in Table <ref type="table" target="#tab_7">6</ref>. Considering all movie clips, a significant and moderately negative correlation is noted between Ex and E, implying that introverts were more immersed with emotional clips during the movie-watching task. A few more significant correlates are observed when mean ratings for quadrant-wise (or emotionally similar) videos are considered. Delineating, Ag is negatively correlated with V for HAHV videos, while the negative correlation between Ex and E manifests for high-arousal (HAHV and HALV) stimuli. Also notable is the moderately negative correlation between Ex and L, and also between Conscientiousness and L for HALV movie clips.</p><p>We also performed linear regression analyses with user self ratings as predictors and personality attributes as the target variables for the different video sets, and the coefficients of determination/squared correlations (R 2 ) for the different video sets are presented in Table <ref type="table" target="#tab_8">7</ref>. R 2 values with the three best predictors along with the predictor names are listed outside parentheses, while squared correlations with the full model are listed within braces. Considering all movie clips, the best linear model is obtained for Ex with V, E and L ratings as predictors. Among the four AV quadrants, significant squared correlations are observed for the Ex and Ag traits with V,E,L predictors, and for the ES trait with arousal, valence and liking ratings as predictors for HAHV clips. A significant model is also obtained for Openness with V,E,L predictors considering mildly positive HALV clips. Overall, it is easy to observe from the table that (i) there is little difference in the predictive power of the best-three-predictor and full models, and (ii) the linear models have rather limited predictive power, with the best model only 17% of the personality scale variance. Cumulatively, Tables <ref type="table" target="#tab_7">6</ref> and<ref type="table" target="#tab_8">7</ref> cumulatively suggest that the relationship between emotional and personality variables is not well modeled using linear statistics, and it is perhaps worthwhile to explore the use of non-linear measures to this end. From here on, given the high degree of correlation between A and E and between the V and L, we will only focus on A and V dimensions in the rest of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Mutual Information Analysis</head><p>Mutual information (MI) is a popular metric to capture non-linear relationships between two random variables, and measures how much information is known about one variable given the other. Formally, the MI between two random vectors X = {x} and Y = {y} is defined as: M I(X, Y ) = x,y P XY (x, y)log P XY (x,y) P X (x).P Y (y) where p XY (x, y) is the joint probability distribution, while P X (x) and P Y (y) are the respective marginal probabilities. We attempted to describe the relationship between emotional ratings and personality scales via the normalized mutual information (NMI) index <ref type="bibr" target="#b51">[51]</ref> defined as:</p><formula xml:id="formula_1">N M I(X, Y ) = M I(X,Y ) √ (H(X)H(Y ))</formula><p>, where H(X) and H(Y ) denote entropies of X and Y .</p><p>NMI with personality scales for arousal and valence ratings are shown in Fig. <ref type="figure" target="#fig_5">5</ref>. In contrast to linear correlations, both A and V share a high degree of mutual information with all five personality traits. Considering all movie clips, emotional ratings share slightly higher MI with A than with V. Also, a strictly higher MI measure is noted when emotionally similar clips are considered instead of all clips. Among personality traits, Ex and Conscientiousness (Con) share the most MI with V,A attributes-in contrast, little correlation is observed between Con and A,V in Table <ref type="table" target="#tab_7">6</ref>). Conversely, lowest MI is noted for Openness (O). One notable difference exists between A and V though-higher MI with arousal is noted for high HV clips, while for all personality traits barring Ag, greater MI with valence is observed for LV clips than for HV clips. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">PERSONALITY MEASURES VS USER RATINGS</head><p>We now examine the relationship between user V,A ratings and personality scales in the context of hypotheses (H1-H3) put forth in the literature. To this end, we determined high/low trait groups (e.g., emotional stable vs neurotic) for each personality dimension by dichotomizing personality measures based on the median score-this generated balanced high and low sets for the Ex and ES traits, and an unbalanced split for the remaining traits, with the most imbalance (33 vs 25) noted for Ag. We then proceeded to analyze the affective ratings for each group. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">H1: Extraversion vs Arousal and Valence</head><p>The correlation between Extraversion and arousal has been investigated in many studies-EEG measurements <ref type="bibr" target="#b3">[4]</ref>, signal detection analysis <ref type="bibr" target="#b41">[41]</ref> and fMRI <ref type="bibr" target="#b2">[3]</ref> have shown lower arousal in extraverts as compared to introverts, consistent with Eyesenck's personality theory. Also, Ex has been found to correlate with positive valence in a number of works <ref type="bibr" target="#b52">[52]</ref>.</p><p>Analyses presented in Table <ref type="table" target="#tab_7">6</ref> reveal little correlation between Ex and A for all video categories. While two-tailed t-tests confirmed that extraverts and introverts rated high A and low A videos differently (p &lt; 0.00001 in both cases), no differences could be identified between their A ratings excepting that extraverts provided marginally lower ratings for HA clips (t(56) = -1.4423, p = 0.0774, left-tailed).</p><p>Focusing on V ratings, positive correlation between Ex and V breaks down for HV in 6. Two-sample t-tests also failed to reveal any differences. Therefore, statistical analyses weakly support the negative correlation between Ex and A, but do not corroborate the positive correlation between Ex and V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">H2: Neuroticism vs Arousal and Valence</head><p>The relationship between Neu and A has also been extensively studied-a positive correlation between Neu and A is revealed through fMRI responses in <ref type="bibr" target="#b2">[3]</ref>, and EEG analysis <ref type="bibr" target="#b3">[4]</ref> corroborates this observation for negative V stimuli. <ref type="bibr" target="#b1">[2]</ref> further remarks that neurotics experience negative emotions stronger than emotionally stable persons. In contrast, differing observations have been made regarding the relationship between Neu and V. Negative correlation between Neu and positive V is noted in <ref type="bibr" target="#b2">[3]</ref>, whereas a positive relationship between the two for low A stimuli is observed in <ref type="bibr" target="#b40">[40]</ref>. <ref type="bibr" target="#b1">[2]</ref> remarks that the Neu-V relation is moderated by situation-while neurotics may feel less positive in unpleasant situations, they experience positive emotions as strongly as ES counterparts in pleasant conditions. Negative correlation between Emotional Stability (ES) and A (or positive correlation between Neu and A) is noted only for HALV clips in Table <ref type="table" target="#tab_7">6</ref>. However, posthoc t-tests failed to reveal differences between A ratings for the two categories. Also, Table <ref type="table" target="#tab_7">6</ref> generally suggests a negative correlation between ES and V-t-test comparisons further revealed marginally lower V ratings provided by ES subjects for LALV clips (t(16) = -1.3712, p = 0.0946, left-tailed). Overall, our data does not support the positive relationship between Neu and A, and suggests a weakly positive correlation between Neu and V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">H3: Openness vs Valence and Arousal</head><p>Among the few works to study Openness, <ref type="bibr" target="#b40">[40]</ref> notes a positive correlation between Openness (O) and V under low arousal conditions, which is attributed to the intelligence and sensitivity of creative individuals 4 , enabling them to better appreciate subtly emotional stimuli. Table <ref type="table" target="#tab_7">6</ref> echoes a positive (even if insignificant) correlation between O and V for LA clips but post-hoc t-tests to compare V ratings of open and closed groups failed to reveal any differences. However, we noted that closed individuals felt 4. Creativity strongly correlates with Openness <ref type="bibr" target="#b53">[53]</ref>.</p><p>somewhat more aroused by HA clips than open individuals (t(56) = -1.5011, p = 0.0695, left-tailed) as shown in Fig. <ref type="figure" target="#fig_6">6(a)</ref>. Fine-grained analysis via left-tailed t-tests to compare quadrant-wise ratings again revealed the slightly higher arousal experienced by closed subjects for HAHV clips (t(16) = -1.3753, p = 0.0940, left-tailed). In summary, our data weakly confirms a positive relationship between O and V as noted in <ref type="bibr" target="#b40">[40]</ref>, but suggests a negative correlation between O and A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Agreeableness and Conscientiousness</head><p>Table <ref type="table" target="#tab_7">6</ref> shows a negative but insignificant correlation between Ag and A for HALV videos. Comparison of A ratings by agreeable and disagreeable groups revealed marginally lower A for agreeable subjects for HA clips (t(56) = -1.2964, p = 0.10, left-tailed), and subsequent quadrantwise comparisons attributed this finding to significantly lower A ratings provided by the agreeable group for strongly negative HALV clips (t(16) = -2.6587, p &lt; 0.01, lefttailed). This trend could possibly be attributed to the association of disagreeable persons with negative feelings such as deceit and suspicion. Table <ref type="table" target="#tab_7">6</ref> also shows a negative correlation between Ag and V for highly positive HAHV clips. T -test comparisons again revealed that agreeable subjects provided somewhat lower V ratings for HV clips (t(56) = -1.4285, p = 0.0793, left-tailed), and this was particularly true of HAHV clips for which significantly lower ratings were provided by the agreeable group (t(16) = -2.0878, p &lt; 0.05, left-tailed). Conscientiousness scale differences did not influence VA ratings in any way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">PHYSIOLOGICAL CORRELATES OF EMOTION AND PERSONALITY</head><p>Linear and non-linear analyses presented in the previous sections suggest that correlations between emotional and personality attributes are better revealed while examining user responses to emotionally similar clips. If explicit ratings provided by users are a conscious reflection of their emotional perception, then the analyses employing physiological signals should also reveal similar patterns. We attempt to identify linear and non-linear physiological correlates of emotion and personality considering responses to all and quadrant-specific clips in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Linear correlates of Emotion and Personality</head><p>We attempted to discover physiological correlates of emotional and the big-five personality attributes via partial Pearson correlations. Given the large number of extracted physiological features (Table <ref type="table" target="#tab_3">3</ref>) as compared to the population size for this study, we first performed a principal component analysis (PCA) on each feature modality to avoid overfitting, and retained those components that explained 99% of the variance. This gave us 8-9 predictors for each of the considered modalities. Table <ref type="table" target="#tab_9">8</ref> presents correlations between these principal components, users' affective ratings and personality scales (R • denotes number of significant correlates). For affective dimensions, we determined significant correlates considering mean user V,A ratings provided for the 36 clips. We also trained regression models with  the significantly correlating components as predictors of the dependent emotion/personality variable, and the squared correlations (R 2 ) of these models are also tabulated.</p><formula xml:id="formula_2">Feature R o R 2 R o R 2 R o R 2 R o R 2 R o R 2 R o R 2 R o R 2 All ECG 1 0.</formula><p>Examining Table <ref type="table" target="#tab_9">8</ref>, the relatively few (maximum of 3) number of significant predictors can be attributed to the sparse number of principal components employed for analysis. Considering correlations with A and V, more correlates are observed for A than for V overall. At least one significant correlate is noted for all modalities except GSR. ECG is found to correlate most with A, with one correlate observed for all video types. ECG also has the most number of correlates with V (one significant correlate for LAHV and LALV clips). One EMO correlate is noted for both A and V respectively in the HAHV and HALV quadrants. A solitary EEG correlate is noted for V considering HALV clips.</p><p>A larger number of physiological correlates are observed for personality traits as compared to emotional attributes. Across all five video types, the least number of correlates are noted for Agreeableness, while most correlates are noted for Openness. The ECG modality again corresponds the maximum number of correlates, while no correlates are observed for GSR. EEG and EMO correlates are mainly noted for the Opennness trait. In general, a larger number of physiological correlates are noted for emotionally similar videos for all traits. Also, linear models with a significant R 2 statistic are mainly obtained with emotion-wise similar clips, suggesting that physiology-based linear models can better predict personality traits while examining user responses under similar affective conditions. Most number of significant models are obtained for Openness, while not even one significant model is obtained for Agreeableness. Finally, focusing on the significant quadrant-specific models, the best models are noted for Extraversion (0.44 with ECG features and HALV videos) and Conscientiousness (0.41 with ECG for LAHV clips). This implies that linear physiological models acquire sufficient power to moderately explain personality variations under such conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Non-linear correlates</head><p>To examine non-linear physiological correlates of emotion and personality, we performed a mutual information analysis as previously between extracted features from the four modalities and the said attributes. Given the varying number of features for each modality, we segregated the NMI distribution over all features and the emotion/personality rating using 10-bin histograms. Fig. <ref type="figure" target="#fig_7">7</ref> presents the first moment or the mean of the NMI histogram distribution computed over the different video sets for each emotional/personality attribute.</p><p>It is easy to note from Fig. <ref type="figure" target="#fig_7">7</ref> that personality attributes share more MI with the user physiological responses than A and V, similar to the linear analyses. GSR features share maximum MI with A (highest value of 0.73 for LAHV clips), while EMO features share the most MI with V (peak of 0.75 for HALV clips). In contrast, peak MI of 0.81 is noted between ECG features and Ex. For both emotion and personality attributes, at least one of the NMIs observed with quadrant-based videos is higher than the NMI with all movie clips, implying that a fine-grained examination of the relationship between sub-conscious physiological responses and conscious self-ratings is more informative. Focusing on affective attributes, higher MI between ratings and physiological responses is noted for A for all modalities except EMO. Among the four modalities, ECG and EMO respectively share the most and least MI with A, while EMO and EEG share the highest and least MI with V.</p><p>Focusing on the big-five personality traits, the highest NMI histogram means over all modalities are observed for Ex and Con followed by ES, Agree and O. This trend is strikingly similar to the pattern of MI between affective ratings and personality scores obtained in Fig. <ref type="figure" target="#fig_4">4</ref>.3. Examining sensing modalities, ECG features share the highest MI with all the personality dimensions, while EEG features correspond to the lowest NMI means.</p><note type="other">Arousal Valence Extra Agree Con ES Open ECG GSR EEG EMO</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">RECOGNITION RESULTS</head><p>We performed binary recognition of both emotional and personality attributes to evaluate if the proposed user-centric framework can effectively achieve both. This section details the experiments and results thereof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Emotion recognition</head><p>A salient aspect of our work is the exclusive use of commercial sensors for examining users' physiological behavior.</p><p>To evaluate if our emotion recognition results are comparable to prior affective works which used laboratory-grade sensors, we followed a procedure identical to the DEAP study <ref type="bibr" target="#b9">[10]</ref>. In particular, the most discriminative physiological features were first identified for each modality using Fisher's linear discriminant with a threshold of 0.3. Features corresponding to each user were then fed to the naive Bayes (NB) and linear SVM classifiers as shown in Table <ref type="table" target="#tab_11">9</ref>. A leaveone-out cross-validation scheme employed where one video is held out for testing, while the other videos are used for training. The best mis-classification cost parameter C for linear SVM is determined via grid search over [10 -3 , 10 3 ] again using leave-one-out cross-validation. Table <ref type="table" target="#tab_11">9</ref> presents the mean F1-scores over all users obtained using the NB and SVM classifiers with unimodal features and the decision fusion (W t est ) technique described in <ref type="bibr" target="#b54">[54]</ref>. In decision test sample label is computed as</p><formula xml:id="formula_3">4 i=1 α * i t i p i .</formula><p>Here, i indexes the four modalities used in this work, p i 's denote posterior SVM probabilities, {α * i } are the optimal weights maximizing the F1-score on the training set and t i = α i F i / 4 i=1 α i F i , where F i denotes the F1-score obtained on the training set with the i th modality. Note from Section 3 that there is an equal distribution of high/low A and V, implying a class ratio (and consequently, a baseline F1-score) of 0.5</p><p>Observing Table <ref type="table" target="#tab_11">9</ref>, above-chance emotion recognition is evidently achieved with physiological features extracted using commercial sensors. The obtained F1-scores are superior to DEAP <ref type="bibr" target="#b9">[10]</ref>, which can possibly be attributed to (1) the use of movie clips, which are found to be better than music videos for emotional inducement as discussed in <ref type="bibr" target="#b8">[9]</ref>, and (2) to the considerably larger number of subjects employed in this study, which results in a larger training set. GSR features produce the best recognition performance for both A and V, while ECG features produce the worst recognition performance. Considering individual modalities, EEG features are better for recognizing A as compared to V, while the remaining three achieve better recognition of V. These results are consistent with earlier observations made in <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b54">[54]</ref>. Considering multimodal results, peripheral (ECG+GSR) features perform better than unimodal features for A recognition, while the best multimodal F1score of 0.71 is obtained for V. Finally, comparing the two employed classifiers, NB achieves better recognition than linear SVM for both A and V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Personality recognition</head><p>For binary personality trait recognition, we first dichotomized the big-five personality trait scores based on the median as in Section 5. This resulted in an even distribution of high and low trait labels for Ex and ES, while an inexact split for the other traits. As baselines, we consider majoritybased voting and random voting according to class ratio.</p><p>Based on majority voting, baseline F1-score for the Ex and ES traits is 0.33, and 0.34 for Ag, 0.35 for Con and 0.36 for O. Via class-ratio based voting, a baseline score of 0.5 is achieved for all traits. We performed PCA on each feature modality in an identical fashion to linear correlation analyses prior to classification. A leave one-subject-out crossvalidation scheme was used to compute the recognition results. Three classifiers were employed for recognition, i) naive Bayes, ii) linear (Lin) SVM and iii) Radial Basis Function (RBF) SVM. The C (linear and RBF SVM) and γ (RBF SVM) parameters were tuned via leave-one-subjectout grid search cross-validation on the training set.</p><p>Table <ref type="table" target="#tab_12">10</ref> presents the recognition results, with the best F1-scores achieved using unimodal and multimodal features respectively denoted in bold and bold italics. For each personality trait and video set, a better-than-chance recognition F1-score (&gt; 0.5) is achieved with at least with one of the considered modalities. Considering user physiological responses to all affective videos, the highest and lowest F1scores are respectively achieved for ES (0.73) and O (0.53) traits-note from Fig. <ref type="figure" target="#fig_2">2(c</ref>) that ES has the second-highest variance among the five personality dimensions, while O corresponds to the lowest variance in personality scores. Excepting for the ES trait, higher recognition scores are generally achieved considering user responses to emotionally similar videos, in line with the findings from linear and nonlinear correlation analyses.</p><p>For all personality traits except O, an F1-score higher than 0.6 is achieved for at least some of the video quadrants. Among feature modalities, ECG features produce the best recognition performance across personality traits and video sets, followed by EEG, GSR and EMO. EEG features are found to be optimal for recognizing Ex, while ECG features achieve good recognition for the Ag, Con and ES traits. EMO and GSR modalities work best for the Opennness trait. Focusing on classifiers, RBF SVM produces the best recognition performance for 13 out of 25 (5 personality traits × 5 video sets) conditions, while linear SVM performs best only for three conditions. Linear classifiers NB and Lin SVM perform best for the Ex trait, while RBF SVM, performs best for the O trait.</p><p>Fusion-based recognition is beneficial, and higher recognition scores are generally achieved via multimodal fusion. With user responses acquired for all videos, the highest and least fusion-based F1 scores are achieved for the ES (0.77 with RBF SVM) and O (0.56 with NB) traits respectively. With quadrant-based videos, a maximum F1-score of 0.78 is noted for Con (with linear SVM). NB classifier works best with fusion-based recognition, and produces best performance for the Ex trait achieving optimal recognition for all the five video sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">DISCUSSION</head><p>The correlation analyses and recognition results clearly convey two aspects related to personality recognition from physiological data (i) A fine-grained analysis of users' physiological responses to emotionally similar movie clips enables better characterization of personality differencesthis reflects in the better linear models obtained for personality traits considering quadrant-specific videos in Table <ref type="table" target="#tab_9">8</ref>, and the generally higher NMIs for the same in Fig. <ref type="figure" target="#fig_7">7</ref>. Furthermore, higher F1-scores are typically obtained when physiological responses to emotionally similar clips are used for personality trait recognition. (ii) The relationship between personality scales and physiological features is better captured via non-linear metrics-considerably high MI is noted between emotional ratings and personality scores as well as between affective physiological responses and personality traits, and this observation is reinforced with RBF-SVM producing the best recognition performance.</p><p>Interesting similarities are also evident from the correlation and recognition experiments. The NB and lin SVM classifiers work best for the Ex and ES personality traits, for which a number of linear correlates can be noted in Table <ref type="table" target="#tab_9">8</ref>. Also, minimum number of linear physiological correlates are noted for the Ag trait, for which linear classifiers do not work well (best recognition is achieved with RBF SVM for all video types except 'All' in Table <ref type="table" target="#tab_11">9</ref>). Likewise, no GSR correlate of emotion is observed in Table <ref type="table" target="#tab_9">8</ref>, which reflects in poor emotion recognition of personality traits with linear classifiers using GSR features in Table <ref type="table" target="#tab_11">9</ref>. Also, only some EMO correlates of personality traits are revealed in Table <ref type="table" target="#tab_9">8</ref>, and this modality achieves inferior personality recognition with linear classifiers.</p><p>Comparing Tables <ref type="table" target="#tab_8">7</ref> and<ref type="table" target="#tab_11">Table 9</ref>, EEG shares the least MI with all personality traits among the considered modalities, and RBF SVM performs poorly with EEG features (only one best F1 score in 25 conditions). Conversely, GSR shares considerable MI with personality dimensions, and GSR features work best with the RBF SVM classifier in Table <ref type="table" target="#tab_11">9</ref>. Some discrepancies also arise between the correlation and recognition results. For example, among the big-five personality traits, Openness shares the least MI with all feature modalities but has a number of linear physiological correlates. However, optimal recognition for this trait is achieved with RBF SVM, even though the achieved unimodal F1-scores are the lowest for this trait.</p><p>It is pertinent to point out some limitations of this study in general. Weak linear correlations are noted between emotional and personality scores in Table <ref type="table" target="#tab_7">6</ref>, and only few physiological correlates of emotion and personality are observed in Table <ref type="table" target="#tab_9">8</ref>, which can partly be attributed to the low variance for three of the personality dimensions and particularly the Openness trait, as seen in Fig. <ref type="figure" target="#fig_2">2(c</ref>). In this context, medianbased dichotomization of the personality scores for binary recognition may not be the most appropriate. However, most user-centered affective studies have also demonstrated recognition in a similar manner and on data compiled from small user populations, due to the inherent difficulty in conducting large-scale affective experiments. Overall, the general consistency in the nature of results observed from the correlation and recognition experiments suggest that data artifacts may have only minimally influenced our analyses, and that reliable affect and personality recognition is achievable via the extracted physiological features. Furthermore, we will make the compiled data publicly available for facilitating related research.</p><p>Even though not analyzed in this work, the ASCER-TAIN database also includes Familiarity and Liking ratings, which could be useful for other research studies. For example, studying the individual and combined in-  fluence of familiarity, liking and personality traits on affective behavior could be relevant and useful information for recommender systems. In particular, personality-aware recommender systems have become more popular and appreciated of late <ref type="bibr" target="#b55">[55]</ref>, but the fact that personality differences show up even as consumers watch affective video content can enable video recommender systems to effectively learn user profiles over time. Familiarity and Liking ratings could be also used to replicate and extend related studies. For example, the study presented in <ref type="bibr" target="#b56">[56]</ref> notes a connection between familiarity, liking and the amount of smiling while listening to music. Also, Hamlen and Shuell <ref type="bibr" target="#b57">[57]</ref> find a positive correlation between liking and familiarity for classical music excerpts, which increases when an associated video is accompanied by audio. Similar effects could be tested with emotional movie clips via ASCERTAIN.</p><p>Finally, the importance of using less-intrusive sensors in affective studies has been widely acknowledged <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b24">[25]</ref>. Minimally invasive and wearable sensors enable naturalistic user response, alleviating stress caused by cumbersome clinical/lab-grade equipment. Choosing minimally invasive sensors is especially critical when complex behavioral phenomena such as emotions are the subject of investigation. While most available affective datasets have been compiled using lab equipment <ref type="bibr" target="#b22">[23]</ref>, ASCERTAIN represents one of the first initiatives to exclusively employ wearable sensors for data collection, which not only enhances its ecological validity, but also repeatability and suitability for large-scale user profiling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">CONCLUSION</head><p>We present ASCERTAIN-a new multimodal affective database comprising implicit physiological responses of 58 users collected via commercial and wearable EEG, ECG, GSR sensors, and a webcam while viewing emotional movie clips. Users' explicit affective ratings and big-five personality trait scores are also made available to examine the impact of personality differences on AR. Among AR datasets, AS-CERTAIN is the first to facilitate study of the relationships among physiological, emotional and personality attributes.</p><p>The personality-affect relationship is found to be better characterized via non-linear statistics. Consistent results are obtained when physiological features are employed for analyses in lieu of affective ratings. Finally, AR performance superior to prior works employing lab-grade sensors is achieved (possibly because of the larger sample size used in this study), and above-chance personality trait recognition is obtained with all considered modalities. Personality differences are better characterized by analyzing responses to emotionally similar clips, as noted from both correlation and recognition experiments. Finally, RBF SVM achieves best personality trait recognition, further corroborating a nonlinear emotion-personality relationship.</p><p>We believe that ASCERTAIN will facilitate future AR studies, and spur further examination of the personalityaffect relationship. The fact that personality differences are observable from user responses to emotion-wise similar stimuli paves the way for simultaneous emotion and personality profiling. As recent research has shown that AR is also influenced by demographics such as age and gender <ref type="bibr" target="#b58">[58]</ref>, we will investigate correlates between affective physiological responses and the aforementioned soft-biometrics in future, coupled with a deeper examination on the relationship between personality and affect. We will also investigate how a-priori knowledge of personality can impact the design of user-centered affective studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">ACKNOWLEDGEMENT</head><p>This study is supported by the Human-Centered Cyberphysical Systems research grant from A*STAR Singapore, the MIUR Cluster Active Ageing at Home project and Sensaura Inc.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. (a) ASCERTAIN study overview. (b) Timeline for each trial.</figDesc><graphic coords="4,25.50,52.98,247.68,146.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. (a) Participant with sensors (EEG, ECG and GSR visible) during the experiment, (b) Mean Arousal-Valence (AV) ratings for the 36 movie clips used in our experiment and (c) Box-plots showing distribution of the big-five personality trait scores for 58 users.</figDesc><graphic coords="4,345.58,584.94,175.44,113.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Bar plot showing proportion of trials for which data quality ranges from best (1) to worst (5).</figDesc><graphic coords="5,289.50,358.99,252.00,130.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Boxplots of the mean Arousal, Valence, Engagement, Liking and Familiarity ratings for the different video sets.</figDesc><graphic coords="7,25.50,35.22,98.03,67.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. NMI between big-five trait scales and A (left), V (right) ratings.</figDesc><graphic coords="7,289.50,297.19,123.48,92.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. Quadrant-wise comparisons of (left) A ratings by open and closed groups, and (right) V ratings by agreeable and disagreeable groups .</figDesc><graphic coords="7,289.50,587.65,252.00,113.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. (From top to bottom) Bar plots showing the means of the NMI histograms for the four modalities. Best viewed under zoom.</figDesc><graphic coords="10,25.50,250.17,72.24,54.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>•</head><label></label><figDesc>Ramanathan Subramanian is with the International Institute of Information Technology, Hyderabad, India. (Email: s.ramanathan@iiit.ac.in)</figDesc><table /><note><p>• Julia Wache, Mojtaba Khomami Abadi, Radu L.Vieriu and Nicu Sebe are with the Dept. of Information Engineering and Computer Science, University of Trento, 38123 Trento, Italy. (Email: julia.wache@unitn.it, khomamiabadi@disi.unitn.it, radulaurentiu.vieru@unitn.it, sebe@disi.unitn.it) • Stefan Winkler is with the Advanced Digital Sciences Center, University of Illinois at Urbana-Champaign, Singapore. (Email: Stefan.Winkler@adsc.com.sg)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1</head><label>1</label><figDesc>Comparison of user-centered affective databases. 'var' denotes variable.</figDesc><table><row><cell>Name</cell><cell cols="2">No. subjects No. stimuli</cell><cell>Recorded signals</cell><cell cols="2">Annotations Affect Personality</cell><cell>Comments</cell></row><row><cell>HUMAINE [13]</cell><cell>var</cell><cell>var</cell><cell>audio, visual, physiological</cell><cell>yes</cell><cell>no</cell><cell>includes 6 sub-collections (some non-public)</cell></row><row><cell>DEAP [10]</cell><cell>32</cell><cell>40</cell><cell>physiological</cell><cell>yes</cell><cell>no</cell><cell>focus on music videos</cell></row><row><cell>DECAF [9]</cell><cell>30</cell><cell>76</cell><cell>face, physiological</cell><cell>yes</cell><cell>no</cell><cell>compares music and movie clips</cell></row><row><cell>MAHNOB-HCI [11]</cell><cell>27</cell><cell>20</cell><cell>face, audio, eye gaze, physiological</cell><cell>yes</cell><cell>no</cell><cell>includes video and image stimuli</cell></row><row><cell>ASCERTAIN</cell><cell>58</cell><cell>36</cell><cell>face, physiological</cell><cell>yes</cell><cell>yes</cell><cell>connects emotion and personality</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2</head><label>2</label><figDesc>Summary of the ASCERTAIN database.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 3</head><label>3</label><figDesc>Extracted features for each modality (feature dimension stated in parenthesis). Statistics denote mean, standard deviation (std), skewness, kurtosis of the raw feature over time, and % of times the feature value is above/below mean±std.Average of first derivative, proportion of negative differential samples, mean number of peaks, mean derivative of the inverse channel signal, average number of peaks in the inverse signal, statistics over each of the 8 signal channels provided by the Neurosky software.</figDesc><table><row><cell>Modality</cell><cell>Extracted features</cell></row><row><cell>ECG (32)</cell><cell>Ten low frequency ([0-2.4] Hz) power spectral densities</cell></row><row><cell></cell><cell>(PSDs), four very slow response ([0-0.04] Hz) PSDs, IBI,</cell></row><row><cell></cell><cell>HR and HRV statistics.</cell></row><row><cell>GSR (31)</cell><cell>Mean skin resistance and mean of derivative, mean differ-</cell></row><row><cell></cell><cell>ential for negative values only (mean decrease rate during</cell></row><row><cell></cell><cell>decay time), proportion of negative derivative samples,</cell></row><row><cell></cell><cell>number of local minima in the GSR signal, average rising</cell></row><row><cell></cell><cell>time of the GSR signal, spectral power in the [0-2.4] Hz</cell></row><row><cell></cell><cell>band, zero crossing rate of skin conductance slow response</cell></row><row><cell></cell><cell>([0-0.2] Hz), zero crossing rate of skin conductance very</cell></row><row><cell></cell><cell>slow response ([0-0.08] Hz), mean SCSR and SCVSR peak</cell></row><row><cell></cell><cell>magnitude.</cell></row><row><cell>Frontal EEG</cell><cell></cell></row><row><cell>(88)</cell><cell></cell></row></table><note><p>1949-3045 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2625250, IEEE Transactions on Affective Computing</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2625250, IEEE Transactions on Affective Computing for HA clips as compared to LA clips (t(34) = 5.1253, p &lt; 0.0001). Similarly, mean V ratings for HV and LV clips were significantly different (t(34) = 17.6613, p &lt; 0.00005).</figDesc><table /><note><p>are used in this work. 1949-3045 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 4 Mean</head><label>4</label><figDesc>Pearson correlations between self-ratings across users. *s denote significant correlations (p &lt; 0.05) upon limiting FDR to 5%.</figDesc><table><row><cell></cell><cell>A</cell><cell>V</cell><cell>E</cell><cell>L</cell><cell>F</cell></row><row><cell>Arousal</cell><cell>1</cell><cell>0.02</cell><cell>0.42*</cell><cell>0.19</cell><cell>0.15</cell></row><row><cell>Valence</cell><cell></cell><cell>1</cell><cell>0.21</cell><cell>0.68*</cell><cell>0.17</cell></row><row><cell>Engagement</cell><cell></cell><cell></cell><cell>1</cell><cell>0.42*</cell><cell>0.24</cell></row><row><cell>Liking</cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell>0.34*</cell></row><row><cell>Familiarity</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 5</head><label>5</label><figDesc>Pearson correlations between personality dimensions (* ⇒ p &lt; 0.05)</figDesc><table><row><cell></cell><cell>E</cell><cell>A</cell><cell>Co</cell><cell>ES</cell><cell>O</cell></row><row><cell>Extraversion</cell><cell>1</cell><cell>0.36*</cell><cell>0.19</cell><cell>-0.12</cell><cell>0.30*</cell></row><row><cell>Agreeableness</cell><cell></cell><cell>1</cell><cell>0.21</cell><cell>0.34*</cell><cell>0.30*</cell></row><row><cell>Conscientiousness</cell><cell></cell><cell></cell><cell>1</cell><cell>0.26</cell><cell>0.04</cell></row><row><cell>Emotional Stability</cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell>-0.10</cell></row><row><cell>Openness</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 6</head><label>6</label><figDesc>Partial correlations between personality scales and self-ratings (* ⇒ p &lt; 0.05).</figDesc><table><row><cell></cell><cell></cell><cell>Ex</cell><cell>Ag</cell><cell>Co</cell><cell>ES</cell><cell>O</cell></row><row><cell></cell><cell>Arousal</cell><cell>0.03</cell><cell>-0.10</cell><cell>0.05</cell><cell>0.07</cell><cell>0.06</cell></row><row><cell>All</cell><cell>Valence Engage</cell><cell>0.19 -0.30*</cell><cell>-0.02 0.01</cell><cell>0.02 0.09</cell><cell>-0.18 0.00</cell><cell>0.07 -0.10</cell></row><row><cell></cell><cell>Liking</cell><cell>-0.13</cell><cell>0.07</cell><cell>-0.22</cell><cell>0.21</cell><cell>-0.02</cell></row><row><cell></cell><cell>Arousal</cell><cell>-0.01</cell><cell>0.02</cell><cell>-0.01</cell><cell>0.22</cell><cell>-0.11</cell></row><row><cell>HAHV</cell><cell>Valence Engage</cell><cell>-0.12 -0.30*</cell><cell>-0.38* 0.16</cell><cell>-0.11 0.17</cell><cell>-0.12 0.10</cell><cell>-0.16 -0.09</cell></row><row><cell></cell><cell>Liking</cell><cell>0.20</cell><cell>0.22</cell><cell>-0.00</cell><cell>0.10</cell><cell>0.22</cell></row><row><cell></cell><cell>Arousal</cell><cell>-0.06</cell><cell>0.07</cell><cell>0.06</cell><cell>0.11</cell><cell>0.04</cell></row><row><cell>LAHV</cell><cell>Valence Engage</cell><cell>-0.03 -0.22</cell><cell>0.05 0.03</cell><cell>-0.08 -0.06</cell><cell>-0.10 -0.10</cell><cell>0.23 -0.24</cell></row><row><cell></cell><cell>Liking</cell><cell>0.20</cell><cell>-0.01</cell><cell>0.13</cell><cell>0.17</cell><cell>0.12</cell></row><row><cell></cell><cell>Arousal</cell><cell>0.03</cell><cell>-0.09</cell><cell>0.02</cell><cell>-0.07</cell><cell>0.09</cell></row><row><cell>LALV</cell><cell>Valence Engage</cell><cell>0.20 -0.22</cell><cell>0.06 -0.01</cell><cell>0.01 0.02</cell><cell>-0.22 -0.05</cell><cell>0.15 -0.04</cell></row><row><cell></cell><cell>Liking</cell><cell>-0.14</cell><cell>0.03</cell><cell>-0.19</cell><cell>0.12</cell><cell>-0.10</cell></row><row><cell></cell><cell>Arousal</cell><cell>0.20</cell><cell>-0.25</cell><cell>-0.00</cell><cell>-0.16</cell><cell>0.09</cell></row><row><cell>HALV</cell><cell>Valence Engage</cell><cell>0.22 -0.30*</cell><cell>0.01 0.01</cell><cell>0.09 0.10</cell><cell>-0.06 0.12</cell><cell>-0.05 -0.10</cell></row><row><cell></cell><cell>Liking</cell><cell>-0.26*</cell><cell>0.03</cell><cell>-0.35*</cell><cell>0.12</cell><cell>-0.07</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 7 R</head><label>7</label><figDesc>2 and best three predictors for the five personality dimensions. Full model coefficients are shown in parentheses. * ⇒ p &lt; 0.05.</figDesc><table><row><cell></cell><cell>Ex</cell><cell>Ag</cell><cell>Co</cell><cell>ES</cell><cell>O</cell></row><row><cell>All</cell><cell>0.14* (0.14) V,E,L</cell><cell>0.02 (0.02) A,V,L</cell><cell>0.07 (0.07) V,E,L</cell><cell>0.06 (0.06) A,V,L</cell><cell>0.02 (0.02) A,V,E</cell></row><row><cell>HAHV</cell><cell>0.16* (0.16) V,E,L</cell><cell>0.17* (0.17) V,E,L</cell><cell>0.05 (0.05) A,V,E</cell><cell>0.12* (0.13) A,V,L</cell><cell>0.05 (0.06) A,V,L</cell></row><row><cell>LAHV</cell><cell>0.07 (0.08) A,E,L</cell><cell>0.02 (0.02) A,V,E</cell><cell>0.02 (0.02) A,E,L</cell><cell>0.04 (0.05) A,E,L</cell><cell>0.13* (0.13) V,E,L</cell></row><row><cell>LALV</cell><cell>0.12 (0.12) V,E,L</cell><cell>0.02 (0.02) A,V,L</cell><cell>0.05 (0.05) A,E,L</cell><cell>0.05 (0.05) A,V,L</cell><cell>0.03 (0.03) A,V,L</cell></row><row><cell>HALV</cell><cell>0.16* (0.20) V,E,L</cell><cell>0.09 (0.09) A,V,L</cell><cell>0.15 (0.16) V,E,L</cell><cell>0.04 (0.05) A,E,L</cell><cell>0.03 (0.04) A,E,L</cell></row><row><cell cols="6">tions, moderate and positive correlations are noted between</cell></row><row><cell cols="6">Extraversion (Ex) and Agreeableness (Ag), as well as be-</cell></row><row><cell cols="6">tween Ex and Openness (O)-prior studies have noted</cell></row><row><cell cols="6">that Ex and O are correlated via the sensation seeking</cell></row><row><cell cols="6">construct [50]. Ag is also found to moderately and positively</cell></row></table><note><p>correlate with Emotional Stability (ES) and O. Conversely, weakly negative-but-insignificant correlations are observed between (i) Ex and ES, and (ii) ES and O.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 8</head><label>8</label><figDesc>Physiological correlates of emotion and personality attributes. R • denotes the number of significant feature correlates, while R 2 is the coefficient of determination for the regression model with the significant correlates as predictors. Bold values denote linear regression models with a significant R 2 statistic.</figDesc><table><row><cell>Arousal</cell><cell>Valence</cell><cell>Extra.</cell><cell>Agreeable</cell><cell>Conscient</cell><cell>Em. Stab.</cell><cell>Open</cell></row><row><cell>Video Set</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 9</head><label>9</label><figDesc>Affective state recognition with linear SVM and Naive Bayes (NB) classifiers. Mean F1-scores over all participants for the four modalities, peripheral Signals (ECG + GSR) and late fusion (W t est ) are shown. Baseline F1-score is 0.5. Maximum unimodal F1-scores are shown in bold.</figDesc><table><row><cell></cell><cell cols="2">ECG</cell><cell cols="2">GSR</cell><cell cols="2">EMO</cell><cell cols="2">EEG</cell><cell cols="2">Peripheral</cell><cell>W t est</cell><cell></cell><cell>Class Ratio</cell></row><row><cell></cell><cell>SVM</cell><cell>NB</cell><cell>SVM</cell><cell>NB</cell><cell>SVM</cell><cell>NB</cell><cell>SVM</cell><cell>NB</cell><cell>SVM</cell><cell>NB</cell><cell>SVM</cell><cell>NB</cell><cell></cell></row><row><cell>Valence</cell><cell>0.56</cell><cell>0.60</cell><cell>0.64</cell><cell>0.68</cell><cell>0.68</cell><cell>0.68</cell><cell>0.56</cell><cell>0.60</cell><cell>0.64</cell><cell>0.60</cell><cell>0.69</cell><cell>0.71</cell><cell>0.50</cell></row><row><cell>Arousal</cell><cell>0.57</cell><cell>0.59</cell><cell>0.61</cell><cell>0.66</cell><cell>0.59</cell><cell>0.61</cell><cell>0.58</cell><cell>0.61</cell><cell>0.62</cell><cell>0.69</cell><cell>0.64</cell><cell>0.67</cell><cell>0.50</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE 10</head><label>10</label><figDesc>Personality recognition considering affective responses to a) all, and b) emotionally homogeneous stimuli. Maximum F1-scores with unimodal classifiers are shown in bold. Maximum fusion scores are denoted in bold italics.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Extravert</cell><cell></cell><cell></cell><cell>Agreeable</cell><cell></cell><cell></cell><cell>Conscient</cell><cell></cell><cell></cell><cell>Em. Stab</cell><cell></cell><cell></cell><cell>Open</cell><cell></cell></row><row><cell>Videos</cell><cell>Method</cell><cell>NB</cell><cell>SVM</cell><cell>SVM</cell><cell>NB</cell><cell>SVM</cell><cell>SVM</cell><cell>NB</cell><cell>SVM</cell><cell>SVM</cell><cell>NB</cell><cell>SVM</cell><cell>SVM</cell><cell>NB</cell><cell>SVM</cell><cell>SVM</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(lin)</cell><cell>(rbf)</cell><cell></cell><cell>(lin)</cell><cell>(rbf)</cell><cell></cell><cell>(lin)</cell><cell>(rbf)</cell><cell></cell><cell>(lin)</cell><cell>(rbf)</cell><cell></cell><cell>(lin)</cell><cell>(rbf)</cell></row><row><cell></cell><cell>ECG</cell><cell cols="2">0.56 0.06</cell><cell>0.53</cell><cell cols="2">0.55 0.45</cell><cell>0.32</cell><cell cols="2">0.60 0.51</cell><cell>0.55</cell><cell cols="2">0.53 0.60</cell><cell>0.58</cell><cell cols="2">0.48 0.35</cell><cell>0.49</cell></row><row><cell></cell><cell>EEG</cell><cell cols="2">0.63 0.52</cell><cell>0.48</cell><cell cols="2">0.52 0.12</cell><cell>0.54</cell><cell cols="2">0.35 0.35</cell><cell>0.31</cell><cell cols="2">0.26 0.46</cell><cell>0.51</cell><cell cols="2">0.34 0.36</cell><cell>0.37</cell></row><row><cell>All</cell><cell>EMO</cell><cell cols="2">0.35 0.31</cell><cell>0.45</cell><cell cols="2">0.40 0.35</cell><cell>0.42</cell><cell cols="2">0.39 0.36</cell><cell>0.34</cell><cell cols="2">0.44 0.36</cell><cell>0.47</cell><cell cols="2">0.50 0.36</cell><cell>0.26</cell></row><row><cell></cell><cell>GSR</cell><cell cols="2">0.45 0.00</cell><cell>0.35</cell><cell cols="2">0.39 0.34</cell><cell>0.27</cell><cell cols="2">0.57 0.35</cell><cell>0.54</cell><cell cols="2">0.49 0.56</cell><cell>0.73</cell><cell cols="2">0.28 0.36</cell><cell>0.53</cell></row><row><cell></cell><cell>W t est</cell><cell cols="2">0.65 0.52</cell><cell>0.57</cell><cell cols="2">0.58 0.46</cell><cell>0.53</cell><cell cols="2">0.65 0.59</cell><cell>0.67</cell><cell cols="2">0.59 0.64</cell><cell>0.77</cell><cell cols="2">0.56 0.42</cell><cell>0.53</cell></row><row><cell></cell><cell>ECG</cell><cell cols="2">0.59 0.00</cell><cell>0.56</cell><cell cols="2">0.48 0.29</cell><cell>0.55</cell><cell cols="2">0.50 0.32</cell><cell>0.52</cell><cell cols="2">0.55 0.46</cell><cell>0.60</cell><cell cols="2">0.45 0.34</cell><cell>0.55</cell></row><row><cell></cell><cell>EEG</cell><cell cols="2">0.63 0.43</cell><cell>0.63</cell><cell cols="2">0.54 0.10</cell><cell>0.10</cell><cell cols="2">0.34 0.35</cell><cell>0.33</cell><cell cols="2">0.19 0.32</cell><cell>0.57</cell><cell cols="2">0.41 0.35</cell><cell>0.45</cell></row><row><cell>HAHV</cell><cell>EMO</cell><cell cols="2">0.39 0.00</cell><cell>0.54</cell><cell cols="2">0.54 0.34</cell><cell>0.62</cell><cell cols="2">0.35 0.37</cell><cell>0.33</cell><cell cols="2">0.46 0.35</cell><cell>0.34</cell><cell cols="2">0.46 0.36</cell><cell>0.35</cell></row><row><cell></cell><cell>GSR</cell><cell cols="2">0.22 0.00</cell><cell>0.31</cell><cell cols="2">0.47 0.34</cell><cell>0.51</cell><cell cols="2">0.53 0.35</cell><cell>0.50</cell><cell cols="2">0.42 0.51</cell><cell>0.46</cell><cell cols="2">0.28 0.36</cell><cell>0.35</cell></row><row><cell></cell><cell>W t est</cell><cell cols="2">0.65 0.43</cell><cell>0.63</cell><cell cols="2">0.53 0.47</cell><cell>0.61</cell><cell cols="2">0.60 0.56</cell><cell>0.53</cell><cell cols="2">0.62 0.62</cell><cell>0.62</cell><cell cols="2">0.59 0.46</cell><cell>0.54</cell></row><row><cell></cell><cell>ECG</cell><cell cols="2">0.55 0.02</cell><cell>0.53</cell><cell cols="2">0.58 0.45</cell><cell>0.60</cell><cell cols="2">0.70 0.78</cell><cell>0.74</cell><cell cols="2">0.55 0.46</cell><cell>0.41</cell><cell cols="2">0.56 0.42</cell><cell>0.49</cell></row><row><cell></cell><cell>EEG</cell><cell cols="2">0.63 0.53</cell><cell>0.63</cell><cell cols="2">0.49 0.12</cell><cell>0.42</cell><cell cols="2">0.34 0.35</cell><cell>0.30</cell><cell cols="2">0.32 0.55</cell><cell>0.54</cell><cell cols="2">0.34 0.36</cell><cell>0.27</cell></row><row><cell>LAHV</cell><cell>EMO</cell><cell cols="2">0.49 0.34</cell><cell>0.51</cell><cell cols="2">0.43 0.35</cell><cell>0.10</cell><cell cols="2">0.58 0.37</cell><cell>0.36</cell><cell cols="2">0.51 0.35</cell><cell>0.39</cell><cell cols="2">0.46 0.37</cell><cell>0.57</cell></row><row><cell></cell><cell>GSR</cell><cell cols="2">0.45 0.00</cell><cell>0.36</cell><cell cols="2">0.51 0.34</cell><cell>0.34</cell><cell cols="2">0.59 0.35</cell><cell>0.62</cell><cell cols="2">0.54 0.52</cell><cell>0.52</cell><cell cols="2">0.28 0.36</cell><cell>0.36</cell></row><row><cell></cell><cell>W t est</cell><cell cols="2">0.67 0.52</cell><cell>0.66</cell><cell cols="2">0.62 0.49</cell><cell>0.60</cell><cell cols="2">0.74 0.78</cell><cell>0.76</cell><cell cols="2">0.62 0.60</cell><cell>0.61</cell><cell cols="2">0.67 0.49</cell><cell>0.59</cell></row><row><cell></cell><cell>ECG</cell><cell cols="2">0.58 0.10</cell><cell>0.49</cell><cell cols="2">0.43 0.29</cell><cell>0.36</cell><cell cols="2">0.55 0.55</cell><cell>0.74</cell><cell cols="2">0.53 0.58</cell><cell>0.50</cell><cell cols="2">0.55 0.36</cell><cell>0.43</cell></row><row><cell></cell><cell>EEG</cell><cell cols="2">0.61 0.63</cell><cell>0.57</cell><cell cols="2">0.19 0.11</cell><cell>0.50</cell><cell cols="2">0.37 0.35</cell><cell>0.59</cell><cell cols="2">0.33 0.39</cell><cell>0.42</cell><cell cols="2">0.41 0.36</cell><cell>0.29</cell></row><row><cell>LALV</cell><cell>EMO</cell><cell cols="2">0.56 0.00</cell><cell>0.33</cell><cell cols="2">0.54 0.18</cell><cell>0.61</cell><cell cols="2">0.30 0.37</cell><cell>0.36</cell><cell cols="2">0.49 0.35</cell><cell>0.49</cell><cell cols="2">0.33 0.36</cell><cell>0.48</cell></row><row><cell></cell><cell>GSR</cell><cell cols="2">0.47 0.00</cell><cell>0.47</cell><cell cols="2">0.50 0.34</cell><cell>0.51</cell><cell cols="2">0.32 0.35</cell><cell>0.50</cell><cell cols="2">0.52 0.59</cell><cell>0.69</cell><cell cols="2">0.28 0.36</cell><cell>0.56</cell></row><row><cell></cell><cell>W t est</cell><cell cols="2">0.64 0.61</cell><cell>0.58</cell><cell cols="2">0.57 0.34</cell><cell>0.59</cell><cell cols="2">0.58 0.56</cell><cell>0.76</cell><cell cols="2">0.69 0.69</cell><cell>0.75</cell><cell cols="2">0.57 0.46</cell><cell>0.63</cell></row><row><cell></cell><cell>ECG</cell><cell cols="2">0.50 0.00</cell><cell>0.51</cell><cell cols="2">0.51 0.32</cell><cell>0.62</cell><cell cols="2">0.57 0.57</cell><cell>0.62</cell><cell cols="2">0.59 0.56</cell><cell>0.66</cell><cell cols="2">0.45 0.33</cell><cell>0.50</cell></row><row><cell></cell><cell>EEG</cell><cell cols="2">0.65 0.53</cell><cell>0.50</cell><cell cols="2">0.42 0.07</cell><cell>0.14</cell><cell cols="2">0.42 0.35</cell><cell>0.33</cell><cell cols="2">0.27 0.32</cell><cell>0.42</cell><cell cols="2">0.34 0.36</cell><cell>0.53</cell></row><row><cell>HALV</cell><cell>EMO</cell><cell cols="2">0.32 0.34</cell><cell>0.30</cell><cell cols="2">0.47 0.35</cell><cell>0.47</cell><cell cols="2">0.42 0.36</cell><cell>0.40</cell><cell cols="2">0.33 0.36</cell><cell>0.44</cell><cell cols="2">0.56 0.36</cell><cell>0.60</cell></row><row><cell></cell><cell>GSR</cell><cell cols="2">0.38 0.26</cell><cell>0.35</cell><cell cols="2">0.33 0.34</cell><cell>0.25</cell><cell cols="2">0.55 0.30</cell><cell>0.48</cell><cell cols="2">0.49 0.47</cell><cell>0.45</cell><cell cols="2">0.30 0.36</cell><cell>0.60</cell></row><row><cell></cell><cell>W t est</cell><cell cols="2">0.67 0.57</cell><cell>0.59</cell><cell cols="2">0.55 0.46</cell><cell>0.55</cell><cell cols="2">0.62 0.58</cell><cell>0.59</cell><cell cols="2">0.59 0.65</cell><cell>0.67</cell><cell cols="2">0.57 0.42</cell><cell>0.66</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, AUGUST 2016</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Dimensions of Personality, ser. The Int&apos;l library of psychology</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Eysenck</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1947">1947</date>
			<publisher>Transaction Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Clarifying the relation between Neuroticism and positive emotions</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Personality and Individual Differences</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="69" to="72" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Personality modulates the effects of emotional arousal and valence on brain activation</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">G</forename><surname>Kehoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Toomey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Balsters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L W</forename><surname>Bokde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Cognitive &amp; Affective Neuroscience</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="858" to="870" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Personality and the EEG: Arousal and emotional arousability</title>
		<author>
			<persName><forename type="first">G</forename><surname>Stenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Personality and Individual Differences</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1097" to="1113" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Personality, event-related potential (ERP) and heart rate (HR) in emotional word processing</title>
		<author>
			<persName><forename type="first">V</forename><surname>De Pascalis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Strippoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Riccardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Vergari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Personality and Individual Differences</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">2004</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A survey of personality computing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vinciarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mohammadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Affective Computing</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Implicit Personality Profiling Based on Psycho-Physiological Responses to Emotional Videos</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Vieriu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Winkler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int&apos;l Conference on Multimodal Interaction</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Emotion elicitation using films</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Levenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition &amp; Emotion</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="87" to="108" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">DECAF: MEG-based Multimodal Database for Decoding Affective Physiological Responses</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Avesani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Patras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Affective Computing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">DEAP: A Database for Emotion Analysis Using Physiological Signals</title>
		<author>
			<persName><forename type="first">S</forename><surname>Koelstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ühl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Soleymani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yazdan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nijholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Patras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Affective Computing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="18" to="31" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A multimodal database for affect recognition and implicit tagging</title>
		<author>
			<persName><forename type="first">M</forename><surname>Soleymani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lichtenauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Affective Computing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="42" to="55" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Inference of Personality Traits and Affect Schedule by Analysis of Spontaneous Reactions to Affective Videos</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A M</forename><surname>Correa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Patras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>in Face and Gesture</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The HUMAINE Database: Addressing the Collection and Annotation of Naturalistic and Induced Emotional Data</title>
		<author>
			<persName><forename type="first">E</forename><surname>Douglas-Cowie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sneddon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Lowry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mcrorie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Devillers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Abrilian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Batliner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Karpouzis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Affective Computing and Intelligent Interaction</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="488" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">You stupid tin box -children interacting with the AIBO robot: A cross-linguistic emotional speech corpus</title>
		<author>
			<persName><forename type="first">A</forename><surname>Batliner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hacker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Steidl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Öth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>D'arcy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Language Resources and Evaluation</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">How to find trouble in communication</title>
		<author>
			<persName><forename type="first">A</forename><surname>Batliner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Spilker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">N</forename><surname>Öth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech communication</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="117" to="143" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Toward detecting emotions in spoken dialogs</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech and Audio Processing</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="293" to="303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The extended Cohn-Kanade dataset (CK+): A complete dataset for action unit and emotion-specified expression</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ambadar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="94" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A 3D facial expression database for facial behavior research</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Rosato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Face and Gesture</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="211" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Web-based database for facial expression analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rademaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Maat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int&apos;l Conference on Multimedia and Expo</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Modeling naturalistic affective states via facial and vocal expressions recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Caridakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Malatesta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kessous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Raouzaiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Karpouzis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int&apos;l Conference on Multimodal Interaction</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="146" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Emotion recognition in human-computer interaction</title>
		<author>
			<persName><forename type="first">N</forename><surname>Fragopanagos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="389" to="405" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Emotion recognition based on joint visual and audio cues</title>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int&apos;l Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1136" to="1139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Video affective content analysis: a survey of state-of-the-art methods</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Emotion recognition based on physiological changes in music listening</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Andre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2067" to="2083" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Using noninvasive wearable computers to recognize human emotions from physiological signals</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Lisetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nasoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Advances in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">2004</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1672" to="1687" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Predicting the valence of a scene from observers eye movements</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Tavakoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Atyabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rantanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Laukka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nefti-Meziani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heikkil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Emotion modulates eye movement patterns and subsequent memory for the gist and details of movie scenes</title>
		<author>
			<persName><forename type="first">R</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Melcher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">NEO-PI-R professional manual: Revised NEO personality and NEO Five-Factor Inventory</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T J</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Mccrae</surname></persName>
		</author>
		<imprint>
			<publisher>NEO-FFI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Florida</forename><surname>Odessa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Assessment Resources</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Lexical predictors of personality type</title>
		<author>
			<persName><forename type="first">S</forename><surname>Argamon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dhawle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Koppel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pennbaker</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interface and the Classification Society of North America</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sensible organizations: Technology and methodology for automatically measuring organizational behavior</title>
		<author>
			<persName><forename type="first">D</forename><surname>Olguin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Waber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Systems, Man, and Cybernetics-Part B: Cybernetics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="43" to="55" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">SALSA: A novel dataset for multimodal group behavior analysis</title>
		<author>
			<persName><forename type="first">X</forename><surname>Alameda-Pineda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Staiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Batrinca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lepri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Lanz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Connecting meeting behavior withExtraversion -A systematic study</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lepri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kalimeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Staiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pianesi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Affective Computing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="443" to="455" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">On the relationship between head pose, social attention and personality prediction for unstructured and dynamic group interactions</title>
		<author>
			<persName><forename type="first">R</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Staiano</surname></persName>
		</author>
		<author>
			<persName><surname>Lanz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Oswald</surname></persName>
		</author>
		<author>
			<persName><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int&apos;l Conference on Multimodal Interaction</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Space speaks: towards socially and personality aware visual surveillance</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lepri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Lanz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Int&apos;l Workshop on Multimodal Pervasive Video Analysis</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="37" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Using linguistic cues for the automatic recognition of personality in conversation and text</title>
		<author>
			<persName><forename type="first">F</forename><surname>Mairesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Mehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="457" to="500" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Friends don&apos;t lie: Inferring personality traits from social network structure</title>
		<author>
			<persName><forename type="first">J</forename><surname>Staiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lepri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Aharony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pianesi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Ubiquitous Computing</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="321" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Don&apos;t ask me what i&apos;m like, just watch and listen</title>
		<author>
			<persName><forename type="first">R</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ACM Int&apos;l Conference on Multimedia</title>
		<imprint>
			<biblScope unit="page" from="329" to="338" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Neuroticism, Extraversion, Conscientiousness and Stress: Physiological Correlates</title>
		<author>
			<persName><forename type="first">A.-M</forename><surname>Brouwer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G V</forename><surname>Schaik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E H</forename><surname>Korteling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B F V</forename><surname>Erp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Toet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Affective Computing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="109" to="117" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Individual differences in the experience of emotions</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Kuiper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clinical Psychology Review</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="791" to="821" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Evaluation of International Affective Picture System (IAPS) ratings in an athlete population and its relations to personality</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Koyuncu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dural</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Catikkas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Personality and Individual Differences</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="461" to="466" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Simple visual reaction time , personality strength of the nervous system : theory approach</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nicholson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Personality and Individual Differences</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="461" to="469" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Autonomic responses to video presentations</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Brumbaugh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kothuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Marci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Siefert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Pfaff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Psychophysiology and Biofeedback</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="293" to="301" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>Physiological correlates of the Big</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Why extraverts are happier than introverts: The role of mood regulation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lischetzke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Eid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of personality</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1127" to="1162" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Personality, media preferences, and cultural participation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kraaykamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">V</forename><surname>Eijck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Personality and Individual Differences</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1675" to="1688" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Effects of personality types on the use of television genre</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Shim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Paul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Broadcasting &amp; Electronic Media</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="287" to="304" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Analyzing personality-related adjectives from an eticemic perspective: the big five marker scale (BFMS) and the Italian AB5C taxonomy</title>
		<author>
			<persName><forename type="first">M</forename><surname>Perugini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Di</forename><surname>Blas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Big Five Assessment</title>
		<imprint>
			<biblScope unit="page" from="281" to="304" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Looking at the viewer: analysing facial activity to detect personal highlights of multimedia contents</title>
		<author>
			<persName><forename type="first">H</forename><surname>Joho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Staiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Jose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia Tools and Applications</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="505" to="523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Controlling the false discovery rate: a practical and powerful approach to multiple testing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Benjamini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hochberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="289" to="300" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Exposure and affect: Overview and meta-analysis of research, 1968-1987</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Bornstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological bulletin</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">265</biblScope>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Relationships among Extraversion, Openness to experience, and sensation seeking</title>
		<author>
			<persName><forename type="first">A</forename><surname>Aluja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Garca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Garca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Personality and Individual Differences</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="671" to="680" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">An overlap invariant entropy measure of 3D medical image alignment</title>
		<author>
			<persName><forename type="first">C</forename><surname>Studholme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hawkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="71" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Influence of Extraversion and Neuroticism on Subjective Well-Being: Happy and Unhappy People</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Mccrae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">668</biblScope>
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Parental descriptions of child personality: Developmental antecedents of the Big Five</title>
		<author>
			<persName><forename type="first">I</forename><surname>Mervielde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">De</forename><surname>Fruyt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jarmuz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="105" to="126" />
		</imprint>
	</monogr>
	<note>Linking openness and intellect in childhood and adulthood</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Fusion of facial expressions and EEG for implicit affective tagging</title>
		<author>
			<persName><forename type="first">S</forename><surname>Koelstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Patras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="164" to="174" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A Comparative User Study on Rating vs. Personality Quiz based Preference Elicitation Methods</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent User Interfaces</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="367" to="371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Play it again Sam: Repeated exposure to emotionally evocative music polarises liking and smiling responses, and influences other affective reports, facial emg, and heart rate</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">V</forename><surname>Witvliet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Vrana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition and Emotion</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="25" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">The effects of familiarity and audiovisual stimuli on preference for classical music</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Hamlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Shuell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bulletin of the Council for Research in Music Education</title>
		<imprint>
			<biblScope unit="page" from="21" to="34" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">What&apos;s good for the goose is not good for the gander: age and gender differences in scanning emotion faces</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Hutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ruffman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journals of Gerontology, Series B: Psychological Sciences and Social Sciences</title>
		<imprint>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
