<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Acoustic Modeling Based on Deep Learning for Low-Resource Speech Recognition: An Overview</title>
				<funder ref="#_yUgCdDA">
					<orgName type="full">Ministry of Education Humanities and Social Sciences Research Planning Fund Project</orgName>
				</funder>
				<funder>
					<orgName type="full">Graduate Student Research Capacity Improvement Program of Beijing Technology and Business University</orgName>
				</funder>
				<funder ref="#_Yra6Su7 #_6zSmkKP #_JBkBkqJ #_V2jfwGZ #_fZDPfkB #_R8GZRUx">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_erxJrPR">
					<orgName type="full">Major projects of the National Social Science Fund</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Chongchong</forename><surname>Yu</surname></persName>
							<email>yucc@btbu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Key Laboratory of Industrial Internet and Big Data</orgName>
								<orgName type="institution">Beijing Technology and Business University</orgName>
								<address>
									<postCode>100048</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Meng</forename><surname>Kang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Key Laboratory of Industrial Internet and Big Data</orgName>
								<orgName type="institution">Beijing Technology and Business University</orgName>
								<address>
									<postCode>100048</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yunbing</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Putian Information Technology Co., Ltd</orgName>
								<address>
									<postCode>100080</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiajia</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Key Laboratory of Industrial Internet and Big Data</orgName>
								<orgName type="institution">Beijing Technology and Business University</orgName>
								<address>
									<postCode>100048</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xia</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Key Laboratory of Industrial Internet and Big Data</orgName>
								<orgName type="institution">Beijing Technology and Business University</orgName>
								<address>
									<postCode>100048</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Acoustic Modeling Based on Deep Learning for Low-Resource Speech Recognition: An Overview</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/ACCESS.2020.3020421</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2020.3020421, IEEE Access VOLUME XX, 2017 1 This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2020.3020421, IEEE Access VOLUME XX, 2017 9</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Low-resource languages</term>
					<term>automatic speech recognition</term>
					<term>acoustic model</term>
					<term>data augmentation</term>
					<term>multitask learning</term>
					<term>transfer learning</term>
					<term>meta learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Speech is the most simple and smooth way of communication in human interaction, which can quickly and accurately convey effective information. Nowadays, people are devoted to studying how to communicate with various smart devices through the medium of speech while using them. At present, there are a variety of voice assistants that understand human voice information through interactive real-time intelligent dialogue and realize automatic operation according to the content, such as Apple's Siri and Google's assistant. Therefore, Automatic Speech Recognition (ASR) is the key technology throughout the human-computer interaction processing. The purpose of ASR is to transform speech signals into textual information, thus providing a strong foundation for further semantic understanding. ASR is an interdisciplinary and comprehensive technology, including computer technology, acoustics, digital signal processing, statistics, linguistics, artificial intelligence, and so on. Thanks to the rapid development of related disciplines, the performance of ASR system has been greatly improved and widely used in various scenarios, including military, medical, and service industries, which greatly saves human resources and improves work efficiency.</p><p>The rapid development of ASR relies more on the support of a large amount of speech data and annotated text. The recognition of majority languages such as Chinese and English has achieved very mature performance, but it is difficult to be applied in many dialects and minority languages. There are many kinds of these languages that lack resources in terms of transcribed speech data, pronunciation dictionaries, language knowledge, text annotation, and others, which are defined as low-resource languages <ref type="bibr" target="#b2">[1]</ref>. It is estimated that there are more than 7,000 languages in the world, among which at least 40% are endangered languages, and about half of them have no written forms <ref type="bibr">[2]</ref>. From the perspective of natural ecology, the language pattern of the contemporary world is polarized between the major languages and the endangered languages.</p><p>As a carrier and an important part of culture, the extinction of language is an irreparable loss to rich language resources of the world and greatly damage the diversity of culture. Faced with the impact of majority languages, globalization, and internet, the international linguistic community has gradually attached importance to the theoretical creation and technological development of language protection. It has adopted high-tech digital means to collect all kinds of endangered languages in the world and established audio information database of digital languages <ref type="bibr" target="#b3">[3]</ref>. However, the processing of speech in the construction of corpora is extremely difficult. All the speech in the audio and video materials obtained through field investigations must be manually annotated to make these corpora more widely understandable. Manual annotation requires a lot of manpower. Moreover, there is a shortage of native speakers or professionals who can perform corpus annotation processing. As a result, a lot of original audio or video materials of many languages are piled up and cannot be processed. Efforts to preserve linguistic diversity and sustainability remain to be explored. Although ordinary ASR systems are not ideal for lowresource languages that are difficult to provide large amounts of training data, the use of speech technology is still the most direct and effective way to conduct research, as demonstrated by unwritten languages. More and more scholars have begun to conduct in-depth research and continuous improvement on speech recognition technology under the condition of limited language resources. This work is called low-resource speech recognition, which has become one of the hot issues and important challenges in the field of ASR. Figure <ref type="figure" target="#fig_0">1</ref> is a statistical graph of the quantity of published articles on the Web of Science website, Engineering Village and Scopus about the keywords Low Resource and Speech Recognition in the past decade. On the whole, the number of papers is increasing year by year, which reflects the international attention on low-resource speech recognition research. Low-resource speech recognition has important research significance, especially playing an irreplaceable role in the protection and promotion of linguistic diversity, cultural inheritance, and human communication in the world.</p><p>The rest of this paper is organized as follows. In Section 2, we briefly review the principle and history of ASR. In Section 3, we introduce the application and improvement of acoustic models based on neural networks and end-to-end structure in low-resource scenario. Section 4 discusses how to improve the performance of low-resource speech recognition from data-wise and model-wise. Section 5 introduces two projects for low-resource languages. The possible future works of the low-resource speech recognition are given in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND OF AUTOMATIC SPEECH RECOGNITION</head><p>The traditional architecture of speech recognition system is shown in the Figure <ref type="figure" target="#fig_2">2</ref>, which consists of four parts: feature extraction, acoustic model, language model, and decoder. The signal preprocessing and feature extraction take speech signal as input, enhance speech quality by eliminating noise and channel distortion and transform signal from time domain to frequency domain for extracting feature vector. After the acoustic model and language model, the best sequence W ? corresponding to the speech is output using the maximum posterior probability criterion and related decoding algorithm. W ? can be calculated by Bayesian formula. The specific calculation method of W ? is shown in the ( <ref type="formula">1</ref>  including Linear Discriminant Analysis (LDA), Maximum Likelihood Linear Transformation (MLLT), i-Vector, feature Maximum Likelihood Linear Regression (fMLLR) and others as speech feature extraction. The breakthrough period of acoustic model was in the 1980s. The method based on statistical models which were represented by the Gaussian Mixture Model-Hidden Markov model (GMM-HMM) method <ref type="bibr" target="#b4">[4]</ref> gradually became dominant in speech recognition research. A series of other related technologies based on the HMM method have also been derived <ref type="bibr" target="#b5">[5]</ref>, such as the use of Maximum Likelihood Linear Regression (MLLR) <ref type="bibr" target="#b6">[6]</ref> and the maximum posterior probability criterion to overcome the problem of parameter adaptation in the HMM training process. Furthermore, the idea of merging states was used to achieve decision tree state tying when there are many training parameters in the case of less training data <ref type="bibr" target="#b7">[7]</ref>. Artificial Neural Network (ANN) also provided a new research idea for speech recognition afterwards. In 2006, Hinton et al. <ref type="bibr" target="#b8">[8]</ref> used Restricted Boltzmann Machine (RBM) to initialize the nodes of the neural network and the Deep Belief Network (DBN) came into being. The network used a non-supervised greedy layer-by-layer method to keep the weight of the modeled object as much as possible, and continuously fitted to obtain the weight. Since then, the combination of deep learning and traditional methods has occupied the mainstream, and Deep Neural Network (DNN) has shown a trend of surpassing the GMM model, instead of using the traditional GMM method to HMM state modeling. The first breakthrough was the DNN-HMM acoustic model, which greatly promoted the application of deep learning in speech recognition <ref type="bibr" target="#b9">[9]</ref>. This is enough to demonstrate the power of deep learning.</p><p>Most importantly, the powerful feature extraction ability of Convolutional Neural Network (CNN) can better understand complex speech features. Recurrent Neural Network (RNN), which is suitable for sequence modeling, can make better use of the characteristics of time series relationships to establish context-dependent models. In particular, the latest end-to-end speech recognition system overcomes the problem of forced alignment of traditional HMM and realizes the overall optimization of sentence sequence. The two most mainstream end-to-end models are the Connectionist Temporal Classification (CTC) and the encoder-decoder model based on attention mechanism.</p><p>Deep learning uses the multi-layer nonlinear structure to transform the low-level features into more abstract highlevel features, and transforms the input features with or without supervision, thereby improving the accuracy of classification or prediction <ref type="bibr" target="#b10">[10]</ref>. Deep learning models generally refer to deeper structural models, which have more layers of nonlinear transformations than traditional shallow models. They are more powerful in expression and modeling <ref type="bibr" target="#b11">[11]</ref>. They also have advantages in complex signal processing such as non-stationary and random speech signals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. LOW-RESOURCE SPEECH RECOGNITION ACOUSTIC MODELS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. ACOUSTIC MODELS WITH NEURAL NETWORKS</head><p>In the current speech recognition system based on neural networks, the common hybrid DNN acoustic model has been gradually replaced by more accurate RNN or CNN. These are the two best options for effectively using variable-length contextual information <ref type="bibr" target="#b12">[12]</ref>. This section introduces some applications and improvements of these two structures in low-resource speech recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) RECURRENT NEURAL NETWORK</head><p>RNN has the ability to remember and have strong modeling capabilities in time series data learning, which solves the problem of not modeling the dynamic characteristics of speech in DNN-HMM and has become the most widely used neural network structure in the field of ASR. In fact, if the memory window of the basic RNN is too long, there will be problems with unstable training, gradient disappearance or explosion, and it is difficult to deal with the problem of long-term dependence. Therefore, Long Short-Term Memory (LSTM) structure <ref type="bibr" target="#b13">[13]</ref> is now commonly used to replace traditional RNN. LSTM is considered to be a complex and delicate network unit which have memory function so that it can store information for a long time. The LSTM structure which can selectively remember historical information contains three types of gates: input gate, forget gate and output gate. The input gate decides when to let the input into the cell unit, the forget gate decides when to remember the memory of the previous denotes the weight matrix between the input gate and the input layer. Where b denotes the bias matrix. Where ? denotes the sigmoid and ? denotes the neuron activation function.</p><p>The method based on RNN has shown excellent performance due to its powerful modeling capability and deeper architecture. For example, deep acoustic model with five layers of LSTM proposed by Google has achieved impressive improvements for large vocabulary speech recognition task <ref type="bibr" target="#b14">[14]</ref>. It is well known that the depth of neural networks is critical for acoustic modeling. However, the stack of multilayer LSTMs in low-resource scenario makes the model more difficult to train, because the performance tends to saturate and decline with the increase of depth. The improvement of ASR performance by LSTM structure with residual learning was studied in <ref type="bibr" target="#b15">[15]</ref>, which introduced cross-layer quick connection in multilayer LSTMs instead of simply stacking layers. These shortcut connections represented feature mapping between shallow and high layers. Not only could they ensure the information flow forward across several layers, but error could pass back across several layers without attenuation. Zhou et al. <ref type="bibr" target="#b16">[16]</ref> further proved the effectiveness of the Shared Hidden Layer (SHL) LSTMs with residual learning for multilingual low-resource speech recognition, which alleviated the degradation problem without adding additional parameters and computational complexity.</p><p>Actually, in order to make full use of the subsequent context information, most of speech recognition systems adopt the BLSTM (Bidirectional LSTM) structure, which is composed of two unidirectional LSTMs superimposed each other. Its output is determined jointly by the state of these two LSTMs and can provide the output layer with complete past and future context information. The calculation processes are as follows: 1 ( , )</p><formula xml:id="formula_0">t t t h LSTM x h ?? - =<label>(8) 1</label></formula><p>( , )</p><formula xml:id="formula_1">t t t h LSTM x h ?? + =<label>(9)</label></formula><p>Where [ , ]</p><formula xml:id="formula_2">t t t h h h =</formula><p>. BLSTM processes data in both directions by using two separate parameter sets, forward parameters and backward parameters.</p><p>Graves <ref type="bibr" target="#b17">[17]</ref> first tried to use BLSTM for acoustic modeling of speech recognition and achieved the best recognition performance at that time on the TIMIT corpus. Subsequently, many researchers have studied the lowresource speech acoustic modeling of BLSTM <ref type="bibr" target="#b18">[18]</ref>- <ref type="bibr" target="#b21">[21]</ref>. Although the BLSTM has achieved good results, the structure has a large number of parameters and some complex training mechanisms. In order to solve this problem, several layers of Bidirectional Gated Recurrent Unit (BGRU) can be added to the model, which are used in combination with BLSTM and usually does not completely replace BLSTM <ref type="bibr" target="#b22">[22]</ref>. Therein GRU <ref type="bibr" target="#b23">[23]</ref> can be understood as a simplified version of LSTM, which not only retains the long-term memory function of LSTM, but also replaces the input gate, forget gate, and output gate in LSTM with update gate and reset gate. In addition, GRU combines the two vectors of cellular state and output. Better expressiveness is shown in low-resource scenario on the condition that GRU has fewer parameters. Kang et al. <ref type="bibr" target="#b24">[24]</ref> proposed local BGRU with residual learning. All timedependency relationships were considered in a fixed local window.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) CONVOLUTIONAL NEURAL NETWORK</head><p>The spectral characteristics of speech signal can be regarded as an image with two dimensions of time and frequency information. Therefore, the extensive application of CNN <ref type="bibr" target="#b25">[25]</ref> in the image field also provides ideas for speech processing. For example, the pronunciation of each person is very different, and the frequency band of the formant is different on the spectrogram. CNN can effectively remove this difference, which is conducive to acoustic modeling in low-resource scenario. Typical CNN is usually divided into two parts: convolutional filter and max-pooling. The convolutional filter captures the local structural characteristics, and each node of its feature map is only convolved with f nodes of the local frequency band of the previous layer. Local convolution has two advantages: (1) the clean spectrum can be used to calculate the characteristics with excellent performance, and only a few of the characteristics will be affected by the noise part, so the robustness of the model is improved; (2) the higher layers of the network combine the calculated values of each frequency band to balance the speech information of adjacent frequency bands. After the convolution operation, max-pooling is conducted to provide additional translation and rotation invariance <ref type="bibr" target="#b26">[26]</ref>.</p><p>In the acoustic model based on CNN, the input feature vectors are divided into N non-overlapping frequency bands { | 0, , 1} Max-pooling follows the convolution operation to achieve the purpose of dimensionality reduction of hidden nodes. The formula is as follows:</p><p>, ,( +1 k-1 max</p><formula xml:id="formula_3">mm ij j i k i ph ? ? ? = )<label>(11)</label></formula><p>where m i p stands for the output of the max-pooling operation, i and j are the indexes of the bands, m is the index of the neurons in a band and k is the pooling size.</p><p>Time Delay Neural Network (TDNN) is the first proposed simple one-dimension CNN applied to speech recognition tasks without pooling or subsampling. It can be used to efficiently build long term time-dependency relationships whether in small or large data scenarios. TDNN has been shown to be effective in learning the temporal dynamics information of signals even from short term feature representations <ref type="bibr" target="#b27">[27]</ref>. The TDNN system for multilingual training was further established in <ref type="bibr" target="#b28">[28]</ref>- <ref type="bibr" target="#b30">[30]</ref>. Moreover, chain model training can significantly improve the speed and Word Error Ratio (WER), which is characterized by Lattice-Free Maximum Mutual Information (LF-MMI) as the training criterion without frame level cross entropy pre-training.</p><p>Initially, CNN was only used as a tool for robust feature extraction, so generally only one or two layers of layers were added at the bottom and then the upper layers were modeled using other neural network structures. For example, a CNN layer was added at the lowest of the hybrid neural network and HMM model on a small vocabulary task to normalize the spectral change of speech signal <ref type="bibr" target="#b31">[31]</ref>. Two-layer CNNs were used for high-dimensional speech feature extraction for low-resource speech recognition. Compared with MFCC features, their frequency domain energy changes are smaller, which is beneficial to the learning of high-level networks <ref type="bibr" target="#b32">[32]</ref>. Then inspired by VGGNet <ref type="bibr" target="#b33">[33]</ref>, a very deep convolutional network architecture with up to 14 weight layers was applied to low-resource speech recognition in <ref type="bibr" target="#b34">[34]</ref>. A deep structure based on Gated Convolutional Network (GCN) was proposed in <ref type="bibr" target="#b35">[35]</ref>, <ref type="bibr" target="#b36">[36]</ref>. GCN is more suitable for the combination of gate mechanism and convolutional operation of sequential tasks. It can better learn the acoustic feature representation, in which gates are used to control the information passed in the hierarchy. It combines the advantages of RNN and CNN on low-resource task to improve training speed and robustness.</p><p>However, the problem of gradient disappearance and overfitting in the optimization process of CNN are still two factors that affect the performance of the model. Convolutional Maxout Neural Network (CMNN) which uses maxout neuron and dropout training is an effective way to solve this problem <ref type="bibr" target="#b37">[37]</ref>, <ref type="bibr" target="#b38">[38]</ref>. The nonlinear function of the original network is changed from sigmoid to maxout. It selects the maximum value for the output of neuron nodes at adjacent positions within the same frequency band, making the model easy to optimize. Dropout discards the neurons in the network with a certain probability during each training, reducing the network parameters to be adjusted to prevent overfitting.</p><p>Compared to computer vision, the behavior of the two dimensions of time and spectrum of speech signals may be quite different. Therefore, the two-dimension convolution structure was proposed in <ref type="bibr" target="#b39">[39]</ref>, which emphasized the importance of considering both time and spectrum in a convolutional filter. The experimental result showed that the two-dimension convolution network was superior to the fully connected DNN in only 10 hours of training data. Over time, spectral convolution became much less important.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. ACOUSTIC MODELS WITH END-TO-END STRUCTURE</head><p>Deep learning algorithms still play a limited role in speech recognition systems in the form of traditional pipeline. The end-to-end model integrates multiple modules in traditional speech recognition such as acoustic model, pronunciation lexicon, and language model into a network for joint training <ref type="bibr" target="#b40">[40]</ref>. The end-to-end model realizes the direct mapping of the input sound sequence to the label sequence without carefully designing the intermediate state, which greatly simplifies the training process and significantly reduces the calculation complexity. This integration reduces the dependence on prior expert knowledge and avoids the obstacle that the traditional speech recognition framework cannot overcome for the scarcity of effective data, so it is also applied in the low-resource speech recognition. End-to-end learning allows us to process a wide variety of sounds, including noisy environments, accents, and different languages <ref type="bibr" target="#b12">[12]</ref>, <ref type="bibr" target="#b41">[41]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) CONNECTIONIST TEMPORAL CLASSIFICATION</head><p>In ASR task, the length of feature sequence of input speech frame is usually greater than that of output label sequence, and the corresponding label is required for each speech frame for effective training. Hence, the speech needs to be preprocessed with frame-by-frame alignment marks before training, which requires to be iterated repeatedly. The proposal of CTC perfectly solves this problem <ref type="bibr" target="#b42">[42]</ref>. It focuses on whether the output is consistent with the label as a whole. It can be trained without the frame level alignment of the label in time and does not care about the prediction of the input data at any time. The output of CTC is the probability of the overall sequence, thereby reducing the tedious work of forcing alignment to get frame-level annotations. In addition, CTC adds an additional blank label to the target label set and uses the blank label to indicate the probability of not issuing any labels at a specific time step.</p><p>Given an input sequence The set of all paths ? of length T is recorded as B -1 (y). Then the CTC needs to perform many-to-one, long-to-short mapping to aggregate multiple paths into a shorter label sequence. The same labels that appear continuously in each path are merged into one and the blank labels are removed. The probability of the target label sequence is calculated as follows:</p><formula xml:id="formula_4">1 () ( | ) ( | ) By P y x P x ? ? - ? = ?<label>(13)</label></formula><p>The loss function of CTC is defined as the sum of negative logarithmic probabilities of the correct label. This means minimizing the following objective function: Where ( , )</p><p>x y D ? denotes training samples. The essence of CTC is a special loss function or optimization criterion for sequence modeling. Its introduction effectively solves the classification problem of time series data. The combination of deep neural network and CTC makes deep learning more fully applied in speech recognition and becomes a hot spot in end-to-end speech recognition research. The output unit of CTC is very flexible, that can be phonemes, glyphs, syllables and other sub-word units, or even whole words <ref type="bibr" target="#b22">[22]</ref>.</p><p>Rosenberg et al. <ref type="bibr" target="#b43">[43]</ref> explored the use of CTC in keyword search and speech recognition in low-resource languages, which did not exceed the result obtained by DNN-HMM but was also competitive. The result indicated the direction for the improvement of the end-to-end speech recognition system. The encoder in CTC model has great potential for improvement. A method of using segmentation to correct CTC loss was proposed in the training process, resulted in improvement while decoding with small beam size <ref type="bibr" target="#b22">[22]</ref>. Vydana et al. <ref type="bibr" target="#b19">[19]</ref> studied the Subspace Gaussian Mixture Model (SGMM) and the joint acoustic model based on RNN-CTC. Experimental results showed that the joint acoustic model trained with RNN-CTC performed better than the SGMM system on 120-hour Indian language data. Wang et al. <ref type="bibr" target="#b44">[44]</ref> combined CTC with Tibetan linguistics knowledge and used bound triphones as a modeling unit to solve the problem of Tibetan acoustic modeling under resource constraints, which made the recognition rate based on the end-to-end acoustic model method exceed the speech recognition system based on BLSTM-HMM. Yu et al. <ref type="bibr" target="#b21">[21]</ref>, <ref type="bibr" target="#b32">[32]</ref> used the BLSTM-CTC joint model to achieve phonemic level speech recognition for a few hours of data.</p><p>However, CTC excludes the case where the output sequence is larger than the input sequence and makes independent assumptions between each time frame, without modeling the interdependence between outputs and ignoring the correlation between frames. RNN-Transducer (RNN-T) combines acoustics and language modeling on the basis of CTC model by adding a prediction network <ref type="bibr" target="#b45">[45]</ref>. RNN-T regards the acoustic model as an encoder, the language model as a prediction network, and the joint network as a decoder. This model has been proven to be effective in speech recognition tasks <ref type="bibr" target="#b46">[46]</ref>, <ref type="bibr" target="#b47">[47]</ref>. But RNN-T is more difficult to train, and it is necessary to pre-training the encoder and the prediction network separately to obtain better result. It has not been well applied in low-resource speech recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) SEQUENCE-TO-SEQUENCE MODELS</head><p>Attention-based model is an end-to-end model of encoderdecoder. The attention mechanism eliminates the need for pre-segment alignment of data and can be used with implicitly learn the soft alignment between input and output sequences, avoiding the conditional independence hypothesis problem in CTC <ref type="bibr" target="#b48">[48]</ref>. The encoder in attentionbased model converts the entire speech input sequence x to the high-level hidden vector sequence</p><formula xml:id="formula_5">1 { , , } L h h h =</formula><p>, and then the decoder uses the attention mechanism to select or assign different weights to the vectors in the hidden vector sequence h in each step of generating output label y , so that the most relevant vector is used for prediction.</p><p>Chorowski et al. <ref type="bibr" target="#b49">[49]</ref> first used attention mechanism for the alignment of input and output sequences in speech recognition task. The encoder was bidirectional RNN. The decoder was the RNN that directly emitted the phoneme stream and sent each symbol based on the context created by a subset of input symbols selected using the attention mechanism. Later in <ref type="bibr" target="#b43">[43]</ref> this model was used for lowresource speech recognition, but the encoder and decoder structure were replaced by GRU. This structure focuses the entire encoding sequence, which must wait until the encoding process is completely completed, thus increasing the delay. The introduction of the Listen, attention and Spell (LAS) <ref type="bibr" target="#b50">[50]</ref> model solved this problem. The listener is a pyramid BLSTM that encodes the input sequence x into high-level feature sequence h . The speller is an attention- based decoder that generates characters y from h . The training difficulty and efficiency of the model can be optimized by scheduled sampling <ref type="bibr" target="#b51">[51]</ref>, label smoothing <ref type="bibr" target="#b52">[52]</ref> , and minimum word error rate training <ref type="bibr" target="#b53">[53]</ref>. Moreover, joint training of attention and CTC can greatly improve the convergence of the model <ref type="bibr" target="#b54">[54]</ref>.</p><p>Transformer is a special encoder-decoder structure that avoids recursion and convolution and completely relies on the attention mechanism to describe the global dependency between input and output <ref type="bibr" target="#b55">[55]</ref>. There are 6 layers in the encoder part, each of which contains two sublayers of multi-head mechanism and position-wise fully connected feed-forward network. A residual connection is used between two sublayers, and then layer normalization is used. Decoder has the same structure as encoder, with the difference that each layer contains three sublayers including two multi-head attention mechanisms and a fully connected layer. The first multi-head attention uses mask operation, and the second multi-head attention focuses on the encoding information of encoder. The transformer structure requires a fixed input length. If a sequence is shorter than this fixed length, the padding should be applied to the blank part behind. The function of mask operation is to keep the padding part out of the attention calculation and ensure that the predicted position can only depend on the position of less than known output. In addition, positional encoding is added to input at the bottoms of these encoder and decoder stacks, which contributes to getting some information about the relative or absolute location of tokens in the sequence. The advantage of this change in the internal structure of encoder and decoder is that the model can parallelize training.</p><p>Transformer was originally used for Neural Machine Translation (NMT) tasks, both training speed and results far surpass other algorithms in <ref type="bibr" target="#b55">[55]</ref>. Transformer was also proved to have good performer for other Natural Language Processing (NLP) tasks <ref type="bibr" target="#b56">[56]</ref>. In the premise of the characteristic of the Transformer completing sequence-tosequence transduction task, people began to try to introduce it into the ASR task.</p><p>ASR Transformer was proposed in <ref type="bibr" target="#b57">[57]</ref>, <ref type="bibr" target="#b58">[58]</ref>. Its structure is basically the same as NMT Transformer. Only a linear transformation with layer normalization is added with the purpose of transforming the log-Mel filterbank feature into a dimension that matches the model input. Zhou et al. <ref type="bibr" target="#b59">[59]</ref> studied using ASR Transformer to complete multilingual speech recognition on 6 low-resource languages. Another speech-Transformer model was put forward by <ref type="bibr" target="#b60">[60]</ref>, which took a two-dimension spectrogram with time axis and frequency axis as input and added two 3 ? 3 CNN layers of 2 steps and M optional additional modules to the front of the encoder. Considering that the combination of time axis and frequency axis might be helpful in modeling of the temporal and spectral dynamics in a spectrogram, a two-dimension attention mechanism was proposed as an additional module to capture temporal and spectral dependencies. Mohamed et al. <ref type="bibr" target="#b61">[61]</ref>, <ref type="bibr" target="#b62">[62]</ref> also combined the convolutional layer with the Transformer, but the positional encoding was eliminated. A two-dimension convolutional block with layer normalization and ReLU and a two-dimension max pooling layer were added at the bottom of encoder of the basic Transformer, also a onedimension convolutional block with layer normalization and ReLU was added at the bottom of decoder <ref type="bibr" target="#b61">[61]</ref>. The addition of convolutional layer in Transformer can better learn the long range acoustic characteristics of speech.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. HOW TO LEARN WITH LOW-RESOURCE LANGUAGES</head><p>In addition to the improvement of the basic acoustic model units mentioned above, some important technologies are applied in low-resource speech recognition, which is often the key to improving the performance, mainly including two aspects of data and model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A?DATA AUGMENTATION</head><p>The amount of data directly affects the performance of deep learning. Deep learning on small data sets is prone to overfitting. Usually how to solve this problem from the data level is considered in the first place. However, collecting additional resources can be difficult for languages that are not widely used. Data augmentation, a technique designed to increase the amount of data needed to train speech recognition systems, has become a widely adopted approach in the field of low-resource speech recognition. Common data augmentation methods include semisupervised training, multi-lingual processing, acoustic data perturbation, and speech synthesis <ref type="bibr" target="#b63">[63]</ref>. Furthermore, multiple data augmentation methods are used in combination instead of a single method in many studies. For instance, semi-supervised training and acoustic data perturbation were combined in <ref type="bibr" target="#b63">[63]</ref>, <ref type="bibr" target="#b64">[64]</ref>. Data from other languages were additionally used in <ref type="bibr" target="#b64">[64]</ref>. Acoustic data perturbation and speech synthesis were combined in <ref type="bibr" target="#b65">[65]</ref>, resulting in a 14.8% relative WER improvement. Semisupervised training refers to train the model with supervised data and unsupervised data through the confidence threshold. Multilingual processing refers to expand the resource-poor data with resource-rich data. The data obtained by these two data augmentation methods are natural. The two methods of acoustic data perturbation and speech synthesis can obtain artificial data, that is, interfere with the raw data in some way and generate the new data by speech synthesis technology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) ACOUSTIC DATA PERTURBATION</head><p>Acoustic data perturbation includes multiple types of data interference to achieve the purpose of expansion, such as speed and volume perturbation, noise injection, increasing reverberation, and so on. This method is easy to implement and has been widely used in low-resource speech recognition tasks, which effectively improves the robustness of the acoustic model. But the main disadvantage of such data is bad quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vocal Tract Length Perturbation (VTLP) augments speech data by random linear distortion along the frequency dimension on the spectrogram. Different from Vocal Tract</head><p>Length Normalization (VTLN) <ref type="bibr" target="#b66">[66]</ref>, it generates a random warp factor ? for each utterance to warp the frequency axis and map the frequency to a new value instead of setting a warp factor for each training and testing speaker <ref type="bibr" target="#b67">[67]</ref>. It laid the foundation for increasing data sets in the field of speech recognition without changing labels. It was later applied to several low-resource speech recognition tasks <ref type="bibr" target="#b63">[63]</ref>, <ref type="bibr" target="#b68">[68]</ref>- <ref type="bibr" target="#b71">[71]</ref>. Speed perturbation created two counterparts of the original training data by modifying the speed to 0.9 and 1.1 of the original rates. Tempo perturbation was additionally used to correct the rhythm of signal, in ensuring signal at the premise of pitch and spectrum unchanged. Due to the change of the signal length, the GMM-HMM system was used to realign the data after the speed perturbation <ref type="bibr" target="#b70">[70]</ref>. Gokay et al. <ref type="bibr" target="#b65">[65]</ref> used speed perturbation, volume perturbation and a combination of the two for data augmentation. Kanda et al. <ref type="bibr" target="#b72">[72]</ref> studied three distortion methods of vocal tract length distortion, speech rate distortion, and frequency-axis random distortion. In a large vocabulary continuous speech recognition task with only 10 hours training sample, the relative WER with DNN-HMM training was reduced by 10.1%. Hsiao et al. <ref type="bibr" target="#b73">[73]</ref> improved the robustness of speech recognition system by artificially adding noise and reverberation. Hartmann et al. <ref type="bibr" target="#b69">[69]</ref> used fMLLR transformation of random speakers to enhance bottleneck features. This approach combined with adding noise and speed perturbation.</p><p>Inspired by data augmentation in the image field, Google proposed an augmentation strategy for the log-Mel spectrum to help the expansion of speech recognition data called SpecAugment <ref type="bibr" target="#b74">[74]</ref>. Its augmentation strategy includes distortion in the time direction and adding masking blocks to the frequency channel and time step. SpecAugment converts ASR from an over-fitting to an under-fitting problem. However, it can get better performance by using a larger network and longer training time. Wang et al. <ref type="bibr" target="#b62">[62]</ref> continued to improve in SpecAugment and proposed a semantic mask based on regularization to train the end-to-end speech recognition model. It shielded all features corresponding to the output token during training, such as a word or a word fragment. The motivation is to encourage the model to fill in missing marks based on context information with fewer acoustic features, so that the model has stronger language modeling capabilities and stronger resistance to acoustic distortion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) SPEECH SYNTHESIS</head><p>Compared with the acoustic data perturbation method, speech synthesis is more flexible. A generation model is often used to obtain new speech data, which is similar to using the Generative Adversarial Networks (GAN) architecture <ref type="bibr" target="#b75">[75]</ref> in the image field to generate synthetic new images that simulate the distribution of input data.</p><p>Voice Conversion (VC) is a technology that converts non-verbal information of a given voice while retaining language information. Kaneko et al. <ref type="bibr" target="#b76">[76]</ref> proposed a nonparallel VC method that did not rely on parallel data called CycleGAN-VC. It used a Cycle-consistent Generative Adversarial Network (CycleGAN) with gated CNN and an identity-mapping loss. CycleGAN used both adversarial and cycle-consistent loss to learn forward and inverse mapping <ref type="bibr" target="#b77">[77]</ref>. This made it possible to find the best pseudo pair from unpaired data. Gated CNN trained using identitymapping loss allowed mapping functions to capture order and hierarchy while retaining linguistic information.</p><p>Because it only learned one-to-one mapping. Kameoka et al. <ref type="bibr" target="#b78">[78]</ref> proposed to use StarGAN <ref type="bibr" target="#b79">[79]</ref> for non-parallel manyto-many VC. This method was an extension to CycleGAN-VC. It introduced a domain classifier to predict which class an input belongs to. Since StarGAN-VC only requires a few minutes of non-parallel and unmarked speech for each speaker, the architecture is very suitable for low-resource VC. Hsu et al. <ref type="bibr" target="#b80">[80]</ref> proposed a non-parallel Variable Autoencoding Wasserstein Generative Adversarial Network (VAW-GAN) VC framework. The Variational Autoencoder (VAE) <ref type="bibr" target="#b81">[81]</ref> simulated the speech characteristics of each speaker and the Wasserstein Generative Adversarial Network (W-GAN) <ref type="bibr" target="#b82">[82]</ref> synthesized speech from different speakers. Thai et al. <ref type="bibr" target="#b83">[83]</ref> used StarGAN-VC and VAW-GAN to perform VC in the Seneca language of 720 minutes. The experimental results showed that data augmentation helped to reduce the WER.</p><p>Another method of Speech synthesis is Text-to-Speech (TTS). Two TTS methods were used as strategies for Speech data augmentation in <ref type="bibr" target="#b65">[65]</ref>, namely the Google Translate Text to Speech (gTTS) and Deep Convolutional TTS (DCTTS) architectures <ref type="bibr" target="#b84">[84]</ref>. About 10 hours of Turkish speech data were synthesized ultimately. In order to solve the problem that synthesized speech can only provide limited speaker diversity for data augmentation in low-resource tasks, Du et al. <ref type="bibr" target="#b85">[85]</ref> proposed a speaker augmentation method that used VAE speaker representation to train the end-to-end TTS system so that TTS could synthesize sounds from unknown new speakers by sampling from the training potential distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B?TRANSFER KNOWLEDGE FROM MODELS</head><p>When deep learning is used for small data, superior performance cannot be achieved generally for a single target task. At this point, the researchers come up with the idea that additional tasks could be learned to improve the performance of network learning. This idea is also widely used in the field of low-resource speech recognition, which provides an effective way to solve the problem of data sparsity. It mainly includes multitask learning, transfer learning, and meta learning. The intuitive comparison of the three is shown in Table <ref type="table" target="#tab_1">1</ref>.</p><p>This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. For monolingual multitask learning in ASR, it is usually required to find tasks related to the language of the main task without additional resources. They can be regarded as abstract phonetic categories and these category labels are used as auxiliary tasks for frame-level classification <ref type="bibr" target="#b86">[86]</ref>- <ref type="bibr" target="#b89">[89]</ref>. For instance, triphone modeling and trigrapheme modeling are highly related learning tasks that can be estimated in parallel under the multitask learning framework <ref type="bibr" target="#b90">[90]</ref>. Chen et al. <ref type="bibr" target="#b91">[91]</ref> took grapheme modeling as an additional learning task and used multi-task learning DNN to learn the phone model of the target language. Fantaye et al. <ref type="bibr" target="#b92">[92]</ref> studied to conduct joint training through multi-task learning for basic acoustic units of a lowresource language Amharic, including syllable, phone and rounded phone.</p><p>Multilingual multitask ASR usually embodies each language as a separate task and builds a model by bringing together multiple related languages. Taking multilingual correlation as a prerequisite, the structure that the input layer and the hidden layer are shared by all languages and the output layer is not shared is usually adopted, that is, the multilingual model of SHL model <ref type="bibr" target="#b16">[16]</ref>, <ref type="bibr" target="#b93">[93]</ref>- <ref type="bibr" target="#b96">[96]</ref>. These hidden layers are shared among different languages, so that the SHL encodes rich senones information that can be used to identify different languages and also makes the input features more discriminative to different languages after layer by layer abstraction <ref type="bibr" target="#b97">[97]</ref>- <ref type="bibr" target="#b99">[99]</ref>. Multilingual multitask ASR derives a Multilingual Bottleneck Features (MBNF) in order to better help acoustic modeling of low-resource languages in a multilingual environment. Unlike the SHL multilingual model, the MBNF-based model contains a bottleneck layer with only a few nodes. And it is usually a linear layer to retain as much multilingual information as possible <ref type="bibr" target="#b100">[100]</ref>- <ref type="bibr" target="#b104">[104]</ref>. The MBNF-based model is shown in the Figure <ref type="figure" target="#fig_7">3</ref>. However, some unnecessary language-specific information may be included in MBNF-based model. In order to ensure that the SHL can learn language-invariant features, adversarial training is introduced in multilingual learning. the language discriminator is added to the model to identify the language labels of each frame using shared features <ref type="bibr" target="#b105">[105]</ref>- <ref type="bibr" target="#b107">[107]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) TRANSFER LEARNING</head><p>Transfer learning uses the similarities among data, tasks, or models to quickly and effectively develop a system with better performance for a new domain by using the knowledge learned from the source domain. Unlike multitask learning which focuses on improving the performance of all task, transfer learning emphasizes the improvement of the performance of the target task by transferring knowledge acquired on similar but different tasks. In transfer learning, the source domain and the target domain should be similar to each other in order to transfer knowledge smoothly. Transfer learning in ASR task is embodied in cross-linguistic acoustic modeling, aiming to transfer knowledge from one or more source language systems built with large amounts of training data to establish a target language system that provides only a limited amount of transcribed audio <ref type="bibr" target="#b108">[108]</ref>, <ref type="bibr" target="#b109">[109]</ref>. Transfer learning is used in two ways for low-resource speech recognition.</p><p>The first method showed as the left of the Figure <ref type="figure">4</ref> is fine-tuning, a process of initializing the weight of network with the pre-training network weight instead of the original random initialization, and then readjustment for the task in the target domain. Because the pre-training model is usually not fully applicable to the target domain, fine-tuning is necessary. This method is suitable for tasks with high similarity between the source and target domains. The implementation of fine-tuning method is relatively simple. The input and hidden layers of the network remain common to the two domains, the model parameters are borrowed and then a new softmax output layer is created for the target low-resource language <ref type="bibr" target="#b30">[30]</ref>. The goal of the output layer is to obtain the posterior information from the monolingual model of low-resource language. The second transfer learning method is weight-based transfer showed as the right of the Figure <ref type="figure">4</ref>, which is realized by transferring the hidden layers. First, an acoustic model is trained using high resource language, retaining the n hidden layers. Then the m randomly initialized hidden layers and a softmax layer are added on top of the n hidden layers. Finally, the transferred model is retrained using target low-resource language. In this case, the pre-training model can be regarded as a feature extractor <ref type="bibr" target="#b32">[32]</ref>. In practice, the pre-training model can be used in combination with multilingual training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) META LEARNING</head><p>Meta learning is also called learning to learn. It acquires experience from previous learning in a systematic and datadriven manner to speed up the learning process of new tasks. Generally, the following steps are required for Meta Learning: (1) metadata describing the previous learning task and the previous learning model needs to be collected, which is the algorithm configuration used to train the model, including hyperparameter settings, pipeline compositions and network architectures, the resulting model evaluations, such as accuracy and training time, the learned model parameters, such as the trained weights of a neural net, as well as measurable properties of the task itself; (2) we need to learn from the previous metadata to extract and transfer knowledge that guides the best model search for the new task <ref type="bibr" target="#b110">[110]</ref>. Different from transfer learning, meta learning makes use of previous knowledge to make the model selfadjust and optimize according to new tasks.</p><p>Meta learning explores how to quickly adapt to unseen data. As meta learning has been widely used in the field of computer vision under the few-shot learning setting <ref type="bibr" target="#b111">[111]</ref>, <ref type="bibr" target="#b112">[112]</ref>. Preliminary attempts have been made in language and speech processing in low-resource scenarios and good results have been achieved, such as machine translation <ref type="bibr" target="#b113">[113]</ref>[114], dialogue generation <ref type="bibr" target="#b114">[114]</ref> and speaker adaptation <ref type="bibr" target="#b115">[115]</ref>. Klejch et al. <ref type="bibr" target="#b115">[115]</ref> studied adapting a speaker-independent model in unseen speaker conditions using limited adaptation data, which could be regarded as a special case of few-shot learning. In this paper, an acoustic model weight adaptive method based on meta learning was proposed. Treating speaker adaptation as a function adapt, its set of parameters ? uses adaptive data to adjust a set of weights ? of the acoustic model ( , )</p><p>fx? into a set of adaptive weights * ? . The 3-hour data divided into 18 speakers was used to train the meta-learner, and finally achieved a lower WER in DNN and TDNN models. Hsu et al. <ref type="bibr" target="#b95">[95]</ref> proposed MetaASR, which was learned from six source tasks through the Model-Agnostic Meta Learning (MAML) algorithm to obtain a good initialization parameter of the shared encoder that performs quick finetuning for four target tasks. The results showed that MetaASR performed better than MultiASR in only four target languages directly. Meta learning is the key to the realization of general artificial intelligence. Although it is not widely used in the field of low-resource speech recognition, it is a direction worth further study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. PROJECTS FOR LOW-RESOURCE LANGUAGES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. IARPA BABEL PROGRAM</head><p>Many speech transcription systems were originally developed for English and have significantly lower performance in non-English language. There is no existing speech technology for those minority languages. And it often takes years to develop and cover a small portion of the world's languages. The effective triage capabilities to assist those few analysts must be rapidly developed. The goal of The IARPA Babel program <ref type="bibr" target="#b116">[116]</ref> is to develop agile and robust speech processing technology that can quickly adapt to any human language, so as to provide effective searching ability for analysts and process a mass of realword recorded speech. The Babel's program worked with diverse languages from the outset and acquired speech data in-country for languages from a broad set of language families, such as Afro-Asiatic, Niger-Congo, Sino-Tibetan, Austronesian, Dravidian, Altaic. In Babel program, data from more than twenty low-resource languages are collected, which allows us to focus on multilingual experiments for feature extraction and acoustic modeling. The detailed information of 23 languages is shown in Table <ref type="table" target="#tab_3">2</ref>. They are available on Linguistic Data Consortium (LDC) website at present <ref type="bibr" target="#b117">[117]</ref>. Audio data is presented as 8kHz 8bit a-law encoded audio in sphere format and 48kHz 24-bit PCM encoded audio in wav format. Transcripts are encoded in UTF-8 in Latin script. Transcripts are included for approximately 80% of the speech. The gender distribution among speakers is approximately equal. speakers' ages range from 16 years to 70 years. Calls were made using different telephones from a variety of environments including the street, a home or office, a public place, and inside a vehicle. This allows us to deal with real recording conditions from the start.</p><p>Since 2013, the National Institute of Standards and Technology (NIST) has held an international keyword search (OpenKWS) evaluation every year. This evaluation is part of the Babel program. The goal of NIST OpenKWS evaluation is to establish a speech recognition system with limited training resources and perform keyword searh tasks within a limited time. In each evaluation, a surprise language is released whose language information is unknown beforehand. The primary measure of performance for NIST OpenKWS is Actual Term-Weight Value (ATWV) <ref type="bibr">[118]</ref>. Its calculation formula is as follows: Where ? is the uniform threshold used to determine whether each possible keyword was a real keyword. K is the number of different keywords. ? is a constant, which is used to punish false alarms rate of the system. The higher the value of ATWV, the better the performance.</p><p>The OpenKWS evaluations for the Babel Program have base period, option period 1, option period 2 and option 3. The performance goals for every period is shown in Table <ref type="table" target="#tab_4">3</ref>.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FIGURE 1 .</head><label>1</label><figDesc>FIGURE 1. Statistics on published papers according to year retrieved in Web of Science, Engineering Village and Scopus with the subject terms of low resource and speech recognition. The horizontal axis represents the year. The vertical axis represents the quantity.</figDesc><graphic url="image-3.png" coords="2,36.95,360.00,213.00,138.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Where W is text label sequence, O is feature vector, () PW is the probability of language model, and ( | ) P O W is the probability of acoustic model. In other words, on the premise of a given text label sequence W , the probability of acoustic feature sequence O is obtained, which can measure the matching degree of the speech feature sequence O and the text label sequence W to construct the model. ( | ) P W O represents the posterior probability of W ,which can be used as the output text sequence of speech recognition with the idea of posterior probability maximization. For a given label sequence, the feature vector is fixed, which does not affect the recognition result, so it can be ignored. The acoustic model plays the most important role in ASR, which is the focus in this paper. The most popular ASR systems in recent years usually use spectrogram, Linear Prediction Coefficient (LPC), Mel Frequency Cepstrum Coefficient (MFCC), Perceptual Linear Predictive (PLP) and feature transformation methods</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>FIGURE 2 .</head><label>2</label><figDesc>FIGURE 2. The traditional architecture of speech recognition system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>band groups pass through weights matrix W , bias vector b and nonlinear transformation function ? of the convolution layer to produce the output i h . The calculation process is as follows: This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2020.3020421, IEEE Access During the convolution operation, the weight matrix W and the bias vector b shift d frequency bands every time, thereby generating a set of convolutional layer frequency band outputs The number s of frequency bands input by the convolutional layer is called band width, and the number d of frequency bands shifted by the convolutional layer every time is called band shift.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>1 {= 1 {</head><label>11</label><figDesc>of length N , we define a set of target labels () n y ? ? ? and an extended set of CTC target output labels * {} ? = ? -. The CTC first maps x to the path length T , t ? ??. The conditional probability of any path ? is calculated as follows:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2020.3020421, IEEE Access VOLUME XX, 2017 9</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>FIGURE 3 .</head><label>3</label><figDesc>FIGURE 3. Illustration of the model based on multilingual bottleneck features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>? respectively represent the number of missed detection and false alarms of keyword kw for ? .()True Nkw is the number of reference occurrences of keyword kw . T stands for the size of test speech corpus.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1 . Comparison of multitask learning, transfer learning, and meta learning.</head><label>1</label><figDesc>Citation information: DOI 10.1109/ACCESS.2020.3020421, IEEE Access</figDesc><table><row><cell>VOLUME XX, 2017</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Illustration of two transfer learning methods for low- resource speech recognition.</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>Softmax layer</cell></row><row><cell>Softmax layer</cell><cell>Pre-training model</cell><cell></cell></row><row><cell>Language 2</cell><cell>Language 1</cell><cell>Language 2</cell></row><row><cell>FIGURE 4.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 . Presentation of language data in IARPA Babel program.</head><label>2</label><figDesc></figDesc><table><row><cell>Language</cell><cell>Release ID</cell><cell>Hours</cell><cell>Regions</cell></row><row><cell>Turkish</cell><cell>105b-v0.5</cell><cell>213</cell><cell>spoken in seven dialect regions in Turkey.</cell></row><row><cell>Tagalog</cell><cell>106-v0.2g</cell><cell>213</cell><cell>North, Central and South dialect regions in Philippines.</cell></row><row><cell>Pashto</cell><cell>104b-v0.4bY</cell><cell>244</cell><cell>spoken in four dialect regions of Afghanistan and Pakistan.</cell></row><row><cell>Georgian</cell><cell>404b-v1.0a</cell><cell>190</cell><cell>Eastern and Western dialect regions in Georgia.</cell></row><row><cell>Cantoness</cell><cell>101b-v0.4c</cell><cell>215</cell><cell>Chinese provinces of Guangdong and Guangxi, and within those provinces, among five dialect groups.</cell></row><row><cell>Bengali</cell><cell>103b-v0.4b</cell><cell>215</cell><cell>spoken in India by native speakers of Bengali born in India.</cell></row><row><cell>Assamese</cell><cell>102b-v0.5a</cell><cell>205</cell><cell>three dialects spoken in Assam, a state in northeastern India.</cell></row><row><cell>Zulu</cell><cell>206b-v0.1e</cell><cell>211</cell><cell>KwaZulu-Natal-urban dialect region of South Africa.</cell></row><row><cell>Vietnamese</cell><cell>107b-v0.7</cell><cell>201</cell><cell>North, North-Central, Central and Southern dialect regions in Vietnam.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 . The performance goals for the OpenKWS evaluation.</head><label>3</label><figDesc></figDesc><table><row><cell>Period</cell><cell>Base</cell><cell>Option 1</cell><cell>Option 2</cell><cell>Option 3</cell></row><row><cell>Transcribed (%)</cell><cell>100%</cell><cell>?75%</cell><cell>?50%</cell><cell>?50%</cell></row><row><cell>Pronunciation Lexicon</cell><cell>100%</cell><cell>?75%</cell><cell>?50%</cell><cell>?50%</cell></row><row><cell>Channels</cell><cell>telephone</cell><cell>telephone and non-telephone</cell><cell>telephone and non-telephone</cell><cell>telephone and non-telephone</cell></row><row><cell>Languages Investigated Development + Surprise</cell><cell>4+1</cell><cell>5+1</cell><cell>6+1</cell><cell>7+1</cell></row><row><cell>Build Time for Surprise</cell><cell>4 weeks</cell><cell>3 weeks</cell><cell>2 weeks</cell><cell>1 weeks</cell></row><row><cell>ATWV</cell><cell>0.3</cell><cell>0.3</cell><cell>0.3</cell><cell>0.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 . The descriptions of the three tasks of LoReHLT 2019.</head><label>4</label><figDesc>Identify named mentions in both the ILs and in English, classify them into predefined entity types, link the mentions to a knowledge base or cluster them if they are not linkable to the knowledge base.</figDesc><table><row><cell>Task</cell><cell>Language</cell><cell>Input</cell><cell>Objective</cell></row><row><cell>MT</cell><cell>IL11, IL12</cell><cell>Text</cell><cell>Translate IL text documents to English.</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Generate situation frames including sentiment emotion about</cell></row><row><cell>SF</cell><cell>IL12, English</cell><cell>Text and Audio</cell><cell>the frame, link those situation frames into knowledge base</cell></row><row><cell></cell><cell></cell><cell></cell><cell>level situations.</cell></row><row><cell>EDL</cell><cell>IL11, IL12, English</cell><cell>Text</cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div><p>This work was supported in part by <rs type="funder">Ministry of Education Humanities and Social Sciences Research Planning Fund Project</rs> under Grant <rs type="grantNumber">16YJAZH072</rs>, in part by <rs type="funder">Major projects of the National Social Science Fund</rs> under Grant <rs type="grantNumber">14ZDB156</rs>, and in part by the <rs type="funder">Graduate Student Research Capacity Improvement Program of Beijing Technology and Business University</rs> in 2020.</p></div>
			</div>
			<div type="funding">
<div><p>This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: <rs type="grantNumber">DOI 10.1109/ACCESS.2020.3020421</rs>, IEEE Access This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: <rs type="grantNumber">DOI 10.1109/ACCESS.2020.3020421</rs>, IEEE Access This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: <rs type="grantNumber">DOI 10.1109/ACCESS.2020.3020421</rs>, IEEE Access This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: <rs type="grantNumber">DOI 10.1109/ACCESS.2020.3020421</rs>, IEEE Access This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: <rs type="grantNumber">DOI 10.1109/ACCESS.2020.3020421</rs>, IEEE Access This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: <rs type="grantNumber">DOI 10.1109/ACCESS.2020.3020421</rs>, IEEE Access</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_yUgCdDA">
					<idno type="grant-number">16YJAZH072</idno>
				</org>
				<org type="funding" xml:id="_erxJrPR">
					<idno type="grant-number">14ZDB156</idno>
				</org>
				<org type="funding" xml:id="_Yra6Su7">
					<idno type="grant-number">DOI 10.1109/ACCESS.2020.3020421</idno>
				</org>
				<org type="funding" xml:id="_6zSmkKP">
					<idno type="grant-number">DOI 10.1109/ACCESS.2020.3020421</idno>
				</org>
				<org type="funding" xml:id="_JBkBkqJ">
					<idno type="grant-number">DOI 10.1109/ACCESS.2020.3020421</idno>
				</org>
				<org type="funding" xml:id="_V2jfwGZ">
					<idno type="grant-number">DOI 10.1109/ACCESS.2020.3020421</idno>
				</org>
				<org type="funding" xml:id="_fZDPfkB">
					<idno type="grant-number">DOI 10.1109/ACCESS.2020.3020421</idno>
				</org>
				<org type="funding" xml:id="_R8GZRUx">
					<idno type="grant-number">DOI 10.1109/ACCESS.2020.3020421</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Author Name: Preparation of Papers for IEEE Access (February 2017) VOLUME XX, <ref type="bibr">2017</ref> 9</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B?DARPA LORELEI PROGRAM</head><p>Humanitarian Assistance Disaster Relief (HADR) crises can occur anywhere in the world. People in need often post real-time information online, and stakeholders involved in crisis management can use this information and analyze them. For example, understanding the comprehensive emotions of the affected population in a particular area may help inform decision makers on how to best allocate resources for effective disaster relief. However, these works can be severely limited by language barriers. The DARPA Low-Resource Languages for Emergent Incidents (LORELEI) program further developed language processing techniques for low-resource languages in the context of this humanitarian crisis. The goal of LORELEI program is to solve the problem that the existing methods cannot achieve the popularization of language technology through the research and development of language technology. It can eliminate the current dependence on huge, manually-translated, manually-transcribed or manually-annotated corpora and use the relevant language resources to fully develop low-resource languages. In this program, rather than translating foreign language materials into English, information elements based on local language materials, including situational descriptions, names, places, events, emotions and relationships, are identified <ref type="bibr" target="#b118">[119]</ref>.</p><p>The exploitation of LORELEI program can be divided into three stages. The first is the language analysis stage. In order to reduce the dependence on specific language information, it is necessary to analyze the common attributes and rules of the language from the known language data to establish a universal language technology model. The known resource-rich language information is mapped to the resource-poor language to establish a projection hypothesis relationship. Then the optimization algorithm for language-specific resource is also studied. The second stage is the language technology development stage. Driven by the knowledge fusion engines, run-time models are built and language processing tools are developed by combining the knowledge of linguistic experts and scenario information. Third, to make it easier for analysts to use and analyze event-related data, the LORELEI program creates web service that integrates the Incident Language (IL) tools. Analysts can use these tools to convert low-resource ILs into English summary or other visual forms by accessing web services. The web service is constantly updated and improved as the data increases <ref type="bibr" target="#b119">[120]</ref>.</p><p>Unlike the Babel which focuses on speech processing, LORELEI focuses on situational awareness when emergencies occur, with an emphasis on text processing in low-resource languages. LDC is building text language packs for LORELEI, which includes data, annotations, NLP tools, lexicons and grammatical resources for 23 representative languages (Uzbek, Turkish, Hausa, Amharic, Arabic, Farsi, Hungarian, Mandarin, Russian, Somali, Spanish, Vietnamese, Yoruba, Akan, Bengali, Hindi, Indonesian, Swahili, Tagalog, Tamil, Thai, Wolof, Zulu) and 12 ILs (Uzbek, Mandarin, Oromo and other undisclosed languages) <ref type="bibr" target="#b120">[121]</ref>. Representative languages packs which contain monolingual text, parallel text, annotation, text processing tools, segmentation, entity tagging, lexicons and gramma are selected to provide broad typological coverage. while ILs are selected to evaluate system performance on a language whose identity is disclosed at the start of the evaluation. There are two tools in the language pack, one to recreate original source data from the processed XML material and the other to condition text data users download from Twitter. Data were collected in discussion forums, news, reference, social network and weblog. All text data is encoded as UTF-8.</p><p>Low Resource Human Languages Technologies (LoReHLT) evaluation is designed in collaboration with NIST and LORELEI program. LoReHLT 2019 [122] included three tasks, Machine Translation (MT), Situation Frame (SF), and Entity Detection and Linking (EDL). The descriptions of the three tasks are shown in Table <ref type="table">4</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>In recent years, with the rise of deep learning, ASR technology has made great progress. Significant results have been achieved in low-resource scenarios. Data augmentation, multilingual and cross-lingual training have become the most widely used. But speech recognition systems still need to be designed with more sophisticated models to deal with speakers with accents or with higher levels of background noise. Low-resource speech recognition may also have the following improvement or breakthrough in the future. First, the improvement of end-to-end system. The integration of additional language knowledge, learning with complex data such as noise and so on still need to be studied. The joint modeling of acoustic model and language model should be strengthened to better explore the correlation and complementarity between acoustics and language, so as to build a more thorough end-to-end speech recognition system to improve performance.</p><p>Second, excavating speech structure knowledge of multimodal information fusion. We can expand from a single mode only for speech to related modes such as images and videos. Because in the era of high popularity of multimedia technology, such data is easy to obtain and relatively easy to label. This is an important research direction for lowresource speech recognition with data scarcity. Her research interests include big data processing, intelligent terminal software design.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Swahili 202b-v1.0d 350 Nairobi dialect region of Kenya. Lao 203b-v3.1a 207 Vientiane dialect region in Laos. Kurmanji Kurdish 205b-v1.0a 203 southeastern and eastern Anatolian regions of Turkey. Haitian Creole 201b-v0.2b 203 Northern, Western and Southern dialect regions in Haiti. Tok Pisin 207b-v1.0e 200 Papuan dialect region of Papua New Guinea</title>
		<imprint/>
	</monogr>
	<note>Telugu 303b-v1.0a 201 Central, East, South and North Telugu dialect regions of India. Kazakh 302b-v1.0a 203 Northeastern and Southern dialect regions of Kazakhstan. Cebuano 301b-v2.0b 191 Cebu-North Kana, Sialo, and Mindanao dialect regions of Philippines. Lithuanian 304b-v1.0b 210 Auk?taitian and Samogitian dialect regions of Lithuania. Igbo 306b-v2.0c</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Amharic 307b-v1.0b 204 Addis Ababa, Shewa, and Gondar dialect regions of Ethiopia. Dholuo 403b-v1.0b 204 South Nyanza and Trans-Yala dialect regions of Kenya. REFERENCES</title>
		<author>
			<persName><forename type="first">Onitsha</forename><surname>Owerri</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Ngwa dialects spoken in Nigeria. Guarani 305b-v1.0c 198 Paraguay</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatic speech recognition for under-resourced languages: a survey</title>
		<author>
			<persName><forename type="first">L</forename><surname>Besacier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Barnard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schultz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="85" to="100" />
			<date type="published" when="2014-01">Jan. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Research on the application of national languages from an international perspective</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Minority Translators Journal</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="11" to="17" />
			<date type="published" when="2020-02">Feb. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A tutorial on hidden markov models and selected applications in speech recognition</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Rabiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="257" to="286" />
			<date type="published" when="1989-02">Feb. 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Language independent and language adaptive acoustic modeling for speech recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="31" to="51" />
			<date type="published" when="2001-08">Aug. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">MLLR transforms as features in speaker recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Stolcke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ferrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kajarekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shriberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Venkataraman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="2425" to="2428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Robust decision tree state tying for continuous speech recognition</title>
		<author>
			<persName><forename type="first">W</forename><surname>Reichl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Speech and Audio Processing</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="555" to="566" />
			<date type="published" when="2000-09">Sep. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006-07">Jul. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep belief networks for phone recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Picheny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="5060" to="5063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An overview of deep-structured learning for information processing</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. APSIPAASC</title>
		<meeting>APSIPAASC<address><addrLine>Xi&apos;an, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning deep architectures for AI</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Now Foundations and Trends</title>
		<imprint>
			<biblScope unit="page" from="1" to="127" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Recent progresses in deep learning based acoustic models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/CAA Journal of Automatica Sinica</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="396" to="409" />
			<date type="published" when="2017-07">Jul. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multilingual convolutional, long short-term memory, deep neural networks for low resource speech recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bukhari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Procedia Computer Science</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="842" to="847" />
			<date type="published" when="2017-04">Apr. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fast and accurate recurrent neural network acoustic models for speech recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Beaufays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH<address><addrLine>Dresden, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1468" to="1472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multidimensional residual learning based on recurrent neural networks for acoustic modeling</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH<address><addrLine>San Francisco, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3419" to="3423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multilingual recurrent neural networks with residual learning for low-resource speech recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="704" to="708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">BUT OpenSAT 2017 speech recognition system</title>
		<author>
			<persName><forename type="first">M</forename><surname>Karafiat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Baskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Szoke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Malenovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vesely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Grezl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cernocky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH<address><addrLine>Hyderabad, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2638" to="2642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An exploration towards joint acoustic modeling for Indian languages: IIIT-H submission for low resource speech recognition challenge for Indian languages</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K</forename><surname>Vydana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K</forename><surname>Gurugubelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">V V</forename><surname>Raju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Vuppala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH<address><addrLine>Hyderabad, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3192" to="3196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Analysis of multilingual BLSTM acoustic model on low and high resource languages</title>
		<author>
			<persName><forename type="first">M</forename><surname>Karafidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Baskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vesely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Grezl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cernocky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP<address><addrLine>Calgary, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5789" to="5793" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Research of endangered languages speech recognition based on dynamic BLSTM and CTC</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Application Research of Computers</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3334" to="3337" />
			<date type="published" when="2019-11">Nov. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Exploring end-to-end techniques for low-resource speech recognition</title>
		<author>
			<persName><forename type="first">V</forename><surname>Bataev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Korenevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Medennikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zatvornitskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPECOM</title>
		<meeting>SPECOM<address><addrLine>Leipzig, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="32" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<ptr target="https://arxiv.org/abs/1412.3555" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Advanced recurrent network-based hybrid acoustic models for low resource speech recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Johnson</surname></persName>
		</author>
		<idno type="DOI">10.1186/s13636-018-0128-6</idno>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Audio, Speech, and Music Processing</title>
		<imprint>
			<date type="published" when="2018-07">Jul. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Convolutional networks for images, speech, and time series</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997-11">Nov. 1997</date>
		</imprint>
	</monogr>
	<note>the Handbook of Brain Theory and Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Acoustic modeling approach of multi-stream feature incorporated convolutional neural network for low-resource speech recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer Applications</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2609" to="2615" />
			<date type="published" when="2016-09">Sep. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A time delay neural network architecture for efficient modeling of long temporal contexts</title>
		<author>
			<persName><forename type="first">V</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH<address><addrLine>Dresden, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3214" to="3218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">TDNN-based multilingual speech recognition system for low resource Indian languages</title>
		<author>
			<persName><forename type="first">N</forename><surname>Fathima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mahima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Iyengar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH<address><addrLine>Hyderabad, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3197" to="3201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Optimizing multilingual knowledge transfer for time-delay neural networks with low-rank factorization</title>
		<author>
			<persName><forename type="first">F</forename><surname>Keith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hartmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Siu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kimball</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP<address><addrLine>Calgary, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4924" to="4928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning from the best: a teacherstudent multilingual framework for low-resource languages</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bagchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hartmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP<address><addrLine>Brighton, United Kingdom</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6051" to="6055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Applying convolutional neural networks concepts to hybrid NN-HMM model for speech recognition</title>
		<author>
			<persName><forename type="first">O</forename><surname>Abdel-Hamid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Penn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP<address><addrLine>Kyoto, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="4277" to="4280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Crosslanguage end-to-end speech recognition research based on transfer learning for the low-resource Tujia language</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Symmetry</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">179</biblScope>
			<date type="published" when="2019-02">Feb. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR<address><addrLine>San Diego, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for LVCSR</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="8614" to="8618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Gated convolutional networks based hybrid acoustic models for low resource speech recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU</title>
		<meeting>ASRU<address><addrLine>Okinawa, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="157" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Advanced convolutional neural network-based hybrid acoustic models for low-resource speech recognition</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Fantaye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Hailu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">36</biblScope>
			<date type="published" when="2020-05">May. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Convolutional maxout neural networks for low-resource speech recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ISCSLP</title>
		<meeting>ISCSLP<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="133" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Research on CMN?based recognition of Kirgiz with less resources</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wushour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Reyiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Modern Electronics Technique</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">24</biblScope>
			<biblScope unit="page" from="132" to="136" />
			<date type="published" when="2018-12">Dec. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for acoustic modeling in low resource languages</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Lane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP<address><addrLine>Brisbane, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2056" to="2060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Deep Speech: scaling up end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.5567</idno>
		<ptr target="https://arxiv.org/abs/1412.5567" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">DeepSpeech2: end-to-end speech recognition in English and Mandarin</title>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Anubhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Casper</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.02595</idno>
		<ptr target="https://arxiv.org/abs/1512.02595" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Santiago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Faustino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jurgen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML<address><addrLine>Pittsburgh, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">End-to-end speech recognition and keyword search on low-resource languages</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Audhkhasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sethy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Picheny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP<address><addrLine>Los Angeles, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5280" to="5284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Towards end to end speech recognition system for Tibetan</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition and Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="359" to="364" />
			<date type="published" when="2017-04">Apr. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Sequence transduction with recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1211.3711</idno>
		<ptr target="https://arxiv.org/abs/1211.3711" />
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Exploring neural transducers for end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">E</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gaur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU</title>
		<meeting>ASRU<address><addrLine>Okinawa, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="206" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Exploring architectures, data and units for streaming end-to-end speech recognition with RNN-Transducer</title>
		<author>
			<persName><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU</title>
		<meeting>ASRU<address><addrLine>Okinawa, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="193" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">An Overview of End-to-End Automatic Speech Recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Symmetry</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2019-08">Aug. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">End-to-end continuous speech recognition using attention-based recurrent NN: first results</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.1602</idno>
		<ptr target="https://arxiv.org/abs/1412.1602" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Listen, Attend and Spell: A neural network for large vocabulary conversational speech recognition</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP<address><addrLine>Shanghai, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4960" to="4964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1171" to="1179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">B</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR<address><addrLine>Las Vegas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Minimum word error rate training for attentionbased sequence-to-sequence models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP<address><addrLine>Calgary, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4839" to="4843" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Joint CTC-attention based endto-end speech recognition using multi-task learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP<address><addrLine>Los Angeles, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4835" to="4839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5999" to="6009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">BERT: Pre-Training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<ptr target="https://arxiv.org/abs/1810.04805" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Syllable-based sequence-tosequence speech recognition with the Transformer in Mandarin Chinese</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH<address><addrLine>Hyderabad, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="791" to="795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A comparison of modeling units in sequence-to-sequence speech recognition with the Transformer on Mandarin Chinese</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICONIP</title>
		<meeting>ICONIP<address><addrLine>Siem Reap, Cambodia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="210" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Multilingual end-to-end speech recognition with a single Transformer on low-resource languages</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.05059</idno>
		<ptr target="https://arxiv.org/abs/1806.05059" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Speech-Transformer: A no-recurrence sequence-to-sequence model for speech recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP<address><addrLine>Calgary, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5884" to="5888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Transformers with Convolutional Context for ASR</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Okhonko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.11660</idno>
		<idno>arXiv: 1904.11660</idno>
		<ptr target="https://arxiv.org/abs/1904.11660" />
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Semantic mask for Transformer based end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.03010</idno>
		<ptr target="https://arxiv.org/abs/1912.03010" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Data augmentation for low resource languages</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ragni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Knill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Rath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J F</forename><surname>Gales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="810" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Speech recognition and keyword spotting for low-resource languages: Babel project research at CUED</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J F</forename><surname>Gales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Knill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ragni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Rath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SLTU</title>
		<meeting>SLTU<address><addrLine>St. Petersburg, Russia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="16" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Improving low resource Turkish speech recognition with data augmentation and TTS</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gokay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yalcin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SSD</title>
		<meeting>SSD<address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="357" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">A frequency warping approach for vocal tract length normalization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICSP</title>
		<meeting>ICSP<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="691" to="694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Vocal Tract Length Perturbation (VTLP) improves speech recognition</title>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML<address><addrLine>Atlanta, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Data augmentation, feature combination, and multilingual neural networks to improve ASR and KWS performance for low-resource languages</title>
		<author>
			<persName><forename type="first">Z</forename><surname>T?ske</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Golik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nolden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schl?ter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1420" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Two-Stage data augmentation for low-resourced speech recognition</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hartmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tsakalidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH<address><addrLine>San Francisco, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2378" to="2382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Audio augmentation for speech recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH<address><addrLine>Dresden, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3586" to="3589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Data augmentation for deep neural network acoustic modeling</title>
		<author>
			<persName><forename type="first">X</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1469" to="1477" />
			<date type="published" when="2015-09">Sep. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Elastic spectral distortion for low resource speech recognition with deep neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Obuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU</title>
		<meeting>ASRU<address><addrLine>Olomouc, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="309" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Robust speech recognition in unknown reverberant and noisy conditions</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hartmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Karafiat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU</title>
		<meeting>ASRU<address><addrLine>Scottsdale, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="533" to="538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">SpecAugment: a simple data augmentation method for automatic speech recognition</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH<address><addrLine>Graz, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2613" to="2617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Generative adversarial networks</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2661</idno>
		<ptr target="https://arxiv.org/abs/1406.2661" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Parallel-Data-Free Voice Conversion Using Cycle-Consistent Adversarial Networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kaneko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kameoka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.11293</idno>
		<ptr target="https://arxiv.org/abs/1711.11293" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10593</idno>
		<ptr target="https://arxiv.org/abs/1703.10593" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">StarGAN-VC: non-parallel many-to-many voice conversion using star generative adversarial networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kameoka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kaneko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hojo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SLT</title>
		<meeting>SLT<address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="266" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">StarGAN: unified generative adversarial networks for multi-domain image-toimage translation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR<address><addrLine>Salt Lake City, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8789" to="8797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Voice conversion from unaligned corpora using variational autoencoding wasserstein generative adversarial networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tsao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3364" to="3368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Auto-Encoding Variational Bayes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<ptr target="https://arxiv.org/abs/1312.6114" />
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Wasserstein GAN</title>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<ptr target="https://arxiv.org/abs/1701.07875" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Synthetic Data augmentation for improving low-resource ASR</title>
		<author>
			<persName><forename type="first">B</forename><surname>Thai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jimerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Arcoraci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Prud'hommeaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ptucha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WNYISPW</title>
		<meeting>WNYISPW<address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Efficiently trainable text-to-speech system based on deep convolutional networks with guided attention</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tachibana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Uenoyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Aihara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP<address><addrLine>Calgary, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4784" to="4788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Speaker augmentation for low resource speech recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7719" to="7723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Multi-task learning in deep neural networks for improved phoneme recognition</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Droppo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="6965" to="6969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Structured output layer with auxiliary targets for context-dependent acoustic modelling</title>
		<author>
			<persName><forename type="first">P</forename><surname>Swietojanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Renals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH<address><addrLine>Dresden, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3605" to="3609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Deep auto-encoder based multi-task learning using probabilistic transcriptions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vesel?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2073" to="2077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Automatic speech recognition using probabilistic transcriptions in Swahili, Amharic and Dinka</title>
		<author>
			<persName><forename type="first">P</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jyothi</surname></persName>
		</author>
		<author>
			<persName><surname>Hasegawa-Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH<address><addrLine>San Francisco, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3524" to="3528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Joint acoustic modeling of triphones and trigraphemes by multi-task learning deep neural networks for low-resource speech recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sivadas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="5592" to="5596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Multitask learning of deep neural networks for low-resource speech recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1172" to="1183" />
			<date type="published" when="2015-07">Jul. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Investigation of various hybrid acoustic modeling units via a multitask learning and deep neural network technique for LVCSR of the low-resource language: Amharic</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Fantaye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Hailu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="105593" to="105608" />
			<date type="published" when="2019-07">Jul. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Addressing data sparsity in DNN acoustic modeling</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tejaswi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Umesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NCC</title>
		<meeting>NCC<address><addrLine>Chennai, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Data-pooling and multitask learning for enhanced performance of speech recognition systems in multiple low resourced languages</title>
		<author>
			<persName><forename type="first">A</forename><surname>Madhavaraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Ramakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NCC</title>
		<meeting>NCC<address><addrLine>Bangalore, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Meta learning for end-to-end lowresource speech recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7844" to="7848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">A study of rank-constrained multilingual DNNs for low-resource ASR</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sahraeian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">V</forename><surname>Compernolle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP<address><addrLine>Shanghai, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5420" to="5424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Research on low-resource Mongolian speech recognition based on multilingual speech data selection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="308" to="313" />
			<date type="published" when="2018-09">Sep. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Cross-language knowledge transfer using multilingual deep neural network with shared hidden layers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP.2013.6639081</idno>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting><address><addrLine>Vancouver, BC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="7304" to="7308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title level="m" type="main">On the use of a multilingual neural network front-end</title>
		<author>
			<persName><forename type="first">S</forename><surname>Scanzio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Laface</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fissore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gemello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mana</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="2711" to="2714" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Efficient methods to train multilingual bottleneck feature extractors for low resource keyword search</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP<address><addrLine>Los Angeles, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5650" to="5654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Pairwise learning using multi-lingual bottleneck features for low-resource query-by-example spoken term detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP<address><addrLine>Los Angeles, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5645" to="5649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Feature exploration for almost zero-resource asr-free keyword spotting using a multilingual bottleneck extractor and correspondence autoencoders</title>
		<author>
			<persName><forename type="first">R</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kamper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Westhuizen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Quinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Niesler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH<address><addrLine>Graz, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3475" to="3479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">The language-independent bottleneck features</title>
		<author>
			<persName><forename type="first">K</forename><surname>Vesely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Karafiat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Grezl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Janda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Egorova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SLT</title>
		<meeting>SLT<address><addrLine>Miani, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="336" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Probabilistic and bottle-neck features for LVCSR of meetings</title>
		<author>
			<persName><forename type="first">F</forename><surname>Gr?ezl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Karafi At</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>?cernocky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP<address><addrLine>Honolulu, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="757" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Adversarial multilingual training for low-resource speech recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP<address><addrLine>Calgary, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4899" to="4903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Language-adversarial transfer learning for low-resource speech recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="621" to="630" />
			<date type="published" when="2019-03">Mar. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Language-invariant bottleneck features from adversarial end-to-end acoustic models for low resource speech recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP<address><addrLine>Brighton, United Kingdom</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6071" to="6075" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">A comparative study of BNF and DNN multilingual training on cross-lingual low-resource speech recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Chng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH<address><addrLine>Dresden, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2132" to="2136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Domain robust feature extraction for rapid low resource ASR development</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dalmia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SLT</title>
		<meeting>SLT<address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="258" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<monogr>
		<title level="m" type="main">Meta-learning: a survey</title>
		<author>
			<persName><forename type="first">J</forename><surname>Vanschoren</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.03548</idno>
		<ptr target="https://arxiv.org/abs/1810.03548" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
		<title level="m" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.05175</idno>
		<ptr target="https://arxiv.org/abs/1703.05175" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<monogr>
		<title level="m" type="main">Matching Networks for One Shot Learning</title>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04080v2.2014</idno>
		<idno>arXiv: 1412.1602</idno>
		<ptr target="https://arxiv.org/abs/1412.1602" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Meta-learning for low-resource neural machine translation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">O K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3622" to="3631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Meta-learning for lowresource natural language generation in task-oriented dialogue systems</title>
		<author>
			<persName><forename type="first">F</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Faltings</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCAI</title>
		<meeting>IJCAI<address><addrLine>Macao, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3131" to="3157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Learning to adapt: a metalearning approach for speaker adaptation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Klejch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fainberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH<address><addrLine>Hyderabad, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="867" to="871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<monogr>
		<title/>
		<author>
			<persName><surname>Babel</surname></persName>
		</author>
		<ptr target="https://www.iarpa.gov/index.php/research-programs/babel" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<monogr>
		<title level="m" type="main">Linguistic Data Consortium</title>
		<ptr target="https://www.ldc.upenn.edu" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">A system for the 2019 sentiment, emotion and cognitive state task of DARPA&apos;s LORELEI project</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">R</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Singla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACII</title>
		<meeting>ACII<address><addrLine>Cambridge, United Kingdom</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="448" to="453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Overview of the DARPA LORELEI program</title>
		<author>
			<persName><forename type="first">C</forename><surname>Caitlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Boyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Translation</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="3" to="9" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">LORELEI language packs: data, tools, and resources for technology development in low resource languages</title>
		<author>
			<persName><forename type="first">S</forename><surname>Strassel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tracey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. LREC</title>
		<meeting>LREC<address><addrLine>Portoro?, Slovenia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
