<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dynamic Prefix-Tuning for Generative Template-based Event Extraction</title>
				<funder ref="#_wXFfWNC #_ccMP32t">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-05-12">12 May 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
							<email>xiaoliu@bit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing Engineering Research Center of High Volume Language Information Processing and Cloud Computing Applications</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Key Laboratory of Intelligent Information Processing and Information Security</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Ministry of Industry and Information Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Institute of Technology</orgName>
								<orgName type="institution">Southeast Academy of Information Technology</orgName>
								<address>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Heyan</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing Engineering Research Center of High Volume Language Information Processing and Cloud Computing Applications</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Key Laboratory of Intelligent Information Processing and Information Security</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Ministry of Industry and Information Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Institute of Technology</orgName>
								<orgName type="institution">Southeast Academy of Information Technology</orgName>
								<address>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ge</forename><surname>Shi</surname></persName>
							<email>shige@bjut.edu.cn</email>
							<affiliation key="aff5">
								<orgName type="department">Faculty of Information Technology</orgName>
								<orgName type="institution">Beijing University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bo</forename><surname>Wang</surname></persName>
							<email>bwang@bit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing Engineering Research Center of High Volume Language Information Processing and Cloud Computing Applications</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Key Laboratory of Intelligent Information Processing and Information Security</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Ministry of Industry and Information Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Institute of Technology</orgName>
								<orgName type="institution">Southeast Academy of Information Technology</orgName>
								<address>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dynamic Prefix-Tuning for Generative Template-based Event Extraction</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-05-12">12 May 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2205.06166v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider event extraction in a generative manner with template-based conditional generation. Although there is a rising trend of casting the task of event extraction as a sequence generation problem with prompts, these generation-based methods have two significant challenges, including using suboptimal prompts and static event type information. In this paper, we propose a generative template-based event extraction method with dynamic prefix (GTEE-DYNPREF) by integrating context information with type-specific prefixes to learn a context-specific prefix for each context. Experimental results show that our model achieves competitive results with the state-of-the-art classification-based model ONEIE on ACE 2005 and achieves the best performances on ERE. Additionally, our model is proven to be portable to new types of events effectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Event extraction is an essential yet challenging task for natural language understanding. Given a piece of text, event extraction systems need to recognize event triggers with specific types and the event arguments with the correct roles in each event record according to an event ontology, which defines the event types and argument roles <ref type="bibr" target="#b4">(Doddington et al., 2004;</ref><ref type="bibr" target="#b0">Ahn, 2006)</ref>. As an example, the context in Figure <ref type="figure" target="#fig_0">1</ref> contains two event records, a Transport event triggered by "returned" and an Arrest-Jail event triggered by "capture". In the Transport event, the Artifact is "the man", the Destination is "Los Angeles" and the Origin is "Mexico". In the Arrest-Jail event, the Person is "the man", the Time is "Tuesday" and the Agent is "bounty hunters". In this work, we focus on the task setting of extracting events without gold entity annotations, which is more practical in real-world applications.</p><p>Most of the event extraction work treats the extraction of event triggers and event arguments as several classification tasks, either learned in a pipelined framework <ref type="bibr" target="#b9">(Ji and Grishman, 2008;</ref><ref type="bibr" target="#b17">Liu et al., 2020;</ref><ref type="bibr" target="#b5">Du and Cardie, 2020;</ref><ref type="bibr" target="#b12">Li et al., 2020)</ref> or a joint formulation <ref type="bibr" target="#b13">(Li et al., 2013;</ref><ref type="bibr" target="#b36">Yang and Mitchell, 2016;</ref><ref type="bibr" target="#b23">Nguyen et al., 2016;</ref><ref type="bibr" target="#b19">Liu et al., 2018;</ref><ref type="bibr" target="#b34">Wadden et al., 2019;</ref><ref type="bibr" target="#b16">Lin et al., 2020)</ref>.</p><p>There is a rising trend of casting the task of event extraction as a sequence generation problem by applying special decoding strategies <ref type="bibr" target="#b24">(Paolini et al., 2021;</ref><ref type="bibr" target="#b22">Lu et al., 2021)</ref> or steering pretrained language models to output conditional generation sequences with discrete prompts <ref type="bibr" target="#b14">(Li et al., 2021;</ref><ref type="bibr" target="#b9">Hsu et al., 2021)</ref>. Compared with classification-based methods, this line of work is more data-efficient and flexible, which requires less annotated data to achieve acceptable model performances, being easier to extend to new event types by slightly modifying the designed prompts and decoding strategies.</p><p>However, these generation-based methods have two significant challenges, which impede achieving competitive results with the classification-based methods. (1) suboptimal prompts: First, they manually design prompts for each event type <ref type="bibr" target="#b14">(Li et al., 2021;</ref><ref type="bibr" target="#b9">Hsu et al., 2021)</ref>, which are suboptimal without tuning and largely affect the model performances.</p><p>(2) static event type information: Second, when extracting events of a particular type, recent generation-based methods will receive the same event type information concerning only the running event type, regardless of the associations between other possible event types.</p><p>To alleviate the above two challenges, we propose a generative template-based event extraction method with dynamic prefixes, denoted as GTEE- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generation-based Event Extraction</head><p>The man returned to Los Angeles from Mexico following his capture Tuesday by bounty hunters.</p><p>Event type Transport.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Static Type Information</head><p>Event type Arrest-Jail.</p><p>The man returned to Los Angeles from Mexico following his capture Tuesday by bounty hunters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Static Type Information</head><p>...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Event type 2</head><p>Event type n</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GTEE-DynPref</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Extracting Transport event</head><p>Event type 1 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dynamic Type Information</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GTEE-DynPref</head><p>Event type 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dynamic Type Information</head><p>The man returned to Los Angeles from Mexico following his capture Tuesday by bounty hunters. DYNPREF. As demonstrated in Figure <ref type="figure" target="#fig_0">1</ref>, we follow the previous work <ref type="bibr" target="#b14">(Li et al., 2021;</ref><ref type="bibr" target="#b9">Hsu et al., 2021)</ref>, extracting event records one type by one type, using the pretrained encoder-decoder language model BART <ref type="bibr" target="#b11">(Lewis et al., 2020)</ref> for conditional generation. For each event type, we first initialize a typespecific prefix consisting of a sequence of tunable vectors as transformer history values <ref type="bibr" target="#b15">(Li and Liang, 2021)</ref>. The type-specific prefix offers tunable event type information for one single type. Then we integrate context information with all type-specific prefixes to learn a context-specific prefix, dynamically combining all possible event type information. We evaluate our model on two widely used event extraction benchmarks, ACE 2005 and ERE. Experimental results show that our model achieves competitive results with the state-of-theart classification-based model ONEIE on ACE 2005 and achieves the best performances on ERE. Additionally, according to the transfer learning results, our model also can be adapted to new types of events effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>This paper is related to the following lines of work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Classification-based Event Extraction</head><p>Event extraction is usually formulated as a sequence labeling classification problem <ref type="bibr" target="#b23">(Nguyen et al., 2016;</ref><ref type="bibr" target="#b35">Wang et al., 2019;</ref><ref type="bibr" target="#b37">Yang et al., 2019;</ref><ref type="bibr" target="#b34">Wadden et al., 2019;</ref><ref type="bibr" target="#b19">Liu et al., 2018)</ref>. Some of them incorporate global features and apply joint inference <ref type="bibr" target="#b16">(Lin et al., 2020;</ref><ref type="bibr" target="#b13">Li et al., 2013;</ref><ref type="bibr" target="#b36">Yang and Mitchell, 2016)</ref> to collectively model event dependencies. Additionally, recent work casts event extraction as a machine reading comprehension (MRC) problem <ref type="bibr" target="#b17">(Liu et al., 2020;</ref><ref type="bibr" target="#b5">Du and Cardie, 2020;</ref><ref type="bibr" target="#b12">Li et al., 2020)</ref> by constructing questions to query event triggers and arguments.</p><p>Our work treats event extraction as a conditional generation task, which is more flexible and portable, which reduces the burden of annotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Generation-based Event Extraction</head><p>There is a rising line of work casting event extraction as a sequence generation problem, such as transforming into translation tasks <ref type="bibr" target="#b24">(Paolini et al., 2021)</ref>, generating with constrained decoding methods <ref type="bibr" target="#b22">(Lu et al., 2021)</ref> and template-based conditional generation <ref type="bibr" target="#b14">(Li et al., 2021;</ref><ref type="bibr" target="#b9">Hsu et al., 2021)</ref>.</p><p>The two closest methods above <ref type="bibr" target="#b14">(Li et al., 2021;</ref><ref type="bibr" target="#b9">Hsu et al., 2021)</ref> both utilize manually designed discrete templates, which caused the sub-optimal problem. Besides, the applied static type instruction does not consider the connections between events within the same context. We replaced the static type instructions with the dynamic prefixes, which are continuous and tunable vectors during training, combining the manual event templates and alleviating the sub-optimal problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Prompt Tuning</head><p>There is a line of work using specific sentence templates with pre-trained models to solve natural language understanding tasks. It natural to come up with prefix-style <ref type="bibr" target="#b1">(Brown et al., 2020)</ref> or cloze-style <ref type="bibr" target="#b25">(Petroni et al., 2019)</ref> prompts based on human introspection, which are called "descrete prompts". Existing works on discrete prompt tuning <ref type="bibr" target="#b31">(Shin et al., 2020;</ref><ref type="bibr" target="#b6">Gao et al., 2021;</ref><ref type="bibr" target="#b27">Schick et al., 2020)</ref> depend on verbalizers to map from class labels to answer tokens. These methods are proven to be effective in the few-shot setting for text classification and conditional text generation tasks <ref type="bibr">(Schick and Sch?tze, 2021b,a,c)</ref>. There are also methods that explore continuous prompts directly operating in the embedding space of the model, like tuning on vectors <ref type="bibr" target="#b15">(Li and Liang, 2021;</ref><ref type="bibr" target="#b10">Lester et al.,</ref>  " for the type instruction, " " for the encoder-decoder language model, and "</p><p>" for the answered prompt as output. <ref type="bibr" target="#b32">Tsimpoukelli et al., 2021)</ref>, initializing with discrete prompts <ref type="bibr" target="#b39">(Zhong et al., 2021;</ref><ref type="bibr">Qin and Eisner, 2021;</ref><ref type="bibr" target="#b7">Hambardzumyan et al., 2021)</ref> and hybrid prompt tuning <ref type="bibr">(Liu et al., 2021b,a;</ref><ref type="bibr" target="#b8">Han et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Generative Template-based Method</head><p>We revisit the task of event extraction as the process of conditional generation and present our base model (GTEE-BASE) as illustrated in Figure <ref type="figure">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Statement</head><p>In the conditional generation task formulation for event extraction, the whole extraction process for a textual context is divided into several subtasks according to event types. Specifically, given an event ontology O with an event type set</p><formula xml:id="formula_0">E = {e i |i ? [1, |E|]}, the input in each subtask S e i ,C</formula><p>for event type e i consists of a context C and a designed prompt P e i . And the output is the answered prompts A e i , containing extracted event records.</p><p>We take one single conditional generation subtask S e i ,C for event type e i as example to explain the following content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Basic Architecture</head><p>As shown in Figure <ref type="figure">2</ref>, the conditional generation subtask is modeled by a pretrained encoder-decoder language model (LM), BART <ref type="bibr" target="#b11">(Lewis et al., 2020)</ref> and T5 <ref type="bibr" target="#b26">(Raffel et al., 2020)</ref>. In the generation process, the encoder-decoder LM models the conditional probability of selecting a new token y i given the previous tokens y &lt;i and the encoder input X . Therefore, the entire probability p(Y|X ) of generating the output sequence Y given the input sequence X is calculated as</p><formula xml:id="formula_1">p(Y|X ) = |Y| i=1 p(yi|y&lt;i, X ) X = [Pe i ; [SEP]; C] Y = Ae i (1)</formula><p>where [ ; ] denotes the sequence concatenation operation and [SEP]<ref type="foot" target="#foot_0">1</ref> is the corresponding separate marker in the applied encoder-decoder LM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Prompt Design</head><p>Similar to the state-of-the-art end-to-end generative method DEGREE-E2E <ref type="bibr" target="#b9">(Hsu et al., 2021)</ref> for event extraction, the prompt P e i for subtask S e i ,C in our base model GTEE-BASE contains the type instruction I e i and the template T e i .</p><p>Type Instruction. A short natural language sequence I e i describing the event type e i in the subtask. We use the pattern "Event type is <ref type="bibr">[MASK]</ref>." to construct type instructions for the event type set E. For example, the type instruction for event type Meet is "Event type is Meet.".</p><p>Template. A type-specific pattern T e i , which contains several placeholders, reflecting how the arguments participant in the event. We use two types of placeholdes, &lt;trg&gt; and &lt;arg&gt;s, for representing trigger and arguments, respectively. The template is consists of a trigger part and a argument part. The two parts are concatenated by a new seperate marker &lt;IN_SEP&gt;. As illstrated in Figure 2, the trigger part is "Trigger &lt;trg&gt;", which is identical for all event types. The argument part is specific to event type e i . Due to the manual efforts of designing and searching for an optimal template, we follow <ref type="bibr" target="#b14">Li et al. (2021)</ref> to reuse the pre-defined argument templates 2 in the ontology O. The original pre-defined argument templates natively contain numeric labels for each &lt;arg&gt; placeholder (as &lt;arg1&gt;) and the slot mappings M to the corresponding argument roles. We also follow <ref type="bibr" target="#b14">Li et al. (2021)</ref> to remove these numeric labels.</p><p>Ground Truth Construction. For each event type e i in the context C, we construct the ground truth sequence G e i ,C for conditional generation by filling the gold event records into the template T e i . If there is no event record of event type e i , the generation ground truth will be "Trigger &lt;trg&gt;". Otherwise, the event record is filled in the template T e i as the output in Figure <ref type="figure">2</ref>. If several arguments are categorized as the same role, these arguments are first sorted by spans and then concatenated by "and". If there are multiple event records, they will be sorted by the spans of the triggers, and the filled sequences will be concatenated by a new separate marker &lt;OUT_SEP&gt;.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training, Inference and Parsing</head><p>Training. The trainable parameters of our base model GTEE-BASE are only the encoder-decoder LM. And we use ? to denote all the trainable parameters. Therefore, the training target is to minimize the negative loglikelihood of all subtasks S e i ,C j in the training set D, where C j denotes the j-th context in D.</p><formula xml:id="formula_2">L ? (D) = - |D| j=1 |E| i=1 log p(Ge i ,C j |Xe i ,C j ) Xe i ,C j = [Pe i ; [SEP]; Cj] (2)</formula><p>Inference. In the inference stage, our base model generates sequences by beam search BEAM = 6. The maximum sequence length is set according to dataset statistics, which is a bit larger than the length of the longest ground truth.</p><p>Parsing. Basically, we parse the event records by template matching and slot mapping according to the ontology O. Please note that not all the generated output sequences are valid. For each generated sequence, we will first try to parse a trigger. If failed, we will skip the sequence. Then if we fail to match &lt;IN_SEP&gt; or the argument part of the template T e i , we will skip the argument parsing and only keep a trigger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Irrelevant Event Types</head><p>By investigating the parsed event records, we find that our model has the bias to generate event records even for irrelevant event types. This will be fatal when the input context does not contain any event record, which will largely hurt the precision score and F1 score. There are 80.28% and 71.02% sentences that do not contain any event records in ACE 2005 and ERE, respectively.</p><p>Therefore, we propose a simple yet effective solution to alleviate this problem by separately training an irrelevance classifier IC. With context C as input, we finetune a BERT mdoel <ref type="bibr" target="#b3">(Devlin et al., 2019)</ref> by feeding the encoded [CLS] vector to a MLP as a binary classifier to see whether the context contains any event records or is entirely irrelevant for the ontology O. It is worth noticing that there may exist other ways to avoid the problem, as <ref type="bibr" target="#b2">Cui et al. (2021)</ref> formulate the NER task as a ranking task to avoid irrelevant entity types in a similar conditional generation task setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Dynamic Prefix-Tuning</head><p>We propose dynamic prefix-tuning with taskspecific prefix and context-specific prefix to alleviate the two main challenges in generation-based event extraction. The framework of our model with dynamic prefix tuning, GTEE-DYNPREF, is shown in Figure <ref type="figure" target="#fig_1">3</ref>. We will introduce the dynamic prefix-tuning step by step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Type-Specific STATIC PREFIX</head><p>Inspired by PREFIX-TUNING <ref type="bibr" target="#b15">(Li and Liang, 2021)</ref>, we use event type-specific prefix STAPREF, which is a pair of two transformer activation sequences {sp, sp }, each containing L continuous D-dim vectors as the history values for the encoder and the decoder, respectively. From the view of the encoder and decoder input, in the subtask S e i ,C , the prefix is virtually prepended for the sequences X and Y in an encoder-decoder LM.</p><formula xml:id="formula_3">X = [sp e i ; X ] Y = [sp e i ; Y]<label>(3)</label></formula><p>The main advantage of these transformer activation sequences is that they provide trainable context for both encoder and decoder, which is also computationally achievable.</p><p>We first initialize a pair of task-specific prefixes {sp e i , sp e i } for each event type e i in the ontology O. In the conditional generation subtask S e i ,C , we then prepend the corresponding pair of taskspecific prefixes {sp e i , sp e i } as transformer activations for the encoder and decoder.  " for the context, " " for the template, " " for the type-specific prefixes, "</p><p>" for the dynamic prefix, " " for the encoder-decoder language model, and " " for the answered prompt as output.</p><p>Hariri submitted his resignation during a 10-minute meeting with the head of state at the Baabda presidential palace  Following <ref type="bibr" target="#b15">Li and Liang (2021)</ref>, we use a trainable embedding tensor P ? R |E|?L?D to model the type-specific prefix sp. For the event type e i in the ontology O, the prefix vector sp t e i at index t is</p><formula xml:id="formula_4">sp t e i = P [ei, t, :]<label>(4)</label></formula><p>The reason we call the task-specific prefix static is that for subtasks of the same event types, the output type instructions are the same. In other words, such prefixes only preserve context concerning one single type of event, ignoring the association between different event types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Context-Specific DYNAMIC PREFIX</head><p>Aiming to capture the associations between different event types when constructing trainable prefixes, we present DYNPREF, which constructs dynamic prefix with context-specific information when prompting pretrained language models.</p><p>As shown in Figure <ref type="figure" target="#fig_3">4</ref>, dp C has the same sequence length L as sp. For each position t, the prefix vector dp t C is computed by dynamically integrating all the prefix vector sp t e i of event type e i in the ontology O according to the context-specific information c by multi-head attention <ref type="bibr" target="#b33">(Vaswani et al., 2017)</ref>. To calculate the context-specific information c, we apply a BERT mdoel <ref type="bibr" target="#b3">(Devlin et al., 2019)</ref> as the context encoder by feeding the context C as input and taking the [CLS] vector as c. (5)</p><p>The context-specific prefix dp C is dynamic because it takes both the type-specific information in ontology O and the unique context information into account when steering LMs.</p><p>Following <ref type="bibr" target="#b15">Li and Liang (2021)</ref>, we compute the decoder transformer activation vector h i , which is a concatenation of all layers, at time step i in encoder-decoder LM recurrently.</p><formula xml:id="formula_5">hi = dp i C , if i &lt; L, LM(yi, h&lt;i|X ), otherwise. (<label>6</label></formula><formula xml:id="formula_6">)</formula><p>The computation of the encoder transformer activation vector is similar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training</head><p>Except for the LM parameters ?, the additional trainable parameters of DYNPREF include the embedding tensor P and the BERT encoder modeling context information. Specially, we follow the training suggestions <ref type="bibr" target="#b15">(Li and Liang, 2021)</ref> and reparametrize the embedding tensor P by modeling a MLP and another embedding tensor P ? R |E|?L?D with small dimension D &lt; D. In the end, P is computed as</p><formula xml:id="formula_7">P [ei, t, :] = MLP(P [ei, t, :])<label>(7)</label></formula><p>Now we use ? to denote all the introduced parameters for DYNPREF.</p><p>The training objective is still to minimize the negative loglikelihood in equation ( <ref type="formula">2</ref>) for ? and ?. However, in our preliminary experiments, we find that jointly learning the LM parameters ? and the DYNPREF parameters ? requires different scales of training hyperparameters, being difficult to learn the ability to extract event arguments. Therefore, we train them separately in three steps: (1) First, we train ? using GTEE-BASE to learn the task information.</p><p>(2) Then we fix the LM parameters ? and mask all other event types except for e i in each subtask S e i ,C , only optimizing ?, to learn the typespecific information for each event type. (3) Last, we remove the masking of event types, remaining the LM parameters fixed and only optimizing ? using DYNPREF, to capture the associations between related event types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiment Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>We conducted experiments on two widely used event extraction benchmarks, ACE 2005 (LDC2006T06) and ERE (LDC2015E29, LDC2015E68, and LDC2015E78). ACE 2005 dataset has 599 annotated English documents, 33 event types, and 22 argument roles. ERE contains 458 English documents, 38 event types, and 21 argument roles.</p><p>We preprocess the datasets following previous work <ref type="bibr" target="#b38">(Zhang et al., 2019;</ref><ref type="bibr" target="#b34">Wadden et al., 2019;</ref><ref type="bibr" target="#b5">Du and Cardie, 2020;</ref><ref type="bibr" target="#b16">Lin et al., 2020;</ref><ref type="bibr" target="#b22">Lu et al., 2021;</ref><ref type="bibr" target="#b9">Hsu et al., 2021)</ref>, and obtain three datasets, ACE05-E, ACE05-E + and ERE-EN. Statistics of the datasets are shown in Table <ref type="table" target="#tab_4">1</ref>. Compared to ACE05-E, both ACE05-E + and ERE-EN contain pronoun roles and multi-token event triggers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation Metrics</head><p>We use the same evaluation criteria in previous work <ref type="bibr" target="#b38">(Zhang et al., 2019;</ref><ref type="bibr" target="#b34">Wadden et al., 2019;</ref><ref type="bibr" target="#b16">Lin et al., 2020;</ref><ref type="bibr" target="#b22">Lu et al., 2021;</ref><ref type="bibr" target="#b9">Hsu et al., 2021)</ref> and report the Precision P , Recall R and F1 score F 1 of trigger classification (Trg-C) and argument classification (Arg-C). ? Trg-C: a trigger is correctly classified if its offset and event type matches the ground truth. ? Arg-C: an argument is correctly classified if its offset, event type and role label all matches the ground truth. Following <ref type="bibr" target="#b22">Lu et al. (2021)</ref>, we also obtain the offset of extracted triggers by string matching in the input context one by one. For the predicted argument, we find the nearest matched string to the predicted trigger as the predicted offset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Baseline Methods</head><p>We compare GTEE-DYNPREF with two groups of event extraction work. The first group is about classification-based event extraction methods.</p><p>? DYGIE++ <ref type="bibr" target="#b34">(Wadden et al., 2019)</ref>: a BERTbased model which captures both withinsentence and cross-sentence context. ? GAIL <ref type="bibr" target="#b38">(Zhang et al., 2019)</ref>: an RL model jointly extracting entity and event.</p><p>? ONEIE <ref type="bibr" target="#b16">(Lin et al., 2020)</ref>: an end-to-end IE system which employs global feature and beam search, which is the state-of-the-art. ? BERT_QA <ref type="bibr" target="#b5">(Du and Cardie, 2020)</ref>: a MRCbased model using multi-turns of separated QA pairs to extract triggers and arguments. ? MQAEE <ref type="bibr" target="#b12">(Li et al., 2020)</ref>: a multi-turn question answering system. The second group contains generation-based event extraction methods.</p><p>? TANL <ref type="bibr" target="#b24">(Paolini et al., 2021)</ref>: a method use translation tasks modeling event extraction in a trigger-argument pipeline. ? BART-GEN <ref type="bibr" target="#b14">(Li et al., 2021)</ref>: a templatebased conditional generation method. ? TEXT2EVENT <ref type="bibr" target="#b22">(Lu et al., 2021)</ref>: a sequenceto-structure generation method. ? DEGREE-E2E <ref type="bibr" target="#b9">(Hsu et al., 2021)</ref>: an endto-end conditional genration method with discrete prompts. Table <ref type="table">3</ref>: Results on ACE05-E for event extraction in the supervised learning setting. The first group of baselines is the classification-based methods and the second group is the generation-based methods. Our proposed GTEE-DYNPREF is also the generation-based method.</p><p>For each group, we bold the highest F1 scores for Trg-C and Arg-C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Implementation Details</head><p>We use the huggingface implementation of BARTlarge as the encoder-decoder LM and BERT-large as the binary irrelevance classifier IC in ?3.5 and the context encoder in ?4.2. We optimized our models by AdamW <ref type="bibr" target="#b21">(Loshchilov and Hutter, 2019)</ref>.</p><p>The hyperparameters we used are shown in Table <ref type="table" target="#tab_5">2</ref>. Each experiment is conducted on NVIDIA A100 Tensor Core GPU 40GB. For simplicity, we randomly initialize<ref type="foot" target="#foot_2">3</ref> the embedding tensor P . As mentioned in ?3.5, there is an overwhelming amount of negative samples compared with positive samples. Therefore, we sample only 4% negative samples in the train and dev split for the three datasets, keeping all samples in test split.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Supervised Learning Setting</head><p>We evaluate the proposed model GTEE-DYNPREF under the supervised learning setting. Table <ref type="table">3</ref> shows the comparison results on ACE05-E against all baseline methods, and Table <ref type="table" target="#tab_6">4</ref> illustrates the results compared with the state-of-the-art in each research line on ACE05-E + and ERE-EN.</p><p>New state-of-the-art. As we can see from Table 3, GTEE-DYNPREF achieves the highest F1 scores for Trg-C and Arg-C on ACE05-E, compared with all the generation-based baselines. Besides, GTEE-DYNPREF is competitive with the state-of-the-art classification-based method ONEIE, outperforming the others. In Table <ref type="table" target="#tab_6">4</ref>, GTEE-DYNPREF achieves competitive Arg-C F1 score with ONEIE on ACE05-E + , while obtaining 7.5% and 4.6% gain of F1 scores for Trg-C and Arg-C, respectively, achieving new state-of-the-art on ERE-EN.</p><p>Trainable prompts boost the performances. Compared with DEGREE, the event extraction method using fixed templates, and TEXT2EVENT, the generative event extraction method without prompts, GTEE-DYNPREF outperforms them in all the datasets, showing the effectiveness of the trainable dynamic prefix with prompts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Transfer Learning Setting</head><p>GTEE-DYNPREF utilizes the event type templates and optimize them with context-specific information in the dynamic prefix, which is easy to extend to a new type of event. Therefore, aiming to verify the ability of GTEE-DYNPREF to learn from new event types, we conduct experiments under the transfer learning setting following <ref type="bibr" target="#b22">Lu et al. (2021)</ref>. Specifically, we divide the event mentions whose context contains no less than eight tokens in ACE05-E + into two subsets, denoted by src and tgt. src contains top-10 frequent types of events and tgt contains the rest 23 types of events. We then randomly split each subset into a train set and a test set with the ratio 4 : 1. Specifically, for transfer learning, we will first pre-train on src-train to learn the task information and then fine-tune on tgt-train for extracting the new types of events. Table <ref type="table" target="#tab_8">6</ref> shows the evaluation results on tgt-test under the transfering learning setting and when solely training on tgt-train without transfering knowledge. We choose the state-of-the-art classification-based model ONEIE and generation-based method TEXT2EVENT as the baselines.</p><p>We can see that GTEE-DYNPREF achieves the highest Trg-C F1 and Arg-C F1 scores, which indicates that with the help of dynamic prefix, GTEE-DYNPREF can be adopted to new types of events more effectively. Additionally, comparing with solely training on tgt, transfering the knowledge  from src allows GTEE-DYNPREF to achieve higher F1 scores than ONEIE and TEXT2EVENT. The reason may be that ONEIE relies on multi-task annotated information, and TEXT2EVENT requires learning the structural information of new types of events. In contrast, GTEE-DYNPREF only requires an easy-to-acquire template, which can be further optimized during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Ablation Study</head><p>In this section, we study the effectiveness of each proposed module by adding them into our base model GTEE-BASE and finally get our final model GTEE-DYNPREF. The results on ACE05-E, ACE05-E + and ERE-EN are presented in Table <ref type="table" target="#tab_7">5</ref>.</p><p>Continuous Prompt vs Discrete Prompt. We first compare GTEE-STAPREF with GTEE-BASE.</p><p>Based on GTEE-BASE with discrete prompts, GTEE-STAPREF further combines type-specific prefixes as to form continuous prompts. It can be observed that there is a 0.8%, 0.7% and 0.9% gain for the Trg-C F1 score on ACE05-E, ACE05-E + and ERE-EN, respectively. Additionally, there is a 0.6%, 1.0% and 0.8% improvement for the Arg-C F1 score, demonstrating the effectiveness and flexibility of STAPREF to model the type-specific information.</p><p>Dynamic Prefix vs Static Prefix. Next we compare GTEE-DYNPREF with GTEE-STAPREF to study the advantages of constructing dynamic prefix. On the basis of GTEE-STAPREF, integrating context-specific information leads to a constent gain for Trg-C F1 score on all the datasets as 0.8%, 0.6% and 0.5%, respectively. There can also be observed a 1.5%, 0.5% and 0.8% increase for the Arg-C F1 scores, respectively. It indicates that integrating context-specific information into typespecific information and transforming static prefix to dynamic is beneficial for generative templatebased event extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Irrelevance Classifier</head><p>Our goal of the irrelevance classifier IC is to recognize the context that does not contain any event records in a given ontology O. According to ?3.5, we train an IC and use it for each dataset separately. Please note that on one specific dataset, we will use the same IC for all the experiments corresponding to that dataset. The accuracy of IC is 95.4%, 93.5% and 94.2% for ACE05E, ACE05E + and ERE-EN, respectively. To further study the influence of IC, we compare the performances of using no IC, trained IC, and gold IC. The compared F1 scores are listed in Table <ref type="table" target="#tab_9">7</ref>. First, we find that with the help of our trained  ICs on each dataset, the Trg-C and Arg-C F1 scores have been improved a lot by more than ten percentage points, indicating the necessity of IC. Second, by replacing the trained IC with the oracle gold IC results, we can still observe possible increasements for F1 scores, suggesting the existence of likely chances for further optimizing IC performances. We leave the optimization for IC as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Intrinsic Evaluation</head><p>We study the intrinsic characteristics of GTEE-DYNPREF by showing the influences of model hyperparameters on ACE05-E + .</p><p>Prefix length L. We first study the impact of prefix length L by grid search in {L|L = 10 * k, k ? N ? k ? 12}. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we studied event extraction in the template-based conditional generation manner. We proposed the dynamic prefix tuning model GTEE-DYNPREF for event extraction. On the one hand the method constructs tunable prefixes to model type-specific information and on the other hand GTEE-DYNPREF captures the associations between event types and calculates a context-specific prefix when steering pretrained language models. Experimental results show that our model achieves competitive results with the state-of-the-art on ACE 2005, which is also proven to be portable to new types of events effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Ethical Consideration</head><p>Event extraction is a standard task in NLP. We do not see any significant ethical concerns. Our work is easy to adapt to new event types by offering some examples and pre-defined templates. Therefore, the expected usages of our work is to identify interesting event records from user textual input such as a piece of sentence or document.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Comparision between the generation-based methods and our method GTEE-DYNPREF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The framework of our dynamic prefix-tuning model GTEE-DYNPREF. We use different colors to differentiate different components as follows. "" for the context, " " for the template, " " for the type-specific prefixes, "" for the dynamic prefix, " " for the encoder-decoder language model, and " " for the answered prompt as output.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Context-specific DYNPREF construction using a context encoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Performances of GTEE-DYNPREF with different dimension D of the embedding tensor P .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Intrinsic evaluation results on ACE05-E + .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Figure 5(a)  shows the Trg-C and Arg-C F1 scores. We can observe that both Trg-C and Arg-C F1 scores increase as the prefix length L increases to 80, afterward, a slight fluctuation. We think the longer L introduces more trainable parameters and a more vital ability to model the context-specific type information. Therefore, we choose 80 as the prefix length in GTEE-DYNPREF.Embedding dimension D . Similarly, we study the impact of the dimension D of the embedding tensor P by increasing from 64 to 1024. The results of Trg-C and Arg-C F1 scores are illustrated in Figure5(b). We find that although the bigger embedding dimension D theoretically provides expressive type-specific information and improves the F1 scores when D &lt;= 512, the continual improvement is interrupted when the embedding dimension is around 512. Thus we set the embedding dimension D = 512 in GTEE-DYNPREF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>2021;</figDesc><table><row><cell cols="3">Extracting events for the Contact:Meet type</cell><cell>Output</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Trigger meeting &lt;IN_SEP&gt; Hariri met with head of</cell></row><row><cell></cell><cell></cell><cell></cell><cell>state in palace place</cell></row><row><cell></cell><cell cols="2">Encoder</cell><cell>Decoder</cell></row><row><cell>Prompt</cell><cell></cell><cell></cell></row><row><cell>Type Instruction</cell><cell>Template</cell><cell>Context</cell></row><row><cell>Event type Meet.</cell><cell>Trigger &lt;trg&gt; &lt;IN_SEP&gt; &lt;arg&gt; met with &lt;arg&gt; in &lt;arg&gt; place</cell><cell cols="2">Hariri submitted his resignation during a 10-minute meeting with the head of state at the Baabda presidential palace</cell></row><row><cell cols="4">Figure 2: The framework of our base model GTEE-BASE. We use different colors to differentiate different</cell></row><row><cell>components as follows. "</cell><cell cols="2">" for the context, "</cell><cell>" for the template, "</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>Dataset statistics.</figDesc><table><row><cell>Dataset</cell><cell cols="4">Split #Sents #Events #Roles</cell></row><row><cell></cell><cell cols="2">Train 17,172</cell><cell>4202</cell><cell>4859</cell></row><row><cell>ACE05-E</cell><cell>Dev Test</cell><cell>923 832</cell><cell>450 403</cell><cell>605 576</cell></row><row><cell></cell><cell cols="2">Train 19,216</cell><cell>4419</cell><cell>6607</cell></row><row><cell>ACE05-E +</cell><cell>Dev Test</cell><cell>901 676</cell><cell>468 424</cell><cell>759 689</cell></row><row><cell></cell><cell cols="2">Train 14,736</cell><cell>6208</cell><cell>8924</cell></row><row><cell>ERE-EN</cell><cell>Dev Test</cell><cell>1209 1163</cell><cell>525 551</cell><cell>730 822</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Hyperparameter setting for our models.</figDesc><table><row><cell>Name</cell><cell cols="2">GTEE-BASE</cell><cell>IC</cell><cell cols="3">GTEE-DYNPREF</cell></row><row><cell>learning rate</cell><cell cols="2">1e-5</cell><cell>2e-5</cell><cell></cell><cell>5e-5</cell><cell></cell></row><row><cell>train batch size</cell><cell cols="2">32*8</cell><cell>16*8</cell><cell></cell><cell>32*8</cell><cell></cell></row><row><cell>epochs</cell><cell>40</cell><cell></cell><cell>12</cell><cell></cell><cell>30</cell><cell></cell></row><row><cell>weight decay</cell><cell cols="2">1e-5</cell><cell>1e-5</cell><cell></cell><cell>1e-5</cell><cell></cell></row><row><cell>gradient clip</cell><cell>5.0</cell><cell></cell><cell>5.0</cell><cell></cell><cell>5.0</cell><cell></cell></row><row><cell>warm-up ratio</cell><cell cols="2">10%</cell><cell>10%</cell><cell></cell><cell>10%</cell><cell></cell></row><row><cell>prefix length L</cell><cell>-</cell><cell></cell><cell>-</cell><cell></cell><cell>80</cell><cell></cell></row><row><cell>embedding dim D</cell><cell>-</cell><cell></cell><cell>-</cell><cell></cell><cell>512</cell><cell></cell></row><row><cell>Model</cell><cell>P</cell><cell>Trg-C R</cell><cell>F1</cell><cell>P</cell><cell>Arg-C R</cell><cell>F1</cell></row><row><cell>classification-based</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DYGIE++</cell><cell>-</cell><cell>-</cell><cell>69.7</cell><cell>-</cell><cell>-</cell><cell>48.8</cell></row><row><cell>GAIL</cell><cell cols="6">74.8 69.4 72.0 61.6 45.7 52.4</cell></row><row><cell>ONEIE</cell><cell>-</cell><cell>-</cell><cell>74.7</cell><cell>-</cell><cell>-</cell><cell>56.8</cell></row><row><cell>BERT_QA</cell><cell cols="6">71.1 73.7 72.3 56.8 50.2 53.3</cell></row><row><cell>MQAEE</cell><cell>-</cell><cell>-</cell><cell>71.7</cell><cell>-</cell><cell>-</cell><cell>53.4</cell></row><row><cell>generation-based</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>TANL</cell><cell>-</cell><cell>-</cell><cell>68.5</cell><cell>-</cell><cell>-</cell><cell>48.5</cell></row><row><cell>BART-GEN</cell><cell cols="6">69.5 72.8 71.1 56.0 51.6 53.7</cell></row><row><cell>TEXT2EVENT</cell><cell cols="6">67.5 71.2 69.2 46.7 53.4 49.8</cell></row><row><cell>DEGREE-E2E</cell><cell>-</cell><cell>-</cell><cell>70.9</cell><cell>-</cell><cell>-</cell><cell>54.4</cell></row><row><cell>GTEE-DYNPREF</cell><cell cols="6">63.7 84.4 72.6 49.0 64.8 55.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc> 61.9 72.8 66.9 51.9 58.8 55.1 Results on ACE05-E + and ERE-EN for event extraction in the supervised learning setting. For each column, we bold the highest score.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">ACE05-E +</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">ERE-EN</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Model</cell><cell></cell><cell></cell><cell cols="2">Trg-C</cell><cell></cell><cell></cell><cell>Arg-C</cell><cell></cell><cell></cell><cell>Trg-C</cell><cell></cell><cell></cell><cell>Arg-C</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell></cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell></cell><cell></cell></row><row><cell></cell><cell>ONEIE</cell><cell></cell><cell cols="13">72.1 73.6 72.8 55.4 54.3 54.8 58.4 59.9 59.1 51.8 49.2 50.5</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">TEXT2EVENT</cell><cell cols="13">71.2 72.5 71.8 54.0 54.8 54.4 59.2 59.6 59.4 49.4 47.2 48.3</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">DEGREE-E2E</cell><cell>-</cell><cell>-</cell><cell cols="2">72.7</cell><cell>-</cell><cell>-</cell><cell>55.0</cell><cell>-</cell><cell>-</cell><cell>57.1</cell><cell>-</cell><cell>-</cell><cell>49.6</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">ACE05-E</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">ACE05-E +</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">ERE-EN</cell><cell></cell></row><row><cell>Model</cell><cell></cell><cell>Trg-C</cell><cell></cell><cell cols="2">Arg-C</cell><cell></cell><cell></cell><cell cols="2">Trg-C</cell><cell></cell><cell cols="2">Arg-C</cell><cell></cell><cell>Trg-C</cell><cell></cell><cell></cell><cell>Arg-C</cell></row><row><cell></cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell cols="19">GTEE-DYNPREF 63.7 84.4 72.6 49.0 64.8 55.8 67.3 83.0 74.3 49.8 60.7 54.7 61.9 72.8 66.9 51.9 58.8 55.1</cell></row><row><cell>GTEE-STAPREF</cell><cell cols="18">62.8 83.9 71.8 47.0 64.2 54.3 66.5 82.8 73.7 49.1 60.4 54.2 61.4 72.2 66.4 50.7 58.5 54.3</cell></row><row><cell>GTEE-BASE</cell><cell cols="18">61.9 83.4 71.0 46.4 63.7 53.7 65.7 82.1 73.0 48.1 59.7 53.2 60.6 71.3 65.5 49.8 57.8 53.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Ablation study results on ACE05-E, ACE05-E + and ERE-EN. From GTEE-BASE to GTEE-DYNPREF, the model performances grows stronger.</figDesc><table><row><cell>Model</cell><cell>P</cell><cell>Trg-C R</cell><cell>F1</cell><cell>P</cell><cell>Arg-C R</cell><cell>F1</cell></row><row><cell>ONEIE w/o TL</cell><cell cols="6">70.8 64.8 67.7 53.2 37.5 44.0</cell></row><row><cell>ONEIE w/ TL</cell><cell cols="6">71.0 64.4 67.6 54.7 38.1 45.0</cell></row><row><cell>?performance</cell><cell cols="2">+0.2 -0.4</cell><cell cols="4">-0.1 +1.5 +0.6 +1.0</cell></row><row><cell>TEXT2EVENT w/o TL</cell><cell cols="6">72.9 62.7 67.4 54.0 38.1 44.7</cell></row><row><cell>TEXT2EVENT w/ TL</cell><cell cols="6">75.1 64.0 69.1 56.0 40.5 47.0</cell></row><row><cell>?performance</cell><cell cols="6">+2.2 +1.3 +1.7 +2.0 +2.4 +2.3</cell></row><row><cell cols="7">GTEE-DYNPREF w/o TL 62.0 75.4 68.1 39.6 53.5 45.5</cell></row><row><cell>GTEE-DYNPREF w/ TL</cell><cell cols="6">64.6 76.7 70.2 43.7 54.1 48.3</cell></row><row><cell>?performance</cell><cell cols="6">+2.6 +1.3 +2.1 +4.1 +0.6 +2.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Transfer learning results on ACE05-E + .</figDesc><table><row><cell>Model</cell><cell cols="6">ACE05-E Trg-C Arg-C Trg-C Arg-C Trg-C Arg-C ACE05-E + ERE-EN</cell></row><row><cell>GTEE-DYNPREF</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>w/o IC</cell><cell>57.2</cell><cell>43.8</cell><cell>61.7</cell><cell>46.4</cell><cell>52.1</cell><cell>44.7</cell></row><row><cell>w/ IC (trained)</cell><cell>72.6</cell><cell>55.8</cell><cell>74.3</cell><cell>54.7</cell><cell>66.9</cell><cell>55.1</cell></row><row><cell>w/ IC (gold)</cell><cell>76.3</cell><cell>58.4</cell><cell>77.2</cell><cell>56.9</cell><cell>72.3</cell><cell>57.4</cell></row><row><cell>GTEE-STAPREF</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>w/o IC</cell><cell>56.9</cell><cell>43.4</cell><cell>61.3</cell><cell>45.9</cell><cell>51.4</cell><cell>44.0</cell></row><row><cell>w/ IC (trained)</cell><cell>71.8</cell><cell>54.3</cell><cell>73.7</cell><cell>54.2</cell><cell>66.4</cell><cell>54.3</cell></row><row><cell>w/ IC (gold)</cell><cell>75.2</cell><cell>57.5</cell><cell>76.6</cell><cell>55.8</cell><cell>71.6</cell><cell>56.9</cell></row><row><cell>GTEE-BASE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>w/o IC</cell><cell>56.4</cell><cell>42.8</cell><cell>60.8</cell><cell>45.1</cell><cell>50.7</cell><cell>43.1</cell></row><row><cell>w/ IC (trained)</cell><cell>71.0</cell><cell>53.7</cell><cell>73.0</cell><cell>53.2</cell><cell>65.5</cell><cell>53.5</cell></row><row><cell>w/ IC (gold)</cell><cell>74.6</cell><cell>55.9</cell><cell>75.1</cell><cell>54.8</cell><cell>70.7</cell><cell>56.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>The F1 scores under different irrelevance classifier settings on ACE05-E, ACE05-E + and ERE-EN.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>In this paper, we use [ * ] to represent the special tokens used in pretrained LM and &lt; * &gt; to indicate the user-defined special tokens.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>The argument template and all the used ontologies can be accessed at https://github.com/raspberryice/ gen-arg except for ERE. Since the ERE event types are subsets of the RAMS AIDA ontology and the KAIROS ontology, following<ref type="bibr" target="#b14">Li et al. (2021)</ref>, we also reuse the argument templates from these ontologies.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>The random initialization is implemented in the torch.nn.EmbeddingLayer class in PyTorch v1.7.1.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We thank the anonymous reviewers for their valuable comments and suggestions. This work is supported by <rs type="funder">National Natural Science Foundation of China</rs> (No. <rs type="grantNumber">U19B2020</rs> and No. <rs type="grantNumber">62106010</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_wXFfWNC">
					<idno type="grant-number">U19B2020</idno>
				</org>
				<org type="funding" xml:id="_ccMP32t">
					<idno type="grant-number">62106010</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Argument Template</head><p>We use templates for ACE and ERE. Table <ref type="table">8</ref> and Table <ref type="table">9</ref> show the argument templates for ACE and ERE, respectively, which is from the RAMS AIDA ontology and the KAIROS ontology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Transfer Learning Details</head><p>The top-10 frequent types of events in the src split of ACE05-E + are listed as follows:</p><p>? Transaction:Transfer-Ownership  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The stages of event extraction</title>
		<author>
			<persName><forename type="first">David</forename><surname>Ahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Annotating and Reasoning about Time and Events</title>
		<meeting>the Workshop on Annotating and Reasoning about Time and Events</meeting>
		<imprint>
			<publisher>Australia. Association for Computational Linguistics</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Alec Radford, Ilya Sutskever, and Dario Amodei</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
	<note>Language models are few-shot learners</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Template-based named entity recognition using BART</title>
		<author>
			<persName><forename type="first">Leyang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.161</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1835" to="1845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The automatic content extraction (ACE) program -tasks, data, and evaluation</title>
		<author>
			<persName><forename type="first">George</forename><surname>Doddington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Przybocki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lance</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Strassel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC&apos;04)</title>
		<meeting>the Fourth International Conference on Language Resources and Evaluation (LREC&apos;04)<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Event extraction by answering (almost) natural questions</title>
		<author>
			<persName><forename type="first">Xinya</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.49</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="671" to="683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Making pre-trained language models better few-shot learners</title>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.295</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3816" to="3830" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">WARP: Word-level Adversarial ReProgramming</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Hambardzumyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hrant</forename><surname>Khachatrian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.381</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021-05">May. 2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4921" to="4933" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">PTR: prompt tuning with rules for text classification</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno>CoRR, abs/2105.11259</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Refining event extraction through cross-document inference</title>
		<author>
			<persName><forename type="first">I-Hung</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuan-Hao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Boschee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prem</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
		<idno>CoRR, abs/2108.12724</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-08: HLT</title>
		<meeting>ACL-08: HLT<address><addrLine>Ohio</addrLine></address></meeting>
		<imprint>
			<publisher>Columbus</publisher>
			<date type="published" when="2008">2021. 2008</date>
			<biblScope unit="page" from="254" to="262" />
		</imprint>
	</monogr>
	<note>DEGREE: a dataefficient generative event extraction model. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The power of scale for parameter-efficient prompt tuning</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Dominican Republic. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3045" to="3059" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.703</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">Marjan Ghazvininejad,. 2020</date>
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Event extraction as multi-turn question answering</title>
		<author>
			<persName><forename type="first">Fayuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuguang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yajuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.73</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="829" to="838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Joint event extraction via structured prediction with global features</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="73" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Documentlevel event argument extraction by conditional generation</title>
		<author>
			<persName><forename type="first">Sha</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.69</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="894" to="908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Prefix-tuning: Optimizing continuous prompts for generation</title>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.353</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4582" to="4597" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A joint neural model for information extraction with global features</title>
		<author>
			<persName><forename type="first">Ying</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.713</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7999" to="8009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Event extraction as machine reading comprehension</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojiang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.128</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1641" to="1651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">2021a. P-tuning v2: Prompt tuning can be comparable to finetuning universally across scales and tasks</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaixuan</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yicheng</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno>CoRR, abs/2110.07602</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Jointly multiple events extraction via attentionbased graph information aggregation</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhunchen</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heyan</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1156</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1247" to="1256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujie</forename><surname>Qian</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Zhilin Yang, and Jie Tang. 2021b. GPT understands, too. CoRR, abs/2103.10385</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 7th International Conference on Learning Representations</title>
		<meeting>7th International Conference on Learning Representations<address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06">2019. 2019. May 6-9, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Text2Event: Controllable sequence-tostructure generation for end-to-end event extraction</title>
		<author>
			<persName><forename type="first">Yaojie</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianpei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialong</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Annan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoyi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.217</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2795" to="2806" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Joint event extraction via recurrent neural networks</title>
		<author>
			<persName><forename type="first">Thien</forename><surname>Huu Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1034</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="300" to="309" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">C?cero Nogueira dos Santos, Bing Xiang, and Stefano Soatto. 2021. Structured prediction as translation between augmented natural languages</title>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Paolini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Athiwaratkun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Krone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Achille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishita</forename><surname>Anubhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria</title>
		<meeting>9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria</meeting>
		<imprint>
			<publisher>OpenReview</publisher>
			<date type="published" when="2021">May 3-7, 2021</date>
		</imprint>
	</monogr>
	<note>net</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Association for Computational Linguistics. Guanghui Qin and Jason Eisner. 2021. Learning how to ask: Querying LMs with mixtures of soft prompts</title>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.410</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5203" to="5212" />
		</imprint>
	</monogr>
	<note>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-totext transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">67</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Automatically identifying words that can serve as labels for few-shot text classification</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helmut</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.488</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain (Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5569" to="5578" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Exploiting cloze-questions for few-shot text classification and natural language inference</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.eacl-main.20</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</title>
		<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="255" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Few-shot text generation with natural language instructions</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Dominican Republic. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="390" to="402" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">It&apos;s not just size that matters: Small language models are also few-shot learners</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.185</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<biblScope unit="page" from="2339" to="2352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts</title>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasaman</forename><surname>Razeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.346</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4222" to="4235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Multimodal few-shot learning with frozen language models</title>
		<author>
			<persName><forename type="first">Maria</forename><surname>Tsimpoukelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serkan</forename><surname>Cabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Ali Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<idno>CoRR, abs/2106.13884</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems<address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09">2017. 2017. December 4-9, 2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Entity, relation, and event extraction with contextualized span representations</title>
		<author>
			<persName><forename type="first">David</forename><surname>Wadden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulme</forename><surname>Wennberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1585</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5784" to="5789" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">HMEAE: Hierarchical modular event argument extraction</title>
		<author>
			<persName><forename type="first">Xiaozhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1584</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5777" to="5783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Joint extraction of events and entities within a document context</title>
		<author>
			<persName><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1033</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="289" to="299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Exploring pre-trained language models for event extraction and generation</title>
		<author>
			<persName><forename type="first">Sen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linbo</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhigang</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongsheng</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1522</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5284" to="5294" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Joint entity and event extraction with generative adversarial imitation learning</title>
		<author>
			<persName><forename type="first">Tongtao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avirup</forename><surname>Sil</surname></persName>
		</author>
		<idno type="DOI">10.1162/dint_a_00014</idno>
	</analytic>
	<monogr>
		<title level="j">Data Intell</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="120" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Factual probing is [MASK]: Learning vs. learning to recall</title>
		<author>
			<persName><forename type="first">Zexuan</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.398</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5017" to="5033" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
