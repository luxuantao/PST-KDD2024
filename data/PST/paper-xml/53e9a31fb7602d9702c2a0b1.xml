<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sigma Lenses: Focus-Context Transitions Combining Space, Time and Translucence</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Emmanuel</forename><surname>Pietriga</surname></persName>
							<email>emmanuel.pietriga@inria.fr</email>
							<affiliation key="aff0">
								<address>
									<addrLine>1 INRIA Bât 490 -Orsay</addrLine>
									<postCode>F-91405</postCode>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">LRI -Univ. Paris-Sud &amp; CNRS</orgName>
								<address>
									<addrLine>490 -Orsay</addrLine>
									<postCode>F-91405</postCode>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sigma Lenses: Focus-Context Transitions Combining Space, Time and Translucence</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4F7C5102186BDA88EA72E4AD0D1CAAD6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Multi-scale interfaces</term>
					<term>Focus + Context</term>
					<term>Fisheye lenses</term>
					<term>Translucence</term>
					<term>Focus targeting</term>
					<term>Controlled experiment H. Information Systems H.5 Information Interfaces and Presentation H.5.2 User Interfaces (H.1.2</term>
					<term>I.3.6)</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Focus + context techniques such as fisheye lenses are used to navigate and manipulate objects in multi-scale worlds. They provide in-place magnification of a region without requiring users to zoom the whole representation and consequently lose context. Their adoption is however hindered by usability problems mostly due to the nature of the transition between focus and context. Existing transitions are often based on a physical metaphor (magnifying glass, fisheye, rubber sheet), and are almost always achieved through a single dimension: space. We investigate how other dimensions, namely time and translucence, can be used to achieve more efficient transitions. We present an extension to Carpendale's framework for unifying presentation space accommodating these new dimensions. We define new lenses in that space, called Sigma lenses, and compare them to existing lenses through experiments based on a generic task: focus targeting. Results show that one new lens, the SPEED-COUPLED BLENDING lens, significantly outperforms all others.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTRODUCTION</head><p>Many techniques can be used in combination with classical pan &amp; zoom to navigate large multi-scale worlds. Among them, a range of bifocal display techniques have been designed, which can be broadly categorized as either overview + detail or focus + context techniques. Overview + detail techniques <ref type="bibr" target="#b25">[25]</ref> usually put the context view in a small inset located in a corner of the screen, leaving most of the latter to the detailed view, while focus + context techniques do the opposite: the context occupies the whole screen except for a small area that provides in-place magnification of a limited region of the context. While overview + detail techniques are generally favored and have been shown to perform well in some situations <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b24">24]</ref>, there are cases where they show their limits: for instance, when navigating a map of a densely populated region to look for particular localities, overview + detail techniques can only use a few pixels to display each of them in the context view. On the contrary, focus + context techniques can convey additional information in the context view, such as the localities' names, thus providing users with more contextual information that can guide navigation. They have also been shown to perform efficiently in other situations, e.g., for large steering tasks <ref type="bibr" target="#b14">[14]</ref> or when selecting small targets with a stylus <ref type="bibr" target="#b27">[27]</ref>.</p><p>Even though they have been studied for some time, the adoption of focus + context techniques remains limited, mostly due to comprehension and low-level interaction problems <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">11]</ref> related to how the transition between the context and the magnified focus region is achieved. Many of the transitions described in the literature are inspired by the physical world and are presented through metaphors such as magnifying glasses, rubber sheets <ref type="bibr" target="#b30">[30]</ref>, and more generally surface deformations <ref type="bibr" target="#b2">[3]</ref>; in other words, spatial transitions caus-ing various problems that can hinder performance. For instance simple magnifying glasses (Figure <ref type="figure" target="#fig_0">1</ref>-a) create occlusion of the immediate context adjacent to the magnified region <ref type="bibr" target="#b28">[28]</ref>; graphical fisheyes <ref type="bibr" target="#b29">[29]</ref>, also known as distortion lenses (Figure <ref type="figure" target="#fig_0">1-b</ref>), make it difficult to acquire targets <ref type="bibr" target="#b11">[11]</ref>, especially for high magnification factors. To cancel the negative effects of distortion associated with fisheyes, Gutwin proposed Speed-coupled flattening lenses <ref type="bibr" target="#b11">[11]</ref>, introducing time as a dimension to transition between focus and context. The comparison of these time-based lenses with plain fisheye lenses demonstrated that the performance of lens-based techniques can be improved by using dimensions other than space to control the transition between focus and context.</p><p>In addition to space and time, we argue that other dimensions readily available in the electronic world can be used to provide more efficient transitions between focus and context. In this paper we introduce a generalization of Carpendale's framework for unifying presentation space <ref type="bibr" target="#b4">[5]</ref>. This generalization encompasses transitions based on two orthogonal dimensions: space and translucence (Figure <ref type="figure" target="#fig_0">1-c</ref>), which can be combined with a third dimension: time (Figure <ref type="figure" target="#fig_0">1-d</ref>). This opens up a large design space, called the Sigma lens design space, in which we identify interesting points. We report on the results of an evaluation of five lenses on the generic task of focus targeting, a basic motor task involved in many highlevel navigation tasks. The main finding of these evaluations is that one new lens, the SPEED-COUPLED BLENDING lens, significantly outperforms all other types of lenses for that task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RELATED WORK</head><p>Focus + context techniques are mainly differentiated by the way they transition between the focus and context regions. Techniques such as the DragMag <ref type="bibr" target="#b32">[32]</ref> and Manhattan lenses <ref type="bibr" target="#b4">[5]</ref> display the focus as an inset which is offset from the corresponding context region so as not to occlude the local context adjacent to that region. In this particular case, there is no actual transition between focus and context, which are simply connected through lines serving as visual cues. Techniques that do not offset the focus region provide an in situ magnification that sits on top of the corresponding context region <ref type="bibr" target="#b19">[19]</ref>. They have to use some type of transition in order to avoid occlusion of the adjacent context. This is almost always achieved by distorting the representation, so as to smoothly integrate the focus into the context. The distortion can affect the entire representation: Graphical Fisheyes <ref type="bibr" target="#b29">[29]</ref>, the Rubber Sheet <ref type="bibr" target="#b30">[30]</ref>, the Document Lens <ref type="bibr" target="#b28">[28]</ref>, the Perspective Wall <ref type="bibr" target="#b21">[21]</ref>. Or it can be restricted to a specific region, in which case they are called constrained lenses <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">19]</ref>. These have the advantage of distorting only a limited region around the focus, leaving most of the context untouched. They have been shown to work better than full screen lenses for some tasks <ref type="bibr" target="#b13">[13]</ref>, and should be favored when the focus has to be relocated often, as they reduce the amount of visual changes during lens movements (focus retargeting). Various improvements to these techniques have been proposed, such as ways to achieve higher magnification <ref type="bibr" target="#b6">[6]</ref>, and visual cues that can help in comprehending distortion <ref type="bibr" target="#b3">[4]</ref>. In almost all cases, however, the transition between focus and context is achieved through one single dimension: space. Magic Lens filters, part of the See-Through Interface <ref type="bibr" target="#b1">[2]</ref>, are powerful generic lenses that are used to modify the rendering of objects seen through them. However, to our knowledge, they have not been used to specifically address the problem of smoothly transitioning between focus and context, whether through space, time, or translucence. Lieberman used translucence in Powers of Ten Thousands <ref type="bibr" target="#b20">[20]</ref>, a bifocal display technique that makes the focus and context views share the same physical screen space, by using multiple translucent layers. But as with the DragMag, there is no actual transition between focus and context, which are overlaid on top of one another. Even though it has been shown to be usable in exploratory studies <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b15">15]</ref>, this type of representation based on transparent or translucent layers is cognitively demanding, causing visual interferences that are the source of serious legibility problems, and requiring additional mental effort from the user to relate focus and context. Translucence remains, however, an interesting dimension which has been used successfully to reclaim some screen real-estate, either in combination with other filters such as in multiblending <ref type="bibr" target="#b0">[1]</ref> or by making the translucence level dynamically vary as a function of cursor movements <ref type="bibr" target="#b12">[12]</ref>. As mentioned earlier, cursor movements have been used in a different context, closer to our problem, for controlling the magnification factor of speed-coupled flattening lenses <ref type="bibr" target="#b11">[11]</ref> over time, with the effect of increasing focus targeting performance compared to the equivalent static fisheye lenses. Another technique, Speed-dependent automatic zooming <ref type="bibr" target="#b17">[17]</ref>, couples zoom level in a window with scroll rate, zooming-out as speed increases. We believe that new types of lenses can be created by more systematically combining the above dimensions, namely space, translucence and time, in order to provide more efficient transitions between focus and context in multi-scale interfaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SPACE, TIME, TRANSLUCENCE, AND BEYOND</head><p>The basic concepts for describing spatial distortion between focus and context have been defined in Carpendale's framework for unifying presentation space <ref type="bibr" target="#b4">[5]</ref>. In this section we reformulate these concepts in a slightly different, but equivalent way in order to accommodate our generalization of transitions between focus and context. This formulation is based on space-scale diagrams <ref type="bibr" target="#b9">[9]</ref> and uses the associated terminology. Basic knowledge about these diagrams is assumed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>General Properties</head><p>We consider the focus and context regions of any lens-based representation as separate viewing windows in a space-scale diagram. The final rendered viewing window is a composition of points from both windows.</p><p>All constrained lenses, no matter how they transition between focus and context, have the following properties:</p><p>• MM : the maximum magnification in the focus region (a.k.a the flat-top), • R I : the radius of the flat-top region, which we call inner radius, • R O : the radius of the lens at its base (i.e., its extent), which we call outer radius, • (x c , y c ) : the coordinates of the lens' center. The final viewing window obtained at rendering time can be seen as a combination of the two abstract windows introduced above: the rendering of the focus window is integrated, after some transformation, in the context window.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spatial Transitions</head><p>The most common transformation consists in displacing all points in the focus window to achieve a smooth transition between focus and context through spatial distortion <ref type="bibr" target="#b4">[5]</ref>. This type of transformation can be defined through a drop-off function, such as a Gaussian (see Figure <ref type="figure" target="#fig_1">2</ref>), which models the magnification profile of the lens. Associated with a distance function d, the drop-off function is defined as:</p><formula xml:id="formula_0">G scale : (x, y, d) → s</formula><p>with s a scaling factor. G scale is usually a monotonically decreasing function with a range of <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">MM]</ref>. This is not a strong requirement; other functions may however introduce discontinuities in the spatial transition.</p><p>The rendering of point (x, y) in the final viewing window is then defined through the displacement function r:</p><formula xml:id="formula_1">r(x, y) = ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ (xc + x-xc MM , yc + y-yc MM ) {∀(x, y)|d(x, y) R I } (1) (xc + x-xc G scale (x,y,d) , yc + y-yc G scale (x,y,d) ) {∀(x, y)|R I &lt; d(x, y) &lt; R O } (2) (x, y) {∀(x, y)|d(x, y) R O } (3)</formula><p>The flat-top region corresponds to case (1), the transition to case (2), and the region beyond the lens boundaries (i.e., the context) corresponds to case (3). Distance function d is based on L p -metrics, and is defined as follows:</p><formula xml:id="formula_2">d : (x, y) → P |x -x c | P + |y -y c | P</formula><p>where (x, y) are the coordinates of a point seen through a lens centered in (x c , y c ), and P ∈ N * . Most lenses are either radial (P = 2, circular shape) or orthogonal (P = ∞, square shape). A Gaussian function is often used to define drop-off function G scale , as it provides one of the smoothest visual transitions between focus and context. Figures <ref type="figure" target="#fig_3">2</ref> and<ref type="figure" target="#fig_0">1-b</ref> illustrate Gaussian distortion lenses. It is not the purpose of  this article to provide a detailed survey of all possible dropoff and distance functions, which are already well-described in the literature <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">6]</ref>.</p><formula xml:id="formula_3">2•R I v u O s s•MM 2•R O 2•MM•R O 2•R I 2•R O Rendering in</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Translucence as a Transition Dimension</head><p>Digital image compositing and more particularly alpha blending represents another, yet unexplored, method for transitioning between the focus and context regions of a constrained lens. As with spatial distortion, the final viewing window obtained at rendering time is a combination of the two abstract viewing windows: here, points of the focus window are composited with points of the context window. For instance, using gradually increasing translucence, it is possible to smoothly blend the focus viewing window into the context, thus achieving a transition without resorting to distortion (see Figures <ref type="figure" target="#fig_4">3</ref> and<ref type="figure" target="#fig_0">1-c</ref>).</p><p>The continuity between focus and context is realized through compositing only. Given two points with (r, g, b) color components, one in the focus window and one in the context window, we note:</p><formula xml:id="formula_4">p context ⊗ α p focus</formula><p>the point p comp resulting from compositing them using Porter &amp; Duff's Source atop Destination alpha blending rule <ref type="bibr" target="#b26">[26]</ref> with a value of α, the source being the focus viewing window and the destination the context viewing window:</p><formula xml:id="formula_5">p comp = α • r focus + (1 -α) • rcontext α • g focus + (1 -α) • gcontext α • b focus + (1 -α) • bcontext</formula><p>As with scale for distortion lenses, the translucence profile can be defined by a drop-off function that maps a translucence level to a point (x, y) located at a distance d from the lens center:</p><formula xml:id="formula_6">G alpha : (x, y, d) → α</formula><p>with α an alpha blending value in [0, α F T ], α F T being the lowest translucence level used in the lens' flat-top. Note that it does not necessarily have to be 1.0 (opaque), though it will often be close to it. Drop-off function G alpha is usually a monotonically decreasing function. Again, this is not a requirement, but other types of functions may introduce discontinuities in the blending gradient.</p><p>The rendering of a point (x, y) in the final viewing window is then defined through the blending function b:</p><formula xml:id="formula_7">b(x, y) = ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ (xc + x-xc MM , yc + y-yc MM ) N α F T (x, y) {∀(x, y)|d(x, y) R I } (1) (xc + x-xc MM , yc + y-yc MM ) N G alpha (x,y,d) (x, y) {∀(x, y)|R I &lt; d(x, y) &lt; R O } (2) (x, y) {∀(x, y)|d(x, y) R O } (3)</formula><p>Figure <ref type="figure" target="#fig_4">3</ref> shows how translucence is used to transition between focus and context in what we call a BLENDING lens. This dimension offers an alternate way to smoothly transition between focus and context without resorting to spatial distortion, thus eliminating the drawbacks specifically associated with the latter. As we will discuss in the evaluation section, this transition type introduces problems of its own. </p><formula xml:id="formula_8">2•R I v u O s s•MM 2•R O 2•MM•R O 2•R I 2•R O Rendering in</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Time-based Transitions</head><p>The transition functions described in the previous sections make it possible to create a broad range of lenses. However, as is the case with most lenses reported in the literature, the properties of these lenses are defined statically. One notable exception is the Speed-coupled flattening lens <ref type="bibr" target="#b11">[11]</ref> which uses the lens' dynamics (velocity and acceleration) to automatically control magnification: basically, MM decreases toward 1.0 as the speed of the lens (operated by the user) increases, therefore "flattening" the lens into the context, and increases back to its original value as the lens comes to a full stop. Speed-coupled flattening lenses have been demonstrated to outperform their static counterpart, and represent a first step in the direction of using time-dependent transitions to improve the usability of lenses.</p><p>The magnification factor of a lens (MM) is an obvious parameter to control over time. There is no reason however to limit the use of the lens' dynamics to this one alone. Other candidates include the lens' radii R I and R O , as well as the lens' translucence value in its flat-top α F T .</p><p>We note F(t) any time-based function returning a numerical value that can be used to dynamically change one or more of the above-mentioned lens properties. In the following we focus on one particular function: the lens' velocity and acceleration over time. This is however just one possible function </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overall Lens Design Space</head><p>Spatial transitions and transitions based on translucence can be combined in a single lens, each with their own drop-off and distance functions. Additionally, several lens properties can be made time-dependent. This makes for a rather complex expression for computing the rendering of a point (x, y) seen through the lens, which reflects the richness of our new design space. Table <ref type="table" target="#tab_0">1</ref> gives a summary of the properties of both existing and new lenses within this design space.</p><p>The first three lenses already exist in the literature. The MAGNIFYING GLASS, illustrated in Figure <ref type="figure" target="#fig_0">1</ref> The last two techniques are new contributions identified while exploring our design space. The BLENDING lens, illustrated in Figures <ref type="figure" target="#fig_0">1-c</ref> and<ref type="figure" target="#fig_4">3</ref>, can be seen as the simplest example of a translucence-based transition: it is like a MAGNIFYING GLASS that gradually blends into the context. Smoothness of transition is achieved without resorting to spatial distortion: context pixels gradually fade out as we get closer to the lens' center, while focus pixels gradually fade in.</p><p>The last lens, called SPEED-COUPLED BLENDING, is even closer to a MAGNIFYING GLASS. It shares all of its properties except that its α F T depends on the lens' movements. When still, the lens looks like a MAGNIFYING GLASS. But the flattop becomes increasingly and uniformly translucent as the lens moves faster, becoming fully transparent beyond a given speed threshold S. The same type of low-pass filter as that governing the behavior of the SPEED-COUPLED FLATTENING lens is used. Figure <ref type="figure" target="#fig_0">1</ref>-d shows a screenshot of a SPEED-COUPLED BLENDING lens moving at slow speed. Figure <ref type="figure" target="#fig_6">4</ref> illustrates the behavior of this lens when moving it from left to right in the scene introduced in the previous section: (t1) the lens stands still on the left side of the color spectrum; (t2) the user starts moving the lens to position it at the other end of the scene: the context can be seen through the focus (which is itself still visible) by translucence; (t3) the lens is moved fast, beyond threshold speed S; the focus is thus completely transparent: only the context is visible; (t4) the user slows down, the lens' focus gradually reappears; (t5) the user has reached the desired position, the lens comes to a full stop, and the focus is opaque again. A small inner circle can be noticed inside the lens at (t2), (t3) and (t4). This circle identifies, at the scale of the context, what region is magnified in the flat-top. The visibility of this translucent circle is controlled by 1α F T : invisible when the lens stands still, it becomes more and more apparent as the lens moves faster, and conversely. This indicator was added as a result of a pilot study: we discovered that feedback, in the context view, of the position and size of the region to be magnified helped targeting objects more efficiently. The same type of indicator was then added to our version of the SPEED-COUPLED FLATTENING lens; in that case, the small circle expands and shrinks during lens movements, as its size directly depends on MM. This inherent instability makes it less convenient than its SPEED-COUPLED BLENDING counterpart, but it is still of great help when targeting an object.</p><p>BLENDING and SPEED-COUPLED BLENDING lenses are just two of several new interesting focus+context techniques that have been identified in the Sigma lens design space. The latter could actually be further extended to include other rendering techniques to achieve focus-context transitions, such as those based on multiblending <ref type="bibr" target="#b0">[1]</ref>, to highlight particular features of objects in the transition area. These are however still too computationally expensive to achieve acceptable frame rates on most personal computers, and are left as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EXPERIMENT 1: FOCUS TARGETING PERFORMANCE</head><p>We conducted an experiment to compare the performance and limits of the three existing and two new lenses described in the previous section. Participants were asked to perform a simple task, namely focus targeting, which consists in putting a given target in the flat-top of the lens. Focus targeting is one of the building blocks of many higher-level navigation tasks such as searching <ref type="bibr" target="#b24">[24]</ref>.</p><p>Focus targeting performance was evaluated at five different magnification factors (MM). Higher magnification factors make the task increasingly difficult: (i) the transition area becomes harder to understand as it must integrate a larger part of the world in the same rendering area, and (ii) it becomes harder to precisely position the target in the flat-top of the lens, the latter being controlled in the motor space of the context window. To test the limits of each lens, we included factors up to 14x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Apparatus</head><p>We used a Dell Precision 380 equipped with a 3 GHz Pentium D processor, an NVidia Quadro FX4500 graphics card, a 1600 x 1200 LCD monitor (21") and a Dell optical mouse. The program was written in Java 1.6 using the open source ZVTM toolkit <ref type="bibr" target="#b23">[23]</ref> which offers a wide range of distortion lenses and could easily be extended to support translucenceand time-based transitions. The application was limited to a 1400 x 1200 window with a black padding of 100 pixels in order to accommodate instruction messages and simulate screen real-estate that would usually be taken by control and information widgets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants</head><p>Ten unpaid adult volunteers (7 male, 3 female), from 23 to 40 year-old (average 26.4, median 25), all with normal or corrected to normal vision, served in the experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task and Procedure</head><p>Our focus targeting task consisted in acquiring a target in the flat-top of the lens as quick as possible. In our experimental setting, the lens was centered on the mouse cursor. The task ended when the participant clicked the left mouse button, provided that the target was fully contained within the flat-top. As focus targeting consists not only in correctly positioning the lens, but also in looking at the magnified target, additional conditions were imposed on some lenses to guarantee sufficient target visibility. The SPEED-COUPLED FLATTENING lens had to be magnifying enough (MM ≥ 60% of max. value), and the SPEED-COUPLED BLENDING lens had to be opaque enough (α F T ≥ 0.4). If all conditions were met when the participant clicked the mouse button for the first time, the targeting was counted as a hit, otherwise, as a miss. Each trial consisted in performing 24 successive focus targeting tasks. As illustrated in Figure <ref type="figure" target="#fig_7">5</ref>, the targets were laid out in a circular manner. The order of appearance forced participants to perform focus targeting tasks in every direction, as recommended by the ISO9241-9 standard <ref type="bibr" target="#b18">[18]</ref>. We decided to have only one target visible at a time, as we noticed during a pilot experiment in which all targets were visible that some participants were often taking advantage of the layout pattern to acquire the object set as the current target by positioning the lens relative to that object's siblings.</p><p>Our experiment was a 5 × 5 within-participant design: each participant had to perform several trials using each of the five lenses (Lens ∈ {MAGNIFYING GLASS, FISHEYE, BLENDING, SPEED-COUPLED FLATTENING, SPEED-COUPLED BLENDING}) with five different magnification factors (MM ∈ {2, 4, 6, 10, 14}). We grouped trials into five blocks, one per lens, so as not to disturb participants with too many changes between lenses. To avoid a non-controlled effect of order, we used a Latin square to compute five different orders of presentation for lenses and assigned two participants per order. Within a Lens block, each participant had to perform ten trials (i.e. 10 × 24 focus targeting tasks), 2 trials with each of the five different values of MM. Trials within a block were presented in a random order after a training phase containing 3 trials (MM = 2, 6 and 14), allowing participants to get familiar with a given lens before empirical measures were collected. The 24 targeting tasks of a trial had to be performed in a row, but participants were allowed to rest between trials. The first targeting task of each trial was ignored. A total of 11500 actual focus targeting tasks were thus taken into account in the analysis. The experimenter first introduced the task, and then each lens immediately before the corresponding block, and made sure that participants did understand how each one worked and how best to operate it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Predictions</head><p>We drew the following predictions based on each lens' properties, the results of previous studies and a theoretical analysis of the motor movements involved in focus targeting.</p><p>H 1 BLENDING lenses outperform FISHEYE lenses. Transitioning through space introduces distortion that makes objects move away from the approaching lens focus before moving toward it very fast, making focus targeting difficult <ref type="bibr" target="#b11">[11]</ref>. Translucence-based transitions used in BLENDING lenses have their own problems, with a negative impact on targeting performance, but these might not be as strong as that commonly associated with distortion.</p><p>H 2 SPEED-COUPLED FLATTENING and SPEED-COUPLED BLEND-ING lenses outperform all other lenses. Each focus targeting task can be divided into two phases: in the first phase, the user moves the lens quickly to reach the target's vicinity, while in the second phase, she moves it slowly to precisely position the target in the focus. In the first phase, the user is not interested in information provided in the focus region since she is trying to reach a distant object in the context as quick as possible. This hypothesis motivated the design of SPEED-COUPLED FLATTENING lenses and is supported by the results of the study conducted in <ref type="bibr" target="#b11">[11]</ref>: SPEED-COUPLED FLATTENING lenses outperform FISHEYE lenses when performing focus targeting tasks (SPEED-COUPLED FLATTENING ≥ FISH-EYE) 1 . Here, we hypothesize that no matter the transition dimensions involved, providing a detailed view during the first phase is of limited value and has a negative effect on performance, leading to the conclusion that smoothly and automatically neutralizing the focus and transition regions during this phase, and then restoring them, can help the user. This leads to the following partial order: SPEED-COUPLED FLATTEN- ING ≥ FISHEYE and SPEED-COUPLED BLENDING ≥ MAGNIFYING GLASS.</p><p>H 3 Focus targeting is easier with MAGNIFYING GLASS lenses and SPEED-COUPLED BLENDING lenses. From a pure motor perspective, the difficulty of a focus targeting task can be evaluated as a view pointing task in a fixed-scale interface <ref type="bibr" target="#b10">[10]</ref>. We can thus use Formula (1) in Figure <ref type="figure" target="#fig_8">6</ref> to quantify the difficulty of moving the lens' flat-top, of size W focus , to a position where it will contain the target, of size W target , initially located at a distance D from the lens' center. Formula (1) computes the Index of Difficulty, ID, of our focus targeting task. The lens' position in the context window is controlled in the visual and motor space of that window. W target , W focus and D are thus expressed in context pixels: in our experiment, W target = 8 pixels and D = 800 pixels, while W focus depends on a given Lens × MM condition: W focus = (2 × R I )/M M . As MM increases, the size of W focus decreases, making the task more difficult. For 1 L1 &gt; L2 means that L1 outperforms L2 lenses of equal size (R O ), the size of the flat-top (R I ), and thus W focus , vary depending on the lens type. MAGNIFYING GLASS and SPEED-COUPLED BLENDING lenses are made of a flattop only: W focus = W lens = 200, while other lenses have to accommodate the transition within the same overall area: W focus = W lens /2 = 100 in our implementation. MAGNIFY-ING GLASS and SPEED-COUPLED BLENDING thus feature a larger flat-top than other lenses with the same overall size, consequently making focus targeting easier from a motor perspective: ID ranges from 3.2 to 6.3 for MAGNIFYING GLASS and SPEED-COUPLED BLENDING while it ranges from 4.2 to 8 for FISHEYE, SPEED-COUPLED FLATTENING and BLENDING. This reasoning however does not take into account non-motor aspects of the task which also depend on the type of lens used. For instance, occlusion caused by the always-opaque MAG- NIFYING GLASS should increasingly hinder performance in the second phase of the task (precise positioning) as MM gets bigger: W focus becomes smaller while W lens remains constant, making the occlusion zone between focus and context on the path to the target larger, along with the chances of losing track of the target.</p><p>Altogether, these three hypotheses only provide a partial order of performance between the five lenses. One strong expectation is that the SPEED-COUPLED BLENDING lens will perform efficiently as it addresses many issues: it does not distort the representation, the dynamically translucent flat-top reduces occlusion problems, and its large size makes the task easier from a motor perspective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and Discussion</head><p>Analysis of variance revealed a significant simple effect on number of errors (miss) for MM (F 4,36 = 46, p &lt; 0.0001), for Lens (F 4,36 = 6, p &lt; 0.0001), and a significant interaction effect on number of errors for Lens × MM (F 16,144 = 4, p &lt; 0.0001). As the mean number of errors for each lens and the total number of errors are low (from 0.05 for BLEND- ING to 0.02 for MAGNIFYING GLASS for a total of 380 misses, about 3%), we focus the following analyses on hits only. We verified that there was no effect of lens presentation order on time and observed that learning effects were not significant for each lens. We observed a significant simple effect on time for Lens (F 4,36 = 66, p &lt; 0.0001). Tukey post hoc tests revealed that SPEED-COUPLED BLENDING is the fastest lens and SPEED-COUPLED BLENDING the slowest, while BLENDING, FISHEYE and SPEED-COUPLED FLATTENING do not significantly differ in terms of performance. We also observed a significant simple effect on time for MM (F 4,36 = 648, p &lt; 0.0001) and a significant interaction effect on time for Lens × MM (F 16,144 = 88, p &lt; 0.0001). Figure <ref type="figure">7</ref> illustrates these results.</p><p>As expected, SPEED-COUPLED BLENDING performs better than all other lenses. Other predictions are only partially supported by the measures we collected. First, H 1 is not supported. Our results reveal that FISHEYE BLENDING. Eliminating distortion by switching from a spatial transition to a smooth translucence-based transition does not seem to provide an advantage. Feedback collected from participants leads us to believe that this might be due to the high cognitive effort required to comprehend transitions based on gradually increasing translucence which, as opposed to distortionbased transitions, do not rely on a familiar physical metaphor. H 2 is partially supported: (i) smoothly neutralizing and restoring the focus of a MAGNIFYING GLASS by making it translucent (α F T as a function of lens speed) does improve performance (SPEED-COUPLED BLENDING &gt; MAGNIFYING GLASS); but (ii) flattening a fisheye (MM as a function of lens speed) does not yield a significant improvement over FISHEYE (SPEED-COUPLED FLATTENING FISHEYE). This last result is surprising since the study reported in <ref type="bibr" target="#b11">[11]</ref> showed that SPEED-COUPLED FLATTENING outperfoms FISHEYE for a distortion level of 5 (i.e., MM = 6). This inconsistency can be explained by taking a closer look at implementation details. First, we implemented SPEED-COUPLED FLATTENING as a constrained lens while it was implemented as a full-screen lens by <ref type="bibr">Gutwin.</ref> In full-screen lenses, distortion affects the whole representation, which thus benefits more from the neutralization effect than constrained lenses that only affect a limited area. Second, as we require that MM ≥ 60% of max. value to end a trial, our task is a little bit longer than the one described in <ref type="bibr" target="#b11">[11]</ref> in the SPEED-COUPLED FLATTENING condition, whereas this constraint does not exist in the FISHEYE condition. Finally, H 3 is supported: SPEED-COUPLED BLENDING, with its large flat-top, outperforms all other lenses starting at MM = 4. Conversely the performance of MAGNIFYING GLASS goes down rapidly as MM gets higher. It becomes the worst lens starting at MM = 6, due to the earlier-mentioned negative effects of occlusion that make precise positioning difficult. It is interesting to note that for the lowest value of MM, MAGNIFYING GLASS outperforms all other lenses. Occlusion caused by the lens' opacity still causes the user to temporarily loose track of the target. But the occlusion zone is small at such low magnification. The negative impact of occlusion on performance is thus not significant compared to the positive impact of the larger flat top. Compared to SPEED- COUPLED BLENDING, MAGNIFYING GLASS also has the advantage of not requiring the user to wait several hundred milliseconds for the flat top to become opaque enough. In the particular case of very low magnification factors (MM ≤ 2), MAGNIFY- ING GLASS should thus be considered by interface designers. We found that SPEED-COUPLED BLENDING lenses outperform SPEED-COUPLED FLATTENING lenses, and attributed this performance gain (i) to the large flat-top of the SPEED-COUPLED BLENDING lens which makes focus targeting easier from a motor perspective, and (ii) to the absence of distortion and reduction of occlusion effects through the coupling of focus translucence with lens speed. Experiment 2 aimed at better understanding the results of the previous experiment by identifying the contribution of both properties to this performance gain. We studied how SPEED-COUPLED BLENDING performed at two "extreme" sizes: 1) the lens has the same size as other lenses (as SPEED-COUPLED BLENDING in the first experiment), and 2) the lens has the same size as the flat-top of lenses which accommodate a transition area and thus feature a smaller flat-top, making focus targeting harder from a motor perspective as explained earlier. We called the latter SPEED-COUPLED BLENDING small and compared it to SPEED-COUPLED BLENDING and SPEED-COUPLED FLATTENING, both from the previous experiment. Apparatus was the same as before. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants</head><p>Six unpaid adult volunteers (5 male, 1 female), from 23 to 40 year-old (average 27.8, median 25.5), all with normal or corrected to normal vision, served in the experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task and Procedure</head><p>The focus targeting task was exactly the same as the one used in Experiment 1. To reduce the length of this experiment, we picked two representative magnification factors: MM ∈ {8, 12}. This experiment was thus a 3 × 2 withinparticipant design. We again grouped trials by lens type. We used a Latin square to compute three different presentation orders for lenses and assigned two participants per order. Each participant performed three Lens blocks. Within a block, each participant had to perform two trials (i.e., 2 × 24 focus targeting tasks), one trial per value of MM. For a given lens presentation order, one participant saw trials in order MM = 8 then MM = 12, while the other one saw trials in order MM = 12 then MM = 8. Each block began with a two-trial training phase, one per value of MM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and Discussion</head><p>Analysis of variance revealed a simple effect of T echnique on time (F 2,10 = 53, p &lt; 0.0001), a simple effect of MM on time (F 1,5 = 230, p &lt; 0.0001), and an interaction effect of T echnique × MM on time (F 2,10 = 8, p &lt; 0.0001).</p><p>Tukey post hoc tests revealed the following lens performance order: SPEED-C. BLENDING &gt; SPEED-C. BLENDING small &gt; SPEED-C. FLATTENING, as illustrated in Figure <ref type="figure">8</ref>. These results show that even at the same level of motor difficulty (i.e., with equal flat-top sizes), the SPEED-COUPLED BLENDING lens still performs better than the SPEED-COUPLED FLATTENING lens. This means that interface designers are given several options to improve upon a classical lens such as FISHEYE: 1) they can either get a smaller but more efficient lens (in terms of focus targeting performance), saving screen real-estate for the context, 2) if the latter is not critical they can make the SPEED- COUPLED BLENDING lens occupy the same space as a FISHEYE would, further improving focus targeting performance, or 3) find a balance between these two solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EXPERIMENT 3: INFLUENCE OF TARGET VISIBILITY</head><p>Translucence can affect targeting performance <ref type="bibr" target="#b12">[12]</ref>, especially when targets are superimposed on a complex background such as a map or photograph. As the simple abstract world we used in the first two experiments might have hidden negative effects of translucence on lenses, we conducted a third experiment to check whether our comparative lens performance ordering was still valid when targeting objects that blend into a realistic background. Apparatus was the same as before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants</head><p>Eight unpaid adult volunteers (6 male, 2 female), from 23-28 years (avg. 24.7, med. 24.5), all with normal or corrected to normal vision, no color blindness, served in the experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task and Procedure</head><p>The task was essentially the same as before, except for the fact that the 24 targets were laid out on a satellite photograph (Figure <ref type="figure" target="#fig_11">9</ref>), and could either be filled with a fully opaque red color (α = 1.0) or with a translucent red (α = 0.5), in which case they blended into the background and were less easily identifiable. The satellite photograph was a 7000x5000 pixels portion of NASA's Blue Marble Next Generation world map <ref type="bibr" target="#b31">[31]</ref>, providing appropriate levels of detail in both the focus and context regions. To limit the length of this ex-periment, we discarded the poorly performing MAGNIFYING GLASS and picked only two representative magnification factors (MM ∈ {8, 12}). This experiment was thus a 4 Lens× 2 MM × 2 Opacity within-participant design. Trials were again grouped by lens type. Participants performed four blocks which were presented in four varying orders computed through a Latin square. Each block was made of four trials (i.e., 4 × 24 focus targeting tasks), one per randomly distributed Opacity × MM condition, and was preceded with a training phase of two trials (MM = 8, Opacity = 1 and MM = 12, Opacity = 0.5). As we were mainly interested in the effect of target visibility on the ending phase of our focus targeting task, we added two visual hints to help participants find the next target's location in the context, so as to control and reduce the associated visual search time as much as possible: 1) after a successful targeting, the next target appeared and its border flashed white for one second; 2) four red bars were located on the four edges of the context window (indicated by black arrows in Figure <ref type="figure" target="#fig_11">9</ref>-a) so that the target was at their virtual intersection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and Discussion</head><p>Results were consistent with that of previous experiments, and matched participants' subjective preferences. Analysis of variance revealed a simple effect of T echnique on time (F 3,21 = 56, p &lt; 0.0001), a simple effect of MM on time (F 1,7 = 212, p &lt; 0.0001), and an interaction effect of T echnique × MM on time (F 3,21 = 12, p &lt; 0.0001).</p><p>Our initial performance ordering was preserved. The only difference was that SPEED-COUPLED FLATTENING significantly outperformed FISHEYE (Figure <ref type="figure" target="#fig_0">10</ref>). We tentatively attribute this higher significance to the more disturbing effects of distortion during lens movements on a complex background, to be confirmed by further evaluations.</p><p>Regarding the specific effect of target visibility, we found a simple effect of Opacity on time (F 1,7 = 14, p = 0.007), and an interaction effect of T echnique × Opacity on time (F 3,21 = 6, p = 0.005), confirming that lens performance does depend on this factor. However, Tukey HSD post hoc tests revealed that conditions Opacity = 0.5 and Opacity = 1 were in two different groups only for the BLENDING lens. This result is not unexpected as the BLENDING lens can be prone to visual interference between focus and context in the transition region depending on the nature of the representation, especially when non-contrasted objects are targeted.</p><p>No matter how aesthetically pleasing (several participants noted that it produced very nice graphical renderings), the BLENDING lens suffers from its earlier-mentioned lack of reliance on a familiar physical metaphor, and proneness to visual interference in the transition region. The SPEED-COUPLED BLENDING lens, however, does not suffer from these problems, as its use of translucence is very different: it can be seen as a magnifying glass whose content smoothly fades out to prevent occlusion at focus targeting time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CONCLUSION</head><p>Sigma lenses form a rich and unified design space which includes a wide range of constrained lenses, from those that transition between focus and context through the spatial di- mension only, to new ones that achieve this transition using a combination of up to three dimensions among space, time, and translucence. We compared two such new lenses with three representatives of the first category using a focus targeting task. Empirical data revealed that our new SPEED- COUPLED BLENDING lens outperforms all other lenses. These results encourage us to further investigate the use of non-spatial dimensions to transition between focus and context. First and foremost, the exploration of our design space has revealed several potentially interesting new lenses, based on innovative combinations of space and translucence, on the coupling of lens speed with properties such as its radii, or on the use of time-based functions other than lens speed. Secondly, we have seen that, depending on the lens and task studied, non-motor aspects can have a significant influence on performance, e.g., flat-top size and legibility, occlusion and search (depending on layout). We thus plan to formally evaluate lenses based on a wider range of tasks, including high-level cognitive ones.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Various transitions between focus and context: (a) step transition causing occlusion, (b) distorting space, (c) using gradually increasing translucence, (d) using a combination of translucence and time.</figDesc><graphic coords="1,318.35,303.17,113.54,91.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figures 2</head><label>2</label><figDesc>Figures 2 illustrates these definitions using a distortion lens applied to a scene made of a series of equal-size rectangles that form a regular color spectrum. The context viewing window corresponds to what is seen in the absence of any lens. If it is positioned at s on scale axis v, the focus viewing window is necessarily positioned at s • MM. Points A and B represent the boundaries of the constrained lens within the context viewing window, at a distance R O from the lens' center C. The focus viewing window is then a flat magnification by a factor of MM of the region delimited by A and B. Its size is thus 2 • MM•R O . R I controls the size of the lens' flat-top. If R I = R O , the lens is a mere magnifier lens (or magnifying glass) as illustrated in Figure 1-a. If R I is zero, the flat-top is reduced to a single point at the center of the lens, which is then the only point at full magnification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Gaussian distortion lens in a 1+1D space-scale diagram, and corresponding 2D rendering.</figDesc><graphic coords="3,324.35,56.81,91.94,78.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Blending lens in a 1+1D space-scale diagram, and corresponding 2D rendering.</figDesc><graphic coords="4,60.83,254.81,93.62,79.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>-a, consists only of a flat-top (R O = R I ) which occludes the immediate surroundings of the magnified region. FISHEYE denotes the common graphical fisheye lens. Here we use a Gaussian drop-off function to transition, through space only, between focus and context (Figures 1-b and 2). The SPEED-COUPLED FLATTENING lens is a variation on the one introduced by Gutwin [11], applied here to constrained lenses. It uses a simple interpolated low-pass filter inspired by the one of trailing widgets [8] as a time-based function to control the magnification factor based on the lens' velocity and acceleration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Speed-Coupled Blending Lens moving from left to right over time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Exp. 1 &amp; 2: Targets' order of appearance in a trial (targets have been made twice their actual relative size for legibility purposes).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Focus Targeting Task in a space scale diagram.</figDesc><graphic coords="6,53.87,53.93,241.10,172.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>1 SCBFigure 7 .</head><label>17</label><figDesc>Figure 7. Mean completion time per T echnique × MM condition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>2 Figure 8 .</head><label>28</label><figDesc>Figure 8. Mean completion time per MM × T echnique condition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Exp. 3: targets are laid out radially (a), and can be either easily distinguished from the background (b), or blend into it (c).</figDesc><graphic coords="8,323.99,60.77,174.74,124.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>3 Figure 10 .</head><label>310</label><figDesc>Figure 10. Mean completion time per Opacity ×T echnique condition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Properties of existing and new lenses in the design space of time, and we plan to investigate other functions in future work.</figDesc><table><row><cell></cell><cell>MAGNIF.</cell><cell>FISHEYE</cell><cell>SPEED-C.</cell><cell cols="2">BLENDING SPEED-C.</cell></row><row><cell></cell><cell>GLASS</cell><cell></cell><cell>FLAT.</cell><cell></cell><cell>BLEND.</cell></row><row><cell>Magnif.</cell><cell>MM</cell><cell>MM</cell><cell cols="2">MM •F(t) MM</cell><cell>MM</cell></row><row><cell>Factor</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Spatial</cell><cell>step</cell><cell cols="2">Gaussian Gaussian</cell><cell>step</cell><cell>step</cell></row><row><cell>drop-off</cell><cell>function</cell><cell></cell><cell></cell><cell>function</cell><cell>function</cell></row><row><cell>α F T</cell><cell>1.0</cell><cell>1.0</cell><cell>1.0</cell><cell>α</cell><cell>α • F(t)</cell></row><row><cell>Blending</cell><cell>step</cell><cell>step</cell><cell>step</cell><cell cols="2">Gaussian step</cell></row><row><cell>drop-off</cell><cell>function</cell><cell>function</cell><cell>function</cell><cell></cell><cell>function</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We wish to thank Olivier Bau, Michel Beaudouin-Lafon, Olivier Chapuis, Andy Cockburn, as well as Sheelagh Carpendale, Yves Guiard, Gonzalo Ramos, Dan Vogel, and the anonymous reviewers for their feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multiblending: displaying overlapping windows simultaneously without the drawbacks of alpha blending</title>
		<author>
			<persName><forename type="first">P</forename><surname>Baudisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gutwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI &apos;04: Proc. Human Factors in Computing Systems</title>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="367" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Toolglass and magic lenses: the see-through interface</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Pier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Buxton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Derose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH &apos;93: Proc. Computer graphics and interactive techniques</title>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="73" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">3-dimensional pliable surfaces: for the effective presentation of visual information</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S T</forename><surname>Carpendale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Cowperthwaite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">D</forename><surname>Fracchia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UIST &apos;95: Proc. ACM Symp. on User Interface Software and Technology</title>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="217" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Making distortions comprehensible</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S T</forename><surname>Carpendale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Cowperthwaite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">D</forename><surname>Fracchia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VL &apos;97: Proc. of the 1997 IEEE Symp. on Visual Languages</title>
		<imprint>
			<publisher>IEEE Comput. Soc</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A framework for unifying presentation space</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S T</forename><surname>Carpendale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Montagnese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UIST &apos;01: Proc</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m">ACM Symp. on User Interface Software and Technology</title>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="61" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Achieving higher magnification in context</title>
		<author>
			<persName><forename type="first">S</forename><surname>Carpendale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ligh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pattison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UIST &apos;04: Proc. ACM Symp. on User Interface Software and Technology</title>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="71" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The usability of transparent overview layers</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Chugh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gutwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Greenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI 98 conference summary on Human factors in computing systems</title>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="301" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hybridpointing: fluid switching between absolute and relative pointing with a direct input device</title>
		<author>
			<persName><forename type="first">C</forename><surname>Forlines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Balakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UIST &apos;06: Proc. ACM Symp. on User Interface Software and Technology</title>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="211" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Space-scale diagrams: understanding multiscale interfaces</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Bederson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI &apos;95: Proc. Human Factors in Computing Systems</title>
		<imprint>
			<publisher>ACM Press/Addison-Wesley</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Target acquisition in multiscale electronic worlds</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Guiard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Beaudouin-Lafon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Hum.-Comput. Stud</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="875" to="905" />
			<date type="published" when="2004-12">Dec. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improving focus targeting in interactive fisheye views</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gutwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI &apos;02: Proc. Human Factors in Computing Systems</title>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="267" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The effects of dynamic transparency on targeting performance</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gutwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dyck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fedak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Graphics Interface</title>
		<imprint>
			<publisher>A K Peters</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="105" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A comparison of fisheye lenses for interactive layout tasks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gutwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fedak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Graphics Interface</title>
		<imprint>
			<publisher>A K Peters</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="213" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fisheyes are good for large steering tasks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gutwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Skopik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI &apos;03: Proc. Human Factors in Computing Systems</title>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="201" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Transparent layered user interfaces: an evaluation of a display design to enhance focused and divided attention</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Vicente</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A S</forename><surname>Buxton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI &apos;95: Proc. Human Factors in Computing Systems</title>
		<imprint>
			<publisher>ACM Press/Addison-Wesley Publishing Co</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="317" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Navigation patterns and usability of zoomable user interfaces with and without an overview</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hornbaek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Bederson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Plaisant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Comput.-Hum. Interact</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="362" to="389" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Speed-dependent automatic zooming for browsing large documents</title>
		<author>
			<persName><forename type="first">T</forename><surname>Igarashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hinckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UIST &apos;00: Proc. ACM Symp. on User Interface Software and Technology</title>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="139" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ergonomic requirements for office work with visual display terminals (VDTs)-Part 9: Requirements for non-keyboard input devices</title>
		<idno>ISO. 9241-9</idno>
	</analytic>
	<monogr>
		<title level="j">International Organization for Standardization</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Techniques for non-linear magnification transformations</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Keahey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INFOVIS &apos;96: IEEE Symp. on Information Visualization</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Powers of ten thousand: navigating in large information spaces</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lieberman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UIST &apos;94: Proc. ACM Symp. on User Interface Software and Technology</title>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="15" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The perspective wall: detail and context smoothly integrated</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Mackinlay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Card</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI &apos;91: Proc. Human Factors in Computing Systems</title>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="173" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Snap-together visualization: a user interface for coordinating visualizations via relational schemata</title>
		<author>
			<persName><forename type="first">C</forename><surname>North</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shneiderman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AVI &apos;00: Proc. working Conf. on Advanced Visual Interfaces</title>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="128" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Pietriga</surname></persName>
		</author>
		<title level="m">VL/HCC&apos;05: IEEE Symp. on Visual Languages and Human-Centric Computing</title>
		<imprint>
			<publisher>IEEE Comput. Soc</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="145" to="152" />
		</imprint>
	</monogr>
	<note>A Toolkit for Addressing HCI Issues in Visual Language Environments</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pointing and Beyond: an Operationalization and Preliminary Evaluation of Multi-scale Searching</title>
		<author>
			<persName><forename type="first">E</forename><surname>Pietriga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Appert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Beaudouin-Lafon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI &apos;07: Proc. Human Factors in Computing Systems</title>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1215" to="1224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Image-browser taxonomy and guidelines for designers</title>
		<author>
			<persName><forename type="first">C</forename><surname>Plaisant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shneiderman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Software</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="21" to="32" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Compositing digital images</title>
		<author>
			<persName><forename type="first">T</forename><surname>Porter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Duff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH &apos;84: Proc. Comput. graphics and interact. techniques</title>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1984">1984</date>
			<biblScope unit="page" from="253" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pointing lenses: facilitating stylus input through visual-and motor-space magnification</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cockburn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Beaudouin-Lafon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI &apos;07: Proc. Human Factors in Computing Systems</title>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="757" to="766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The document lens</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Mackinlay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UIST &apos;93: Proc. ACM Symp. on User Interface Softw. and Tech</title>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="101" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Graphical fisheye views</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="73" to="83" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Stretching the rubber sheet: a metaphor for viewing large layouts on small screens</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Snibbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">J</forename><surname>Tversky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Reiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UIST &apos;93: Proc. ACM Symp. on User Interface Software and Technology</title>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="81" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">The Blue Marble Next Generation -A true color earth dataset including seasonal dynamics from MODIS. Published by the NASA Earth Observ</title>
		<author>
			<persName><forename type="first">R</forename><surname>Stockli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vermote</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Saleous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Simmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Herring</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The DragMag image magnifier</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ware</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI &apos;95 conference companion, Human Factors in Computing Systems</title>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="407" to="408" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
