<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Action recognition with appearance-motion features and fast search trees</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2010-11-12">12 November 2010</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
							<email>k.mikolajczyk@surrey.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Vision, Speech and Signal Processing</orgName>
								<orgName type="institution">University of Surrey</orgName>
								<address>
									<settlement>Guildford</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">H</forename><surname>Uemura</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Vision, Speech and Signal Processing</orgName>
								<orgName type="institution">University of Surrey</orgName>
								<address>
									<settlement>Guildford</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Action recognition with appearance-motion features and fast search trees</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2010-11-12">12 November 2010</date>
						</imprint>
					</monogr>
					<idno type="MD5">89EAAEDD4171BBBD55E8034C81AF2B30</idno>
					<idno type="DOI">10.1016/j.cviu.2010.11.002</idno>
					<note type="submission">Received 27 February 2010 Accepted 1 November 2010</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Action recognition Feature extraction Dominant motion compensation Sport action classification</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we propose an approach for action recognition based on a vocabulary of local appearancemotion features and fast approximate search in a large number of trees. Large numbers of features with associated motion vectors are extracted from video data and are represented by many trees. Multiple interest point detectors are used to provide features for every frame. The motion vectors for the features are estimated using optical flow and a descriptor based matching. The features are combined with image segmentation to estimate dominant homographies, and then separated into static and moving ones despite the camera motion. Features from a query sequence are matched to the trees and vote for action categories and their locations. Large number of trees make the process efficient and robust. The system is capable of simultaneous categorisation and localisation of actions using only a few frames per sequence. The approach obtains excellent performance on standard action recognition sequences. We perform large scale experiments on 17 challenging real action categories from various sport disciplines. We demonstrate the robustness of our method to appearance variations, camera motion, scale change, asymmetric actions, background clutter and occlusion.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Significant progress has been made in classification of static scenes and action recognition is receiving increasing attention in computer vision community. Many existing methods <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b7">9,</ref><ref type="bibr" target="#b35">37,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b42">45,</ref><ref type="bibr" target="#b46">49]</ref> obtain high classification scores for simple action sequences with exaggerated motion, static and uniform background in controlled environments. It is however hard to make a visual correspondence between a lab controlled action and a real action of the same category as their appearance, motion and clutter significantly differ. Real action scenes represent a challenge rarely addressed in the literature. Our main goal in this paper is not only to propose a generic solution which could handle actions in real environment, but also to demonstrate how the performance for the controlled environment and the real one can differ. An illustration of frequently used action recognition benchmark data and real example of the same category is in Fig. <ref type="figure" target="#fig_0">1</ref>. The need for using real world data is argued in image recognition community i.e. in the Pascal Visual Object Classes Challenge <ref type="bibr" target="#b4">[6]</ref> the 2010 challenge includes an action recognition teaser. The TREC Video Retrieval Evaluation is another example of large community efforts to provide a real application data and independent evaluation benchmark. TRECVid also introduced a human action detection task in airport surveillance videos. Initial results for this data with spatiotemporal descriptors and bag-of-words are reported in <ref type="bibr" target="#b50">[53]</ref>.</p><p>In this paper we address the problem of recognising object-actions with a data driven method, which does not require long sequences or high level reasoning. We build on our previous work from <ref type="bibr" target="#b31">[33,</ref><ref type="bibr" target="#b33">35,</ref><ref type="bibr" target="#b45">48]</ref> which we combine together and modify the system components to optimise the efficiency and recognition accuracy. The main contribution is a generic solution to action classification including localisation of objects performing actions. We draw from existing work recently undertaken in recognition and retrieval of static images <ref type="bibr" target="#b21">[23,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b40">42]</ref>. Our approach follows the popular paradigm, which is the use of local features, vocabulary based representation and voting. Such systems have been very successful in retrieval and recognition of static images. However, recognition of actions is a distinct problem and a number of modifications must be proposed to adopt it to this new application scenario. Compared to existing, approaches which usually focus on one of the issues associated with action recognition and make strong assumptions about the camera motion or background, our system can deal with appearance variations, camera motion, scale change, asymmetric actions, background clutter and occlusion. So far, very little has been done to address all of these issues simultaneously. The key idea explored here is the use of large number of features represented in many search trees, in contrast to many existing action classification methods based on a single, small codebook and SVM <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b36">38,</ref><ref type="bibr" target="#b42">45]</ref>. This message also comes from the static object recognition <ref type="bibr" target="#b4">[6]</ref>, where efficient search methods using many different features from large number of examples provide the best results. The advantage of using multiple trees has been demonstrated in image retrieval <ref type="bibr" target="#b40">[42]</ref>. In this paper the trees are built from various types of features, representing appearance-action models and are learnt efficiently from videos as well as from static images. Moreover, we use a simple nearest neighbour classifier unlike other methods based on SVMs <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b36">38,</ref><ref type="bibr" target="#b42">45]</ref>, although for comparison we also provide results with SVMs.</p><p>Another contribution of this paper is a feature extraction approach with dominant motion compensation. The features represent local appearance combined with local motion information extracted from a video regardless the camera motion and background clutter. These features allow accurate classification and localisation of multiple actions performed simultaneously.</p><p>Among other contributions, we implement an object-action representation which allows to hypothesise an action category, its location and pose from a single feature. We show how to make use of static training data and static features to support an action hypothesis. In contrast to the other systems our method can simultaneously classify the entire sequence as well as recognise and localise multiple actions within the sequence. Finally, we consider the use of new action categories and recognition results reported in this paper as one of our major contributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Relevant work</head><p>Early approaches to human centred motion analysis have been reviewed in <ref type="bibr" target="#b0">[1]</ref> under three categories: body parts motion analysis, tracking, and activity recognition. It is argued there that 'the key to successful execution of high-level tasks is to establish feature correspondence between consecutive frames, which still remains a bottleneck in the whole processing'. A decade has passed since this review but the observation is still valid and our approach is focusing on improving the quality of feature correspondence for action recognition task.</p><p>Recently, a boosted space-time window classifier from <ref type="bibr" target="#b16">[18]</ref> was applied to real movie sequences in <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b20">22]</ref>. However, boosting systems are known to require weak classifiers and large number of training examples to generalise, otherwise the performance is low. Also space-time features, space-time pyramids and multichannel non-linear SVMs do not allow for efficient processing of large databases.</p><p>Another frequently followed class of approaches is based on spatio-temporal features computed globally <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">7,</ref><ref type="bibr" target="#b49">52]</ref> or locally <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b7">9,</ref><ref type="bibr" target="#b36">38,</ref><ref type="bibr" target="#b42">45]</ref>. Both methods suffer from various drawbacks. Global methods cannot recognise multiple actions simultaneously or localise them spatially. In these methods recognition can be done by computing similarity between globally represented actions using cross-correlation <ref type="bibr" target="#b5">[7]</ref> or histograms of spatio-temporal gradients <ref type="bibr" target="#b49">[52]</ref>. Spatio-temporal interest points <ref type="bibr" target="#b18">[20]</ref> result in a very memory efficient representation but are too sparse to build action models robust to camera motion, background clutter, occlusion, motion blur, etc. Moreover, local features are often used to represent the entire sequence as a distribution, which at the end results in a global representation. It was demonstrated in <ref type="bibr" target="#b46">[49]</ref> that as few as 5-25 spatio-temporal interest points give high recognition performance on standard test data. We argue that this number is insufficient for real actions. The need for more features has been observed in <ref type="bibr" target="#b3">[5]</ref>, where Harris interest point detector was combined with a Gabor filter to extract more spatio-temporal points. This argument was also emphasised by <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b36">38]</ref>, which proposed a hybrid of spatio-temporal and static features to improve the recognition performance. This shifts the attention from motion towards the appearance of objects performing actions. In this context it seems more appropriate to address object-action categorisation problem rather than action via motion only.</p><p>A different class of approach relies upon one strong assumption that body parts can be reliably tracked <ref type="bibr" target="#b41">[43]</ref>, even though existing tracking tools often fail in real video data. These methods use relatively large temporal extent and recognise more complex actions often viewed from multiple cameras, thus are less relevant to this work.</p><p>Another frequent assumption in the literature is a static camera and uniform background. This is valid for many surveillance applications but not for the general action recognition problem. In <ref type="bibr" target="#b19">[21]</ref> the camera was assumed to be fixed. The action recognition approach from <ref type="bibr" target="#b48">[51]</ref>, claims to be the first to deal with camera motion, explored multi-view geometry. This solution however requires multiple camera setup or very similar actions captured from different viewpoints, which limits the range of possible applications. Other work relevant to camera motion estimation and dominant plane segmentation perform combined motion and image segmentation <ref type="bibr" target="#b44">[47]</ref> or plane estimation <ref type="bibr" target="#b47">[50]</ref>, but these are concerned with either precise segmentation of moving regions or accurate reconstruction of 3D scene structure. Iterative estimation of dominant planes based on optical flow was also explored for robot navigation in <ref type="bibr" target="#b39">[41]</ref>. A recent approach that addresses similar problems is from <ref type="bibr" target="#b24">[26]</ref>. A divisive information-theoretic algorithm is employed to group semantically related features and AdaBoost is chosen to integrate all features for recognition. Realistic data was considered but the training complexity of this approach is high.</p><p>A number of recently proposed methods attempt to deal with complex scenes, background clutter, occlusion, large variations in appearance and motion. The volumetric features have been extended in <ref type="bibr" target="#b17">[19]</ref> to handle occlusion of individual features. Colour based segmentation of spatio-temporal volumes was applied to the video and the volumetric segments were then efficiently matched to models. Sequences without significant camera motion were considered and manual interaction was required.</p><p>The complementary nature of appearance and motion features was also emphasised in <ref type="bibr" target="#b13">[15]</ref>. Their main concern however was the inaccurate alignment of action locations in space-time volumes, which makes it difficult to match test examples against the training data. A static camera was assumed, and background subtraction used, for extracting appearance features. Their main contribution was a simulated annealing multiple instance learning SVM that iteratively evaluates action candidates and allows relabelling of training examples if their scores are low, to build a more accurate classifier. The classification however requires initialisation of action location, which was achieved with a head detector.</p><p>Another related approach was proposed in <ref type="bibr" target="#b15">[17]</ref>. They focused on similar problems to our paper but addressed them differently. Spatio-temporal interest points were considered as votes in the video volume with weights estimated as log likelihood ratios calculated from positive and negative matches. Fast matching was done using Locality Sensitive Hashing <ref type="bibr" target="#b11">[13]</ref>. The main contribution however was in efficient extension of sliding window with branchand-bound to search in a 3D volume for the sub-volumes of actions. Our approach avoids the problem of searching for subvolumes as the action locations are indicated by local maxima in the voting space and not by undefined volumes. Moreover, the problem of camera motion is not addressed there. A biologically inspired system based on a neurobiological model of motion processing was investigated in <ref type="bibr" target="#b14">[16]</ref> as an extension of their image classification method. The system consists of a hierarchy of spatio-temporal feature detectors of increasing complexity followed by the SVM classifier. Background subtraction was applied to reduce the area over which the proposed high complexity features were computed.</p><p>An efficient prototype based approach for action recognition robust to moving cameras and dynamic background was proposed in <ref type="bibr" target="#b23">[25]</ref>. Tree based prototype matching and look up table indexing was adopted to search in a large collection of prototypes. The proposed representation is a binary silhouette combined with the motion fields computed for bounding box of human performing actions. A human detector or background subtraction is necessary for this system to provide bounding boxes. Moreover, the motion compensation is done by a simple median of flow vectors which assumes single plane normal to the camera axis.</p><p>In contrast to the methods discussed above our approach does not require additional object detectors or manual interaction, does not assume simple background model or static camera, and last but not least it is applicable to any object-action category, including humans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Overview</head><p>The main components of the system are illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>. The representation of object-action categories is based on multiple vocabulary trees. Training of object-action representations starts with feature extraction which includes scale invariant feature detection, motion estimation, and region description discussed in Section 2. Motion vectors of features are then compensated with dominant motion which is estimated in Section 3. These features are used to build appearance-motion model of object-action categories that is discussed in Section 4. Fast matching is done by Approximate Nearest Neighbour search with randomised kd-trees as explained in Section 5. Section 5.2 discusses the recognition where features and their motion vectors are first extracted from the query sequence and matched to the trees. The features that match to the tree nodes accumulate scores for different categories and vote for their locations and scales within a frame. The learning and recognition process is very efficient due to the use of many trees and highly parallelised architecture discussed in Section 5.3. Finally, experimental results are presented in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Local features</head><p>Local features with associated motion vectors are the crucial elements of our object-action representation. The features are localised by four different detectors and described by Gradient Location-Orientation Descriptor. The dimensionality of descriptors is reduced and the features are tracked to obtain motion vectors. The details of these operations are given in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Interest points</head><p>Given the frames of action sequences we apply various stateof-the-art interest point detectors: Maximally Stable Extremal Regions (MSER) <ref type="bibr" target="#b27">[29]</ref>, Harris-Laplace and Hessian-Laplace <ref type="bibr" target="#b32">[34]</ref>, and Pairs of Adjacent Segments (PAS) <ref type="bibr" target="#b8">[10]</ref>. Thus, we extract four types of image features which represent complementary information from the image i.e. blobs, junctions and contours. These features proved very powerful in many recognition systems <ref type="bibr" target="#b21">[23,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b37">39]</ref> and we adopt them by introducing modifications discussed below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1.">Harris-Laplace and Hessian-Laplace</head><p>Hessian-Laplace extracts various types of blobs and Harris-Laplace finds corners and other junctions. The features are robust to image noise, blur, photometric and geometric changes and their number can be controlled with a single parameter <ref type="bibr" target="#b29">[31,</ref><ref type="bibr" target="#b28">30]</ref>. The density of features in the image is crucial for reliable recognition in particular when the object of interest occupies only a small part of the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2.">Maximally Stable Extremal Regions (MSER)</head><p>This detector provides very stable and repeatable blob-like features of high localisation and scale accuracy. However, the evaluation in <ref type="bibr" target="#b29">[31]</ref> revealed that the recognition performance suffers from low number of such features. To increase this number we run the MSER detector at two image scales of a grayvalue image, red-green as well as blue-yellow projected images if colour is available. This results in six versions of the input image which are then processed with the same MSER detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3.">Pairs of Adjacent Segments (PAS)</head><p>Inspired by the method proposed in <ref type="bibr" target="#b8">[10]</ref> we design shape features that explicitly handle background clutter. It is based on edges on the boundaries of segments within the measurement region. We first apply Canny detector and efficient graph based segmentation <ref type="bibr" target="#b6">[8]</ref> to the input image. The outcome of these two methods is then combined to select edges of the boundaries of neighbouring segments. The connected edges are separated into contours that belong to the same segmented region (see Fig. <ref type="figure" target="#fig_2">3c-e</ref>) and into edge primitives that do not exhibit sudden gradient orientation changes. Each pair of connected edge primitives form an individual feature (e.g., Fig. <ref type="figure" target="#fig_2">3f</ref>). The average of edge point coordinates determines the centre of the feature and the square root of the determinant of their covariance matrix gives an estimate of the feature scale (size). Thus, the scale estimation is not affected by the background unlike in the interest points (cf. Fig. <ref type="figure" target="#fig_2">3b</ref>). Using pairs of edges provides robustness to background clutter but reduces the discriminative power of a feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Feature descriptors</head><p>Each feature is described by a set of parameters: (x, y) -location, r -scale, which determines the size of the measurement region, b -dominant gradient orientation angle, which is estimated from gradient orientations within the measurement region <ref type="bibr" target="#b25">[27,</ref><ref type="bibr" target="#b32">34]</ref>. Given these parameters the measurement region can be normalised to a fixed size and orientation and we can compute a Gradient Location-Orientation Histogram (GLOH) <ref type="bibr" target="#b32">[34]</ref>. The descriptor has different parameters adapted to feature type. Interest points (Hessian-Laplace, Harris-Laplace and MSER) are described with 17 bins in log-polar location grid and 8 orientation bins over 2p range of angles, thus 136 dimensional descriptor (cf. Fig. <ref type="figure" target="#fig_2">3g</ref>). The segment features use 17 bins in log-polar location grid and six orientation bins over p range of angles, resulting in 102 dimensions. The direction of the edge gradient is discarded and orientation only is used in PAS, hence p range of angles. There are 100s up to 1000s of features per frame in contrast to other action recognition methods <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b42">45,</ref><ref type="bibr" target="#b46">49]</ref> which extract only 10s of spatio-temporal features but do not deal with sequences containing more than one action, camera motion or complex background. An example frame with a subset of detected interest points is displayed in Fig. <ref type="figure">4a</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Dimensionality reduction</head><p>High dimensional features are very discriminative, slow to compute the similarity distance and make data structures for fast nearest neighbour search ineffective. Dimensionality reduction makes it possible to avoid these problems and has positive effect on the robustness of the descriptor to intra-class appearance variations. Recently, a dimensionality reduction techniques more effective than PCA, yet based on global statistics was introduced in <ref type="bibr" target="#b31">[33,</ref><ref type="bibr" target="#b34">36]</ref>. Two global covariance matrices C and b C are estimated for correctly matched and non-matched features, respectively, from image pairs representing the same scenes from different viewpoints:</p><formula xml:id="formula_0">C ¼ X l ij ¼1 ðx i À x j Þðx i À x j Þ T ; b C ¼ X l ij ¼0 ðx i À x j Þðx i À x j Þ T<label>ð1Þ</label></formula><p>where x are feature vectors; l ij is a label equal to 1 if x i and x j are correctly matched features and 0 otherwise. The matrices are then used to compute a set of projection vectors v by maximizing:</p><formula xml:id="formula_1">JðvÞ ¼ v T b Cv v T Cv<label>ð2Þ</label></formula><p>Since correctly matched features are difficult to identify in category recognition we adopt a different strategy. Given a feature extracted by a combination of detector-descriptor (e.g., MSER and GLOH) we generate 20 additional features by rotating, scaling and blurring its measurement region and compute new descriptors. In this way we generate matched regions by warping patches and intra-class covariance matrix C is then estimated from their descriptors. Non-matched features are randomly collected from the entire set to estimate inter-class covariance matrix b C . Maximising the ratio of matrices gives the projection vectors v in a similar way to popular Fisher Discriminant Analysis <ref type="bibr" target="#b9">[11]</ref>.</p><p>We select a number of eigenvectors associated with e largest eigenvalues to form the basis for linear projections. The parameter e is different for various feature types and automatically determined by the sum of eigenvalues which is equal to 80% of the sum of all eigenvalues. This typically results in 10-30 dimensions out of original 102 and 136 of GLOH. The descriptors from warped regions are also used to estimate the similarity threshold that is used for matching in motion estimation as well as for the recognition in Section 5. We build a histogram of similarity distances (Euclidean) for matched and non-matched descriptors to select the threshold between them. Note that each feature type (detector-descriptor) is projected with different set of vectors to reduce the number of dimensions and the features are compared only within the same type. It leads to great reduction of memory requirements, increase of efficiency and more importantly it makes the kd-tree structures very effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Motion estimation</head><p>To represent an action it is essential to use a reliable local motion estimator. There has been a lot of work in the area of feature tracking with important contributions from <ref type="bibr" target="#b43">[46]</ref>. We follow this approach and use a pyramidal implementation of the classical Lucas-Kanade Tracker (KLT) <ref type="bibr" target="#b26">[28]</ref>. The optical flow is computed at the lowest level of the pyramid and then propagated to the higher levels. Thus the lower level provides an initialisation for tracking at the higher resolution. The number of pyramid levels is four and the used patch size is 15 Â 15 pixels. These parameters provide a good tradeoff between the accuracy of the motion estimation and the robustness to large motion, zoom, and light changes. This operation gives an estimate of motion magnitude and direction for each feature extracted in the previous stage.</p><p>In addition to the optical flow based tracking, which is efficient but often gives erroneous motion vectors, we apply a descriptor based verification to reject the uncertain motion vectors. Every match candidate indicated by the optical flow vector is verified by computing similarity score between descriptors using the Euclidean distance. If the distance is larger than the threshold the pair of matches is removed from the set. A substantial number of points which are due to occlusion and background clutter are removed, it is therefore crucial to start with a large number of features so that a sufficient number remains after tracking and matching. Typically there are on average 1500 features per frame (320 Â 240) from all the detectors. Note that some features do not move. These features are labelled static and serve for refinement of object-action hypotheses, which is discussed in Section 5.2. Fig. <ref type="figure">4</ref> shows example frames with feature tracks for various types of camera motion. Note that for the handheld camera displayed in Fig. <ref type="figure">4b</ref> there is noticeable motion even though the camera was held still. This shows the necessity of using multi-plane motion compensation otherwise the field of possible applications is limited to fixed cameras only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Motion compensation</head><p>Given a number of features with associated motion vectors extracted from consecutive frames the problem is to separate the local motions characterising the actions from the dominant camera motion. Frequently used single plane assumption does not hold in many applications as there is often the ground plane and the background plane in the outdoor scenes or even more planes in the indoor scenes. This requires image segmentation into multiple dominant motion planes which can then be used to correct the local motion vectors. We approach this problem by combining colour based image segmentation, already exploited for feature extraction (cf. Section 2), with estimation of dominant homographies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Image segmentation</head><p>The purpose of the segmentation maps is to identify features that potentially belong to the same physical surface. Different planes are often separated by colour or intensity gradients which are detected by the segmentation method. Fig. <ref type="figure">5</ref> shows examples of segmented frames with displayed keypoint features. Given the detected features and the segmentation mask we allocate features-to-segments. A feature is allocated to a segment if a disc of five pixel radius centred on this feature overlaps with the segment. Feature-to-segment allocation is represented by S f matrix (cf. Fig. <ref type="figure" target="#fig_3">6a</ref>), where S f (S m ,f i ) = 1 indicates that feature f i belongs to segment S m . Note that a feature can belong to several segments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Homography estimation</head><p>Homography is the appropriate model as perspective distortions are frequent in both indoor and outdoor scenes. The homography estimation is done for every segment with more than 10 features by applying RANSAC (RANdom SAmpling Consensus) to the features allocated to the segment. RANSAC samples 4 points at every iteration and estimates the homography. The homography is obtained if the number of inliers from the segment does not change for more than 10 iterations or the maximum number of 100 iterations is reached. After processing the segments we obtain a list of homographies (cf. Fig. <ref type="figure" target="#fig_3">6</ref>)b. The result of this operation is also matrix H f (cf. Fig. <ref type="figure" target="#fig_3">6c</ref>) which indicates features from the whole image that are inliers to the homographies. Given the matrices the task is now to select the homography with the largest number of inliers. Matrix H S is the product of matrices S f and H f , and represents the number of inliers for every segment S m and homography H n (cf. Fig. <ref type="figure" target="#fig_3">6d</ref>). Next, the inliers are added column-wise and the dominant homography is indicated by the column with largest number of inliers: maxð</p><formula xml:id="formula_2">P h Þ.</formula><p>Given the dominant homography we iteratively merge segments which contain more than 80% of inliers to this homography H S ðS m ; H n Þ= P s P :8, The merged segments are then removed from matrix {S {f by removing the corresponding rows. New matrix H S is produced and the procedure is repeated to find the second dominant homography. This is repeated for three dominant homographies if there are more than 20% of the initial number of features remaining in S f and H f matrices. The remaining small segments are merged with the surrounding areas. These are usually the outliers representing local action motion. Fig. <ref type="figure" target="#fig_4">7</ref> shows the dominant plane segmentation for the frames presented in Fig. <ref type="figure">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Motion compensation</head><p>Motion compensation is crucial when there is a camera motion but it is often difficult to identify the foreground and the background from purely data driven segmentation in particular when there are close up views of object-actions. Given the dominant planes and their homographies between frames we can obtain the plane motion for any point in the frame. We subtract the plane motion from the motion of all local features that were allocated to this plane during the homography estimation. In this way only the motion vectors different from the plane motion remain. This is done for every dominant plane in the frame. The same holds for features on the boundaries between two planes where both homographies are valid. The main risk with this approach is that one of the object-actions can be identified as a dominant plane and its motion would then be canceled. Some techniques avoiding this issue can be found in <ref type="bibr" target="#b12">[14]</ref>. The impact of this on our action recognition results is not significant as: (1) we use only three domi-Fig. <ref type="figure">5</ref>. Segmentation results with displayed features for different frames. The features within the obtained regions are used to estimate homography per segment. Note that the regions belong to ground plane, background or individuals performing actions. nant planes at most; (2) the local motion is more discriminative than the global one. For example, the global forward motion in running or jogging is the main source of ambiguity between these actions, therefore cancelling it and recognising from local motion only can be beneficial. The proposed method is reliable as long as the background is piece-wise planar and a plane contains more features that move according to a perspective model than the foreground. This is a frequent scenario which causes problems if no motion compensation is done. In crowded scenes acquired from a short distance the clutter is high and inconsistent motion of individuals introduces noise. In that case the number of features that follow perspective motion is small, the estimate is deemed unreliable and no compensation is done.</p><p>Motion compensated features in our example frames are displayed in Fig. <ref type="figure" target="#fig_5">8</ref>. It can be observed that only local motions remain which are then used to recognise the actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Action representation</head><p>This section discusses the representation of action object categories used in our system. We first present the model that captures relations between features and then discuss the vocabulary construction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Shape model</head><p>Our category representation consists of object parts, their motion directions and relations between parts. The parts are visual words (local features) represented by appearance descriptors with associated motion vectors (cf. Section 2). We use a star shape model to capture the relations between object and its moving parts. Similar model was successfully used for object detection in <ref type="bibr" target="#b21">[23,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b30">32]</ref> and it is adapted here to actions. The training frames are annotated by bounding boxes which allow to estimate the size and the centre of the training example, hence feature location, scale and orientation with respect to the object. Fig. <ref type="figure" target="#fig_6">9</ref> shows the representation with local features and parameters estimated during training which are then used to recognize and localize an object-action instance. Each feature contains occurrence parameter vector r = [a, x, y, r, d, b, c, /, l]; a -label of the action category, (x, y) -feature coordinates, r -scale (blue<ref type="foot" target="#foot_1">1</ref> circles in Fig. <ref type="figure" target="#fig_6">9</ref> or Fig. <ref type="figure" target="#fig_2">3</ref>), d -distance to the object centre, b -dominant gradient orientation angle, c -angle between the vector to the object centre and the gradient orientation, / -angle between the motion direction and the gradient orientation, and l -motion magnitude. Angle c is invariant to similarity transformations. With these parameters we can construct a local coordinate system for every query feature. The local coordinate system is defined by the origin x q , y q , orientation of the axis given by angle b q and scale r q . Thus, given a single match to a model feature we can hypothesise the action label, calculate location as well as the pose of an object-action instance in the local coordinate system. Every query feature can draw a hypothesis if its appearance and motion is similar to the model feature. Given a   match between query feature q and model feature m the centre of the hypothesis is computed by:</p><formula xml:id="formula_3">H 1 H M S 1 S M M 1 H H f 1 f F 0 1 1 1 0 0 1 f F 0 f 1 1 0 1 1 S M S 1 S f . f H = H s S f H f H s 4 34</formula><formula xml:id="formula_4">x h y h ¼ x q y q " # þ r q r m d m cosðb q À c m Þ sinðb q À c m Þ " #<label>ð3Þ</label></formula><p>where indexes q and m indicate the query and the model feature parameters, respectively. (x h , y h ) is the location of the action hypothesis within the image, r q /r m is the scale of the hypothesis w.r.t the model, and b q À b m is its orientation angle (see Fig. <ref type="figure" target="#fig_6">9c</ref>).</p><p>The aspect ratio of the category is learned from the training data and represented by the average of all training examples. This is multiplied by the scale of the hypothesis during recognition.</p><p>The angle between the dominant gradient orientation and the motion direction of a feature is characteristic for a given time instance of an object-action category and it is used during recognition together with the appearance descriptor to establish a match between a query feature and a model feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Vocabulary construction</head><p>In the proposed representation the appearance descriptor, which requires more memory than occurrence parameters, can be shared among various action categories (see Fig. <ref type="figure" target="#fig_6">9</ref>) resulting in a compact representation. A visual word can then be assigned multiple occurrence parameter vectors r. There are many methods for constructing a visual vocabulary, most of them based on clustering of features <ref type="bibr" target="#b21">[23,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b37">39]</ref>. In our experiments with large vocabularies (P1M) the visual words obtained by clustering descriptors or by randomly sampling descriptors lead to very similar performance, however the latter is much less computationally intensive. We therefore randomly sample 400,000 descriptors from each of the four detector-descriptor types, which form a visual vocabulary of 1.6M words. Additional advantage of the randomly built set is its redundancy, which significantly improves the matching accuracy. The size of the vocabulary is limited by the efficiency of matching, which is discussed in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Recognition with search trees</head><p>In this section we present our approach to fast matching of features to the vocabulary. We exploit the idea of randomised kdtrees successfully used in image retrieval <ref type="bibr" target="#b40">[42]</ref>. In contrast to <ref type="bibr" target="#b33">[35]</ref> where the objective was to cluster and compress the amount of information in the feature set, we now focus on the Approximate Nearest Neighbour (ANN) search <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b31">33]</ref> with kd-trees, which is much more efficient than with flat codebooks in <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b21">23,</ref><ref type="bibr" target="#b35">37,</ref><ref type="bibr" target="#b36">38,</ref><ref type="bibr" target="#b42">45]</ref> or metric trees in <ref type="bibr" target="#b31">[33,</ref><ref type="bibr" target="#b33">35,</ref><ref type="bibr" target="#b37">39]</ref>. The loss of matching accuracy due to ANN is compensated by the large number of trees, redundancy in the vocabulary, and significantly increased number of leaf nodes (visual words) that can be searched efficiently.</p><p>The process discussed here is applied to each of the four vocabulary types. We start by partitioning the vocabulary into subsets of equal sizes, again by random sampling. The number of partitions is a predefined parameter and it is directly related to the number of available CPUs. The more variability of the descriptors in the partition the better matching accuracy, hence random sampling. We therefore build a kd-tree for every partition and use the set of trees for recognition as discuss in Sections 5.1 and 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Tree construction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1.">Kd-tree</head><p>A kd-tree structure <ref type="bibr" target="#b31">[33,</ref><ref type="bibr" target="#b40">42]</ref> is built for each subset of visual words. The features are already projected to 10-30 discriminative dimensions, therefore we randomly select one dimension d i and use its median value as threshold t i . The index of the selected dimension and the threshold is recorded in the top node and the data is split in two children partitions using this threshold. The selection of a dimension and threshold for the children partitions continues as long as the variance in the dimension exceeds a threshold. This threshold is not critical and can be set very small such that every leaf node contains a single feature. Larger thresholds may slightly accelerate the tree construction. See Fig. <ref type="figure" target="#fig_7">10</ref> for illustration. An alternative approach to fast search can be Locality Sensitive Hashing <ref type="bibr" target="#b11">[13]</ref>. Our features however are low dimensional for which kd-trees are preferred choice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2.">Approximate Nearest Neighbour (ANN) matching</head><p>Given a query feature, its value in predefined dimension d i is first compared to threshold t i in the top node. Depending on the outcome a different dimension is considered from the child cell in exactly the same way. This continues until the query feature reaches the leaf node of the tree (cf. Fig. <ref type="figure" target="#fig_7">10</ref>). There are many different search strategies that can be applied here such as priority queues, branch-and-bound, limited number of visited nodes. We .  use the error bound which terminates the search if the distance from the closest cell to the query exceeds d = d(f q , f NN )/(1 + ),</p><note type="other">.</note><p>where d(f q , f NN ) is the distance between query feature f q and nearest neighbour (NN) found so far f NN ; is a positive termination parameter. In other words this approach guarantees that no subsequent point to be found can be closer to f q than d. Detailed discussion on the selection of and performance implications can be found in <ref type="bibr" target="#b31">[33]</ref>. Multiple trees allow to generate more matches for object-action localisation which significantly improves the recognition performance. Furthermore, the proposed architecture of multiple trees makes it possible to parallelise the processing as the matching can be performed simultaneously in all trees. We use this technique during training and recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3.">Training</head><p>The training consists of estimating importance weights of the leaf nodes (visual words) and assigning occurrence parameters. Each leaf node contains a vector of weights w n = [w n,0 ,. . .,w n,a ,. . .], where weight w n,a indicates how discriminative node n is for category a. The weight estimation is performed by matching features from all positive and negative examples to the trees. This returns one nearest neighbour match per tree for every query feature. We use all NN matches to estimate the weights and occurrences. We first estimate the probability to match node n by features from positive examples p n;a ¼ Fn;a Fa , which is the ratio of the number of training features F n,a from a that matched to this node to the number of all features in this category F a . We set F n,a = 1 if no feature matched to node n. In addition to the positive training data we also use background category b which consists of image examples of scenes without our object-action categories. Background data is used to estimate p n;b ¼</p><formula xml:id="formula_5">F n;b F b</formula><p>. The weight of the node is then given by w n;a ¼ logð p n;a p n;b Þ. In addition to the weight vector w n , each leaf node contains a list of parameter vectors r n,i = [a n,i ,d n,i ,r n,i ,b n,i ,c n,i , / n,i ,l n,i ] from the features that matched to this leaf node. These parameters allow us to hypothesise the positions and scales of object-actions during recognition as discussed in Section 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Recognition</head><p>Given the trained trees the recognition starts by extracting features from the query sequence. In addition to the process described in Section 2, we compute a symmetric copy of each query feature to handle asymmetric object-actions. This is done by swapping bins in the GLOH descriptor and inverting parameters (x q , b q , c q , / q ) with respect to the dominant gradient direction. In this way with very little overhead we can handle various actions performed in symmetric directions, e.g., walking, even if the training set contains examples in one direction only. Note that this handles only actions symmetric to those captured in the training data i.e. walking towards camera result in different appearance than moving to the left of the camera since the viewpoint changes. Next, the number of dimensions is reduced with the projection vectors estimated during training (cf. Section 2) and features are matched to the trees. Each query feature f q from frame t accumulates weights for different categories from all the leaf nodes (visual words) it matches to: w a;t;f ¼ P n k a;/ w n;a;f , where k a,/ is the fraction of occurrence vectors r n,i of class a for which the motion angles agree j/ q À / n,i j &lt; T / . If f q is static or motion angles are different the weight is labelled static.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1.">Sequence classification</head><p>To classify the sequence we integrate the weights over motion features and frames: w a ¼ P t P f w a;t;f . In contrast to <ref type="bibr" target="#b36">[38]</ref> we do not use the static features here, otherwise they dominate the score and we cannot distinguish between similar categories, e.g., running and jogging. The action is present if its accumulated weight w a exceeds a fixed threshold. Thus, the classification is based on features in motion only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2.">Action localisation</head><p>To find the location and size of the object performing an action we use the occurrence vectors r n,i associated with every leaf node.</p><p>A 3D voting space (x, y, r) is created for each object-action category (cf. Fig. <ref type="figure" target="#fig_6">9d</ref>). The voting space is quantised such that the locations bins x, y are 10 pixels large and the scale is distributed exponentially r n = 1.2r nÀ1 , according to scale space theory <ref type="bibr" target="#b28">[30]</ref>. Query feature f q matches to a number of leaf nodes. Parameter vector r q of query feature f q that matches to leaf node n casts a vote in the 3D space for each parameter vector r n,i stored in that node if the motion angles are similar j/ q À / n,i j &lt; T / . The weight w a,t,f is equally distributed among all the motion votes (matched leaf nodes) cast by this feature and the corresponding bins in the voting space are incremented by these votes. The coordinates of the bins are computed with Eq. ( <ref type="formula" target="#formula_4">3</ref>). Features from five neighbouring frames in each time direction are included to populate the voting space. Once all the features in motion cast their votes, the hypotheses are given by local 3D maxima in the voting space (cf. Fig. <ref type="figure" target="#fig_6">9d</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3.">Refinement</head><p>Local maxima in the voting spaces are often drawn by only a few features in motion. We use static votes to improve the robustness of the recognition process. The 3D space bin which corresponds to the local maximum as well as its neighbouring bins are incremented by the weights of the static votes pointing to these bins. Thus, the motion based hypothesis is supported by the object appearance. This often changes the ranking of the hypotheses and improves the localisation accuracy. If the action hypothesis is due to noise in the motion field, there are usually few additional static features that contribute to the score. The scores are thresholded to obtain the final list of object-actions with their positions and scales within the frame. In addition to that, from all the votes contributing to the local maximum we can compute a histogram of b q À b n,i and estimate the global pose angle which is then used to rotate the displayed bounding box. Pose angle can be incorporated in the voting space as a 4th dimension, however it results in a very sparse distribution of votes and less reliable local maxima. Pose estimation is not used for the evaluation in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4.">SVM validation</head><p>Following the trends in computer vision community we add an optional validation step based on non-linear SVM. For that a histogram of features formed by the leaf nodes (visual words) of the trees is build and used to represent the candidate. Histograms for training examples are computed in a similar way and we use up to 30 action examples per category. We compute the v 2 distance between the candidate and the training examples, which gives a 30 dimensional vector. This vector is then fed into SVM. Similar technique can be used for classification, however, we focus on the efficiency of the system and this validation step is only for the sake of experimental comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Efficient implementation</head><p>Many recognition systems work sequentially, which requires large memory and high computational power. Alternatively, GPU processors are deployed to speed up image processing. We adopt a different strategy to achieve high efficiency of training and recognition. Our system is designed such that many operations can be done independently and simultaneously. This makes it possible to parallelise the training and recognition and run it with multiple processes on a single or many machines if available. The features are extracted and partitioned into subsets and all the trees are then trained in parallel. For example, 5000 frames of action sequences give approximately 7M features of which we build 32 trees from randomly sampled subsets each containing 50000 visual words. It takes approximately 2h to train the system on eight Pentium IV 3 GHz machines but running it in a sequential way takes 26h. Estimating the training time without separating features into subsets was beyond our time constraints. Recognition takes .5s up to 3s per frame but it largely depends on the number of features extracted from the image. SVM based classification adds additional 5s. The main bottleneck of the processing is the feature extraction and motion compensation which take 90% of the processing time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experimental results</head><p>This section discusses the experimental setup, results for motion compensation and performance of our action recognition system on various data. We use several datasets to train and evaluate the performance of our system. The KTH action sequences were introduced in <ref type="bibr" target="#b42">[45]</ref> and frequently used in many action recognition papers <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b35">37,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b46">49]</ref>. We present the results for this data and compare to the others methods. However, recognition performance reported in the literature for the KTH data has already saturated on the level of 90-100%. Similar observations can be made for Weizmann action sequences <ref type="bibr" target="#b10">[12]</ref>. We therefore acquired a more challenging sequence of actions which are included in the KTH set, but performed simultaneously with more complex background, occlusion and camera motion. Example frames are displayed in Fig. <ref type="figure" target="#fig_8">11</ref>(middle and bottom rows). This sequence is named multi-KTH and is used for testing only. To train and test the system we annotated every 5th frame of each sequence with bounding boxes using an interactive interface supported by colour based Mean Shift tracking. In total, we annotated 11,464 frames from 599 sequences of six KTH categories and 753 frames from multi-KTH sequence. Automatic annotation of KTH actions was possible due to their uniform background. To test the performance of the motion compensation approach we collect four sequences in addition to multi-KTH with moving camera, multiple planes and cluttered background. These sequences show two indoor and two outdoor scenes with moving foreground objects and are made available in the Internet <ref type="bibr" target="#b51">[54]</ref>. The sequences are annotated in a similar way to the other video data, by bounding boxes on humans performing actions.</p><p>The Olympic Games are an abundant source of actions with high intra class similarities yet extremely challenging due to background clutter, large camera motion, motion blur and appearance variations. We select 10 different disciplines with various viewpoints and separate them in 17 action categories. The categories, the number of sequences and frames are summarised in Table <ref type="table" target="#tab_3">2</ref>. Image examples are displayed in Fig. <ref type="figure" target="#fig_10">12</ref>. Each sequence contains more than one period of repetitive actions. We annotated every 5th frame of each sequence with bounding boxes. We annotated 5065 frames from 166 sequences of 17 sport categories. In addition to the sequences we use images from VOC Pascal <ref type="bibr" target="#b4">[6]</ref>: 1000 pedestrians, 200 horses and 200 bicycles, to capture large appearance variations. Finally, there are 1000 background images containing urban scenes and landscapes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2.">Performance measures</head><p>We evaluate the performance in a similar way to <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b35">37,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b42">45,</ref><ref type="bibr" target="#b46">49]</ref>. We refer to this test as 'classification'. In addition to that, we report results for detection of individual actions within frames termed 'localisation'. Given the ground truth bounding box G in a frame and bounding box D output by our method, the detection is correct if the intersection/union of G and D areas is larger than .5 and the category label is correct i.e. G\D G[D &gt; :5. The detection results are presented with average precision, which is the area be- low precision-recall curve, as proposed in <ref type="bibr" target="#b4">[6]</ref>. All the experiments are done with leave-one-out cross-validation, that is we train on all sequences except one in a given category, which is used for testing to make the results comparable to <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b35">37,</ref><ref type="bibr" target="#b46">49]</ref>. Within that sequence, we perform recognition for every annotated frame and compare with the ground truth. The results are averaged for all frames and all sequences of a given action. For each analysed frame we use features from five additional frames in both time directions to populate the voting space (cf. Section 5.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Motion compensation</head><p>We first present the evaluation of the camera motion compensation on five sequences introduced in Section 6.1. We extract features, perform tracking and estimate dominant planes. To evaluate the performance we compare the average magnitude of motion for foreground and background regions before and after motion compensation. In Table <ref type="table" target="#tab_2">1</ref> we can observe a significant reduction of motion magnitude in the background while local foreground motion remains detectable and reliable. The background motion is reduced to 20% of the original motion on average and for the foreground more than 70% of motion remain. The average ratio foreground/ background magnitude before the compensation is 1.05 and increases to 4.1 for the compensated frames. This makes the action recognition task tractable even with significant camera motion. We perform a classification experiment and compare the performance of our system to the other methods <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b35">37,</ref><ref type="bibr" target="#b46">49]</ref>. In this experiment we discard the action location information and assume there is only one action category in the sequence or none as explained in Section 5.2. Table <ref type="table" target="#tab_3">2</ref> left shows the confusion matrix for the KTH data. It is interesting to observe that a system based on appearance with little motion information extracted from few frames can produce results comparable to a method that analyses entire sequence <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b35">37,</ref><ref type="bibr" target="#b46">49]</ref>. All action categories obtain high recognition score which favourably compare to the state-of-the-art results. There is 1% of misclassification between hand clapping and waving. Higher classification error of 4% occurs between jogging and running as well as walking. Other approaches also suffer from confusions between these categories. We improve the state-of-the art classification results for four of those categories, namely handwaving, jogging, walking and running (see Table <ref type="table" target="#tab_3">2 left</ref>). The results for other actions are comparable to the state-of-the art scores.</p><p>We also investigated the influence of the number of frames over which we integrate the votes. The results are high even if we use only a pair of frames as a query sequence. The recognition scores in Table <ref type="table" target="#tab_3">2</ref> are increased by up to 5% by using five neighbouring frames in each time direction, which helps in discriminating between similar categories, e.g., waving -clapping, running -jog-   ging. However, using more than 10 frames does not introduce significant improvements in these categories although for more complex sport actions in Section 6.4, 20 frames give better results.</p><p>Training on 16 sequences and testing on remaining nine, as done in <ref type="bibr" target="#b38">[40,</ref><ref type="bibr" target="#b42">45]</ref> produces similar results with a small drop of performance by 2% for running and jogging. Mean average precision of our approach is 95.3% which favourably compares also to the recent results of 93.3% reported in <ref type="bibr" target="#b15">[17]</ref>. An approach from <ref type="bibr" target="#b23">[25]</ref> gives high performance of 95.8% but it is based on a state-of-the-art human detector and holistic features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.2.">Localisation</head><p>In addition to the classification we also present the results for recognition and localisation in Table <ref type="table" target="#tab_3">2</ref> right (localisation). In contrast to many other methods our system can localise the actions and handle different actions performed simultaneously. In addition to the class label it estimates the location and size of various actions, thus the number of possible false positives per image is very high compared to the classification systems from other papers. Given that the KTH object-actions are on a uniform background with exaggerated motion, the score for boxing, clapping, is nearly as high as for the classification. The results for localisation test drop by 8-12% for walking, jogging and running mainly due to errors in size estimation (cf. Table <ref type="table" target="#tab_3">2</ref> right).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.3.">Motion features</head><p>In this test we evaluate the contribution from the features that remain in motion after global motion compensation. Some categories can be easily distinguished from a single frame without motion information due to very specific appearance of objects and background. In the context of the KTH data, static features are not discriminative enough and confusions between similar categories significantly increase if they are used for classification and initial localisation. Table <ref type="table" target="#tab_3">2</ref> (motion) shows that the detection is still high if the features in motion are used only, which corresponds to the classification and the initial localisation (cf. Section 5.2). The results further improve if the refinement with static features follows the initial localisation, which is shown in Table <ref type="table" target="#tab_3">2</ref> (localisation). Large number of static features help refine the hypotheses by increasing the score and improving estimation of pose parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.4.">Multi-KTH</head><p>For comparison we performed the localisation test on multi-KTH sequence. Fig. <ref type="figure" target="#fig_10">12</ref> (top row) shows frames of five KTH actions performed simultaneously. We used the KTH data to train the system and the detection results are displayed in Table <ref type="table" target="#tab_3">2</ref> right (multi-KTH). The results decrease by 14% for hand-waving up to by 20% for boxing. This demonstrates the difference in system performance one can expect when more realistic data is used. The lower recognition rate than for KTH data is due to occlusion, camera motion, panning and zooming as well as differences between the background in training and testing sequences. The drop is even more significant when no camera motion compensation is used (see bottom row Table <ref type="table" target="#tab_3">2</ref> right). Motion vectors are then unreliable and correct results are obtained only when the camera does not move. Fig. <ref type="figure" target="#fig_8">11</ref> shows example frames with the recognition results displayed by colour boxes for different actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Sport actions</head><p>We demonstrated that our system can handle basic actions on static background or with camera motion. However, the real challenge is to recognise and localise real world actions acquired in an uncontrolled environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.1.">Appearance-motion</head><p>Figs. 1, and 12 (rows 2-4) show examples from 17 categories of sport actions. We perform the classification and localisation tests as described in the previous section and show the results in Table <ref type="table" target="#tab_4">3</ref>. Sport actions give more realistic estimates of recognition capabilities and the scores are significantly lower than for the KTH data. Some categories can be reliably recognised from static features only, e.g., weight lifting or rowing. However, for the majority of object-actions, motion information acts as focus-of-attention and allows to discard many features from the background. Note that it also excludes the context on which many image classification methods rely. We found that only 5-10% of query features are correctly matched with respect to both, appearance and motion. It is therefore essential to have a large number of features extracted from the query frames such that the initial voting is robust to noise. Similarly, static images used for training are essential for .93 <ref type="bibr" target="#b35">[37]</ref> 1.0 <ref type="bibr" target="#b35">[37]</ref> .75 <ref type="bibr" target="#b46">[49]</ref> .90 <ref type="bibr" target="#b3">[5]</ref> .88 <ref type="bibr">[</ref> capturing large appearance variations. We observed an improvement of .09 for horse categories by using additional static data for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.2.">Static features</head><p>This test corresponds to recognition based on the appearance only. It confirms the observations from the KTH data that static features tend to dominate in the classification and the performance for all categories is low (see Table <ref type="table" target="#tab_4">3</ref> (static only)). For example, for horse ride and jump, static features draw many false hypotheses due to significant clutter in these scenes. The motion information improves the results by 15% on average. Unfortunately, features in motion can be significantly affected by motion blur which occurs in some categories, e.g., gymnastics. Robustness to such effects is improved by using large number of features extracted with different detectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.3.">SVM validation</head><p>Finally we estimate the improvements given by the SVM based validation. The top 100 local maxima detected in the Hough space are used as hypotheses that are then evaluated by the SVM classifier. The SVM is trained with features from 20 frames per category. We observe farther improvement by 4% compared to localisation accuracy based on voting only in Table <ref type="table" target="#tab_4">3</ref> (SVM). The efficiency however is reduced as distances between high dimensional histograms of 1.6M bins have to be computed for each training example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.">Discussion</head><p>The proposed system consists of a number of components that have been adapted to action recognition. The components are controlled by a number of parameters that affect the performance. In summary the observations made from our experiments are:</p><p>The quality and quantity of local features is the main factor affecting the reliability of motion compensation and objectaction representation. These are controlled by few parameters such as the feature strength threshold, the number of histogram bins in the descriptor, the number of dimensions remaining after dimensionality reduction, and the similarity threshold for rejecting false matches. The parameters have been optimised by maximising matching score between pairs of images as proposed in <ref type="bibr" target="#b32">[34]</ref>. Camera motion compensation is of crucial importance in videos with complex background. Image segmentation helps in detecting multiple planes in the image but the method is relatively insensitive to the segmentation output as long as the number of segments is significantly larger than the number of planes. The size of the vocabulary has been arbitrarily chosen to 1.6M which is a tradeoff between the matching efficiency, memory limits and the size of the training data. Larger codebooks may be necessary when dealing with many diverse action categories. The quality of matches is also controlled by the approximation error in nearest neighbour search. The recognition performance drops if the approximation is below 60%. The search efficiency is reduced if the accuracy of more than 90% is set. The selection of the error parameter is mentioned in Section 5 and discussed in detail in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b31">33]</ref>. The number of frames used for recognition as well as the quantisation of the voting space depend on the average number of features per frame and the accuracy of action localisation required from the system. The number of features directly affects the number of votes that populate the voting space, thus the reliability of local maxima. The larger number of votes the finer quantisation steps can be used to discriminate between closely neighbouring object-actions in the video. Finally, sequential use of motion and static features, rather than simultaneous or only one of these, has large impact on the recognition performance as demonstrated by the results on several databases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>In this paper we have presented and evaluated an approach to action recognition via local features, tracking and camera motion compensation. A number of recent action recognition methods have been reviewed and their limitations have been discussed.</p><p>The system is capable of simultaneous recognition and localisation of various object-actions within the same sequence. It works on data from an uncontrolled environment with camera motion, background clutter and occlusion. The key idea here is the use of a large number of low dimensional local features which capture joint appearance-motion information and allow for efficient recognition via randomised search trees.</p><p>The proposed approach is one of very few that explicitly address the problem of camera motion. Our motion compensation technique has been designed to make use of the same features as those in the subsequent action representation and recognition. The experiments show that the ratio of local to global motion significantly increases after the compensation which simplifies the localisation and recognition task.</p><p>We have conducted extensive experiments for a large number of real action categories and demonstrated high accuracy of the system. The system obtains an excellent performance on standard test data, compared to other approaches and improves upon stateof-the-art results. We have also presented results on a challenging video sequence including various actions performed simultaneously in uncontrolled environment. We have applied the proposed approach to sport action recognition but its design allows for any type of appearance-motion categories. We have demonstrated that the approach based on large numbers of static features with motion vectors can compete with state-of-the-art methods relying on spatio-temporal features and non-linear SVM classifiers. The data and its annotation is made available at <ref type="bibr" target="#b51">[54]</ref> and may serve as a benchmark for methods addressing the problem of camera motion and background clutter.</p><p>Possible improvements can be made in local motion estimation which is very noisy, in particular for small objects and for scenes with camera motion. Another direction to explore is tracking of individual features over longer time period to capture complex motion. This would also make the representation more discriminative and help in resolving ambiguities between similar actions, e.g., jogging and running. Finally, the proposed system can be extended to recognise static as well as moving objects simultaneously using appearance and motion information when available.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Examples of an object-action category from frequently used action recognition benchmark and realistic example of the same action category. Note the clutter, occlusion, possible viewpoint change and appearance variations.</figDesc><graphic coords="2,156.56,68.03,132.20,98.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The main components of the feature extraction and the recognition system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Local features. (a) Image. (b) Example of interest point with measurement region. (c) Canny edges. (d) Segmented region. (e) A connected edge with different segments in colour. (f) PAS feature. (g) Location grid of GLOH descriptor.</figDesc><graphic coords="4,53.84,67.91,491.81,63.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Inliers selection. (a) Feature-to-segment matrix of inliers. (b) Homoraphy estimation from segment features. (c) Feature to homography matrix of inliers. (d) Combined segment and homography matrix of inliers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Result of merging regions from Fig. 5 that have similar homographies between frames. (a) Static camera. (b) Handheld camera with small motion. (c) Camera panning. (d) Zooming. Note that the merging resulted in correctly separated ground and background planes.</figDesc><graphic coords="6,82.21,206.65,439.72,90.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Motion compensated frames. Compare with frames in Fig.4, respectively. Note that only trajectories of features that differ from the plane motion remain.</figDesc><graphic coords="6,82.21,354.05,439.72,79.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Object-action representation. Examples of features with motion-appearance parameters for jogging (a) and boxing (b). (c) Local coordinate system defined by a query feature and hypothesised object centre based on a single match between the query feature and the model. (d) Voting space for one object-action category with votes, where the disc size illustrates the weight of the vote and the cluster of votes indicates an action centre.</figDesc><graphic coords="7,138.74,67.91,197.72,117.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>6. 1 .</head><label>1</label><figDesc>Experimental settings 6.1.1. Data</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Example results for KTH action sequences (top row) and for multi-KTH sequence (middle and bottom row). Different actions are colour coded: red-walking, greenhand-waving, yellow-hand clapping, blue-boxing, black-jogging, magenta-running. Note the scale and the viewpoint change from frame (a) to frame (d).</figDesc><graphic coords="9,44.00,468.71,496.49,248.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Examples of object-action categories with correct detections in yellow and false positives in red. (Top row) Frames from multi-KTH sequence. (Row 4) Pose angle was estimated only for this sequence and all recognition results are for fixed orientation. Some of the segment features that contributed to the score are displayed in yellow.</figDesc><graphic coords="10,53.84,67.91,496.15,261.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1</head><label>1</label><figDesc>Motion compensation results for individual sequences. The table shows average motion magnitude in pixels. Motion reduction is the fraction of motion that remains after the compensation.</figDesc><table><row><cell>Sequence</cell><cell cols="3">Without motion compensation</cell><cell></cell><cell></cell><cell cols="3">With motion compensation</cell><cell></cell><cell></cell><cell>Motion reduction</cell></row><row><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell></cell></row><row><cell>Foreground</cell><cell>6.3</cell><cell>7.5</cell><cell>9.2</cell><cell>.00</cell><cell>10.7</cell><cell>5.6</cell><cell>4.9</cell><cell>5.9</cell><cell>.00</cell><cell>5.8</cell><cell>.7</cell></row><row><cell>Background</cell><cell>3.2</cell><cell>9.6</cell><cell>12.9</cell><cell>3.7</cell><cell>14.1</cell><cell>.7</cell><cell>1.1</cell><cell>2.2</cell><cell>.7</cell><cell>4.2</cell><cell>.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc>Human action recognition results. (left) Confusion matrix for KTH actions. (right) Results for KTH and multi-KTH sequences and comparison with other methods.</figDesc><table><row><cell>Action</cell><cell>Hand</cell><cell>Hand</cell><cell cols="5">Boxing Jogging Walking Running Test</cell><cell>Hand</cell><cell>Hand</cell><cell>Boxing</cell><cell>Jogging</cell><cell cols="2">Walking Running</cell></row><row><cell></cell><cell>clapping</cell><cell>waving</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>clapping</cell><cell>waving</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">clap-ping .99</cell><cell>.01</cell><cell>.00</cell><cell>.00</cell><cell>.00</cell><cell>.00</cell><cell>classification</cell><cell>.99</cell><cell>.98</cell><cell>.99</cell><cell>.92</cell><cell>.94</cell><cell>.90</cell></row><row><cell>wav-ing</cell><cell>.01</cell><cell>.98</cell><cell>.01</cell><cell>.00</cell><cell>.00</cell><cell>.00</cell><cell>state-of-the-art</cell><cell>1.0 [49]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc>Action classification and localisation results for Olympic Games categories. The bottom rows show results for static features only and for the system with final verification by the SVM.</figDesc><table><row><cell>37]</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>K. Mikolajczyk, H. Uemura / Computer Vision and Image Understanding 115 (2011) 426-438</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>For interpretation of colour in all figures, the reader is referred to the web version of this article.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This research was supported by UK EPSRC EP/F003420/1 Grant and BBC Research and Development.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Human motion analysis: a review</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="428" to="440" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An optimal algorithm for approximate nearest neighbor searching fixed dimensions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Arya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Mount</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Netanyahu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Silverman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Association for Computing Machinery</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="891" to="923" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Actions as space-time shapes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Blank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gorelick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference of Computer Vision</title>
		<meeting>the International Conference of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Behavior recognition via sparse spatio-temporal teatures</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance</title>
		<meeting>the IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The PASCAL Visual Object Classes Challenge</title>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Recognizing action at a distance</title>
		<author>
			<persName><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference of Computer Vision</title>
		<meeting>the International Conference of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Efficient graph based image segmentation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="167" to="181" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hybrid models for human motion recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Fanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference of Computer Vision</title>
		<meeting>the International Conference of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Accurate object detection with deformable shape models learnt from images</title>
		<author>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Fukunaga</surname></persName>
		</author>
		<title level="m">Introduction to Statististical Pattern Recognition</title>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Actions as space-time shapes</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gorelick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2247" to="2253" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Approximate nearest neighbors: towards removing the curse of dimensionality</title>
		<author>
			<persName><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Symposium on the Theory of Computing</title>
		<meeting>the ACM Symposium on the Theory of Computing</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A unified approach to moving object detection in 2D and 3D scenes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Anandan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="577" to="589" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Action detection in complex scenes with spatial and temporal ambiguities</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference of Computer Vision</title>
		<meeting>the International Conference of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A biologically inspired system for action recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference of Computer Vision</title>
		<meeting>the International Conference of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Discriminative subvolume search for efficient action detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient visual event detection using volumetric features</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference of Computer Vision</title>
		<meeting>the International Conference of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Event detection in crowded videos</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference of Computer Vision</title>
		<meeting>the International Conference of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On space-time interest points</title>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="107" to="123" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Retrieving actions in movies</title>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference of Computer Vision</title>
		<meeting>the International Conference of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning realistic human actions from movies</title>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marsza?ek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rozenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Robust object detection with interleaved categorization and segmentation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="259" to="289" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Efficient clustering and matching for object class recognition</title>
		<author>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Recognizing actions by shape-motion prototype trees</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference of Computer Vision</title>
		<meeting>the International Conference of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recognizing realistic actions from videos &apos;&apos;in the Wild</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An iterative image registration technique with an application to stereo vision</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conferences on Artificial Intelligence</title>
		<meeting>the International Joint Conferences on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Robust wide baseline stereo from maximally stable extremal regions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Scale and affine invariant interest point detectors</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="63" to="86" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Local features for object class recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference of Computer Vision</title>
		<meeting>the International Conference of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multiple object class detection with a generative model</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Improving descriptors for fast tree matching by optimal linear projection</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference of Computer Vision</title>
		<meeting>the International Conference of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A performance evaluation of local descriptors</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1615" to="1630" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Action recognition with motion-apperance vocabulary forest</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Uemura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning linear discriminant projections for dimensionality reduction of image descriptors</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unsupervised learning of human action categories using spatial-temporal words</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A hierarchical model of shape and appearance for human action classification</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Scalable recognition with a vocabulary tree</title>
		<author>
			<persName><forename type="first">D</forename><surname>Nister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Stewenius</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Discriminative subsequence mining for action classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bakir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tsuda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference of Computer Vision</title>
		<meeting>the International Conference of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Model-based plane-segmentation using optical flow and dominant plane</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ohnishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Imiya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science (LNCS)</title>
		<imprint>
			<biblScope unit="volume">4418</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Object retrieval with large vocabularies and fast spatial matching</title>
		<author>
			<persName><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Automatic annotation of everyday movements</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Neural Information Processing Systems</title>
		<meeting>the Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Recognizing human actions: a local SVM approach</title>
		<author>
			<persName><forename type="first">C</forename><surname>Schuldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Pattern Recognition</title>
		<meeting>the International Conference on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Good features to track</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">An a contrario decision framework for regionbased motion detection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bouthemy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="163" to="178" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Feature tracking and motion compensation for action recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Uemura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ishikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Extracting spatio-temporal interest points using global information</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference of Computer Vision</title>
		<meeting>the International Conference of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Segmentation of a piece-wise planar scene from perspective images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Recognizing human actions in videos acquired by uncalibrated moving cameras</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference of Computer Vision</title>
		<meeting>the International Conference of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Event-based analysis of video</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Detecting video events based on action recognition in complex scenes using spatio-temporal descriptor</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">data, feature detectors, and evaluation protocols</title>
		<ptr target="www.featurespace.org" />
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
