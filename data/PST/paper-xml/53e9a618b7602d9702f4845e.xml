<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EIGENTONGUE FEATURE EXTRACTION FOR AN ULTRASOUND-BASED SILENT SPEECH INTERFACE</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">T</forename><surname>Hueber13</surname></persName>
						</author>
						<author>
							<persName><forename type="first">G</forename><surname>Aversano3</surname></persName>
						</author>
						<author>
							<persName><forename type="first">G</forename><surname>Cholle</surname></persName>
						</author>
						<author>
							<persName><forename type="first">B</forename><surname>Denby'</surname></persName>
						</author>
						<author>
							<persName><forename type="first">G</forename><surname>Dreyfus'</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Y</forename><surname>Oussar'</surname></persName>
						</author>
						<author>
							<persName><forename type="first">P</forename><surname>Roussel</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratoire d&apos;Electronique</orgName>
								<orgName type="institution">Ecole Superieure de Physique et de Chimie Industrielles de la Ville de Paris (ESPCI-Paristech)</orgName>
								<address>
									<addrLine>10 rue Vauquelin</addrLine>
									<postCode>75231</postCode>
									<settlement>Paris Cedex 05 France</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<addrLine>2Universit6 Pierre et Marie Curie -Paris VI, B.C. 252, 4 place Jussieu</addrLine>
									<postCode>75252, Cedex 05</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">3Laboratoire Traitement et Communication de l&apos;Information</orgName>
								<orgName type="department" key="dep2">Ecole Nationale Superieure des Telecommunications</orgName>
								<orgName type="laboratory">Tract Visualization Lab</orgName>
								<orgName type="institution">ENST-Paristech)</orgName>
								<address>
									<addrLine>46 rue Barrault, 13 4Vocal</addrLine>
									<postCode>75634</postCode>
									<settlement>Paris Cedex</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of Maryland Dental School</orgName>
								<address>
									<addrLine>666 W. Baltimore Street</addrLine>
									<postCode>21201</postCode>
									<settlement>Baltimore</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">EIGENTONGUE FEATURE EXTRACTION FOR AN ULTRASOUND-BASED SILENT SPEECH INTERFACE</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">823171F786BF5094997137F8D1450B30</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>image processing</term>
					<term>speech synthesis</term>
					<term>neural network applications</term>
					<term>communication systems</term>
					<term>silent speech interface</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The article compares two approaches to the description of ultrasound vocal tract images for application in a "silent speech interface," one based on tongue contour modeling, and a second, global coding approach in which images are projected onto a feature space of Eigentongues. A curvaturebased lip profile feature extraction method is also presented. Extracted visual features are input to a neural network which learns the relation between the vocal tract configuration and line spectrum frequencies (LSF) contained in a one-hour speech corpus. An examination of the quality of LSF's derived from the two approaches demonstrates that the eigentongues approach has a more efficient implementation and provides superior results based on a normalized mean squared error criterion.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>There has been significant interest recently in the notion of a "silent speech interface (SSI)" -a portable device used as an alternative to tracheo-oesophageal speech for larynx cancer patients, for situations where silence must be maintained, or for voice communication in noisy environments. Approaches based on electromyography <ref type="bibr" target="#b1">[1]</ref>, a non-audible murmur microphone <ref type="bibr" target="#b2">[2]</ref>, and ultrasound and optical imagery <ref type="bibr">([3]</ref>, <ref type="bibr" target="#b4">[4]</ref>) have appeared in the literature.</p><p>We present here results of a visuo-acoustic SSI study based on a one-hour corpus comprising ultrasound and optical imagery of the vocal tract. The use of a corpus of this size -which was motivated by the desire to interface to a concatenative speech synthesizer -has led to the development of robust feature extraction techniques in order to accommodate the wide variety of articulator configurations appearing in the corpus. In particular, an Eigentongues approach has been introduced in order to address the problem of ultrasound frames in which the tongue images poorly. Section 2 of the article details data acquisition and ultrasound image preprocessing, while section 3 describes the feature extraction techniques used in the image (ultrasound and optical) and speech signal analyses. Modeling of the link between visual and acoustic features is introduced in section 4, along with experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">DATA ACQUISITION AND PREPROCESSING</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Data acquisition</head><p>Data were taken using a 30 Hz ultrasound machine and the Vocal Tract Visualization Lab HATS system <ref type="bibr" target="#b5">[5]</ref>, which maintains acoustic contact between the throat and the ultrasound transducer during speech. A lip profile image is embedded into the ultrasound image, as shown in figure <ref type="figure" target="#fig_0">1</ref>. The speech dataset used consists of 720 sentences, organized in 10 lists, from the IEEE/Harvard corpus <ref type="bibr" target="#b6">[6]</ref>, spoken by a male native American English speaker. The IEEE sentences were chosen because they are constructed to have roughly equal intelligibility across lists and all have approximately the same duration, number of syllables, grammatical structure and intonation. After cleaning the database, the resulting speech was stored as 72473 JPEG frames and 720 WAV audio files sampled at 11025 Hz.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Ultrasound image preprocessing</head><p>In order to select a region of interest, the ultrasound images are first reduced to a 50 (radial angle) by 50 (azimuthal angle) semi-polar grid. To decrease the effects of speckle, the reduced images are filtered using the anisotropic diffusion filter proposed by Yu [7]. This iterative process introduces intra-region smoothing while inhibiting interregion smoothing <ref type="bibr" target="#b8">[8]</ref>, via a local coefficient of variation <ref type="bibr" target="#b9">[9]</ref>, so that speckle is removed without destroying important image features. A typical result after these two preprocessing steps is illustrated in figure <ref type="figure" target="#fig_1">2(a)</ref>. As in <ref type="bibr" target="#b3">[3]</ref> and <ref type="bibr" target="#b4">[4]</ref>, our first approach considers the tongue surface to be the only ultrasound image information relevant to the prediction of speech characteristics. Tongue contour candidate points are defined as maxima of the smoothed vertical intensity gradient. Then, in the present work, a Least Median Square (LMS, <ref type="bibr" target="#b10">[10]</ref>)-based spline interpolation method, tolerating up to 50% outlier points, is used in order to retain only relevant tongue contour candidates; this is an improvement over the contour extraction method implemented in <ref type="bibr" target="#b3">[3]</ref> and <ref type="bibr" target="#b4">[4]</ref>.</p><p>A typical tongue contour is shown in figure 2(b). Due to refraction, however, the tongue surface will be poorly imaged when the tongue surface is at angles nearly parallel to the ultrasound beam, as in the case of the phoneme lil for example. The contour extraction described previously fails in such frames -which are found to constitute some 15 % of our database -since the tongue surface is simply no longer visible in them. These "outlier frames" are detected automatically using the area of the convex hull of intensity gradient maxima. Below, we present a more global feature extraction approach which provides a solution to the missing contour problem. The second approach features the use of Principal Component Analysis (PCA), or Karhunen-Loeve expansion, for describing the ultrasound images. The first step is to create a finite set of orthogonal images, which constitutes, up to a certain accuracy, a subspace for the representation of all likely tongue configurations. These images are referred to as Eigentongues, a term inspired by the Eigenface method of Turk and Pentland <ref type="bibr" target="#b11">[11]</ref>. The first three Eigentongues, obtained after a PCA on 1000 reduced and filtered ultrasound images, are shown in figure <ref type="figure" target="#fig_2">3</ref>. Once the set of Eigentongues has been created, the images of subsequent tongue configurations can be represented quite compactly in terms of their projections onto the set of Eigentongues, as shown in figure <ref type="figure" target="#fig_3">4</ref>. The Eigentongue components encode the maximum amount of relevant information in the images, mainly tongue position, of course, but also other structures such as the hyoid bone, muscles, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Optical image feature extraction</head><p>The optical image feature extraction consists of a description of the lip profile. We propose an algorithm based on the observation of Attneave that information along a visual contour is concentrated in regions of high curvature, rather than distributed uniformly <ref type="bibr" target="#b12">[12]</ref>. The lip edge profile is easily extracted using the Sobel method. The curvature of this two-dimensional curve is then computed using the Turning Angle introduced by Feldman <ref type="bibr" target="#b13">[13]</ref>. Upper/lower lip and commissure positions coincide with extrema of the curvature, as shown as figure <ref type="figure" target="#fig_4">5</ref>, while the values of the curvature at these points give local lip shape information. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Speech signal description</head><p>For each 33 ms audio frame (dictated by the 30 Hz ultrasound rate), twelve LSF's are calculated using a preaccentuation filter, linear predictive coding and a Hann window with a half-frame overlap. The robustness of LSF coefficients is known to assure the stability of the LPC filter <ref type="bibr" target="#b14">[14]</ref>. A voiced/unvoiced flag and fundamental frequency (for voiced frames) are also computed, using a simple autocorrelation-based method. These last two features are not used in the visuo-acoustic modeling which follows, but allow a qualitative, audible comparison of our different results, if desired, via LPC synthesis using the predicted autoregressive filter coefficients and a realistic excitation function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">VISUO-ACOUSTIC MODELING</head><p>Our first feature extraction method, described in sections 3.2 and 3.1.1, provides 15 features per frame, including 9 for the lips (position and curvature of upper/lower lips and commissure) and 6 for the tongue (4th order spline coefficients and interval of definition). The second, Eigentongue method gives 29 features per frame, the first 20 Eigentongue components plus lip features. A multilayer perceptron (MLP) is used to perform the mapping between these input visual features and the 12 LSF's <ref type="bibr" target="#b15">[15]</ref>. A separate network is used for each LSF in order to limit the number of adjustable parameters in the model. A total of 71502 frames are used for training, with an independent set of 971 frames for model selection and validation. We now compare the LSF prediction obtained from the two methods. Because each LSF is defined upon its own interval, we introduce a normalized measure of the quality of the prediction a, along with an estimate of its standard deviation E <ref type="bibr" target="#b16">[16]</ref>:</p><formula xml:id="formula_0">N I (Yi _ aymf a = 0 i=l j/N ' Ynax -Ymin E =aoc &gt;4 2N</formula><p>where N is the number of examples in the validation database, y are the true LSF's, and y the predicted LSF's.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Comparing Contour and Eigentongue approaches</head><p>For the tongue contour method, in order to obtain reasonable training results, the "outlier frames" for which the automatic contour extraction algorithm (described in section 3.1.1) failed were removed from the training set. As the Eigentongue feature extraction approach does not restrict relevant information to a specific structure, no outlier is generated when that structure is not imaged, and thus all frames may be used in the visuo-acoustic modeling with this method. Columns 1 and 2 of Table The table shows that LSF's 4, 6, 8 and 10 are the best predicted by tongue contour and lip profile features, and that using Eigentongues provides an improvement in overall prediction quality which is small, but statistically I -1247 significant. The filtering step described in section 2.2 is in fact not essential for the Eigentongue feature extraction, as image regions of high intensity variability will be associated with the higher order Eigentongues, which are not used. Similar results are obtained using Eigentongues obtained from unfiltered images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Introducing 'history' into the input variables</head><p>The use of Eigentongues allows all of the video frames to participate in the training, which is not the case for the contour method due to the missing frames. We can then in a simple way take account of the intrinsically dynamic nature of speech production in our visuo-acoustic modeling by providing the training algorithm, at frame n, with the Eigentongue and lip variables of frames n-I and n-2, as well. An additional small improvement in the prediction of LSF's 2, 4, 6, 7 and 11 is seen, as compared to the static modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION AND PERSPECTIVES</head><p>A new turning-angle algorithm for the description of lip profiles has been introduced, which, because curvature- based, should hopefully make the method robust against the variability of lip shapes between speakers. Two methods for feature extraction from ultrasound images have been presented and compared. The visuo-acoustic modeling with Eigentongues gives better results than those obtained using tongue contours as input. The Eigentongue method is easier to implement, appears to take more information into account, and is not prone to failures due to instrumental effects, thus allowing the dynamic nature of speech to be taken into account in a natural way. It could be interesting, however, in future work, to combine the two approaches in the context of active appearance models <ref type="bibr" target="#b17">[17]</ref>. The model we propose is at present able to predict an acoustical description of speech with errors ranging from 11% to 16%. Whether this performance is adequate for application in an SSI will only become apparent once a concatenative speech synthesis model using our predicted quantities as inputs has been experimented. The elaboration of such a test, as well as the use of alternative dynamic process modeling techniques (Hidden Markov Models, Time Delay Neural Networks <ref type="bibr" target="#b19">[18]</ref>) are currently underway.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">ACKNOWLEDGEMENT</head><p>The authors would like to acknowledge useful discussions with Isabelle Bloch. This work was supported in part by CNFM (Comite National de Formation en Microelectronique).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Example of an ultrasound vocal tract image with embedded lip profile: (a) tongue surface; (b) hyoid bone ; (c) hyoid and mandible acoustic shadows; (d) muscle, fat and connective tissue within the tongue.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Reduced and filtered ultrasound image (a) and tongue surface contour fit by a 4th order spline (b) 3. FEATURE EXTRACTION 3.1. Ultrasound image feature extraction 3.1.1. Tongue contour extraction</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. The first three Eigentongues (1-3 from left to right)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. A reduced ultrasound image (left) and its re-synthesis (right) using 20 Eigentongue components</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Lip profile description using curvature computation (left: lip contour; right: curvature of lip contour)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>1 compare the results of the two approaches. Comparison of tongue contour based modeling and Eigentongue based modeling. Quoted errors, e, are estimates of the standard deviation of a using a Gaussian assumption</figDesc><table><row><cell>LSF number</cell><cell cols="3">Tongue Contour Parameter a (% of total dynamic range) Eit Eigentongue + history</cell></row><row><cell>1</cell><cell>18.7±0.4</cell><cell>16.9±0.4</cell><cell>16.4±0.4</cell></row><row><cell>2</cell><cell>16.1 ±0.4</cell><cell>14.4±0.3</cell><cell>13.7±0.3</cell></row><row><cell>3</cell><cell>14.3 ± 0.3</cell><cell>12.4 ± 0.3</cell><cell>12.3 ± 0.3</cell></row><row><cell>4</cell><cell>13.1 ± 0.3</cell><cell>11.8 ± 0.3</cell><cell>10.8 ± 0.2</cell></row><row><cell>5</cell><cell>14.2±0.3</cell><cell>11.5±0.3</cell><cell>11.9±0.3</cell></row><row><cell>6</cell><cell>13.1 ± 0.3</cell><cell>11.8 ± 0.3</cell><cell>10.6 ± 0.2</cell></row><row><cell>7</cell><cell>15.7 ± 0.4</cell><cell>13.7 ± 0.3</cell><cell>12.6 ± 0.3</cell></row><row><cell>8</cell><cell>13.1 ± 0.3</cell><cell>11.8 ± 0.3</cell><cell>12.1 ± 0.3</cell></row><row><cell>9</cell><cell>14.6 ± 0.3</cell><cell>12.8 ± 0.3</cell><cell>12.4 ± 0.3</cell></row><row><cell>10</cell><cell>12.9±0.3</cell><cell>11.2±0.2</cell><cell>11.2±0.2</cell></row><row><cell>11</cell><cell>14.5±0.3</cell><cell>13.7±0.3</cell><cell>11.4±0.2</cell></row><row><cell>12</cell><cell>16.3 ± 0.4</cell><cell>14.5 ± 0.3</cell><cell>14.4 ± 0.3</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sub Auditory Speech Recognition Based on EMG/EPG Signals</title>
		<author>
			<persName><forename type="first">C</forename><surname>Jorgensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agabon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Neural Networks</title>
		<meeting>the International Joint Conference on Neural Networks</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="3128" to="3133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A Tissue-conductive Acoustic Sensor Applied in Speech Recognition for Privacy</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Nakajima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Heracleous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Saruwatari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shikano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Smart Objects &amp; Ambient Intelligences Oc-EUSAI 2005</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="93" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Prospects for a Silent Speech Interface Using Ultrasound Imaging</title>
		<author>
			<persName><forename type="first">B</forename><surname>Denby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Oussar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dreyfus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<meeting><address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Speech Synthesis from Real Time Ultrasound Images of the Tongue</title>
		<author>
			<persName><forename type="first">B</forename><surname>Denby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<meeting><address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A Guide to Analysing Tongue Motion from Ultrasound Images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clinical Linguistics and Phonetics</title>
		<imprint>
			<biblScope unit="page" from="359" to="366" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">IEEE Recommended Practice for Speech Quality Measurements</title>
		<author>
			<persName><surname>Ieee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio and Electroacoustics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="225" to="246" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Speckle Reducing Anisotropic Diffusion</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Acton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1260" to="1270" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Scale-Space and Edge Detection Using Anisotropic Diffusion</title>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="629" to="639" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Digital Image Enhancement and Noise Filtering by Use of Local Statistics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="165" to="168" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Robust Regression and Outlier Detection</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Rousseeuw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Leroy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
			<publisher>John Wiley &amp; Sons, Inc</publisher>
			<pubPlace>New York, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Face Recognition Using Eigenfaces</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Turk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="586" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Some Informational Aspects of Visual Perception</title>
		<author>
			<persName><forename type="first">F</forename><surname>Attneave</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psych. Review</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="183" to="193" />
			<date type="published" when="1954">1954</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Information Along Contours and Object Boundaries</title>
		<author>
			<persName><forename type="first">J</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psych. Review</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="page" from="243" to="252" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Application of Line-Spectrum Pairs to Low-Bit-Rate Speech Encoders</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fransen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<meeting><address><addrLine>Tampa, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Dreyfus</surname></persName>
		</author>
		<title level="m">Neural Networks: Methodology and Applications</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Cramer</surname></persName>
		</author>
		<title level="m">Mathematical Methods of Statistics</title>
		<meeting><address><addrLine>Princeton</addrLine></address></meeting>
		<imprint>
			<publisher>Princeton University Press</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Active Appearance Models: Theory and Cases</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Stegmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fisker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Thodberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hyldstrup</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Proc</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m">Danish Conference on Pattern Recognition and Image Analysis</title>
		<imprint>
			<publisher>AUC Press</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="49" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Phoneme Recognition Using Time Delay Neural Networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hanazawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shikano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="328" to="339" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
