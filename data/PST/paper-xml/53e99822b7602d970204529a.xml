<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Synchronization Without Contention</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">John</forename><forename type="middle">M</forename><surname>Mellor-Crummey</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><forename type="middle">L</forename><surname>Scott+</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Center for Research on Parallel Computation</orgName>
								<orgName type="institution">Rice University</orgName>
								<address>
									<postBox>P.O. Box 1892</postBox>
									<postCode>77251-1892</postCode>
									<settlement>Houston</settlement>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">University of Rochester Rochester</orgName>
								<address>
									<postCode>14627</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Synchronization Without Contention</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E9317DEC2D3D080B142F1C4FCA56DDA3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Conventional wisdom holds that contention due to busy-wait synchronization is a major obstacle to scalability and acceptable performance in large shared-memory multiprocessors. We argue the contrary, and present fast, simple algorithms for contention-free mutual exclusion, reader-writer control, and barrier synchronization. These algorithms, based on widely available fetch-and-@ instructions, exploit local access to shared memory to avoid contention. We compare our algorithms to previous approaches in both qualitative and quantitative terms, presenting their performance on the Sequent Symmetry and BBN Butterfly multiprocessors. Our results highlight the importance of local access to shared memory, provide a case against the construction of so-called "dance hall" machines, and suggest that special-purpose hardware support for synchronization is unlikely to be cost effective on machines with sequentially consistent memory.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Busy-waiting synchronization, including locks and barriers, is fundamental to parallel programming on shared-memory multiprocessors. Busy waiting is preferred over schedulerbased blocking when scheduling overhead exceeds expected wait time, when processor resources are not needed for other tasks (so that the lower wake-up latency of busy waiting need not be balanced against an opportunity cost), or in the implementation of an operating system kernel where schedulerbased blocking is inappropriate or impossible.</p><p>Because busy-wait mechanisms are often used to protect or separate very small critical sections, their performance is a matter of paramount importance. Unfortunately, typical implementations of busy waiting tend to produce large amounts of memory and interconnection network contention, introducing performance bottlenecks that become markedly more pronounced in larger machines and applications. When many processors busy-wait on a single synchronization variable, they create a hot spot that gets a disproportionate share of the network traffic. Pfister and Norton <ref type="bibr" target="#b19">[22]</ref> showed that the presence of hot spots can severely degrade performance for all traffic in multistage interconnection networks, not just traffic due to synchronizing processors. Agarwal and Cherian <ref type="bibr">[2]</ref> found that references to synchronization variables on cachecoherent multiprocessors cause cache line invalidations much more often than non-synchronization references. They also observed that synchronization accounted for as much as 49% of network traffic in simulations of a 64-processor "dance hall" machine, in which each access to a shared variable traverses the processor-memory interconnection network. Pfister and Norton <ref type="bibr" target="#b19">[22]</ref> argue for message combining in multistage interconnection networks. They base their argument primarily on anticipated hot-spot contention for locks, noting that they know of no quantitative evidence to support or deny the value of combining for general memory traffic. (Hardware combining appears in the original designs for the NYU Ultracomputer [lo], the IBM RP3 <ref type="bibr" target="#b18">[21]</ref>, and the BBN Monarch <ref type="bibr" target="#b21">[24]</ref>.) Other researchers have suggested adding special-purpose hardware solely for synchronization, including synchronization variables in the switching nodes of multistage interconnection networks <ref type="bibr" target="#b11">[14]</ref> and lock queuing mechanisms in the cache controllers of cache-coherent multiprocessors <ref type="bibr">[9,</ref><ref type="bibr" target="#b17">20]</ref>. The principal purpose of these mechanisms is to reduce contention caused by busy waiting. Before adopting them, it is worth considering the extent to which software techniques can achieve a similar effect.</p><p>Our results indicate that the cost of synchronization in systems without combining, and the impact that synchronization activity will have on overall system performance, is much less than previously thought. This paper describes novel algorithms that use distributed data structures to implement protocols for busy-wait synchronization. All that our algorithms require in the way of hardware support is a simple set of f etch-and-@ operations1 and a memory hierarchy in which each processor is able to read some portion of shared memory without using the interconnection network. By allocating data structures appropriately, we ensure that each processor busy waits only on locally-accessible variables on which no other processor spins, thus eliminating hot-spot contention caused by busy waiting. On a machine in which shared memory is distributed (e.g., the BBN Butterfly [4], the IBM RP3 <ref type="bibr" target="#b18">[21]</ref>, or a shared-memory hypercube [6]), a processor busy waits only on flag variables located in its local portion of the shared memory. On a machine with coherent caches, a processor busy waits only on flag variables resident in its cache; flag variables for different processors are placed in different cache lines, to eliminate false sharing.</p><p>The implication of our work is that efficient synchronization algorithms can be constructed in software for sharedmemory multiprocessors of arbitrary size. Special-purpose synchronization hardware can offer only a small constant factor of additional performance for mutual exclusion, and at best a logarithmic factor for barrier ~~n c h r o n i z a t i o n . ~</p><p>In addition, the feasibility and performance of busy-waiting algorithms with local-only spinning provides a case against "dance-hall" architectures, in which shared memory locations are equally far from all processors.</p><p>We present scalable algorithms for mutual exclusion spin locks, reader-writer spin locks, and multi-processor barriers in section 2. We relate these algorithms to previous work in section 3, and present performance results in section 4 for both a distributed shared-memory multiprocessor (the BBN Butterfly I), and a machine with coherent caches (the Sequent Symmetry Model B). Our architectural conclusions are summarized in section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Scalable Synchronization</head><p>In this section we present novel busy-wait algorithms for mutual exclusion, reader-writer control, and barrier synchronization. All are designed to use local-only spinning. The readerwriter lock requires atomic fetch-and-store (xmem) and compare-and-swap4 instructions. a specified 'new' value is written into the memory location.) The mutual-exclusion lock requires f etch-and-store, and benefits from the availability of compare-and-swap. The barrier requires nothing more than the usual atomicity of memory reads and writes.</p><p>Our pseudo-code notation is meant to be more-or-less self explanatory. Line breaks terminate statements (except in obvious cases of run-on), and indentation indicates nesting in control constructs. The keyword shared indicates that a declared variable is to be shared by all processors. This declaration implies no particular physical location for the variable, but we often specify locations in comments and/or accompanying text. The keywords processor private indicate that each processor is to have a separate, independent copy of a declared variable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Mutual Exclusion</head><p>Our spin lock algorithm (called the MCS lock, after our initials):</p><p>guarantees FIFO ordering of lock acquisitions; spins on locally-accessible flag variables only; requires a small constant amount of space per lock; and works equally well (requiring only O(1) network transactions per lock acquisition) on machines with and without coherent caches.</p><p>The MCS lock (algorithm 1) was inspired by the QOSB (Queue On Synch Bit) hardware primitive proposed for the cache controllers of the Wisconsin Multicube [9], but is implemented entirely in software. Like QOSB, our lock algorithm maintains a queue of processors requesting a lock; this organization enables each processor to busy wait on a unique, locally-accessible flag variable. To use our locking algorithm, a process allocates a locallyaccessible record containing a link pointer and a Boolean flag in a scope enclosing the calls to the locking primitives (each processor can use a single statically-allocated record if lock acquisitions are never nested). One additional private temporary variable is employed during the acquire-lock operation. Processors holding or waiting for the same lock are chained together by the links in the local records. Each processor spins on its own local flag. The lock itself contains a pointer to the record for the processor at the tail of the queue, or nil if the lock is not held. Each processor in the queue holds the address of the record for the processor behind it-the processor it should resume after acquiring and releasing the lock. Compare-and-swap enables a processor to determine whether it is the only processor in the queue, and if so remove itself correctly, as a single atomic action. The spin in acquirelock waits for the lock to become free. The spin in releaselock compensates for the timing window between the f etch-and-store and the assignment to pred-&gt;next in acquirelock. <ref type="bibr">Both</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Reader-Writer Control</head><p>The MCS spin lock algorithm can be modified to grant concurrent access to readers without affecting any of the lock's fundamental properties. As with traditional semaphorebased approaches to reader-writer synchronization [7], we can design versions that provide fair access to both readers and writers, or that grant preferential access to one or the other. We present a fair version here. A read request is granted when all previous write requests have completed. A write request is granted when all previous read and write requests have completed.</p><p>Code for the reader-writer lock appears in algorithm 3. As in the MCS spin lock, we maintain a linked list of requesting processors. In this case, however, we allow a requester to read and write fields in the link record of its predecessor (if any).</p><p>To ensure that the record is still valid (and has not been deallocated or reused), we require that a processor access its predecessor's record before initializing the record's next pointer. At the same time, we force every processor that has a successor to wait for its next pointer to become non-nil, even if the pointer will never be used. As in the MCS lock, the existence of a successor is determined by examining L-&gt;tail.</p><p>A reader can begin reading if its predecessor is a reader that is already active, but it must first unblock its successor (if any) if that successor is a waiting reader. To ensure that a reader is never left blocked while its predecessor is reading, each reader uses compare-and-swap to atomically test if its predecessor is an active reader, and if not, notify its predecessor that it has a waiting reader as a successor.</p><p>Similarly, a writer can proceed if its predecessor is done and there are no active readers. A writer whose precedessor is a writer can proceed as soon as its predecessor is done, as in the MCS lock. A writer whose predecessor is a reader must go through an additional protocol using a count of active readers, since some readers that started earlier may still be active. When the last reader of a group finishes (reader-count = O), it must resume the writer (if any) next in line for access. This may require a reader to resume a writer that is not its direct successor. When a writer is next in line for access, we write its name in a global location. We use f etch-and-store to read and erase this location atomically, ensuring that a writer proceeds on its own if and only if no reader is going to try to resume it. To make sure that reader-count never reaches zero prematurely, we increment it before resuming a blocked reader, and before updating the next pointer of a reader whose reading successor proceeds on its own. next-blocked := f a l s e procedure end_write(L: "lock; I : "qnode) with I", LA i f next != n i l or not compare.and.swap(Stai1, I , n i l ) // wait u n t i l succ inspects my s t a t e repeat while next = n i l i f next-&gt;class = reading atomic.increment(reader.count) next-blocked := f a l s e procedure end_read(L : "lock; I : "qnode) with I", LA i f next != n i l or not compare.and.swap(Stai1, I , n i l ) // wait u n t i l succ inspects my s t a t e repeat while next = n i l i f successor~class = writer next-writer := next i f fetch.and.decrement(reader.count) = 1 // I ' m l a s t reader, wake writer i f any w : "qnode := fetch.and_store(hext.writer, n i l ) i f w != n i l w-&gt;blocked := f a l s e Algorithm 3: A fair reader-writer lock with local-only spinning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Barrier Synchronization</head><p>We have devised a new barrier algorithm that spins on locally-accessible flag variables only;</p><p>requires only 0(P) space for P processors; performs the theoretical minimum number of network transactions (2P -2) on machines without broadcast; and performs O(1og P ) network transactions on its critical path.</p><p>To synchronize P processors, our barrier employs a pair of P-node trees. Each processor is assigned a unique tree node, which is linked into an arrival tree by a parent link, and into a wakeup tree by a set of child links. It is useful to think of these as separate trees, because the fan-in in the arrival tree differs from the fan-out in the wakeup tree.' A processor does not examine or modify the state of any other nodes except to signal its arrival at the barrier by setting a flag in its parent's node, and when notified by its parent that the barrier has been achieved, to notify each of its children by setting a flag in each of their nodes. Each processor spins only on flag variables in its own tree node. To achieve a barrier, each processor executes the code shown in algorithm 4.</p><p>Our tree barrier achieves the theoretical lower bound on the number of network transactions needed to achieve a barrier on machines that lack broadcast. At least P -1 processors must signal their arrival to some other processor, requiring P -1 network transactions, and must then be informed of wakeup, requiring another P -1 network transactions. The length of the critical path in our algorithm is proportional to flog4 Pl + flog2 P I . The first term is the time to propagate arrival up to the root, and the second term is the time to propagate wakeup back down to all of the leaves. On a machine with coherent caches and unlimited replication of cache lines, we could replace the wakeup phase of our algorithm with a busy wait on a global flag. We explore this alternative on the Sequent Symmetry in section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related Work</head><p>In <ref type="bibr">[18]</ref> we survey existing spin lock and barrier algorithms, examining their differences in detail and in many cases presenting improvements over previous-published versions. Many of these algorithms appear in the performance results of section 4; we describe them briefly here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Spin locks</head><p>The simplest algorithm for mutual exclusion repeatedly polls a Boolean flag with a test-and-set instruction, attempting to acquire the lock by changing the flag from false to true. A processor releases the lock by setting it to false. Protocols based on the so-called test-and-test-and-set <ref type="bibr" target="#b22">[25]</ref> reduce memory and interconnection network contention by polling with read requests, issuing a test-and-set only when the lock appears to be free. Such protocols eliminate contention on cache-coherent machines while the lock is held but with 'We use a fan-out of 2 because it results in the the shortest critical path required to resume P spinning processors for a tree of uniform degree. We use a fan-in of 4 (1) because it produced the best performance in Yew, Tzeng, and Lawrie's experiments <ref type="bibr" target="#b24">[27]</ref> with a related technique they call software combining, and (2) because the ability to pack four bytes in a word permits an optimization on many machines in which a parent can inspect status information for all of its children simultaneously at the same cost as inspecting the status of only one. Alex Schaffer and Paul Dietz have pointed out that slightly better performance might be obtained in the wakeup tree by assigning more children to processors near the root. P competing processors can still induce 0(P) network traffic (as opposed to O(1) for the MCS lock) each time the lock is freed. Alternatively, a test-and-set lock can be designed to pause between its polling operations. <ref type="bibr">Anderson [3]</ref> found exponential backoff to be the most effective form of delay; our experiments confirm this result.</p><p>A "ticket lock" [18] (analogous to a busy-wait implementation of a semaphore constructed using an eventcount and a sequencer <ref type="bibr" target="#b20">[23]</ref>) consists of a pair of counters, one containing the number of requests to acquire the lock, and the other the number of times the lock has been released. A process acquires the lock by performing a f etch-and-increment on the request counter and waiting until the result (its ticket) is equal to the value of the release counter. It releases the lock by incrementing the release counter. The ticket lock ensures that processors acquire the lock in FIFO order, and reduces the number of f etch-and-@ operations per lock acquisition. Contention is still a problem, but the counting inherent in the lock allows us to introduce a very effective form of backoff, in which each processor waits for a period of time proportional to the difference between the values of its ticket and the release counter.</p><p>In an attempt to eliminate contention entirely, Anderson [3] has proposed a locking algorithm that requires only a constant number of memory references through the interconnection network per lock acquisition/release on cachecoherent multiprocessors. The trick is for each processor to use f etch-and-increment to obtain the address of a location on which to spin. Each processor spins on a different location, in a different cache line. Anderson's experiments indicate that his algorithm outperforms a test-and-set lock with exponential backoff when contention for the lock is high [3]. A similar lock has been devised by <ref type="bibr">Graunke and Thakkar [ll]</ref>. Unfortunately, neither lock generalizes to multiprocessors without coherent caches, and both require staticallyallocated space per lock linear in the number of processors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Barriers</head><p>In the simplest barrier algorithms (still widely used in practice), each processor increments a centralized shared count as it reaches the barrier, and spins until that count (or a flag set by the last arriving processor) indicates that all processors are present. Like the simple test-and-set spin lock, such centralized barriers induce enormous amounts of contention. Moreover, Agarwal and Cherian [2] found backoff schemes to be of limited use for large numbers of processors. Our results (see section 4) confirm this conclusion.</p><p>In a study of what they call "software combining," Yew, Tzeng, and Lawrie <ref type="bibr" target="#b24">[27]</ref> note that their technique could be used to implement a barrier. They organize processors into groups of k, for some k, and each group is assigned to a unique leaf of a k-ary tree. Each processor performs a f etch-and-- increment on the count in its group's leaf node; the last processor to arrive at a node continues upwards, incrementing a count in the node's parent. Continuing in this fashion, a single processor reaches the root of the tree and initiates a reverse wave of wakeup operations. Straightforward application of software combining suggests the use of f etch-and-- decrement for wakeup, but we observe in [18] that simple reads and writes suffice. A combining tree barrier still spins on non-local locations, but causes less contention than a centralized counter barrier, since at most k -1 processors share any individual variable.</p><p>Hensgen, Finkel, and Manber [12] and Lubachevsky <ref type="bibr" target="#b14">[17]</ref> have devised tree-style "tournament" barriers. In their algorithms, processors start at the leaves of a binary tree. One processor from each node continues up the tree to the next "round" of the tournament. The "winning" processor is statically determined; there is no need for f etch-and-@. In Hensgen, Finkel, and Manber's tournament barrier, and in one of two tournament barriers proposed by Lubachevsky, processors spin for wakeup on a single global flag, set by the tournament champion. This technique works well on cachecoherent machines with broadcast and unlimited cache line replication, but does not work well on distributed-memory machines. Lubachevsky presents a second tournament barrier that employs a tree for wakeup as well as arrival; this barrier will work well on cache-coherent machines that limit cache line replication, but does not permit local-only spinning on distributed-memory machines. In <ref type="bibr">[18]</ref> we present a bidirectional tournament barrier (based on Hensgen, Finkel, and Manber's algorithm) that spins only on locally-accessible variables, even on distributed-memory machines. As shown in section 4, however, the resulting barrier is slightly slower than algorithm 4. The latter employs a squatter tree, with processors assigned to both internal and external nodes.</p><p>To the best of our knowledge, only one barrier other than our tree or modified tournament achieves the goal of localonly spinning on machines without coherent caches (though its authors did not address this issue). Hensgen, Finkel, and Manber [12] describe a "dissemination barrier" based on their own algorithm for disseminating information in a distributed system, and on an earlier "butterfly barrier" of Brooks <ref type="bibr">[5]</ref>.</p><p>Processors in a butterfly or dissemination barrier engage in a series of symmetric, pairwise synchronizations, with no distinguished latecomer or champion. The dissemination barrier performs O(P log P ) accesses across the interconnection network, but only O(1og P ) on its critical path. If these accesses contend for a serialized resource such as a central processormemory bus, then a tree or tournament barrier, with only 0(P) network accesses, would be expected to out-perform it. However, on machines in which multiple independent network accesses can proceed in parallel, our experiments indicate that the dissemination barrier outperforms its competitors by a small constant factor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Empirical Performance Results</head><p>We have measured the performance of various spin lock and barrier algorithms on the BBN Butterfly 1, a distributed shared memory multiprocessor, and the Sequent Symmetry Model B, a cache-coherent, shared-bus multiprocessor. We were unable to test our reader-writer lock on either of these machines because of the lack of a compare-and-swap instruction, though recent experiments on a BBN TC2000 <ref type="bibr" target="#b16">[19]</ref> confirm that it scales extremely well. The MCS lock was implemented using the alternative version of r e l e a s e l o c k from algorithm 2. Anyone wishing to reproduce our results or extend our work to other machines can obtain copies of our C and assembler source code via anonymous ftp from titan.rice.edu (/public/scalable-synch).</p><p>Our results were obtained by embedding a lock acquisitionlrelease pair or a barrier episode inside a loop and averaging over a large number of operations. In the spin lock graphs, each data point (P, T ) represents the average time T for an individual processor to acquire and release the lock once, with P processors competing for access. In the barrier graphs, points represent the average time required for P processors to achieve a single barrier. Reported values include the overhead of an iteration of the test loop. To eliminate any effects due to timer interrupts or scheduler activity, on the Butterfly all timing measurements were made with interrupts disabled; on the Symmetry, the tmp-aff i n i t y system call was used to bind processes to processors. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Spin locks</head><p>Figure <ref type="figure">1</ref> shows the performance on the Butterfly of several spin lock algorithms. (Measurements were made for all numbers of processors; the tick marks simply differentiate between line types.) Figure <ref type="figure">2</ref> provides an expanded view of the four best performers, and figure <ref type="figure">3</ref> shows analogous results on the Symmetry. Since the Symmetry lacks an atomic fetch-- and-increment instruction, a ticket lock cannot be readily implemented and Anderson's lock is implemented as he suggests [3], by protecting its data structures with an outer test-and-set lock. So long as Anderson's lock is used for critical sections whose length exceeds that of the operations on its own data structures, processors do not seriously compete for the outer lock. In the absence of coherent caches, we modified Anderson's algorithm on the Butterfly to scatter the slots of its array across the available processor nodes. This modification reduces the impact of contention by spreading The top two curves in figure <ref type="figure">1</ref> are for simple test-and-set and ticket locks without backoff. They scale extremely badly. Most of the concern in the literature over synchronizationinduced contention appears to have been based on the behavior represented by the top curve in this graph. Linear backoff in a test-and-set lock doesn't help much, but exponential backoff for the test-and-set lock and proportional backoff for the ticket lock lead to much better scaling behavior. The MCS lock scales best of all. Figure <ref type="figure">2</ref> suggests that these three algorithms would continue to perform well even with thousands of processors competing. The slope of the curve for the MCS lock is 0.00025 ps per processor.</p><p>In many of the curves, the time to acquire and release a lock in the single processor case is significantly larger than the time required when multiple processors are competing for the lock. The principal reason for this apparent anomaly is that parts of each acquire/release protocol can execute in parallel when multiple processors compete. What we are measuring in our trials with many processors is not the time to execute an acquire/release pair from start to finish, but rather the length of time between consecutive lock acquisitions on separate processors. Complicating matters is that the time required to release an MCS lock depends on whether another processor is waiting.</p><p>The peak in the cost of the MCS lock on two processors reflects the lack of compare-and-swap. Some fraction of the time, a processor releasing the lock finds that its next variable is n i l but then discovers that it has a successor after all when it performs its f etch-and-store on the lock's tail pointer. Entering this timing window necessitates an additional fetch-and-store to restore the state of the queue, with a consequent drop in performance. The longer critical sections on the Symmetry (introduced to be fair to Anderson's lock) reduce the likelihood of hitting the window, thereby reducing the size of the two-processor peak. With compare-and-swap that peak would disappear altogether. Several factors skew the absolute numbers on the Butterfly, making them somewhat misleading. First, the test-and-set lock is implemented completely in line, while the others use subroutine calls. Second, the atomic operations on the Butterfly are inordinately expensive in comparison to their nonatomic counterparts. Third, those operations take 16-bit operands, and cannot be used to manipulate pointers directly. We believe the numbers on the Symmetry to be more representative of actual lock costs on modern machines; our recent experience on the TC2000 confirms this belief. We note that the single-processor latency of the MCS lock on the Symmetry is only 31% higher t,han that of the simple t e s t -and-set lock.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Busy-wait</head><p>In an attempt to uncover contention not evident in the spin lock timing measurements, we obtained an estimate of average network latency on the Butterfly by measuring the total time required to probe the network interface cont,roller on each of the processor nodes during a spin lock test. Table <ref type="table" target="#tab_2">1</ref> presents our results in the form of percentage increases over the value measured on an idle machine. In the Lock Node column, probes were made from the processor on which the lock itself resided (this processor was otherwise idle in all our tests); in the Idle Node column, probes were made from another processor not participating in the spin lock test. Values in the two columns are very different for the test-and-set and ticket locks (particularly without backoff) beca,use competition for access to the lock is focused on a central hot spot, and steals network interface bandwidth from the process attempting to perform the latency measurement on the lock node. Values in the two columns are similar for Anderson's lock because its data structure (and hence its induced contention) is distributed throughout the machine. Values are both similar and low for the MCS lock because its data structure is distributed and because each processor refrains from spinning on the remote portions of that data structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Barriers</head><p>Figure <ref type="figure">4</ref> shows the performance on the Butterfly of several  The next uses exponential backoff. The third backs off in proportion to the total number of processors in the barrier, with the hope that doing so will not only allow the remaining processors to reach the barrier, but also allow processors that have already arrived to notice that the barrier has been achieved. At a minimum, these three barriers require time linear in the number of participating processors. The combining tree barrier <ref type="bibr" target="#b24">[27]</ref> (modified to avoid f etch-and-@ operations during wakeup [18]) appears to perform logarithmically, but it still spins on remote locations, and has a large enough constant to take it out of the running.</p><p>Timings for the three most scalable algorithms on the Butterfly appear in expanded form in figure <ref type="figure" target="#fig_6">5</ref>. The dissemination barrier [12] performs best, followed by our tree barrier (algorithm 4), and by Hensgen, Finkel, and Manber's tournament barrier [12] (with our code <ref type="bibr">[18]</ref> for wakeup on machines lacking broadcast-see section 3). All three of these barriers scale logarithmically in the number of participating processors, with reasonable constants.</p><p>The comparative performance of barriers on the Symmetry is heavily influenced by the presence of coherent caches. Figure 6 presents results for the dissemination, tree, and tournament barriers (this time with the original wakeup flag for the tournament), together with two other algorithms that capitalize on the caches. One is a simple centralized barrier; the other combines the arrival phase of our tree barrier with a central wakeup flag. Since the bus-based architecture of the Symmetry serializes all network accesses, each algorithm's scaling behavior is ultimately determined by the number of network accesses it requires. The dissemination barrier requires 0(P log P ) ; each of the other algorithms requires only 0(P). Our experiments show that the arrival tree outperforms even the counter-based barrier for more than 16 processors; its 0(P) writes are cheaper than the latter's 0(P) f etch-and-@ operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">The Importance of Local Shared Memory</head><p>Because our software techniques can eliminate contention by spinning on locally-accessible locations, they argue in favor of architectures in which shared memory is physically distributed or coherently cached. On "dance hall" machines, in which shared memory must always be accessed through the processor-memory interconnect, we see no way to eliminate synchronization-related contention in software (except perhaps with a very high latency mechanism based on interprocessor messages or interrupts). All that any algorithm can do on such a machine is reduce the impact of hot spots, by distributing load throughout the memory and interconnection network. Both Cedar and the Ultracomputer include processor-local memory, but only for private code and data. The Monarch provides a small amount of local memory as a "poor man's instruction cache." In none of these machines can local memory be modified remotely. We consider the lack of local shared memory to be a significant architectural shortcoming; the inability to take full advantage of techniques such as those described in this paper is a strong argument against the construction of dance hall machines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dance hall machines include bus-based multiprocessors without coherent caches, and multistage interconnection net-</head><p>To quantify the importance of local shared memory, we used our Butterfly 1 to simulate a machine in which all shared memory is accessed through the interconnection network. By flipping a bit in the segment register for the synchronization variables on which a processor spins, we can cause the processor to go out through the network to reach these variables (even though they are in its own memory), without going through the network to reach code and private data. This trick effectively flattens the two-level shared memory hierarchy of the Butterfly into a single level organization similar to that of Cedar, the Monarch, or the Ultracomputer.</p><p>Figure <ref type="figure" target="#fig_8">7</ref> compares the performance of the dissemination and tree barrier algorithms for one and two level memory hierarchies. The bottom two curves are the same as in figures 4 and 5. The top two curves show the corresponding performance of the barrier algorithms when all accesses to shared memory are forced to go through the interconnection network: the time to achieve a barrier increases linearly with the number of processors participating.</p><p>In a related experiment, we measured the impact on network latency of executing the dissemination or tree barriers with and without local access to shared memory. The results appear in table 4.3. As in table 1, we probed the network interface controller on each processor to compare network latency of an idle machine with the latency observed during a 60 processor barrier. Table <ref type="table" target="#tab_2">1</ref> shows that when processors 1 dissemination 1 18% 117% 1 barrier 1 local polling 1 network polling 1</p><p>Table 2: Increase in network latency (relative to that of an idle machine) on the Butterfly caused by 60 processor barriers using local and network polling strategies. tree are able to spin on shared locations locally, average network latency increases only slightly. With only network access to shared memory, latency more than doubles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and Conclusions 10%</head><p>The principal conclusion of our work is that memory and interconnect contention due to busy-wait synchronization in shared-memory multiprocessors need not be a problem. This conclusion runs counter to widely-held beliefs. We have presented empirical performance results for a wide variety of busy-wait algorithms on both a cache-coherent multiprocessor and a multiprocessor with distributed shared memory. These results demonstrate that appropriate algorithms that exploit locality in a machine's memory hierarchy can virtually eliminate synchronization-related contention.</p><p>Although the scalable algorithms presented in this paper are unlikely to match the synchronization performance of a combining network, they will come close enough to provide an extremely at tractive alternative to complex, expensive h a r d ~a r e . ~ All that our algorithms require is locallyaccessible shared memory and common atomic instructions. The scalable barrier algorithms rely only on atomic read and write. The MCS lock uses fetch-and-store and (if available) compare-and-swap. Each of these instructions is useful for more than busy-wait locks. Herlihy has shown, for example <ref type="bibr" target="#b10">[13]</ref>, that compare-and-swap is a universal primitive for building non-blocking concurrent data structures. Because of their general utility, fetch-and-@ instructions are substantially more attractive to the programmer than specialpurpose synchronization primitives.</p><p>Our measurements on the Sequent Symmetry indicate that special-purpose synchronization mechanisms such as the QOSB instruction [9] are unlikely to outperform our MCS lock by more than 30%. A QOSB lock will have higher singleprocessor latency than a test-and-set lock [9, p.681, and its performance should be essentially the same as the MCS lock when competition for a lock is high. Goodman, Vernon, and Woest suggest that a QOSB-like mechanism can be implemented at very little incremental cost (given that they are already constructing large coherent caches with multidimensional snooping). We believe that this cost must be extremely low to make it worth the effort.</p><p>Of course, increasing the performance of busy-wait locks and barriers is not the only possible rationale for implement-124% 'Pfister and Norton <ref type="bibr" target="#b19">[22]</ref> estimate that message combining will increase the size and/or cost of an interconnection network 6to 32-fold. ing synchronization mechanisms in hardware. Recent work on weakly-consistent shared memory [I, 8, 161 has suggested the need for synchronization "fences" that provide clean points for memory semantics. Combining networks, likewise, may improve t h e performance of memory with bursty access patterns (caused, for example, by sharing after a barrier). We do not claim that hardware support for synchronization is unnecessary, merely that the most commonly-cited rationale for it-that it is essential t o reduce contention due t o synchronization-is invalid.</p><p>For future shared-memory multiprocessors, our results argue in favor of providing distributed memory or coherent caches, rather than dance-hall memory without coherent caches (as in Cedar, t h e Monarch, or the Ultracomputer). Our results also indicate that combining networks for such machines must be justified on grounds other than t h e reduction of synchronization overhead. We strongly suggest that future multiprocessors include a full set o f f etch-and-@ operations (especially f e t ch-and-st o r e and compare-and-swap).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>type qnode = record next : "qnode // ptr to successor in queue locked : Boolean // busy-waiting necessary type lock = "qnode // ptr to tail of queue // I points to a queue link record allocated // (in an enclosing scope) in shared memory // locally-accessible to the invoking processor procedure acquire_lock(L : "lock; I : "qnode) var pred : "qnode I-&gt;next := nil // initially, no successor pred := fetch_and_store(L, I) // queue for lock if pred != nil // lock was not free I-&gt;locked := true // prepare to spin pred-&gt;next := I // link behind predecessor repeat while I-&gt;locked // busy-wait for lock procedure release_lock(L * "lock; I : "qnode) if I-&gt;next = nil // no known successor if compare-and-swap(L, I, nil) return // no successor, lock free repeat while I-&gt;next = nil // wait for succ. I-&gt;next-&gt;locked := false // pass lock Algorithm 1: A queue-based spin lock with local-only spinning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>type qnode = record class : (reading, writing) next : "qnode state : record blocked : Boolean // need to spin successor.class : (none, reader, writer) type lock = record tail : "qnode := nil reader-count : integer := 0 next-writer : "qnode := nil // I points to a queue link record allocated // (in an enclosing scope) in shared memory // locally-accessible to the invoking processor procedure start.write(L : "lock; I : "qnode) with I", LA class := writing; next := nil; state := [true, none] pred : "qnode := fetch.and.store(Stai1, I) if pred = nil next-writer := I if reader-count = 0 and fetch.and_store(hext.writer,nil)=I // no reader who will resume me blocked := false else // must update successor.class before // updating next pred-&gt;successor_class := writer pred-&gt;next := I repeat while blocked procedure start_read(L : "lock; I : "qnode) with I", LA class := reading; next := n i l s t a t e := [true, none] pred : "qnode := fetch.and.store(Stai1, I ) i f pred = n i l atomic.increment(reader.count) blocked := f a l s e // f o r successor else i f pred-&gt;class = writing or compare_and_swap(Spred-&gt;state, [true, none] , [true, reader] ) // pred i s a writer, or a waiting // reader. pred w i l l increment // reader-count and release me pred-&gt;next := I repeat while blocked else // increment reader-count and go atomic.increment(reader.count) pred-&gt;next := I blocked := f a l s e i f successor~class = reader repeat while next = n i l atomic.increment(reader.count)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>type treenode = record wsense : Boolean parentpointer : "Boolean childpointers : array [O. .I] of "Boolean havechild : array [O. .3] of Boolean cnotready : array [O. .3] of Boolean dummy : Boolean // pseudo-data processor private vpid : integer // a unique "virtual processor" index processor private sense : Boolean shared nodes : array [O..P-11 of treenode // nodes[vpid] is allocated i n shared memory // locally-accessible t o processor vpid // f o r each processor i , sense is i n i t i a l l y true // i n nodes [i] :// havechild[j] = (4*i+j &lt; P) = havechild and wsense = f a l s e procedure tree-barrier with nodes[vpid] do repeat u n t i l cnotready = [ f a l s e , f a l s e , f a l s e , f a l s e ] cnotready := havechild // i n i t f o r next time parentpointer" := f a l s e // signal parent i f vpid != 0 // not root, wait u n t i l parent wakes me repeat u n t i l wsense = sense // signal children i n wakeup t r e e childpointers[O] " := sense childpointers[l] " := sense sense := not sense Algorithm 4: A scalable, distributed, tree-based barrier with only local spinning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>-Figure 1 :Figure 2 :</head><label>12</label><figDesc>Figure 1: Performance of spin locks on the Butterfly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>e-a test &amp; test &amp; set a a test &amp; set, exp. Performance of spin locks on the Symmetry it throughout the interconnection network and memory.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 4: Performance of barriers on the Butterfly. 500 450 400 350 300 Time 250 (fts)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Performance of barriers on the Symmetry.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>7 :</head><label>7</label><figDesc>Performance of tree and dissemination barriers on the Butterfly with and without local access to shared memory. work architectures such as Cedar [26], the BBN Monarch [24], and the NYU Ultracomputer [lo].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>release-lock operation, without compare-and-swap, appears in algorithm 2. Like the code in procedure release_lock(L : "lock; I : "qnode) var old-tail : "qnode if I-&gt;next = nil // no known successor old-tail := fetch_and_store(L, nil) if old-tail = 1 // really no successor return // We accidentally removed some processors // from the queue and have to put them back. usurper := fetch_and_store(L, old-tail) // wait for pointer to victim list repeat while I-&gt;next = nil if usurper != nil // usurper in queue ahead of our victims // link victims after the last usurper usurper-&gt;next := I-&gt;next else // pass lock to first victim I-&gt;next-&gt;locked := false else // pass lock to successor I-&gt;next-&gt;locked := false Algorithm 2: Code for release-lock, without compare-- and-swap.</head><label></label><figDesc>spins are local. Alternative code for the</figDesc><table><row><cell>algorithm 1, it spins on locally-accessible, processor-specific</cell></row><row><cell>memory locations only, requires constant space per lock, and</cell></row><row><cell>requires only O(1) network transactions regardless of whether</cell></row><row><cell>the machine provides coherent caches. It does not guarantee</cell></row><row><cell>FIFO ordering, however, and admits the theoretical possibil-</cell></row><row><cell>ity of starvation, though lock acquisitions are likely to remain</cell></row><row><cell>very nearly FIFO in practice. A correctness proof for the</cell></row><row><cell>MCS lock appears in a technical report [18].</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Lock t est-and-set</head><label></label><figDesc></figDesc><table><row><cell cols="2">Increase in</cell></row><row><cell cols="2">Network Latency</cell></row><row><cell cols="2">Measured From Lock Node 1 Idle Node</cell></row><row><cell>1420%</cell><cell></cell></row><row><cell>882%</cell><cell></cell></row><row><cell>992%</cell><cell></cell></row><row><cell>53%</cell><cell></cell></row><row><cell>75%</cell><cell>67%</cell></row><row><cell>4%</cell><cell>2%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Increase in network latency (relative to that of an idle machine) on the Butterfly caused by 60 processors competing for a busy-wait lock.</figDesc><table /></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Supported in part by the National Science Foundation under Cooperative Agreement number CCR-9045252.</p><p>^supported in part by the National Science Foundation under Institutional Infrastructure grant CDA-8822724.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Weak ordering-a new definition</title>
		<author>
			<persName><forename type="first">V</forename><surname>Adve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Computer Architecture</title>
		<meeting>the International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1990-05">May 1990</date>
			<biblScope unit="page" from="2" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adaptive backoff synchronization techniques</title>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cherian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Computer Architecture</title>
		<meeting>the International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1989-05">May 1989</date>
			<biblScope unit="page" from="396" to="406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The performance of spin lock alternatives for shared-memory multiprocessors</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Anderson</surname></persName>
		</author>
		<idno>6148</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Parallel and Distributed Systems</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="6" to="16" />
			<date type="published" when="1986-03">Jan. 1990. Mar. 1986</date>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>BBN Laboratories</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note>BBN Laboratories. Butterfly parallel processor overview</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Brooks 111. The butterfly barrier</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Parallel Programming</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Brooks 111. The shared memory hypercube</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Parallel Computing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="235" to="245" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Concurrent control with &apos;readers&apos; and &apos;writers</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Courtois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Heymans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Parnas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="667" to="668" />
			<date type="published" when="1971-10">Oct. 1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Memory consistency and event ordering in scalable shared-memory multiprocessors</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gharachorloo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lenoski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Laudon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Hennessy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Computer Architecture</title>
		<meeting>the International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1990-05">May 1990</date>
			<biblScope unit="page" from="15" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Efficient synchronization primitives for large-scale cache-coherent multiprocessors</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Vernon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Woest</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the Third International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="1989-04">Apr. 1989</date>
			<biblScope unit="page" from="64" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The NYU Ultracomputer -Designing an MIMD shared memory parallel computer</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gottlieb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grishman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Kruskal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Mcauliffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rudolph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Snir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers, C</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="175" to="189" />
			<date type="published" when="1983-02">Feb. 1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Two algorithms for barrier synchronization</title>
		<author>
			<persName><forename type="first">G</forename><surname>Graunke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thakkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hensgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Manber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Parallel Programming</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
	<note>Synchronization algorithms [12]</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A methodology for implementing highly concurrent data structures</title>
		<author>
			<persName><forename type="first">M</forename><surname>Herlihy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second ACM Symposium on Principles and Practice of Parallel Programming</title>
		<meeting>the Second ACM Symposium on Principles and Practice of Parallel Programming</meeting>
		<imprint>
			<date type="published" when="1990-03">Mar. 1990</date>
			<biblScope unit="page" from="197" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distributed synchronizers</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Jayasimha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1988 International Conference on Parallel Processing</title>
		<meeting>the 1988 International Conference on Parallel Processing</meeting>
		<imprint>
			<date type="published" when="1988-08">Aug. 1988</date>
			<biblScope unit="page" from="23" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient synchronization on multiprocessors with shared memory</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Kruskal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rudolph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Snir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth ACM Symposium on Principles of Distributed Computing</title>
		<meeting>the Fifth ACM Symposium on Principles of Distributed Computing</meeting>
		<imprint>
			<date type="published" when="1986">1986</date>
			<biblScope unit="page" from="218" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Synchronization with multiprocessor caches</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Ramachandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Computer Architecture</title>
		<meeting>the International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1990-05">May 1990</date>
			<biblScope unit="page" from="27" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Synchronization barrier and related tools for shared memory parallel programming</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lubachevsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1989 International Conference on Parallel Processing</title>
		<meeting>the 1989 International Conference on Parallel Processing</meeting>
		<imprint>
			<date type="published" when="1989-08">Aug. 1989</date>
			<biblScope unit="page" from="11" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Algorithms for scalable synchronization on shared-memory multiprocessors</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mellor-Crummey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Scott</surname></persName>
		</author>
		<idno>Also COMP TR90-114</idno>
		<imprint>
			<date type="published" when="1990-04">Apr. 1990. May 1990</date>
			<biblScope unit="volume">342</biblScope>
		</imprint>
		<respStmt>
			<orgName>Computer Science Department, University of Rochester ; Department of Computer Science, Rice University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Scalable readerwriter synchronization for shared-memory multiprocessors</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mellor-Cmmmey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third ACM Symposium on Principles and Practice of Parallel Programming</title>
		<meeting>the Third ACM Symposium on Principles and Practice of Parallel Programming</meeting>
		<imprint>
			<date type="published" when="1991-04">Apr. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Working Group of the IEEE Computer Society Microprocessor Standards Committee. SCI (scalable coherent interface): An overview of extended cache-coherence protocols</title>
		<idno>Draft 0.59 P1596/Part III-D</idno>
		<imprint>
			<date type="published" when="1596-02">1596. Feb. 5, 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The IBM research parallel processor prototype (RP3): Introduction and architecture</title>
		<author>
			<persName><forename type="first">G</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1985 International Conference on Parallel Processing</title>
		<meeting>the 1985 International Conference on Parallel Processing</meeting>
		<imprint>
			<date type="published" when="1985-08">Aug. 1985</date>
			<biblScope unit="page" from="764" to="771" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hot spot&quot; contention and combining in multistage interconnection networks</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">A</forename><surname>Norton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers, C</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="943" to="948" />
			<date type="published" when="1985-10">Oct. 1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Synchronization with eventcounts and sequencers</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Kanodia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="115" to="123" />
			<date type="published" when="1979-02">Feb. 1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The Monarch parallel processor hardware design</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Rettberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Crowther</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Carvey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Tomlinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="30" />
			<date type="published" when="1990-04">Apr. 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dynamic decentralized cache schemes for MIMD parallel processors</title>
		<author>
			<persName><forename type="first">L</forename><surname>Rudolph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Segall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Computer Architecture</title>
		<meeting>the International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1984">1984</date>
			<biblScope unit="page" from="340" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Architecture of the Cedar parallel supercomputer</title>
		<author>
			<persName><forename type="first">P.-C</forename><surname>Yew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CSRD report</title>
		<imprint>
			<biblScope unit="volume">609</biblScope>
			<date type="published" when="1986-08">Aug. 1986</date>
		</imprint>
		<respStmt>
			<orgName>Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Distributing hotspot addressing in large-scale multiprocessors</title>
		<author>
			<persName><forename type="first">P.-C</forename><surname>Yew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N.-F</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Lawrie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers, C</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="60" to="69" />
			<date type="published" when="1987-04">Apr. 1987. June 1990</date>
		</imprint>
	</monogr>
	<note>Computer</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
