<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Real-Time 3D Reconstruction and 6-DoF Tracking with an Event Camera</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hanme</forename><surname>Kim</surname></persName>
							<email>hanme.kim@imperial.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Stefan</forename><surname>Leutenegger</surname></persName>
							<email>s.leutenegger@imperial.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
							<email>a.davison@imperial.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Real-Time 3D Reconstruction and 6-DoF Tracking with an Event Camera</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7DAF643F5C720C430A071F1709504943</idno>
					<idno type="DOI">10.1007/978-3-319-46466-4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>6-DoF tracking</term>
					<term>3D reconstruction</term>
					<term>Intensity reconstruction</term>
					<term>Visual odometry</term>
					<term>SLAM</term>
					<term>Event-based camera</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a method which can perform real-time 3D reconstruction from a single hand-held event camera with no additional sensing, and works in unstructured scenes of which it has no prior knowledge. It is based on three decoupled probabilistic filters, each estimating 6-DoF camera motion, scene logarithmic (log) intensity gradient and scene inverse depth relative to a keyframe, and we build a real-time graph of these to track and model over an extended local workspace. We also upgrade the gradient estimate for each keyframe into an intensity image, allowing us to recover a real-time video-like intensity sequence with spatial and temporal super-resolution from the low bit-rate input event stream. To the best of our knowledge, this is the first algorithm provably able to track a general 6D motion along with reconstruction of arbitrary structure including its intensity and the reconstruction of grayscale video that exclusively relies on event camera data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Event cameras offer a breakthrough new paradigm for real-time vision, with potential in robotics, wearable devices and autonomous vehicles, but it has proven very challenging to use them in most standard computer vision problems. Inspired by the superior properties of human vision <ref type="bibr" target="#b1">[2]</ref>, an event camera records not image frames but an asynchronous sequence of per-pixel intensity changes, each with a precise timestamp. While this data stream efficiently encodes image dynamics with extremely high dynamic range and temporal contrast, the lack of synchronous intensity information means that it is not possible to apply much of the standard computer vision toolbox of techniques. In particular, the multi-view correspondence information which is essential to estimate motion and structure is difficult to obtain because each event by itself carries little information and no signature suitable for reliable matching.</p><p>Approaches aiming at simultaneous camera motion and scene structure estimation therefore need also to jointly estimate the intensity appearance of the scene, or at least a highly descriptive function of this such as a gradient map. So far, this has only been successfully achieved in the reduced case of pure camera rotation, where the scene reconstruction takes the form of a panorama image.</p><p>In this paper we present the first algorithm which performs joint estimation of 3D scene structure, 6-DoF camera motion and up to scale scene intensity from a single hand-held event camera moved in front of an unstructured static scene. Our approach runs in real-time on a standard PC. The core of our method is three interleaved probabilistic filters, each estimating one unknown aspect of this challenging Simultaneous Localisation and Mapping (SLAM) problem: camera motion, scene log intensity gradient and scene inverse depth. From pure event input our algorithm generates various outputs including a real-time, high bandwidth 6-DoF camera track, scene depth map for one or multiple linked keyframes, and a high dynamic range reconstructed video sequence at a userchosen frame-rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Event-Based Cameras</head><p>The event camera or silicon retina is gradually becoming more widely known by researchers in computer vision, robotics and related fields, in particular since the release as a commercial device for researchers of the Dynamic Vision Sensor (DVS) <ref type="bibr" target="#b13">[14]</ref> shown in Fig. <ref type="figure" target="#fig_0">1(c</ref>). The pixels of this device asynchronously report log intensity changes of a pre-set threshold size as a stream of asynchronous events, each with pixel location, polarity, and microsecond-precise timestamp. Figure <ref type="figure" target="#fig_0">1</ref> visualises some of the main properties of the event stream; in particular the almost continuous response to very rapid motion and the way that the output data-rate depends on scene motion, though in practice almost always dramatically lower than that of standard video. These properties offer the potential to overcome the limitations of real-world computer vision applications, relying on conventional imaging sensors, such as high latency, low dynamic range, and high power consumption.</p><p>Recently, cameras have been developed that interleave event data with conventional intensity frames (DAVIS <ref type="bibr" target="#b2">[3]</ref>), or per-event intensity measurement (ATIS <ref type="bibr" target="#b20">[21]</ref>). Our framework could be extended to make use of these image measurements this would surely make joint estimation easier. However, in a persistently dynamic motion, they may not be useful. Also, they partially break the appeal and optimal information efficiency of a pure event-based data stream. We therefore believe that first solving the hardest problem of not relying on standard image frames will be useful on its own and provides the insights to make best use of additional measurements if they are available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Related Work</head><p>Early published work using event cameras focused on tracking moving objects from a fixed point of view, successfully showing the superior high speed measurement and low latency properties <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8]</ref>. However, work on tracking and reconstruction of more general, previously unknown scenes with a freely moving event camera, which we believe is the best place to take full advantage of its remarkable properties, has been limited. The clear difficulty is that most methods normally used in tracking and mapping, such as feature detection and matching or whole image alignment, cannot be directly applied to its fundamentally different visual measurement stream.</p><p>Cook et al. <ref type="bibr" target="#b6">[7]</ref> proposed an interacting network which interprets a stream of events to recover different visual estimate 'maps' of scenes such as intensity, gradient and optical flow while estimating global rotating camera motion. More recently, Bardow et al. <ref type="bibr" target="#b0">[1]</ref> presented an optical flow and intensity estimation using an event camera which allows any camera motion as well as dynamic scenes.</p><p>An early 2D SLAM method was proposed by Weikersdorfer et al. <ref type="bibr" target="#b23">[24]</ref> which tracks a ground robot pose while reconstructing a planar ceiling map with an upward looking DVS camera. Mueggler et al. <ref type="bibr" target="#b18">[19]</ref> presented an onboard 6-DoF localisation flying robot system which is able to track its relative pose to a known target even at very high speed. To investigate whether current techniques can be applied to a large scale visual SLAM problem, Milford et al. <ref type="bibr" target="#b15">[16]</ref> presented a simple visual odometry system using a DVS camera with loop closure built on top of the SeqSLAM algorithm using events accumulated into frames <ref type="bibr" target="#b16">[17]</ref>.</p><p>In a much more constrained and hardware-dependent setup, Schraml et al. <ref type="bibr" target="#b21">[22]</ref> developed a special 360 • rotating camera that consists of a pair of dynamic vision line sensors which creates 3D panoramic scenes aided by its embedded encoders and stereo event streams. Combined with an active projector, Matsuda et al. <ref type="bibr" target="#b14">[15]</ref> showed that high quality 3D object reconstruction can be achievable which is better than for laser scanners or RGB-D cameras in some specific situations.</p><p>The most related work to our method is the simplified SLAM system based on probabilistic filtering proposed by Kim et al. <ref type="bibr" target="#b11">[12]</ref>, which estimates spatial gradients which are then integrated to reconstruct high quality and high dynamic range planar scenes while tracking global camera rotation. Their method has a similar overall concept to ours with multiple interacting probabilistic filters, but is limited to pure rotation camera motion and panorama reconstruction. Also it is not completely real-time because of the computational complexity of the particle filter used in their tracking algorithm.</p><p>There have been no previous published results on estimating 3D depth from a single moving event camera. Most researchers working with event cameras have assumed that this problem is too difficult, and attempts at 3D estimation have combined an event camera with other sensors: a standard frame-based CMOS camera <ref type="bibr" target="#b4">[5]</ref>, or an RGB-D camera <ref type="bibr" target="#b22">[23]</ref>. These are, of course, possible practical ways of using an event camera for solving SLAM problems. However, we believe that resorting to standard sensors discards many of the advantages of processing the efficient and data-rick pure event stream, as well as introducing extra complication including synchronisation and calibration problems to be solved. One very interesting approach if the application permits is to combine two event cameras in a stereo setup <ref type="bibr" target="#b3">[4]</ref>. The nicest part of that method is the way that stereo matching of events can be achieved based on coherent timestamps.</p><p>Our work in this paper was inspired by a strong belief that depth estimation from a single moving event camera must be possible, because if the device is working correctly and recording all pixel-wise intensity changes then all of the information present in a standard video stream must be available in principle, at least up to scale. In fact, the high temporal contrast and dynamic range of event pixels means that much more information should be present in an event stream than in standard video at the same resolution. In particular, the results of Kim et al. <ref type="bibr" target="#b11">[12]</ref> on sub-pixel tracking and super-resolution mosaic reconstruction from events gave a strong indication that the accurate multi-view correspondence needed for depth estimation is possible. The essential insight to extending Kim et al.'s approach towards getting depth from events is that once the camera starts to translate, if two pixels have the same intensity gradient, the one which is closer to the camera move past the camera faster and therefore emit more events than the farther one. This is the essential mechanism built into our probabilistic filter for inverse depth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Following many recent successful SLAM systems such as PTAM <ref type="bibr" target="#b12">[13]</ref>, DTAM <ref type="bibr" target="#b19">[20]</ref>, and LSD-SLAM <ref type="bibr" target="#b9">[10]</ref>, which separate the tracking and mapping components based on the assumption that the current estimate from one component is accurate enough to lock for the purposes of estimating the other, the basic structure of our approach relies on three interleaved probabilistic filters. One tracks the global 6-DoF camera motion; the second estimates the log intensity gradients in a keyframe image -a representation which is also in parallel upgraded into a full image-like intensity map. Finally the third filter estimates the inverse depths of a keyframe. It should be noted that we essentially separate the mapping part into two, i.e. the gradient and inverse depth estimations, considering fewer number of events caused by parallax while almost all events carry gradient information. We also build a textured semi-dense 3D point cloud from selected keyframes with their associated reconstructed intensity and inverse depth estimate. We do not use an explicit bootstrapping method as we have found that, starting from scratch, alternating estimation very often lead to convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Preliminaries</head><p>We denote an event as e(u, v) = (u, v, p, t) where u and v are pixel location, p is polarity and t is microsecond-precise timestamp -our event-based camera has the fixed pre-calibrated intrinsic matrix K and all event pixel locations are prewarped to remove radial distortion. We also define two important time intervals τ and τ c , as in <ref type="bibr" target="#b11">[12]</ref>, which are the time elapsed since the most recent previous event from any pixel and at the same pixel respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Event-Based Camera 6-DoF Tracking</head><p>We use an Extended Kalman Filter (EKF) to estimate the global 6-DoF camera motion over time with its state x ∈ R 6 , which is a minimal representation of the camera pose c with respect to the world frame of reference w, and covariance matrix P x ∈ R 6×6 . The state vector is mapped to a member of the Lie group SE(3), the set of 3D rigid body transformations, by the matrix exponential map:</p><formula xml:id="formula_0">T wc = exp 6 i=1 x i G i = R wc t w 0 1 , (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where G is the Lie group generator for SE(3), R wc ∈ SO(3), and t w ∈ R 3 . The basic idea is to find (assuming that the current log intensity and inverse depth estimates are correct) the camera pose which best predicts a log intensity change consistent with the event just received, as shown in Fig. <ref type="figure" target="#fig_1">2(a)</ref>.</p><p>Motion Prediction. We use a 6-DoF (translation and rotation) constant position motion model for motion prediction; the variance of the prediction is proportional to the time interval:</p><formula xml:id="formula_2">x (t|t-τ ) = x (t-τ |t-τ ) + n ,<label>(2)</label></formula><formula xml:id="formula_3">P x (t|t-τ ) = P (t-τ |t-τ ) x + P n ,<label>(3)</label></formula><p>where each component of n is independent Gaussian noise in all six axes i.e. n i ∼ N (0, σ 2 i τ ), and P n = diag(σ 2 1 τ, . . . , σ 2 6 τ ). , where the previous event was received at the same pixel, and a reconstructed image-like log intensity keyframe with inverse depth by taking a log intensity difference between two corresponding ray-triangle intersection points, p (t) w and p (t-τc) w , as shown in Fig. <ref type="figure" target="#fig_2">3</ref>:</p><formula xml:id="formula_4">z x = ±C , (<label>4</label></formula><formula xml:id="formula_5">)</formula><formula xml:id="formula_6">h x (x (t|t-τ ) ) = I l p (t) w -I l p (t-τc) w ,<label>(5)</label></formula><p>where</p><formula xml:id="formula_7">I l (p w ) = (1 -a -b)I l (v 0 ) + aI l (v 1 ) + bI l (v 2 ) . (<label>6</label></formula><formula xml:id="formula_8">)</formula><p>Here ±C is a known event threshold -its sign is decided by the polarity of an event. I l is a log intensity value based on a reconstructed log intensity keyframe, and v 0 , v 1 , and v 2 are three vertices of an intersected triangle. To obtain a corresponding 3D point location p w in the world frame of reference, we use raytriangle intersection <ref type="bibr" target="#b17">[18]</ref> which yields a vector (l, a, b) where l is the distance to the triangle from the origin of the ray and a, b are the barycentric coordinates of the intersected point which is then used to calculate an interpolated log intensity.</p><p>In the EKF framework, the camera pose estimate and its uncertainty covariance matrix are updated by the standard equations at every event using: , in the world frame of reference using a ray-triangle intersection method <ref type="bibr" target="#b17">[18]</ref> to compute the value of a measurement -a log intensity difference between two points given an event e(u, v), the current keyframe pose T wk , the current camera pose estimate T , to find a displacement vector between them, which is then used to calculate a motion vector m to compute the value of a measurement (g • m) at a midpoint pk based on the brightness constancy and the linear gradient assumption.</p><formula xml:id="formula_9">x (t|t) = x (t|t-τ ) + W x ν x ,<label>(7)</label></formula><formula xml:id="formula_10">P (t|t) x = I 6×6 -W x ∂h x ∂x (t|t-τ ) P (t|t-τ ) x ,<label>(8)</label></formula><p>where the innovation ν x and Kalman gain W x are defined by the standard EKF definitions. The measurement uncertainty is a scalar variance σ 2</p><p>x , and we omit the Jacobian ∂hx ∂x (t|t-τ ) derivation due to the space limitation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Gradient Estimation and Log Intensity Reconstruction</head><p>We now use the updated camera pose estimate to incrementally improve the estimates of the log intensity gradient at each keyframe pixel based on a pixelwise EKF. However, because of the random walk nature of our tracker which generates a noisy motion estimate, we first apply a weighted average filter to the new camera pose estimate. To reconstruct super resolution scenes by harnessing the very high speed measurement property of the event camera, we use a higher resolution for keyframes than for the low resolution sensor. This method is similar to the one in <ref type="bibr" target="#b11">[12]</ref>, but we model the measurement noise properly to get better gradient estimate, and use a parallelisable reconstruction method for speed.</p><p>Pixel-Wise EKF Based Gradient Estimation. Each pixel of the keyframe holds an independent gradient estimate g(p k ) = (g u , g v ) , consisting of log intensity gradients g u and g v along the horizontal and vertical axes in image space respectively, and a 2 × 2 uncertainty covariance matrix P g (p k ). At initialisation, all gradients are initialised to zero with large variances. We assume, based on the rapidity of events, a linear gradient between two consecutive events at the same event camera pixel, and update the midpoint pk of the two projected points p . We now define z g , a measurement of the instantaneous event rate at this pixel, and its measurement model h g based on the brightness constancy equation (g • m)τ c = ±C, where g is a gradient estimate and m = (m u , m v ) is a motion vector -the displacement between two corresponding pixels in the current keyframe divided by the elapsed time τ c as shown in Fig. <ref type="figure" target="#fig_2">3</ref>:</p><formula xml:id="formula_11">z g = ± C τ c , (<label>9</label></formula><formula xml:id="formula_12">)</formula><formula xml:id="formula_13">h g = (g(p k ) • m) , (<label>10</label></formula><formula xml:id="formula_14">)</formula><p>where m = p</p><formula xml:id="formula_15">(t) k -p (t-τc) k τ c . (<label>11</label></formula><formula xml:id="formula_16">)</formula><p>The current gradient estimate and its uncertainty covariance matrix at that pixel are updated independently in the same way as in the measurement update of our tracker following the standard EKF equations.</p><p>The Jacobian ∂hg ∂g(p k ) (t-τc) of the measurement function with respect to changes in gradient is simply (m u , m v ), and the measurement noise N g is:</p><formula xml:id="formula_17">N g = ∂z g ∂C P C ∂z g ∂C = σ 2 C τ 2 c , (<label>12</label></formula><formula xml:id="formula_18">)</formula><p>where σ 2 C is the sensor noise with respect to the event threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Log Intensity Reconstruction.</head><p>Along with the pixel-wise EKF based gradient estimation method, we perform interleaved absolute log intensity reconstruction running on a GPU. We define our convex minimisation function as: min</p><formula xml:id="formula_19">I l Ω ||g(p k ) -∇I l (p k )|| h d + λ||∇I l (p k )|| h r dp k . (<label>13</label></formula><formula xml:id="formula_20">)</formula><p>Here the data term represents the error between estimated gradients g(p k ) and those of a reconstructed log intensity ∇I l (p k ), and the regularisation term enforces smoothness, both under a robust Huber norm. This function can be written using the Legendre Fenchel transformation <ref type="bibr" target="#b10">[11]</ref> as follows:</p><p>min where we can solve by maximising with respect to p:</p><formula xml:id="formula_21">I l max q max p { p, g -∇I l - d 2 ||p|| 2 -δ p (p) + q, ∇I l - r 2λ ||q|| 2 -δ q (q)} ,<label>(14)</label></formula><formula xml:id="formula_22">p (n+1) = p (n) +σp(g-∇I l ) 1+σp d max 1, p (n) +σp(g-∇I l ) 1+σp d ,<label>(15)</label></formula><p>maximising with respect to q:</p><formula xml:id="formula_23">q (n+1) = q (n) +σq∇I l 1+ σq r λ max 1, 1 λ q (n) +σq∇I l 1+ σq r λ , (<label>16</label></formula><formula xml:id="formula_24">)</formula><p>and minimising with respect to I l :</p><formula xml:id="formula_25">I (n+1) l = I (n) l -σ I l (divp (n+1) -divq (n+1) ) . (<label>17</label></formula><formula xml:id="formula_26">)</formula><p>We visualise the progress of gradient estimation and log intensity reconstruction over time during hand-held event camera motion in Fig. <ref type="figure" target="#fig_5">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Inverse Depth Estimation and Regularisation</head><p>We now use the same camera pose estimate as in the gradient estimation and a reconstructed log intensity keyframe to incrementally improve the estimates of the inverse depth at each keyframe pixel based on another pixel-wise EKF.</p><p>As in camera pose estimation, assuming that the current camera pose estimate and reconstructed log intensity are correct, we aim to update the inverse depth estimate to best predict the log intensity change consistent with the current event polarity as shown in Fig. <ref type="figure" target="#fig_1">2(b)</ref>.</p><p>Pixel-Wise EKF Based Depth Estimation. Each pixel of the keyframe holds an independent inverse depth state value ρ(p k ) with variance σ 2 ρ(p k ) . At initialisation, all inverse depths are initialised to nominal values with large variances. In the same way as in our tracking method, we calculate the value of a measurement z ρ which is a log intensity difference between two corresponding ray-triangle intersection points p (t) w and p (t-τc) w as shown in Fig. <ref type="figure" target="#fig_2">3</ref>:</p><formula xml:id="formula_27">z ρ = ±C , (<label>18</label></formula><formula xml:id="formula_28">)</formula><formula xml:id="formula_29">h ρ = I l p (t) w -I l p (t-τc) w . (<label>19</label></formula><formula xml:id="formula_30">)</formula><p>In the EKF framework, we stack the inverse depths of all three vertices ρ = (ρ v0 , ρ v1 , ρ v2 ) which contribute to the intersected 3D point and update them with their associated 3 × 3 uncertainty covariance matrix at every event in the same way of the measurement update of our tracker following the standard EKF equations. The measurement noise N ρ is a scalar variance σ 2 ρ , and we omit the Jacobian ∂hρ ∂ρ (t-τc) derivation due to the space limitation.</p><p>Inverse Depth Regularisation. As a background process running on a GPU, we perform inverse depth regularisation on keyframe pixels with high confidence inverse depth estimate whenever there has been a large change in the estimates. We penalise deviation from a spatially smooth inverse depth map by assigning each inverse depth value the average of its neighbours, weighted by their respective inverse variances as described in <ref type="bibr" target="#b8">[9]</ref>. If two adjacent inverse depths are different more than 2σ, they do not contribute to each other to preserve discontinuities due to occlusion boundaries. We visualise the progress of inverse depth estimation and regularisation over time as event data is captured during hand-held event camera motion in Fig. <ref type="figure" target="#fig_6">5</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Our algorithm runs in real-time on a standard PC with typical scenes and motion speed, and we have conducted experiments both indoors and outdoors. We recommend viewing our video<ref type="foot" target="#foot_0">1</ref> which illustrates all of the key results in a better form than still pictures and in real-time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Single Keyframe Estimation</head><p>We demonstrate the results from our algorithm as it tracks against and reconstructs a single keyframe in a number of different scenes. In Fig. <ref type="figure" target="#fig_7">6</ref>, for each scene we show column by column an image-like view of the event streams, estimated gradient map, reconstructed intensity map with super resolution and high dynamic range properties, estimate depth map and semi-dense 3D point cloud. The 3D reconstruction quality is generally good, though we can see that there are sometimes poorer quality depth estimates near to occlusion boundaries and where not enough events have been generated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multiple Keyframes</head><p>We evaluated the proposed method on several trajectories which require multiple keyframes to cover. If the camera has moved too far away from the current keyframe, we create a new keyframe from the most recent estimation results and reconstruction. To create a new keyframe, we project all 3D points based on the current keyframe pose and the estimated inverse depth into the current camera pose, and propagate the current estimates and reconstruction only if they have high confidence in inverse depth. Figure <ref type="figure">7</ref> shows one of the results in a semi-dense 3D point cloud form constructed based on generated keyframes each consisting of reconstructed super-resolution and high dynamic range intensity and inverse depth map. The bright RGB 3D coordinate axes represent the current camera pose while the darker ones show all keyframe poses generated in this experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Video Rendering</head><p>Using our proposed method, we can turn an event-based camera into a high speed and high dynamic range artificial camera by rendering video frames based on ray-casting as shown in Fig. <ref type="figure">8</ref>. Here we choose to render at the same low resolution as event-based input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">High Speed Tracking</head><p>We evaluated the proposed method on several trajectories which include rapid motion (e.g. shaking hand). The top graph in Fig. <ref type="figure" target="#fig_9">9</ref> shows the estimated camera pose history, and the two groups of the insets below show an image-like event  visualisation, a rendered video frame showing the quality of our tracker, and a motion blurred standard camera video frame as a reference of rapid motion. Our current implementation is not able to process this very high event-rate (up to 1 M events per second in this experiment) in real-time, but we believe it is a simple matter of engineering to run at this extremely high rate in real-time in the near future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Discussion</head><p>Our results so far are qualitative, and we have focused on demonstrating the core novelty of our approach in breaking through to get joint estimation of depth, 6-DoF motion and intensity from pure event data with general motion and unknown general scenes. There are certainly still weakness in our current approach, and while we believe that it is remarkable that our approach of three interleaved filters, each of which operates as if the results of the others are correct, works at all, there is plenty of room for further research. It is clear that the interaction of these estimation processes is key, and in particular that the relatively slow convergence of inverse depth estimates tends to cause poor tracking, then data association errors and a corruption of other parts of the estimation process. We will investigate this further, and may need to step back from our current approach of real-time pure event-by-event processing towards a partially batch estimation approach in order to get better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>To the best of our knowledge, this is the first 6-DoF tracking and 3D reconstruction method purely based on a stream of events with no additional sensing, and it runs in real-time on a standard PC. We hope this opens up the door to practical solutions to the current limitations of real-world SLAM applications. It is worth restating that the measurement rate of the event-based camera is on the order of a microsecond, its independent pixel architecture provides very high dynamic range, and the bandwidth of an event stream is much lower than a standard video stream. These superior properties of event-based cameras offer the potential to overcome the limitations of real-world computer vision applications relying on conventional imaging sensors.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Event-based camera: (a): in contrast to standard video frames shown in the upper graph, a stream of events from an event camera, plotted in the lower graph, offers no redundant data output, only informative pixels or no events at all. Red and blue dots represent positive and negative events respectively (this figure was recreated inspired by the associated animation of [19]: https://youtu.be/LauQ6LWTkxM?t=35s). (b): image-like visualisation by accumulating events within a time interval -white and black pixels represent positive and negative events respectively. (c): the first commercial event camera, DVS128, from iniLabs Ltd. (Color figure online)</figDesc><graphic coords="3,57.48,66.74,337.60,129.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Camera pose and inverse depth estimation. (a): based on the assumption that the current log intensity estimate (shown as the colour of the solid line) and inverse depth estimate (shown as the geometry of the solid line) are correct, we find current camera pose T (t) wc most consistent with the predicted log intensity change since the previous event at the same pixel at pose T (t-τc) wc compared to the current event polarity. (b): similarly for inverse depth estimation, we assume that the current reconstructed log intensity and camera pose estimate are correct, and find the most probable inverse depth consistent with the new event measurement. (Color figure online)</figDesc><graphic coords="6,47.79,53.84,328.36,90.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Basic geometry for; tracking and inverse depth estimation: we find two corresponding ray-triangle intersection points, p (t) w and p (t-τc) w</figDesc><graphic coords="7,56.97,54.53,338.44,174.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>τc) wc, the reconstructed log intensity and inverse depth keyframe, gradient estimation: we project two intersection points onto the current keyframe, p</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Typical progression (left to right) of gradient estimation and log intensity reconstruction as a hand-held camera browses a 3D scene. The colours and intensities on the top row represent the orientations and strengths of the gradients of the scene (refer to the colour chart in the top right). In the bottom row, we see these gradient estimates upgraded to reconstructed intensity images. (Color figure online)</figDesc><graphic coords="9,57.48,54.14,337.36,131.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Typical temporal progression (left to right) of inverse depth estimation and regularisation as a hand-held camera browses a 3D scene. The colours on the top row represent the different depths of the scene (refer to the colour chart in the top right and the associated semi-dense 3D point cloud on the bottom row). (Color figure online)</figDesc><graphic coords="10,43.29,417.38,337.48,122.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. in various settings of the different aspects of our joint estimation algorithm. (a) visualisation of the input event stream; (b) estimated gradient keyframes; (c) reconstructed intensity keyframes with super resolution and high dynamic range properties; (d) estimated depth maps; (e) semi-dense 3D point clouds.</figDesc><graphic coords="12,43.29,80.15,337.48,425.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .Fig. 8 .</head><label>78</label><figDesc>Fig. 7. 3D cloud of an indoor scene constructed from multiple keyframes, showing keyframe poses with their intensity and depth map estimates. (Color figure online)</figDesc><graphic coords="13,57.48,251.87,337.12,65.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. The top graph shows the estimated camera pose history, and the two groups of the insets below show an image-like event visualisation, a rendered frame showing the quality of our tracker, and a motion blurred standard camera video frame as a reference of rapid motion (up to 5 Hz in this experiment).</figDesc><graphic coords="14,41.79,53.93,340.12,147.76" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://youtu.be/yHLyhdMSw7w.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. Hanme Kim was supported by an EPSRC DTA scholarship and the Qualcomm Innovation Fellowship 2014. We thank Jacek Zienkiewicz, Ankur Handa, Patrick Bardow, Edward Johns and other colleagues at Imperial College London for many useful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Simultaneous optical flow and intensity estimation from an event camera</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bardow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Leutenegger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neuromorphic Chips</title>
		<author>
			<persName><forename type="first">K</forename><surname>Boahen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific American</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A 240 × 180 130 dB 3 µs latency global shutter spatiotemporal vision sensor</title>
		<author>
			<persName><forename type="first">C</forename><surname>Brandli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Delbruck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Solid-State Circ. (JSSC)</title>
		<imprint>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2333" to="2341" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Event-based 3D reconstruction from neuromorphic retinas</title>
		<author>
			<persName><forename type="first">J</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ieng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Posch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Benosman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="27" to="38" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Low-latency event-based visual odometry</title>
		<author>
			<persName><forename type="first">A</forename><surname>Censi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Robotics and Automation</title>
		<meeting>the IEEE International Conference on Robotics and Automation</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A pencil balancing robot using a pair of AER dynamic vision sensors</title>
		<author>
			<persName><forename type="first">J</forename><surname>Conradt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lichtsteiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Delbruck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Circuits and Systems (ISCAS</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Interacting maps for fast visual interpretation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gugelmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jug</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Krautz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Neural Networks (IJCNN)</title>
		<meeting>the International Joint Conference on Neural Networks (IJCNN)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast sensory motor control based on event-based hybrid neuromorphic-procedural system</title>
		<author>
			<persName><forename type="first">T</forename><surname>Delbruck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lichtsteiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Circuits and Systems (ISCAS)</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semi-dense visual odometry for a monocular camera</title>
		<author>
			<persName><forename type="first">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV</title>
		<meeting>the International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">LSD-SLAM: large-scale direct monocular SLAM</title>
		<author>
			<persName><forename type="first">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schöps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-10605-2_54</idno>
	</analytic>
	<monogr>
		<title level="m">ECCV 2014</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Fleet</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">8690</biblScope>
			<biblScope unit="page" from="834" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Applications of the Legendre-Fenchel transformation to computer vision problems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
		<idno>DTR11-7</idno>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Imperial College London</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Simultaneous mosaicing and tracking with an event camera</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Benosman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Ieng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<publisher>BMVC</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Parallel tracking and mapping for small AR workspaces</title>
		<author>
			<persName><forename type="first">G</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Mixed and Augmented Reality (ISMAR)</title>
		<meeting>the International Symposium on Mixed and Augmented Reality (ISMAR)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A 128 × 128 120 dB 15 µs latency asynchronous temporal contrast vision sensor</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lichtsteiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Posch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Delbruck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Solid-State Circ. (JSSC)</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="566" to="576" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">MC3D: motion contrast 3D scanning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Matsuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Cossairt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computational Photography (ICCP</title>
		<meeting>the IEEE International Conference on Computational Photography (ICCP</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Towards visual SLAM with event-based cameras</title>
		<author>
			<persName><forename type="first">M</forename><surname>Milford</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Leutenegger</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Problem of Mobile Sensors: Setting Future Goals and Indicators of Progress for SLAM Workshop in Conjunction with Robotics: Science and Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Place recognition with event-based cameras and a neural implementation of SeqSLAM</title>
		<author>
			<persName><forename type="first">M</forename><surname>Milford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Leutenegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Webb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Innovative Sensing for Robotics: Focus on Neuromorphic Sensors Workshop at the IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fast, minimum storage ray/triangle intersection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Möller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Trumbore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Graph. Tools</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="28" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Event-based, 6-DOF pose tracking for high-speed maneuvers</title>
		<author>
			<persName><forename type="first">E</forename><surname>Mueggler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/RSJ Conference on Intelligent Robots and Systems</title>
		<meeting>the IEEE/RSJ Conference on Intelligent Robots and Systems</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">DTAM: dense tracking and mapping in real-time</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lovegrove</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV</title>
		<meeting>the International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A QVGA 143 dB dynamic range framefree PWM image sensor with lossless pixel-level video compression and timedomain CDS</title>
		<author>
			<persName><forename type="first">C</forename><surname>Posch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Matolin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wohlgenannt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Solid-State Circ. (JSSC)</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="259" to="275" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Event-driven stereo matching for realtime 3D panoramic vision</title>
		<author>
			<persName><forename type="first">S</forename><surname>Schraml</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Belbachir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Event-based 3D SLAM with a depth-augmented dynamic vision sensor</title>
		<author>
			<persName><forename type="first">D</forename><surname>Weikersdorfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Adrian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Conradt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Robotics and Automation</title>
		<meeting>the IEEE International Conference on Robotics and Automation</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Simultaneous localization and mapping for event-based vision systems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Weikersdorfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Conradt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision Systems (ICVS)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
