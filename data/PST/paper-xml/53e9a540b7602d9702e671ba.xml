<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ajay</forename><forename type="middle">L</forename><surname>Kumar</surname></persName>
						</author>
						<author>
							<persName><surname>Ballihi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Unité Associée au CNRST (URAC 29)</orgName>
								<orgName type="department" key="dep2">Faculté des Sciences</orgName>
								<orgName type="laboratory" key="lab1">Laboratoire d&apos;Informatique Fondamentale de Lille (</orgName>
								<orgName type="laboratory" key="lab2">UMR CNRS/Lille 8022)</orgName>
								<orgName type="institution">Cedex France and LRIT</orgName>
								<address>
									<settlement>Villeneuve d&apos;Ascq</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Université Mohammed V-Agdal</orgName>
								<address>
									<settlement>Rabat</settlement>
									<country>Morocco. B</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Ben Amor and M. Daoudi are with Institut Mines-Télécom</orgName>
								<address>
									<country>Télécom Lille</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="laboratory" key="lab1">LIFL (</orgName>
								<orgName type="laboratory" key="lab2">UMR CNRS/Lille1 8022)</orgName>
								<address>
									<settlement>Villeneuve d&apos;Ascq Cedex</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Departement of Statistics</orgName>
								<orgName type="institution">Florida State University</orgName>
								<address>
									<postCode>32306</postCode>
									<settlement>Tallahassee</settlement>
									<region>FL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department" key="dep1">CNRST (URAC 29)</orgName>
								<orgName type="department" key="dep2">Faculté des Sciences</orgName>
								<orgName type="institution">Université Mohammed V-Agdal</orgName>
								<address>
									<settlement>Rabat</settlement>
									<country key="MA">Morocco</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7498AB4447E5EF093ED4FBCB239ED2AC</idno>
					<idno type="DOI">10.1109/TIFS.2012.2209876</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Boosting 3-D-Geometric Features for Efficient Face</head><p>Recognition and Gender Classification Lahoucine Ballihi, Boulbaba Ben Amor, Mohamed Daoudi, Senior Member, IEEE, Anuj Srivastava, Senior Member, IEEE, and Driss Aboutajdine, Senior Member, IEEE Abstract-We utilize ideas from two growing but disparate ideas in computer vision-shape analysis using tools from differential geometry and feature selection using machine learning-to select and highlight salient geometrical facial features that contribute most in 3-D face recognition and gender classification. First, a large set of geometries curve features are extracted using level sets (circular curves) and streamlines (radial curves) of the Euclidean distance functions of the facial surface; together they approximate facial surfaces with arbitrarily high accuracy. Then, we use the wellknown Adaboost algorithm for feature selection from this large set and derive a composite classifier that achieves high performance with a minimal set of features. This greatly reduced set, consisting of some level curves on the nose and some radial curves in the forehead and cheeks regions, provides a very compact signature of a 3-D face and a fast classification algorithm for face recognition and gender selection. It is also efficient in terms of data storage and transmission costs. Experimental results, carried out using the FRGCv2 dataset, yield a rank-1 face recognition rate of 98% and a gender classification rate of 86% rate.</p><p>Index Terms-Face recognition, gender classification, geodesic path, facial curves, machine learning, feature selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>S INCE facial biometrics is natural, contact free, nonintru- sive, and psychologically supported, it has emerged as a popular modality in the biometrics community. Unfortunately, the technology for 2-D image-based face recognition still faces difficult challenges, such as pose variations, changes in lighting conditions, occlusions, and facial expressions. Due to the robustness of 3-D observations to lighting conditions and pose variations, face recognition using shapes of facial surfaces has become a major research area in the last few years. Many of the state-of-the-art methods have focused on the variability caused by facial deformations, e.g., those due to face expressions, and have proposed methods that are robust to such shape variations. At the same time, gender classification is emerging as an interesting problem that can be a useful preprocessing step for face recognition. Gender is similar to other soft biometric traits, such as skin color, age, eyes colors, and so on, used by humans to distinguish their peers. Most existing work on gender classification uses 2-D-images to extract distinctive facial features like hair density and inner morphology of the face, but 3-D shape has not yet been used extensively for gender classification. Several works in psychology have shown that gender has close relationships both with 2-D information and 3-D shape <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, and it motivates the use of 3-D shapes for gender classification.</p><p>The development of a practical, high-performance system for automatic face recognition and gender classification is an important issue in intelligent systems. In this work, we focus on a feature selection technique from machine-learning that is fully automatic and versatile enough for different applications like face recognition and gender classification. The features comes from different types of facial curves extracted from facial surfaces in an intrinsic fashion, and comparisons of these curve features is based on latest advances in shape analysis of parameterized curves using tools from differential geometry. In the process we also develop an effective approach for tackling facial expressions variation, an important focus of the face recognition grand challenge. Our approach offers the advantage of classifying either facial identity and/or gender, both independent of the ethnicity. Specifically, the main contributions of this paper include:</p><p>• A new geometric feature-selection approach for efficient 3-D face recognition that seeks most relevant characteristics for recognition while handling the challenge of facial expressions. In particular, we are interested in finding those facial curves that are most suitable for 3-D face recognition. • A new gender classification approach using the 3-D face shape represented by collections of curves. In particular, we are interested in finding those facial curves that are most suitable for gender discrimination. The rest of the paper is organized as follows. Section II summarizes existing approaches on 3-D face recognition with an emphasis on facial curve-based and facial feature-based methods. It also presents some progress in 3-D imaging-based gender classification. Section III overviews the proposed approach for both the target applications. In Section IV, we present procedures for extracting facial curves. Section V recalls the main ideas of the Riemannian geometric shape analysis framework to compare and match facial curves. In Section VI, we give formulations to the classification problem and describes the use of the boosting procedure to achieve the feature selection step, for each of the two applications. Experimental evaluations and comparative studies to previous approaches are given in Section VII. We conclude in the paper with a discussion and summary in Section VIII. <ref type="foot" target="#foot_0">1</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>As the proposed approach combines curve-based face comparison with feature selection techniques, we mainly focus on previous methods that primarily use local facial feature-selection and holistic facial curves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Feature Selection-Based 3-D Face Recognition</head><p>Several methods have been proposed to analyze the discriminative power of different facial regions or features for face recognition. Daniyal et al. <ref type="bibr" target="#b3">[4]</ref> proposed an algorithm in which a face is represented as a vector of distances between pairs of facial landmarks. They selected the landmarks by exhaustive search over possible combinations of used/unused landmarks, comparing the recognition rates, and concluded that the best selection corresponded to the landmarks located around the eyes and the nose. In the 3-D face recognition approach used by Faltemier et al. <ref type="bibr" target="#b4">[5]</ref>, the nose tip and 28 small regions were selected automatically for improving recognition. More recently, Wang et al. <ref type="bibr" target="#b5">[6]</ref> computed a signed shape difference map (SSDM) between two aligned 3-D faces as a intermediate representation for the shape comparison. Based on the SSDMs, Haar-like, Gabor, and Local Binary Pattern (LBP) were used to encode both the local similarity and the change characteristics between facial shapes. The most discriminative local features were selected optimally by boosting. Using similar features, Li et al. <ref type="bibr" target="#b6">[7]</ref> proposed to design a feature pooling and ranking scheme in order to collect various types of low-level geometric features, such as curvatures, and ranked them according to their sensitivities to facial expressions. They applied sparse representations to the collected low-level features and achieved good results on the GAVAB database. In <ref type="bibr" target="#b7">[8]</ref> Ocegueda et al. proposed a Markov Random Field model for the analysis of lattices (e.g., image or 3-D meshes) in terms of the discriminative information of their vertices. They observed that the nose and the eyes are consistently marked as discriminative regions of the face in a face recognition system. Li et al. <ref type="bibr" target="#b8">[9]</ref> proposed an expression-robust 3-D face recognition approach by learning weighted sparse representation of encoded normal information, which they called multiscale local normal patterns (MS-LNPs) facial surface shape descriptor. They utilized the learned average quantitative weights related to different facial physical components to enhancing the robustness of their system to expression variations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Curve-Based Face Representation</head><p>The basic idea of these approaches is to represent a surface using an indexed family of curves which provide an approximate representation of the surface. Samir et al. <ref type="bibr" target="#b9">[10]</ref>, for instance, used the level curves of the height function to define facial curves. Since these curves are planar, they used shape analysis of planar curves, taken from <ref type="bibr" target="#b10">[11]</ref>, to compare and deform faces; nonlinear matching problem was not studied here (that is, the mapping was fixed to be linear). The authors proposed to compare facial surface by using two metrics: Euclidean mean and geometric mean. However, there were no discussion on how to obtain optimal curves. Later in <ref type="bibr" target="#b11">[12]</ref>, the same authors used the level curves of the geodesic distance function that resulted in 3-D curves. They used a nonelastic metric and a path-straightening method to compute geodesics between these curves. Here also, the matching was not studied and the correspondence of curves and points across faces was simply linear. In <ref type="bibr" target="#b12">[13]</ref>, Mpiperis et al. proposed a geodesic polar parametrization of the facial surface. With this parametrization, the intrinsics attributes do not change under isometric deformation when the mouth is closed. Otherwise, it violates the isometry assumption and thus they adapt their geodesic polar parametrization by disconnecting the lips. Through this representation, they proposed an elastically deformable model algorithm that establishes correspondence among a set of faces. Then, they construct bilinear models that decouple the identity and facial expression factors. The invariance to facial expressions is obtained by fitting these models to unknown faces. The main limitation of this approach is the need for a large set which should also be annotated with respect to facial expressions. In <ref type="bibr" target="#b13">[14]</ref>, Drira et al. explored the use of shapes of noses for performing partial human biometrics. More recently, in <ref type="bibr" target="#b14">[15]</ref>, the same authors proposed similar shape analysis approach this time using radial curves. They model elastic deformations of facial surfaces (including opening the mouth) as an optimal reparametrization (or matching) problem that they solve using the dynamic programming algorithm. This approach provided promising results on GAVAB database even where the probe pose is nonfrontal. In <ref type="bibr" target="#b15">[16]</ref>, Berretti et al. used the geodesic distance on the face to extract iso-geodesic facial stripes. Equal width iso-geodesic facial stripes were used as nodes of the graph and edges between nodes were labeled with descriptors, referred to as 3-D Weighted Walkthroughs (3DWWs), that captured mutual relative spatial displacement between all the pairs of points of the corresponding stripes. Face partitioning into iso-geodesic stripes and 3DWWs together provided an approximate representation of local morphology of faces that exhibits smooth variations for changes induced by facial expressions. More recently Ballihi et al. <ref type="bibr" target="#b2">[3]</ref> propose a new curve selection approach for efficient 3-D face recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Gender Classification</head><p>The human face presents a clear sexual dimorphism that makes face gender classification an extremely efficient and fast cognitive process <ref type="bibr" target="#b16">[17]</ref>. Although a significant progress has been made, the task of automated, robust face gender classification is still a distant goal. 2-D Image-based methods are inherently limited by variability in imaging factors such as illumination and pose. An emerging solution is to use laser scanners for capturing three-dimensional (3-D) observations of human faces, and use this data in performing face gender classification. Bruce et al. <ref type="bibr" target="#b0">[1]</ref> performed an interesting experiment in which they tested the human visual system that is accurate at deciding whether faces are male or female, even when cues from hairstyle, makeup, and facial hair are minimized. The authors found that subjects were considerably less accurate when asked to judge the sex of 3-D representations of faces obtained by laser-scanning, compared photographs that were taken with hair concealed and eyes closed. They proved that the average male face differs from the average female face by having a more protuberant nose/brow and more prominent chin/jaw. The effects of manipulating the shapes of the noses and chins of the laser-scanned heads were assessed and significant effects of such manipulations on the apparent masculinity or femininity of the heads were revealed. In O'Toole et al. <ref type="bibr" target="#b1">[2]</ref>, the authors assumed that the sex of a face is perhaps its most salient feature. They applied principal components analysis (PCA) separately to the three-dimensional structure and gray level image data from laser-scanned human heads. The results showed that the three-dimensional head data supported more accurate sex classification than the gray level image data, across a range of PCA-compressed (dimensionality-reduced) representations of the heads. Jing et al. <ref type="bibr" target="#b17">[18]</ref> investigated gender classification based on 2.5D facial surface normals (facial needle-maps) which can be recovered from 2-D intensity images using a non-Lambertian Shape-from-shading (SFS) method. They described a weighted principal geodesic analysis (WPGA) method to extract features from facial surface normals to increase the gender discriminating power in the leading eigenvectors. They adopted a Bayesian method for gender classification. Xiaoguang et al. <ref type="bibr" target="#b18">[19]</ref> exploited the range information of human faces for ethnicity identification using a Support Vector Machine (SVM). An integration scheme is also proposed for ethnicity and gender identifications by combining the registered range and intensity images. Yuan et al. <ref type="bibr" target="#b19">[20]</ref> proposed a fusion-based gender classification method, based on SVM, for 3-D frontal neutral expression facial. A method for fusion of information from four regions (upper region of the face, the lower region of the face, the nose and the left eye) was proposed.</p><p>From the above discussion it is clear that a majority of current methods on curve-based 3-D face recognition used a holistic representation/parametrization of facial surfaces. In this paper, we consider curves as geometric features that capture local facial shape and we propose to learn the most relevant curves using adaptive boosting. Thus, we propose to represent a facial surface by two types of facial curves, radials and levels, for 3-D face recognition and gender classification. This strategy raises a few issues : (i) How to define curves on facial surfaces?, (ii) How to compare shapes of facial curves?, and (iii) How to select the most relevant curves for 3-D face recognition and gender classification? To address these issues, our strategy includes the following steps:</p><p>1) A facial surface representation by collection of curves of level sets (circular curves) and streamlines (radial curves) of a distance function; 2) A geometric shape analysis framework based on Riemannian geometry to compare pairwise facial curves;</p><p>3) A boosting method to highlight geometric features according to the target application; 4) A through experimental evaluation that compares the proposed solution with latest methods on a common data set and common experimental settings. As demonstrated later, the proposed approach achieves highest performance for the face recognition task, with the additional computational advantage of using a compact signature. Furthermore, it is one of the first approaches to address the gender classification problem using 3-D face images. To the best of our knowledge no previous work has proposed a unique framework for 3-D face recognition and gender classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. OVERVIEW OF THE PROPOSED APPROACH</head><p>In this work, we combine ideas from shape analysis using tools from differential geometry and feature selection derived from machine learning to select and highlight salient 3-D geometrical facial features. After preprocessing of 3-D scans, we represent resulting facial surfaces by a finite indexed collections of circular and radial curves. The comparison of pairwise curves, extracted from faces, is based on shape analysis of parameterized curves using differential geometry tools. According to the target application, the extracted features are trained as weak classifiers and the most discriminative features are selected optimally by adaptive boosting. For the case of gender recognition, the classification is formulated as a binary problem (Male/Female classes) and we propose to use the inter-and intrapersonal comparisons formulation to achieve feature selection for face identification, which is basically a Multiclass classification problem. Fig. <ref type="figure" target="#fig_0">1</ref> overviews the proposed approach with the target applications, face recognition and gender classification. Accordingly, it consists on the following steps:</p><p>• The Offline training step, learns the most salient circular and radial curves from the sets of extracted ones, according to each application in a supervised fashion. In face recognition, for instance, construct feature vectors by comparing pairwise curves extracted from facial surfaces. Next, feed these examples, together with labels indicating if they are interclass or not. Thus, the adaptive boosting selects and learns iteratively the weak classifiers and adding them to a final strong classifier, with suitable weights. As a result of this step, we keep the -earliest selected features for the testing step. • The Online test step, performs classification of a given test face. In the identity recognition problem, a probe face is compared to the gallery faces using only individual scores computed based on selected features which are fused using the arithmetic mean. In the gender recognition problem, a test face is compared to computed templates of Male and Female classes using curves selected for that purpose. The templates are computed, once for all, within the training stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. 3-D FACIAL CURVES EXTRACTION</head><p>Let be a facial surface denoting the output of a preprocessing step that crops the mesh, fills holes, removes noise, and prepares the mesh for curve extraction. We extract radial curves emanating from a reference anchor point (the tip of the nose) and circular curves having with the same point as the pole, using simple procedures detailed in the following paragraphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Radial Curves</head><p>Let denote the radial curve on which makes an angle with a reference radial curve; the superscript denotes that it is a radial curve. The reference curve is chosen to be the vertical curve once the face has been rotated to the upright position. In practice, each radial curve is obtained by slicing the facial surface by a plane that has the nose tip as its origin and makes an angle with the plane containing the reference curve, as shown in Fig. <ref type="figure" target="#fig_2">3</ref>. That is, a curve is obtained by slicing the facial surface by defined by the angle with the vertical plane and having as origin the nose tip. We repeat this step to extract radial curves from the facial surface at equal angular separation. Each curve is indexed by the angle .</p><p>To avoid pose variations problem, all probe faces are aligned with the first face model (generic model) of FRGCv2 database. This step is achieved by performing a coarse alignment by translating the probe face to a reference face, using their noses tips. This coarse alignment step is followed by a fine alignment using the ICP algorithm, as illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>. The process of curve extraction then follows.</p><p>If needed, we can approximately reconstruct from these radial curves according to as illustrated in Fig. <ref type="figure" target="#fig_2">3</ref>. This indexed collection of radial curves captures the shape of a facial surface and forms the first mathematical representation of that surface.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Circular Curves</head><p>Let denote the circular curve on which makes a distance from the reference point (nose tip). A similar procedure is employed to extract these curves. The only difference is the slicing function which is now a sphere having the reference point as center and variable radius . The intersection of a given sphere and the facial surface defines equidistant points from the reference point, in the surface. Fig. <ref type="figure" target="#fig_3">4</ref> illustrates results of such extraction procedure. We note that any points ordering is needed for both kind of curves since the slicing procedure kept edges between points. However, a curve subsampling procedure is introduced to achieve the same number of points for all curves (100 points per curve here).</p><p>Similarly to radial curves, we can also approximately reconstruct from these circular curves according to as illustrated in Fig. <ref type="figure" target="#fig_3">4</ref>, we describe the geometric framework which allow matching and comparison of curves. Fig. <ref type="figure" target="#fig_4">5</ref> gives some results of facial curves extraction on several 3-D faces.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. GEOMETRIC SHAPE ANALYSIS OF FACIAL CURVES</head><p>In the last few years, many approaches have been developed to analyze shapes of 2-D curves. We can cite approaches based on Fourier descriptors, moments or the median axis. More recent works in this area consider a formal definition of shape spaces as a Riemannian manifold of infinite dimension on which they can use the classic tools for statistical analysis. The recent results of Michor and Mumford <ref type="bibr" target="#b20">[21]</ref>, Klassen et al. <ref type="bibr" target="#b10">[11]</ref>, and Yezzi and Mennucci <ref type="bibr" target="#b21">[22]</ref> show the efficiency of this approach for 2-D curves. Joshi et al. <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref> have recently proposed a generalization of this work to the case of curves defined in . We will adopt this work to our problem since our 3-D curves are defined in .</p><p>We start by considering a curve in . While there are several ways to analyze shapes of curves, an elastic analysis Fig. <ref type="figure">6</ref>. Illustration of elastic metric. In order to compare the two curves in (a), some combination of stretching and bending are needed. The elastic metric measures the amounts of these deformations. The optimal matching between the two curves is illustrated in (b).</p><p>of the parametrized curves is particularly appropriate in our application-face analysis under facial expression variations. This is because (1) such analysis uses the square-root velocity function representation which allows us to compare local facial shapes in presence of elastic deformations, (2) this method uses a square-root representation under which the elastic metric reduces to the standard metric and thus simplifies the analysis, (3) under this metric the Riemannian distance between curves is invariant to the reparametrization. To analyze the shape of , we shall represent it mathematically using a square-root representation of as follows; for an interval , let be a curve and define to be its square-root velocity function (SRVF), given by:</p><formula xml:id="formula_0">(1)</formula><p>Here is a parameter and is the Euclidean norm in . We note that is a special function that captures the shape of and is particularly convenient for shape analysis, as we describe next. The classical elastic metric for comparing shapes of curves becomes the -metric under the SRVF representation <ref type="bibr" target="#b23">[24]</ref>. This point is very important as it simplifies the calculus of elastic metric to the well-known calculus of functional analysis under the -metric. Also, the squared -norm of , given by: , is the length of . If we set , implying all curves are rescaled to unit length, then translation and scaling variability have been removed by this mathematical representation of curves.</p><p>Consider the two curves in Fig. <ref type="figure">6</ref>(a), let us fix the parametrization of the top curve to be arc-length, i.e., traverse that curve with a constant speed equal to one. In order to better match that curve with the bottom one, one should know at what rate we are going to move along the bottom curve so that points reached at the same time on two curves are as close as possible under some geometric criterion. In other words, peaks and valleys should be reached at the same time. Fig. <ref type="figure">6(b)</ref> illustrates the matching where point 1 on the top curve matches to point 11 on the bottom curve. The part between the point 1 and 2 on the top curve shrinks on the curve 2. Therefore, the point 2 matches the point 22 on the second curve. An elastic metric is the measure of that shrinking. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Radial Open Curves</head><p>The set of all unit-length curves in is given by . With the -metric on its tangent spaces, becomes a Riemannian manifold. Since the elements of have a unit norm, is a hypersphere in the Hilbert space</p><p>. In order to compare the shapes of two radial curves, we can compute the distance between them in under the chosen metric. This distance is found to be the length of the minor arc connecting the two elements in . Since is a hypersphere, the formulas for the geodesic and the geodesic length are already well known. The geodesic length between any two points is given by:</p><p>(</p><p>and the geodesic path , is given by:</p><p>(</p><p>where . It is easy to see that several elements of can represent curves with the same shape. For example, if we rotate a face in , and thus its facial curves, we get different SRVFs for the curves but their shapes remain unchanged. Another similar situation arises when a curve is reparametrized; a reparameterization changes the SRVF of curve but not its shape. In order to handle this variability, we define orbits of the rotation group and the reparameterization group as equivalence classes in . Here, is the set of all orientation-preserving diffeomorphisms of (to itself) and the elements of are viewed as reparameterization functions. For example, for a curve and a function , the curve is a reparameterization of . The corresponding SRVF changes according to . We define the equivalent class containing as:</p><p>The set of such equivalence class is called the shape space of elastic curves <ref type="bibr" target="#b22">[23]</ref>. To obtain geodesics and geodesic distances between elements of , one needs to solve the optimization problem. The resulting shape space is the set of such equivalence classes: <ref type="bibr" target="#b3">(4)</ref> We denote by the geodesic distance between the corresponding equivalence classes and in shape space . In Fig. <ref type="figure" target="#fig_5">7</ref> we show geodesic paths between radial curves and the facial surfaces obtained by Delaunay triangulation of the set of points of radial curves. In 7(a) we show a geodesic path between two facial surfaces of the same person, while in 7(b) we show the same for faces belonging to different persons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Circular Closed Curves</head><p>We will use to denote the circular closed curves. Using SRVF representation as earlier, we can define the set of closed curves in by . The quantity is the total displacement in while moving from the origin of the curve until the end. If it is zero, the corresponding curve is closed. Thus, the set represents the set of all closed curves in . It is called a preshape space since curves with same shapes but different orientations and reparameterizations can be represented by different elements of . To define a shape, its representation should be independent of its rotation and reparameterization. This is obtained mathematically by a removing the rotation group and the reparameterization group from . As described in <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, we define the orbits of the rotation group and the reparameterization group as equivalence classes in . The resulting shape space is: <ref type="bibr" target="#b4">(5)</ref> To define geodesics on preshape and shape spaces we need a Riemannian metric. For this purpose we inherit the standard -metric of the large space . For any , the standard inner-product is given by: (</p><p>The computation of geodesics and geodesic distances utilize the intrinsic geometries of these spaces. While the detailed description of the geometries of and are given in <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, we briefly mention the tangent and normal spaces of . It can be shown that the set of all functions normal to at a point are given by: <ref type="bibr" target="#b6">(7)</ref> where form an orthonormal basis of . Thus, the tangent space at any point is given by: (8) Now, an important tool in our framework is the construction of a geodesic path between two elements of , under the Riemannian metric given by <ref type="bibr" target="#b5">(6)</ref>. Given two curves and , represented by their SRVF respectively and , we need to find a geodesic path between the orbits and in the space . We use in this context, a numerical method called the pathstraightening method <ref type="bibr" target="#b24">[25]</ref> which connects the two points and an arbitrary path and then updates this path repeatedly in the negative direction of the gradient of energy given by: (</p><p>It has been proven in <ref type="bibr" target="#b24">[25]</ref> that the critical points of defined by ( <ref type="formula" target="#formula_4">9</ref>) are geodesic paths in . We denote by the geodesic distance between the corresponding equivalence classes and in . In Fig. <ref type="figure" target="#fig_6">8</ref> we show geodesic paths between circular curves and the facial surfaces obtained by Delaunay triangulation of the set of points of circular curves. In 8(a) we show a geodesic path between two facial surfaces of the same person, while in 8(b) we show the same for faces belonging to different persons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Extension to Facial Surfaces Shape Analysis</head><p>In this section we extend our study from shapes of curves to shapes of facial surfaces. We represent the surface of the facial surface by a collection of 3-D circular and radial curves, <ref type="bibr" target="#b9">(10)</ref> where represents the circular curves, is the cardinality of the set of circular curves, represents the radial curve and is the cardinality of the set of radial curves. Two shapes of facial surfaces are compared by comparing their corresponding facial curves. The distance between two facial surfaces and could be defined by: <ref type="bibr" target="#b10">(11)</ref> VI. BOOSTING FOR GEOMETRIC FEATURE SELECTION Radial and circular curves capture locally the shape of the faces. However, their comparison under different expressions runs into trouble. In fact, their shapes are affected by changes in facial expressions. For that purposes, we introduce a feature selection step to identify (or localize) the most stable and most discriminative curves. We propose to use the well known machine learning algorithm AdaBoost introduced by Freund and Schapire in <ref type="bibr" target="#b25">[26]</ref>. Recall that, boosting is based on iterative selection of weak classifiers by using a distribution of training samples. At each iteration, the best weak classifier is provided and weighted by the quality of its classification. In practice, the individual circular curves and radial curves are used as weak classifiers. After iterations, the most relevant facial curves are returned by the algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Face Recognition</head><p>To train and test Adaboost classifier for this application, we use the 3-D face models of FRGCv2 dataset. For each radial and circular curve, we compute the All versus All (4007 4007) similarity matrix. We then split the matrices as we keep the Gallery versus Probe of size 466 3541 for the testing and uses the remaining Probe versus Probe submatrices, of the size of 3541 3541, for the training. Thus, we separate the training and the testing samples (the set of individual distances related to facial curves) and these disjoint sets serve as inputs to Adaboost, as illustrated in Fig. <ref type="figure" target="#fig_7">9</ref>.</p><p>From these areas of the matrices, we extract two kinds of scores (i) the match scores (intrapersonal comparisons) and (ii) the nonmatch scores (interpersonal comparison). Together these scores form an input to the boosting algorithm. More formally, we consider a set of pairs corresponding to similarity scores between radial or circular curves at the same level, with or . can take two values: 0 in the case of non-match score and 1 in the case of match score. For each circular or radial curve, the weak classifier determines the optimal threshold classification function such that the minimum number of samples are misclassified. Each weak classifier will take a value of distance computed based on a radial or circular curve, threshold it, and classify the comparison as positive (intraclass) or negative (interclass), depending on the distance value being under or over the threshold . <ref type="bibr" target="#b11">(12)</ref> where, denotes for the weak hypothesis given by . The final strong classifier is defined by a set of a weak classifiers weighted by a set of weights . The pseudocode of AdaBoost algorithm is summarized in algorithm 1. Initialization of weights it depends on the value of where .</p><p>• For : 1-Normalize the weights such that . 2-For each curve (feature), train a weak classifier that uses a single curve. The error of classifier is determined with the corresponding weight :</p><p>3-Choose the classifier with the lowest error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4-Update the weights</head><p>, where and , if the sample is correctly classified by and 1 else.</p><p>• The final hypothesis is a weighted linear combinations of the hypotheses where the weights are inversely proportional to the training errors. The strong classifier is given by: The set of selected curves returned by Adaboost is shown in Fig. <ref type="figure" target="#fig_8">10</ref>. The first row shows the locations of the selected curves on different sessions of the same person with different expressions, whereas, the second row gives curves location on different subjects. We note that the boosting algorithm selects iso-curves located on the nasal region, which is stable under expressions and radial curves avoiding two parts. The first one is the lower part of the face since its shape is affected by expressions, particularly when the mouth is open. The second area corresponds to the eye/eyebrow regions. Shapes of radial curves passing throw these regions change when conveying expressions. In contrast, the most stable area cover the nasal/forehead regions.</p><p>To demonstrate the usefulness of the curve selection step, different graphs in Fig. <ref type="figure" target="#fig_9">11</ref> plot the rate of False Acceptance versus the rate of False Rejection for different configurations. These curves are produced from the Probe versus Probe matrices (i.e., using the training set). As shown in Fig. <ref type="figure" target="#fig_9">11</ref>(b), minimum errors are given by fusing scores of selected radial and selected circular curves. We note also that the selection performed on radial curves only or circular curves only minimizes the errors compared to the use of all radial curves or circular curves, respectively.</p><p>The online testing step consists on comparing faces and by the fusion of scores related to selected curves as following: <ref type="bibr" target="#b12">(13)</ref> where is the cardinality of the set of selected circular curves and the cardinality of the set of selected radial curves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Gender Classification</head><p>For 3-D face-based gender classification task, we first compute Male and Female representative templates using the geometric shape analysis framework for open and closed curves. In fact, this framework allows us to compute intrinsic means (Karcher mean) of curves that we extend to facial surfaces. Then, within the training step, we compute intraclass (same gender) and interclass (different gender) pairwise distances (for each curve index) between sample faces and the templates. Finally, the most discriminative geometric features are selected optimally by boosting as done in face recognition application. For the testing step, distances to male and female templates are computed (based only on selected features), and the nearest neighbor algorithm denotes the class (Male/Female) membership. Different steps are detailed in the following: Male/Female Geometric Templates Computation: One advantage of the proposed geometrical framework for shape analysis of curves is to calculate some statistics as the "mean" of facial curves and to extend it to facial surfaces, called Karcher mean <ref type="bibr" target="#b23">[24]</ref>. The Riemannian structure defined on a Riemannian manifold enables us to perform such statistical analysis for computing faces mean and variance. There are at least two ways of defining a mean value for a random variable that takes values on a nonlinear manifold. The first definition, called extrinsic mean, involves embedding the manifold in a larger vector space, computing the Euclidean mean in that space, and then projecting it down to the manifold. The other definition, called the intrinsic mean or the Karcher mean utilizes the intrinsic geometry of the manifold to define and compute a mean on that manifold. It is defined as follows: Let denotes the length of the geodesic path from curves in . To calculate the Karcher mean of facial curves in , we define the variance function: <ref type="bibr" target="#b13">(14)</ref> The Karcher mean is then defined by: <ref type="bibr" target="#b14">(15)</ref> The intrinsic mean may not be unique, i.e., there may be a set of points in for which the minimizer of is obtained. is an element of that can be seen as the smallest geodesic path length from all given facial surfaces. We present a commonly used algorithm for finding Karcher mean for a given set of facial surfaces (by using their curves). This approach, presented in algorithm 2 uses the gradient of , in the space , to iteratively update the current mean . The same pseudo-algorithm will be obtained for radial curves defined in the shape space .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 Karcher Mean Algorithm</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Set</head><p>. Choose some time increment . Choose a point as an initial guess of the mean. (For example, one could just take .) 1-For each choose the tangent vector which is tangent to the geodesic from to . The vector is proportional to the gradient at of the function . 2-Flow for time along the geodesic which starts at and has velocity vector . Call the point where you end up . 3-Set and go to step 1.</p><p>Male template facial surface is computed by averaging ten males facial surfaces of different person as shown in Fig. <ref type="figure" target="#fig_10">12</ref>. Female template facial surface is computed by averaging ten females facial surfaces of different person as shown in Fig. <ref type="figure" target="#fig_11">13</ref>.</p><p>Geometric Feature Selection: To train and test the boosting algorithm for this application, we use 20 previous 3-D faces of the FRGCv1 dataset for training and 466 subjects of FRGCv2 for testing. Firstly, we selected a subset of faces of men and women (ten from each class) from FRGCv1, to calculate  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE I INPUT FEATURE VECTORS OF BOOSTING ALGORITHM FOR GENDER CLASSIFICATION</head><p>the templates for both male and female classes denoted respectively and . Then, we computed pairwise distances (based on curves) between test images and both of templates. Thus, we obtained a matrix containing feature vectors (distance based on curves) which will be used to train Ababoost algorithm, as illustrated in Table <ref type="table">I</ref>. From this matrix, we extract two kinds of scores (i) the match scores (intragender comparisons) and (ii) the nonmatch scores (intergender comparison).</p><p>Both score lists represent the input of the boosting algorithm. More formally, we consider a set of pairs where is a similarity score between two curves at the same level and can take two values: 0 in the case of non-match score and 1 in the case of match score. For each curve , the weak learner determines the optimal threshold classification function, such that the minimum number of samples are misclassified. A weak classifier thus consists of a geometric feature and a threshold , such that: <ref type="bibr" target="#b15">(16)</ref> Fig. <ref type="figure" target="#fig_13">15</ref> shows the location of selected curves on different sessions of some male faces whereas, Fig. <ref type="figure" target="#fig_12">14</ref> shows the location of selected curves on different sessions of some female faces.  We note that the boosting algorithm selects circular curves on the cheeks region, which is discriminative shape of 3-D face for gender classification and radial curves avoiding two parts. The most stable areas for gender classification cover the cheeks/ sellion regions.</p><p>Classification: As described in Table <ref type="table">I</ref>, this time round, we calculated different matrices of distances of all selected circular and radial curves between template faces and the 466 test faces of FRGCv2. The pseudocode of the proposed gender classification algorithm is given in algorithm 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 3 Gender Classification Algorithm</head><p>• Input: A set of curves where is either circular curves or radial curves. is the total number of selected circular and Radial curves.</p><p>• For each face request : 1-Calculate the geodesic distances:</p><p>• The final decision for classification is given by: Fig. <ref type="figure" target="#fig_0">16</ref>. Cumulative Match Characteristic curve for selected radial and level curves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. EXPERIMENTAL RESULTS</head><p>In the following, we present conducted experiments and the obtained results with the proposed methods. In particular, we report 3-D face recognition performances on FRGCv2 <ref type="bibr" target="#b26">[27]</ref> and provide a comparative study with state-of-the-art. FRGCv2 dataset contains 4 007 3-D scans of 466 people, in which more than 40% of the models are nonneutral. A standard evaluation protocol for identification and verification biometric scenarios supports this data set. Furthermore, we give gender classification performances achieved by our approach, using the same dataset. We note that the subjects in FRGCv2 dataset are 57% male and 43% female.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. 3-D Face Recognition Results</head><p>1) Identification: For testing on FRGCv2 dataset, only the identification evaluation was carried out. In fact, as mentioned in Section VI since our approach requires a training stage, it was tested on a subset of this dataset following the FRGC evaluation protocol for the identification scenario, as following. We kept, for the test, the Gallery versus Probe (of size 466 3541) similarity matrices. The remaining submatrices (i.e., Probe versus Probe similarity scores) were used to train the feature selection step by boosting algorithm. This means that disjointly similarity vectors are used for the training and the test. Following these settings, our approach achieved 98.02% as rank-1 recognition rate and reached 99% in rank-5 as illustrated in the CMC plot (Cumulative Match Characteristic) given is Fig. <ref type="bibr" target="#b15">16</ref>. We recall that the approach used here is based on both radial and circular facial curves selection.</p><p>As shown in Table <ref type="table" target="#tab_0">II</ref>, the selected curves provides better recognition rate that using radial or circular individually. We point out that the most relevant circular curves are located on the nasal region, which means that the nasal shape significantly contribute to face recognition. This is due to the fact that its shape is stable to facial expressions. We note also that the use combining all the curves by using ( <ref type="formula">13</ref>) provides the best recognition rate. In addition to performance improvement, the curve selection results on a more compact biometric signature which reduce the time-processing of one-to-one face matching.</p><p>2) Comparative Study With State-of-the-Art: Following the FRGC standard protocol for the identification scenario, the Table <ref type="table" target="#tab_1">III</ref> shows identification results of previous approaches (curve-based, feature selection-based, and others) by keeping the earliest scan of the 466 subjects in the gallery and the remaining for testing. We note that experiments reported in <ref type="bibr" target="#b27">[28]</ref> and <ref type="bibr" target="#b15">[16]</ref> follow a modified protocol by keeping the earliest neutral scan in the gallery and the remaining as test images. It is clear that the proposed method outperforms the majority of state-of-the-art methods. Only the approach proposed recently by Wang et al. <ref type="bibr" target="#b5">[6]</ref>, based on boosting of descriptors (Haar-like, Gabor, and Local Binary Pattern (LBP)) computed on the Shape Difference Map between faces, achieved a better result 98.3% which means that ten more faces are recognized by this approach.</p><p>As shown in Table <ref type="table" target="#tab_1">III</ref>, the proposed approach outperforms the state-of-the-art except the work of <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Gender Classification</head><p>The proposed gender classification of 3-D face scans has been experimented using the FRGCv2 database. This was motivated by the fact that this dataset contains the largest number of subjects compared to existing 3-D face datasets as Bosphorus, BU-3DFE, etc. To evaluate the proposed approach, we have considered 466 3-D images related to the 466 subjects of FRGCv2 data set. Thus, if several sessions exist in the dataset, we select the earliest (neutral or nonneutral) one for our experiment. We use also few 3-D images taken from FRGCv1 to compute male and female templates, as described in Section VI. The difficulty encountered to compare our approach to related work, is there is no standard protocol to compare gender classification results, unlike FRGC standard protocol for 3-D face recognition. Most of previous approaches <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b19">[20]</ref> reported classification results on a subset taken from FRGCv1 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Classification Results:</head><p>We conducted experiments by first computing Male and Female templates using sample 3-D scans taken from FRGCv1. Then, comparisons between those templates and the 466 test images (of FRGCv2) based on their circular and radial curves were computed to build the feature vectors. Finally, two experiments, detailed below, were carried out:</p><p>• Experiment 1. Curve selection-based. To select the most relevant combination of curves using Adaboost algorithm, we first use 10 male 3-D face models and 10 female models from FRGCv1 to compute the male and female templates. Then, we compute pairwise curve distances between the   same sample models and the templates, in order to build feature vectors, used for the training step. Now, given new test face from the 466 3-D models of FRGCv2, we compute the pairwise distances to the templates then make the decision as result of the nearest neighbor algorithm. Following this setting, our approach achieved 84.12% as average gender classification rate. We note that, in this experiment, after feature selection step, accumulated distances from selected curves only are used and classification is achieved by nearest neighbor classification. As reported in Table <ref type="table" target="#tab_2">IV</ref>, we note also that using the combination of selected circular curves and selected radial curves achieved better performances compared to selected circular curves or selected radial curves, taken individually.    <ref type="table" target="#tab_4">VI</ref> shows gender classification results compared to previous approaches tested on different subsets. In <ref type="bibr" target="#b34">[35]</ref> the authors used a subset of FRGCv1, six female subjects and four male subjects, while in <ref type="bibr" target="#b18">[19]</ref> a subset of FRGCv1 is used with only 28 female subjects and 80 male subjects. However they used more than one session of each subject. Note that this ad-hoc division does not guarantee that all subjects will have a neutral expression, some FRGCv2 subjects are scanned with arbitrary facial expression.</p><p>The analysis of some misclassified examples given by Figs. 17 and 18 shows that there are two major reasons of this misclassification. The first one is the bad quality of 3-D scans, such as some occlusions in relevant regions which affected the shape of curves. The second reason lies in the fact that only based on the shape information, there is some confusion (even for a person), to recognize correctly the gender of the person. One solution of this problem and for improving the proposed approach is to introduce the texture information, which contains complementary (as hair density, etc.) in order to consolidate the classifier decision.</p><p>Fig. <ref type="figure" target="#fig_17">19</ref> shows some faces with selected iso-level and radial curves for both applications face recognition and gender classification, the blue curves are selected for 3-D face gender classification, the red curves are selected for 3-D face recognition while the black curves are the common selected curves for both classifications.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSION</head><p>In this paper, by combining tools from Riemannian geometry and the well-known Adaboost algorithm, we have proposed to select the most discriminative curves for facial recognition and gender classification. The experiments, carried out on FRGCv2 including neutral and nonneutral images, demonstrate the effectiveness of the proposed approach. Based only on 17 curves, including 12 radial curves and 5 circular curves, our fully automatic approach achieved rank-1 recognition rate of 98.02%. For gender classification, 19 curves with 12 radial and 7 circular, were selected and achieved a classification rate of 86.05%. The algorithm computation time was on the order of 0.68 second (for recognition rate) and 0.76 second (for gender classification) to compare two faces with selected curves instead of 2.64 second with all curves. The boosting selects those curves passing through stable regions on the face for different applications. This approach is efficient in terms of computation time, data storage and transmission costs.</p><p>The proposed approach can be extended in different directions in order to improve the performances and to address other important classification tasks. For example, the texture information could be associated to shape information to consolidate the classifier decision. In fact, texture channel provides additional informations which could be complementary to the shape. Additionally, more facial attributes recognition, such as ethnicity and age estimation could be addressed using the same framework.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of the proposed approach, including both stages of training and testing and both target applications: identity recognition and gender classification.</figDesc><graphic coords="4,60.00,65.10,469.02,274.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Probe model pose normalization by registration with the first gallery face of , a coarse alignment is performed by translating the probe face according to the translation vector formed by the tips of the noses. A fine registration is then achieved by the ICP algorithm.</figDesc><graphic coords="4,306.00,389.16,242.11,103.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Procedure for extraction of radial curves, a curve is obtained by slicing the facial surface by defined by the angle with the vertical plane and having as origin the nose tip.</figDesc><graphic coords="5,48.00,64.14,234.00,129.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Procedure for extraction of circular curves, a curve is obtained by slicing the facial surface by defined by the radius and having as center the nose tip.</figDesc><graphic coords="5,43.98,249.12,241.98,96.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Examples of facial representation by circular and radial curves. The first row illustrates preprocessed faces of male subjects, the second row gives preprocessed faces of females.</figDesc><graphic coords="5,42.00,401.16,246.00,109.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. (a) Intraclass geodesics between facial surfaces and their associated radial curves. (b) Interclass geodesics between facial surfaces and their associated radial curves.</figDesc><graphic coords="6,43.02,64.14,504.00,90.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. (a) Intraclass geodesics between facial surfaces and their associated circular curves. (b) Interclass geodesics between facial surfaces and their associated circular curves.</figDesc><graphic coords="7,45.00,64.14,504.00,88.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. All versus Probe distance matrices splitting; the Gallery versus Probe distance matrix is kept for testing, as suggested in the FRGC evaluation protocol and the remaining submatrices Probe versus Probe are used for training.</figDesc><graphic coords="8,42.00,63.12,244.02,151.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. The most discriminative radial and circular curves selected by Boosting for face recognition, given on different faces.</figDesc><graphic coords="8,304.98,64.14,246.00,91.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. ROC curves produced from the training set (a) all radial and circular curves, (b) selected radial and selected level curves, (c) all radial curves, (d) selected circular curves, (e) selected radial curves, and (f) all circular curves.</figDesc><graphic coords="9,108.00,68.10,376.98,280.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Different facial surfaces of different male persons taken from FRGCv1 and their Karcher mean. (a) Sample faces (from FRGCv1) used to compute the template face. (b) Female template face (Karcher mean).</figDesc><graphic coords="10,40.98,63.12,246.00,108.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Different facial surfaces of different female persons taken from FRGCv1 and their Karcher mean. (a) Sample faces (from FRGCv1) used to compute the template face. (b) Female template face (Karcher mean).</figDesc><graphic coords="10,49.02,227.16,229.98,91.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. The most discriminative radial and circular curves selected by Boosting for gender classification, given on different female faces.</figDesc><graphic coords="10,304.98,63.12,246.00,127.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. The most discriminative radial and circular curves selected by Boosting for gender classification, given on different male faces.</figDesc><graphic coords="10,304.98,236.10,246.00,133.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>• Experiment 2 .</head><label>2</label><figDesc>Classifier decision-based. In order to evaluate the boosting classifier results, we have conducted, following the same setting, experiments using two machine learning methods, SVM and Neural Network. Instead of feature selection step, we consider the final classifier decision. For example, we consider the binary decision of the strong classifier of Adaboost. Table V summarizes the obtained results. Training and testing steps are carried out using a 10-fold cross-validation experiment. According to this, the 466 subjects are split into disjoint subsets for training and test. Using 10-fold cross validation, training is repeated 10 times, with each of the 10 subsets used exactly once as the test data. Finally, the results from the ten steps are averaged to produce a single estimation of the performance of the classifier for the experiment. In this way, all observations are used for both training and test, and each observation is used for test exactly once.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 17 .</head><label>17</label><figDesc>Fig. 17. Different female 3-D faces misclassified by our approach. The first row gives 3-D data, the second gives the corresponding texture of 3-D data.</figDesc><graphic coords="12,314.04,182.16,226.92,94.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 18 .</head><label>18</label><figDesc>Fig. 18. Different female 3-D faces misclassified by our approach. The first row shows 3-D data; the second row shows the corresponding texture of 3-D data</figDesc><graphic coords="12,314.04,311.16,226.92,88.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 19 .</head><label>19</label><figDesc>Fig. 19. Examples of selected circular and radial curves for different applications. The first row shows different sessions for different male persons; the second shows different sessions for different female persons.</figDesc><graphic coords="13,40.98,183.12,247.02,135.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE II RANK</head><label>II</label><figDesc>-1/COMPUTATION COST (IN SECONDS) FOR DIFFERENT CONFIGURATIONS</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE III COMPARISON</head><label>III</label><figDesc>WITH STATE-OF-THE-ART APPROACHES ON FRGCV2 (RANK-1 RECOGNITION RATE)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE IV EXPERIMENTAL</head><label>IV</label><figDesc>COMPARISON OF GENDER CLASSIFICATION METHODS USING DIFFERENT TYPES OF SELECTED CURVES</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE V EXPERIMENTAL</head><label>V</label><figDesc>COMPARISON OF GENDER CLASSIFICATIONS METHODS USING DIFFERENT MACHINE LEARNING TECHNIQUES</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE VI GENDER</head><label>VI</label><figDesc>CLASSIFICATION RESULTS, COMPARISON TO STATE-OF-THE-ART RESULTS</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>This work was presented in part in the 4th Eurographics Workshop on 3-D Object Retrieval,</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2011" xml:id="foot_1"><p><ref type="bibr" target="#b2">[3]</ref>.</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the ANR under the projects ANR FAR 3D ANR-07-SESU-004 and the 3D Face Analyzer ANR 2010 INTB 0301</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Currently, he is an associate-professor in Institut Mines-Telecom/Telecom Lille1. He is also a member of the Computer Science Laboratory in University Lille 1 (LIFL UMR CNRS 8022). His research interests are mainly focused on statistical three-dimensional face analysis and recognition and facial expression recognition using 3-D. He is coauthor of several papers in refereed journals and proceedings of international conferences. He has been involved in French and International projects and has served as program committee member and reviewer for international journals and conferences. After spending the year 1996-1997 at Brown University as a visiting researcher, he joined FSU as an assistant professor in 1997. He is currently a professor in Department of Statistics at Florida State University (FSU), Tallahassee. His research is focused on pattern theoretic approaches to problems in image analysis, computer vision, and signal processing. In particular, he has developed computational tools for performing statistical inferences on certain nonlinear manifolds, including shape manifolds of curves and surfaces. He has published more than 150 peer-reviewed journal and conference articles in these areas. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Driss Aboutajdine</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sex discrimination: How do we tell the difference between male and female faces?</title>
		<author>
			<persName><forename type="first">V</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Burton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Healey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Coombes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Linney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="131" to="152" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sex classification is better with three-dimensional head structure than with image intensity information</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>O'toole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">F</forename><surname>Troje</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Bülthoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="75" to="84" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Selecting 3D curves on the nasal surface using adaboost for person authentication</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ballihi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Amor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Daoudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Aboutajdine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3DOR</title>
		<meeting>3DOR</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="101" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Compact signatures for 3D face recognition under varying expressions</title>
		<author>
			<persName><forename type="first">F</forename><surname>Daniyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cavallaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 6th IEEE Int. Conf. Advanced Video and Signal Based Surveillance, 2009 (AVSS&apos;09)</title>
		<meeting>6th IEEE Int. Conf. Advanced Video and Signal Based Surveillance, 2009 (AVSS&apos;09)</meeting>
		<imprint>
			<date type="published" when="2009-09">Sep. 2009</date>
			<biblScope unit="page" from="302" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A region ensemble for 3-d face recognition</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Faltemier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Forensics Security</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="62" to="73" />
			<date type="published" when="2008-01">Jan. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Robust 3D face recognition by local shape difference boosting</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1858" to="1870" />
			<date type="published" when="2010-10">Oct. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Expression-insensitive 3D face recognition using sparse representation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Z</forename><surname>0002</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="2575" to="2582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Which parts of the face give out your identity?</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ocegueda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">A</forename><surname>Kakadiaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="641" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning weighted sparse representation of encoded facial normal information for expression-robust 3D face recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Morvan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf</title>
		<meeting>Int. Joint Conf</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">0</biblScope>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Three-dimensional face recognition using shapes of facial curves</title>
		<author>
			<persName><forename type="first">C</forename><surname>Samir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Daoudi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1847" to="1857" />
			<date type="published" when="2006-11">Nov. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Analysis of planar shapes using geodesic paths on shape spaces</title>
		<author>
			<persName><forename type="first">E</forename><surname>Klassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Mio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="372" to="383" />
			<date type="published" when="2004-03">Mar. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An intrinsic framework for analysis of facial surfaces</title>
		<author>
			<persName><forename type="first">C</forename><surname>Samir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Daoudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Klassen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="80" to="95" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">3-D face recognition with the geodesic polar representation</title>
		<author>
			<persName><forename type="first">I</forename><surname>Mpiperis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Malassiotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Strintzis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Forensics Security</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="537" to="547" />
			<date type="published" when="2007-09">Sep. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A Riemannian analysis of 3D nose shapes for partial human biometrics</title>
		<author>
			<persName><forename type="first">H</forename><surname>Drira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Amor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Daoudi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Computer Vision</title>
		<meeting>IEEE Int. Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="2050" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Pose and expression-invariant 3D face recognition using elastic radial curves</title>
		<author>
			<persName><forename type="first">H</forename><surname>Drira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Amor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Daoudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">3D face recognition using isogeodesic stripes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Berretti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Bimbo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2162" to="2177" />
			<date type="published" when="2010-12">Dec. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">What&apos;s the Difference Between Men and Women? Evidence From Facial Measurement</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Burton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dench</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="153" to="176" />
			<date type="published" when="1993">1993</date>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Print</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Facial gender classification using shape from shading and weighted principal geodesic analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A P</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Hancock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICIAR</title>
		<meeting>ICIAR</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="925" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multimodal facial gender and ethnicity identification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICB</title>
		<meeting>ICB</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="554" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A fusion-based method for 3D facial gender classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2010 The 2nd Int. Conf. Computer and Automation Engineering (ICCAE)</title>
		<meeting>2010 The 2nd Int. Conf. Computer and Automation Engineering (ICCAE)</meeting>
		<imprint>
			<date type="published" when="2010-02">Feb. 2010</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="369" to="372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Riemannian geometries on spaces of plane curves</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Michor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mumford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Eur. Math. Soc</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1" to="48" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Conformal metrics and true &quot;gradient flows&quot; for curves</title>
		<author>
			<persName><forename type="first">A.-J</forename><surname>Yezzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mennucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="913" to="919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A novel representation for Riemannian analysis of elastic curves in</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Klassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Jermyn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Shape analysis of elastic curves in euclidean spaces</title>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Klassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Jermyn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1415" to="1428" />
			<date type="published" when="2011-07">Jul. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Geodesics between 3D closed curves using path-straightening</title>
		<author>
			<persName><forename type="first">E</forename><surname>Klassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="95" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A decision-theoretic generalization of on-line learning and an application to boosting</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2nd Eur. Conf. Computational Learning Theory (EuroCOLT&apos;95)</title>
		<meeting>2nd Eur. Conf. Computational Learning Theory (EuroCOLT&apos;95)<address><addrLine>London U.K.</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="23" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Overview of the face recognition grand challenge</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Scruggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Worek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="947" to="954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An efficient multimodal 2D-3D hybrid approach to automatic face recognition</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Mian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Owens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1927" to="1943" />
			<date type="published" when="2007-11">Nov. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A 3D face matching framework for facial curves</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">B</forename><surname>Ter Haar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Veltkamp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Graph. Models</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="77" to="91" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Drira</surname></persName>
		</author>
		<ptr target="http://www.telecomlille1.eu/people/drira/disser-tation.pdf" />
		<title level="m">Statistical Computing on Manifolds for 3D Face Analysis and Recognition</title>
		<meeting><address><addrLine>University Lille 1, Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Three-dimensional face recognition in the presence of facial expressions: An annotated deformable model approach</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">A</forename><surname>Kakadiaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Passalis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Murtuza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Karampatziakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Theoharis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="640" to="649" />
			<date type="published" when="2007-04">Apr. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A novel geometric facial representation based on multi-scale extended local binary patterns</title>
		<author>
			<persName><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ardabilian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2011 IEEE Int. Conf. Automatic Face Gesture Recognition and Workshops (FG 2011)</title>
		<meeting>2011 IEEE Int. Conf. Automatic Face Gesture Recognition and Workshops (FG 2011)</meeting>
		<imprint>
			<date type="published" when="2011-03">Mar. 2011</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">3D face recognition using log-gabor templates</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Fookes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. British Machine Vision Conf., Edinborugh</title>
		<meeting>British Machine Vision Conf., Edinborugh<address><addrLine>Scotland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A novel fusion-based method for expression-invariant gender classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1065" to="1068" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Gender classification using shape from shading</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A P</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Hancock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Fookes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC, U.K</title>
		<meeting>BMVC, U.K<address><addrLine>Edinburgh, Scotland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2007. 2006</date>
			<biblScope unit="page" from="769" to="778" />
		</imprint>
	</monogr>
	<note>Proc. BMVC</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
