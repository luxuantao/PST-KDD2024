<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EvenNet: Ignoring Odd-Hop Neighbors Improves Robustness of Graph Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Runlin</forename><surname>Lei</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhewei</forename><surname>Wei</surname></persName>
							<email>zhewei@ruc.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Zhen</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
							<email>yaliang.li@gmail.com</email>
						</author>
						<title level="a" type="main">EvenNet: Ignoring Odd-Hop Neighbors Improves Robustness of Graph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Neural Networks (GNNs) have received extensive research attention for their promising performance in graph machine learning. Despite their extraordinary predictive accuracy, existing approaches, such as GCN and GPRGNN, are not robust in the face of homophily changes on test graphs, rendering these models vulnerable to graph structural attacks and with limited capacity in generalizing to graphs of varied homophily levels. Although many methods have been proposed to improve the robustness of GNN models, the majority of these techniques are restricted to the spatial domain and employ complicated defense mechanisms, such as learning new graph structures or calculating edge attentions. In this paper, we study the problem of designing simple and robust GNN models in the spectral domain. We propose EvenNet, a spectral GNN corresponding to an even-polynomial graph filter. Based on our theoretical analysis in both spatial and spectral domains, we demonstrate that EvenNet outperforms full-order models in generalizing across homophilic and heterophilic graphs, implying that ignoring odd-hop neighbors improves the robustness of GNNs. We conduct experiments on both synthetic and real-world datasets to demonstrate the effectiveness of EvenNet. Notably, EvenNet outperforms existing defense models against structural attacks without introducing additional computational costs and maintains competitiveness in traditional node classification tasks on homophilic and heterophilic graphs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graph Neural Networks (GNNs) have gained widespread interest for their excellent performance in graph representation learning tasks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29]</ref>. GCN is known to be equivalent to a low-pass filter <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b20">21]</ref>, which leverages the homophily assumption that "connected nodes are more likely to have the same label" as the inductive bias. Such assumptions fail in heterophilic settings <ref type="bibr" target="#b36">[37]</ref>, where connected nodes tend to have different labels, encouraging research into heterophilic GNNs <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b36">37]</ref>. Among them, spectral GNNs with learnable polynomial filters <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b14">15]</ref> adaptively learn suitable graph filters from training graphs and achieve promising performance on both homophilic and heterophilic graphs. If the training graph is heterophilic, a high-pass or composite-shaped graph filter is empirically obtained.</p><p>While GNNs are powerful in graph representation learning, recent studies suggest that they are vulnerable to adversarial attacks, where graph structures are perturbed by inserting and removing edges on victim graphs to lower the predictive accuracy of GNNs <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b30">31]</ref>. Zhu et al. <ref type="bibr" target="#b35">[36]</ref> first established the relationship between graph homophily and structural attacks. They claimed that existing attack mechanisms tend to introduce heterophily to homophilic graphs, which significantly degrade the performance of GNNs with low-pass filters. On the one hand, several attempts are made to improve the robustness of GNNs against the injected heterophily from the spatial domain <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref>. These methods either compute edge attention or learn new graph structures with node features, requiring high computational costs in the spatial domain. On the other hand, while spectral GNNs hold superiority on heterophilic graphs, their performance under structural perturbation is unsatisfactory as well, which arouses our interest in exploring the robustness of current spectral methods. In this study, we consider homophily-heterophily inductive learning tasks, which naturally model non-targeted structural attacks. We observe that structure attacks enlarge the homophily gap between training and test graphs besides introducing heterophily, challenging spectral GNNs to generalize across different homophily levels. Consequently, despite their outstanding performance on heterophilic graphs, spectral GNNs such as GPRGNN have poor generalization ability when the training and test graphs have different homophily. For example, suppose we now have two friend-enemy networks like the ones in Figure <ref type="figure" target="#fig_0">1</ref>. If friends are more likely to become neighbors, representing the relationship "like", the network is homophilic. If enemies form more links corresponding to the relationship "hate", the network becomes heterophilic. If we apply spectral GNNs trained on "like" networks (where a low-pass filter is obtained) to "hate" networks, we will mistake enemies for friends on "hate" networks. Despite the strength of spectral GNNs in approximating optimal graph filters of arbitrary shapes, the lack of constraints on learned filters makes it difficult for them to generalize.</p><p>To improve the performance of current spectral methods against adversarial attacks, we design a novel spectral GNN that realizes generalization across homophily. Our contributions are:</p><p>? We proposed EvenNet, a simple yet effective spectral GNN that can be generalized to graphs of different homophily. EvenNet discards messages from odd-order neighbors inspired by balance theory, deriving a graph filter with only even-order terms. We provide a detailed theoretical analysis in the spatial domain to illustrate the advantages of EvenNet in generalizing to graphs of different homophily.</p><p>? We propose Spectral Regression Loss (SRL) to evaluate the performance of graph filters on specific graphs in the spectral domain. We theoretically analyze the relationship between graph filters and graph homophily, confirming that EvenNet with symmetric constraints is more robust in homophily-heterophily inductive learning tasks.</p><p>? We conduct comprehensive experiments on both synthetic and real-world datasets. The empirical results validate the superiority of EvenNet in generalizing to test graphs of different homophily without introducing additional computational complexity while remaining competitive in traditional node classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>Notations. Let G = (V, E) denote an undirected graph, where N = |V| is the number of nodes. Let A ? {0, 1} N ?N denote the adjacency matrix. Concretely, A ij = 1 indicates an edge between nodes v i and v j . Graph Laplacian is defined as L = D -A, along with a normalized version</p><formula xml:id="formula_0">L = I -D -1/2 AD -1/2</formula><p>, where I is the identity matrix and D is a diagonal degree matrix with each</p><formula xml:id="formula_1">diagonal element D ii = N i=1 A ij .</formula><p>It is known that L is a symmetric positive semidefinite matrix that can be decomposed as L = U ?U T , where ? = diag{? 0 , . . . , ? N -1 } is a diagonal eigenvalue matrix with 0 = ? 0 ? ? 1 ? . . . ? ? N -1 ? 2, and U is a unitary matrix consisting of eigenvectors.</p><p>For multi-class node classification tasks, nodes in G are divided into K classes {C 0 , . . . , C K-1 }. Each node v i is attached with an F dimension feature and a one-hot class label. Let X ? R N ?F be the input feature matrix and Y ? R N ?K = (y 0 , . . . , y K-1 ) be the label matrix, where y i is the indicator vector of class C i . Let R = Y Y and the size of class C k be R k .</p><p>Graph filtering. The graph filtering operation on graph signal X is defined as Z = ?(U g(?)U T X), where g(?) is the so-called graph filter, and ? is the normalization function. Directly learning g(?) requires eigendecomposition (EVD) of time complexity O(N 3 ). Recent studies suggest using polynomials to approximate g(?) instead, which is:</p><formula xml:id="formula_2">U g(?)U T X ? U K-1 i=0 w k ? k U X = K-1 i=0 w k Lk X,</formula><p>where {w k } are polynomial coefficients. We can also denote a K-order polynomial graph filter as a filter function g(?) = K k=0 w k ? k that maps eigenvalue ? ? [0, 2] to g(?). Homophily. Homophily reflects nodes' preferences for choosing neighbors. For a graph of strong homophily, nodes show a tendency to form connections with nodes of the same labels. The ratio of homophily h measures the level of overall homophily in a graph. Several homophily metrics have been proposed with different focuses <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b21">22]</ref>. We adopt edge homophily following <ref type="bibr" target="#b36">[37]</ref>, defined by</p><formula xml:id="formula_3">h = |{(u, v) : (u, v) ? E ? y u = y v }| |E| .<label>(1)</label></formula><p>By definition, h ? [0, 1] is the fraction of intra-class edges in the graph. The closer h is to 1, the more homophilic a graph is.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Proposed Method: EvenNet</head><p>In this section, we first introduce our motivation and the methodology of EvenNet. We then explain how EvenNet enhances the robustness of spectral GNNs from the perspective of both spatial and spectral domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Motivations</head><p>Reconsider the toy example in Figure <ref type="figure" target="#fig_0">1</ref>. Relationships between nodes are opposite on homophilic and heterophilic graphs, being straightforward but erratic under changes in the graph structure. Unconstrainted spectral GNNs tend to overuse such unstable relationships and fail to generalize across homophily. In contrast, a robust model should rely on more general topological information beyond homophily.</p><p>Balance theory <ref type="bibr" target="#b5">[6]</ref>, which arose from signed networks, offers a good perspective: "The enemy of my enemy is my friend, and the friend of my friend is also my friend." Balance theory always holds as a more general law, regardless of how structural information is revealed on the graph. As a result, we can obtain a more robust spectral GNN under homophily change by incorporating balance theory into graph filter design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">EvenNet</head><p>Denote propagation matrix as</p><formula xml:id="formula_4">P = I -L = D -1 2 AD -1 2 . A K-order polynomial graph filer is defined as g( L) = K k=0 w k</formula><p>Lk , where w k , k = 0, . . . , K are learnable parameters. We can rewrite the filter as</p><formula xml:id="formula_5">g( L) = K k=0 w k (I -L) k = K k=0 w k P k since w k is learnable.</formula><p>Then, we discard the monomials in g( L) containing odd-order P , obtaining:</p><formula xml:id="formula_6">g even ( L) = K/2 k=0 w k (I -L) 2k = K/2 k=0 w k P 2k .<label>(2)</label></formula><p>In practice, we decouple the transformation of input features and graph filtering process following <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18]</ref>. Our model then takes the simple form:</p><formula xml:id="formula_7">Z = f (X, P ) = K/2 k=0 w k P 2k t(X) ,<label>(3)</label></formula><p>where t is an input transformation function (e.g. MLP), and Z is the output node representation that can be fed into a softmax activation function for node classification tasks.</p><p>From the perspective of the spectral domain, g even keeps both low and high frequencies components and suppresses medium-frequency components, which is a band-rejection filter with the filter function inherently symmetric about ? = 1. We provide a theoretical analysis to demonstrate further the advantages of g even in Section 3.3 and 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Analysis from the Spatial Domain</head><p>Recently, Chen et al. <ref type="bibr" target="#b7">[8]</ref> analyzed the performance of graph filters under certain homophily. They concluded that graph filters operate as a potential reconstruction mechanism of the graph structure. A graph filter g( L) achieves better performance in a binary node classification task when the homophily of the transformed graph is high. The transformed homophily can therefore be seen as an indicator of the performance of graph filters on specific tasks. We now provide the well-defined transformed homophily adopted from <ref type="bibr" target="#b7">[8]</ref>.</p><p>Definition 1. (k-step interaction probability) For a propagation matrix P = D -1 2 AD -1 2 , the k-step interaction probability matrix is</p><formula xml:id="formula_8">?k = R -1 2 Y P k Y R -1 2 . Definition 2.</formula><p>(k-homophily degree) For a graph G with the k-step interaction probability ?k , its k-homophily degree H k ( ?) is defined as</p><formula xml:id="formula_9">H k ( ?) = 1 N K-1 l=0 R l ?k ll - m =l R m R l ?k lm .</formula><p>The transformed 1-homophily degree with filter g( L) is H 1 (g(I -?)).</p><p>By definition, the k-homophily degree reflects the average possibility of deriving a node's label from its k-hop neighbors. In Theorem 1, we show that even-order filters achieve more robust performance under homophily change by enjoying a lower variance of transformed homophily degree without losing average performance. The detailed proof is provided in Appendix A.1, including discussions about multi-class cases. Theorem 1. In a binary node classification task, assume the edge homophily h ? [0, 1] is a random variable that belongs to a uniform distribution. An even-order graph filter achieves no less E H H 1 g(I -?) with lower variation than the full-order version.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Analysis from Spectral Domain</head><p>Similar to Section 3.3, we first proposed Spectral regression loss (SRL) as an evaluation metric of graph filters in the spectral domain. In a binary node classification task, suppose the dimension of inputs F = 1. Denote the difference of labels as ?y = y 0 -y 1 . A graph filtering operation is defined as Z = ?(U g(?)U T X). Desirable filtering produces distinguishable node representations correlated to ?y to identify node labels. Let ? = U ?y and ? = U X. The classification task in the spectral domain is then a regression problem in the form of ?(?) = ?(g(?)?). We adopt Mean Squared Error (MSE) as the objective function of the regression problem and vector normalization as ?. Then SRL is defined as follows: Definition 3. (Spectral regression loss.) Denote ? = (? 0 , . . . , ? N -1 ) , ? = (? 0 , . . . , ? N -1 ) . In a binary node classification task, Spectral Regression Loss (SRL) of filter g(?) on graph G is:</p><formula xml:id="formula_10">L(G) = N -1 i=0 ? ? ? i ? N - g(? i )? i N -1 j=0 g(? 2 j )? 2 j ? ? 2 (4) = 2 - 2 ? N N -1 i=0 ? i g(? i )? i N -1 j=0 g(? 2 j )? 2 j .<label>(5)</label></formula><p>A graph filter that achieves lower SRL is of higher performance in the task.</p><p>Filters that Minimize SRL. Suppose ? i = w? i + , where w &gt; 0 reflects the correlation between labels and features in the spectral domain and be the noise term. If is close to 0, indicating the feature is free of noise and highly predictive, an all-pass filter (for example, MLP) with g(? i ) = 1 already minimizes SRL. If the noise becomes dominant, SRL is approximately equal to</p><formula xml:id="formula_11">1 N N -1 i=0 (? i - g(?i) ? j g(?j ) 2 ) 2 .</formula><p>In this noise-dominant case, an ideal filter is linearly correlated to ? and entirely structure-based to achieve a lower SRL. Most real-world situations lie between these two opposite settings. As a result, the shape of an ideal graph filter lies between an all-pass filter and an ?-dependent filter.</p><p>From the discussion above, we have shown that the performance of graph filters is related to the correlation between g(?) and ?. By connecting ? and h in Theorem 2, we establish the relationship between graph homophily and the performance of graph filters.</p><p>Theorem 2. For a binary node classification task on a k-regular graph G, let h be edge homophily and ? i be the i-th smallest eigenvalue of L, then</p><formula xml:id="formula_12">1 -h = N -1 i=0 ? 2 i ? i 2 N -1 j=0 ? i (6)</formula><p>The above equation can be extended to general graphs by replacing the normalized Laplacian L with the unnormalized L.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Notice that</head><formula xml:id="formula_13">N -1 i=0 ? 2</formula><p>i ? i is a convex combination of non-decreasing {? i } with weights ? 2 i . On a homophilic graph where h is close to 1, the right-hand side of Equation 6 is close to 0, implying larger weights for smaller ? i . A low-pass filter that suppresses high-frequency components is more correlated with such ? and therefore achieves lower SRL. From previous works, we have known that low-pass filters hold superiority on homophilic graphs, which is consistent with our analysis.</p><p>In the case of generalization, the distribution of {? i } is not fixed. A graph filter that minimizes the SRL on training graphs could achieve poor results on a test graph of different homophily. Remember that vanilla GCN could be worse than MLP on many heterophilic graphs. The same conclusion can be applied to learnable filters without any constraints, as they only tried to minimize SRL of training graphs. In Theorem 3, we prove that even-order design helps spectral GNNs better generalize between homophilic and heterophilic graphs as a practical constraint to current filters. Theorem 3. For a homophilic graph G 1 with non-increasing {? i } and a heterophilic graph G 2 with non-decreasing {? i }, an even-order filter g even that is trained on one of the graphs achieves a lower SRL gap |L(G 1 ) -L(G 2 )| when generalized to the other graph than a trained full-order filter.</p><p>Theorem 3 reveals a trade-off in filter design between fitting the training graph and generalizing across graphs of different homophily. While naive low-pass filters and high-pass filters work better on graphs with certain homophily, EvenNet tolerates imperfect filter learning and becomes more robust under homophily changes. A specific example of ring graphs is given in the following proposition. We see that EvenNet intrinsically satisfies the necessary condition for perfect generalization. Proposition 1. Consider two ring graphs G 1 and G 2 of 2n nodes, n ? N + . Suppose h(G 1 ) = 0 and h(G 2 ) = 1. Assume the spectrum of input difference ? = c1, where c &gt; 0 is a constant. Then the necessary condition for a graph filter g(?) to achieve</p><formula xml:id="formula_14">L(G 1 ) = L(G 2 ) is g(0) = g(2).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Spectral GNNs. GNNs have become prevalent in graph representation learning tasks. Among them, Spectral GNNs focus on designing graph filters with filter functions that operate on eigenvalues of graph Laplacian <ref type="bibr" target="#b4">[5]</ref>. Graph filters could be fixed <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b28">29]</ref> or approximated with polynomials. ChebNet <ref type="bibr" target="#b9">[10]</ref> adopts Chebyshev polynomials to realize faster localized spectral convolution. ARMA <ref type="bibr" target="#b2">[3]</ref> achieves a more flexible filter approximation with Auto-Regressive Moving Average filters. GPRGNN <ref type="bibr" target="#b8">[9]</ref> connects graph filtering with graph diffusion and learns coefficients of polynomial filters directly. BernNet <ref type="bibr" target="#b14">[15]</ref> utilizes Bernstein approximation to learn arbitrary filtering functions. Although learnable graph filters perform well on heterophilic graphs, they have difficulties generalizing if a homophily gap exists between training and test graphs.</p><p>GNNs for Heterophily. Previous works pointed out the weakness of vanilla GCN on graphs with heterophily. Recently, various GNNs have been proposed to tackle this problem. Geom-GCN <ref type="bibr" target="#b21">[22]</ref> uses a novel neighborhood aggregation scheme to capture long-distance information. Zhu et al. <ref type="bibr" target="#b36">[37]</ref> introduce several designs that are helpful for GNNs to learn representations beyond homophily. FAGCN <ref type="bibr" target="#b3">[4]</ref> adaptively combines signals of different frequencies in message passing via a selfgating mechanism. While these methods can handle heterophilic graphs, they are not guaranteed to generalize across graphs of different homophily.</p><p>Robust GNNs. In the field of designing robust GNNs, existing methods can be divided into two main categories: 1) Models utilizing new graph structures. GNN-Jaccard <ref type="bibr" target="#b29">[30]</ref> and GNN-SVD <ref type="bibr" target="#b11">[12]</ref> preprocess the input graph before applying vanilla GCN. ProGNN <ref type="bibr" target="#b15">[16]</ref> jointly learns a better graph structure and a robust model. 2) Attention-based models. RGCN <ref type="bibr" target="#b34">[35]</ref> uses variance-based attention to evaluate the credibility of nodes' neighbors. GNNGuard <ref type="bibr" target="#b33">[34]</ref> adopts neighbor importance estimation, aligning higher scores to trustworthy neighbors. TWIRLS <ref type="bibr" target="#b31">[32]</ref> applies an attention mechanism inspired by classical iterative methods PGD and IRLS. These methods are effective against structural attacks. However, the learned graph structure cannot be applied to inductive learning settings and requires additional memory. At the same time, attention-based models are limited in the spatial domain and need high computational costs. On the contrary, EvenNet improves the robustness of spectral GNNs without introducing additional computational costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiment</head><p>We conduct three experiments to test the ability of EvenNet in (1) generalizing across homophily on synthetic datasets, (2) defending against non-targeted structural attacks, and (3) supervised node classification on real-world datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Baselines</head><p>We compare our EvenNet with the following methods. (1) Method only using node features: A 2-layer MLP. (2) Methods achieving promising results on homophilic graphs: GCN <ref type="bibr" target="#b16">[17]</ref>, GAT <ref type="bibr" target="#b25">[26]</ref>, GCNII <ref type="bibr" target="#b6">[7]</ref>. (3) Methods handling heterophilic settings: H2GCN <ref type="bibr" target="#b36">[37]</ref>, FAGCN <ref type="bibr" target="#b3">[4]</ref>, GPRGNN <ref type="bibr" target="#b8">[9]</ref>. We also include five advanced defense models in the experiment about adversarial attacks, including RobustGCN <ref type="bibr" target="#b34">[35]</ref>, GNN-SVD <ref type="bibr" target="#b11">[12]</ref>, GNN-Jaccard <ref type="bibr" target="#b29">[30]</ref>, GNNGuard <ref type="bibr" target="#b33">[34]</ref>, and ProGNN <ref type="bibr" target="#b15">[16]</ref>. We implement the above models with the help of PyTorch Geometric <ref type="bibr" target="#b12">[13]</ref> and DeepRobust libraries <ref type="bibr" target="#b18">[19]</ref>. Details about hyperparameters and network architectures are deferred to Appendix C.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation on synthetic datasets</head><p>Datasets. In the first experiment testing generalization ability, we use cSBM model to generate graphs with arbitrary homophily levels following <ref type="bibr" target="#b8">[9]</ref>. Specifically, we divide nodes into two classes of equal size. Each node is attached with a feature vector randomly sampled from a class-specific Gaussian distribution. The homophily level of a graph is controlled by parameter ? ? [-1, 1]. A larger |?| indicates that the generated graph provides stronger topological information, while ? = 0 means only node features are helpful for prediction. Note that if ? &gt; 0, the graph is more homophilic and vice versa. Details about cSBM dataset are left to Appendix B.2 .</p><p>Settings. We set up node classification tasks in the inductive setting. We generate three graphs of the same size for each sub-experiment, one graph each for training, validation, and testing. Graphs for validation and testing share the same ? test , while training graphs either take ? train = ? test or ? train = -? test . If ? train = -? test , the training and test graphs are of opposite homophily but provide the same amount of topological information. A model manages to generalize across homophily when it realizes high prediction accuracy in both scenarios. In practice, we choose (? train , ? test ) ? {(?0.5, ?0.5), (?0.75, ?0.75)}. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results. The results are presented in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Performance under non-targeted structural adversarial attacks</head><p>Datasets. For adversarial attacks, we use four public graphs, Cora, Citeseer, PubMed <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b32">33]</ref> and ACM <ref type="bibr" target="#b26">[27]</ref> available in DeepRobust Library <ref type="bibr" target="#b18">[19]</ref>. We use the same preprocessing method and splits as <ref type="bibr" target="#b37">[38]</ref>, where the node set is split into 10% for training, 10% for validation, and 80% for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attack methods. Graph structural attacks can be categorized into poison attacks and evasion attacks.</head><p>In poison attacks, attack models are trained to lower the performance of a surrogate GNN model. The training graph and the test graph are both allowed to be perturbed but only with a limited amount of modifications, which are referred to as perturb ratios. Evasion attacks only happen during the inference phase, which means GNNs are trained on clean graphs. In our study, we include two poison attacks, Metattack (Meta) <ref type="bibr" target="#b37">[38]</ref> and MinMax attack <ref type="bibr" target="#b30">[31]</ref> with GCN the surrogate model, and an evasion variant of DICE attack <ref type="bibr" target="#b27">[28]</ref>. For poison attacks, we use the same setting in <ref type="bibr" target="#b33">[34]</ref> and set the perturb ratio for poison attacks to be 20%. For the evasion DICE, we randomly remove intra-class edges and add inter-class edges on the test graph while keeping the graph structure between labeled nodes unchanged. We set the perturb ratio of DICE attack in {0.4, 0.8, 1.2, 1.6}. From Figure <ref type="figure">2</ref>, it can be seen that all attacks result in homophily gaps between training graphs and the test graphs.</p><p>Results. Defense results are presented in Table <ref type="table" target="#tab_2">2</ref> and Figure <ref type="figure" target="#fig_1">3</ref>. For DICE attack, the performance of all methods significantly decreases along with the increase of homophiy gap except EvenNet. Interestingly, when the homophily gap is enormous, EvenNet enjoys a performance rebound, consistent with our topological information theory (strong homo. and strong hetero. are both helpful for prediction). For poison attacks, EvenNet achieves five SOTA compared with advanced defense models. Unlike spatial defense models, EvenNet is free of introducing extra time or space complexity.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Performance on real-world graph datasets</head><p>We evaluate EvenNet on real-world datasets to examine the performance of EvenNet on clean graphs. Besides of the datasets used in Section 5.3, we additionally include four public heterophilic datasets: Actor, Cornell, Squirrel and Texas <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25]</ref>. For all datasets, we adopt dense splits the same as <ref type="bibr" target="#b21">[22]</ref> to perform full-supervised node classification tasks, where the node set is split into 60% for training, 20% for validation, and 20% for testing. The results are shown in Table <ref type="table" target="#tab_3">3</ref>. While EvenNet sacrifices its performance for robustness, it is still competitive on most datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Ablation study</head><p>To analyze the effect of introducing odd-order components into graph filters, we develop a regularized variant of EvenNet named EvenReg. EvenReg adopts a full-order learnable graph filter, with the coefficients of odd-order monomials being punished as a regularization term. The training loss of EvenReg then takes the form:</p><formula xml:id="formula_15">L = L pred + ? K/2 k=0 |w 2k+1 |</formula><p>, where L pred is the classification loss and ? is a hyper-parameter controlling the degree of regularization.</p><p>We set ? = 0.05 and repeat experiments in Section 5.2. The results are presented in Table <ref type="table" target="#tab_4">4</ref>. The performance of EvenReg lies between full-order GPRGNN and EvenNet, indicating the introduced odd orders impede spectral GNNs to generalize across homophily. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this study, we investigate the ability of current GNNs to generalize across homophily. We observe that all existing methods experience severe performance degradation if a large homophily gap exists between training and test graphs. To overcome this difficulty, we proposed EvenNet, a simple yet effective spectral GNN robust under homophily change of graphs. We provide a detailed theoretical analysis to illustrate the advantages of EvenNet in generalization. We conduct experiments on both synthetic and real-world datasets. The empirical results verify the superiority of EvenNet in inductive learning across homophily and defense under structural attacks by sacrificing only a tiny amount of predictive accuracy on clean graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Additional proofs</head><p>A.1 Proof of Theorem 1</p><p>Before proving Theorem 1, We first demonstrate the superiority of even-hop neighbors over odd-hop neighbors from the perspective of random walks.</p><p>In a binary node classification task, denote the probability of a random walk of length k that starts and ends with nodes of the same label as p k , k &gt; 0. Suppose the edge homophily level h is a random variable that belongs to a uniform distribution in [0, 1] and p 1 = h, then:</p><formula xml:id="formula_16">Lemma 1. If k is odd, E h [p k ] = 1 2 . If k is even, E h [p k ] ? 1 2 .</formula><p>Proof.</p><formula xml:id="formula_17">If k = 1, p 1 = h, E h [p 1 ] = E h [h] = 1 2 . If k = 2, p 2 = h 2 + (1 -h) 2 = 2(h -1 2 ) 2 + 1 2 ? 1 2 , E h [p 2 ] = 1 0 p 2 dh ? 1 2 . For k &gt; 2, p k = p k-2 (1 -h) 2 + h 2 + (1 -p k-2 ) (2h(1 -h)) = 4p k-2 h 2 -4p k-2 h + p k-2 -2h 2 + 2h E h [p k ] = E h [E h [p k | p k-2 ]] = E h 1 0 p k dh = E h 1 3 p k-2 + 1 3 = 1 3 (E h [p k-2 + 1]) That is, for E h [p k-2 ] = 1 2 , E h [p k ] = 1 2 ; for E h [p k-2 ] ? 1 2 , E h [p k ] ? 1 2 .</formula><p>Therefore, Lemma 1 is proved.</p><p>Multi-class Cases. We now provide a brief discussion of the superiority of even-hop neighbors in multi-class node classification tasks following <ref type="bibr" target="#b36">[37]</ref>. Definition 4. Matrix Q ? R K?K is an independent between-class random walk matrix if it holds the following properties:</p><p>? Q is a random walk matrix.</p><formula xml:id="formula_18">? ?i = j, Q ii = Q jj . ? ?i = j, m = n, Q ij = Q mn .</formula><p>Suppose there are K classes of nodes in the graph, where the number of nodes of each class is the same, and node labels are assigned independently. Denote h as the edge homophily level, the 1-step between-class random walk matrix P is in the form of:</p><formula xml:id="formula_19">P = ? ? ? ? ? h 1-h K-1 ? ? ? 1-h K-1 1-h K-1 h ? ? ? 1-h K-1 . . . . . . . . . . . . 1-h K-1 1-h K-1 ? ? ? h ? ? ? ? ?</formula><p>, where P ij denotes the probability of a 1-step random walk that start with a node of label i and ends with a node of label j. By definition, P is an independent between-class random walk matrix.</p><p>Lemma 2. If M ? R K?K is an independent between-class random walk matrix, P = M P is an independent between-class random walk matrix as well.</p><p>Proof. It can be verified that P is still a random walk matrix, and for all i = j, m = n:</p><formula xml:id="formula_20">P ii = K-1 m=0 M im P mi = K-1 m=0 M jm P mj = P jj P ij = K-1 k=0 M ik P kj = K-1 k=0</formula><p>M mk P kn = P mn By definition, P is an independent between-class random walk matrix.</p><p>Denote the k-step between-class random walk matrix as P k . From Lemma 2, we can conclude that P k is an independent between-class random walk matrix for all k ? N + . In Lemma 3, we illustrate the advantages of even-order propagation by comparing the interaction probability between classes. Lemma 3. If k is even, the intra-class interaction probability P k ii is no less than inter-class interaction probability P k ij , i = j.</p><p>Proof. For k = 2:</p><formula xml:id="formula_21">P 2 ii = h 2 + (1 -h) 2 K -1 P 2 ij = 2h(1 -h) K -1 + (K -2)(1 -h) 2 (K -1) 2 P 2 ii -P 2 ij = h - 1 -h K -1 2 ? 0</formula><p>The inequality is tight when h = 1-h K-1 . For k = 2m, m &gt; 1, m ? N + , P m is an independent between-class random walk matrix that can be written as:</p><formula xml:id="formula_22">P m = ? ? ? ? ? ? h 1-h K-1 ? ? ? 1-h K-1 1-h K-1 h ? ? ? 1-h K-1 . . . . . . . . . . . . 1-h K-1 1-h K-1 ? ? ? h ? ? ? ? ? ? ,</formula><p>The above proof for k = 2 can be generalized to all h ? [0, 1]. Therefore, P m ii ? P m ij is satisfied as well.</p><p>Note that Lemma 3 is not satisfied for odd k if h is relatively smaller than 1-h K-1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Theorem 1</head><p>Proof. According to Definition 2, for a graph G with the k-step interaction probability ?k , its k-homophily degree H k ( ?) is defined as</p><formula xml:id="formula_23">H k ( ?) = 1 N K-1 l=0 R l ?k ll - m =l R m R l ?k lm .</formula><p>The transformed 1-homophily degree with filter g( L) is H 1 (g(I -?)).</p><p>Specifically, in a binary node classification problem, the k-homophily degree is:</p><formula xml:id="formula_24">H k ( ?) = 1 N R 0 ?k 00 + R 1 ?k 11 -2 R 0 R 1 ?k 01</formula><p>Denote a k-order polynomial graph filter as g k ( L) = w 0 + w 1 L + . . . + w k Lk , Let P i 00 The transformed 1-homophily degree of g k ( L) is:</p><formula xml:id="formula_25">H 1 g k (I -?) = 1 N R 0 g k I -? 00 + R 1 g k I -? 11 -2 R 0 R 1 g k I -? 01 = ( ? R 0 - ? R 1 ) 2 N (w 0 + . . . + w k ) - R 0 ?00 + R 1 ?11 -2 ? R 0 R 1 ?01 N (w 1 + 2w 2 + . . . + kw k ) + R 0 ?2 00 + R 1 ?2 11 -2 ? R 0 R 1 ?2 01 N (w 2 + 3w 3 + . . .) -. . . = c -H 1 ( ?)(w 1 + 2w 2 + . . . + kw k ) + H 2 ( ?)(w 2 + 3w 3 + . . .) + . . . = c + k i=1 ? i H i ( ?),</formula><p>where c is a constant and each ? i is a linear sum of {w i } that for arbitrary x ? R:</p><formula xml:id="formula_26">k i=0 ? i x i = k i=0 w i (1 -x) i .</formula><p>Following Lemma 1, the average possibility of deriving a node's label from its odd-hop neighbors is</p><formula xml:id="formula_27">1 2 , which means E h [H i ( ?)] = 0 and Var h [H i ( ?)] ? 0 for odd i.</formula><p>By removing the odd-order terms, the transformed 1-homophily degree does not decrease on average, but enjoys a lower variation.</p><p>Rewrite filter g k ( L) as g k ( L) = ? 0 + ? 1 (I -L) + . . . + ? k (I -L) k and set ? i = 0 for odd i, the graph filter is then in the form of :</p><formula xml:id="formula_28">g k ( L) = k/2 i=0 ? i (I -L) 2i = k/2 i=0 ? i P 2i ,</formula><p>which is exactly the graph filter of EvenNet. Therefore, EvenNet has lower variation on the transformed 1-homophily degree without sacrificing the average performance.</p><p>A.2 Proof of Theorem 1 Lemma 4. Given unnormalized graph Laplacian L = U L ? L U L and its eigenvalues {? i }, denote the number of edges on the graph is m, and label difference as ?y = y 0 -y 1 ? R N ?1 . For unnormalized spectrum of label difference on L is ? = U L ?y = (? 0 , ? 1 , . . . , ? N -1 ) , then</p><formula xml:id="formula_29">N -1 i=0 ? i = 2m (7) 1 -h = N -1 i=0 (? i ) 2 ? i 2 N -1 j=0 ? j</formula><p>Proof. Denote the trace of a matrix M as tr(M ), the degree of node v i as d i .</p><p>N -1</p><formula xml:id="formula_30">i=0 ? i = tr(L) = N -1 i=0 d i = 2m</formula><p>The Dirichlet energy of the label difference is defined as:</p><formula xml:id="formula_31">E(?y) = ?y L?y = (i,j)?E</formula><p>(?y i -?y j ) 2 = 4</p><p>(i,j)?E</p><formula xml:id="formula_32">1{?y i = ?y j } = 4(1 -h)m<label>(8)</label></formula><p>Using L = U L ? L U T L , the Dirichlet energy of the label difference can also be expressed as:</p><formula xml:id="formula_33">E(?y) = ?y T U L ? L U L ?y = ? ? L ? = N -1 i=0 ? i (? i ) 2<label>(9)</label></formula><p>By integrating equations 7 8 9, we get:</p><formula xml:id="formula_34">1 -h = N -1 i=0 (? i ) 2 ? i 2 N -1 j=0 ? j Proof of Theorem 2</formula><p>Proof. For a k-regular graph, denote normalized graph Laplacian as L = U ?U , then</p><formula xml:id="formula_35">L = D -1/2 LD -1/2 = D -1/2 U L ? L U L D -1/2 = 1 k U L ? L U L<label>(10)</label></formula><p>From equation 10, we get U = U L , ? i = ? i /k. Denote the spectrum of label difference on L as ?, then ? = ? . By substituting ? i with k? i in Lemma 4, we acquire the equation in Theorem 2.</p><p>If ? is normalized as in the SRL that satisfies N -1 i=0 ? 2 i = 1, Theorem 2 is then in the form of:</p><formula xml:id="formula_36">1 -h = N N -1 i=0 (? i ) 2 ? i 2 N -1 j=0 ? j A.3 Proof of Theorem 3 Proof. Denote filter g(?) as g(?) = K i=0 w i (1 -?) i , g even (?) = 1 2 (g(?) + g(2 -?)</formula><p>) and g odd = g(?) -g even (?). The filter g even (?) is free of odd order terms. The odd filter g odd (?) can be seen as the gap between the full-order filter and the even-order filter.</p><formula xml:id="formula_37">g even (?) = K/2 k=0 w k (1 -?) 2k g odd (?) = K/2 k=0 w k (1 -?) 2k+1 .</formula><p>We now consider the SRL gap of the odd-order filter to illustrate the effect of removing odd-order terms. The regression problem in the spectral domain with normalized ? is:</p><formula xml:id="formula_38">? = ?(g(?)?), N -1 i=0 ? 2 i = 1.</formula><p>Suppose ? and ? is positive correlated in the form of E[?] = w?, w &gt; 0, and ? N -1 = 2 for both G 1 and G 2 (to ensure both graphs can achieve h = 0).</p><p>The SRL of filter g odd and g even between normalized ? and normalized g(?)? is:</p><formula xml:id="formula_39">L odd (G) = 2 - 1 T odd N i=0 ? 2 i g(? i ) odd = 2 - 1 T odd ? ? N//2 i=0 ? 2 i g(? i ) odd + N i=N//2 ? 2 i |g(? i ) odd | ? ? = 2 - 1 T odd ? ? N//2 i=0 (? 2 i -? 2 N -i-1 )g(? i ) odd ? ? = 2 -L o<label>(11)</label></formula><formula xml:id="formula_40">L even (G 1 ) = 2 - 1 T even ? ? N//2 i=0 (? 2 i + ? 2 N -i-1 )g(? i ) even ? ? = 2 -L e ,<label>(12)</label></formula><p>where</p><formula xml:id="formula_41">T type = N -1 i=0 g type (? i ) 2 ? 2 i .</formula><p>Compare equations 11 and 12. If g(? i ) is of a different monotonicity against {? i }, which happens when a trained odd-filter is generalized to graphs of opposite homophily, L o becomes negative. In contrast, L e is always positive and benefits for reducing SRL. Suppose L o is the approximate SRL gap between g(?) and g even (?). The instability of L o implies L g (G train ) &lt; L geven (G train ) and L g (G test ) &gt; L geven (G test ), reflecting a larger SRL gap of fullorder filters than the even-order filters.</p><p>More generally, for the cases where ? N -1 &lt; 2, we can still adopt the idea of discarding odd-order terms. Rewrite g(?) as g (?) = K i=0 (? mid -?) i , where ? mid is the median of {? i }. By applying the same analysis above, we can see that it is still beneficial to remove odd-order terms from g (?) to narrow the SRL gap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Proof of Proposition 1</head><p>Proof. In the case where h(G 1 ) = 0 and h(G 2 ) = 1, denote the label difference of G 1 and G 2 as ?y 1 and ?y 2 , where ?y 1 = (1, -1, 1, -1, . . . , 1, -1) , ?y 2 = (1, . . . , 1) . Denote U = (u 0 , u 1 , . . . , u N -1 ) , where u i is the i-th eigenvector of U and u k (n) is the n-th element of u k .</p><p>On ring graphs, denote a kn = sin(?(k + 1)n/N, b kn = cos(?kn/N ), then the normalized u k (n) satisfies:</p><formula xml:id="formula_42">u k (n) = ? ? ? ? ? ? ? ? ? ? ? ? ? a kn ? N/2 , for odd k, k &lt; N -1 b kn ? N/2 , for even k cos(?n) ? N , for odd k, k = N -1 1 ? N for even k, k = 0</formula><p>Let the normalized spectrum of G 1 be ? = U T ?y 1 = (? 0 , . . . , ? N -1 ) , the normalized spectrum of G 2 be ? = U T ?y 2 = (? 0 , . . . , ? N -1 ) .</p><p>Suppose</p><formula xml:id="formula_43">1 ? i &lt; N -1 is odd, ? i = 1 ? N/2 N -1 i=0 (-1) i a ki , ? i = 1 ? N/2 N -1</formula><p>i=0 a ki and ? = ?(i+1) N , then:</p><formula xml:id="formula_44">? i -? i = 1 N/2 N/2 n=1 sin ?(i + 1)(2n -1) N = 1 N/2 N/2 n=1 sin ((2n -1)?) = 1 N/2 N/2 n=1 sin(?) sin ((2n -1)?) sin(?) = 1 N/2 N/2 n=1 (cos (2n -2) ?) -cos (2n?) 2 sin(?) = 1 N/2 cos(0) -cos(N ?) 2 sin ? = 0</formula><p>Therefore, for odd i and 1 ? i ? N -2, ? i = ? i . The conclusion can be generalized to even i and 1 ? i ? N -2 using the same method.</p><p>For i = 0 and i</p><formula xml:id="formula_45">= N -1, we have ? 0 -? 0 = - ? N , ? N -1 -? N -1 = - ? N . The spectral gap between G 1 and G 2 is: L g (G 1 ) -L g (G 2 ) = N -1 i=0 2(? i -? i )g(? i ) N -1 j=0 g(? j ) 2 = 2 ? N g(0) -2 ? N g<label>(2)</label></formula><p>N -1 j=0 g(? j ) 2</p><p>Therefore, the necessary condition for the spectral gap to be 0 is g(0) = g(2).  We conduct cSBM datasets following <ref type="bibr" target="#b8">[9]</ref> in the inductive setting. Denote a cSBM graph G as G ? cSBM(n, f, ?, ?), where n is the number of nodes, f is the dimension of features, and ? and ? are hyperparameters respectively controlling the proportion of contributions from the graph structure and node features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Dataset Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Hyperparameter settings</head><p>Node classification on cSBM Datasets &amp; Common datasets For all models, we use early stopping 200 with a maximum of 1000 epochs. All hidden size of layers are set to be 64. We use the Adam optimizer and search the optimal leaning rate over {0.001, 0.005, 0.01, 0.05} and weight decay {0.0, 0.0005}. For all models, the linear dropout is searched over {0.1, 0.3, 0.5, 0.7, 0.9}. For the model-specific hyperparameters, we refer to the optimal hyperparameters reported in corresponding papers. For MLP, we include 2 linear layers. For GCN and H2GCN, we set the number of convolutional layers be 2. For GAT, we use 8 attention heads with 8 hidden units each in the first convolutional layer, and 1 attention head and 64 hidden units in the second convolutional layer.</p><p>For FAGCN, we search the number of layers over {2, 4, 8}, over {0.3, 0.4, 0.5}. For GCNII, we set ? = 0.5 and search the number of layers over {8, 16, 32}, ? over {0.1 0.3 0.5}. For GPRGNN and EvenNet, we set the number of linear layers be 2 and ? = 0.1. For both models, we search the dropout rate for the propagation layer over {0.3, 0.5, 0.7} and the order of graph filter over {4, 6, 8, 10}.</p><p>Against adversarial attacks. For the poisson attacks, we use a 2-layer GCN as the surrogate model. We use the strongest variant of Metattack, which is "Meta-Self" as the attack strategy. For the defense models, we carefully follow their provided guidelines of hyperparameter settings, and use the optimal hyperparameters as they reported. For other models, we use the Adam optimizer with leaning rate 0.01, weight decay 0.0005 and dropout rate 0.5. For FAGCN, we use 8 convolutional layers and a fixed = 0.3. For GCNII, we use 16 convolutional layers and a fixed ? = 0.2. For GPRGNN and EvenNet, we set the order of graph filter be 4 in the DICE attack and 10 in poison attacks. For all models, we use early stopping 30 with a maximum of 200 epochs. Other hyperparameters are kept the same as the ones in the node-classification experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Additional Defense Results</head><p>Homophily gap We include the homophily gap between training and test graph for Citeseer and ACM datasets in </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Two friend-enemy networks of opposite homophily.</figDesc><graphic url="image-1.png" coords="2,365.40,142.19,138.60,77.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: DICE attack on four homophilic datasets. EvenNet is marked with " ".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig 4 and 5 .Figure 4 :Figure 5 :</head><label>545</label><figDesc>Figure 4: Homophily level of training graphs and test graphs on Citeseer after attacks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>When ? train = ? test , GPRGNN achieves the highest predictive accuracy as it best fits the desired graph filter. However, when ? train = -? test , all methods except EvenNet suffer from a huge performance drop. Vanilla GCN, which corresponds to a low-pass filter, achieves desirable performance only when the test graph is homophilic. GPRGNN overfits training graphs most, resulting in more severe performance degradation on test graphs of Homophily level of training graphs and test graphs on Cora after DICE attack, Metattack and MinMax attack. All attacks result in homophily gap between training and test graphs. . opposite homophily. EvenNet is the only method that achieves more than 75% accuracy on all datasets among all the models, which is robust in generalization across homophily.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>7UDLQ</cell><cell></cell><cell>7UDLQ</cell></row><row><cell>+RPRSKLOO\OHYHO</cell><cell>',&amp;(</cell><cell>7UDLQ 7HVW</cell><cell>0HWD</cell><cell>7HVW</cell><cell>0LQ0D[</cell><cell>7HVW</cell></row><row><cell></cell><cell>3HUWXUE5DWLR</cell><cell></cell><cell>3HUWXUE5DWLR</cell><cell></cell><cell>3HUWXUE5DWLR</cell><cell></cell></row><row><cell cols="2">Figure 2:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Average node classification accuracy(%) and absolute performance gap(%) between experiments of the same ? train over ten repeated experiments on synthetic cSBM datasets. The best result is highlighted by bold font, the second best result is underlined.</figDesc><table><row><cell>? train</cell><cell></cell><cell>0.75</cell><cell></cell><cell>0.50</cell><cell></cell><cell>-0.50</cell><cell>-0.75</cell></row><row><cell>? test</cell><cell>0.75</cell><cell cols="2">-0.75 gap(?) 0.50</cell><cell cols="2">-0.50 gap(?) -0.50</cell><cell>0.50 gap(?) -0.75</cell><cell>0.75 gap(?)</cell></row><row><cell>MLP</cell><cell cols="2">57.92 57.24</cell><cell cols="2">0.68 63.65 64.26</cell><cell cols="2">0.61 63.28 63.83</cell><cell>0.55 56.92 59.24</cell><cell>2.32</cell></row><row><cell>GCN</cell><cell cols="2">75.24 60.31</cell><cell cols="2">15.11 78.98 63.21</cell><cell cols="2">15.77 63.27 76.67</cell><cell>13.40 60.48 77.88</cell><cell>17.40</cell></row><row><cell>GAT</cell><cell cols="2">74.15 60.55</cell><cell cols="2">13.60 75.64 61.96</cell><cell cols="2">13.68 64.43 71.02</cell><cell>6.59 63.19 71.61</cell><cell>8.42</cell></row><row><cell>GCNII</cell><cell cols="2">83.12 54.30</cell><cell cols="2">28.82 78.07 58.43</cell><cell cols="2">19.64 72.32 67.68</cell><cell>4.64 65.93 62.92</cell><cell>3.01</cell></row><row><cell>H2GCN</cell><cell cols="2">76.41 54.81</cell><cell cols="2">21.60 78.86 58.89</cell><cell cols="2">19.97 78.43 59.77</cell><cell>18.66 76.29 55.92</cell><cell>20.37</cell></row><row><cell>FAGCN</cell><cell cols="2">81.29 60.44</cell><cell cols="2">20.85 78.73 60.28</cell><cell cols="2">18.45 79.45 60.62</cell><cell>18.83 85.78 57.34</cell><cell>28.44</cell></row><row><cell cols="3">GPRGNN 95.93 53.52</cell><cell cols="2">42.41 84.42 56.16</cell><cell cols="2">28.26 84.18 63.76</cell><cell>20.42 95.99 66.49</cell><cell>29.52</cell></row><row><cell>EvenNet</cell><cell cols="2">95.29 94.59</cell><cell cols="2">0.70 82.37 82.57</cell><cell cols="2">0.20 81.99 79.81</cell><cell>2.18 94.79 96.25</cell><cell>1.46</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Average node classification accuracy (%) against non-targeted poison attacks Metattack and MinMax attack with perturb ratio 20% over 5 different splits. The best result is highlighted by bold font, the second best result is underlined.</figDesc><table><row><cell cols="2">Dataset</cell><cell cols="6">Meta-cora Meta-citeseer Meta-acm MM-cora MM-citeseer MM-acm</cell></row><row><cell>MLP</cell><cell></cell><cell>58.60</cell><cell>62.93</cell><cell>85.74</cell><cell>59.81</cell><cell>63.72</cell><cell>85.66</cell></row><row><cell>GCN</cell><cell></cell><cell>63.76</cell><cell>61.98</cell><cell>68.29</cell><cell>69.21</cell><cell>68.02</cell><cell>69.37</cell></row><row><cell>GAT</cell><cell></cell><cell>66.51</cell><cell>63.66</cell><cell>68.50</cell><cell>69.50</cell><cell>67.04</cell><cell>69.26</cell></row><row><cell cols="2">GCNII</cell><cell>66.57</cell><cell>64.23</cell><cell>78.53</cell><cell>73.01</cell><cell>72.26</cell><cell>82.90</cell></row><row><cell cols="2">H2GCN</cell><cell>71.62</cell><cell>67.26</cell><cell>83.75</cell><cell>66.76</cell><cell>69.66</cell><cell>84.84</cell></row><row><cell cols="2">FAGCN</cell><cell>72.14</cell><cell>66.59</cell><cell>85.93</cell><cell>64.90</cell><cell>66.33</cell><cell>81.49</cell></row><row><cell cols="2">GPRGNN</cell><cell>69.08</cell><cell>65.63</cell><cell>86.23</cell><cell>77.33</cell><cell>72.21</cell><cell>83.64</cell></row><row><cell cols="2">RobustGCN</cell><cell>60.38</cell><cell>60.44</cell><cell>62.29</cell><cell>68.53</cell><cell>63.16</cell><cell>61.60</cell></row><row><cell cols="2">GNN-SVD</cell><cell>64.83</cell><cell>64.98</cell><cell>84.55</cell><cell>66.33</cell><cell>64.97</cell><cell>81.08</cell></row><row><cell cols="2">GNN-Jaccard</cell><cell>68.30</cell><cell>63.40</cell><cell>67.81</cell><cell>72.98</cell><cell>68.43</cell><cell>69.03</cell></row><row><cell cols="2">GNNGuard</cell><cell>75.98</cell><cell>68.57</cell><cell>62.19</cell><cell>73.23</cell><cell>66.14</cell><cell>66.15</cell></row><row><cell cols="2">ProGNN</cell><cell>75.25</cell><cell>68.15</cell><cell>83.99</cell><cell>77.91</cell><cell>72.26</cell><cell>73.51</cell></row><row><cell cols="2">EvenNet</cell><cell>76.26</cell><cell>68.54</cell><cell>89.55</cell><cell>78.42</cell><cell>73.98</cell><cell>87.35</cell></row><row><cell>$FFXUDF\</cell><cell>&amp;RUD</cell><cell></cell><cell></cell><cell>&amp;LWHVHHU</cell><cell></cell><cell></cell><cell>0/3 *&amp;1 *$7 *&amp;1B69' *&amp;1B-DFFDUG</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>5*&amp;1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>)$*&amp;1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>+*&amp;1</cell></row><row><cell>$FFXUDF\</cell><cell>$&amp;0</cell><cell></cell><cell></cell><cell>3XE0HG</cell><cell></cell><cell></cell><cell>*&amp;1B*XDUG *&amp;1,, *35*11 (YHQ1HW</cell></row><row><cell></cell><cell></cell><cell>3HUWXUE5DWLR</cell><cell></cell><cell></cell><cell>3HUWXUE5DWLR</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Average node classification accuracy(%) on real-world benchmark datasets over 10 different splits. The best result is highlighted by bold font, the second best result is underlined.</figDesc><table><row><cell>Model</cell><cell>Cora</cell><cell>Cite.</cell><cell cols="2">Pubm. Cham. Texas Corn. Squi. Actor</cell></row><row><cell>MLP</cell><cell cols="3">74.88 74.82 85.58</cell><cell>46.65 89.50 90.17 32.33 41.30</cell></row><row><cell>GCN</cell><cell cols="3">87.19 80.87 87.51</cell><cell>63.28 80.66 74.09 46.42 34.21</cell></row><row><cell>GAT</cell><cell cols="3">88.21 81.36 89.42</cell><cell>64.02 81.63 81.97 47.87 36.21</cell></row><row><cell>GCNII</cell><cell cols="3">87.91 82.13 86.41</cell><cell>50.76 86.23 89.83 36.35 41.68</cell></row><row><cell>FAGCN</cell><cell cols="3">88.83 80.35 89.34</cell><cell>56.67 89.18 90.16 39.10 41.18</cell></row><row><cell>H2GCN</cell><cell cols="3">87.59 79.69 88.68</cell><cell>55.88 88.52 85.57 34.45 39.62</cell></row><row><cell cols="4">GPRGNN 88.34 80.16 90.08</cell><cell>67.13 93.44 92.45 51.93 41.62</cell></row><row><cell>EvenNet</cell><cell cols="3">87.25 79.67 89.52</cell><cell>66.13 93.77 92.13 49.80 41.69</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Average node classification accuracy(%) of EvenReg over 10 repeated experiments on synthetic cSBM datasets.</figDesc><table><row><cell>? train</cell><cell cols="2">0.75</cell><cell cols="2">0.50</cell><cell cols="2">-0.50</cell><cell cols="2">-0.75</cell></row><row><cell>? test</cell><cell>0.75</cell><cell>-0.75</cell><cell>0.50</cell><cell cols="2">-0.50 -0.50</cell><cell>0.50</cell><cell>-0.75</cell><cell>0.75</cell></row><row><cell cols="9">GPRGNN 95.93 53.52 84.42 56.16 84.18 63.76 95.99 66.49</cell></row><row><cell>EvenNet</cell><cell cols="8">95.29 94.59 82.37 82.57 81.99 79.81 94.79 96.25</cell></row><row><cell>EvenReg</cell><cell cols="8">95.44 93.90 84.05 78.06 83.72 75.33 95.40 95.73</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>The statistics of real-world Datasets are included in table B.1. In the node classification task, we transform heterophilic datasets into undirected ones following<ref type="bibr" target="#b8">[9]</ref>. In the adversarial attack experiments, we select the largest connected component of each graph for attacks, training, validation and test following<ref type="bibr" target="#b37">[38]</ref>.</figDesc><table><row><cell></cell><cell cols="3">Cora Citeseer PubMed</cell><cell cols="5">ACM Chameleon Squirrel Cornell Texas</cell><cell>Actor</cell></row><row><cell cols="2">Nodes 2,708</cell><cell>3,327</cell><cell>19,717</cell><cell>3,025</cell><cell>2,277</cell><cell>5,201</cell><cell>183</cell><cell>183</cell><cell>7,600</cell></row><row><cell cols="2">Edges 5,278</cell><cell>4,552</cell><cell cols="2">44,324 13,128</cell><cell cols="2">31,371 198,353</cell><cell>277</cell><cell cols="2">279 26,659</cell></row><row><cell cols="2">Features 1,433</cell><cell>3,703</cell><cell>500</cell><cell>1,870</cell><cell>2,325</cell><cell>2,089</cell><cell cols="2">1,703 1,703</cell><cell>932</cell></row><row><cell>Classes</cell><cell>7</cell><cell>6</cell><cell>3</cell><cell>3</cell><cell>5</cell><cell>5</cell><cell>5</cell><cell>5</cell><cell>5</cell></row><row><cell>Homophily Level</cell><cell>0.81</cell><cell>0.74</cell><cell>0.80</cell><cell>0.82</cell><cell>0.23</cell><cell>0.22</cell><cell>0.30</cell><cell>0.09</cell><cell>0.22</cell></row></table><note><p>B.1 Real-world Dataset statistics</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Statistics of real-world datasets.</figDesc><table /><note><p>B.2 Synthetic Datasets.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Preprint. Under review.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We assume the number of classes is 2, and each class is of the same size n/2. Each node v i is assigned with a label y i ? {-1, +1} and an f -dimensional Gaussian vector x i = ? n y i u + Zi ? f , where u ? N (0, I/f ) and Z is a random noise term.</p><p>Assume the generated graph is of average degree d, and denote the adjacency matrix as A. The graph structure of the cSBM graph is:</p><p>The parameter ? discussed in the experiments is in the form of ? = arctan( ? m? n f ) * 2 ? , where m &gt; 0 is a constant. A larger |?| reflects a larger ? over ?, that is the proportion of information from the graph structure is larger.</p><p>In practice, we choose n = 3000, f = 2000, d = 5, m = 3 ? 3</p><p>2 for all graphs. The choices of ? and ? and the resulted homophily ratio are listed in Table <ref type="table">B</ref>.2. As discussed in <ref type="bibr" target="#b10">[11]</ref>, only the hyperparameters ? and ? that satisfy </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Experiment Details C.1 Experimental Device</head><p>Experiments are conducted on a device with an NVIDIA TITAN V GPU (12GB memory), Intel(R) Xeon(R) Silver 4114 CPU (2.20GHz), and 1TB of RAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Model Architectures</head><p>For GPRGNN, GCNII, GNN_Guard and ProGNN, we rely on the official released code. For FAGCN, we implement the method with Pytorch Geometric(PyG) based on the released code. For H2GCN, we rely on the PyG version implemented by <ref type="bibr" target="#b19">[20]</ref>. Defense models are based on the DeepRobust Library implemented versions <ref type="bibr" target="#b18">[19]</ref>. Other methods are based on the PyG implemented versions <ref type="bibr" target="#b12">[13]</ref>.</p><p>The URL and commit number are presented in Table <ref type="table">C</ref>  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mixhop: Higher-order graph convolutional architectures via sparsified neighborhood mixing</title>
		<author>
			<persName><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hrayr</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nazanin</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Ver Steeg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aram</forename><surname>Galstyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Analyzing the expressive power of graph neural networks in a spectral perspective</title>
		<author>
			<persName><forename type="first">Muhammet</forename><surname>Balcilar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Renton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>H?roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Ga?z?re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S?bastien</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Honeine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Graph neural networks with convolutional arma filters</title>
		<author>
			<persName><forename type="first">Maria</forename><surname>Filippo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniele</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Grattarola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cesare</forename><surname>Livi</surname></persName>
		</author>
		<author>
			<persName><surname>Alippi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TPAMI, 2021</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Beyond low-frequency information in graph convolutional networks</title>
		<author>
			<persName><forename type="first">Deyu</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huawei</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2001">2021. 4, 5.1</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Structural balance: a generalization of heider&apos;s theory</title>
		<author>
			<persName><forename type="first">Dorwin</forename><surname>Cartwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Harary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">277</biblScope>
			<date type="published" when="1956-03">1956. 3.1</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Simple and deep graph convolutional networks</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhewei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zengfeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">When does a spectral graph neural network fail in node classification</title>
		<author>
			<persName><forename type="first">Zhixian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.07902,2022.3.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Adaptive universal generalized pagerank graph neural network</title>
		<author>
			<persName><forename type="first">Eli</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olgica</forename><surname>Milenkovic</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In ICLR, 2021. 1, 3.2, 4, 5.1, 5.2, B.1, B.2</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">Micha?l</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Contextual stochastic block models</title>
		<author>
			<persName><forename type="first">Yash</forename><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhabrata</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Montanari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elchanan</forename><surname>Mossel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<meeting><address><addrLine>B.2</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">All you need is low (rank): Defending against adversarial attacks on graphs</title>
		<author>
			<persName><forename type="first">Negin</forename><surname>Entezari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saba</forename><forename type="middle">A</forename><surname>Al-Sayouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amirali</forename><surname>Darvishzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evangelos</forename><forename type="middle">E</forename><surname>Papalexakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2001-05-04">2020. 1, 4, 5.1</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast graph representation learning with pytorch geometric</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Eric Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2002">2019. 5.1, C.2</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bernnet: Learning arbitrary graph spectral filters via bernstein approximation</title>
		<author>
			<persName><forename type="first">Mingguo</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhewei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zengfeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongteng</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS, 2021</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Graph structure learning for robust graph neural networks</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaorui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianfeng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2001-05-04">2020. 1, 4, 5.1</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2001-05-04">2017. 1, 4, 5.1</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Predict then propagate: Combining neural networks with personalized pagerank for classification on graphs</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In ICLR, 2018. 3.2, 4</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Deeprobust: A pytorch library for adversarial attacks and defenses</title>
		<author>
			<persName><forename type="first">Yaxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.06149</idno>
		<imprint>
			<date type="published" when="2002">2020. 5.1, 5.3, C.2</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Large scale learning on non-homophilous graphs: New benchmarks and strong simple methods</title>
		<author>
			<persName><forename type="first">Derek</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hohne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linda</forename><surname>Sijia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vaishnavi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omkar</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><surname>Bhalerao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nam</forename><surname>Ser</surname></persName>
		</author>
		<author>
			<persName><surname>Lim</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In NIPS, 2021. 2, C.2</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Revisiting graph neural networks</title>
		<author>
			<persName><forename type="first">Hoang</forename><surname>Nt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takanori</forename><surname>Maehara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">All we have is low-pass filters. arXiv: Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Geom-gcn: Geometric graph convolutional networks</title>
		<author>
			<persName><forename type="first">Hongbin</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingzhe</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Chen-Chuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2004-05-04">2020. 1, 2, 4, 5.4</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-scale attributed node embedding</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Benedek Rozemberczki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rik</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Complex Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">5</biblScope>
			<pubPlace>AI magazine</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Social influence analysis in large-scale networks</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Heterogeneous graph attention network</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houye</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanfang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2003">2019. 5.3</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Hiding individuals and communities in a social network</title>
		<author>
			<persName><forename type="first">Marcin</forename><surname>Waniek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomasz</forename><surname>Michalak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Talal</forename><surname>Rahwan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Wooldridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Nature Human Behaviour</title>
		<imprint>
			<date type="published" when="2003">2016. 5.3</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amauri</forename><surname>Holanda De Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adversarial examples for graph data: Deep insights into attack and defense</title>
		<author>
			<persName><forename type="first">Huijun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuriy</forename><surname>Tyshetskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Docherty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liming</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2001-05-04">2019. 1, 4, 5.1</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Topology attack and defense for graph neural networks: An optimization perspective</title>
		<author>
			<persName><forename type="first">Kaidi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongge</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsui-Wei</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyi</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xue</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAL</title>
		<imprint>
			<date type="published" when="2003">2019. 1, 5.3</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Graph neural networks inspired by classical iterative algorithms</title>
		<author>
			<persName><forename type="first">Yongyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangkun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinjing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhewei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zengfeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wipf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2003">2016. 5.3</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Gnnguard: Defending graph neural networks against adversarial attacks</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2003-05-01">2020. 1, 4, 5.1, 5.3</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Robust graph convolutional networks against adversarial attacks</title>
		<author>
			<persName><forename type="first">Dingyuan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2001-05-04">2019. 1, 4, 5.1</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">On the relationship between heterophily and robustness of graph neural networks</title>
		<author>
			<persName><forename type="first">Jiong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchen</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Loveland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">T</forename><surname>Schaub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.07767</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Beyond homophily in graph neural networks: Current limitations and effective designs</title>
		<author>
			<persName><forename type="first">Jiong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingxiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Heimann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2001">2020. 1, 2, 4, 5.1, A.1</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Adversarial attacks on graph neural networks via meta learning</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Z?gner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2001">2019. 1, 5.3, B.1</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
