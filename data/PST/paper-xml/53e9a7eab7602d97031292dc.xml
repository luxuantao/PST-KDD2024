<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Automatic Detection and Recognition of Signs From Natural Scenes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Xilin</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Jie</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Alex</forename><surname>Waibel</surname></persName>
						</author>
						<title level="a" type="main">Automatic Detection and Recognition of Signs From Natural Scenes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">74D83A33641A6C22E8381D3C0C9655E1</idno>
					<idno type="DOI">10.1109/TIP.2003.819223</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T03:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Affine rectification</term>
					<term>optical character recognition (OCR)</term>
					<term>sign detection</term>
					<term>sign recognition</term>
					<term>text detection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present an approach to automatic detection and recognition of signs from natural scenes, and its application to a sign translation task. The proposed approach embeds multiresolution and multiscale edge detection, adaptive searching, color analysis, and affine rectification in a hierarchical framework for sign detection, with different emphases at each phase to handle the text in different sizes, orientations, color distributions and backgrounds. We use affine rectification to recover deformation of the text regions caused by an inappropriate camera view angle. The procedure can significantly improve text detection rate and optical character recognition (OCR) accuracy. Instead of using binary information for OCR, we extract features from an intensity image directly. We propose a local intensity normalization method to effectively handle lighting variations, followed by a Gabor transform to obtain local features, and finally a linear discriminant analysis (LDA) method for feature selection. We have applied the approach in developing a Chinese sign translation system, which can automatically detect and recognize Chinese signs as input from a camera, and translate the recognized text into English.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>W E work, live, and play in a so-called information so- ciety where we communicate with people and information systems through diverse media in increasingly varied environments. A wealth of information is embedded in natural scenes. Signs are good examples of objects in natural environments that have high information content. They make our lives easier when we are familiar with them, but they pose problems or even danger when we are not. For example, a tourist might not be able to understand a sign in a foreign country that specifies warnings or hazards. Automatic sign translation, in conjunction with spoken language translation, can help international tourists to overcome these barriers.</p><p>Signs are everywhere in our lives. A sign is an object that suggests the presence of a fact. It can be a displayed structure bearing letters or symbols, used to identify something or advertise a place of business. It can also be a posted notice bearing a designation, direction, safety advisory, or command. In this Manuscript received <ref type="bibr">June 27, 2002</ref>; revised <ref type="bibr">July 7, 2003</ref>. This work was supported in part by DARPA under TIDES project. The associate editor coordinating the review of this manuscript and approving it for publication was Dr. Jianying Hu.</p><p>X. Chen, J. Yang, and A. Waibel are with the School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, 15213 USA (e-mail: xlchen@cs.cmu.edu; yang@cs.cmu.edu; waibel@cs.cmu.edu).</p><p>J. Zhang is with the Mobile Technologies, LLC, Pittsburgh, PA, 15213 USA (e-mail: jingzhang@mobytrans.com).</p><p>Digital Object Identifier 10.1109/TIP. <ref type="bibr">2003</ref>.819223</p><p>research we focus on detecting and recognizing text on signs. An automatic sign translation system utilizes a video camera to capture an image containing signs, detects signs in the image, recognizes signs, and translates results of sign recognition into a target language. Automatic detection and recognition of text from natural scenes are prerequisites for an automatic sign translation task. Automatic detection and recognition of text from natural scenes is a very difficult task. The primary challenge lies in the variety of text: it can vary in font, size, orientation, and position. Text can also be blurred from motion or occluded by other objects. As signs exist in three-dimensional space, text on signs in scene images can be distorted by slant, tilt, and shape of objects on which they are found <ref type="bibr" target="#b17">[18]</ref>. In addition to the horizontal left-to-right orientation, other orientations include vertical, circularly wrapped around another object, slanted, sometimes with the characters tapering (as in a distinct angle away from the camera), and even mixed orientations within the same text area, such as text on a T-shirt or wrinkled sign. Although many commercial OCR systems work well on high quality scanned documents under a controlled environment, they have a much higher error rate for sign recognition tasks because of low quality images.</p><p>In this paper, we propose an approach for automatic detection and recognition of text from natural scenes. The proposed approach embeds multiresolution and multiscale edge detection, adaptive searching, color analysis, and affine rectification in sign detection. Compared with the existing text detection algorithms, this new framework can better handle the dynamics of text detection in natural scenes. Instead of using only binary information as most other OCR systems, we extract features from the intensity image directly, and avoid potentially losing information during the binarization processing, which is irreversible. We propose a local intensity normalization method to effectively handle luminance variations of the captured character image. We then utilize a Gabor transform to obtain local features and an LDA method for feature selection. We have successfully applied the proposed approach to a Chinese-English sign translation system, which can automatically detect and recognize Chinese signs captured from a camera, and translate the recognized text into English.</p><p>The rest of this paper is organized as follows: In Section II, we discuss problems in automatic detection and recognition of signs with text, the related work, and the approach to the problems. In Section III, we present the method for text detection, including multiresolution and multiscale edge detection, adaptive searching, color analysis, affine rectification, and layout analysis technologies. In Section IV, we introduce the intensitybased OCR for Chinese character recognition. We describe a local normalization algorithm for enhancing robustness against lighting variations. In Section V, we present some experiments on the proposed approach and evaluation results. Section VI concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PROBLEM DESCRIPTION</head><p>In this research, we are interested in automatic detection and recognition of signs with text. The application scenario is as follows. A user uses a camera to capture an image or a sequence of images. The sign detection algorithm automatically detects various signs in the scene and then provides information about the location of signs within an image and some of their attributes, such as sign color distributions, sign shapes, etc. The detected regions are then fed into the recognition module for classification.</p><p>The work is related to existing research in text detection from general backgrounds <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b27">[28]</ref>, video OCR <ref type="bibr" target="#b19">[20]</ref>, and recognition of text on special objects such as license plates and containers. Mullot et al. reported early attempts on container and car license plate recognition in 1991 <ref type="bibr" target="#b16">[17]</ref>. Some other researchers published their efforts on container text detection and recognition later <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b11">[12]</ref>. The latest work on container recognition can be found in Kumano et al. <ref type="bibr" target="#b10">[11]</ref>. In those applications, bar code and passive radio based technology could be a good substitution for the vision-based technology. In recent years, some researchers have attempted to deal with the sign recognition and translation with video and photography. The early work was focused mainly on the concept itself and required manual selection of the area containing the sign <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b28">[29]</ref>. The recent attempts have moved toward automatic detection and recognition of signs from natural scenes <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b32">[33]</ref>.</p><p>"Video OCR" was motivated by digital library and visual information retrieval tasks. The text in video, especially in the form of subtitles, provides such meaningful information that video OCR has become an important tool in video labeling and retrieving. The text in a video stream can be a part of the scene, or come from computer-generated text, which is overlaid on the image (e.g., captions in broadcast news programs). In such a text detection and recognition task, an image sequence provides a lot of useful information that can be used to detect text and enhance the image's resolution <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b26">[27]</ref>. Compared with video OCR tasks, text detection from natural scenes faces more challenges. Non-professional equipment usually makes the image poorer than that of other video OCR tasks, such as detecting captions, which are unified when they are generated. The photographer or videographer's movement can also cause unstable input images. In addition, real-time requirement and limited resources, such as the applications in a palm-size PDA (Personal Digital Assistant), have increased challenges in algorithm implementation.</p><p>Two different methods, area analysis and edge analysis, have been successfully used for detecting text in an image. Area analysis is based on analyzing certain features in an area, e.g., texture and color analysis <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b27">[28]</ref>. Discrete cosine transform (DCT) and wavelet transform are widely used for area analysis <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>. A major advantage of the DCT area analysis method is that DCT coefficients can be obtained directly from a JPEG or MPEG image, while on the other hand the wavelet transform can provide more stable features compared with DCT method. A disadvantage of the area-based methods is that they are sensitive to lighting and scale changes. They often fail to detect text if the size is too large or too small. The other method, edge analysis, is based on more stable edge features <ref type="bibr" target="#b33">[34]</ref>, and is thus more suitable for text detection from natural scenes. However, we have to pay special attention to filtering out "noise," because noises can add extra edges. Usually, a statistical classifier or neural network is applied to make these decisions in the area-based method, and a syntactic classifier is used for the edge-based method. In order to detect different sizes of text, the multiresolution/multiscale method has been used in text detection algorithms <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b27">[28]</ref>.</p><p>OCR is one of the most successful subsets of pattern recognition. For clearly segmented printed materials, state-of-the-art techniques offer virtually error-free OCR for several important alphabetic systems including Latin, Greek, Cyrillic, Hebrew, and their variants. However, when the size of the character set in the language is large, such as in the Chinese or Korean writing systems, or the characters are not separated from one another, such as in Arabic or Deva-nagari print, the error rates of OCR systems are still far from that of human readers, and the gap is exacerbated when the quality of the text image is compromised, for example by a camcorder or camera. The image captured from a camera in a natural environment will be rich in noise while the image obtained from a scanner in the classic OCR can provide high resolution under controllable lighting conditions. In order to address these challenges, we need more robust features for classification. Although most current OCR systems, including commercial systems, use binary features, we believe that intensity-based features are more reliable for our task. Insufficient resolution is a major problem for an OCR system using a camera as its input device. Intensity-based OCR, first introduced by Pavlidis <ref type="bibr" target="#b18">[19]</ref>, can avoid information loss during binarization, which is irretrievable in subsequent procedure(s). The disadvantage of the intensity-based method is that it is computationally more expensive.</p><p>We have successfully applied the proposed methods in developing a Chinese sign translation system <ref type="bibr" target="#b30">[31]</ref>. We choose Chinese for several reasons. First, Chinese is one of the major languages of the world and quite different from European languages. Second, a western tourist might face a serious language barrier in China because English is not commonly used there. Third, statistics shows that more people will visit China in the future than ever before. Finally, technologies developed for Chinese sign translation can be easily extended to other languages. In the rest of the paper, we will use some examples of Chinese signs to illustrate the recognition technologies, and we also use some English and Arabic signs to test the multiple linguistic detection capability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. TEXT DETECTION</head><p>Fig. <ref type="figure" target="#fig_0">1</ref> illustrates some images of text signs. It is a challenging problem for an automatic sign detection system to detect text   from these images because of affine deformations, highlights, shadows, and specularity. We have to deal with these variations. Table <ref type="table" target="#tab_0">I</ref> lists the possible effects on text in an image when lighting, orientation, location, and view angle change, where orientation is the angle between the normal of a sign and optical axis of the camera, and the view angle is view scope. In general, a long focus lens has a narrow view angle and a short focus lens has a wide view angle. An auto white-balanced camera will cause additional changes.</p><p>To work around these changes in an image, we use a hierarchical detection framework that embeds multiresolution and multiscale edge detection, adaptive searching, color analysis, and affine rectification algorithms. We combine multiresolution and multiscale edge detection technique to effectively detect text in different sizes. We employ a Gaussian mixture model (GMM) to represent background and foreground, and perform color segmentation in selected color spaces. We use affine rectification to recover deformation of the text regions caused by an inappropriate camera view angle. After affine rectification for each sign region in the image, we perform text detection again in rectified regions within the image to refine detection results. This scheme is shown in Fig. <ref type="figure" target="#fig_1">2</ref>. We will discuss the detection method in more detail in the following sub-sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Detection of Candidate Text</head><p>Normally, intensity of an image is a major information source for text detection, but it is sensitive to lighting variations. On the other hand, the gradient of the intensity (edge) is less sensitive to lighting changes. Therefore, we use edge-based features in the coarse detection phase. Keeping in mind the purposes of signs, we assume the following:</p><p>1) The text is designed with high contrast to its background in both color and intensity images. 2) Each character is composed of one or several connected regions.</p><p>3) The characters in the same context have almost the same size in most of the cases, especially for Chinese. 4) The characters in the same context have almost the same foreground and background patterns. Based on these assumptions, the main idea of the detection algorithm for coarse detection is as follows:</p><p>A multiscale Laplacian of Gaussian (LOG) edge detector is used to obtain the edge set. The properties of the edge set associated with each edge patch, such as size, intensity, mean, and variance, within the surrounding rectangle, are then calculated. Some edge patches will be excluded from further consideration based on certain criteria applied to the properties, and the rest will be passed to a recursive procedure. The procedure attempts to merge adjoining edge patches with similar properties and re-calculate the properties recursively until no update can be made. A similar idea for candidate search has been reported in <ref type="bibr" target="#b34">[35]</ref>. We have enhanced the algorithm by adding more criteria for edge candidate filtering and using a pyramid structure to handle various variations in our implementation."</p><p>With LOG, we can obtain enhanced correspondences on different edge scales by using a suitable deviation . Since characters in the same context share some common properties, we can use them to analyze the layout and affine parameters, and refine detection results. Color distribution of the foreground and background is one such important property.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Color Modeling</head><p>Signs are designed for humans to view at a distance. Therefore they have highly distinguishable colors on their foregrounds and backgrounds, and also a high intensity contrast in their gray scale images. This property seems to make it easy to segment text and to describe characters using marginal distributions in a color space. However, it is almost impossible to obtain uniform color distributions of the foreground and background because of lighting sources, shadows, dirt, etc. In this research, we use a GMM to characterize color distributions of the foreground and background of a sign, and more specifically, for each character, we use the following model:</p><p>(1) where is the color distribution of the background, and is the color distribution of the foreground. In a hieroglyphic system, e.g., Chinese, a character with more strokes has a higher percentage in its foreground than that of a character with fewer strokes. Furthermore, since transitions between foreground and background are similar for a fixed size character with different font styles, a character with bold font has less variance in its foreground than the same character without bold, because a bold font character occupies a relatively larger area in its foreground compared to a normal font character. The background is the opposite: a bold font character has a larger variance in its background than a normal font character. Therefore, in (1), provides a cue on the complexity of the character, indicates the contrast for a color space invariant to the lighting condition, and , yields the font style. Thus, we can describe the character with (</p><p>). Based on the fact that signs are designed for human perception, their foregrounds and backgrounds should be separable using one marginal distribution in a color space under an ideal lighting condition. If there exist multiple lighting sources and shadows, contrasts of foreground and background might change significantly across the entire sign. Therefore, we model the distribution of each character separately rather than the entire sign as a whole. Most cameras use an RGB color space; other color spaces, such as XYZ, YUV, YCrCb, HSI, etc., can be easily converted into the RGB space via linear or nonlinear color conversion, or vice versa <ref type="bibr" target="#b4">[5]</ref>. However, the RGB space is not necessarily the best one for representing colors for machine perception. As we know, a triple in the RGB space can represent not only color but also brightness. If the corresponding elements in two points, and , are proportional, they will have the same color but different brightness. Each space has its special properties. For example, HSI space can better handle lighting changes than RGB space. Equation (2) gives the relation between RGB and HSI <ref type="bibr" target="#b4">[5]</ref>. We normalize HSI within the range of <ref type="bibr">[0,</ref><ref type="bibr">255]</ref>  </p><p>Fig. <ref type="figure" target="#fig_2">3</ref> contains the histograms of two pairs of Chinese characters from two signs in our sign database. It can be observed from Fig. <ref type="figure" target="#fig_2">3</ref> that at least one of the components can be used for distinguishing the foreground and background if the lighting is uniformly distributed and the surface is smooth, e.g., the first line in Fig. <ref type="figure" target="#fig_2">3</ref>. Otherwise and even may fail in segmentation. For example the character in the second line of Fig. <ref type="figure" target="#fig_2">3</ref> can only be well segmented by component because of the highlight. The characters in lines three and four can also only be segmented by component because the original sign was carved on a rough surface of a stone. The disadvantage of using is that the hue of a pixel likely diffuses to its adjoined pixels, but it works for most of the cases except a black sign on a white background or vice versa, in which cases,  <ref type="figure" target="#fig_2">3</ref> component can be used to identify the situations, and then component is used for segmentation.</p><p>Table II lists the GMM parameters of the characters in Fig. <ref type="figure" target="#fig_2">3</ref>. It is obvious that the GMM parameters can reflect how easily each component can be segmented and described in terms of character properties. Thus, we can define each component's confidence as <ref type="bibr" target="#b2">(3)</ref>. Intuitively, a component with one or more sign characters should have a high confidence if it has an obvious two-class distribution, which means the distance between the two classes is large, and the variances of each class is small <ref type="bibr" target="#b2">(3)</ref> where is the confidence for component (</p><p>). The higher the value , the greater the confidence of the corresponding component.</p><p>These GMM parameters are used for layout analysis and estimation of affine parameters. They can also be used for character conversion from color to intensity image. We apply an expectation maximization (EM) algorithm to estimate these GMM parameters. To differentiate background and foreground, we enlarge the boundary of the character by 2 pixels on each side and calculate the color distribution of the region between the original boundary and the enlarged boundary. This distribution should be the same or similar to the background distribution. We then can determine the background distribution in the GMM by comparing distributions in the GMM to this distribution. The confidence can be used for measuring the performance of each subspace in segmentation. Fig. <ref type="figure" target="#fig_3">4</ref> shows some examples of segmentation using different components. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Layout Analysis and Affine Parameter Estimation</head><p>The objective of layout analysis is to align characters in an optimal way, so that characters that belong to the same context will be grouped together. A text layout has two cluster features: intrinsic and extrinsic features. The intrinsic features are those which do not change with the camera position and the extrinsic features are those ones which change with the camera position. The intrinsic features includes font style, color, and contrast; the extrinsic features include character size, sign shape, etc. Both the intrinsic and extrinsic features can provide cues for layout analysis. Moreover, the extrinsic features can also provide some information on the affine transform that occurred while snapping the image.</p><p>The text on signs is usually aligned to form a row-based or column-based structure. Even if a sign is deformed (as by an affine transform), the text on the sign might still be in a line. However, the line may be tilted to a certain degree, which will cause problems in layout analysis and further recognition. However, the text characters which are in the same context should have a similar size. Therefore, text alignment will provide some cues to recover deformation of the sign if there exists an affine transform for the text. The threshold for Algorithm 1 needs to be selected experimentally. For our Chinese sign database, the value was set as 0.6.</p><p>The affine parameters can be obtained from each layout region when the region includes multiple characters. We can use two different heuristics for this: one is the size change of the characters and the other is the parallel hypothesis of the characters, provided that all characters in the same layout are in the same text plane.</p><p>There are at least two ways to obtain the spatial parallel lines within each sign frame from a given image:</p><p>1) If the sign is inside a rectangle frame, we should be able to extend the background area to the boundary of the frame.</p><p>If the boundary of the sign region is within the image scope, we will use lines to fit it; otherwise we will try the next method. If we can fit the boundary with four lines, we assume that they are two pairs of parallel lines, which will be used for estimation of affine parameters. Some examples are shown in Section III-D. This method assumes the risk that the frame is not a rectangle but just a general trapezium. Although this seldom happens, we can detect it using the cue from the next step. 2) If the boundary of the sign is outside of the image scope or not four lines that can be found to fit the boundary, it is still possible to fit parallel lines from text only. We start from fitting the corner of the surrounding rectangle of the characters along the winner direction obtained from Algorithm 1. We select the corner with higher average edge intensity between upper-left and upper-right corners to fit the upper line, and do the same with the lower line.</p><p>If we can get more than two aligned anear lines of text with similar properties, such as size, contrast, color distribution, etc., we can get the second pair of parallel lines using the similar method. However, a sign is usually a short phrase or sentence. Therefore, it might be difficult to get the second pair of parallel lines. If that is the case, we can use the left boundary of the left-most character and right boundary of the right-most character in the same text row for estimating the second pair of parallel lines. Compared with the parameter estimation from the rectangle frame, this method is less accurate in the vertical direction, but our experiments indicate that the error has little effect on detection rate and recognition accuracy in most cases. In Section V, we show some examples in which part of boundary of the sign is outside the image, and we have to use the text itself to estimate affine parameters. 2) Affine Parameter Estimation: Several methods exist for obtaining affine parameters from texture <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b25">[26]</ref>. However, texture based methods require rich texture in the region of the estimation. Texture in a sign region is sometimes not rich enough for those algorithms. In this research, we use a nontexture based method as discussed below.</p><p>As shown in Fig. <ref type="figure" target="#fig_5">5</ref>, suppose that we have the following two lines , associated with text in the image plane, which are mapped from the spatial parallel lines and (4)</p><p>If and are not parallel in the image plane, and the vanish (intersection) point is , then we can get the normalized spatial direction of and as <ref type="bibr" target="#b4">(5)</ref> where is the focal length of the camera, and can be obtained from calibration. We also assume that the focal length is much smaller than the distance of the object from the camera. If the two lines and are parallel in the image, we can get the normalized spatial direction of and as <ref type="bibr" target="#b5">(6)</ref> where we must keep ; otherwise, the direction of this vector needs to be inverted.</p><p>Assume that the second pair of spatial parallel lines can be obtained from the following equation: <ref type="bibr" target="#b6">(7)</ref> As in (4), we can obtain the normalized spatial direction from (8) when two projected lines are not parallel in the image plane, and the vanish point is <ref type="bibr" target="#b7">(8)</ref> Otherwise, we can obtain the direction from ( <ref type="formula">9</ref>) when the projected lines in the image plane are parallel <ref type="bibr" target="#b8">(9)</ref> where must hold, otherwise, the direction of the vector must be inverted. Then, we calculate the normal of the text plane as <ref type="bibr" target="#b9">(10)</ref> Since there is no guarantee is orthotropic to the other two vectors, we need to perform the following conversion: <ref type="bibr" target="#b10">(11)</ref> This will guarantee that all three vectors are orthotropic, and we can then use them in the subsequent affine rectification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Affine Rectification</head><p>It is possible to reconstruct a front view of the sign if we know the normal of the sign plane under the camera coordinate system. Fig. <ref type="figure" target="#fig_6">6</ref> depicts an image in four coordinate systems:</p><p>1) The camera coordinate system, , is the basic coordinate system.</p><p>2) The text plane coordinate system, , applies the text plane as plane, and uses , and as its axes. The origin of the system can be selected randomly on the plane, and is located at under the coordinate system. However, it is desirable that we select a point at a character that is as close to the origin of as possible.</p><p>3) The ideal text plane,</p><p>, is located at the same origin as but uses the vectors (1 0 0), (0 1 0), and (0 0 1) as its axes. 4) The image coordinate system is a 2-D coordinate system while the other three are 3-D coordinate systems. The mapping from a point in the text plane to a point in the image coordinate system can be written as <ref type="bibr" target="#b11">(12)</ref> where, <ref type="bibr" target="#b12">(13)</ref> In order to reconstruct a front view of the text plane, we can use an affine rectification <ref type="bibr" target="#b13">(14)</ref> Considering that the text plane is on the plane, we have . Since the origin of both and is mapped to the point in the image plane, we have <ref type="bibr" target="#b14">(15)</ref> From ( <ref type="formula">12</ref>) to <ref type="bibr" target="#b14">(15)</ref>, we obtain ( <ref type="formula">16</ref>) Equation ( <ref type="formula">16</ref>) can be used to restore the front view of the text from an affined text image. A proper interpolation should be applied because it is not a one-to-one mapping for the digitalized image. We use a B-spline interpolation in our current implementation. To avoid additional blur in the interpolation, all edge pixels are interpolated only along the edge direction while the other points are done by surface interpolations. Fig. <ref type="figure" target="#fig_7">7</ref> contains some examples of the restored mapping from deformed images. Fig. <ref type="figure" target="#fig_7">7</ref>(a) shows the affined signs. Fig. <ref type="figure" target="#fig_7">7(b</ref>) and (c) show the ones recovered with B-spline interpolations using different reference origins in the text plane. In these two examples, the rectangle boundary of the sign is used estimating affine parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. INTENSITY-BASED OCR</head><p>A sign can contain graphic or text content. In this research, we focus on the text signs only. As mentioned in Section II, OCR for the characters captured from natural scenes faces more challenges than that of document analysis. For a traditional document analysis task, a scanner with a stable embedded lighting system is used to obtain high quality images, which are then easily binarized. For a sign recognition task, however, because the sign image is captured by a camera from natural scenes under various lighting conditions, the signal to noise ratio (SNR) is much lower. If we use binary features for OCR, we cannot guarantee effectively removing noises before the binarization processing. Although by carefully selecting color spaces we can reduce noises to a certain degree, we have no way to distinguish noises and useful information within the image. The binarization processing will weigh noises and useful information the same. Furthermore, the segmentation of foreground and background cannot be perfect because of noises. Fig. <ref type="figure" target="#fig_8">8</ref> illustrates some examples from both binary and intensity images, where the color space is carefully selected and hue is used to obtain both binary and intensity images. It is obvious that the binary images are much noisier, while the intensity images have less of a problem separating the background and foreground. Thus intensity images keep all the information essential to the decision making stage. Wang and Pavlidis showed the advantages of direct feature extraction from gray scale image for OCR <ref type="bibr" target="#b23">[24]</ref>. Our experiments also indicate that the intensity based OCR has advantages over binary OCR for images with low SNR. To avoid irretrievably losing information during the binarization processing, we use the intensity character image directly for feature extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Preprocessing</head><p>The characters captured by a camera from natural scenes vary in size, font style, color, and contrast. Furthermore, a character may vary with an affine deformation if the optical axis is not perpendicular to the character plane (see Section III). We use a scaling algorithm to normalize the size of the character images from different signs. Color variation does not cause a major problem in the OCR phase because we don't use a color image directly. In fact, signs are usually designed with high contrast in both color and gray scale images. We can seldom find a sign using pure colors in both foreground and background, e.g., red characters on a green background. Even in that case, we can deal with it using HSI color space as discussed in Section III-B.</p><p>Our intensity based OCR uses a gray scale image as its input. We convert a color image into a gray scale image before we further process it. Gray scale sign images come in two forms: bright characters on a dark background or dark characters on a bright background. We can resolve these two into a single case by inverting the foreground and background if the text is darker than its background. To determine whether the text is darker or brighter, we apply a method similar to the one described in Section III-B, but only in gray scale (single component).</p><p>Lighting sources in natural scenes cause another variation in sign recognition. For example, the sun can cast a highlight point on a sign, and the location of the point will change with time. In addition, many other factors, such as multiple lighting sources and the reflective properties of the surface, will cause uneven intensity distribution of the foreground and background. More specially, the intensity distribution of all strokes within a text region changes in a large dynamic scope: some have obvious contrast, some not, and some are highlighted while others may be dark. We utilize a localized intensity normalization method before feature extraction to reduce intensity distribution changes. The localized intensity normalization is intended to leave the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Feature Extraction</head><p>We use a Gabor wavelet for feature extraction. Because of its superior mathematic properties, Gabor wavelet has been widely used for data compression <ref type="bibr" target="#b20">[21]</ref>, face recognition <ref type="bibr" target="#b24">[25]</ref>, texture analysis <ref type="bibr" target="#b15">[16]</ref>, recognition <ref type="bibr" target="#b3">[4]</ref> and other image processing tasks in recent years. In OCR applications, Gabor wavelet has been applied to binary images <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref> and recently applied to a video stream <ref type="bibr" target="#b31">[32]</ref>. Yoshimura and his colleagues even report using Gabor for feature extraction and linear vector quantization (LVQ) for feature selection from a video stream.</p><p>Gabor wavelet is a sinusoidal plane wave with a particular frequency and orientation, modulated by a Gaussian envelope. It can characterize a spatial frequency structure in the image while preserving information about spatial relations, and is therefore suitable for extracting orientation-dependent frequency contents from patterns. A complex-valued 2-D Gabor function modulated by a Gaussian envelope is defined as follows: <ref type="bibr" target="#b16">(17)</ref> where The parameter is the deviation of the Gaussian envelope and and are the wavelength and orientation of the Gabor function, respectively. Fig. <ref type="figure" target="#fig_10">10</ref> illustrates frequency responses for the Gabor filters in four orientations (0 , 45 , 90 , and 135 ).</p><p>For a given pixel at with intensity in an image, its Gabor feature can be treated as a convolution <ref type="bibr" target="#b17">(18)</ref> Suppose that frequencies and orientations are used to extract a Gabor feature. We can have a vector of complex coefficients for each position. We call this vector a jet <ref type="bibr" target="#b9">[10]</ref>, which is used to represent the position of local features. In our application, we divide a character into 7 7 grids as shown in Fig. <ref type="figure" target="#fig_11">11</ref>, which results in a dimension feature vector for a character.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Feature Transforms and Recognition</head><p>We would like to reduce the number of dimensions of feature vectors because they are computationally expensive and because  not all of them are effective for recognition. LVQ and LDA are two common tools for dimension reduction. We use LDA in this research because it can be used not only for dimension reduction, but also for feature optimization. LDA is a method used to find a transform that can maximize the between-class scatter matrix and minimize the within-class scatter matrix simultaneously, as in ( <ref type="formula">19</ref>) <ref type="bibr" target="#b18">(19)</ref> The new space, , is then the most discriminative space. Feature vector in the original feature space is projected to this new space and yields the new feature using <ref type="bibr" target="#b19">(20)</ref>, which can be used for classification <ref type="bibr" target="#b19">(20)</ref> We obtain the reference vectors ( ) for all 3755 characters in GB-2312-80, a standard for commonly used Chinese words, from the training set. The training set includes six different fonts: SongTi, HeiTi, KaiTi, LiShu, YaoTi, and YouYuan. All of these character images are gray scale. For each type of font, we generate the standard character images from the standard font library, and then pass them through a low-pass filter that produces the gray scale image. We have compared two types of training sets: single sample per font and multiple samples per font. We generate the multiple samples (here, we generate 25 samples) by adding noise and skew deformations to the character images. We calculate the average Gabor vector ( ) for each type of font from these images. The within-class and between-class scatter matrixes are <ref type="bibr" target="#b20">(21)</ref> Fig. <ref type="figure" target="#fig_1">12</ref>. Examples of sign detection. <ref type="bibr" target="#b21">(22)</ref> where , and . Finally, we obtain the discriminative space during the training phase. Usually, a hierarchical classifier can be used for a large class set if the feature is ordered or partly ordered. Considering that the LDA has already provided the maximum discriminant space and that the feature from LDA is not an ordered feature, we must use the nearest classifier in recognition; several distance measurements are compared, including street block distance, Eulerian distance, and vector angle distance. We find that the vector angle distance is significantly better than others. The accuracy reaches 92.46% for our testing set when we use the vector angle distance, while it can only reach 82.61% when we use the Eulerian distance.</p><p>We have compared the results from two types of training sets: single sample per font and multiple samples per font. The experimental results indicate that these give similar performance in most of the cases, although sometimes the result from the training set of single sample per font is better than that of multiple samples per font. Our explanation is that the single sample method is a center-based method, and the standard image from the font library is the perfect center of all possible images of the character.  Note: is the angle between normal of text plane and -axis of the camera.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS AND DISCUSSION</head><p>We have performed several experiments to evaluate the proposed detection and recognition methods. Figs. 12 and 13 contain examples of automatic sign detection, where the rectangles give detection results. These multilingual signs include Chinese, English and Arabic. Signs captured from a camera might have some distortion because of nonfrontal viewing angle, which can reduce detection rate and recognition accuracy. The affine rectification can correct such distortion. Fig. <ref type="figure" target="#fig_12">13</ref> shows two sets of examples of text detection, where the left side of each group [(a) and (c)] is the detection result without affine rectification, and the right side of each group [(b) and (d)] is the result with affine rectification. Observe that only partial text can be detected from distorted images because the affine deformation makes the text tilt along a certain direction and vary in size. For the corresponding right parts, the text has almost the same size and is aligned in nearly horizontally after affine rectification; now the detection algorithm can successfully find all the text regions. In Fig. <ref type="figure" target="#fig_12">13</ref>(a), the boundary of the sign frame is completely within the image, and frame information can be used for parameter estimation. In Fig. <ref type="figure" target="#fig_12">13(c</ref>), however, part of the sign frame is outside the image. Therefore, we use the parallel lines from text itself to estimate the affine parameters. In this case, we will need to extend the area for rectification beyond just the text area.</p><p>We further tested the improvement of OCR accuracy with affine rectification. Experiments were performed on Chinese signs. Fifty images, which had different amounts of deformation, were selected from our Chinese sign database (a total of more than 2000 images). These images include a total 251 characters to be recognized. The results are listed in Table <ref type="table" target="#tab_3">III</ref>. Without affine rectification, the recognition rate decreases rapidly as the angle between normal of the text plane and -axis of camera coordinate system increases. The restoration can improve the results significantly, especially when the angle ranges from 30 to 50 . If the angle is larger than that, the characters cannot be recovered properly due to the amount of the information lost in the imaging procedure.</p><p>We also use our sign detection algorithms in other text detection tasks such as text in video and text on paper. Fig. <ref type="figure" target="#fig_4">14</ref> shows some examples of the video images from Microsoft Research Asia <ref type="bibr" target="#b7">[8]</ref>. Fig. <ref type="figure" target="#fig_5">15</ref> shows some examples of the text on book covers and a CD box cover.</p><p>We have evaluated the proposed approach on a Chinese sign recognition task. The current training set includes all level 1 characters in Chinese national standard character set GB2312-80 (a total of 3755 different characters). We also randomly selected 1630 character images from our sign library, which contains more than 8000 characters in more than 2000 sign images from natural scenes, to form the testing set. The different characters in the testing set cover roughly 1/5 of the level 1 Chinese characters. Fig. <ref type="figure" target="#fig_6">16</ref> shows some examples from the testing set. The recognition accuracy is 92.46%. This is an encouraging result because the character images in the testing set are captured from natural scenes, with variations in fonts, lighting conditions, rotation, and even affine deformation. For further verification of the robustness of the proposed approach against noises, we add the zero mean Gaussian noise to each character image in the testing set. For a given pixel in the character image whose intensity is , we have <ref type="bibr" target="#b22">(23)</ref> where Parameter represents the intensity of the noise. Fig. <ref type="figure" target="#fig_7">17</ref> illustrates the impacts on a Chinese character from testing set when we add noise , , and , respectively.</p><p>The top curve of Fig. <ref type="figure" target="#fig_8">18</ref> shows the recognition rate of 1630 characters in testing set when we add different intensities of Gaussian noise. Compared to the original testing set, this is only about a 1% decrease in recognition rate when 10% noise is added and about 6.5% decrease when 20% noise is added. This figure also illustrates the effectiveness of local intensity normalization; the recognition accuracy is about 24% to 28% higher than without intensity normalization. We also performed similar experiments on the binarization method. In the binary experiments, we also use the one simple per font training set. The results indicate that the accuracy is slightly lower for the original images, and that the accuracy decreases much faster than that of the intensity-based method when noise level is increased. Fig. <ref type="figure" target="#fig_15">19</ref> illustrates the process from detection to recognition. Fig. <ref type="figure" target="#fig_15">19(a</ref>) is the original input image, and (b) is the combination of detected candidates from two different resolutions. Fig. <ref type="figure" target="#fig_15">19(c</ref>) is the detection result without affine rectification. It can be seen that only part of the characters are detected in (c). Similar to the example in Fig. <ref type="figure" target="#fig_12">13</ref>(c), no rectangle sign frame can be found within the image. Therefore, the lines are fit from text. Note that the higher edge intensity corners are used for fitting in Fig. <ref type="figure" target="#fig_15">19(d</ref>). The detected result with affine rectification is in Fig. <ref type="figure" target="#fig_15">19(e)</ref>, where all characters are detected and all Chinese characters are recognized correctly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, we present an approach for automatic detection and recognition of signs for application to sign translation. The proposed approach can robustly detect and recognize text signs from natural scenes. The edge-based method is used for coarse detection accompanied by a multiresolution scheme for different sign sizes. By combining the layout analysis and affine rectification, we obtain a markedly improved detection rate. An intensity-based OCR method is employed for sign recognition, where we apply a localized normalization method to enhance feature extraction, and use Gabor transform to extract features and LDA to select and reduce the dimensionality of the feature space. The method has been applied to a Chinese OCR task, which includes all level 1 characters in Chinese national standard character set GB2312-80 (total 3755 different characters). The detection and recognition approach has been implemented on different platforms, such as desktop, laptop, and palm-size PDA with an application of a Chinese sign translation system, which can automatically detect Chinese text input from a camera, recognize the text, and translate the recognized text into English. We are combining the traditional 2-D based recognition method with 3-D preprocessing technology to better understand the text in a 3-D world.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Some examples of signs captured from natural scenes.</figDesc><graphic coords="3,44.82,62.28,237.60,180.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Multiresolution text detection with affine rectification schema.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Color distributions of different fonts, backgrounds, and lighting conditions.</figDesc><graphic coords="4,41.88,62.28,509.56,306.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Examples of segmentation using different components.</figDesc><graphic coords="5,315.24,62.30,222.87,257.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>1 ) 4 .</head><label>14</label><figDesc>Layout Analysis: The layout analysis algorithm is listed in Algorithm 1: Algorithm 1 (Layout Analysis) Input: The candidates of text regions and the associated attribute sets: and , where , , and are the center, height, and width of the surrounding rectangle respectively, and , , , are the mean vectors and covariance matrices corresponding to the foreground and background color distributions; Output: The Layout regions . transform to find all possible line segments (), which are fitted by the centers of . These line segments will form several compatible sets:Only one of the will be the winner of the candidate layout direction. The winner should satisfy the following crilength of all line segments exclusive the shortest one in and . If , where is a threshold, will be the winner of this layout region. If , will be the winner of this layout region. Otherwise, Removing all layouts with only some small candidate text regions. 5. Finding the vertices of the surrounding quadrangle for each layout region.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Two spatial parallel lines and their imaging in the image plane.</figDesc><graphic coords="7,86.34,62.28,154.56,99.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Illustration of four coordinate systems.</figDesc><graphic coords="7,306.90,62.28,239.52,114.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Restoration from the affined images.</figDesc><graphic coords="8,46.32,62.28,237.60,280.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Comparison of (a), (c) binary characters and (b), (d) gray scale characters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Example of localized intensity normalization.</figDesc><graphic coords="9,309.12,62.28,235.08,167.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Gabor filters in four different orientations.</figDesc><graphic coords="9,400.50,263.42,52.36,52.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Regions for feature extraction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Examples of sign detection with distortion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 14 .Fig. 15 .</head><label>1415</label><figDesc>Fig. 14. Examples of text detection from video images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 16 .Fig. 17 .Fig. 18 .</head><label>161718</label><figDesc>Fig. 16. Various characters from signs in natural scenes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 19 .</head><label>19</label><figDesc>Fig. 19. Example of automatic sign detection and recognition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I EFFECTS</head><label>I</label><figDesc>ON TEXT IN AN IMAGE CAUSED BY VARIOUS IMAGING CONDITIONS</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II GMM</head><label>II</label><figDesc>PARAMETERS FOR THE CHARACTERS IN FIG.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III COMPARING</head><label>III</label><figDesc>OF RECOGNITION RESULT WITH RESPECT TO AFFINE RECTIFICATION</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank the associate editor and anonymous reviewers for their invaluable comments and suggestions led to a great improvement on this manuscript. They also would like to thank Z. Sloane for his proofreading this manuscript.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Xilin Chen (M'98) received the B.S., M.S., and Ph.D. degrees in computer science from Harbin Institute of Technology, <ref type="bibr">China, in 1988</ref><ref type="bibr">China, in , 1991</ref><ref type="bibr">China, in , and 1994, respectively. , respectively.</ref> He has been a Professor at Harbin Institute of Technology since 1999. He has been a Visiting Scholar at Carnegie Mellon University, Pittsburgh, PA, since 2001. His research interests are image processing, pattern recognition, computer vision and multimodal interface.</p><p>Dr. Chen has served as a program committee member for several international and national conferences. He is a Professor of computer science at CMU and the University of Karlsruhe, Germany. He directs the Interactive Systems Laboratories (http://www.is.cs.cmu.edu) at both universities with research emphasis in speech recognition, handwriting recognition, language processing, speech translation, machine learning, and multimodal and multimedia interfaces. At CMU, he also serves as Associate Director of the Language Technology Institute and as Director of the Language Technology Ph.D. program. He was one of the founding members of CMU's Human Computer Interaction Institute (HCII) and continues on its core faculty. He was one of the founders of C-STAR, the international consortium for speech translation research and served as its Chairman from 1998 to 2000. His team developed the JANUS speech translation system, the JANUS speech recognition toolkit, and a number of multimodal systems including the meeting room, the Genoa Meeting recognizer, and meeting browser. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Image recognition for shipping container tracking and</title>
		<author>
			<persName><forename type="first">E</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Imag</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="62" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Document restoration using 3D shape: A general deskewing algorithm for arbitrarily warped documents</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Seales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="367" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Character extraction of license plates from video</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="502" to="507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Handwritten Chinese character recognition using spatial Gabor filters and self-organizing feature maps</title>
		<author>
			<persName><forename type="first">D</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICIP</title>
		<meeting>ICIP</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="940" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Woods</surname></persName>
		</author>
		<title level="m">Digital Image Processing</title>
		<meeting><address><addrLine>Reading, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Addison-Wesley</publisher>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Recognition of handprinted Chinese characters using Gabor features</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hamamoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Uchimura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Masamizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tomita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd ICDAR</title>
		<meeting>3rd ICDAR</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="819" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">High performance Chinese OCR based on Gabor features, discriminative feature extraction and model training</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1517" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Automatic performance evaluation for video text detection</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 6th ICDAR</title>
		<meeting>6th ICDAR<address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-10-13">Sept. 10-13, 2001</date>
			<biblScope unit="page" from="545" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automatic text location in images and video frames</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2055" to="2076" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Representation of local geometry in the visual system</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Koenderink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Van Doorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biol. Cybern</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="367" to="375" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Development of container identification mark recognition system</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kumano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Miyamoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tamagawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ikeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Inst. Electron., Inform., Commun. Eng. D-II</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1073" to="1083" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automatic extraction of characters in complex scene images</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Pattern Recognit. Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="67" to="82" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automatic text detection and tracking in digital video</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="147" to="156" />
			<date type="published" when="2000-01">Jan. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automatic text recognition for video indexing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Lienhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Multimedia 96</title>
		<meeting>ACM Multimedia 96</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Text extraction in MPEG compressed video for content-based indexing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 15th ICPR</title>
		<meeting>15th ICPR</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="409" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gabor filter-based edge detection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mehrotra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Namuduri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ranganathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1479" to="1494" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatic extraction methods of container identity number and registration plates of cars</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mullot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Bourdon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Courtellemont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Labiche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecourtier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Industrial Electronics, Control, Instrumentation</title>
		<meeting>Int. Conf. Industrial Electronics, Control, Instrumentation</meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="volume">2591</biblScope>
			<biblScope unit="page" from="1739" to="1744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Recognition of characters in scene images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ohya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Akamatsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal Machine Intell</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="214" to="220" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Recognition of printed text under realistic conditions</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pavlidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="317" to="326" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Video OCR for digital news archives</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">K</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Workshop on Content-Based Access of Image and Video Database</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Wavelet transforms and neural networks for compression and recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Szu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Telfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="695" to="708" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Translation camera</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Okada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Takeda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 14th ICPR</title>
		<meeting>14th ICPR</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="613" to="617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Shape Recovering of a Solid of Resolution from Apparent Distortions of Patterns</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<idno>CMU-CS-80-133</idno>
	</analytic>
	<monogr>
		<title level="j">PA</title>
		<imprint>
			<date type="published" when="1984">1984</date>
		</imprint>
		<respStmt>
			<orgName>Carnegie-Mellon Univ., Pittsburgh</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Direct gray-scale extraction of features for character recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pavlidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1053" to="1067" />
			<date type="published" when="1993-10">Oct. 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Face recognition by elastic bunch graph match</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wiskott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Fellous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kruger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Malsburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="764" to="768" />
			<date type="published" when="1997-07">July 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Recovering surface shape and orientation from texture</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Witkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="17" to="45" />
			<date type="published" when="1981-08">Aug. 1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A Robust algorithm for text extraction in color video</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. on Multimedia and Expo</title>
		<meeting>IEEE Int. Conf. on Multimedia and Expo</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="797" to="800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">TextFinder: An automatic system to detect</title>
		<author>
			<persName><forename type="first">V</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Riseman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1224" to="1229" />
			<date type="published" when="1999-11">Nov. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Smart sight: A tourist assistant system</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Denecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Dig. Papers 3rd Int. Symp. Wearable Computers</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="73" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Toward automatic sign translation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Human Language Technology Conf</title>
		<meeting>Human Language Technology Conf<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-03">Mar. 2001</date>
			<biblScope unit="page" from="269" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Automatic detection and translation of text from natural scenes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2101" to="2104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Grayscale character recognition by Gabor jets projection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yoshimura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Etoh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kondo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yokoya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 15th ICPR</title>
		<meeting>15th ICPR</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="335" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A robust approach for recognition of text embedded in natural scenes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hanneman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 16th ICPR</title>
		<meeting>16th ICPR</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="204" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Locating text in complex color images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Karu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1523" to="1536" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">An adaptive algorithm for text detection from natural scenes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR&apos;-1</title>
		<meeting>CVPR&apos;-1</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="84" to="89" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
