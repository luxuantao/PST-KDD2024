<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Prediction-Based Power Oversubscription in Cloud Platforms</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-10-29">29 Oct 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Alok</forename><surname>Kumbhare</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Reza</forename><surname>Azimi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ioannis</forename><surname>Manousakis</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Anand</forename><surname>Bonde</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Felipe</forename><surname>Frujeri</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nithish</forename><surname>Mahalingam</surname></persName>
						</author>
						<author>
							<persName><roleName>Seyyed</roleName><forename type="first">Pulkit</forename><surname>Misra</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ahmad</forename><surname>Javadi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Bianca</forename><surname>Schroeder</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Marcus</forename><surname>Fontoura</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ricardo</forename><surname>Bianchini</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Microsoft</forename><surname>Azure</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Microsoft</forename><surname>Research</surname></persName>
						</author>
						<title level="a" type="main">Prediction-Based Power Oversubscription in Cloud Platforms</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-10-29">29 Oct 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2010.15388v1[cs.DC]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Prior work has used power capping to shave the rare power peaks and add more servers to the datacenter, thereby oversubscribing its resources and lowering capital costs. This works well when the workloads and their server placements are known. Unfortunately, these factors are unknown in public clouds, forcing providers to limit the oversubscription so that performance is never impacted.</p><p>In this paper, we argue that providers can use predictions of workload performance criticality and virtual machine (VM) resource utilization to increase oversubscription. This poses many challenges, such as identifying the performance-critical workloads from black-box VMs, creating support for criticalityaware power management, and increasing oversubscription while limiting the impact of capping. We address these challenges for the hardware and software infrastructures of Microsoft Azure. The results show that we enable a 2× increase in oversubscription with minimum impact to critical workloads.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Motivation. Large Internet companies continue building datacenters to meet the increasing demand for their services. Each datacenter costs hundreds of millions of dollars to build. Power plays a key role in datacenter design, build out, IT capacity deployment, and physical infrastructure cost.</p><p>The power delivery infrastructure forms a hierarchy of devices that supply power to different subsets of the deployed IT capacity at the bottom level. Each device includes a circuit breaker to prevent damage to the IT infrastructure in the event of a power overdraw. When a breaker trips, the hardware downstream loses power, causing a partial blackout.</p><p>To avoid tripping breakers, designers conservatively provision power for each server based on either its maximum nameplate power or its peak draw while running a powerhungry benchmark, such as SPEC Power <ref type="bibr" target="#b18">[19]</ref>. The maximum number of servers is then the available power (or breaker limit) divided by the per-server provisioned value. This provisioning leads to massive power under-utilization. As the IT demand increases, it also requires building new datacenters even when there are available resources (space, cooling, networking) in existing ones, thus incurring huge unnecessary capital costs.</p><p>To improve efficiency and avoid these costs, prior work has proposed combining power capping and oversubscription <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b31">[32]</ref>. The idea is to leverage actual server utilization and statistical multiplexing across workloads to oversubscribe the * Azimi, Javadi, and Schroeder were at Microsoft Research during this work. delivery infrastructure by adding more servers to the datacenter, while ensuring that the power draw remains below the breakers' limits. This is achieved by continuously monitoring the power draw at each level and using power capping (via CPU voltage/frequency and memory bandwidth throttling), when necessary. As throttling impacts performance, these approaches carefully define which workloads can be throttled and by how much. For example, Facebook's Dynamo relies on predefined workload priority groups, and throttles each server based on the priority of the workload it runs <ref type="bibr" target="#b31">[32]</ref>. Using Dynamo, Facebook was able to oversubscribe its datacenters by 8%, lowering costs by hundreds of millions of dollars.</p><p>This oversubscription approach works well when workloads and their server placements are known. Unfortunately, public cloud platforms violate these assumptions. First, each server runs many VMs, each with its workload, performance, and power characteristics. Hence, throttling the entire server would impact performance-critical (e.g., interactive services) and non-critical (e.g., batch) workloads alike. Second, VMs dynamically arrive and depart from each server, producing varying mixes of characteristics and preventing predefined server groupings or priorities. Third, each VM must be treated as a black box, as customers are often reluctant to accept deep inspection of their VMs. Thus, the platform does not know which VMs are performance-critical and which ones are not. For these reasons, oversubscription in public clouds has been limited so that performance is never impacted. Our work. In this paper, we argue that cloud providers can increase oversubscription substantially by carefully scheduling VMs and managing power, based on predictions of workload performance criticality and VM CPU utilization. Our insight is that there are many non-critical workloads (e.g., batch jobs) that can tolerate a slightly higher rate of capping events and/or deeper throttling; the capping of performance-critical workloads must be controlled more tightly. Using predictions to identify these workloads and place them carefully across the datacenter provides the power slack and criticality-awareness needed to increase oversubscription.</p><p>Accurately predicting (black-box) workload criticality is itself a challenge. Prior work <ref type="bibr" target="#b5">[6]</ref> associated a diurnal utilization pattern with user interactivity and the critical need for high performance. It inferred criticality using the Fast Fourier Transform (FFT) algorithm on the CPU utilization time series for each workload. Here, we present a more accurate and robust pattern-matching algorithm, and a machine learning (ML) model that uses the algorithm's output during training. Summary. We make the following main contributions:</p><p>1) An algorithm and ML model for predicting performance criticality, and a model for predicting VM utilization.</p><p>2) A VM placement policy that uses these predictions to minimize the number of capping events and their impact.</p><p>3) A per-VM power capping system that uses predictions of criticality to protect certain VMs.</p><p>4) A strategy that leverages the contributions above to increase the amount of oversubscription.</p><p>5) Implementation and results for the infrastructure of Azure, showing large potential cost savings.</p><p>6) Lessons from production deployment of our work. Though we build upon Azure's infrastructure, our conceptual contributions (e.g., predicting criticality; using predictions in VM placement, power budget enforcement, and oversubscription) apply directly to any cloud platform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND AND CONTEXT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Typical power delivery and server deployment</head><p>At the top of the power delivery hierarchy, the electrical grid provides power to a sub-station that is backed up by a generator. An Uninterruptible Power Supply (UPS) unit provides battery backup while the generator is starting up. The UPS feeds one power distribution unit (PDU) per row of servers. Each row PDU supplies power to several rack PDUs, each of which feeds a few server chassis. Each chassis contains a few power supplies (PSUs) and dozens of blade servers. <ref type="foot" target="#foot_0">1</ref> A PDU trips its circuit breaker when the power draw exceeds the rated value (budget) for the unit, causing a power outage.</p><p>Designers deploy servers so that breakers never trip, leading to wasted resources (chiefly space, cooling, and networking). Combining hierarchical power capping and oversubscription enables more capacity to be deployed and better utilizes resources <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b31">[32]</ref>. For example, if designers find that the historical per-row power draw is consistently lower than the row PDU budget, they can "borrow" power from each row to add more rows (until they run out of row space) under the UPS budget. The extra rows oversubscribe the power at the UPS level. The servers downstream from the oversubscribed PDUs/UPS must be power-capped, whenever they are about to draw power that has already been borrowed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Azure's existing power capping mechanisms</head><p>For clarity and ease of experimentation, in this paper we explore power budget enforcement at the chassis level.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> shows Azure's current mechanisms. Each chassis contains a manager that exposes the management interface. To enable capping, Azure first sets the total chassis-level power budget at the chassis manager and PSUs (step 1), and each individual blade power cap in its board management chip (BMC) (step 2). Azure sets each blade budget to its even share of the chassis budget.</p><p>Under normal operation, no capping takes place, i.e. each server is free to draw more power than its even share, as long as the total chassis draw is below the chassis budget. The PSUs monitor the chassis draw (step 3) and alert the blades directly when the chassis budget is about to be exceeded (step 4). Upon an alert, the blade power must be brought below its evenshare cap. (Uneven caps are infeasible due to the overhead of dynamically reapportioning and reinstalling blade budgets.) The BMC splits the cap evenly across its sockets and uses Intel's Running Average Power Limit (RAPL) <ref type="bibr" target="#b6">[7]</ref> to lower the blade power (step 5). RAPL throttles the entire socket (slowing down all cores equally) and memory using a feedback loop until the cap is respected. Typically, RAPL brings the power below the cap in less than 2 seconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Azure's existing VM scheduler</head><p>A cloud platform first routes an arriving VM to a server cluster. Within each cluster, a VM scheduler is responsible for placing the VM on a server. The scheduler uses heuristics to tightly pack VMs, considering the incoming VM's multiple resource requirements and each server's available resources.</p><p>Azure's scheduler implements its heuristics as two sets of rules. It first applies constraint rules (e.g., does the server have enough resources for the VM?) to filter invalid servers, and then applies preference rules each of which orders the candidate servers based on a preferred metric (e.g., a packing score derived from available resources). It then weights each candidate based on its order on the preference list for each rule and the rule's pre-defined weight. Finally, it picks a server with the highest aggregate weight for allocation. No rules currently consider power draws or capping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Azure's existing ML system</head><p>To integrate predictions into VM scheduling in practice, we target Resource Central <ref type="bibr" target="#b5">[6]</ref>, the existing ML and predictionserving system in Azure. The system provides a REST service for clients (in our case the VM scheduler) to query for predictions. It can receive input features (e.g., user, VM size, guest OS) that are known at deployment time from the scheduler, execute one of our ML models (criticality or CPU utilization), and respond with the prediction and a confidence score. Model training is done in the background, e.g. once a day.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PREDICTION-BASED OVERSUBSCRIPTION</head><p>Cloud providers provision servers conservatively, and have full-server capping mechanisms and capping-oblivious VM schedulers. We propose to provision servers more aggressively by making the infrastructure smarter and finer-grained via VM behavior predictions. Key challenges include having to create or adapt many components (e.g., scheduler, chassis manager) to the predictions, while ensuring that the tradeoff between oversubscription and performance is tightly controlled.</p><p>Next, we overview our design and then detail its main components. Then, we describe our strategy for provisioning servers to balance cost savings and performance impact.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overview</head><p>Figure <ref type="figure" target="#fig_1">2</ref> overviews our system and its operation, showing the existing and new/modified components in different colors. We modify the VM scheduler to use predictions of VM performance criticality and resource utilization. We implement our ML models so they can be managed and served by the existing ML and prediction-serving system. We modify the chassis manager to query the chassis power draw and interact with our new per-VM power controller. The controller manages its server's power draw during a capping event.</p><p>In detail, a request to deploy a set of VMs arrives at our VM scheduler (arrow #1). The scheduler then queries the ML system for predictions of workload performance criticality and resource utilization (#2). Using these predictions, it decides on which servers to place the VMs (#3). After selecting each VM's placement, the scheduler tags the VM with its predicted workload type and instructs the destination server to create it. Each chassis manager frequently polls its local PSUs to determine whether the power draw for the chassis crosses a threshold just below the chassis budget. (This threshold enables the controller to perform per-VM capping and hopefully avoid needing full-server RAPL.) When this is the case, the manager alerts the controller of each server in the chassis (#4).</p><p>Upon receiving the alert, the controller at each server manages the server's (even) share of the chassis budget across the local VMs based on their workload types. It does this by first throttling the CPU cores used by non-performance-critical VMs (#5). Throttling these VMs may be enough to keep the power below the chassis budget, and protect the performancecritical VMs. If it is not enough, the PSUs alert the servers' BMCs (#6), which will then use RAPL as a last resort to lower the chassis power below the budget (#7). Limiting impact on non-critical VMs. Though we protect critical VMs from throttling, we limit the performance impact on non-critical VMs in three ways. First, for long-term server provisioning, our oversubscription strategy carefully selects chassis budgets to limit the number of capping events and their severity to predefined acceptable values (e.g., no more than 1% events for non-critical VMs, each lowering the core frequency to no less than 75% of the maximum). Second, for medium-term management, our scheduler places VMs seeking to minimize the number of events and their severity. Finally, in the shortest term, our per-VM controller increases the core frequency of non-critical VMs as soon as possible.</p><p>Treating external VMs as performance-critical. Providers who prefer to treat all paying customers the same can easily do so by assuming that all external VMs are performance-critical; only internal VMs (e.g., running the provider's own managed services) would be classified into the two criticality types. We explore this assumption in Section IV-F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Predicting VM criticality and utilization</head><p>Our approach depends on predicting VM performance criticality and utilization at arrival time, i.e. just before the VMs are deployed. We train supervised ML models to produce these predictions based on historical VM arrival data and telemetry that was collected after those VMs were deployed. Since VMs are black boxes, our telemetry consists of CPU utilization data only, as deep inspection is not an option. Inferring criticality. Predicting criticality requires a method to determine the VM labels, i.e. whether the workload of each VM is performance-critical or not, before we can train a model. As in prior work <ref type="bibr" target="#b5">[6]</ref>, we consider a workload critical if it is user-facing, i.e. a human is interacting with the workload (e.g., front-end webservers, backend databases), and less critical otherwise (e.g., batch, development and testing workloads). As user-facing workloads exhibit utilization patterns that repeat daily (e.g., high during the day, low at night), the problem reduces to identifying VMs whose time series of CPU utilizations exhibit 24-hour periods <ref type="bibr" target="#b5">[6]</ref>.</p><p>Obviously, some background VMs may exhibit 24-hour periods. This is not a problem as we seek to be conservative (i.e., it is fine to classify a non-user-facing workload as user-facing, but not vice-versa). Moreover, some daily batch jobs have strict deadlines, so classifying them as user-facing correctly reflects their needs. Importantly, focusing on the CPU utilization signal works well even when the CPU is not the dominant resource, as the CPU is always a good proxy for periodicity (e.g., network-bound interactive workloads exhibit more CPU activity during the day than at night).</p><p>We considered but discarded other approaches for inferring whether a VM's workload is user-facing. For example, observing whether a VM exchanges messages does not work because many non-user-facing workloads communicate externally (e.g., to bring data in for batch processing). Identifying periodicity. There are statistical methods for identifying periods in time series, such as FFT or the autocorrelation function (ACF). For example, <ref type="bibr" target="#b5">[6]</ref> assumes a workload is user-facing if the FFT indicates a 24-hour period. We evaluated ACF and FFT methods on 840 workloads on Azure.</p><p>Surprisingly, we find that both methods lead to frequent misclassifications. We identify three culprits:</p><p>1) The diurnal patterns in user-facing workloads often have significant noise and interruptions. For example, we observe user-facing workloads with clear 24-hour periods for many days, interrupted by a period of constant or random load, causing them to be mis-classified as non-user-facing.</p><p>2) The diurnal patterns often exhibit increasing/decreasing trends (e.g., the workload becomes more popular over time), and varying magnitudes of peaks/valleys across days. These effects cause some user-facing workloads to be mis-classified.</p><p>3) There are many machine-generated workloads with periods of 1 hour, 4 hours, 6 hours or other divisors of 24 hours, which therefore also have 24-hour periods, leading to machinegenerated workloads that are mis-classified as user-facing.</p><p>Part of the problem is that ACF and FFT are very general tools with different goals, e.g. decomposing a signal for compact representation and capturing general correlations, not solutions for our specific problem of 24-hour periods. Criticality algorithm. Thus, we devise a new algorithm that is more robust and targeted at our specific problem. Our idea is to extract from a VM's utilization time series a template for a typical 24-hour period and then check how well this template captures most days in the series. We design the template extraction and comparison to be robust to noise and interruptions to deal with issue #1 above. We pre-process the data using methods from time series analysis to address #2. To deal with #3, we extract templates for shorter periods (8 and 12 hours) and ensure that the 24-hour template is the best fit. These periods subsume the other short periods.</p><p>More precisely, the input to our pattern-matching algorithm is the average CPU utilization for each 30-minute interval over 5 weekdays. (Shorter workloads cannot be classified and should be conservatively assumed user-facing.) For each utilization time series, the algorithm does the following:</p><p>1) It de-trends and normalizes the time series, so that all days exhibit utilizations within the same rough range. De-trending scales each utilization based on the mean of the previous 24 hours, whereas normalization divides each utilization by the standard deviation of the whole time series.</p><p>2) It extracts the 24-hour template by identifying, for each time of the day (in 30-minute chunks), its "typical" utilization computed as the median of all utilizations in the pre-processed series that were reported at this time of the day.</p><p>3) It overlays the template over the pre-processed series for each day and computes the average deviation for each utilization, after excluding the 20% largest deviations. If the scores are close to 0, the workload is likely to be userfacing. Ultimately, it classifies a time series as user-facing, if its Compare8 value is lower than a threshold (Section IV-B). Criticality prediction. The algorithm above produces labels that we use to train an ML model to classify arriving VMs as user-facing or non-user-facing. Specifically, we train a Random Forest using the labels and many features (pertaining to the arriving VM and its cloud subscription) available at arrival time: the percentage of user-facing VMs in the subscription, the percentage of VMs that lived at least 7 days in the subscription, the total number of VMs in the subscription, the percentage of VMs in each CPU utilization bucket, the averages of the VMs' average and 95th-percentile CPU utilizations in the subscription, the arriving VM's number of cores and memory size, and the arriving VM's type. Utilization prediction. For utilization predictions, we train a two-stage model to predict 95th-percentile VM CPU utilization based on labels produced by previous VM executions (actual 95th-percentile utilizations over the VMs' lifetimes) and the same VM features we use in the criticality model. Since predicting utilization exactly is hard, our model predicts it into 4 buckets: 0%-25%, 26%-50%, and so on. The first stage of the model is a Random Forest that predicts whether or not the 95th-percentile utilization is above 50%. In the second stage, we have a Random Forest for buckets 1-2 and another for buckets 3-4. We train these latter forests with just the VMs we can predict with high-confidence (≥ 60%) in the first stage.</p><p>We experimented with single-stage models, but they did not produce accurate predictions with enough confidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Modified VM scheduler</head><p>Our ability to increase oversubscription and the efficacy of the per-VM power controller depend on the placement of VMs in each cluster. Better placements have a balanced distribution of power draws across the different chassis to reduce the number of capping events (Goal #1); and a balanced distribution of cap-able power (drawn by non-user-facing VM cores) across servers, so the controller can lower the power during an event without affecting critical VMs (Goal #2). The scheduler must remain effective at packing VMs while minimizing the number of deployment failures (Goal #3).</p><p>Given these goals, we modify Azure's VM scheduler to become criticality-and utilization-aware, using predictions at VM arrival time. Our policy is a preference rule that sorts the feasible servers based on a "score". Each server's score considers the predicted 95th-percentile CPU utilization of the VMs already placed in the same chassis (targets Goal #1), and the predicted criticality and 95th-percentile CPU utilization of the VMs already placed on the same server (targets Goal #2). The policy only considers CPU utilization because the CPUs are the dominant source of dynamic power in Azure's servers. As Section IV-E shows, our policy does not degrade the packing of VMs onto servers, nor does it increase the percentage of VM deployment failures (achieves Goal #3).</p><p>Algorithm 1 shows our rule (SortCandidates) and two supporting routines. We show the predictions with the Predict-edWorkloadType and PredictedP95Util superscripts. The rule ultimately computes the score for each candidate server (line #6). The higher the score, the more preferable the server. The score is a function of how preferable the server (line #5) and</p><formula xml:id="formula_0">Algorithm 1 Criticality-&amp; utilization-aware VM placement 1: function SORTCANDIDATES(V , ζ)</formula><p>V : VM to be placed, ζ: list of candidate servers 2:</p><p>ω ← V P redictedW orkloadT ype 3:</p><formula xml:id="formula_1">for c i in ζ do 4: κ i ← SCORECHASSIS(c i .Chassis) 5: η i ← SCORESERVER(ω, c i ) 6: c i .score ← α × κ i + (1 − α) × η i 7: return ζ.SORTDESC(c i .score) 8: function SCORECHASSIS(C) 9:</formula><p>for n i in C.Servers do 10:</p><p>for v j in n V M s i do 11:</p><formula xml:id="formula_2">ρ P eak ← ρ P eak + v P redictedP 95U til j × v cores j 12: ρ M ax ← ρ M ax + n cores i 13: return 1 − ρ P eak ρ M ax 14: function SCORESERVER(ω, N ) 15: for v i in N U F V M s do 16: γ U F ← γ U F + v P redictedP 95U til i × v cores i 17: for v i in N N U F V M s do 18: γ N U F ← γ N U F + v P redictedP 95U til i × v cores i 19: if ω = U F then 20: return 1 2 × 1 + γ N U F −γ U F N cores 21: else 22: return 1 2 × 1 + γ U F −γ N U F</formula><p>N cores its chassis (line #4) are for the VM to be placed. Both server and chassis intermediate scores range from 0 to 1. We weight the intermediate scores to give them differentiated importance. We select the best value for the α weight in Section IV-E. Function ScoreChassis computes the chassis score for a candidate server by conservatively estimating its aggregate chassis CPU utilization, i.e. assuming all VMs scheduled to the chassis are at their individual 95th-percentile utilization at the same time. This value is the sum of the predicted 95th-percentile utilizations for the VMs scheduled to the chassis, divided by the maximum core utilization (#cores in chassis×100%). This ratio is proportional to utilization. We subtract it from 1, so that higher values are better (line #13).</p><p>Function ScoreServer scores a candidate server differently depending on the type of VM that is being deployed. First, it sums up the predicted 95th-percentile utilizations of the user-facing VMs (lines #15-16) and non-user-facing VMs (lines #17-18) independently. When a user-facing VM is being deployed, we compute how much more utilized the non-userfacing VMs on the server are than the user-facing ones. We do the reverse for a non-user-facing VM. The reversal is the key to balancing the cap-able power across servers. Adding 1 and dividing by 2 ensure that the resulting score will be between 0 and 1 (lines #20 and #22), while higher values are better.</p><p>The algorithm takes only 7 milliseconds, which is negligible as VM creation takes on the order of seconds <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Per-VM power capping controller</head><p>Azure's existing in-server capping system uses RAPL and out-of-band (i.e., independently of software on the server) PSU alerts to throttle all cores equally. This approach is simple and safe, but may strongly impact the user-facing workloads. On the other hand, a software-only approach that runs in-band (i.e., as a platform-level service) can be aware of workload types, poll the PSUs, and use per-core DVFS to cap nonuser-facing VMs when necessary. However, this solution is vulnerable to software defects and communication issues.</p><p>To achieve safety and flexibility, we use a hybrid solution. Our modified chassis manager polls the PSUs every 200 milliseconds and alerts our in-band controller when the chassis power draw is close to the chassis budget. The controller uses per-core DVFS to cap the cores running non-user-facing VMs. To account for (1) high power draws that may occur between polls or (2) the inability of the controller to bring power below the budget, we use the out-of-band mechanisms as a backup.</p><p>To power-manage the cores per-VM, we use the coregrouping feature of the hypervisor (e.g., cpupools in Xen, cpugroups in Hyper-V) to split the logical cores into two classes: high-priority and low-priority. We assign the userfacing VMs and the I/O VM (e.g., Domain0 in Xen, Root VM in Hyper-V) to the high-priority class, and the non-userfacing VMs to the low-priority one. The hypervisor ensures that any threads associated with the given VM are scheduled only on the logical cores of its group.</p><p>Upon receiving an alert from the chassis manager, the per-VM power controller compares the server's power draw to its budget. If the current draw is higher than the budget, the controller immediately lowers the frequency of the lowpriority cores to the minimum p-state, i.e. half of the maximum frequency; the lowering of the frequency may entail a lower voltage as well. The goal is to lower the server's power draw as quickly as possible without affecting the important workloads. However, this large frequency reduction may overshoot the needed power reduction. To reduce the impact on the non-userfacing VMs, the controller then enters a feedback loop where each iteration involves (1) checking the server power meter and (2) increasing the frequency of N low-priority cores to the next higher p-state, until the power is close to the budget. It selects the highest frequency that keeps the power below this threshold. N = 4 works well in our experiments.</p><p>It is possible that cutting the frequency of the low-priority cores in half is not enough to bring the power below the server's budget. For example, a VM placement where there are not enough non-user-facing VMs in the workload mix, nonuser-facing VMs exhibiting lower utilization than predicted, or a controller bug can cause this problem. In this case, the outof-band mechanism will kick in as backup. Though RAPL will apply to all cores indiscriminately, protection from overdraw must take precedence over performance loss.</p><p>The controller lifts the cap after some time (30 secs by default), allowing all cores to return to maximum performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Oversubscription strategy</head><p>We now describe our oversubscription strategy, which uses our capping system and placement policy, historical VM arrivals, and historical power draws, to increase server density. It relies on the algorithm below for computing an aggressive power budget for all the chassis of each hardware generation. Adapting it to find budgets for larger aggregations (e.g., rack, row) is straightforward. We refer to the uncapped, nominal core frequency as the "maximum" frequency.</p><p>To configure the algorithm, we need to select the maximum acceptable rate of capping events (e.g., #events per week) for user-facing (emax U F ) and non-user-facing (emax N U F ) VMs, and the minimum acceptable core frequency (e.g., half the maximum frequency) for user-facing (f min U F ) and nonuser-facing (f min N U F ) VMs. If we want no performance impact for user-facing VMs, we set emax U F = 0 and f min U F = maximum frequency. As we describe next, our 5-step algorithm finds the lowest chassis power budget that satisfies emax U F , emax N U F , f min U F , and f min N U F . Estimate future behaviors based on history:</p><p>1) Estimate the historical average ratio of user-facing virtual cores in the allocated cores (β). Estimate the historical average P95 utilization of virtual cores in user-facing (util U F ) and non-user-facing (util N U F ) VMs. Profile the hardware:</p><p>2) Estimate how much server power can be reduced by lowering core frequency at util U F and util N U F , given f min U F and f min N U F , respectively. This step produces two curves for power draw (one curve for each average utilization), as a function of frequency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compute power budgets based on historical draws:</head><p>3) Sort the historical chassis-level power draws (one reading per chassis per unit of time) in descending order.</p><p>4) Start from the highest power draw as the first candidate budget and progressively consider lower draws until we find P min . For each candidate power budget, we check that the rate of capping events would not exceed f max U F or f max N U F (considering the higher draws already checked), and the attainable power reduction from capping is sufficient (given β and the curves from step 2).</p><p>5) To compute the final budget, add a buffer (e.g., 10%) to the budget from step 4 to account for future variability of β or substantial increases in chassis utilization.</p><p>We can use the difference between the overall budget computed in step 5 and the provisioned power to add more servers to the datacenter. Because we protect user-facing VMs and use our VM scheduling policy, this difference is substantially larger than in prior approaches, as we show in Section IV-F. Example. Suppose we are willing to accept rates of 0.1% and 1% capping events for user-facing and non-user-facing VMs, respectively. Suppose further that, upon a capping event, we are willing to lower the core frequencies to 75% and 50% of the maximum, respectively. Now, assume we have 10000 historical chassis power draws (collected from every chassis), and that the highest draws have been 2900W, 2850W, and 2850W. We first consider 2900W. If we were to set the chassis budget to just below that value to say 2890W, there would be 1 capping event out of 10000 observations, i.e. a rate of 0.01%, and we would have to shave 10W during the event. Given the acceptable capping rates and minimum frequencies, we can operate with the data from step 1 and the curves from step 2 to determine (1) whether we could reduce power by 10W, and (2) whether there would be an impact on user-facing VMs. If we can achieve the reduction, we count 1 event out of 10000 that would affect non-user-facing VMs. If the user-facing VMs would also have to be throttled, we would count 1 event out of 10000 that would affect those VMs. Since both rates are lower than 0.1% and 1%, we can now check a budget just below 2850W, say 2840W. We repeat the process for this budget, which would lead to 3 events out of 10000, and the need to lower power by 60W (2900-2840) once and 10W (2850-2840) twice. Then the next lower draw and so on, until we violate the desired capping rates and minimum frequencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EVALUATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Methodology</head><p>Data analysis. We evaluate our criticality algorithm and ML models (Section IV-B) using standard metrics, such as precision and recall from predictions. We compute the metrics based on the entire VM workload of Azure in April 2019. Real experiments. We run experiments on the same hardware that Azure uses in production. We use a chassis with 12 servers, each containing 40 cores split into two sockets. At their nominal frequency, each server draws between 112W (idle) and 310W (100% CPU utilization). At half this frequency, each server draws from 111W to 169W.</p><p>Our single-server experiments (Section IV-C) explore the dynamic behavior of our per-VM power capping controller and resulting VM workload performance for a combination of userfacing and non-user-facing VMs and various power budgets. For comparison, we use the existing full-server capping controller (Intel's RAPL) in Azure. Our chassis-level experiments (Section IV-D) explore our system on 12 servers, including PSU alerts. For comparison, we use the existing chassis-level mechanisms in Azure (Figure <ref type="figure" target="#fig_0">1</ref>).</p><p>For both sets of experiments, we use instances of a latencycritical transaction processing application (similar to TPC-E) for the user-facing workload, and instances of a batch Hadoop computation (Terasort) for the non-user-facing workload. For the user-facing workload, we use real inputs from Azure's team responsible for it, whereas we use synthetic input data for the non-user-facing computation. Simulation. We evaluate our modified VM scheduler in simulation (Section IV-E), leveraging the same simulator that Azure uses to evaluate changes to the VM scheduler before putting them in production; our only extension is to simulate calls to the ML system. An event generator drives the simulation with a sequence of VM arrivals. For each arrival, it invokes the production scheduling algorithm (or the scheduling algorithm with the addition of our policy) for server placement decisions. Running the actual scheduler code in the simulator ensures that simulations are faithful to reality.</p><p>We simulate a cluster of 60 chassis in 20 racks. The simulator produces VM arrivals based on distributions matching Azure's load in April 2019. Table I lists the main statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Criticality algorithm and ML models</head><p>Criticality algorithm. To evaluate our pattern-matching algorithm (Section III-B), we first compare its classifications to   Thus, the algorithm can accurately classify workloads based on their Compare8 value. For a quantitative assessment, we compare it to two well-known approaches for finding periodicity in a time series, ACFs and FFTs, for the same set of workloads. For both approaches, we do the same pre-processing and disambiguate between user-facing and machine-generated workloads using the same methods as in our algorithm.</p><p>As we want to protect user-facing VMs, we must achieve high recall for this class as the recall indicates the probability of correctly identifying these VMs. Table <ref type="table" target="#tab_2">II</ref> shows the precision and recall, for two high recall targets (0.99 and 0.98) for whether a workload is user-facing. Our algorithm achieves the target recall with much higher precision than its counterparts. ML models. We now evaluate our models for Azure's entire VM workload. Table <ref type="table" target="#tab_2">III</ref> lists the percentage of predictions with confidence score higher than 60% (3rd column), and the per-bucket recalls and precisions (4th-7th columns) and the accuracy (rightmost column) for those high-confidence predictions. The VM scheduler disregards predictions with lower confidence and conservatively assumes the VM being  TABLE III: Random Forest (RF) and Gradient Boosting (GB) models recall (R), precision (P), and accuracy for high-confidence deployed will be user-facing and will exhibit 100% 95thpercentile utilization. For comparison, we show results for the equivalent Gradient Boosting (GB) models.</p><p>The table shows that our criticality model achieves 99% recall for user-facing VMs (Bucket 2), which is critical for protecting these VMs. The most important features for our model are the percentage of user-facing VMs observed in the cloud subscription, the percentage of VMs that live longer than 7 days in the subscription, and the total number of VMs in the subscription. The GB model achieves similar results.</p><p>Our utilization model also does well with good recall and precision (83-93%) for the most popular buckets (1 and 4), and good accuracy (84%) for the 73% of high-confidence predictions. Here, the most important features are the average of the VMs' 95-percentile CPU utilizations in the subscription, the average of the VMs' average CPU utilizations in the subscription, and the percentages of VMs in each CPU utilization bucket in the subscription. The GB model achieves similar accuracy, but with fewer high-confidence predictions and lower recall for the two middle (least popular) buckets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Per-VM capping controller experiments</head><p>We run experiments on a server with our user-facing application running on a VM with 20 virtual cores and our nonuser-facing application running simultaneously on another VM with 20 virtual cores. Each execution takes 10 minutes.</p><p>Figure <ref type="figure" target="#fig_2">4</ref> plots the dynamic power behaviors and core frequencies of full-server and per-VM capping with caps at 230W. In the bottom graph, we plot the lowest frequency of any non-user-facing core. The experiments have capping enabled throughout their executions. For comparison, we show the power profile of an experiment without any cap.</p><p>When unconstrained (no cap), the power significantly exceeds 250W. In contrast, full-server and per-VM capping keep the power draw below 230W. Because of the lower target of our controller (225W for the 230W cap), its power draws are slightly below those of full-server capping most of the time. The frequency curve depicts the adjustments that our controller makes to the performance of the non-user-facing VM. The steep drop to the lowest frequency occurs when the controller abruptly lowers the frequency to the minimum value when the power first exceeds the target. After that point, its feedback component smoothly increases and decreases the frequency.</p><p>Figure <ref type="figure" target="#fig_7">5</ref> shows the impact of capping during these experiments on the 95th-percentile latency of the user-facing application <ref type="bibr">(</ref> to the unconstrained performance of each application. We also include bars for capping at 250W, 240W, 220W, and 210W.</p><p>The results show that full-server capping imposes a large tail latency degradation, especially for the lower caps. When the cap is 230W, the degradation is already 18%, which is often unacceptable for user-facing applications. For lower caps, full-server capping provides even worse tail latency (35% degradation for 210W). In contrast, our controller keeps tail latency very close to the unconstrained case, until the cap is so low (210W) that it becomes impossible to protect the userfacing application and RAPL needs to engage. This positive result comes at the cost of performance loss for the non-userfacing application. While full-server capping keeps running time fairly close to the unconstrained case, our controller degrades it by 28% for the 230W cap. This is the right tradeoff, as non-user-facing workloads have looser performance needs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Chassis-level capping experiments</head><p>We now study the power draw at the chassis level, and the impact of different capping granularities (full-server vs per-VM) and VM placements. We experiment with a 12-server chassis running 36 copies of our user-facing application (each on a VM with 4 virtual cores), and 36 copies of our nonuser-facing application (each running on a VM with 6 virtual cores). In terms of VM placement, we explore two extremes: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>User-facing Non-user-facing</head><p>Full-server Per-VM Fig. <ref type="figure">6</ref>: Chassis dynamics and performance vs placement.</p><p>(1) balanced placement, where we place the user-facing and non-user-facing VMs in round-robin fashion across the servers, i.e. 3 VMs of each type on each server; and (2) imbalanced, where we segregate user-facing and non-user-facing VMs on different sets of servers. Each experiment runs for 26 minutes. Figure <ref type="figure">6</ref>(left) plots the dynamic behavior of the chassis for an overall budget of 2450W for the two capping approaches. For comparison, we also plot the no-cap case. In these experiments, we use the balanced placement as an example.</p><p>As expected, both capping granularities are able to limit the power draw to the chassis budget, whereas the no-cap experiment substantially exceeds this value. We observe the same trends under the imbalanced placement approach. The VM placement does not matter in terms of the power profiles because the capping enforcement ensures no budget violations.</p><p>However, VM placement has a large impact on application performance. Figure <ref type="figure">6</ref>(right) plots the impact of VM placement and capping granularity on the average 95th-percentile latency of the user-facing applications (4 leftmost bars) and on the average running time of the non-user-facing applications (4 rightmost bars). We normalize to the no-cap results.</p><p>Per-VM capping under a balanced placement keeps the average tail latency the same as the no-cap experiment, despite the tight 2450W budget. In contrast, per-VM capping degrades performance as much as full-server capping when the placement is imbalanced. These results show that our controller protects user-facing VMs when the VM placement allows it.</p><p>Full-server capping provides slightly better performance than per-VM capping for the non-user-facing applications. More interestingly, the results for balanced placement are slightly worse than for imbalanced placement, regardless of the capping granularity. For per-VM capping, the reason is that servers with only non-user-facing VMs need to reduce the frequency of fewer cores. For full-server capping, the reason is that servers with only non-user-facing VMs tend to have higher utilization, so a smaller reduction in frequency is enough for a large power reduction. Comparing the two rightmost bars, we see that full-server capping hurts performance slightly less than per-VM capping in the imbalanced case, as RAPL lowers frequency more slowly than our controller.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Cluster VM scheduler simulation</head><p>In the previous section, we explored extreme and manuallyproduced VM placements in controlled experiments. However, in practice, placements are determined by the VM scheduler. To evaluate our modified scheduler, we implement our VM placement policy in Azure's VM scheduler, and simulate a cluster using 30 days of VM arrivals (Section IV-A). The simulator reports four main metrics:</p><p>• Deployment failure rate: the percent of VM deployment requests rejected by the scheduler due to resource unavailability or fragmentation. This rate has a direct impact on users, so our policy should not increase it;</p><p>• Average empty server ratio: the percent of servers without any VMs in the cluster averaged over time. Empty servers can accommodate the largest VM sizes, so our policy should not decrease this ratio;</p><p>• Standard deviation of the average chassis score, i.e. 1 − (ρ P eak /ρ M ax ) (Algorithm 1, line 13), for each chassis. This metric reflects how balanced the chassis are with respect to their power loads. Lower values are better and mean better balance and fewer power capping events;</p><p>• Standard deviation of the average server score, i.e. (1/2)× (1 + (γ N U F − γ U F )/N cores ) (Algorithm 1, line 20), for each server. This metric reflects how balanced the servers are in terms of UF and NUF core 95th-percentile utilizations. Lower values mean better balance and that we are more likely to only need to cap NUF VMs. Results. Figure <ref type="figure">7</ref> shows the results for these metrics, as a function of the α weight in our policy (Algorithm 1, line 6). α = 1 means that the server score is irrelevant, whereas α = 0 means that the chassis score is irrelevant. From left to right in each graph, the "NoRule" (black) bar represents the existing scheduler; the leftmost (blue) bar in each group represents our modified scheduler using our policy and ML predictions; the next (green) bar represents the modified scheduler using oracle predictions; and the rightmost (orange) the modified scheduler using criticality predictions, but no utilization predictions.</p><p>Comparing the black and blue bars illustrates the benefit of our policy and predictions. Figures <ref type="figure">7(a</ref>) and (b) show that our modified scheduler impacts the failure rate slightly for low values of α (not at all for high values), while slightly decreasing the percentage of empty servers regardless of α. The reason is that our policy may use a few more servers in the interest of better balancing the load. In fact, Figures <ref type="figure">7(c</ref>) and (d) confirm that the load is more balanced using our policy and predictions. These latter graphs also show that the value of α is important. α = 0 produces much worse utilization balancing across chassis than other values. At the same time, α = 1 produces as poor server utilization balancing as the existing scheduler, whereas other values produce much better server balancing. These observations confirm that it is key to balance both across chassis and servers, as in our policy. α = 0.8 strikes a good compromise between the importance of these types of balancing. Impact of prediction accuracy. Comparing the blue and green bars illustrates the impact of mispredictions and predictions with low confidence. Figures <ref type="figure">7(c</ref>) and (d) show that oracle predictions produce only slightly better balancing than our real predictions for certain values of α. Impact of utilization predictions. Comparing the blue and orange bars illustrates the importance of having both criticality and utilization predictions. Clearly, it is critical to predict Fig. <ref type="figure">7</ref>: Key scheduler metrics, as a function of α.</p><p>the workload type of each VM, as we want to protect the performance of user-facing VMs during capping events. The results demonstrate that having utilization predictions is also important. The lack of such predictions degrades the balancing substantially for most values of α, and thus increases the capping rate and limits the power reduction during an event (thereby decreasing the potential for oversubscription).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Oversubscription increases</head><p>We now estimate the amount of oversubscription and dollar savings that result from our lower chassis power budgets. To do so, we translate the amount of budget we can reduce in each 60-chassis cluster into the infrastructure cost we would avoid. Oversubscription allows servers to be added to an existing datacenter (assuming space, cooling, and networking are available, as it is often the case), avoiding the cost of building a corresponding fraction of a new datacenter.</p><p>We instantiate our 5-step oversubscription strategy (Section III-E) with power telemetry from 1440 chassis over 3 months in 2018. We also use VM statistics from April 2019. Specifically, the 95th-percentile core utilizations for non-userfacing (util N U F ) and user-facing (util U F ) VMs were 44% and 65%, respectively, and the ratio of user-facing cores in the allocated cores (β) was 40%. We add a buffer of 10% to the chassis budget (step 5). For the results where providers treat external (i.e., third-party) VMs differently than internal (i.e., first-party) VMs, we adjust these parameters accordingly, while keeping the same amount of buffer.</p><p>Table <ref type="table" target="#tab_4">IV</ref> lists results for several types of provisioning: 1) "Traditional" provisioning (no oversubscription); 2) State-of-the-art full-server capping without VM insights. In this approach, the power capping events need to be rare and the throttling has to be light to prevent performance loss to user-facing VMs. To model this approach with our provisioning strategy, we use emax U F + emax N U F = 0.1%, f min U F = f min N U F = 75%;</p><p>3) Predictions-based per-VM capping and scheduling, without impact on user-facing VMs. We use emax U F = 0, f min U F = 100%, emax N U F = 1% and f min N U F = 50%; 4) Predictions-based per-VM capping and scheduling, with minimal impact on user-facing VMs. To make this approach comparable to the others, we set the overall rate of capping events at 1%. Specifically, we use emax U F = 0.1%, f min U F = 75%, emax N U F = 0.9% and f min N U F = 50%;   The state-of-the-art approach achieves 6.2% oversubscription. This amount is comparable to that (8%) achieved by Facebook <ref type="bibr" target="#b31">[32]</ref>, which knows the (single) workload that runs on each server. Public cloud platforms do not have this luxury.</p><p>In contrast, our approach (#3 and #4) can almost double the oversubscription and savings. We achieve more than 12% oversubscription with minimal impact on user-facing VMs. Assuming a datacenter campus of 128MW and an infrastructure cost of $10/W [2], 12.1% oversubscription translates into $154.9M in savings; an increase in savings of $75.5M over the state of the art. As providers can oversubscribe many campuses, the savings would be much higher in practice.</p><p>When providers prefer to treat external and internal VMs differently (approaches #5-#8), they can do so at the cost of a lower increase in oversubscription. For example, when treating all external VMs as user-facing and protecting the performance of user-facing VMs (#5), the increase in savings becomes $28.1M. At the other extreme, where we treat the premium external VMs as user-facing and allow minimal impact on user-facing VMs (#8), the increase in savings returns to $75.5M. The reason is that this provisioning approach has enough non-critical VMs, and oversubscription is limited only by the rate of capping events.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. LESSONS FROM PRODUCTION DEPLOYMENT</head><p>So far, we have deployed our per-VM capping controller and ML models in production in Azure's datacenters. Next, we discuss some of the lessons from these deployments. Hypervisor support for per-VM power capping. Our prototype controller (Section III-D) leveraged the hypervisor's core-grouping feature to manage the frequency of each VM's physical cores. In production, Azure typically prefers not to restrict a VM to a subset of cores, so we could not rely on this feature. Instead, we had to extend the hypervisor to (1) add the capability to dynamically specify the frequency for a VM, and (2) carry the frequency to whichever cores it schedules the VM on during the context switch (changing the frequency takes tens of microseconds, whereas a scheduling quantum lasts 10 milliseconds). As most VMs are small (Table <ref type="table" target="#tab_0">I</ref>), there was no need to manage frequency on a per-virtual-core basis. Additional types of throttleable VMs. Some first-party customers were concerned about the impact of per-VM capping on their non-user-facing VMs. To alleviate their concerns, we added a configurable prioritized throttling list to our system. Using the list, we first consider low priority and internal non-production VMs for throttling and throttle production (including third-party, if configured) non-user-facing VMs as a last resort, i.e. when throttling the other types is insufficient. Metrics to measure impact. Since VMs are black boxes, we cannot use any workload-specific metric to evaluate per-VM capping in production. Instead, our deployed system measures how long and how hard VMs are being capped. The data shows that our system is successful at protecting production userfacing VMs, while prioritizing the VMs that do get throttled. Increasing rack density with per-VM capping. While deploying our system, we learned that Azure was installing fewer servers per rack -28 instead of 36 -when deploying a new generation of power hungrier servers. Having fewer servers per rack reduces the probability that the rack power draw will hit the provisioned limit and cause capping using RAPL. With our per-VM capping system in place, Azure will go back to installing 36 servers per rack. This is another type of power oversubscription that per-VM capping enables. Killing VMs. Some first-party customers indicated that they would prefer their VMs to be killed rather than throttled, as their services can handle losing VMs but an unpredictable impact due to throttling is not acceptable. Under extreme power draws, killing these customers' VMs can help protect production user-facing VMs and throttle fewer non-user-facing ones. We will soon add this capability to our system. Server support for per-VM management. Our production experience has highlighted the drawbacks of managing VM power per-component (e.g., core, uncore, memory). We expect that cloud providers would prefer to raise the level of abstraction from individual components to entire VMs, even if VM power would have to approximate. This would enable advances that have been too complex for production use, such as power-aware VM placement, enforcing per-VM power limits, and making capping and killing decisions based on VM power. We are working with silicon vendors towards this end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. RELATED WORK</head><p>Our paper is the first to use ML predictions for increasing power oversubscription in public cloud platforms. Next, we discuss some of the most closely related works. Leveraging predictions. Some works predict resource demand, resource utilization, or job/task length for provisioning or scheduling purposes, e.g. <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b27">[28]</ref>. In contrast, we introduce a new algorithm and ML models for predicting workload type and high-percentile utilization, seeking to protect critical workloads from capping and place VMs in a criticality-and capping-aware manner. Server power capping. Most efforts have focused on selecting the DVFS setting required to meet a tight power budget as applications execute, e.g. <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b21">[22]</ref>- <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b33">[34]</ref>. Both modeling/optimization and feedback techniques have been used. The inputs to the selection have been either application-level metrics (e.g., request latency), low-level performance counters, or operator annotations (e.g., high priority application). Our capping controller uses per-core DVFS and feedback, so it adds to this body of work. However, it also uses predictions about the VMs' performance-criticality as its inputs. Our approach seems to be the best for a cloud platform, since criticality information and application-level metrics are typically not available, and collecting and tracking low-level counters at scale involves undesirable overhead. Cluster-wide workload placement/scheduling. Many works select workload placements to reduce performance interference or energy usage, e.g. <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b32">[33]</ref>. Unfortunately, they are often impractical for a cloud provider, relying on extensive profiling, application-level metrics, short-term load predictions, and/or aggressive resource reallocation (e.g., via live migration). Live migration is particularly problematic, as it retains contended resources, may produce traffic bursts, and may impact VM availability; it is better to place VMs where they can stay. Our scheduler uses predictions in VM placement. Unlike prior work, it reduces the number and impact of capping events, and increases power oversubscription. Datacenter oversubscription. Researchers have proposed to use statistical oversubscription, where one profiles the aggregate power draw of multiple services and deploy them to prevent correlated peaks <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b30">[31]</ref>. Our work extends these works by using predictions to place the workload, inform capping, and increase oversubscription. Our oversubscription strategy is also the first to carefully control the extent and impact of capping on important cloud workloads.</p><p>Others have studied hierarchical capping in production datacenters <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b31">[32]</ref>. Our paper focuses on chassis-level power budget enforcement to make our experimentation easier. However, our techniques extrapolate directly. For example, for row-level budget enforcement, we can place VMs across rows trying to balance rows and servers.</p><p>Finally, researchers have proposed using energy storage to shave power peaks in oversubscribed datacenters <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b17">[18]</ref>. When peaks last long, this approach may require large amounts of storage, which our work does not require. Nevertheless, the two approaches are orthogonal and can be combined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSIONS</head><p>We proposed prediction-based techniques for increasing power oversubscription in cloud platforms, while protecting important workloads. Our techniques can increase oversubscription by 2×. We discussed lessons from deploying our techniques in production. We conclude that recent advances in ML and prediction-serving systems can unleash further innovations in cloud resource provisioning and management.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Chassis-level alert-based power capping.</figDesc><graphic url="image-1.png" coords="2,311.98,50.54,251.06,124.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: System overview.</figDesc><graphic url="image-2.png" coords="3,311.98,50.54,251.06,126.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>4 )</head><label>4</label><figDesc>It repeats steps 2 and 3 to compute average deviations for 8-hour and 12-hour templates, and then computes two scores: 24-hour average deviation divided by 8-hour average deviation (called Compare8), and 24-hour average deviation divided by 12-hour average deviation (called Compare12).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>size dist. (cores) 1 (33%), 2 (27%), 4 (21%), 8 (10%), 16 (5%), 24 (3%), &gt;=32 (1%) Deployment size dist. (#VMs) 1 (39%), 2 (14%), 3-5 (16%), 6-10 (9%), 11-15 (8%), 16-25 (5%), &gt;25(9%) VM lifetime dist. (hours) 1 (52%), 2 (5%), 3-5 (10%), 6-10 (9%), 10-25(7%), 26-720 (8%), &gt;720 (9%) Workload type buckets user-facing (UF), non-user-facing (NUF) P95 utilization buckets 0-25%, 26-50%, 51-75%, 76-100% Avg UF:NUF core ratio 4:6 Avg UF and NUF P95 util 65% (bucket #3), 44% (bucket #2) # simulation days 30</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Algorithm compared to manual classification. our own manual labeling of 840 workloads on Azure.Figure 3 shows one colored dot for each workload with coordinates corresponding to its Compare8 and Compare12 values. The colors indicate whether we deem the workload clearly userfacing, possibly user-facing, clearly machine-generated, or clearly non-user-facing. The figure shows that Compare8 can separate the first two groups, which the algorithm should conservatively classify as user-facing, from the last two. A vertical bar at Compare8=0.72 gets all important workloads to the left of the bar, and the vast majority of unimportant ones to the right of it. Compare12 does not separate the classes well.Thus, the algorithm can accurately classify workloads based on their Compare8 value. For a quantitative assessment, we compare it to two well-known approaches for finding periodicity in a time series, ACFs and FFTs, for the same set of workloads. For both approaches, we do the same pre-processing and disambiguate between user-facing and machine-generated workloads using the same methods as in our algorithm.As we want to protect user-facing VMs, we must achieve high recall for this class as the recall indicates the probability of correctly identifying these VMs. TableIIshows the precision and recall, for two high recall targets (0.99 and 0.98) for whether a workload is user-facing. Our algorithm achieves the target recall with much higher precision than its counterparts. ML models. We now evaluate our models for Azure's entire VM workload. TableIIIlists the percentage of predictions with confidence score higher than 60% (3rd column), and the per-bucket recalls and precisions (4th-7th columns) and the accuracy (rightmost column) for those high-confidence predictions. The VM scheduler disregards predictions with lower confidence and conservatively assumes the VM being</figDesc><graphic url="image-3.png" coords="7,335.34,191.98,200.85,144.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 :Fig. 5 :</head><label>45</label><figDesc>Fig. 4: Server power dynamics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>5 )</head><label>5</label><figDesc>Predictions-based per-VM capping and scheduling for internal VMs only (all external VMs considered user-facing), without impact on user-facing VMs; 6) Predictions-based per-VM capping and scheduling for internal VMs only (all external VMs considered user-facing), with minimal impact on user-facing VMs; 7) Predictions-based per-VM capping and scheduling for internal and non-premium external VMs, without impact on user-facing VMs; and 8) Predictions-based per-VM capping and scheduling for internal and non-premium external VMs, with minimal impact on user-facing VMs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Simulation parameters.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>Pattern-matching vs ACF vs FFT.</figDesc><table><row><cell cols="3">Prediction Model % High</cell><cell>Bucket 1</cell><cell>Bucket 2</cell><cell>Bucket 3</cell><cell>Bucket 4</cell><cell>Accuracy</cell></row><row><cell></cell><cell></cell><cell>Conf.</cell><cell>R -P</cell><cell>R -P</cell><cell>R -P</cell><cell>R -P</cell></row><row><cell>Criticality</cell><cell>GB</cell><cell>99%</cell><cell cols="2">67% -77% 99% -99%</cell><cell>NA</cell><cell>NA</cell><cell>98%</cell></row><row><cell></cell><cell>RF</cell><cell>99%</cell><cell cols="2">69% -78% 99% -99%</cell><cell>NA</cell><cell>NA</cell><cell>98%</cell></row><row><cell>P95 util</cell><cell>GB</cell><cell>68%</cell><cell cols="4">95% -85% 47% -77% 51% -79% 94% -80%</cell><cell>82%</cell></row><row><cell></cell><cell>RF</cell><cell>73%</cell><cell cols="4">93% -87% 61% -76% 65% -81% 92% -83%</cell><cell>84%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV :</head><label>IV</label><figDesc>Comparison between provisioning approaches.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">We refer to "blades" and "servers" interchangeably throughout the paper.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Benchmarking vm startup time in the cloud</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">I</forename><surname>Abrita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sarker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Abrar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Adnan</surname></persName>
		</author>
		<editor>Benchmarking, Measuring, and Optimizing, C. Zheng and J. Zhan</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="53" to="64" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The datacenter as a computer: Designing warehouse-scale machines</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Barroso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Hölzle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Computer Architecture</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Energy Efficient Resource Management in Virtualized Cloud Data Centers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Beloglazov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Buyya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Cluster, Cloud and Grid Computing</title>
				<meeting>the 10th International Conference on Cluster, Cloud and Grid Computing</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dynamic Placement of Virtual Machines for Managing SLA Violations</title>
		<author>
			<persName><forename type="first">N</forename><surname>Bobroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kochut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Beaty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Integrated Network Management</title>
				<meeting>the International Symposium on Integrated Network Management</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Workload Prediction Using ARIMA Model and its Impact on Cloud Applications&apos; QoS</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>Calheiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Masoumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Buyya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cloud Computing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Resource central: Understanding and predicting workloads for improved resource management in large cloud platforms</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cortez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bonde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Muzio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Russinovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fontoura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bianchini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Symposium on Operating Systems Principles</title>
				<meeting>the 26th Symposium on Operating Systems Principles</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">RAPL: Memory Power Estimation And Capping</title>
		<author>
			<persName><forename type="first">H</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gorbatov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">R</forename><surname>Hanebutte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Khanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Low-Power Electronics and Design</title>
				<meeting>the International Symposium on Low-Power Electronics and Design</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Quasar: Resource-Efficient and QoS-Aware Cluster Management</title>
		<author>
			<persName><forename type="first">C</forename><surname>Delimitrou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Architectural Support for Programming Languages and Operating Systems</title>
				<meeting>the 19th International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Power provisioning for a warehouse-sized computer</title>
		<author>
			<persName><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-D</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Barroso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th Annual International Symposium on Computer Architecture</title>
				<meeting>the 34th Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Press: Predictive Elastic Resource Scaling for Cloud Systems</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wilkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Network and Service Management</title>
				<meeting>the International Conference on Network and Service Management</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Statistical profiling-based techniques for effective power provisioning in data centers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Govindan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Urgaonkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sivasubramaniam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Baldini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th ACM European conference on Computer systems</title>
				<meeting>the 4th ACM European conference on Computer systems</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Leveraging stored energy for handling power emergencies in aggressively provisioned datacenters</title>
		<author>
			<persName><forename type="first">S</forename><surname>Govindan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sivasubramaniam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Urgaonkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">SIGARCH Comput. Archit. News</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Per-Application Power Delivery</title>
		<author>
			<persName><forename type="first">A</forename><surname>Guliani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Swift</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Eurosys</title>
				<meeting>Eurosys</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Smoothoperator: Reducing power fragmentation and improving power utilization in large-scale datacenters</title>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Architectural Support for Programming Languages and Operating Systems</title>
				<meeting>the 23rd International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An analysis of efficient multi-core global power management policies: Maximizing performance for a given power budget</title>
		<author>
			<persName><forename type="first">C</forename><surname>Isci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Buyuktosunoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Cher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martonosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th annual IEEE/ACM international symposium on microarchitecture</title>
				<meeting>the 39th annual IEEE/ACM international symposium on microarchitecture</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Empirical Prediction Models for Adaptive Resource Provisioning in the Cloud</title>
		<author>
			<persName><forename type="first">S</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Keung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Generation Computer Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Workload Characterization and Prediction in the Cloud: A Multiple Time Series Approach</title>
		<author>
			<persName><forename type="first">A</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Anerousis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Network and Service Management</title>
				<meeting>the International Conference on Network and Service Management</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Managing distributed ups energy for effective power capping in data centers</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kontorinis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Aksanli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sampson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Homayoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pettis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Rosing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International Symposium on Computer Architecture</title>
				<meeting>the 39th International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Identifying shades of green: The specpower benchmarks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lange</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Server-level power control</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lefurgy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ware</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Conference on Autonomic Computing</title>
				<meeting>the 4th International Conference on Autonomic Computing</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A Scalable Priority-Aware Approach to Managing Data Center Server Power</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Lefurgy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rajamani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Allen-Ware</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Heimsoth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on High Performance Computer Architecture</title>
				<meeting>the International Symposium on High Performance Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fastcap: An efficient and fair algorithm for power capping in many-core systems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Draper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bianchini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Symposium on Performance Analysis of Systems and Software</title>
				<meeting>the IEEE International Symposium on Performance Analysis of Systems and Software</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Heracles: Improving Resource Efficiency at Scale</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Govindaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="450" to="462" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Scalable power control for manycore architectures running multi-threaded applications</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Cpm in cmps: Coordinated power management in chip-multiprocessors</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Srikantaiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kandemir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM/IEEE International Conference for High Performance Computing, Networking, Storage and Analysis</title>
				<meeting>the ACM/IEEE International Conference for High Performance Computing, Networking, Storage and Analysis</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">DeepDive: Transparently Identifying and Managing Performance Interference in Virtualized Environments</title>
		<author>
			<persName><forename type="first">D</forename><surname>Novakovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Novakovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kostic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bianchini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the USENIX Annual Technical Conference</title>
				<meeting>the USENIX Annual Technical Conference</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">No &quot;power&quot; struggles: Coordinated multi-level power management for the data center</title>
		<author>
			<persName><forename type="first">R</forename><surname>Raghavendra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Architectural Support for Programming Languages and Operating Systems</title>
				<meeting>the 13th International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficient Autoscaling in the Cloud Using Predictive Models for Workload Forecasting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gokhale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Cloud Computing</title>
				<meeting>the International Conference on Cloud Computing</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Production Hardware Overprovisioning: Real-world Performance Optimization Using an Extensible Power-Aware Resource Management Framework</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sakamoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kondo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Inoue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ueda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Patki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ellsworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rountree</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schulz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Parallel and Distributed Processing Symposium</title>
				<meeting>the International Parallel and Distributed Processing Symposium</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">pMapper: Power and Migration Cost Aware Application Placement in Virtualized Systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Distributed Systems Platforms and Open Distributed Processing</title>
				<meeting>the International Conference on Distributed Systems Platforms and Open Distributed Processing</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Increasing large-scale data center capacity by statistical power control</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th European Conference on Computer Systems</title>
				<meeting>the 11th European Conference on Computer Systems</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dynamo: Facebook&apos;s data center-wide power management system</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Meza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International Symposium on Computer Architecture</title>
				<meeting>the 43rd International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Bubble-flux: Precise Online QoS Management for Increased Utilization in Warehouse Scale Computers</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Breslow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual International Symposium on Computer Architecture</title>
				<meeting>the 40th Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Maximizing Performance Under a Power Cap: A Comparison of Hardware, Software, and Hybrid Techniques</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hoffmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Architectural Support for Programming Languages and Operating Systems</title>
				<meeting>the International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
