<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Analysis of Redundancy and Application Balance in the SPEC CPU2006 Benchmark Suite</title>
				<funder>
					<orgName type="full">IBM</orgName>
				</funder>
				<funder ref="#_QY3kTfa #_ZUCsjkc">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Aashish</forename><surname>Phansalkar</surname></persName>
							<email>aashish@ece.utexas.edu</email>
						</author>
						<author>
							<persName><forename type="first">Ajay</forename><surname>Joshi</surname></persName>
							<email>ajoshi@ece.utexas.edu</email>
						</author>
						<author>
							<persName><forename type="first">Lizy</forename><forename type="middle">K</forename><surname>John</surname></persName>
							<email>ljohn@ece.utexas.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">ECE Department</orgName>
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">ISCA&apos;07</orgName>
								<address>
									<addrLine>June 9-13</addrLine>
									<postCode>2007</postCode>
									<settlement>San Diego</settlement>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Analysis of Redundancy and Application Balance in the SPEC CPU2006 Benchmark Suite</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The recently released SPEC CPU2006 benchmark suite is expected to be used by computer designers and computer architecture researchers for pre-silicon early design analysis. Partial use of benchmark suites by researchers, due to simulation time constraints, compiler difficulties, or library or system call issues is likely to happen; but a random subset can lead to misleading results. This paper analyzes the SPEC CPU2006 benchmarks using performance counter based experimentation from several state of the art systems, and uses statistical techniques such as principal component analysis and clustering to draw inferences on the similarity of the benchmarks and the redundancy in the suite and arrive at meaningful subsets.</p><p>The SPEC CPU2006 benchmark suite contains several programs from areas such as artificial intelligence and includes none from the electronic design automation (EDA) application area. Hence there is a concern on the application balance in the suite. An analysis from the perspective of fundamental program characteristics shows that the included programs offer characteristics broader than the EDA programs' space. A subset of 6 integer programs and 8 floating point programs can yield most of the information from the entire suite.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>SPEC, since its formation in 1988, has served a long way in developing and distributing technically credible real-world application-based benchmarks for computer vendors, computer architects, researchers, and consumers.</p><p>The SPEC CPU benchmark suite, which was first released in 1989 as a collection of ten compute-intensive benchmark programs, is now in its fifth generation and has grown to 29 programs. In order to keep pace with the technological advancements, compiler improvements, and emerging workloads, in each generation of SPEC CPU benchmark suites, new programs are added, programs susceptible to unfair compiler optimizations are retired, program run times are increased, and memory access intensity of programs is increased <ref type="bibr" target="#b4">[4]</ref> <ref type="bibr" target="#b11">[11]</ref> <ref type="bibr" target="#b24">[24]</ref>. The SPEC CPU2006 benchmark suite comprises of A poorly chosen set of benchmark programs may not accurately depict the true performance of a processor design. On one hand, selecting too few benchmarks may not cover the entire spectrum of applications that may be executed on a computer system; while on the other hand, selecting too many similar programs will increase evaluation time without providing additional information. Therefore, in order to reduce the benchmarking effort, a benchmark suite should have programs that are representative of a wide range of application areas without having many programs with similar characteristics. Understanding similarity between programs can help in selecting benchmark programs that are distinct, but are still representative of the target workload space.</p><p>The microprocessor report from October 2006 presents an article <ref type="bibr" target="#b18">[18]</ref> which analyzes the SPEC CPU2006 benchmark suite, and raises a question, "Is SPEC CPU2006 well-balanced?" There are several programs from certain application areas -for example, in the integer suite, there are 3 programs (458.sjeng, 445.gobmk, 473.astar) from the Artificial Intelligence area, and in the floating point suite, there are 4 Fluid Dynamics programs (410.bwaves, 434.zeusmp, 437.leslie3d, 470.lbm), but no benchmarks from Electronic Design Automation (EDA) application area. The previous generation SPEC CPU suites contained EDA applications, 175.vpr and 300.twolf (CPU00), espresso and eqntott (CPU89 and 92). Is losing these applications that are representative of EDA workloads, a weakness of CPU2006? Or, do some other programs included in the suite have characteristics similar to the EDA programs? When multiple programs from one area are included, is there sufficient uniqueness to warrant their inclusion? In this paper, we analyze these issues based on (dis)similarity between fundamental performance characteristics of benchmarks.</p><p>The article in microprocessor report <ref type="bibr" target="#b18">[18]</ref> also discusses redundancy in benchmark suites. It states -"In truth, rather than too few programs, the several SPEC CPU suites have tended to contain too many programs: that is, they invariably comprehend redundant programs that add little or nothing to the mixture of operations represented and whose inclusion or exclusion makes little or no difference to the overall score achieved." How true is this about SPEC CPU2006? How much redundancy is there in the CPU2006 suite? Analysis of (dis)similarity between programs can also help in addressing this question. Also, partial use of benchmark suite is common for simulation based studies. Citron <ref type="bibr" target="#b2">[2]</ref> [3] conducted a survey on benchmark subsets used by computer architecture researchers in top computer architecture conferences and showed that partial use of subsets can lead to incorrect and misleading inferences. The information about similarity between programs can be used to identify a representative subset of programs and their input sets, as opposed to a random selection of a partial suite.</p><p>In this paper we apply multivariate statistical analysis techniques such as Principal Components Analysis (PCA) and cluster analysis to (i) study the balance of the CPU2006 benchmark suite, (ii) identify similarity/dissimilarity between CPU2006 programs, (iii) identify similarity between multiple input sets and find representative ones, and (iv) propose subsets of CPU2006 programs that are representative of the whole set. The characterization of programs in this paper is based on run-time program profile information and measurements from five different state-of-the-art machines that represent the most popular architectures.</p><p>The remainder of this paper is organized as follows. Section 2 gives an overview of the SPEC CPU2006 benchmark suite and its instruction stream characteristics.</p><p>Section 3 describes the methodology used to measure redundancy between programs and proposes a representative subset of programs and inputs sets. Section 4 studies the application balance in the SPEC CPU2006 suite and compares them to programs from SPEC CPU2000. Section 5 surveys previous research work related to our study. Finally in Section 6 we conclude with a summary of the key results from this study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">OVERVIEW OF SPEC CPU2006</head><p>The SPEC CPU2006 suite, like its predecessors is divided into two parts: the integer component (CINT2006 benchmarks) and the floating point component (CFP2006 benchmarks). The integer group consists of 12 programs, written in C and C++, and the floating point group consists of 17 programs written in C, C++, and FORTRAN languages. In this section, we provide an overview of the runtime characteristics of SPEC CPU2006 integer and floating-point benchmarks in terms of their instruction mix and instruction locality. These runtime characteristics are measured on a Pentium D processor (2.1 GHz, 16KB L1 data and instruction caches, and 2x2MB (4MB) L2 cache) system running SUSE Linux 10.1 and were compiled using Intel C/C++, and FORTRAN compiler V9.1. The performance counter measurements were carried out using the PAPI <ref type="bibr" target="#b5">[5]</ref> tool set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Instruction Mix</head><p>Table <ref type="table" target="#tab_0">1</ref>, shows the dynamic instruction count and the instruction mix of the programs. The dynamic instruction count of 24 out of the 29 benchmarks is of the order of a few trillion instructions, as compared to a maximum of few hundred billion instructions per program in the CPU2000 suite -further exacerbating the problem of simulation time.</p><p>There are several interesting observations from the instruction mix of the programs. For integer programs, the percentage of branches in the dynamic instruction stream is close to the typical 20%. However, two programs, 456.hmmer and 464.h264ref only have 7% branches. In 483.xalancbmk, one of three C++ programs in the integer suite has 25% branches. In comparison, the other two C++ programs, 471.omnetapp and 473.astar, are more typical with 20% and 15% branch instructions respectively. Among the floating-point programs, 447.deall, 450.soplex and 453.povray have approximately 15% branches where as most of the other floatingpoint programs have less than 5% branch instructions. There are a few floating-point programs, 410.bwaves, 470.lbm, 436.cactusADM, which have less than 1% branches. The large average dynamic basic block size in these programs suggests a high degree of parallelism that can be exploited by out-of-order microarchitectures. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Instruction Locality Based on Subroutine Profiling</head><p>In order to understand the code locality in the CPU2006 programs, we perform subroutine profiling using the PIN dynamic instrumentation tool <ref type="bibr" target="#b17">[17]</ref>. PIN can identify hot subroutines based on subroutine call frequency. It can also count the number of dynamic/static instructions in the subroutines. Figure <ref type="figure" target="#fig_1">1</ref> shows the locality characteristics for at least one input set of integer and floating point benchmarks. Appendix I shows a table with the summary of data measured for this experiment. The cumulative percentage of dynamic instructions executed by a program is shown on Y-axis and the cumulative count of static instructions is shown on the X-axis with a log scale. The first point in the line plot for each benchmark represents the hottest subroutine -Xcoordinate shows the number of static instructions in the routine and Y-axis the percentage of dynamic instructions that it represents. The second, third, fourth and fifth points respectively represent the top five, ten, fifteen and twenty hot subroutines. Many programs initially show a steep upward climb as the static instruction count increases, which suggests very good instruction locality.   As shown in Figure <ref type="figure" target="#fig_1">1</ref> the top twenty subroutines cover 80% or more of the dynamic instructions in almost all benchmarks. The integer benchmark 456.hmmer shows a very high reuse of code in the hottest subroutine. More than 95% of the instructions come from the hottest subroutine which has 11,080 static instructions. Similarly the floating point benchmarks 436.CactusADM and 470.lbm show a very high code-reuse and hence good instruction locality. On the other hand, the 20 hot subroutines in 471.omnetpp, 483.xalancbmk and 403.gcc account for a very low percentage of dynamic instructions, suggesting a relatively poor instruction locality. The trend observed in the previous generation SPEC CPU benchmark suites <ref type="bibr" target="#b19">[19]</ref> continues with gcc exhibiting the poorest instruction locality among all the benchmark programs. In SPEC CPU2006, 403.gcc-9 has the poorest instruction locality with 5 million static instructions only accounting for approximately 45% of the dynamic instructions. As such, we can conclude that similar to the programs in the previous generation SPEC CPU benchmark suites, most of the SPEC CPU2006 benchmarks exhibit good instruction locality, but there are some notable exceptions with very large instruction footprints and relatively poor instruction locality.</p><p>Apart from the instruction locality, there are other microarchitectural attributes which can be measured during execution of the benchmark programs. Table <ref type="table" target="#tab_2">2</ref> shows the range of some important characteristics measured using performance monitoring counters on a Pentium D system described earlier in this section, illustrating the diversity of the benchmarks in the CPU2006 suite. Many characteristics are seen to vary orders of magnitude between the minimum and maximum. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">REDUNDANCY IN SPEC CPU2006 SUITE</head><p>Although the SPEC CPU2006 benchmark suite comprises of programs from different application domains, it is possible that they exhibit similar program characteristics. In this section we outline the methodology to measure the (dis)similarity in fundamental program characteristics of benchmarks. We then apply this technique to measure the redundancy of programs in the SPEC CPU2006 benchmark suite, and propose a subset of representative programs and input sets that can be used if the time required to simulate the entire benchmark suite is prohibitive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Methodology</head><p>We measured fundamental program characteristics related to their instruction locality, data locality, branch predictability, and instruction mix, using hardware performance counters. These characteristics are microarchitecture-dependent and the results could be biased by the idiosyncrasies of a particular machine. Therefore, in order to eliminate this bias we measured the program characteristics on five different state-of-the-art machines with four different Instruction Set Architectures (ISAs) and compilers (IBM Power, Sun UltraSPARC, Itanium, and x86). Table <ref type="table" target="#tab_3">3</ref> shows the list of performance counter based characteristics that were measured for each program on five different machines. We performed a correlation analysis between every performance counter characteristic and Cycles-Per-Instruction (CPI), and only selected characteristics that showed a good correlation to performance. This process eliminates the performance counters that exhibit a large variation but have little or no impact on performance. Note that the important metrics that affect performance for the integer and floating-point programs are different. In addition to these characteristics, the variability in microarchitectures, ISAs, and compilers across the five machines helps in capturing the differences between the benchmarks. The hardware performance counter data used in this study was measured by various members of the SPEC CPU subcommittee members on their state-of-the-art machines.</p><p>Also, the methodology outlined in this section was used by the SPEC CPU subcommittee as one of the factors in evaluating the candidates for the SPEC CPU2006 benchmark suite <ref type="bibr" target="#b12">[12]</ref>. The confidentiality requirements prevent us from disclosing the details of the machines on which this data was measured nor the performance counter data from individual machines.</p><p>Since the measurements are carried out on five different machines, each performance counter characteristic-machine pair is treated as a variable. If we have n machines and we measure m characteristics for each machine, we have n x m variables for each program. There is a pitfall of directly using these raw variables to measure similarity between programs. It is possible that some of these variables are correlated and measure the same inherent benchmark property. Therefore, using a large number of correlated variables will unduly overemphasize the importance of a particular benchmark property. In order to remove the correlation between these variables, this dataset is pre-processed using Principal Components Analysis (PCA) <ref type="bibr" target="#b7">[7]</ref>. Clustering Analysis (CA) is then used to group programs with similar program characteristics. We now describe the two statistical analysis techniques -Principal Component Analysis and Cluster Analysis. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Removing Correlation using PCA</head><p>Considering the six characteristics measured on each of the five different machines, we have thirty characteristics per program. It is humanly impossible to simultaneously look at all the data and draw meaningful conclusions from them. Hence PCA is used to analyze the data. In order to isolate the effect of varying ranges of each parameter, the data is first normalized to a unit normal distribution, i.e. a normal distribution with mean equal to zero and standard deviation equal to 1, for each variable. PCA helps to reduce the dimensionality of a data set while retaining most of the original information. PCA computes new variables, so-called principal components, which are linear combinations of the original variables, such that all the principal components are uncorrelated. PCA transforms p variables X 1 , X 2 ,...., X p into p principal components (PC) Z 1 ,Z 2 ,?,Z p such that:</p><formula xml:id="formula_0">? = = p j j ij i X a Z 0 This transformation has the property Var [Z 1 ] ? Var [Z 2 ]</formula><p>??? Var [Z p ] which means that Z 1 contains the most information and Z p the least. Given this property of decreasing variance of the PCs, we can remove the components with the lower values of variance from the analysis. This reduces the dimensionality of the data set while controlling the amount of information that is lost. We use a standard technique (Kaiser Criterion) to choose PCs where only the top few PCs which have eigenvalues greater than or equal to one are retained. In order to capture the entire information from p original variables, p PCs may be required, but if there are several correlated variables, a small number of PCs can capture most of the information from the original variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Measuring Similarity using Cluster Analysis</head><p>Clustering is a statistical technique that can be used to group programs with similar features. There are two commonly used clustering techniques -K-means clustering and hierarchical clustering. The K-means clustering algorithm divides a set of N programs into K groups, where K is a value specified by the user. Therefore, in order to evaluate different grouping possibilities one needs to cluster programs for different values of K and then select the best fit. On the other hand, as the name suggests, hierarchical clustering is useful in simultaneously looking at multiple clustering possibilities and the user can select the desired number of clusters using a dendrogram. Therefore, in this paper we use hierarchical clustering algorithm. Hierarchical clustering is a bottom up approach and starts with a matrix of distance between N cases or benchmarks. The distance is the Euclidean distance between the program characteristics. The algorithm used for hierarchical clustering is as follows: 1. Assign each program to its own cluster, such that if we have N programs we have N clusters. 2. Find the closest (most similar) pair of clusters and merge them into a single cluster, so that we have one less cluster. 3. Compute distances (similarity) between the new cluster and the old cluster. 4. Repeat steps 2 and 3 until all items are grouped into a single cluster of size N. This hierarchical clustering process can be represented as a tree or dendrogram, where each step in the clustering process is illustrated by a joint in the tree (e.g.: Figure <ref type="figure" target="#fig_2">2</ref>). The numbered scale on the horizontal axis corresponds to the linkage distance between programs.</p><p>We now apply this methodology to find a representative subset of programs from the SPEC CPU2006 benchmark suite.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Subsetting SPEC CPU2006 Benchmarks</head><p>To keep pace with advancements in technology and the increase in size of on-chip caches, the data footprint and run time of SPEC CPU benchmark programs has been significantly increased. However for architectural studies that use cycleaccurate simulators, it is virtually impossible to simulate all programs and input sets in a reasonable amount of time. If the same amount of information can be obtained from a smaller subset of representative programs, it would certainly help architects and researchers to cut down the simulation time without compromising on the inferences drawn from their studies.</p><p>This section demonstrates the result of applying PCA and cluster analysis for selecting a subset of benchmark programs when an architect or researcher is constrained by time and wants to select a reduced subset of programs from the suite. Figure <ref type="figure" target="#fig_2">2</ref> shows a dendrogram for CINT2006 benchmarks obtained after applying PCA and Hierarchical Clustering on the performance counter data from Table <ref type="table" target="#tab_3">3</ref>. The Euclidean distance between the benchmarks is used as a measure of dissimilarity and singlelinkage distance is computed to create a dendrogram. Seven Principal Components (PCs) with eigen values greater than one are chosen and they retain 94% of the variance. In the dendrogram in Figure <ref type="figure" target="#fig_2">2</ref> the horizontal axis shows the linkage distance indicating the dissimilarity between the benchmarks. The ordering on the Y-axis does not have particular significance, except that benchmarks are positioned close to each other when the distance is smaller. Benchmarks that are outliers have larger linkage distances with the rest of the clusters formed in a hierarchical way. One can use this dendrogram to select a representative subset of programs. For example, if a researcher wants to reduce his simulation budget to just six benchmarks, then drawing a vertical line at linkage distance of 4, as shown in Figure <ref type="figure" target="#fig_2">2</ref>, will give a subset of six benchmarks (k=6). Drawing a line at a point close to 4.5 yields a subset of four benchmarks (k=4). Table <ref type="table" target="#tab_4">4</ref> shows the resulting subsets of the CINT2006 suite. In clusters where there are more than two programs, the representative of cluster i.e. the benchmark closest to the center of the cluster is chosen as a representative. As we traverse from left to right on the dendrogram the number of benchmarks in the subset keep decreasing. This helps the user to select appropriate benchmarks when simulation time is a constraint.    <ref type="table" target="#tab_5">5</ref>. The distance of each of the benchmarks in the cluster to the cluster center has to be recalculated and a representative can be chosen. In Figure <ref type="figure" target="#fig_3">3</ref> there are two main clusters which split at extreme right because the branch characteristics of the benchmarks, 447.dealII, 450. soplex and, 453.povray exhibit a comparatively higher branch misprediction rate. In the next section we evaluate the representativeness of these subsets.</p><p>One should note that clustering and subsetting gives importance to unique features and differences. It helps to eliminate redundancy and duplicated efforts in experimentation. However, one should not mistake the mix of program types in a subset as the mix of program types in real-world workloads. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluating Representativeness of Subsets</head><p>We evaluate the usefulness of the subsets, proposed in the previous section, to estimate the speedup of the entire suite on eight different commercial systems. The SPEC web page 1 presents performance results of SPEC CPU2006 benchmarks on commercial computer systems. We used these results to evaluate the efficacy of the subset of programs proposed in the previous section. We obtained the execution times for all benchmarks on 8 different platforms and their execution times on a reference machine. We then compared the weighted average (geometric mean) speedup from the subset against the average (geometric mean) speedup from the entire component (CINT or CFP) of the suite. When calculating the average speedup from the subset, each benchmark of the subset was assigned a weight proportional to the number of benchmarks in its cluster.    Figure <ref type="figure" target="#fig_6">4</ref> shows the comparison for CINT2006 benchmarks using the subset of 4 and 6 benchmarks shown in Table <ref type="table" target="#tab_4">4</ref>. For CINT component the subset of 4 programs shows an average error of 5.8% and a maximum error of 10.1%. The subset of 6 benchmarks shows an average error of 3.8% and a maximum error 1 http://www.spec.org/cpu2006/results/cpu2006.html of 8%. This shows that even a subset of 4 benchmarks out of 12 CINT benchmarks has a very good predictive power in estimating the speedup shown by the entire suite.</p><p>Figure <ref type="figure" target="#fig_7">5</ref> shows the validation of CFP2006 benchmarks using the subsets from Table <ref type="table" target="#tab_5">5</ref>. The maximum error in the floating point subset of 6 is higher than that in the integer benchmark subset. For a subset of 6 the average error is 10.8% with the maximum error of 19%. Hence we look at a subset of 8 benchmarks which shows the average error of 7% and the maximum error is 12%. From these results, we observe that 6 out of 12 integer benchmarks and 8 out of 17 floating point benchmarks form a good representative subset. It is interesting to observe that a third or half of the benchmarks in a suite can contain most of the information in the entire suite.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Selecting representative input sets</head><p>Many benchmarks in the CPU2006 have multiple input sets. For example, 403.gcc benchmark has nine input sets. A reportable SPEC result for each benchmark is supposed to comprise of all its input sets. However, for simulation based studies, researchers typically select one input set. Instead of selecting an input set in an ad hoc manner, clustering analysis can also be used to select a representative input set. The program characteristics shown in Table <ref type="table" target="#tab_3">3</ref> were measured for all the different benchmarks and input sets. PCA and clustering analysis was performed on this data to find similarity between input sets of each benchmark.</p><p>Figure <ref type="figure" target="#fig_8">6</ref> shows the dendrogram for input sets and the benchmarks for the integer component. The Kaiser criterion results in choosing seven PCs covering 89% of variance for this analysis. Some benchmarks have only one input set and are hence represented only by their name. In some benchmarks, all input sets appear clustered together, where as in many cases, some input sets are very different from the other input sets of the same benchmark. As an example, the behavior of 403.gcc-9 is significantly different from its sibling input sets. In this analysis, a benchmark's input set closest to the whole (aggregated) benchmark run is selected as the representative input set. In CINT2006, the benchmarks that have multiple input sets are 400.perlbench, 401.bzip2, 403.gcc, 445.gobmk, 456.hmmer, 464.h264ref and 473.astar. Similarly, we also perform the analysis for input sets of CFP2006 programs. Figure <ref type="figure" target="#fig_9">7</ref> shows the dendrogram showing similarity between inputs sets of programs from CFP2006. Six PCs covering 88% of variance are chosen. In this category there are only two benchmarks with multiple input sets -416.gamess and 450.soplex.</p><p>For each of these benchmarks we list the most representative input set in Table <ref type="table" target="#tab_7">6</ref>. This information will be useful in selecting the most representative input set for each program.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Subsets based on branch and memory access characteristics</head><p>Often, researchers focus on optimizing the design to take advantage of certain program characteristcs. In this section, we separately analyze the similarity of programs based on data access performance characteristics and branch prediction characteristics. This similarity information can be used when conducting data cache and branch prediction studies. In this section we analyze all programs in the suite without classifying them into CINT and CFP groups. The data accesses characteristics were also measured on three different systems and after applying PCA, 4 PCs were retained which account for 84% of the variance. Figure <ref type="figure" target="#fig_12">9</ref> shows a scatter plot based on the first 4 PCs. For the purpose of clarity only certain benchmarks are labeled in the scatter plots. The benchmarks that have a more negative value of PC1 e.g. 429.mcf, 471.omnettp, 462.libquantum exhibit a poor data access behavior and hence result in higher data cache miss-rates. The CFP benchmarks that are located at the top show very high percentage of memory accesses and hence should also be considered when selecting benchmarks for cache studies. In summary, when selecting benchmarks for a certain study the user should look at the workload space of those characteristics and select benchmarks to ensure that the entire workload is covered. Only selecting outliers will exercise worst case or best case behavior and can lead to misleading conclusions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">BALANCE IN THE SPEC CPU2006 SUITE 4.1 Case study on EDA Applications</head><p>The concern in the October 2006 microprocessor report article <ref type="bibr" target="#b18">[18]</ref> on CPU2006, whether the suite is balanced, stems from the fact that certain application areas have multiple representative programs (see Table <ref type="table" target="#tab_8">7</ref>), whereas certain areas are not represented in the suite. For example, in the integer suite, there are 3 programs (458.sjeng, 445.gobmk, 473.astar) from the artificial intelligence area, and in the floating point suite there are 4 programs (410.bwaves,434.zeusmp, leslie3d, lbm) from the fluid dynamics area. In this section we provide a case study on the characteristics of the Electronic Design Automation (EDA) applications. The CPU2006 suite contains none, where as the earlier SPEC CPU suites contained more than one EDA applications (vpr, twolf, espresso, eqntott). Is losing the applications from the EDA area a weakness of CPU2006? Are other programs from other application areas similar to EDA applications that their absence is not an issue? We perform a similarity analysis to understand this. Program characteristics are measured for all the SPEC CPU2000 integer programs and projected in the workload space after performing PCA on the characteristics of CINT2006 benchmarks. Figure <ref type="figure" target="#fig_14">10</ref> shows the projections of the workload space. From these figures it is evident that the EDA tool benchmarks in CPU2000 lie close to 473.astar and 401.bzip2 from CPU2006 benchmarks. Since the EDA programs are well within the envelope of the workload space covered by the new suite, the elimination of EDA programs may not be a major concern.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Differences between benchmarks from the same application area</head><p>Any two benchmarks that belong to the same application area can show different behavior on certain architecture. Are the programs from SPEC CPU2006 suite, which belong to the same application area really different? The similarity analysis described in Section 3 can answer this question. Let us consider one application area from Table <ref type="table" target="#tab_8">7</ref> at a time and refer to Figures <ref type="figure" target="#fig_2">2</ref> and<ref type="figure" target="#fig_3">3</ref> to answer the question. In case of artificial intelligence area, 458.sjeng and 473.astar show very similar behavior and can be found quite close to each other in the workload space, while 445.gobmk is much further away from its siblings. The equation solver applications do not lie close to each other and hence justify their presence in the suite. 410.bwaves and 437.leslied, are relatively close to each other than the other two programs in their application area. Both the programs in molecular dynamics are different and relatively close to each other with the linkage distance of less than 2 between them. 465.tonto and 416.gamess also have a linkage distance of less than 2. On the contrary, the 4 programs from the Engineering and Operational Research domain are significantly different from each other. In Table <ref type="table" target="#tab_8">7</ref> two programs that belong to the same application area and show similar program characteristics are highlighted in bold. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparing CPU2006 benchmarks with CPU2000</head><p>SPEC CPU subcommittee took efforts to include challenging applications to stay relevant in the light of fast and powerful emerging machines. Looking back after the selection, how did these changes affect the overall characteristics of programs in CPU2000? Are the programs fundamentally different from the ones in SPEC CPU2000?</p><p>From Figure <ref type="figure" target="#fig_14">10</ref> it is evident that the CINT2006 benchmarks are spread farther in the workload space as compared to CINT2000 benchmarks and cover a wider area in the workload space. In the PC1 Vs PC2 scatter plot (Figure <ref type="figure" target="#fig_12">9</ref>(a)), many of the CPU 2000 programs are located close to the (0,0), whereas, the new programs such as 483.xalancbmk and 456.hmmer provide coverage for very far away spots in the workload space. In the PC3-PC4 scatter plot (Figure <ref type="figure" target="#fig_14">10(a)</ref>), one can easily notice the diversity added by the programs 445.gobmk, 483.xalancbmk, 462.libquantum and 458.sjeng. They extend the envelope of the benchmark space significantly. It is also interesting to note that the benchmarks from CPU2000 suite that were retained e.g. mcf, bzip2, perl, gcc show similar behavior as their predecessors. This might mean that only the dynamic instruction count and data footprint of these benchmarks changed but the control flow almost remained the same. The increased data footprint will exercise the big caches in recent processors. Due to space constraints we could not show the scatter plots for the CFP2006 benchmarks but we see that the floating benchmarks in CPU2006 are even more diverse compared to the ones in CPU2000. Overall, one can observe increased diversity in the new suite.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Sensitivity of programs to performance characteristics</head><p>In this section, we present a classification of programs based on their sensitivity to branch predictors and data cache across five machines that were used to do the analysis in Section 3. In order to measure the sensitivity of a program to branch predictor and L1 D-cache configuration, for every machine we ranked programs based on these characteristics. The difference in ranks of a program across all machines is then computed. The resulting number is indicative of sensitivity of that program for a given characteristic. Table <ref type="table" target="#tab_9">8</ref> shows the classification of programs based on their sensitivity to branch predictor and L1 D-cache configuration It is organized as follows. For each characteristic (branch misprediction rate and L2 data cache miss-rate) the workloads (program-input pairs) are categorized into one of the low, medium and high ranges. The most striking observation from this is that 462.libquantum, 456.hmmer, and 464.h264ref show the most variation in D-cache misses across the five machines. 456.hmmer shows a lot of variation for branches where as 458.sjeng, 473.astar and 445.gobmk show higher misprediction rates. As observed by Vandierendonck et.al. <ref type="bibr" target="#b23">[23]</ref> in CPU2000, we also observed that 409.gcc from CPU2006 ranks relatively low in variation across the five machines used in the experiment. The explanation is that every machine is equally good or equally bad for gcc, eliminating the sensitivity to platforms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">RELATED WORK</head><p>In <ref type="bibr" target="#b8">[8]</ref> and <ref type="bibr">[9]</ref> Eeckhout et.al measured a mixture of microarchitecture dependent and microarchitecture independent metrics and presented similarity information of programs. Vandierendonck and Bosschere <ref type="bibr" target="#b23">[23]</ref> analyzed the SPEC CPU2000 benchmark suite peak results on 340 different machines representing eight architectures, and used PCA to identify the redundancy in the benchmark suite. They found that a small number of CPU 2000 programs have nearly the same predictive power as the entire suite in ranking machines. Giladi and Ahituv <ref type="bibr" target="#b10">[10]</ref> also had a similar approach towards finding a subset of programs. They found that the ten programs of SPEC89 suite could be reduced to six without affecting the SPEC rating. Phansalkar et.al. <ref type="bibr" target="#b20">[20]</ref> and Joshi et.al. <ref type="bibr" target="#b15">[15]</ref> characterized benchmarks using microarchitecture independent metrics to find a representative subset and study the difference between the previous generations of SPEC CPU benchmarks. The metrics are independent of microarchitecture but compiler and ISA dependent. However, since CPU2006 benchmarks have very long runtime (23 out of 29 benchmarks have more than one trillion instructions), significantly higher time would be required to measure these characteristics. The approach used in this paper tries to achieve the component of microarchitecture independence by characterizing benchmarks on a wide range of systems with different ISAs, compilers and microarchitectures. Yi et.al <ref type="bibr" target="#b27">[27]</ref> compare all the different subsetting approaches and show that use of statistical techniques for subsetting can lead to improved accuracy. Citron <ref type="bibr" target="#b2">[2]</ref>[3] presented subsets based on use by the computer architecture research community and showed that partial use of suites can lead to misleading results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSIONS</head><p>There are concerns about program redundancy programs in the SPEC CPU2006 benchmark suite and that benchmarks from some certain commonly used application areas are missing. Using performance counter data from five different state of the art machines, and statistical analysis techniques such as PCA and clustering, we analyze the similarity and balance of the recently released SPEC CPU2006 suite. Dendrograms illustrating the similarity between benchmarks and scatter plots showing the workload space are presented for overall selection of metrics, as well separately for branch and data access metrics.</p><p>We show that 6 out of the 12 integer programs and 8 of the 17 FP programs can capture most of the information from the FP benchmarks. It is observed that not all programs in the same application area are similar. Some of them are more similar to the programs in another application area. When analyzed from the perspective of program characteristics, an unrepresented area such as electronic design automation (EDA) may not be a major weakness of the suite. We also identify one input set as a representative input set for programs that have multiple input sets. Not all input sets of a benchmark result in similar behavior. Some input sets make a program appear more similar to another benchmark rather than its own sibling.</p><p>It is less than a year since SPEC CPU2006 has been released. If Citron's observation regarding use of benchmarks <ref type="bibr" target="#b2">[2]</ref>[3] holds, it will be a quite some time before the research community can succeed in compiling and simulating the entire benchmarks. Even if the intentional misuse may not happen, partial use of the suite will be inevitable due to difficulties with compilation, system call issues, libraries etc. The information presented in this paper should help researchers in selecting a representative set of benchmarks, if subsetting is unavoidable. It should also help in understanding and interpreting simulation results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">DISCLAIMER</head><p>All the observations and analysis done in this paper on SPEC CPU2006 benchmarks are the authors' opinions and should not be used as official or unofficial guidelines from SPEC in selecting benchmarks for any purpose. This paper only provides guidelines for researchers and academic users to choose a subset of benchmarks should the need be.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Instruction locality based on code reuse in the top 20 hot subroutines for SPEC CPU2006 benchmarks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Dendrogram showing similarity between CINT2006 Programs.</figDesc><graphic url="image-1.png" coords="5,54.00,519.12,238.32,157.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Dendrogram showing similarity between CFP2006 Programs.</figDesc><graphic url="image-2.png" coords="5,317.88,72.00,240.12,162.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3</head><label>3</label><figDesc>Figure3shows the dendrogram for floating point benchmarks in CPU2006. Five PCs are chosen using the Kaiser criterion which retains 85% of the variance. The two vertical arrows show the points at which the subsets of size 6 and 8 are formed. The resulting clusters are shown in Table5. The distance of each of the benchmarks in the cluster to the cluster center has to be recalculated and a representative can be chosen. In Figure3there are two main clusters which split at extreme right because the branch characteristics of the benchmarks, 447.dealII, 450. soplex and, 453.povray exhibit a comparatively higher branch misprediction rate. In the next section we evaluate the representativeness of these subsets.One should note that clustering and subsetting gives importance to unique features and differences. It helps to eliminate redundancy and duplicated efforts in experimentation. However, one should not mistake the mix of program types in a subset as the mix of program types in real-world workloads.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Validation of CINT2006 subset using performance scores of eight systems from the SPEC CPU website.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Validation of CFP2006 subset using performance scores of eight systems from the SPEC CPU website.</figDesc><graphic url="image-3.png" coords="6,317.88,471.48,235.08,214.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Dendrogram showing similarity between programinput set for each benchmark in the SPEC CPU2006 suite.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Dendrogram showing similarity between programinput set for each program from CFP2006.</figDesc><graphic url="image-4.png" coords="7,54.00,241.08,227.52,191.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. CINT and CFP programs in the PC workload space using branch predictor characteristics.</figDesc><graphic url="image-5.png" coords="7,317.88,72.00,244.32,180.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figures 8</head><label>8</label><figDesc>shows the scatter plot based on the first 2 PCs of the branch characteristics, covering approximately 92% of the variance. The CFP benchmarks are clustered together but the CINT programs clearly show diversity. A few CINT workloads overlap with the cluster of CFP workloads on the right side of the plot. These CINT benchmarks (464.h264ref, 456.hmmer) have fewer branches like the other floating programs around them. Majority of the floating point programs have very little diversity in their branch characteristics. The benchmarks with high value of PC1, e.g. majority of CFP benchmarks and 464.h264ref and 456.hmmer show easy to predict branches. On the other hand CINT benchmarks are more spread out in the space and show significantly diverse behavior. The data used for this analysis was measured from three different systems.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. CINT and CFP programs plotted in the PC space using memory access characteristics (PC1 Vs. PC2).</figDesc><graphic url="image-7.png" coords="8,59.04,268.68,229.80,162.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>(a) PC1 Vs. PC2 (b) PC3 Vs. PC4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Scatterplot showing position of EDA applications in the workload space.</figDesc><graphic url="image-9.png" coords="8,317.88,378.00,243.96,169.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Dynamic Instruction Count and Instruction Mix of SPEC CPU2006 Integer and Floating-Point Benchmarks.</figDesc><table><row><cell></cell><cell>Inst. Count</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Name -Language</cell><cell>(Billion)</cell><cell>Branches</cell><cell>Loads</cell><cell>Stores</cell></row><row><cell></cell><cell cols="2">CINT 2006</cell><cell></cell><cell></cell></row><row><cell>400.perlbench -C</cell><cell>2,378</cell><cell>20.96%</cell><cell>27.99%</cell><cell>16.45%</cell></row><row><cell>401.bzip2 -C</cell><cell>2,472</cell><cell>15.97%</cell><cell>36.93%</cell><cell>12.98%</cell></row><row><cell>403.gcc -C</cell><cell>1,064</cell><cell>21.96%</cell><cell>26.52%</cell><cell>16.01%</cell></row><row><cell>429.mcf -C</cell><cell>327</cell><cell>21.17%</cell><cell>37.99%</cell><cell>10.55%</cell></row><row><cell>445.gobmk -C</cell><cell>1,603</cell><cell>19.51%</cell><cell>29.72%</cell><cell>15.25%</cell></row><row><cell>456.hmmer -C</cell><cell>3,363</cell><cell>7.08%</cell><cell>47.36%</cell><cell>17.68%</cell></row><row><cell>458.sjeng -C</cell><cell>2,383</cell><cell>21.38%</cell><cell>27.60%</cell><cell>14.61%</cell></row><row><cell>462.libquantum-C</cell><cell>3,555</cell><cell>14.80%</cell><cell>33.57%</cell><cell>10.72%</cell></row><row><cell>464.h264ref-C</cell><cell>3,731</cell><cell>7.24%</cell><cell>41.76%</cell><cell>13.14%</cell></row><row><cell>471.omnetpp-C++</cell><cell>687</cell><cell>20.33%</cell><cell>34.71%</cell><cell>20.18%</cell></row><row><cell>473.astar-C++</cell><cell>1,200</cell><cell>15.57%</cell><cell>40.34%</cell><cell>13.75%</cell></row><row><cell>483.xalancbmk-C++</cell><cell>1,184</cell><cell>25.84%</cell><cell>33.96%</cell><cell>10.31%</cell></row><row><cell></cell><cell cols="2">CFP 2006</cell><cell></cell><cell></cell></row><row><cell>410.bwaves -Fortran</cell><cell>1,178</cell><cell>0.68%</cell><cell>56.14%</cell><cell>8.08%</cell></row><row><cell>416.gamess -Fortran</cell><cell>5,189</cell><cell>7.45%</cell><cell>45.87%</cell><cell>12.98%</cell></row><row><cell>433.milc -C</cell><cell>937</cell><cell>1.51%</cell><cell>40.15%</cell><cell>11.79%</cell></row><row><cell>434.zeusmp-C,Fortran</cell><cell>1,566</cell><cell>4.05%</cell><cell>36.22%</cell><cell>11.98%</cell></row><row><cell>435.gromacs-C, Fortran</cell><cell>1,958</cell><cell>3.14%</cell><cell>37.35%</cell><cell>17.31%</cell></row><row><cell>436.cactusADM-C, Fortran</cell><cell>1,376</cell><cell>0.22%</cell><cell>52.62%</cell><cell>13.49%</cell></row><row><cell>437.leslie3d -Fortran</cell><cell>1,213</cell><cell>3.06%</cell><cell>52.30%</cell><cell>9.83%</cell></row><row><cell>444.namd -C++</cell><cell>2,483</cell><cell>4.28%</cell><cell>35.43%</cell><cell>8.83%</cell></row><row><cell>447.dealII -C++</cell><cell>2,323</cell><cell>15.99%</cell><cell>42.57%</cell><cell>13.41%</cell></row><row><cell>450.soplex -C++</cell><cell>703</cell><cell>16.07%</cell><cell>39.05%</cell><cell>7.74%</cell></row><row><cell>453.povray -C++</cell><cell>940</cell><cell>13.23%</cell><cell>35.44%</cell><cell>16.11%</cell></row><row><cell>454.calculix -C, Fortran</cell><cell>3,041</cell><cell>4.11%</cell><cell>40.14%</cell><cell>9.95%</cell></row><row><cell>459.GemsFDTD -Fortran</cell><cell>1,420</cell><cell>2.40%</cell><cell>54.16%</cell><cell>9.67%</cell></row><row><cell>465.tonto -Fortran</cell><cell>2,932</cell><cell>4.79%</cell><cell>44.76%</cell><cell>12.84%</cell></row><row><cell>470.lbm -C</cell><cell>1,500</cell><cell>0.79%</cell><cell>38.16%</cell><cell>11.53%</cell></row><row><cell>481.wrf -C, Fortran</cell><cell>1,684</cell><cell>5.19%</cell><cell>49.70%</cell><cell>9.42%</cell></row><row><cell>482.sphinx3 -C</cell><cell>2,472</cell><cell>9.95%</cell><cell>35.07%</cell><cell>5.58%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Range of important performance characteristics of SPEC CPU2006 benchmarks</figDesc><table><row><cell>Metric</cell><cell>Min</cell><cell>Max</cell></row><row><cell>I-cache miss ratio</cell><cell>~ 0</cell><cell>1.7%</cell></row><row><cell>L1 D-cache miss ratio</cell><cell>6.3%</cell><cell>33%</cell></row><row><cell>L2 cache misses per</cell><cell>~0</cell><cell>2.4%</cell></row><row><cell>instruction (per L2 access)</cell><cell>(0.01%)</cell><cell>(49%)</cell></row><row><cell>DTLB miss ratio</cell><cell>0.2%</cell><cell>8.4%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Program Characteristics used for measuring similarity between Integer and Floating-Point programs.</figDesc><table><row><cell cols="3">Integer benchmarks</cell><cell></cell><cell cols="2">Floating-Point</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">benchmarks</cell></row><row><cell>Integer</cell><cell cols="2">operations</cell><cell>per</cell><cell cols="2">Floating point operations per</cell></row><row><cell>instruction</cell><cell></cell><cell></cell><cell></cell><cell cols="2">instruction</cell></row><row><cell cols="4">L1 instruction cache misses per</cell><cell>Memory</cell><cell>references</cell><cell>per</cell></row><row><cell>instruction</cell><cell></cell><cell></cell><cell></cell><cell cols="2">instruction</cell></row><row><cell cols="4">Number of branches per</cell><cell cols="2">L2 data cache misses per</cell></row><row><cell>instruction</cell><cell></cell><cell></cell><cell></cell><cell cols="2">instruction</cell></row><row><cell>Number</cell><cell>of</cell><cell cols="2">mispredicted</cell><cell cols="2">L2 data cache misses per L2</cell></row><row><cell cols="3">branches per instruction</cell><cell></cell><cell>accesses</cell></row><row><cell cols="4">L2 data cache misses per</cell><cell>Data</cell><cell>TLB</cell><cell>misses</cell><cell>per</cell></row><row><cell>instruction</cell><cell></cell><cell></cell><cell></cell><cell cols="2">instruction</cell></row><row><cell cols="4">Instruction TLB misses per</cell><cell cols="2">L1 data cache misses per</cell></row><row><cell>instruction</cell><cell></cell><cell></cell><cell></cell><cell cols="2">instruction</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Representative subset of SPEC CINT2006 programs.</figDesc><table><row><cell>Subset of Four Programs</cell><cell>400.perlbench, 462.libquantum,473.astar,483.xalancbmk</cell></row><row><cell>Subset of</cell><cell>400.perlbench, 471.omnetpp, 429.mcf,</cell></row><row><cell>Six</cell><cell>462.libquantum, 473.astar,</cell></row><row><cell>Programs</cell><cell>483.xalancbmk</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Representative subset of SPEC CFP2006 programs.</figDesc><table><row><cell>Subset of Six Programs</cell><cell>437.leslie3d, 454.calculix, 436.cactusADM, 447.dealII, 470.lbm, 453.povray</cell></row><row><cell>Subset of Eight Programs</cell><cell>437.leslie3d, 454.calculix, 459.GemsFDTD,436.cactusADM, 447.dealII, 450.soplex, 470.lbm, 453.povray</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>List of representative input sets for SPEC CPU2006 programs.</figDesc><table><row><cell cols="2">CINT2006 benchmarks</cell><cell cols="2">464.h264avc -input set 2</cell></row><row><cell cols="2">400.perlbench -input set 1</cell><cell>473.astar</cell><cell>-input set 2</cell></row><row><cell>401.bzip2</cell><cell>-input set 4</cell><cell></cell></row><row><cell>403.gcc</cell><cell>-input set 1</cell><cell cols="2">CFP2006 benchmarks</cell></row><row><cell cols="2">445.gobmk -input set 5</cell><cell cols="2">416.gamess -input set 3</cell></row><row><cell cols="2">456.hmmer -input set 2</cell><cell cols="2">450.soplex -input set 1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 .</head><label>7</label><figDesc>Classification of programs based on application areas.</figDesc><table><row><cell>Application area</cell><cell></cell><cell>Benchmarks</cell></row><row><cell>Artificial Intelligence</cell><cell></cell><cell>458.sjeng, 445.gobmk,473.astar</cell></row><row><cell>Equation solver</cell><cell></cell><cell>436.cactusADM, 459.GemsFDTD</cell></row><row><cell>Fluid Dynamics</cell><cell></cell><cell>410.bwaves,434.zeusmp, 437.leslie3D, 470.lbm</cell></row><row><cell cols="2">Molecular Dynamics</cell><cell>435.gromacs, 444.namd</cell></row><row><cell>Quantum Chemistry</cell><cell></cell><cell>465.tonto, 416.gamess</cell></row><row><cell>Engineering</cell><cell>and</cell><cell>454.calculix,447.dealII,</cell></row><row><cell cols="2">Operational Research</cell><cell>450.soplex, 453.povray</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 .</head><label>8</label><figDesc>Sensitivity of Programs to Branch Misprediction Rate and L1 D-cache Miss-rate across five different platforms.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Branch Prediction</cell><cell></cell></row><row><cell>High</cell><cell cols="3">456.hmmer-1, 456.hmmer, 456.hmmer-2</cell><cell></cell></row><row><cell></cell><cell cols="3">471.omnetpp, 429.mcf, 473.astar-1, 473.astar,</cell><cell></cell></row><row><cell></cell><cell cols="3">464.h264ref-1, 473.astar-2, 400.perlbench-1,</cell><cell></cell></row><row><cell>Medium</cell><cell>401.bzip2-4,</cell><cell>462.libquantum,,</cell><cell>401.bzip2-3,</cell><cell>401.bzip2-2,</cell></row><row><cell></cell><cell cols="4">400.perlbench, 401.bzip2, 445.gobmk-3, 401.bzip2-1, 464.h264ref,</cell></row><row><cell></cell><cell cols="4">401.bzip2-5,, 403.gcc-8, 458.sjeng,401.bzip2-6, 403.gcc-4</cell></row><row><cell></cell><cell>464.h264ref-3,</cell><cell>445.gobmk,</cell><cell>445.gobmk-1,</cell><cell>445.gobmk-4,</cell></row><row><cell>Low</cell><cell cols="4">445.gobmk-2, 445.gobmk-5, 400.perlbench-2, 464.h264ref-2, 403.gcc-7, 403.gcc-6, 400.perlbench-3, 483.xalancbmk, 403.gcc-2,</cell></row><row><cell></cell><cell cols="3">403.gcc-5, 403.gcc-1, 403.gcc, 403.gcc-9, 403.gcc-3</cell><cell></cell></row><row><cell></cell><cell></cell><cell>L1 D-cache</cell><cell></cell><cell></cell></row><row><cell>High</cell><cell cols="4">462.libquantum, 464.h264ref-2, 464.h264ref-3, 464.h264ref, 456.hmmer-1</cell></row><row><cell>Medium</cell><cell cols="4">456.hmmer, 456.hmmer-2, 400.perlbench-2, 400.perlbench-3, 445.gobmk-3, 403.gcc-7</cell></row><row><cell></cell><cell cols="4">400.perlbench, 403.gcc-8, 483.xalancbmk, 473.astar-2, 403.gcc,</cell></row><row><cell></cell><cell cols="4">400.perlbench-1, 473.astar, 464.h264ref-1, 445.gobmk, 473.astar-</cell></row><row><cell></cell><cell cols="4">1, 445.gobmk-4, 471.omnetpp, 429.mcf, 403.gcc-9, 403.gcc-3,</cell></row><row><cell>Low</cell><cell cols="4">445.gobmk-2, 401.bzip2-3, 401.bzip2-5, 445.gobmk-1, 403.gcc-6,</cell></row><row><cell></cell><cell cols="4">403.gcc-5, 401.bzip2-2, 401.bzip2-6, 403.gcc-2, 403.gcc-1,</cell></row><row><cell></cell><cell cols="4">401.bzip2-1, 401.bzip2, 403.gcc-4, 401.bzip2-4, 445.gobmk-5,</cell></row><row><cell></cell><cell>458.sjeng</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="8.">ACKNOWLEDGEMENTS</head><p>The authors express their gratitude to the following SPEC member companies (<rs type="affiliation">IBM, Sun Microsystems, Intel, AMD, Apple, HP, SGI</rs>) for providing performance counter data from their systems. We enjoyed the opportunity to interact with the <rs type="institution">SPEC CPU Subcommittee</rs> and provide clustering analysis during selection of <rs type="grantNumber">CPU2006</rs> programs [12]. The researchers are supported in part by <rs type="funder">National Science Foundation</rs> under grant number <rs type="grantNumber">0429806</rs>. <rs type="person">Ajay Joshi</rs> is supported by an <rs type="funder">IBM</rs> fellowship.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_QY3kTfa">
					<idno type="grant-number">CPU2006</idno>
				</org>
				<org type="funding" xml:id="_ZUCsjkc">
					<idno type="grant-number">0429806</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX I -Subroutine Profile Summary of CPU2006</head><p>The columns show the information for the hottest subroutine, the cumulative data for the top 5,10 and 20 hot subroutine. Each of these columns has two sub-columns which show the cumulative percentage of dynamic instructions executed in the respective number of hot routines and the cumulative static count e.g. 22.3% of the dynamic instructions executed are from the hottest subroutine which has 140998 static instructions. Also, the top five hot subroutines account for 41.06% of dynamic instructions which have a total of 466157 static instructions. This data is generated using PIN <ref type="bibr" target="#b17">[17]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cumulative</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Alt</surname></persName>
		</author>
		<ptr target="http://cache-www.intel.com/cd/00/00/22/64/226491_226491.pdf" />
		<title level="m">Performance Modeling Using Compilers&quot; White paper Intel Corp</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">MisSPECulation: partial and misleading use of spec CPU2000 in computer architecture conferences</title>
		<author>
			<persName><forename type="first">D</forename><surname>Citron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th Annual International Symposium on Computer Architecture</title>
		<meeting>the 30th Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2003">June 9-11, 2003</date>
			<biblScope unit="page" from="52" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<author>
			<persName><forename type="first">D</forename><surname>Citron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hennessy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sohi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Use and Abuse of SPEC: An ISCA Panel</title>
		<imprint>
			<date type="published" when="2003-08">Jul/Aug, 2003</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="73" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Overview of the SPEC Benchmarks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Dixit</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
	<note>The Benchmark Handbook</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Using PAPI for hardware performance monitoring on Linux Systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dongarra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>London</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Terpstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Linux Clusters: The HPC Revolution</title>
		<imprint>
			<publisher>Linux Clusters Institute</publisher>
			<date type="published" when="2001-06">June 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Evolution and Evaluation of SPEC benchmarks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dujmovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Dujmovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGMETRICS Performance Evaluation Review</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2" to="9" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Principal Components Analysis</title>
		<author>
			<persName><forename type="first">G</forename><surname>Dunteman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Sage Publications</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Designing computer architecture research workloads</title>
		<author>
			<persName><forename type="first">L</forename><surname>Eeckhout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Vandierendonck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">De</forename><surname>Bosschere</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="65" to="71" />
			<date type="published" when="2003-02">Feb 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Quantifying the impact of input data sets on program behavior and its applications</title>
		<author>
			<persName><forename type="first">L</forename><surname>Eeckhout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Vandierendonck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">De</forename><surname>Bosschere</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Instruction Level Parallelism</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="33" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">SPEC as a Performance Evaluation Measure</title>
		<author>
			<persName><forename type="first">R</forename><surname>Giladi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ahituv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="33" to="42" />
			<date type="published" when="1995-08">Aug 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">SPEC CPU2000: Measuring CPU Performance in the New Millenium</title>
		<author>
			<persName><forename type="first">J</forename><surname>Henning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<date type="published" when="2000-07">July 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Performance Counters and Development of SPEC CPU</title>
		<author>
			<persName><forename type="first">J</forename><surname>Henning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Architecture News</title>
		<imprint>
			<date type="published" when="2006-03">2006. March 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Workload Characterization: Motivation, Goals and Methodology</title>
		<author>
			<persName><forename type="first">L</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sabarinathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workload Characterization: Methodology and Case Studies</title>
		<editor>
			<persName><forename type="first">L</forename><surname>John</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">M G</forename><surname>Maynard</surname></persName>
		</editor>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1998-11">November 1998</date>
			<biblScope unit="page" from="3" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Program Balance and its impact on High Performance RISC Architecture</title>
		<author>
			<persName><forename type="first">L</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hulina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Coraor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Symposium on High Perf Comp Arch</title>
		<meeting>of the International Symposium on High Perf Comp Arch</meeting>
		<imprint>
			<date type="published" when="1995-01">Jan 1995</date>
			<biblScope unit="page" from="370" to="379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Measuring Benchmark Characteristics Using Inherent Program Characteristics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Phansalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Eeckhout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>John</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="769" to="782" />
			<date type="published" when="2006">Jun2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Choosing Representative Slices of Program Execution for Microarchitecture Simulations: A Preliminary Application to the Data Stream</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lafage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Seznec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Workload Characterization (WWC-2000)</title>
		<imprint>
			<date type="published" when="2000-09">Sept 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">PIN:Building Customized Program Analysis Tools with Dynamic Instrumentation</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Luk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Muth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Klauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lowney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hazelwood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2005 ACM SIPLAN Conference on Programming Language Design and Implementation</title>
		<meeting>2005 ACM SIPLAN Conference on Programming Language Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="190" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Mcghan</surname></persName>
		</author>
		<title level="m">SPEC CPU2006 Benchmark Suite, Microprocessor Report</title>
		<imprint>
			<date type="published" when="2006-10-10">October 10, 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Measuring Program Similarity: Experiments with SPEC CPU Benchmark Suites</title>
		<author>
			<persName><forename type="first">A</forename><surname>Phansalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Eeckhout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>John</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Performance Analysis of Systems and Software</title>
		<imprint>
			<date type="published" when="2005-03">March 2005</date>
			<biblScope unit="page" from="10" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Four Generations of SPEC CPU Benchmarks: What has changed and what has not?</title>
		<author>
			<persName><forename type="first">A</forename><surname>Phansalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Eeckhout</surname></persName>
		</author>
		<author>
			<persName><surname>John</surname></persName>
		</author>
		<idno>TR-041026-1</idno>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
		<respStmt>
			<orgName>Laboratory of Computer Architecture, The University of Texas at Austin</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Reilly</surname></persName>
		</author>
		<ptr target="http://www.iiswc.org/iiswc2006/IISWC2006S2.1.pdf" />
		<title level="m">IEEE International Symposium on Workload Characterization</title>
		<imprint>
			<date type="published" when="2006-10">Oct 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Automatically Characterizing Large Scale Program Behavior</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sherwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Perelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hamerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Architecture Support for Programming Languages and Operating Systems</title>
		<meeting>of International Conference on Architecture Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="45" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Many Benchmarks Stress the Same Bottlenecks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Vandierendonck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bosschere</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Workshop on Computer Architecture Evaluation using Commerical Workloads (CAECW-7)</title>
		<meeting>of the Workshop on Computer Architecture Evaluation using Commerical Workloads (CAECW-7)</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="57" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An Overview of Common Benchmarks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Weicker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<biblScope unit="page" from="65" to="75" />
			<date type="published" when="1990-12">Dec 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Applying SMARTS to SPEC CPU2000</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wunderlich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoe</surname></persName>
		</author>
		<idno>2003-1</idno>
		<imprint>
			<date type="published" when="2003-06">June 2003</date>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">CALCM Technical Report</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Simulation of Computer Architectures: Simulators, Benchmarks, Methodologies, and Recommendations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lilja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="268" to="280" />
			<date type="published" when="2006-03">Mar. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sendag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Eeckhout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lilja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>John</surname></persName>
		</author>
		<title level="m">Evaluating Benchmark Subsetting Approaches&quot; International Symposium on Workload Characterization</title>
		<imprint>
			<date type="published" when="2006-10">October 2006</date>
			<biblScope unit="page" from="93" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A Statistically Rigorous Approach for Improving Simulation Methodology</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lilja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hawkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Intl Conf on High Performance Computer Architecture</title>
		<meeting>of Intl Conf on High Performance Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2003-02">Feb 2003</date>
			<biblScope unit="page" from="281" to="291" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
