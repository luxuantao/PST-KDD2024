<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Learned Performance Model for the Tensor Processing Unit</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-08-03">3 Aug 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Samuel</forename><forename type="middle">J</forename><surname>Kaufman</surname></persName>
							<email>kaufmans@cs.washington.edu</email>
						</author>
						<author>
							<persName><forename type="first">Phitchaya</forename><forename type="middle">Mangpo</forename><surname>Phothilimthana</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
							<email>yanqiz@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Mike</forename><surname>Burrows</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Paul G. Allen School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Google Research Mountain View</orgName>
								<address>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Google Research Mountain View</orgName>
								<address>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Google Research Mountain View</orgName>
								<address>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Learned Performance Model for the Tensor Processing Unit</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-08-03">3 Aug 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2008.01040v1[cs.PF]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>data/model parallelism -layout assignment -operation fusion -remeteralization -memory assignment -tiling etc random</term>
					<term>genetic</term>
					<term>simulated annealing</term>
					<term>MCMC</term>
					<term>MCTS</term>
					<term>RL</term>
					<term>etc</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Accurate hardware performance models are critical to efficient code generation. They can be used by compilers to make heuristic decisions, by superoptimizers as an minimization objective, or by autotuners to find an optimal configuration of a specific program. However, they are difficult to develop because contemporary processors are complex, and the recent proliferation of deep learning accelerators has increased the development burden. We demonstrate a method of learning performance models from a corpus of tensor computation graph programs for the Tensor Processing Unit (TPU). We train a neural network over kernel-level sub-graphs from the corpus and find that the learned model is competitive to a heavily-optimized analytical cost model used in the production XLA compiler. * Work done at Google Research. 2 An autotuner automatically searches a space of configurations of a program, and selects the best-performing configuration according to a performance metric, such as execution time, throughput, or power consumption.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Performance models are used in compiler optimizations <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b10">11]</ref>, reinforcement learning <ref type="bibr" target="#b19">[20]</ref>, and Neural Architecture Search (NAS) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b12">13]</ref>. A performance model is particularly useful for compiler optimizations because collecting performance numbers (e.g., execution time) from a real machine can be expensive or infeasible, such as during ahead-of-time compilation when we have no access to the target hardware. For example, LLVM's loop vectorizer uses a performance model to compute the optimal vectorization and unroll factors <ref type="bibr" target="#b17">[18]</ref>. GCC's auto-vectorizer uses a performance model to decide when to apply loop-peeling, loop-versioning, outer-loop vectorization, and intra-iteration vectorization <ref type="bibr" target="#b10">[11]</ref>. In addition, a performance model can be used by a compiler autotuner 2 as a faster alternative to evaluate candidate configurations in a search space <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21]</ref>.</p><p>It is challenging and time-consuming to develop an accurate analytical performance model to predict program performance metrics, such as the execution time, on a modern processor. Program performance is tightly coupled with the underlying complex processor architecture as well as the performance-affecting decisions that are made during compilation <ref type="bibr" target="#b2">[3]</ref>. However, a performance model often lacks an in-depth view of the processor architecture and low-level generated code. The recent proliferation of deep learning accelerators has only exacerbated this problem as it demands rapid, repeated development of performance models targeting new accelerators.</p><p>Figure <ref type="figure">1:</ref> The compiler autotuner typically relies on real hardware to evaluate the performance of generated code. We propose a learned performance model as a cheaper alternative to obtain reward signals without using the hardware.</p><p>In this paper, we propose to automatically learn a performance model to predict execution time of tensor programs on a Tensor Processing Unit (TPU), an accelerator for machine learning workloads <ref type="bibr" target="#b16">[17]</ref>. Similar to prior work <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b18">19]</ref>, we formulate the runtime estimation problem as a regression task. Our approach extracts features directly from an unmodified program representation, minimizing manual effort to develop; in contrast, Halide's learned performance model requires additional performance counters generated from a static analyzer <ref type="bibr" target="#b1">[2]</ref>. Note that tensor programs contain complex, multi-level nested loops, whose runtimes are harder to predict than those of loop-free instruction sequences, such as those tackled by Ithemal <ref type="bibr" target="#b18">[19]</ref>. To represent a tensor program naturally and generalize the performance model to unseen programs, we use a graph-based neural network. We show that our model generalizes relatively well to unseen tensor kernels and is retargetable to different optimization tasks.</p><p>We evaluate our performance model on predicting runtimes for two different tensor compiler optimization tasks -operator fusion and tile size selection -on a TPU. The trained performance model is applied to evaluate program configurations generated by an autotuner for the Accelerated Linear Algebra (XLA) compiler <ref type="bibr" target="#b22">[23]</ref>, in replacement of the real hardware, as depicted in Fig. <ref type="figure">1</ref>.</p><p>In summary, this paper presents the following contributions:</p><p>? We develop the first learned performance model for tensor programs, which contain multilevel nested loops, that (1) is generalized to unseen programs, (2) is retargetable for different compiler optimization tasks, and (3) does not rely on any additional static analysis.</p><p>? We show better generalization results using a graph neural network. The test accuracy on the fusion optimization task is 13% and 10% better than a sequential model's and the hand-tuned analytical model built into the compiler's respectively.</p><p>? We integrate our learned performance model into an XLA fusion autotuner, and demonstrate that when access to real hardware accelerators is limited, the performance model helps the autotuner discover fusion configs that are up to 19% faster than the default configs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Ithemal uses a hierarchical recurrent neural network to estimate the throughput of x86-64 basic blocks running on highly complex processors <ref type="bibr" target="#b18">[19]</ref>. Basic blocks are relatively short, loop-free sequences of instructions (6.06 instructions on average). In contrast, our work addresses larger machine learning kernels with implicit nested loops (which are represented naturally as graphs), containing up to a thousand operators. Ithemal was evaluated on its ability to generalize to held-out basic blocks. However, our method is tested for its ability to generalize to wholly novel tensor programs and targeting a drastically different processor.</p><p>Both the code-feature-based performance model <ref type="bibr" target="#b6">[7]</ref> and Halide's performance model <ref type="bibr" target="#b1">[2]</ref> use simple neural nets to predict runtime from manually-engineered features produced by a static analyzer that examines an optimized program. Since extracting these features from an XLA graph is non-trivial, we train a more complex neural net, using features that can be extracted directly from the XLA graph, with sufficient capacity to recover similarly powerful representations.</p><p>AutoTVM also uses a learned performance model to optimize tensor kernels, by ranking candidates <ref type="bibr" target="#b4">[5]</ref>. However, AutoTVM's model shows limited ability to generalize between kernels and is trained for per-kernel search over a kernel-specific set of parameters. In contrast, our model can be used to both estimate the runtime of an entire tensor program and rank configuration candidates per kernel, and our model generalizes to novel kernels and applications.</p><p>Additionally, approaches to NAS often employ a closely related idea, learning models to predict the error or error curve of an deep learning model architecture <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b23">24]</ref>. Others, such as ReNAS <ref type="bibr" target="#b24">[25]</ref>, learn to rank sets of candidate neural architectures rather than predict runtimes in isolation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Target Tasks and Hardware</head><p>Our goal is to predict runtime of XLA programs on a TPU. XLA -a machine learning compiler for multiple hardware targets -is used as a backend for various machine learning programming frameworks, including TensorFlow <ref type="bibr" target="#b0">[1]</ref>, PyTorch <ref type="bibr" target="#b21">[22]</ref>, and JAX <ref type="bibr" target="#b9">[10]</ref>. An XLA program consists of basic blocks, called computations; loop bodies and conditions in a computation are represented as pointers to other computations. Each computation is represented by a directed acyclic graph called a computation graph. A node in a computation graph represents a tensor operation, processing one or more input tensors into a single output. An edge connects an output tensor from one node to an input tensor of another node. In this paper, we apply a performance model to two specific optimization tasks -operator fusion (program-level) and tile-size selection (kernel-level) -for XLA programs running on the TPU v2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Operator Fusion</head><p>Operator fusion is an important program-level optimization that merges multiple operations into a single unit. Before this pass, a node in a computation graph is a single primitive tensor operation (e.g. convolution, element-wise add, etc). When two producer-consumer ops are fused, the intermediate data is immediately consumed by the consumer, without the need to perform read and write transactions with main memory, thereby reducing data communication. After the fusion pass, a node in a computation graph is either a single primitive op or a fused op with many primitive ops. In this paper, we call a node in an optimized computation graph a kernel, as illustrated in Fig. <ref type="figure">2</ref>.</p><p>We have developed a fusion autotuner that searches for the fastest fusion configuration of an XLA program. It has found up to 15% speedup on some production deep learning models, but the autotuning process is slow, with most of its time spent compiling and executing programs on the TPU. The search space is also extremely large, containing up to 2 40,000 configuration candidates, so we need a fast mechanism to evaluate as many candidates as possible within a time budget. Therefore, we propose using a learned performance model to reduce evaluation time on real hardware. Currently, there is no manual performance model built for this task in XLA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Tile-Size Selection</head><p>Tile-size selection is a performance-critical, kernel-level optimization. The goal is to select an optimal tile size for a kernel's output tensor that fits in the fast scratchpad memory; one tile is computed at a time and copied to the slower main memory before the next tile is computed. The number of valid tile sizes ranges from two to 500,000 depending on the kernel. XLA selects the tile size based on a manually-written analytical performance model. This model is extremely complex, taking several person-years to develop. Ultimately, we would like to replace this manual performance model with the learned performance model, demonstrating a new, less costly way of developing compilers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Hardware Accelerator</head><p>Our performance model is developed for the TPU, a fast, energy-efficient deep learning accelerator. Its architecture is in some ways simpler and in others more complex than modern general-purpose processors like x86. It has no out-of-order execution, hardware caching, or virtual memory. However, it incorporates a VLIW instruction set, 2D registers, a matrix multiplication unit, and a cross-lane unit. This TPU does not support multi-threading; one kernel is executed at a time, reading from and writing to main memory at start and termination respectively. Thus, we can compute the total runtime of an entire program by summing the runtimes of its kernel executions. This approach of estimating the total program runtime from kernels' runtimes can be applied to many accelerators; prior work has shown that this technique is sufficiently accurate for graph rewrites <ref type="bibr" target="#b14">[15]</ref> and parallelization configurations autotuning <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b20">21]</ref> on GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Method</head><p>Our approach first decomposes a XLA program into kernels and formulates the kernel runtime estimation problem as a regression task. We can then compute the program's total runtime by summing the kernel runtimes. This approach confers two benefits. First, this simple decomposition remains general enough that we can apply the neural network model to various tasks, including both whole-program optimizations and kernel-level optimizations. Second, it introduces a restriction consistent with how a compiler transforms a high-level program into a set of optimized kernels, reducing the size of the graphs for which our model will be trained to produce embeddings by orders of magnitude, therefore improving the sample-parameter ratio and at no cost. This improves our model's ability to generalize to unseen programs.</p><p>The rest of this section focuses on our neural network model that predicts the execution time of each individual kernel. For the purpose of predicting cost, we represent a kernel as a directed graph with nodes corresponding to primitive operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Model Architecture</head><p>Figure <ref type="figure">3</ref> depicts the architecture of our performance model for predicting the execution time (y ) of a kernel. Inputs to the model are opcodes (x o ), non-opcode features of the ops (X f ), and a directed adjacency matrix (A) that captures the connections of ops in the kernel. A row of X f includes attributes extracted from an XLA program representation, such as an output tensor shape, tensor layout, striding, padding, tile size, and where applicable, convolution filter size. Kernel inputs are expressed by nodes with the parameter opcode, and outputs are expressed via an extra feature associated with the output nodes. The opcode (x o i ) of an operation i is embedded into a vector of floats via a simple embedding lookup table. An op's features occupy a fixed region of the X f i vector.</p><p>Neighborhood Embedding We use a single feedforward layer f 1 followed by GraphSAGE <ref type="bibr" target="#b11">[12]</ref> (without edge sampling) to combine information from the opcode, the op's features, and the graph structure to generate the node's embedding. The embedding of node i considering k-hop neighbors can be computed as follows:</p><formula xml:id="formula_0">? k i = l 2 f k 3 concat ? k-1 i , j?neighbors(i) f k 2 (? k-1 j when k &gt; 0; ? 0 i = f 1 (X i )<label>(1)</label></formula><p>where f k 2...3 denote feedforward layers specific to depth k. l 2 denotes L2 normalization. neighbors(i) is a set of immediate neighbors of node i.</p><p>is a reduction chosen during hyperparameter search.</p><p>We employ GraphSAGE because (i) a tensor computation kernel is naturally represented as a graph, and (ii) learning node representations conditioned only on their own features and local neighborhoods has shown to improve generalization. In our setting, we expect the effect of most ops to be determined by their own properties and nearby ops'. These are the sorts of features that can be learned from neighborhood, so our choice of model encourages an inductive bias toward mostly local contributions.</p><p>Kernel Embedding Once we have node embeddings ? k , we create the embedding (?) of the kernel by computing sum, mean, and max over rows of ? k . Then, we pass ?, the concatenation of a combination of the sum, mean, and max vectors into the final feedforward layer (without activation), to produce the estimated execution time (y ) of the kernel; the exact combination of sum, mean, and max vectors is tuned via hyperparameter search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Objectives &amp; Training</head><p>Shared Architecture For both fusion and tile-size selection tasks, we use the same neural net model architecture and node features X f i , which include a tile size feature of the kernel the node belongs to. We represent the tile size feature as a fixed-length sub-vector, in which elements are the sizes of a tile from minor to major dimensions, ending with their sum and product; including the product of all dimensions' sizes is crucial as it represents the volume of the tensor.</p><p>Fusion Task In this task, we would like the neural network to predict kernel runtimes in an absolute unit (nanoseconds) so that we can use the predictions to compute total program runtime. Thus, we train the neutral network model using the common squared error loss, (y i -y i ) 2 , against log-transformed targets. We apply log transformation because targets vary widely, ranging from a nanosecond to a second. Using this loss function, our model is biased towards fitting long-running kernels more than short-running kernels. This is desirable because small kernels do not contribute much to overall program runtime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tile-Size Selection Task</head><p>In this task, we are interested in the relative speed between different tile sizes within each kernel. Therefore, the performance model does not need to predict runtime, but instead should be able to rank tile sizes by speed within each kernel. With this intuition, we train the model with a pairwise rank loss <ref type="bibr" target="#b3">[4]</ref>:</p><formula xml:id="formula_1">L = n i=1 n j=1 ?(y i -y j ) ? pos(y i -y j ) n ? (n -1)/2 (2)</formula><p>where n is the number of samples in each batch; pos(z) is 1 if z &gt; 0, or 0 otherwise; ?(z) is either the hinge function (1 -z) + or logistic function log(1 + e -z ), tuned via hyperparameter search. With this loss function, we modify our batching mechanism by grouping samples of different tile sizes of the same kernel into the same batch.</p><p>Alternatively, we can use the same MSE loss as in the fusion task, but weight a loss value of each sample appropriately so that the model is optimized for all kernels equally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Dataset</head><p>Our dataset consists of computation graphs from 104 XLA programs that implement either production models or common models used in research.</p><p>Fusion Dataset We run our fusion autotuner with a random search strategy to generate 50,000 fusion configurations or until timeout (four hours using 50 machines) for each input computation graph. The graphs are then decomposed according to these fusion configurations, yielding 207 million fused kernels (examples) after duplicate elimination. We observe that program runtimes differ by no more than 4% between runs on the TPU. To improve stability, we execute each kernel 3 times, then interpret the minimum runtime as our targets. Examples in this dataset are heavily skewed.</p><p>Approximately half have runtimes below 5?s, but they contribute little to total program runtimes, so the kernels that take at least 5?s are of more interest. Tile-Size Dataset We compile each XLA program using the compiler's default fusion heuristics, obtaining an optimized computation graph that we decompose into kernels. For each kernel, we query the compiler for a list of valid tile sizes. The target for each kernel/tile-size pair (example) is the minimum runtime from three runs. A kernel may have as many as 500,000 valid tile sizes, so we measure runtimes for as many as possible for each kernel within 30 minutes across 50 machines.</p><p>Dataset Splitting We estimate our approach's ability to generalize in two ways, corresponding to two separate test splits: one split where held-out test programs were chosen randomly, and another where held-out test programs were manually chosen to minimize their (subjective) similarity to programs in the training set. See Table <ref type="table" target="#tab_0">1</ref> for relevant statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluation</head><p>In this section, we show that our learned performance model is comparable to the manually-written model used in XLA for TPU: 10% more accurate on the fusion dataset (Section 6.1) while performing slightly worse on the tile-size dataset (Section 6.2). Additionally, we integrated the model into the XLA fusion autotuner, and show that it can help the autotuner discover faster programs when access to real hardware accelerators is limited (Section 6.3).</p><p>For all experiments, we trained our models on a single NVidia V100 instance, 96GB of RAM, with 10 CPU cores for data processing. For all the learned models, we did a hyperparameter search (presented in Supplementary Material) and selected the best-performing models on the validation split.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Fusion Task Accuracy</head><p>To understand the accuracy of our proposed performance model, we compare mean absolute percentage error (MAPE) and rank correlation between our model and two baselines. We compare our approach's ability to generalize to novel programs against both an existing analytical performance model and an LSTM baseline. We run separate experiments, including separate hyperparameter searches, for each dataset split described in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analytical Baseline</head><p>The XLA compiler backend has a mature analytical performance model that estimates the execution time of a kernel on a TPU, as described in Section 3.2. However, this analytical model was not intended for predicting the runtime of an entire computation graph, so estimated costs of different types of kernels (e.g., fused kernels with and without convolutions) are in different scales. Hence, we map the model's output to an estimated runtime by scaling with a coefficient associated with the kernel's type. Coefficients are determined by executing each program in the test set on the real hardware target with a default fusion configuration, and dividing the actual total runtime for all kernels of each type by the estimate in its original scale. <ref type="foot" target="#foot_0">3</ref>LSTM Baseline Prior work proposes an LSTM-based performance model for x86 basic blocks <ref type="bibr" target="#b18">[19]</ref>.</p><p>To understand the effect of representing program examples as graphs rather than sequences, we compare our proposed graph neural network to an LSTM trained over topologically sorted sequences of nodes, whose embeddings are the same per-node representations used in our proposed model. split. Our proposed model substantially outperforms both an LSTM baseline and the analytical model both in terms of predicting absolute performance and in terms of rank correlation. On kernels with &lt;5?s runtimes, results are similar in terms of runtime predictions, while our model shows higher correlation; our model, the LSTM baseline, and the analytical model have median MAPEs of 8.4, 12.1, and 21.0 and median Kendall's ? coefficients of .82, .82, and .71 respectively. All models, including the analytical performance model, perform poorly on at least one application when evaluating on ?5?s kernels. This shows that the tensor programs vary widely, and it is challenging to build a perfect performance model for all programs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As seen in</head><p>On the harder task -programs which were chosen deliberately to have the least similarity to training programs -the comparison between our model and the analytical model is less conclusive but competitive. On kernels with runtimes ?5?s, our model, the LSTM baseline, and the analytical model have median MAPEs of 31.8, 40.0, and 12.6 respectively, while their median Kendall's ? coefficients are .71, .70, and .92.</p><p>Overall, these results suggest that our learned performance model is competitive with the analytical baseline. This motivates our later experiments in applying it to a downstream task (in Section 6.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Tile-Size Task Accuracy</head><p>In this experiment, we drop the LSTM baseline as it is inferior to the graph-based model for our application domain. We train our model with two different loss functions -MSE and rank loss -as explained in Section 4.2, and compare our model against the same analytical model. In this task, we are interested in only relative runtimes between different tile sizes within each kernel. Thus, we measure the models' accuracy only on the Kendall correlation, and not MAPE. We compute the correlation between targets and predictions of tile-size runtimes within each kernel, and then compute the average over all kernels in each program. Recall that the analytical model is developed specifically for this task, and we do not need to predict runtime in nanoseconds; as a result, the scaling coefficients used in the fusion task are no longer needed.</p><p>Table <ref type="table" target="#tab_3">3</ref> displays the result on the random dataset split. Our best learned performance model (trained using pairwise rank loss) performs slightly worse than the analytical performance model: .07 lower correlation; on the harder split: .16 lower correlation. Regarding the loss function, we found that the model trained using the pairwise rank loss performs better than with MSE; .04 and .13 higher correlation on the random and hard splits respectively. This result confirms our intuition that training a model to predict relative speeds is easier than absolute runtimes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Fusion Autotuner Integration</head><p>We integrate the best learned performance model from Section 6.1 in the XLA fusion autotuner. We modify the autotuner to support evaluating fusion configs by either executing generated kernels on real hardware or estimating their runtimes using the learned model, running prediction on a CPU. The analytical model is not used in this experiment because it cannot estimate runtimes for kernels that do not have tile-size options; kernels that are not fusion, convolution, or data formatting operations.</p><p>Experiment Setup. Since our target hardware is in demand and more scarce than CPUs, we aim to minimize the time we use the accelerators during autotuning. Hence, we limit the time to use the accelerators to five minutes in our experiment setup. We run simulated annealing search using the learned performance model (from Section 6.1) for one hour on a CPU. After that, we run as many top fusion configs in the order ranked by the predicted costs on the real hardware within the five-minute time limit. The baseline is the original autotuner, which uses only the real hardware to evaluate fusion configs, running for five minutes. We run the autotuner in two modes: starting the search from (i) a default config and (ii) a random config. A default config is the configuration generated by the default heuristic algorithm in the compiler given a specific program.</p><p>In this experiment, we run the autotuner on a set of programs that gain significant speedup from autotuning according to our prior data. Although some programs (Transformer, Char2Feats, and ResNet-parallel) are in our training set, most kernels are not because our training data is not generated from the simulated annealing search starting from a default fusion configuration.</p><p>Result. We run the autotuner on each program 20 times and report the best speedup found over the default configuration in Fig. <ref type="figure" target="#fig_1">4a</ref>. Using the learned performance model together with the hardware, we are able to discover fusion configurations that are on average 2% faster than using the hardware alone, and they are on average only 1% slower than the best known configurations found when running the autotuner on hardware for four hours. When running simulated annealing starting from a random configuration (Fig. <ref type="figure" target="#fig_1">4b</ref>), the benefit from the performance model is even more pronounced. On average, using the performance model led to discovering 8% faster configurations compared to not using the performance model. This result demonstrates that the learned performance model can indeed help generate faster code in practice when an access to a hardware target is limited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We have presented first steps toward learning a performance model for tensor programs. We have found that a model trained on our corpus of research and production models can generalize well to programs with some similarity to our training set, usually matching or improving upon the performance of the best known analytical baseline for our target hardware, and performs acceptably well on programs which differ substantially. When evaluating on the task for which the analytical model is heavily-optimized (e.g. tile-size selection), our learned model is slightly worse. However, while the learned cost model is less accurate, its requires much less effort to develop. Finally, we demonstrated that the learned cost model can be employed by an autotuner to discover faster tensor programs than using hardware targets alone when hardware access is limited.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: Optimized XLA graph, in which each node (called kernel) in turn contains a graph of primitive op(s).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Runtime speedup found by autotuning with and without the learned performance model over the default heuristic configuration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The number of unique programs and kernels in the fusion and tile-size datasets. M = million.</figDesc><table><row><cell></cell><cell>Manual Split</cell><cell></cell><cell></cell><cell></cell><cell>Random Split</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Programs</cell><cell></cell><cell>Kernels</cell><cell></cell><cell>Programs</cell><cell></cell><cell>Kernels</cell><cell></cell></row><row><cell>Split</cell><cell cols="2">Fusion Tile-Size</cell><cell cols="4">Fusion Tile-Size Fusion Tile-Size</cell><cell cols="2">Fusion Tile-Size</cell></row><row><cell>Train</cell><cell>79</cell><cell cols="2">92 198.6M</cell><cell>23.0M</cell><cell>78</cell><cell cols="2">93 157.5M</cell><cell>21.8M</cell></row><row><cell>Val.</cell><cell>6</cell><cell>6</cell><cell>2.6M</cell><cell>.8M</cell><cell>8</cell><cell>8</cell><cell>30.1M</cell><cell>1.6M</cell></row><row><cell>Test</cell><cell>6</cell><cell>6</cell><cell>6.1M</cell><cell>.5M</cell><cell>8</cell><cell>8</cell><cell>20.3M</cell><cell>1.4M</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 ,</head><label>2</label><figDesc>our model, the LSTM baseline, and the analytical model have median MAPE of 13.9, 26.6, and 23.9 on longer-running kernels when considering the random dataset</figDesc><table><row><cell></cell><cell>MAPE</cell><cell></cell><cell></cell><cell>Kendall's ?</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="6">Our Model LSTM Analytical Our Model LSTM Analytical</cell></row><row><cell>ConvDRAW</cell><cell>42.6</cell><cell>62.3</cell><cell>21.6</cell><cell>0.65</cell><cell>0.52</cell><cell>0.77</cell></row><row><cell>WaveRNN</cell><cell>8.5</cell><cell>29.7</cell><cell>322.9</cell><cell>0.91</cell><cell>0.73</cell><cell>0.70</cell></row><row><cell>NMT Model</cell><cell>17.0</cell><cell>68.9</cell><cell>26.3</cell><cell>0.91</cell><cell>0.82</cell><cell>0.91</cell></row><row><cell>SSD</cell><cell>24.1</cell><cell>49.3</cell><cell>55.9</cell><cell>0.86</cell><cell>0.70</cell><cell>0.76</cell></row><row><cell>RNN</cell><cell>15.5</cell><cell>23.5</cell><cell>20.5</cell><cell>0.91</cell><cell>0.85</cell><cell>0.86</cell></row><row><cell>ResNet v1</cell><cell>7.2</cell><cell>14.4</cell><cell>11.5</cell><cell>0.91</cell><cell>0.84</cell><cell>0.88</cell></row><row><cell>ResNet v2</cell><cell>6.7</cell><cell>14.3</cell><cell>13.3</cell><cell>0.90</cell><cell>0.83</cell><cell>0.86</cell></row><row><cell>Translate</cell><cell>12.3</cell><cell>22.4</cell><cell>27.2</cell><cell>0.83</cell><cell>0.81</cell><cell>0.74</cell></row><row><cell>Median</cell><cell>13.9</cell><cell>26.6</cell><cell>23.9</cell><cell>0.90</cell><cell>0.81</cell><cell>0.81</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Fusion dataset: Mean absolute percentage error of predicted kernel runtimes for kernels with ?5?s true runtimes, which account for the majority of total runtime in our programs, on the random-split test set.</figDesc><table><row><cell></cell><cell cols="3">Our Model (Rank Loss) Our Model (MSE Loss) Analytical</cell></row><row><cell>ConvDRAW</cell><cell>0.64</cell><cell>0.66</cell><cell>0.79</cell></row><row><cell>WaveRNN</cell><cell>0.46</cell><cell>0.56</cell><cell>0.65</cell></row><row><cell>NMT Model</cell><cell>0.74</cell><cell>0.66</cell><cell>0.81</cell></row><row><cell>SSD</cell><cell>0.64</cell><cell>0.60</cell><cell>0.77</cell></row><row><cell>RNN</cell><cell>0.42</cell><cell>0.37</cell><cell>0.55</cell></row><row><cell>ResNet v1</cell><cell>0.72</cell><cell>0.67</cell><cell>0.73</cell></row><row><cell>ResNet v2</cell><cell>0.74</cell><cell>0.69</cell><cell>0.73</cell></row><row><cell>Translate</cell><cell>0.76</cell><cell>0.62</cell><cell>0.92</cell></row><row><cell>Median</cell><cell>0.68</cell><cell>0.64</cell><cell>0.75</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Tile-size dataset: Mean Kendall's tau between targets and predictions within each kernel, on all applications in the random-split test set.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>The analytical performance model does not support kernels without tile-size options, which account for 1% of kernels in our dataset. We ignore these kernels in our comparisons in Section 6.1.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments and Disclosure of Funding</head><p>We would like to thank the XLA team, especially <rs type="person">Amit Sabne</rs> and <rs type="person">Bjarke Roune</rs>, on feedback and helps while developing this project, <rs type="person">Hyeontaek Lim</rs> on code review, <rs type="person">Sudip Roy</rs> on paper feedback, and <rs type="person">Rishabh Singh</rs> on occasional guidance.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">TensorFlow: A System for Large-Scale Machine Learning</title>
		<author>
			<persName><forename type="first">Mart?n</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherry</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pete</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqiang</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th USENIX Symposium on Operating Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning to Optimize Halide with Tree Search and Random Programs</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karima</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Riyadh</forename><surname>Baghdadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tzu-Mao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Micha?l</forename><surname>Gharbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kayvon</forename><surname>Fatahalian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fr?do</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ragan-Kelley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2019-07">July 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Chaos in Computer Performance</title>
		<author>
			<persName><forename type="first">Hugues</forename><surname>Berry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gracia P?rez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Temam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chaos</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">13110</biblScope>
			<date type="published" when="2006">2006</date>
			<pubPlace>Woodbury, N.Y.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning to rank using gradient descent</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erin</forename><surname>Renshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Lazier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Deeds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicole</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Hullender</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Machine Learning, ICML &apos;05</title>
		<meeting>the 22nd International Conference on Machine Learning, ICML &apos;05<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning to Optimize Tensor Programs</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eddie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziheng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Krishnamurthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32Nd International Conference on Neural Information Processing Systems, NIPS&apos;18</title>
		<meeting>the 32Nd International Conference on Neural Information Processing Systems, NIPS&apos;18</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Peephole: Predicting network performance before training</title>
		<author>
			<persName><forename type="first">Boyang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<idno>CoRR, abs/1712.03351</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast Compiler Optimisation Evaluation Using Code-feature Based Performance Prediction</title>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Dubach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Cavazos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bj?rn</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grigori</forename><surname>Fursin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">P</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>O'boyle</surname></persName>
		</author>
		<author>
			<persName><surname>Temam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Conference on Computing Frontiers, CF &apos;07</title>
		<meeting>the 4th International Conference on Computing Frontiers, CF &apos;07</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Efficient Multi-objective Neural Architecture Search via Lamarckian Evolution</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Hendrik Metzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.09081</idno>
		<imprint>
			<date type="published" when="2018-04">Apr 2018</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Hendrik Metzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.05377</idno>
		<title level="m">Neural Architecture Search: A Survey</title>
		<imprint>
			<date type="published" when="2018-08">Aug 2018</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Compiling machine learning programs via high-level tracing</title>
		<author>
			<persName><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>James Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Leary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd SysML Conference</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Auto-Vectorization in GCC</title>
		<author>
			<persName><surname>Gcc</surname></persName>
		</author>
		<ptr target="https://www.gnu.org/software/gcc/projects/tree-ssa/vectorization.html" />
		<imprint>
			<date type="published" when="2019-08">August 2019</date>
		</imprint>
	</monogr>
	<note>last modified 18-August-2019</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Chi-Hung</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu-Huan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jhao-Hong</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsin-Ping</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chun-Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shih-Chieh</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia-Yu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.10332</idno>
		<title level="m">MONAS: Multi-Objective Neural Architecture Search using Reinforcement Learning</title>
		<imprint>
			<date type="published" when="2018-06">Jun 2018</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Tapas: Train-less accuracy predictor for architecture search</title>
		<author>
			<persName><forename type="first">R</forename><surname>Istrate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Scheidegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mariani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nikolopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bekas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C I</forename><surname>Malossi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Optimizing dnn computation with relaxed graph substitutions</title>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Warszawski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Aiken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd SysML Conference</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Beyond data and model parallelism for deep neural networks</title>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Aiken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd SysML Conference</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Norman</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cliff</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nishant</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raminder</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suresh</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Boden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Al</forename><surname>Borchers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rick</forename><surname>Boyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Luc</forename><surname>Cantin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clifford</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Coriell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Daley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Dau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Gelb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tara</forename><surname>Vazir Ghaemmaghami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajendra</forename><surname>Gottipati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Gulland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Richard</forename><surname>Robert Hagmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Hogberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hundt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Hurt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alek</forename><surname>Jaffey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Jaworski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harshit</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khaitan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Killebrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Lacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Laudon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diemthu</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuyuan</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Lucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gordon</forename><surname>Lundin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Mackean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maire</forename><surname>Maggiore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kieran</forename><surname>Mahony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ray</forename><surname>Narayanaswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathy</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Nix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Norrie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Narayana</forename><surname>Omernick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Penukonda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Phelps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emad</forename><surname>Salek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Samadiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Severn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Sizikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jed</forename><surname>Snelham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Souter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Steinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mercedes</forename><surname>Swing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Thorson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horia</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erick</forename><surname>Toma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Tuttle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walter</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doe</forename><surname>Wilcox</surname></persName>
		</author>
		<author>
			<persName><surname>Hyun Yoon</surname></persName>
		</author>
		<title level="m">Proceedings of the 44th Annual International Symposium on Computer Architecture, ISCA &apos;17</title>
		<meeting>the 44th Annual International Symposium on Computer Architecture, ISCA &apos;17</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Auto-Vectorization in LLVM</title>
		<author>
			<persName><surname>Llvm</surname></persName>
		</author>
		<ptr target="https://bcain-llvm.readthedocs.io/projects/llvm/en/latest/Vectorizers" />
		<imprint>
			<date type="published" when="2020-02">Feb-2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ithemal: Accurate, Portable and Fast Basic Block Throughput Estimation using Deep Neural Networks</title>
		<author>
			<persName><forename type="first">Charith</forename><surname>Mendis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Renda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Saman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Amarasinghe</surname></persName>
		</author>
		<author>
			<persName><surname>Carbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML</title>
		<meeting>the 36th International Conference on Machine Learning, ICML</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Azalia</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Goldie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Yazgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ebrahim</forename><surname>Songhori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Young-Joon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omkar</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungmin</forename><surname>Bae</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.10746</idno>
		<title level="m">Chip placement with deep reinforcement learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pipedream: Generalized pipeline parallelism for dnn training</title>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Harlap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amar</forename><surname>Phanishayee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Seshadri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><forename type="middle">R</forename><surname>Devanur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">R</forename><surname>Ganger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><forename type="middle">B</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM Symposium on Operating Systems Principles, SOSP &apos;19</title>
		<meeting>the 27th ACM Symposium on Operating Systems Principles, SOSP &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, highperformance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Alch?-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><surname>Tensorflow</surname></persName>
		</author>
		<author>
			<persName><surname>Xla</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org/xla" />
		<title level="m">Optimizing Compiler for TensorFlow</title>
		<imprint>
			<date type="published" when="2019-09-19">19-September-2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiran</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<title level="m">Neural predictor for neural architecture search</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Renas:relativistic evaluation of neural architecture search</title>
		<author>
			<persName><forename type="first">Yixing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shangling</forename><surname>Jui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
