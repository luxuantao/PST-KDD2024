<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Framework for Practical Parallel Fast Matrix Multiplication</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Austin</forename><forename type="middle">R</forename><surname>Benson</surname></persName>
							<email>arbenson@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Computational and Mathematical Engineering</orgName>
								<orgName type="institution">Stanford University Stanford</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Grey</forename><surname>Ballard</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Sandia National Laboratories Livermore</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Framework for Practical Parallel Fast Matrix Multiplication</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">ACA68E32E124AF52F87F874B283E7DE2</idno>
					<idno type="DOI">10.1145/2688500.2688513</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>G.4 [Mathematical software]: Efficiency; G.4 [Mathematical software]: Parallel and vector implementations fast matrix multiplication</term>
					<term>dense linear algebra</term>
					<term>parallel linear algebra</term>
					<term>shared memory</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Matrix multiplication is a fundamental computation in many scientific disciplines. In this paper, we show that novel fast matrix multiplication algorithms can significantly outperform vendor implementations of the classical algorithm and Strassen's fast algorithm on modest problem sizes and shapes. Furthermore, we show that the best choice of fast algorithm depends not only on the size of the matrices but also the shape. We develop a code generation tool to automatically implement multiple sequential and shared-memory parallel variants of each fast algorithm, including our novel parallelization scheme. This allows us to rapidly benchmark over 20 fast algorithms on several problem sizes. Furthermore, we discuss a number of practical implementation issues for these algorithms on shared-memory machines that can direct further research on making fast algorithms practical.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Matrix multiplication is one of the most fundamental computations in numerical linear algebra and scientific computing. Consequently, the computation has been extensively studied in parallel computing environments (cf. <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b33">34]</ref> and references therein). In this paper, we show that fast algorithms for matrix-matrix multiplication can achieve higher performance on sequential and shared-memory parallel architectures for modestly sized problems. By fast algorithms, we mean ones that perform asymptotically fewer floating point operations and communicate asymptotically less data than the classical algorithm. We also provide a code generation framework to rapidly implement sequential and parallel versions of over 20 fast algorithms. Our performance results in Section 5 show that several fast algorithms can outperform the Intel Math Kernel Library (MKL) dgemm (double precision general matrix-matrix multiplica-Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. PPoPP <ref type="bibr">'15, , February 7-11, 2015</ref>, San Francisco, CA, USA. Copyright c 2015 ACM 978-1-4503-3205-7/15/02. . . $15.00. http://dx.doi.org/10.1145/2688500.2688513 tion) routine and Strassen's algorithm <ref type="bibr" target="#b31">[32]</ref>. In parallel implementations, fast algorithms can achieve a speedup of 5% over Strassen's original fast algorithm and greater than 15% over MKL.</p><p>However, fast algorithms for matrix multiplication have largely been ignored in practice. For example, numerical libraries such as Intel's MKL <ref type="bibr" target="#b18">[19]</ref>, AMD's Core Math Library (ACML) <ref type="bibr" target="#b0">[1]</ref>, and the Cray Scientific Libraries package (LibSci) <ref type="bibr" target="#b7">[8]</ref> do not provide implementations of fast algorithms, though we note that IBM's Engineering and Scientific Subroutine Library (ESSL) <ref type="bibr" target="#b17">[18]</ref> does include Strassen's algorithm. Why is this the case? First, users of numerical libraries typically consider fast algorithms to be of only theoretical interest and never practical for reasonable problem sizes. We argue that this is not the case with our performance results in Section 5. Second, fast algorithms do not provide the same numerical stability guarantees as the classical algorithm. In practice, there is some loss in precision in the fast algorithms, but they are not nearly as bad as the worst-case guarantees <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b26">27]</ref>. Third, the LINPACK benchmark<ref type="foot" target="#foot_0">1</ref> used to rank supercomputers by performance forbids fast algorithms. We suspect that this has driven effort away from the study of fast algorithms.</p><p>Strassen's algorithm is the most well known fast algorithm, but this paper explores a much larger class of recursive fast algorithms based on different base case dimensions. We review these algorithms and methods for constructing them in Section 2. The structure of these algorithms makes them amenable to code generation, and we describe this process and other performance tuning considerations in Section 3. In Section 4, we describe three different methods for parallelizing fast matrix multiplication algorithms on shared-memory machines. Our code generator implements all three parallel methods for each fast algorithm. We evaluate the sequential and parallel performance characteristics of the various algorithms and implementations in Section 5 and compare them with MKL's implementation of the classical algorithm as well as an existing implementation of Strassen's algorithm.</p><p>The goal of this paper is to help bridge the gap between theory and practice of fast matrix multiplication algorithms. By introducing our tool of automatically translating a fast matrix multiplication algorithm to high performance sequential and parallel implementations, we enable the rapid prototyping and testing of theoretical developments in the search for faster algorithms. We focus the attention of theoretical researchers on what algorithmic characteristics matter most in practice, and we demonstrate to practical researchers the utility of several existing fast algorithms besides Strassen's, motivating further effort towards high performance implementations of those that are most promising. Our contributions are summarized as follows:</p><p>Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org.</p><p>• By using new fast matrix multiplication algorithms, we achieve better performance than Intel MKL's dgemm, both sequentially and with 6 and 24 cores on a shared-memory machine.</p><p>• We demonstrate that, in order to achieve the best performance for matrix multiplication, the choice of fast algorithm depends on the size and shape of the matrices. Our new fast algorithms outperform Strassen's on the multiplication of rectangular matrices.</p><p>• We show how to use code generation techniques to rapidly implement sequential and shared-memory parallel fast matrix multiplication algorithms.</p><p>• We provide a new hybrid parallel algorithm for shared-memory fast matrix multiplication.</p><p>• We implement a fast matrix multiplication algorithm with asymptotic complexity O(N 2.775 ) for square N ×N matrices (discovered by Smirnov <ref type="bibr" target="#b30">[31]</ref>). In terms of asymptotic complexity, this is the fastest matrix multiplication algorithm implementation to date. However, our performance results show that this algorithm is not practical for the problem sizes that we consider.</p><p>Overall, we find that Strassen's algorithm is hard to beat for square matrix multiplication, both in serial and in parallel. However, for rectangular matrices (which occur more frequently in practice), other fast algorithms can perform much better. The structure of the fast algorithms that perform well tend to "match the shape" of the matrices, an idea that we will make clear in Section 5. We also find that bandwidth is a limiting factor towards scalability in shared-memory parallel implementations of fast algorithms. Our parallel implementations of fast algorithms suffer when memory bandwidth does not scale linearly with the number of cores. Finally, we find that algorithms that are theoretically fast in terms of asymptotic complexity do not perform well on problems of modest size that we consider on shared-memory parallel architectures. We discuss these conclusions in more detail in Section 6.</p><p>Finally, all of the software used for this paper is available at https://github.com/arbenson/fast-matmul.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related Work</head><p>There are several sequential implementations of Strassen's fast matrix multiplication algorithm <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b16">17]</ref>, and parallel versions have been implemented for both shared-memory <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b24">25]</ref> and distributedmemory architectures <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13]</ref>. For our parallel algorithms in Section 4, we use the ideas of breadth-first and depth-first traversals of the recursion trees, which were first considered by Kumar et al. <ref type="bibr" target="#b24">[25]</ref> and Ballard et al. <ref type="bibr" target="#b2">[3]</ref> for minimizing memory footprint and communication.</p><p>Apart from Strassen's algorithm, a number of fast matrix multiplication algorithms have been developed, but only a small handful have been implemented. Furthermore, these implementations have only been sequential. Hopcroft and Kerr showed how to construct recursive fast algorithms where the base case is multiplying a p × 2 by a 2×n matrix <ref type="bibr" target="#b15">[16]</ref>. Bini et al. introduced the concept of arbitrary precision approximate (APA) algorithms for matrix multiplication and demonstrated a method for multiplying 3 × 2 by 2 × 2 matrices which leads to a general square matrix multiplication APA algorithm that is asymptotically faster than Strassen's <ref type="bibr" target="#b4">[5]</ref>. Schönhage also developed an APA algorithm that is asymptotically faster than Strassen's, based on multiplying square 3 × 3 matrices <ref type="bibr" target="#b29">[30]</ref>. These APA algorithms suffer from severe numerical issues-both lose at least half the digits of accuracy with each recursive step. While no exact solution can have the same complexity as Bini's algorithm <ref type="bibr" target="#b15">[16]</ref>, it is still an open question if there exists an exact fast algorithm with the same complexity as Schönhage's. Pan used factorization of trilinear forms and a base case of 70×70 square matrix multiplication to construct an exact algorithm asymptotically faster than Strassen's algorithm <ref type="bibr" target="#b28">[29]</ref>. This algorithm was implemented by Kaporin <ref type="bibr" target="#b21">[22]</ref>, and the running time was competitive with Strassen's algorithm on a sequential machine. Recently, Smirnov presented optimization tools for finding many fast algorithms based on factoring bilinear forms <ref type="bibr" target="#b30">[31]</ref>, and we will use these tools for finding our own algorithms in Section 2.</p><p>There are several lines of theoretical research (cf. <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b35">36]</ref> and references therein) that prove existence of fast APA algorithms with much better asymptotic complexity than the algorithms considered here. Unfortunately, there remains a large gap between the substantial theoretical work and what we can practically implement.</p><p>Renewed interest in the practicality of Strassen's and other fast algorithms is motivated by the observation that not only is the arithmetic cost reduced when compared to the classical algorithm, the communication costs also improve asymptotically <ref type="bibr" target="#b3">[4]</ref>. That is, as the relative cost of moving data throughout the memory hierarchy and between processors increases, we can expect the benefits of fast algorithms to grow accordingly. We note that communication lower bounds <ref type="bibr" target="#b3">[4]</ref> apply to all the algorithms presented in this paper, and in most cases they are attained by the implementations used here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Notation and Tensor Preliminaries</head><p>We briefly review basic tensor preliminaries, following the notation of Kolda and Bader <ref type="bibr" target="#b23">[24]</ref>. Scalars are represented by lowercase Roman or Greek letters (a), vectors by lowercase boldface (x), matrices by uppercase boldface (A), and tensors by boldface Euler script letters (T). For a matrix A, we use a k and a i j to denote the kth column and i, j entry, respectively. A tensor is a multidimensional array, and in this paper we deal exclusively with order-3, real-valued tensors; i.e., T ∈ R I×J×K . The kth frontal slice of T is</p><formula xml:id="formula_0">T k = t :,:,k ∈ R I×J . For u ∈ R I , v ∈ R J , w ∈ R K , we define the outer product tensor T = u • v • w ∈ R I×J×K</formula><p>with entries t i jk = u i v j w k . Addition of tensors is defined entry-wise. The rank of a tensor T is the minimum number of rank-one tensors that generate T as their sum. Decompositions of the form T = R r=1 u r • v r • w r lead to fast matrix multiplication algorithms (Section 2.2), and we use U, V, W to denote the decomposition, where U, V, and W are matrices with R columns given by u r , v r , and w r . Of the various flavors of products involving tensors, we will need to know that, for</p><formula xml:id="formula_1">a ∈ R I and b ∈ R J , T × 1 a × 2 b = c ∈ R K , with c k = a T T k b, or c k = I i=1 J j=1 t i jk a i b j .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Fast Matrix Multiplication</head><p>We now review the preliminaries for fast matrix multiplication algorithms. In particular, we focus on factoring tensor representations of bilinear forms, which will facilitate the discussion of the implementation in Sections 3 and 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Recursive Multiplication</head><p>Matrices are self-similar, i.e., a submatrix is also a matrix. Arithmetic with matrices is closely related to arithmetic with scalars, and we can build recursive matrix multiplication algorithms by manipulating submatrix blocks. For example, consider multiplying</p><formula xml:id="formula_2">C = A • B, C 11 C 12 C 21 C 22 = A 11 A 12 A 21 A 22 • B 11 B 12 B 21 B 22 ,</formula><p>where we have partitioned the matrices into four submatrices. Throughout this paper, we denote the block multiplication of M ×K and K × N matrices by M, K, N . Thus, the above computation is 2, 2, 2 . Multiplication with the classical algorithm proceeds by combining a set of eight matrix multiplications with four matrix additions:</p><formula xml:id="formula_3">M 1 = A 11 • B 11 M 2 = A 12 • B 21 M 3 = A 11 • B 12 M 4 = A 12 • B 22 M 5 = A 21 • B 11 M 6 = A 22 • B 21 M 7 = A 21 • B 12 M 8 = A 22 • B 22 C 11 = M 1 + M 2 C 12 = M 3 + M 4 C 21 = M 5 + M 6 C 22 = M 7 + M 8</formula><p>The multiplication to form each M i is recursive and the base case is scalar multiplication. The number of flops performed by the classical algorithm for N × N matrices, where N is a power of two, is 2N 3 -N<ref type="foot" target="#foot_1">2</ref> . The idea of fast matrix multiplication algorithms is to perform fewer recursive matrix multiplications at the expense of more matrix additions. Since matrix multiplication is asymptotically more expensive than matrix addition, this tradeoff results in faster algorithms. The most well known fast algorithm is due to Strassen, and follows the same block structure:</p><formula xml:id="formula_4">S 1 = A 11 + A 22 S 2 = A 21 + A 22 S 3 = A 11 S 4 = A 22 S 5 = A 11 + A 12 S 6 = A 21 -A 11 S 7 = A 12 -A 22 T 1 = B 11 + B 22 T 2 = B 11 T 3 = B 12 -B 22 T 4 = B 21 -B 11 T 5 = B 22 T 6 = B 11 + B 12 T 7 = B 21 + B 22 M r = S r T r , 1 ≤ r ≤ 7 C 11 = M 1 + M 4 -M 5 + M 7 C 12 = M 3 + M 5 C 21 = M 2 + M 4 C 22 = M 1 -M 2 + M 3 + M 6</formula><p>We have explicitly written out terms like T 2 = B 11 to hint at the generalizations provided in Section 2.2. Strassen's algorithm uses 7 matrix multiplications and 18 matrix additions. The number of flops performed by the algorithm is 7N log 2 7 -6N 2 = O(N 2.81 ), when we assume a base case of N = 1.</p><p>There are natural extensions to Strassen's algorithm. We might try to find an algorithm using fewer than 7 multiplications; unfortunately, we cannot <ref type="bibr" target="#b36">[37]</ref>. Alternatively, we could try to reduce the number of additions. This leads to the Strassen-Winograd algorithm, which reduces the 18 additions down to 15. We explore such methods in Section 3.3. We can also improve the constant on the leading term by choosing a bigger base case dimension (and using the classical algorithm for the base case). This turns out not to be important in practice because the base case will be chosen to optimize performance rather than flop count. Lastly, we can use blocking schemes apart from 2, 2, 2 , which we explain in the remainder of this section. This leads to a host of new algorithms, and we show in Section 5 that they are often faster in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Fast Algorithms as Low-Rank Tensor Decompositions</head><p>The approach we use to devise fast algorithms exploits an important connection between matrix multiplication (and other bilinear forms) and tensor computations. We detail the connection in this section for completeness; see <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b22">23]</ref> for earlier explanations.</p><p>A bilinear form on a pair of finite-dimensional vector spaces is a function that maps a pair of vectors to a scalar and is linear in each of its inputs separately. A bilinear form B(x, y) can be represented by a matrix D of coefficients: B(x, y) = x T Dy = i j d i j x i y j , where we note that x and y may have different dimensions. In order to describe a set of K bilinear forms B k (x, y) = z k , 1 ≤ k ≤ K, we can use a three-way tensor T of coefficients:</p><formula xml:id="formula_5">z k = I i=1 J j=1 t i jk x i y j ,<label>(1)</label></formula><p>or, in more succinct tensor notation, z = T × 1 x × 2 y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Low-Rank Tensor Decompositions</head><p>The advantage of representing the operations using a tensor of coefficients is a key connection between the rank of the tensor to the arithmetic complexity of the corresponding operation. Consider the "active" multiplications between elements of the input vectors (e.g., x i • y j ). The conventional algorithm, following Equation (1), will compute an active multiplication for every nonzero coefficient in T. However, suppose we have a rank-R decomposition of the tensor,</p><formula xml:id="formula_6">T = R i=1 u i • v i • w i , so that t i jk = R r=1 u ir v jr w kr<label>(2)</label></formula><p>for all i, j, k, where U, V, and W are matrices with R columns each. We will also use the equivalent notation T = U, V, W . Substituting Equation (2) into Equation ( <ref type="formula" target="#formula_5">1</ref>) and rearranging, we have for k = 1, . . . , K, z k = R r=1 (s r • t r ) w kr = R r=1 m r w kr , where s = U T x, t = V T y, and m = s * t. 2 This reduces the number of active multiplications (now between linear combinations of elements of the input vectors) to R. Here we highlight active multiplications with (•) notation.</p><p>Assuming R &lt; nnz (T), this reduction of active multiplications, at the expense of increasing the number of other operations, is valuable when active multiplications are much more expensive than the other operations. This is the case for recursive matrix multiplication, where the elements of the input vectors are (sub)matrices, as we describe below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Tensor Representation of Matrix Multiplication</head><p>Matrix multiplication is a bilinear operation, so we can represent it as a tensor computation. In order to match the notation above, we vectorize the input and output matrices A, B, and C using row-wise ordering, so that x = vec (A), y = vec (B), and z = vec (C).</p><p>For every triplet of matrix dimensions for valid matrix multiplication, there is a fixed tensor that represents the computation so that T × 1 vec (A) × 2 vec (B) = vec (C) holds for all A, B, and C. For example, if A and B are both 2 × 2, the corresponding 4 × 4 × 4 tensor T has frontal slices</p><formula xml:id="formula_7">             1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0              ,              0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0              ,              0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0              ,              0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1              . This yields, for example, T 3 × 1 vec (A) × 2 vec (B) = a 21 • b 11 + a 22 • b 21 = c 21 .</formula><p>By Strassen's algorithm, we know that although this tensor has 8 nonzero entries, its rank is at most 7. Indeed, that algorithm corresponds to a low-rank decomposition represented by the following triplet of matrices, each with 7 columns:</p><formula xml:id="formula_8">U =              1 0 1 0 1 -1 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1 1 0 1 0 0 -1              , V =              1 1 0 -1 0 1 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 -1 0 1 0 1              , W =              1 0 0 1 -1 0 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 1 -1 1 0 0 1 0              .</formula><p>As in the previous section, for example,</p><formula xml:id="formula_9">s 1 = u T 1 vec (A) = a 11 +a 22 , t 1 = v T 1 vec (B) = b 11 + b 22 , and c 11 = (Wm) 1 = m 1 + m 4 -m 5 + m 7 .</formula><p>Note that in the previous section, the elements of the input matrices are already interpreted as submatrices (e.g., A 11 and M 1 ); here we represent them as scalars (e.g., a 11 and m 1 ).</p><p>We need not restrict ourselves to the 2, 2, 2 case; there exists a tensor for matrix multiplication given any set of valid dimensions. When considering a base case of M ×K by K ×N matrix multiplication (denoted M, K, N ), the tensor has dimensions MK×KN×MN and MKN non-zeros. In particular, t i jk = 1 if the following three conditions hold: (1) (i -1) mod K = ( j -1)/N ; (2) ( j -1) mod N = (k -1) mod N; and (3) (i -1)/K = (k -1)/N . Otherwise, t i jk = 0 (here we assume entries are 1-indexed).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Approximate Tensor Decompositions</head><p>The APA algorithms discussed in Section 1.1 arise from approximate tensor decompositions. With Bini's algorithm, for example, the factor matrices (i.e., the corresponding U, V, W ) have entries 1/λ and λ. As λ → 0, the low-rank tensor approximation approaches the true tensor. However, as λ gets small, we suffer from loss of precision in the floating point calculations of the resulting fast algorithm. Setting λ = √ minimizes the loss of accuracy for one step of Bini's algorithm, where is machine precision <ref type="bibr" target="#b5">[6]</ref>, but even in this case at least half the digits are lost with a single recursive step of the algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Finding Fast Algorithms</head><p>We conclude this section with a description of a method for searching for and discovering fast algorithms for matrix multiplication. Our search goal is to find low-rank decompositions of tensors corresponding to matrix multiplication of a particular set of dimensions, which will identify fast, recursive algorithms with reduced arithmetic complexity. That is, given a particular base case M, K, N and the associated tensor T, we seek a rank R and matrices U, V, and W that satisfy Equation (1). Table <ref type="table">1</ref> summarizes the algorithms that we find and use for numerical experiments in Section 5.</p><p>The rank of the decomposition determines the number of active multiplications, or recursive calls, and therefore the exponent in the arithmetic cost of the algorithm. The number of other operations (additions and inactive multiplications) will affect only the constants in the arithmetic cost. For this reason, we want sparse U, V, and W matrices with simple values (like ±1), but that goal is of secondary importance compared to minimizing the rank R. Note that these constant values do affect performance of these algorithms for reasonable matrix dimensions in practice, though mainly because of how they affect the communication costs of the implementations rather than the arithmetic cost. We discuss this in more detail in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Equivalent Algorithms</head><p>Given an algorithm U, V, W for base case M, K, N , we can transform it to an algorithm for any of the other 5 permutations of the base case dimensions with the same number of multiplications. This is a well known property <ref type="bibr" target="#b14">[15]</ref>; here we state the two transformations that generate all permutations in our notation. We let P I×J be the permutation matrix that swaps row-order for column-order in the vectorization of an I × J matrix. In other words, if A is I × J,</p><formula xml:id="formula_10">P I×J • vec (A) = vec A T . Proposition 2.1. Given a fast algorithm U, V, W for M, K, N , P K×N V, P M×K U, P M×N W is a fast algorithm for N, K, M . Proposition 2.2. Given a fast algorithm U, V, W for M, K, N , P M×N W, U, P K×N V is a fast algorithm for N, M, K .</formula><p>Fast algorithms for a given base case also belong to equivalence classes. Two algorithm are equivalent if one can be generated from another based on the following transformations <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21]</ref>. </p><formula xml:id="formula_11">(Y -T ⊗ X)U, (Z -T ⊗ Y)V, (X ⊗ Z -T )W for any nonsingular matrices X ∈ R M×M , Y ∈ R K×K , Z ∈ R N×N .</formula><p>Table <ref type="table">1</ref>. Summary of fast algorithms. Algorithms without citation were found by the authors using the ideas in Section 2.3. An asterisk denotes an approximation (APA) algorithm. The number of multiplications is equal to the rank R of the corresponding tensor decomposition. The multiplication speedup per recursive step is the expected speedup if matrix additions were free. This speedup does not determine the fastest algorithm because the maximum number of recursive steps depend on the size of the sub-problems created by the algorithm. By Propositions 2.1 and 2.2, we also have fast algorithms for all permutations of the base case M, K, N . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Numerical Search</head><p>Given a rank R for base case M, K, N , Equation ( <ref type="formula" target="#formula_5">1</ref>) defines (MKN) 2 polynomial equations of the form given in Equation <ref type="bibr" target="#b1">(2)</ref>.</p><p>Because the polynomials are trilinear, alternating least squares (ALS) can be used to iteratively compute an approximate (numerical) solution to the equations. That is, if two of the three factor matrices are fixed, the optimal third factor matrix is the solution to a linear least squares problem. Thus, each outer iteration of ALS involves alternating among solving for U, V, and W, each of which can be done efficiently with a QR decomposition, for example. This approach was first proposed for fast matrix multiplication search by Brent <ref type="bibr" target="#b6">[7]</ref>, but ALS has been a popular method for general low-rank tensor approximation for as many years (see <ref type="bibr" target="#b23">[24]</ref> and references therein).</p><p>The main difficulties ALS faces for this problem include getting stuck at local minima, encountering ill-conditioned linear least-squares problems, and, even if ALS converges to machineprecision accuracy, computing dense U, V, and W matrices with floating point entries. We follow the work of Johnson and McLoughlin <ref type="bibr" target="#b20">[21]</ref> and Smirnov <ref type="bibr" target="#b30">[31]</ref> in addressing these problems. We use multiple starting points to handle the problem of local minima, add regularization to help with the ill-conditioning, and encourage sparsity in order to recover exact factorizations (with integral or rational values) from the approximations.</p><p>The most useful techniques in our search have been (1) exploiting equivalence transformations [21, Eq. ( <ref type="formula">6</ref>)] to encourage sparsity and obtain discrete values and (2) using and adjusting the regularization penalty term [31, Eq. (4-5)] throughout the iteration. As described in earlier efforts, algorithms for small base cases can be discovered nearly automatically. However, as the values M, N, and K grow, more hands-on tinkering using heuristics seems to be necessary to find discrete solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Implementation and Practical Considerations</head><p>We now discuss our code generation method for fast algorithms and the major implementation issues. All experiments were conducted on a single compute node on NERSC's Edison. Each node has two 12-core Intel 2.4 GHz Ivy Bridge processors and 64 GB of memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Code Generation</head><p>Our code generator automatically implements a fast algorithm in C++ given the U, V, and W matrices representing the algorithm. The generator simultaneously produces both sequential and parallel implementations. We discuss the sequential code in this section and the parallel extensions in Section 4. For computing C = A • B, the following are the key ingredients of the generated code:</p><p>• Using the entries in the U and V matrices, form the temporary matrices S r and T r , 1 ≤ r ≤ R, via matrix additions and scalar multiplication. The S r and T r are linear combinations of sub-blocks of A and B, respectively. For each S r and T r , the corresponding linear combination is a custom implementation. Scalar multiplication by ±1 is replaced with native addition / subtraction operators. The code generator can produce three variants of matrix additions, which we describe in Section 3.2. When a column of U or V contains a single non-zero element, there is no matrix addition (only scalar multiplication). In order to save memory, the code generator does not form a temporary matrix in this case. The scalar multiplication is piped through to subsequent recursive calls and is eventually used in a base case call to dgemm.</p><p>• Recursive calls to the fast matrix multiplication routine compute M r = S r • T r , 1 ≤ r ≤ R.</p><p>• Using the entries of W, linear combinations of the M r form the output C. Matrix additions and scalar multiplications are again handled carefully, as above.</p><p>• Common subexpression elimination detects redundant matrix additions, and the code generator can automatically implement algorithms with fewer additions. We discuss this process in more detail in Section 3.3.</p><p>• Dynamic peeling <ref type="bibr" target="#b32">[33]</ref> accounts for matrices whose dimensions are not evenly divided by the base case of the fast algorithm. This method handles the boundaries of the matrix at each recursive level, and requires no additional memory. (Other methods, such as zero-padding, require additional memory). With dynamic peeling, the implementation can multiply matrices of any dimensions.</p><p>Figure <ref type="figure" target="#fig_1">1</ref> shows performance benchmarks of the code generator's implementation. In order to compare the performance of matrix multiplication algorithms with different computational costs, we use the effective GFLOPS metric for P × Q × R matrix multiplication:</p><formula xml:id="formula_12">effective GFLOPS = 2PQR -PR time in seconds • 1e-9.<label>(3)</label></formula><p>We note that effective GFLOPS is only the true GFLOPS for the classical algorithm (the fast algorithms perform fewer floating point operations). However, this metric lets us compare all of the algorithms on an inverse-time scale, normalized by problem size <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b26">27]</ref>. We compare our code-generated Strassen implementation with MKL's dgemm and a tuned implementation of Strassen-Winograd from D'Alberto et al. <ref type="bibr" target="#b8">[9]</ref> (recall that Strassen-Winograd performs the same number of multiplications but fewer matrix additions than Strassen's algorithm). The code generator's implementation outperforms MKL and is competitive with the tuned implementation.  <ref type="formula" target="#formula_12">3</ref>)) of our code generator's implementation of Strassen's algorithm against MKL's dgemm and a tuned implementation of the Strassen-Winograd algorithm <ref type="bibr" target="#b8">[9]</ref>. The problems sizes are square. The generated code easily outperforms MKL and is competitive with the tuned code. Thus, we are confident that the general conclusions we draw with code-generated implementations of fast algorithms will also apply to hand-tuned implementations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Handling Matrix Additions</head><p>While the matrix multiplications constitute the bulk of the running time, matrix additions are still an important performance optimization. We call the linear combinations used to form S r , T r , and C i j addition chains. For example, S 1 = A 11 +A 22 is an addition chain in Strassen's algorithm. We consider three different implementations for the addition chains:</p><p>1. Pairwise: With r fixed, compute S r and T r using the daxpy BLAS routine for all matrices in the addition chain. This requires nnz (u r ) calls to daxpy to form S r and nnz (v r ) calls to form T r . After the recursive computations of the M r , we follow the same strategy to form the output. The ith sub-block (rowwise) of C requires nnz w i,: daxpy calls. <ref type="foot" target="#foot_2">3</ref>2. Write-once: With r fixed, compute S r and T r with only one write for each entry (instead of, for example, nnz (v r ) writes for S r with the pairwise method). In place of daxpy, stream through the necessary submatrices of A and B and combine the entries to form S r and T r . This requires reading some submatrices of A and B several times, but writing to only one output stream at a time. Similarly, we write the output matrix C once and read the M r several times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Streaming:</head><p>Read each input matrix once and write each temporary matrix S r and T r once. Stream through the entries of each sub-block of A and B, and update the corresponding entries in all temporary matrices S r and T r . Similarly, stream through the entries of the M r and update all submatrices of C.</p><p>Each daxpy call requires two matrix reads and one matrix write (except for the first call in an addition chain, which is a copy and requires one read and one write). Let nnz (U, V, W) = nnz (U) + nnz (V) + nnz (W). Then the pairwise additions perform 2 • nnz (U, V, W) -2R -MN submatrix reads and nnz (U, V, W) submatrix writes. However, the additions use an efficient vendor implementation.</p><p>The write-once additions perform nnz (U, V, W) submatrix reads and at most 2R + MN submatrix writes. We do not need to write any data for the columns of U and V with a single nonzero entry. These correspond to addition chains that are just a copy, for example, T 2 = B 11 in Strassen's algorithm. While we perform fewer reads and writes than the pairwise additions, the complexity of our code increases (we have to write our own additions), and we can no longer use a tuned daxpy routine. We do not worry about code complexity because we use code generation. Since the problem is bandwidth-bound and compilers can automatically vectorize for loops, we don't expect the latter concern to be an issue.</p><p>Finally, the streaming additions perform MK+KN+R submatrix reads and at most 2R + MN submatrix writes. This is fewer reads than the write-once additions, but we have increased the complexity of the writes. Specifically, we alternate writes to different memory locations, whereas with the write-once algorithm, we write to a single (contiguous) output stream.</p><p>The three methods also have different memory footprints. With pairwise or write-once, S r and T r are formed just before computing M r . After M r is computed, the memory becomes available. On the other hand, the streaming algorithm must compute all temporary matrices S r and T r simultaneously, and hence needs R times as much memory for the temporary matrices. We will explore the performance of the three methods at the end of Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Common Subexpression Elimination</head><p>The S r , T r , and M r matrices often share subexpressions. For example, in our 4, 2, 4 fast algorithm (see Table <ref type="table">1</ref>), T 11 and T 25 are:</p><formula xml:id="formula_13">T 11 = B 24 -B 12 -B 22 T 25 = B 23 + B 12 + B 22</formula><p>Both T 11 and T 25 share the subexpression B 12 + B 22 , up to scalar multiplication. Thus, there is opportunity to remove additions / subtractions:</p><formula xml:id="formula_14">Y 1 = B 12 + B 22 T 11 = B 24 -Y 1 T 25 = B 23 + Y 1</formula><p>At face value, eliminating additions would appear to improve the algorithm. However, there are two important considerations. First, using Y 1 with the pairwise or write-once approaches requires additional memory (with the streaming approach it requires only additional local variables).</p><p>Second, we discussed in Section 3.2 that an important metric is the number of reads and writes. If we use the write-once algorithm, we have actually increased the number of reads and writes. Originally, forming T 11 and T 25 required six reads and two writes. By eliminating the common subexpression, we performed two fewer reads in forming T 11 and T 25 but needed an additional two reads and one write to form Y 1 . In other words, we have read the same amount of data and written more data. In general, eliminating the same length-two subexpression k times reduces the number of matrix reads and writes by k -3. Thus, a length-two subexpression must appear at least four times for elimination to reduce the total number of reads and writes in the algorithm.</p><p>Figure <ref type="figure" target="#fig_2">2</ref> shows the performance all three matrix addition methods from Section 3.2, with and without common subexpression elimination (CSE). For CSE, we greedily eliminate length-two subexpressions. In general, the write-once algorithm without CSE performs the best on the rectangular matrix multiplication problem sizes. For these problems, CSE lowers performance of the writeonce algorithm and has little to modest effect on the streaming and pairwise algorithms. For square matrix problems, the best variant is less clear, but write-once with no elimination often performs the highest. We use write-once without CSE for the rest of our performance experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Recursion Cutoff Point</head><p>In practice, we take only a few steps of recursion before calling a vendor-tuned library classical routine as the base case (in our case, Intel MKL's dgemm). One method for determining the cutoff point is to benchmark each algorithm and measure where the implementation outperforms dgemm. While this is sustainable for the analysis of any individual algorithm, we are interested in a large class of fast algorithms. Furthermore, a simple set of cutoff points limits understanding of the performance and will have to be re-measured for different architectures. Instead, we provide a rule of thumb based on the performance of dgemm.</p><p>Figure <ref type="figure">3</ref> shows the performance of Intel MKL's sequential and parallel dgemm routines. We see that the routines exhibit a "ramp-up" phase and then flatten for sufficiently large problems. In both serial and parallel, multiplication of square matrices (N × N × N computation) tends to level at a higher performance than the problem shapes with a fixed dimension (N × 800 × N and N × 800 × 800). Our principle for recursion is to take a recursive step only if the sub-problems fall on the flat part of the curve. If the ratio of performance drop in the DGEMM curve is greater than the speedup per step (as listed in Table <ref type="table">1</ref>), then taking an additional recursive step cannot improve performance. <ref type="foot" target="#foot_3">4</ref> Finally, we note that some of our parallel algorithms call the sequential dgemm routine in the base case. Both curves will be important to our parallel fast matrix multiplication algorithms in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Parallel Algorithms for Shared Memory</head><p>We present three algorithms for parallel fast matrix multiplication: depth-first search (DFS), breadth-first search (BFS), and a hybrid of the two (HYBRID). In this work, we target shared memory machines, although the same ideas generalize to distributed memory. For example, DFS and BFS ideas are used for a distributed memory implementation of Strassen's algorithm <ref type="bibr" target="#b26">[27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Depth-First Search</head><p>The DFS algorithm is straightforward: when recursion stops, the classical algorithm uses all threads on each sub-problem. In other words, we use parallel matrix multiplication on the leaf nodes of a depth-first traversal of the recursion tree. At a high-level, the code path is exactly the same as in the sequential case, and the main parallelism is in library calls. The advantages of DFS are that the memory footprint matches the sequential algorithm and the code is simpler-parallelism in multiplications is hidden inside library calls. Furthermore, matrix additions are trivially parallelized. The key disadvantage of DFS is that the recursion cutoff point is larger (Figure <ref type="figure">3</ref>), limiting the number of recursive steps. On Edison's 24core compute node, the recursion cutoff point is around N = 5000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Breadth-First Search</head><p>The BFS algorithm uses task-based parallelism. Each leaf node in the matrix multiplication recursion tree is an independent task. The recursion tree also serves as a dependency graph: we need to compute all M r , 1 ≤ r ≤ R, (children) before forming the result (parent). The major advantage of BFS is we can take more recursive steps because the recursion cutoff point is based on the sequential dgemm curves. Matrix additions to form S r and T r are part of the task that computes M r . In the first level of recursion, matrix additions to form C i j from the M r are handled in the same way as DFS, since all threads are available.</p><p>The BFS approach has two distinct disadvantages. First, it is difficult to load balance the tasks because the number of threads  <ref type="formula" target="#formula_12">3</ref>)) comparison of common subexpression elimination (CSE) and the three matrix addition methods: write-once, streaming, and pairwise (see Section 3.2). The 4, 2, 4 fast algorithm computed N × 1600 × N ("outer product" shape) for varying N, and the 4, 2, 3 fast algorithm computed N × N × N (square multiplication). Write-once with no CSE tends to have the highest performance, especially for the 4, 2, 4 algorithm. Pairwise is slower because it performs more reads and writes. Figure <ref type="figure">3</ref>. Performance curves of MKL's dgemm routine in serial (left) and in parallel (right) for three different problem shapes. The performance curves exhibit a "ramp-up" phase and then flatten for large enough problems. Performance levels near N = 1500 in serial and N = 5000 in parallel. For large problems in both serial and parallel, N × N × N multiplication is faster than N × 800 × N, which is faster than N × 800 × 800. We note that sequential performance is faster than per-core parallel performance due to Intel Turbo Boost, which increases the clock speed from 2.4 to 3.2 GHz. With Turbo Boost, peak sequential performance is 25.6 GFLOPS. Peak parallel performance is 19.2 GFLOPS/core. may not divide the number of tasks evenly. Also, with only one step of recursion, the number of tasks can be smaller than the number of threads. For example, one step of Strassen's algorithm produces only 7 tasks and one step of the fast 3, 2, 3 algorithm produces only 15 tasks. Second, BFS requires additional memory since the tasks are executed independently. In a fast algorithm for M, K, N with R multiplies, each recursive step requires a factor R/(MN) more memory than the output matrix C to store the M r . There are additional memory requirements for the S r and T r matrices, as discussed in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Hybrid</head><p>Our novel hybrid algorithm compensates for the load imbalance in BFS by applying the DFS approach on a subset of the base case problems. With L levels of recursion and P threads, the hybrid algorithm applies task parallelism (BFS) to the first R L -(R L mod P) multiplications. The number of BFS sub-problems is a multiple of P, so this part of the algorithm is load balanced. All threads are used on each of the R L mod P remaining multiplications (DFS).</p><p>An alternative approach uses another level of hybridization: evenly assign as many as possible of the remaining R L mod P multiplications to disjoint subsets of P &lt; P threads (where P divides P), and then finish off the still-remaining multiplications with all P threads. This approach reduces the number of small multiplications assigned to all P threads where perfect scaling is harder to achieve. However, it leads to additional load balancing concerns in practice and requires a more complicated task scheduler.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Implementation</head><p>The code generation from Section 3.1 produces code that can compile to the DFS, BFS, or HYBRID parallel algorithms. We use OpenMP to implement each algorithm. The overview of the parallelization is:</p><p>• DFS: Each dgemm call uses all threads. Matrix additions are always fully parallelized.</p><p>• BFS: Each recursive matrix multiplication routine and the associated matrix additions are launched as an OpenMP task. At each recursive level, the taskwait barrier ensures that all M r matrices are available to form the output matrix.</p><p>• HYBRID: Matrix multiplies are either launched as an OpenMP task (BFS), or the number of MKL threads is adjusted for a parallel dgemm (DFS). This is implemented with the if conditional clause of OpenMP tasks. Again, taskwait barriers ensure that the M r matrices are computed before forming the output matrix.</p><p>We use an explicit synchronization scheme with OpenMP locks to ensure that the DFS steps occur after the BFS tasks complete. This ensures that there is no oversubscription of threads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Shared-Memory Bandwidth Limitations</head><p>The performance gains of the fast algorithms rely on the cost of matrix multiplications to be much larger than the cost of matrix additions. Since matrix multiplication is compute-bound and matrix addition is bandwidth-bound, these computations scale differently with the amount of parallelism. For large enough matrices, MKL's dgemm achieves near-peak performance of the node (Figure <ref type="figure">3</ref>). On the other hand, the STREAM benchmark <ref type="bibr" target="#b27">[28]</ref> shows that the node achieves around a five-fold speedup in bandwidth with 24 cores. In other words, in parallel, matrix multiplication is near 100% parallel efficiency and matrix addition is near 20% parallel efficiency.</p><p>The bandwidth bottleneck makes it more difficult for parallel fast algorithms to be competitive with parallel MKL. To illuminate this issue, we will present performance results with both 6 and 24 cores. Using 6 cores avoids the bandwidth bottleneck and leads to much better performance per core.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Performance Comparisons</head><p>Figure <ref type="figure" target="#fig_3">4</ref> shows the performance of the BFS, DFS, and HYBRID parallel methods with both 6 and 24 cores for three representative algorithms. The left plot shows the performance of Strassen's algorithm on square problems. With 6 cores, HYBRID does the best for small problems. Since Strassen's algorithm uses 7 multiplies, BFS has poor performance with 6 cores when using one step of recursion. While all 6 cores can do 6 multiplies in parallel, the 7th multiply is done sequentially (with HYBRID, the 7th multiply uses all 6 cores). With two steps of recursion, BFS has better load balance but is forced to work on smaller sub-problems. As the problems get larger, BFS outperforms HYBRID due to synchronization overhead when HYBRID switches from BFS to DFS steps. When the matrix dimension is around 15,000, the fast algorithm achieves a 25% speedup over MKL. Using 24 cores, HYBRID and DFS are the fastest. With one step of recursion, BFS can achieve only sevenfold parallelism. With two steps, there are 49 sub-problems, so one core is assigned 3 sub-problems while all others are assigned 2. In general, we see that it is much more difficult to achieve speedups with 24 cores. However, Strassen's algorithm has a modest performance gain over MKL for large problem sizes (∼ 5% faster). The middle plot of Figure <ref type="figure" target="#fig_3">4</ref> shows the 4, 2, 4 fast algorithm (26 multiplies) for N × 2800 × N problems. With 6 cores, HYBRID is fastest for small problems and BFS becomes competitive for larger problems, where the performance is 15% better than MKL. In Section 5, we show that 4, 2, 4 is also faster than Strassen's algorithm for these problems. With 24 cores, we see that HYBRID is drastically faster than MKL on small problems. For example, HY-BRID is 75% faster on 3500 × 2800 × 3500. <ref type="foot" target="#foot_4">5</ref> As the problem sizes get larger, we experience the bandwidth bottleneck and HYBRID achieves around the same performance as MKL. BFS uses one step of recursion and is consistently slower since it parallelizes 24 of 26 multiplies and uses only 2 cores on the last 2 multiplies. While multiple steps of recursion creates more load balance, the sub-problems are small enough that performance degrades even more. DFS follows a similar ramp-up curve as MKL, but the sub-problems are still too small to see a performance benefit.</p><p>The right plot of Figure <ref type="figure" target="#fig_3">4</ref> shows the 4, 3, 3 fast algorithm (29 multiplies) for N × 3000 × 3000. We see similar trends as for the other problem sizes. With 6 cores, HYBRID does well for all problem sizes. Speedups are around ∼ 5% for large problems. With 24 cores, HYBRID is again drastically faster than MKL for small problem sizes and about the same as MKL for large problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Performance Experiments</head><p>We now present performance results for a variety of fast algorithms on several problem sizes. Based on the results of Section 4.5, we take the best of BFS and HYBRID when using 6 cores and the best of DFS and HYBRID when using 24 cores. For rectangular problem sizes in both sequential and parallel, we take the best of one or two steps of recursion. And for square problem sizes, we take the best of one, two, or three steps of recursion. Additional recursive steps do not improve the performance for the problem sizes we consider.</p><p>The square problem sizes for parallel benchmarks require the most memory-for some algorithms, three steps of recursion results in out-of-memory errors. In these cases, the original problem consumes 6% of the memory. For these algorithms, we only record the best of one or two steps of recursion in the performance plots. Finally, all timings are the median of five trials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Sequential Performance</head><p>Figures <ref type="figure" target="#fig_4">5</ref> and<ref type="figure" target="#fig_5">6</ref> summarize the sequential performance of several fast algorithms. For N × N × N problems (Figure <ref type="figure" target="#fig_4">5</ref>), we test the algorithms in Table <ref type="table">1</ref> and some of their permutations. For example, we test 4, 4, 2 and 4, 2, 4 , which are permutations of 2, 4, 4 . In total, over 20 algorithms are tested for square matrices. Two of these algorithms, Bini's 3, 2, 2 and Schönhage's 3, 3, 3 are APA algorithms. We note that APA algorithms are of limited practical interest; even one step of recursion causes numerical errors in at least similar speedups using our code generator and a classical, 2, 3, 4 recursive algorithm (24 multiplies).  Performance on an "outer product" shape, N × 1600 × N. Exact fast algorithms that have a similar outer product shape (e.g., 4, 2, 4 ) tend to have the highest performance. (Right): Performance of multiplication of tall-and-skinny matrix by a small square matrix, N × 2400 × 2400. Again, fast algorithms that have this shape (e.g., 4, 3, 3 ) tend to have the highest performance. half the digits (a better speedup with the same or better numerical accuracy can be obtained by switching to single precision). For the problem sizes N × 1600 × N and N × 2400 × 2400 (Figure <ref type="figure" target="#fig_5">6</ref>), we evaluate the algorithms that are comparable to, or outperform, Strassen's algorithm. The results are summarized as follows:</p><p>1. All of the fast algorithms outperform MKL for large enough problem sizes. These algorithms are implemented with our code generator and use only the high-level optimizations described in Section 3.1. Since the fast algorithms perform less computation and communication, we expect this to happen.</p><p>2. For square matrices, Strassen's algorithm often performs the best. This is mostly due to its relatively small number of matrix additions in comparison to other fast algorithms. On large problem sizes, Strassen's algorithm provides around a 20% speedup over MKL's dgemm. The right plot of Figure <ref type="figure" target="#fig_4">5</ref> shows that some algorithms are competitive with Strassen's algorithm on large problems. These algorithms have large speedups per recursive step (see Table <ref type="table">1</ref>). While Strassen's algorithm can take more recursive steps, memory constraints and the cost of additions with additional recursive steps cause Strassen's algorithm to be on par with these other algorithms.</p><p>3. Although Strassen's algorithm has the highest performance for square matrices, other fast algorithms have higher performance for N × 1600 × N and N × 2400 × 2400 problem sizes (Figure <ref type="figure" target="#fig_5">6</ref>). The reason is that the fixed dimension constrains the number of recursive steps. With multiple recursive steps, the matrix sub-blocks become small enough so that dgemm does not achieve good performance on the sub-problem. Thus, algo-rithms that get a better speedup per recursive step have higher performance for these problem sizes.</p><p>4. For rectangular matrices, algorithms that "match the shape" of the problem tend to perform the best. For example, 4, 2, 4 and 3, 2, 3 both have the "outer product" shape of the N × 1600 × N problem sizes and have the highest performance. Similarly, 4, 2, 3 and 4, 3, 3 have the highest performance of the exact algorithms for N × 2400 × 2400 problem sizes. The 4, 2, 4 and 4, 3, 3 algorithms provide around a 5% performance improvement over Strassen's algorithm and a 10% performance improvement over MKL on N × 1600 × N and N × 2400 × 2400, respectively. The reason follows from the performance explanation from Result 3. Only one or two steps of recursion improve performance. Thus, algorithms that match the problem shape and have high speedups per step perform the best.</p><p>5. Bini's 3, 2, 2 APA algorithm typically has the highest performance on rectangular problem sizes. However, we remind the reader that the approximation used by this algorithm results in severe numerical errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Parallel Performance</head><p>Figure <ref type="figure">7</ref> shows the parallel performance for multiplying square matrices and Figure <ref type="figure" target="#fig_7">8</ref> shows the parallel performance for N × 2800 × N and N × 3000 × 3000 problem sizes. We include performance on both 6 and 24 cores in order to illustrate the bandwidth issues discussed in Section 4.5. We observe the following patterns in the parallel performance data:  We also benchmarked the asymptotically fastest implementation of square matrix multiplication. The algorithm consists of composing 3, 3, 6 , 3, 6, 3 , 6, 3, 3 base cases. At the first recursive level, we use 3, 3, 6 ; at the second level 3, 6, 3 ; and at the third, 6, 3, 3 . The composed fast algorithm is for 3 • 3 • 6, 3 • 6 • 3, 6 • 3 • 3 = 54, 54, 54 . Each step of the composed algorithm computes 40 3 = 64000 matrix multiplications. The asymptotic complexity of this algorithm is Θ(N ω 0 ), with ω 0 = 3 log 54 (40) ≈ 2.775.</p><p>Although this algorithm is asymptotically the fastest, it does not perform well for the problem sizes considered in our experi-ments. For example, with 6 cores and BFS parallelism, the algorithm achieved only 8.4 effective GFLOPS/core multiplying square with dimension N = 13000. This is far below MKL's performance (Figure <ref type="figure">7</ref>). We conclude that while the algorithm may be of theoretical interest, it does not perform well on the modest problem sizes of interest on shared memory machines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>Our code generation framework lets us benchmark a large number of existing and new fast algorithms and test a variety of implementation details, such as how to handle matrix additions and how to implement the parallelism. However, we performed only high-level optimizations; we believe more detailed tuning of fast algorithms can provide performance gains. Based on the performance results we obtain in this work, we can draw several conclusions in bridging the gap between the theory and practice of fast algorithms.</p><p>First, in the case of multiplying square matrices, Strassen's algorithm consistently dominates the performance of exact algorithms (in sequential and parallel). Even though Smirnov's exact algorithm and Schönhage's APA algorithm are asymptotically faster in theory, they never outperform Strassen's for reasonable matrix dimensions in practice (sequential or parallel). This sheds some doubt on the prospect of finding a fast algorithm that will outperform Strassen's on square matrices; it will likely need to have a small base case and still offer a significant reduction in multiplications.</p><p>On the other hand, another conclusion from our performance results is that for multiplying rectangular matrices (which occurs more frequently than square in practice), there is a rich space for improvements. In particular, fast algorithms with base cases that match the shape of the matrices tend to have the highest performance. There are many promising algorithms, and we suspect that algorithm-specific optimizations will prove fruitful.</p><p>Third, in the search for new fast algorithms, our results confirm the importance of the (secondary) metric of sparsity of the U, V, W factor matrices. Although the arithmetic cost associated with the sparsity is negligible in practice, the communication cost associated with each nonzero can be performance limiting. We note that the communication costs of the streaming additions algorithm  <ref type="formula" target="#formula_12">3</ref>)) of fast algorithms on rectangular problems using only 6 cores (top row) and all 24 cores (bottom row). Problem sizes are an "outer product" shape, N × 2800 × N and multiplication of tall-and-skinny matrix by a small square matrix, N × 3000 × 3000. With six cores, all fast algorithms outperform MKL, and new fast algorithms achieve about a 5% performance gain over Strassen's algorithm. With 24 cores, bandwidth is a bottleneck and MKL outperforms fast algorithms. is independent of the sparsity, but the highest-performing additions algorithm in practice is the write-once algorithm, which is sensitive to the number of nonzeros.</p><p>Fourth, we have identified a parallel scaling impediment for fast algorithms on shared-memory architectures. Because the memory bandwidth often does not scale with the number of cores, and because the additions and multiplications are separate computations in our framework, the overhead of the additions compared to the multiplications worsens in the parallel case. This hardware bottleneck is unavoidable on most shared-memory architectures, though we note that it does not occur in distributed memory where aggregate memory bandwidth scales with the number of nodes.</p><p>We would like to extend our framework to the distributedmemory case, in part because of the better prospects for parallel scaling. A larger fraction of the time is spent in communication for the classical algorithm on this architecture, and fast algorithms can reduce the communication cost in addition to the computational cost in this case <ref type="bibr" target="#b2">[3]</ref>. Similar code generation techniques will be helpful in exploring performance in this case.</p><p>As matrix multiplication is the main computational kernel in linear algebra libraries, we also want to incorporate these fast algorithms into frameworks like BLIS <ref type="bibr" target="#b34">[35]</ref> and PLASMA <ref type="bibr" target="#b25">[26]</ref> to see how they affect a broader class of numerical algorithms.</p><p>Finally, we have not explored the numerical stability of the exact algorithms in order to compare their results. While theoretical bounds can be derived from each algorithm's U, V, W representation, it is an open question which algorithmic properties are most influential in practice; our framework will allow for rapid empirical testing. As numerical stability is an obstacle to widespread use of fast algorithms, extensive testing can help alleviate (or confirm) common concerns.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Proposition 2 . 3 .</head><label>23</label><figDesc>If U, V, W is a fast algorithm for M, K, N , then the following are also fast algorithms for M, K, N : UP, VP, WP for any permutation matrix P; UD x , VD y , WD z for any diagonal matrices D x , D y , and D z such that D x D y D z = I;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Effective performance (Equation (3)) of our code generator's implementation of Strassen's algorithm against MKL's dgemm and a tuned implementation of the Strassen-Winograd algorithm<ref type="bibr" target="#b8">[9]</ref>. The problems sizes are square. The generated code easily outperforms MKL and is competitive with the tuned code.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Effective performance (Equation (3)) comparison of common subexpression elimination (CSE) and the three matrix addition methods: write-once, streaming, and pairwise (see Section 3.2). The 4, 2, 4 fast algorithm computed N × 1600 × N ("outer product" shape) for varying N, and the 4, 2, 3 fast algorithm computed N × N × N (square multiplication). Write-once with no CSE tends to have the highest performance, especially for the 4, 2, 4 algorithm. Pairwise is slower because it performs more reads and writes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Effective performance (Equation (3)) comparison of the BFS, DFS, and HYBRID parallel implementations on representative fast algorithms and problem sizes. We use 6 and 24 cores to show the bandwidth limitations of the matrix additions. (Left): Strassen's algorithm on square problems. With 6 cores, we see significant speedups on large problems. (Middle): The 4, 2, 4 fast algorithm (26 multiplies) on N × 2800 × N problems. HYBRID performs the best in all cases. With 6 cores, the fast algorithm consistently outperforms MKL. With 24 cores, the fast algorithm can achieve significant speedups for small problem sizes. (Right): The 4, 3, 3 fast algorithm (29 multiplies) on N × 3000 × 3000 problems. HYBRID again performs the best. With 6 cores, the fast algorithm gets modest speedups over MKL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Effective sequential performance (Equation (3)) of a variety of fast algorithms on N × N × N problem sizes distributed across three plots. Each data point is the best of one, two, or three steps of recursion-additional recursive steps did not improve performance. Bini and Schönhage are approximate algorithms, and all others are exact fast algorithms. MKL and Strassen's are repeated on all three plots for comparison. All of the fast algorithms outperform MKL for large enough problem sizes, and Strassen's algorithm usually performs the best.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Effective sequential performance (Equation (3)) of fast matrix multiplication algorithms on rectangular problem sizes. (Left):Performance on an "outer product" shape, N × 1600 × N. Exact fast algorithms that have a similar outer product shape (e.g., 4, 2, 4 ) tend to have the highest performance. (Right): Performance of multiplication of tall-and-skinny matrix by a small square matrix, N × 2400 × 2400. Again, fast algorithms that have this shape (e.g., 4, 3, 3 ) tend to have the highest performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure</head><label></label><figDesc>Figure 7. Effective parallel performance (Equation (3)) of fast algorithms on square problems using only 6 cores (top row) and all 24 cores (bottom row). With 6 cores, bandwidth is not a bottleneck and we see similar trends to the sequential algorithms. With 24 cores, speedups over MKL are less dramatic, but Strassen's (bottom left), 3, 3, 2 (bottom left), and 4, 3, 3 (bottom right) all outperform MKL and have similar performance. Bini and Schöhage have high performance, but they are APA algorithms and suffer from severe numerical problems.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Effective parallel performance (Equation (3)) of fast algorithms on rectangular problems using only 6 cores (top row) and all 24 cores (bottom row). Problem sizes are an "outer product" shape, N × 2800 × N and multiplication of tall-and-skinny matrix by a small square matrix, N × 3000 × 3000. With six cores, all fast algorithms outperform MKL, and new fast algorithms achieve about a 5% performance gain over Strassen's algorithm. With 24 cores, bandwidth is a bottleneck and MKL outperforms fast algorithms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc><ref type="bibr" target="#b6">7</ref>. Effective parallel performance (Equation (3)) of fast algorithms on square problems using only 6 cores (top row) and all 24 cores (bottom row). With 6 cores, bandwidth is not a bottleneck and we see similar trends to the sequential algorithms. With 24 cores, speedups over MKL are less dramatic, but Strassen's (bottom left), 3, 3, 2 (bottom left), and 4, 3, 3 (bottom right) all outperform MKL and have similar performance. Bini and Schöhage have high performance, but they are APA algorithms and suffer from severe numerical problems.1. With 6 cores, bandwidth scaling is not a problem, and we find many of the same trends as in the sequential case. All fast algorithms outperform MKL. Apart from the APA algorithms, Strassen's algorithm is typically fastest for square matrices. The 3, 2, 3 fast algorithm has the highest performance for the N × 2800 × N problem sizes, while 4, 3, 3 and 4, 2, 3 have the highest performance for the N × 3000 × 3000. These algorithms match the shape of the problem.</figDesc><table><row><cell>2. With 24 cores, MKL's dgemm is typically the highest perform-</cell></row><row><cell>ing algorithm for rectangular problem sizes (bottom row of Fig-</cell></row><row><cell>ure 8). In these problems, the ratio of time spent in additions to</cell></row><row><cell>time spent in multiplications is too large, and bandwidth limita-</cell></row><row><cell>tions prevent the fast algorithms from outperforming MKL.</cell></row><row><cell>3. With 24 cores and square problem sizes (bottom row of Fig-</cell></row><row><cell>ure 7), several algorithms outperform MKL. Strassen's algo-</cell></row><row><cell>rithm provides a modest speedup over MKL (around 5%) and</cell></row><row><cell>is one of highest performing exact algorithms. The 4, 3, 3 and</cell></row><row><cell>4, 2, 4 fast algorithms outperform MKL and are competitive</cell></row><row><cell>with Strassen's algorithm. The square problem sizes spend a</cell></row><row><cell>large fraction of time in matrix multiplication, so the bandwidth</cell></row><row><cell>costs for the matrix additions have less impact on performance.</cell></row><row><cell>4. Again, the APA algorithms (Bini's and Schönhage's) have high</cell></row><row><cell>performance on rectangular problem sizes. It is still an open</cell></row><row><cell>question if there exists a fast algorithm with the same complex-</cell></row><row><cell>ity as Schönhage's algorithm. Our results show that a significant</cell></row><row><cell>performance gain is possible with such an algorithm.</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://www.top500.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Here ( * ) denotes element-wise vector multiplication.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Because daxpy computes y ← αx + y, we make a call for each addition in the chain as well as one call for an initial copy.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>Note that the inverse is not necessarily true, the speedup depends on the overhead of the additions.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>This result is an artifact of MKL's parallelization on these problem sizes and is not due to the speedups of the fast algorithm. We achieved</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was supported in part by an appointment to the Sandia National Laboratories Truman Fellowship in National Security Science and Engineering, sponsored by Sandia Corporation (a wholly owned subsidiary of Lockheed Martin Corporation) as Operator of Sandia National Laboratories under its U.S. Department of Energy Contract No. DE-AC04-94AL85000. Austin R. Benson is also supported by an Office of Technology Licensing Stanford Graduate Fellowship. This research used resources of the National Energy Research Scientific Computing Center, which is supported by the Office of Science of the U.S. Department of Energy under Contract No. DE-AC02-05CH11231.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">AMD core math library user guide</title>
		<author>
			<persName><surname>Amd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2014. Version 6.0</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Extra high speed matrix multiplication on the Cray-2</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Scientific and Statistical Computing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="603" to="607" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Communication-optimal parallel algorithm for Strassen&apos;s matrix multiplication</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ballard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Holtz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lipshitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM Symposium on Parallelism in Algorithms and Architectures</title>
		<meeting>the 24th ACM Symposium on Parallelism in Algorithms and Architectures</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="193" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Graph expansion and communication costs of fast matrix multiplication</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ballard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Holtz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">32</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">O(n 2.7799 ) complexity for n × n approximate matrix multiplication</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Capovani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Romani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lotti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing Letters</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="234" to="235" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Approximate solutions for the bilinear form computational problem</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Romani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="692" to="697" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Algorithms for matrix multiplication</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Brent</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1970">1970</date>
			<pubPlace>Stanford, CA, USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Cray application developer&apos;s environment user&apos;s guide</title>
		<author>
			<persName><surname>Cray</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2012. Release 3.1</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Exploiting parallelism in matrix-computation kernels for symmetric multiprocessor systems</title>
		<author>
			<persName><forename type="first">P</forename><surname>D'alberto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bodrato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nicolau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Mathematical Software</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On varieties of optimal algorithms for the computation of bilinear mappings I. the isotropy group of a bilinear mapping</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">F</forename><surname>De Groote</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">GEMMW: a portable level 3 BLAS Winograd variant of Strassen&apos;s matrix-matrix multiply algorithm</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Heroux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Slishman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Physics</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Powers of tensors and fast matrix multiplication</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Symbolic and Algebraic Computation</title>
		<meeting>the International Symposium on Symbolic and Algebraic Computation</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="296" to="303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A high performance parallel Strassen implementation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Grayson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Van De Geijn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Parallel Processing Letters</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">01</biblScope>
			<biblScope unit="page" from="3" to="12" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Accuracy and stability of numerical algorithms</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Higham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Duality applied to the complexity of matrix multiplication and other bilinear forms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hopcroft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Musinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="159" to="173" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On minimizing the number of multiplications necessary for matrix multiplication</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Hopcroft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Kerr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="30" to="36" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Strassen&apos;s algorithm for matrix multiplication: Modeling, analysis, and implementation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Huss-Lederman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Jacobson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tsao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Turnbull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Supercomputing &apos;96</title>
		<meeting>Supercomputing &apos;96</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="9" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Engineering and scientific software library guide and reference</title>
		<author>
			<persName><surname>Ibm</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
	<note>Release 3</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Math kernel library reference manual</title>
		<author>
			<persName><surname>Intel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2014. Version 11.2</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Communication lower bounds for distributed-memory matrix multiplication</title>
		<author>
			<persName><forename type="first">D</forename><surname>Irony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Toledo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tiskin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Parallel and Distributed Computing</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1017" to="1026" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Noncommutative bilinear algorithms for 3 x 3 matrix multiplication</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Mcloughlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="595" to="603" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The aggregation and cancellation techniques as a practical tool for faster matrix multiplication</title>
		<author>
			<persName><forename type="first">I</forename><surname>Kaporin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">315</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="469" to="510" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The Art of Computer Programming</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Knuth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Seminumerical Algorithms, 2nd Edition</title>
		<imprint>
			<publisher>Addison-Wesley</publisher>
			<date type="published" when="1981">1981</date>
			<biblScope unit="volume">II</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Tensor decompositions and applications</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Kolda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Bader</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Review</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="455" to="500" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A tensor product formulation of Strassen&apos;s matrix multiplication algorithm with memory reduction</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sadayappan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Programming</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="275" to="289" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multithreading in the PLASMA library</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kurzak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luszczek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yarkhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Faverge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Langou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bouwmeester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dongarra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Dongarra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Faverge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Herault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multicore Computing: Algorithms, Architectures, and Applications</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">119</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Communicationavoiding parallel Strassen: Implementation and performance</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lipshitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ballard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference for High Performance Computing, Networking, Storage, and Analysis</title>
		<meeting>the International Conference for High Performance Computing, Networking, Storage, and Analysis</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page">101</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A survey of memory bandwidth and machine balance in current high performance computers</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Mccalpin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TCCA Newsletter</title>
		<imprint>
			<biblScope unit="page" from="19" to="25" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Strassen&apos;s algorithm is not optimal: Trilinear technique of aggregating, uniting and canceling for constructing fast algorithms for matrix operations</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">Y</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Symposium on Foundations of Computer Science</title>
		<meeting>the IEEE Symposium on Foundations of Computer Science</meeting>
		<imprint>
			<date type="published" when="1978">1978</date>
			<biblScope unit="page" from="166" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Partial and total matrix multiplication</title>
		<author>
			<persName><forename type="first">A</forename><surname>Schönhage</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="434" to="455" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The bilinear complexity and practical algorithms for matrix multiplication</title>
		<author>
			<persName><forename type="first">A</forename><surname>Smirnov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Mathematics and Mathematical Physics</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1781" to="1795" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Gaussian elimination is not optimal</title>
		<author>
			<persName><forename type="first">V</forename><surname>Strassen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Numerische Mathematik</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="354" to="356" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Tuning Strassen&apos;s matrix multiplication for memory efficiency</title>
		<author>
			<persName><forename type="first">M</forename><surname>Thottethodi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Lebeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference for High Performance Computing, Networking, Storage, and Analysis</title>
		<meeting>the International Conference for High Performance Computing, Networking, Storage, and Analysis</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">SUMMA: Scalable universal matrix multiplication algorithm</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Van De Geijn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Watts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Concurrency-Practice and Experience</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="255" to="274" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">BLIS: A framework for rapidly instantiating BLAS functionality</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">G</forename><surname>Van Zee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Van De Geijn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Mathematical Software</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multiplying matrices faster than Coppersmith-Winograd</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">V</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Forty-Fourth Annual ACM Symposium on Theory of Computing</title>
		<meeting>the Forty-Fourth Annual ACM Symposium on Theory of Computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="887" to="898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">On multiplication of 2 × 2 matrices</title>
		<author>
			<persName><forename type="first">S</forename><surname>Winograd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linear Algebra and its Applications</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="381" to="388" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
