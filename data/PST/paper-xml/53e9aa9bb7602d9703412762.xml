<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Estimation and Use of Uncertainty in Pseudo-relevance Feedback</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kevyn</forename><surname>Collins-Thompson</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Language Technologies Institute School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213-8213</postCode>
									<region>PA</region>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Jamie</forename><surname>Callan</surname></persName>
							<email>kct|callan@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Language Technologies Institute School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213-8213</postCode>
									<region>PA</region>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Estimation and Use of Uncertainty in Pseudo-relevance Feedback</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BCE0EF680C757ACD35D159C1FCC4D74F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3.3 [Information Retrieval]: Retrieval Models Algorithms</term>
					<term>Experimentation Query expansion</term>
					<term>pseudo-relevance feedback</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing pseudo-relevance feedback methods typically perform averaging over the top-retrieved documents, but ignore an important statistical dimension: the risk or variance associated with either the individual document models, or their combination. Treating the baseline feedback method as a black box, and the output feedback model as a random variable, we estimate a posterior distribution for the feedback model by resampling a given query's top-retrieved documents, using the posterior mean or mode as the enhanced feedback model. We then perform model combination over several enhanced models, each based on a slightly modified query sampled from the original query. We find that resampling documents helps increase individual feedback model precision by removing noise terms, while sampling from the query improves robustness (worst-case performance) by emphasizing terms related to multiple query aspects. The result is a meta-feedback algorithm that is both more robust and more precise than the original strong baseline method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Uncertainty is an inherent feature of information retrieval. Not only do we not know the queries that will be presented to our retrieval algorithm ahead of time, but the user's information need may be vague or incompletely specified by these queries. Even if the query were perfectly specified, language in the collection documents is inherently complex and ambiguous and matching such language effectively is a formidable problem by itself. With this in mind, we wish to treat many important quantities calculated by the re-trieval system, whether a relevance score for a document, or a weight for a query expansion term, as random variables whose true value is uncertain but where the uncertainty about the true value may be quantified by replacing the fixed value with a probability distribution over possible values. In this way, retrieval algorithms may attempt to quantify the risk or uncertainty associated with their output rankings, or improve the stability or precision of their internal calculations.</p><p>Current algorithms for pseudo-relevance feedback (PRF) tend to follow the same basic method whether we use vector space-based algorithms such as Rocchio's formula <ref type="bibr" target="#b16">[16]</ref>, or more recent language modeling approaches such as Relevance Models <ref type="bibr" target="#b10">[10]</ref>. First, a set of top-retrieved documents is obtained from an initial query and assumed to approximate a set of relevant documents. Next, a single feedback model vector is computed according to some sort of average, centroid, or expectation over the set of possibly-relevant document models. For example, the document vectors may be combined with equal weighting, as in Rocchio, or by query likelihood, as may be done using the Relevance Model<ref type="foot" target="#foot_0">1</ref> . The use of an expectation is reasonable for practical and theoretical reasons, but by itself ignores potentially valuable information about the risk of the feedback model.</p><p>Our main hypothesis in this paper is that estimating the uncertainty in feedback is useful and leads to better individual feedback models and more robust combined models. Therefore, we propose a method for estimating uncertainty associated with an individual feedback model in terms of a posterior distribution over language models. To do this, we systematically vary the inputs to the baseline feedback method and fit a Dirichlet distribution to the output. We use the posterior mean or mode as the improved feedback model estimate. This process is shown in Figure <ref type="figure" target="#fig_0">1</ref>. As we show later, the mean and mode may vary significantly from the single feedback model proposed by the baseline method. We also perform model combination using several improved feedback language models obtained by a small number of new queries sampled from the original query. A model's weight combines two complementary factors: the model's probability of generating the query, and the variance of the model, with high-variance models getting lower weight. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">SAMPLING-BASED FEEDBACK</head><p>In Sections 2.1-2.5 we describe a general method for estimating a probability distribution over the set of possible language models. In Sections 2.6 and 2.7 we summarize how different query samples are used to generate multiple feedback models, which are then combined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Modeling Feedback Uncertainty</head><p>Given a query Q and a collection C, we assume a probabilistic retrieval system that assigns a real-valued document score f (D, Q) to each document D in C, such that the score is proportional to the estimated probability of relevance. We make no other assumptions about f (D, Q). The nature of f (D, Q) may be complex: for example, if the retrieval system supports structured query languages <ref type="bibr" target="#b12">[12]</ref>, then f (D, Q) may represent the output of an arbitrarily complex inference network defined by the structured query operators. In theory, the scoring function can vary from query to query, although in this study for simplicity we keep the scoring function the same for all queries. Our specific query method is given in Section 3.</p><p>We treat the feedback algorithm as a black box and assume that the inputs to the feedback algorithm are the original query and the corresponding top-retrieved documents, with a score being given to each document. We assume that the output of the feedback algorithm is a vector of term weights to be used to add or reweight the terms in the representation of the original query, with the vector normalized to form a probability distribution. We view the the inputs to the feedback black box as random variables, and analyze the feedback model as a random variable that changes in response to changes in the inputs. Like the document scoring function f (D, Q), the feedback algorithm may implement a complex, non-linear scoring formula, and so as its inputs vary, the resulting feedback models may have a complex distribution over the space of feedback models (the sample space). Because of this potential complexity, we do not attempt to derive a posterior distribution in closed form, but instead use simulation. We call this distribution over possible feedback models the feedback model distribution. Our goal in this section is to estimate a useful approximation to the feedback model distribution.</p><p>For a specific framework for experiments, we use the language modeling (LM) approach for information retrieval <ref type="bibr" target="#b15">[15]</ref>. The score of a document D with respect to a query Q and collection C is given by p(Q|D) with respect to language models θQ and θD estimated for the query and document respectively. We denote the set of k top-retrieved documents from collection C in response to Q by DQ(k, C). For simplicity, we assume that queries and documents are gen-erated by multinomial distributions whose parameters are represented by unigram language models.</p><p>To incorporate feedback in the LM approach, we assume a model-based scheme in which our goal is take the query and resulting ranked documents DQ(k, C) as input, and output an expansion language model θE, which is then interpolated with the original query model θQ:</p><formula xml:id="formula_0">θNew = (1 -α) • θQ + α • θE (1)</formula><p>This includes the possibility of α = 1 where the original query mode is completely replaced by the feedback model. Our sample space is the set of all possible language models LF that may be output as feedback models. Our approach is to take samples from this space and then fit a distribution to the samples using maximum likelihood. For simplicity, we start by assuming the latent feedback distribution has the form of a Dirichlet distribution. Although the Dirichlet is a unimodal distribution, and in general quite limited in its expressiveness in the sample space, it is a natural match for the multinomial language model, can be estimated quickly, and can capture the most salient features of confident and uncertain feedback models, such as the overall spread of the distibution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Resampling document models</head><p>We would like an approximation to the posterior distribution of the feedback model LF . To accomplish this, we apply a widely-used simulation technique called bootstrap sampling ( <ref type="bibr" target="#b7">[7]</ref>, p. 474) on the input parameters, namely, the set of top-retrieved documents.</p><p>Bootstrap sampling allows us to simulate the approximate effect of perturbing the parameters within the black box feedback algorithm by perturbing the inputs to that algorithm in a systematic way, while making no assumptions about the nature of the feedback algorithm.</p><p>Specifically, we sample k documents with replacement from DQ(k, C), and calculate an expansion language model θ b using the black box feedback method. We repeat this process B times to obtain a set of B feedback language models, to which we then fit a Dirichlet distribution. Typically B is in the range of 20 to 50 samples, with performance being relatively stable in this range. Note that instead of treating each top document as equally likely, we sample according to the estimated probabilities of relevance of each document in DQ(k, C). Thus, a document is more likely to be chosen the higher it is in the ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Justification for a sampling approach</head><p>The rationale for our sampling approach has two parts. First, we want to improve the quality of individual feedback models by smoothing out variation when the baseline feedback model is unstable. In this respect, our approach resembles bagging <ref type="bibr" target="#b4">[4]</ref>, an ensemble approach which generates multiple versions of a predictor by making bootstrap copies of the training set, and then averages the (numerical) predictors. In our application, top-retrieved documents can be seen as a kind of noisy training set for relevance.</p><p>Second, sampling is an effective way to estimate basic properties of the feedback posterior distribution, which can then be used for improved model combination. For example, a model may be weighted by its prediction confidence, estimated as a function of the variability of the posterior around the model.  <ref type="figure">2</ref>: Visualization of expansion language model variance using self-organizing maps, showing the distribution of language models that results from resampling the inputs to the baseline expansion method. The language model that would have been chosen by the baseline expansion is at the center of each map. The similarity function is Jensen-Shannon divergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Visualizing feedback distributions</head><p>Before describing how we fit and use the Dirichlet distribution over feedback models, it is instructive to view some examples of actual feedback model distributions that result from bootstrap sampling the top-retrieved documents from different TREC topics.</p><p>Each point in our sample space is a language model, which typically has several thousand dimensions. To help analyze the behavior of our method we used a Self-Organizing Map (via the SOM-PAK package <ref type="bibr" target="#b9">[9]</ref>), to 'flatten' and visualize the high-dimensional density function <ref type="foot" target="#foot_1">2</ref> .</p><p>The density maps for three TREC topics are shown in Figure <ref type="figure">2</ref> above. The dark areas represent regions of high similarity between language models. The light areas represent regions of low similarity -the 'valleys' between clusters. Each diagram is centered on the language model that would have been chosen by the baseline expansion. A single peak (mode) is evident in some examples, but more complex structure appears in others. Also, while the distribution is usually close to the baseline feedback model, for some topics they are a significant distance apart (as measured by Jensen-Shannon divergence), as in Subfigure 2c. In such cases, the mode or mean of the feedback distribution often performs significantly better than the baseline (and in a smaller proportion of cases, significantly worse).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Fitting a posterior feedback distribution</head><p>After obtaining feedback model samples by resampling the feedback model inputs, we estimate the feedback distribution. We assume that the multinomial feedback models { θ1, . . . , θB} were generated by a latent Dirichlet distribution with parameters {α1, . . . , αN }. To estimate the {α1, . . . , αN }, we fit the Dirichlet parameters to the B language model samples according to maximum likelihood using a generalized Newton procedure, details of which are given in Minka <ref type="bibr" target="#b13">[13]</ref>. We assume a simple Dirichlet prior over the {α1, . . . , αN }, setting each to αi = μ • p(wi | C), where μ is a parameter and p(• | C) is the collection language model estimated from a set of documents from collection C. The parameter fitting converges very quickly -typically just 2 or 3 iterations are enough -so that it is practical to apply at query-time when computational overhead must be small. In practice, we can restrict the calculation to the vocabulary of the top-retrieved documents, instead of the entire collection. Note that for this step we are re-using the existing retrieved documents and not performing additional queries.</p><p>Given the parameters of an N -dimensional Dirichlet distribution Dir(α) the mean μ and mode x vectors are easy to calculate and are given respectively by</p><formula xml:id="formula_1">μi = α i P α i</formula><p>(2) and xi = α i -1</p><formula xml:id="formula_2">P α i -N .</formula><p>(3) We can then choose the language model at the mean or the mode of the posterior as the final enhanced feedback model. (We found the mode to give slightly better performance.)</p><p>For information retrieval, the number of samples we will have available is likely to be quite small for performance reasons -usually less than ten. Moreover, while random sampling is useful in certain cases, it is perfectly acceptable to allow deterministic sampling distributions, but these must be designed carefully in order to approximate an accurate output variance. We leave this for future study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Query variants</head><p>We use the following methods for generating variants of the original query. Each variant corresponds to a different assumption about which aspects of the original query may be important. This is a form of deterministic sampling. We selected three simple methods that cover complimentary assumptions about the query.</p><p>No-expansion Use only the original query. The assumption is that the given terms are a complete description of the information need.</p><p>Leave-one-out A single term is left out of the original query.</p><p>The assumption is that one of the query terms is a noise term.</p><p>Single-term A single term is chosen from the original query. This assumes that only one aspect of the query, namely, that represented by the term, is most important.</p><p>After generating a variant of the original query, we combine it with the original query using a weight αSUB so that we do not stray too 'far'. In this study, we set αSUB = 0.5. For example, using the Indri <ref type="bibr" target="#b12">[12]</ref> query language, a leave-oneout variant of the initial query that omits the term 'ireland' for TREC topic 404 is: #weight(0.5 #combine(ireland peace talks) 0.5 #combine(peace talks))</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">Combining enhanced feedback models from multiple query variants</head><p>When using multiple query variants, the resulting enhanced feedback models are combined using Bayesian model combination. To do this, we treat each word as an item to be classified as belonging to a relevant or non-relevant class, and derive a class probability for each word by combining the scores from each query variant. Each score is given by that term's probability in the Dirichlet distribution. The term scores are weighted by the inverse of the variance of the term in the enhanced feedback model's Dirichlet distribution. The prior probability of a word's membership in the relevant class is given by the probability of the original query in the entire enhanced expansion model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EVALUATION</head><p>In this section we present results confirming the usefulness of estimating a feedback model distribution from weighted resampling of top-ranked documents, and of combining the feedback models obtained from different small changes in the original query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">General method</head><p>We evaluated performance on a total of 350 queries derived from four sets of TREC topics: 51-200 (TREC-1&amp;2), 351-400 (TREC-7), 401-450 (TREC-8), and 451-550 (wt10g, TREC-9&amp;10). We chose these for their varied content and document properties. For example, wt10g documents are Web pages with a wide variety of subjects and styles while TREC-1&amp;2 documents are more homogeneous news articles. Indexing and retrieval was performed using the Indri system in the Lemur toolkit <ref type="bibr" target="#b12">[12]</ref> [1]. Our queries were derived from the words in the title field of the TREC topics. Phrases were not used. To generate the baseline queries passed to Indri, we wrapped the query terms with Indri's #combine operator. For example, the initial query for topic 404 is: #combine(ireland peace talks)</p><p>We performed Krovetz stemming for all experiments. Because we found that the baseline (Indri) expansion method performed better using a stopword list with the feedback model, all experiments used a stoplist of 419 common English words. However, an interesting side-effect of our resampling approach is that it tends to remove many stopwords from the feedback model, making a stoplist less critical. This is discussed further in Section 3.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Baseline feedback method</head><p>For our baseline expansion method, we use an algorithm included in Indri 1.0 as the default expansion method. This method first selects terms using a log-odds calculation described by Ponte <ref type="bibr" target="#b14">[14]</ref>, but assigns final term weights using Lavrenko's relevance model <ref type="bibr" target="#b10">[10]</ref>.</p><p>We chose the Indri method because it gives a consistently strong baseline, is based on a language modeling approach, and is simple to experiment with. In a TREC evaluation using the GOV2 corpus <ref type="bibr">[6]</ref>, the method was one of the topperforming runs, achieving a 19.8% gain in MAP compared to using unexpanded queries. In this study, it achieves an average gain in MAP of 17.25% over the four collections.</p><p>Indri's expansion method first calculates a log-odds ratio o(v) for each potential expansion term v given by</p><formula xml:id="formula_3">o(v) = X D log p(v|D) p(v|C)<label>(4)</label></formula><p>over all documents D containing v, in collection C. Then, the expansion term candidates are sorted by descending o(v), and the top m are chosen. Finally, the term weights r(v) used in the expanded query are calculated based on the relevance model</p><formula xml:id="formula_4">r(v) = X D p(q|D)p(v|D) p(v) p(D)<label>(5)</label></formula><p>The quantity p(q|D) is the probability score assigned to the document in the initial retrieval. We use Dirichlet smoothing of p(v|D) with μ = 1000. This relevance model is then combined with the original query using linear interpolation, weighted by a parameter α.</p><p>By default we used the top 50 documents for feedback and the top 20 expansion terms, with the feedback interpolation parameter α = 0.5 unless otherwise stated. For example, the baseline expanded query for topic 404 is: #weight(0.5 #combine(ireland peace talks) 0.5 #weight(0.10 ireland 0.08 peace 0.08 northern ...)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Expansion performance</head><p>We measure our feedback algorithm's effectiveness by two main criteria: precision, and robustness. Robustness, and the tradeoff between precision and robustness, is analyzed in Section 3.4. In this section, we examine average precision and precision in the top 10 documents (P10). We also include recall at 1,000 documents.</p><p>For each query, we obtained a set of B feedback models using the Indri baseline. Each feedback model was obtained from a random sample of the top k documents taken with replacement. For these experiments, B = 30 and k = 50. Each feedback model contained 20 terms. On the query side, we used leave-one-out (LOO) sampling to create the query variants. Single-term query sampling had consistently worse performance across all collections and so our results here focus on LOO sampling. We used the methods described in Section 2 to estimate an enhanced feedback model from the Dirichlet posterior distribution for each query variant, and to combine the feedback models from all the query variants. We call our method 'resampling expansion' and denote it as RS-FB here. We denote the Indri baseline feedback method as Base-FB. Results from applying both the baseline expansion method (Base-FB) and resampling expansion (RS-FB) are shown in Table <ref type="table" target="#tab_1">1</ref>.</p><p>We observe several trends in this table. First, the average precision of RS-FB was comparable to Base-FB, achieving an average gain of 17.6% compared to using no expansion across the four collections. The Indri baseline expansion gain was 17.25%. Also, the RS-FB method achieved consistent improvements in P10 over Base-FB for every topic set, with an average improvement of 6.89% over Base-FB for all 350 topics. The lowest P10 gain over Base-FB was +3.82% for TREC-7 and the highest was +11.95% for wt10g. Finally, both Base-FB and RS-FB also consistently improved recall over using no expansion, with Base-FB achieving better recall than RS-FB for all topic sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Retrieval robustness</head><p>We use the term robustness to mean the worst-case average precision performance of a feedback algorithm. Ideally, a robust feedback method would never perform worse than using the original query, while often performing better using the expansion.</p><p>To evaluate robustness in this study, we use a very simple measure called the robustness index (RI) <ref type="foot" target="#foot_2">3</ref> . For a set of queries Q, the RI measure is defined as:</p><formula xml:id="formula_5">RI(Q) = n+ -n- |Q| (<label>6</label></formula><formula xml:id="formula_6">)</formula><p>where n+ is the number of queries helped by the feedback method and nis the number of queries hurt. Here, by 'helped' we mean obtaining a higher average precision as a result of feedback. The value of RI ranges from a minimum   shown are the actual number of queries hurt by feedback (n-) for each method and collection. Queries for which initial average precision was negligible (≤ 0.01) were ignored, giving the remaining query count in column N .</p><p>of -1.0, when all queries are hurt by the feedback method, to +1.0 when all queries are helped. The RI measure does not take into account the magnitude or distribution of the amount of change across the set Q. However, it is easy to understand as a general indication of robustness.</p><p>One obvious way to improve the worst-case performance of feedback is simply to use a smaller fixed α interpolation parameter, such as α = 0.3, placing less weight on the (possibly risky) feedback model and more on the original query. We call this the 'small-α' strategy. Since we are also reducing the potential gains when the feedback model is 'right', however, we would expect some trade-off between average precision and robustness. We therefore compared the precision/robustness trade-off between our resampling feedback algorithm, and the simple small-α method. The results are summarized in Figure <ref type="figure" target="#fig_1">3</ref>. In the figure, the curve for each topic set interpolates between trade-off points, beginning at x=0, where α = 0.5, and continuing in the direction of the arrow as α decreases and the original query is given more and more weight. As expected, robustness continuously increases as we move along the curve, but mean average precision generally drops as the gains from feedback are eliminated. For comparison, the performance of resampling feedback at α = 0.5 is shown for each collection as the circled point. Higher and to the right is better. This figure shows that resampling feedback gives a somewhat better trade-off than the small-α approach for 3 of the 4 collections.  Table <ref type="table" target="#tab_2">2</ref> gives the Robustness Index scores for Base-FB and RS-FB. The RS-FB feedback method obtained higher robustness than Base-FB on three of the four topic sets, with only slightly worse performance on TREC-8.</p><p>A more detailed view showing the distribution over relative changes in AP is given by the histogram in Figure <ref type="figure" target="#fig_2">4</ref>. Compared to Base-FB, the RS-FB method achieves a noticable reduction in the number of queries significantly hurt by expansion (i.e. where AP is hurt by 25% or more), while preserving positive gains in AP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Effect of query and document sampling methods</head><p>Given our algorithm's improved robustness seen in Section 3.4, an important question is what component of our system is responsible. Is it the use of document re-sampling, the use of multiple query variants, or some other factor? The results in Table <ref type="table" target="#tab_3">3</ref> suggest that the model combination based on query variants may be largely account for the improved robustness. When query variants are turned off and the original query is used by itself with document sampling, there is little net change in average precision, a small decrease in P10 for 3 out of the 4 topic sets, but a significant drop in robustness for all topic sets. In two cases, the RI measure drops by more than 50%.</p><p>We also examined the effect of the document sampling method on retrieval effectiveness, using two different strategies. The 'uniform weighting' strategy ignored the relevance scores from the initial retrieval and gave each document in the top k the same probability of selection. In contrast, the 'relevance-score weighting' strategy chose documents with probability proportional to their relevance scores. In this way, documents that were more highly ranked were more likely to be selected. Results are shown in Table <ref type="table" target="#tab_4">4</ref>.</p><p>The relevance-score weighting strategy performs better overall, with significantly higher RI and P10 scores on 3 of the 4 topic sets. The difference in average precision between the methods, however, is less marked. This suggests that uniform weighting acts to increase variance in retrieval results: when initial average precision is high, there are many relevant documents in the top k and uniform sampling may give a more representative relevance model than focusing on the highly-ranked items. On the other hand, when initial precision is low, there are few relevant documents in the bottom ranks and uniform sampling mixes in more of the non-relevant documents.</p><p>For space reasons we only summarize our findings on sample size here. The number of samples has some effect on precision when less than 10, but performance stabilizes at around 15 to 20 samples. We used 30 samples for our experiments. Much beyond this level, the additional benefits of more samples decrease as the initial score distribution is more closely fit and the processing time increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">The effect of resampling on expansion term quality</head><p>Ideally, a retrieval model should not require a stopword list when estimating a model of relevance: a robust statistical model should down-weight stopwords automatically depending on context. Stopwords can harm feedback if selected as feedback terms, because they are typically poor discriminators and waste valuable term slots. In practice, however, because most term selection methods resemble a tf • idf type of weighting, terms with low idf but very high tf can sometimes be selected as expansion term candidates. This happens, for example, even with the Relevance Model approach that is part of our baseline feedback. To ensure as strong a baseline as possible, we use a stoplist for all experiments reported here. If we turn off the stopword list, however, we obtain results such as those shown in Table <ref type="table" target="#tab_5">5</ref> where four of the top ten baseline feedback terms for TREC topic 60 (said, but, their, not) are stopwords using the Base-FB method. (The top 100 expansion terms were selected to generate this example.)</p><p>Indri's method attempts to address the stopword problem by applying an initial step based on Ponte <ref type="bibr" target="#b14">[14]</ref> to select less-common terms that have high log-odds of being in the top-ranked documents compared to the whole collection. Nevertheless, this does not overcome the stopword problem completely, especially as the number of feedback terms grows.</p><p>Using resampling feedback, however, appears to mitigate the effect of stopwords automatically. In the example of Table 5, resampling feedback leaves only one stopword (their) in the top ten. We observed similar feedback term behavior across many other topics. The reason for this effect appears to be the interaction of the term selection score with the top-m term cutoff. While the presence and even proportion of particular stopwords is fairly stable across different document samples, their relative position in the top-m list is not, as sets of documents with varying numbers of better, lower-frequency term candidates are examined for each sample. As a result, while some number of stopwords may appear in each sampled document set, any given stopword tends to fall below the cutoff for multiple samples, leading to its classification as a high-variance, low-weight feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RELATED WORK</head><p>Our approach is related to previous work from several areas of information retrieval and machine learning. Our use of query variation was inspired by the work of YomTov et al. <ref type="bibr" target="#b21">[20]</ref>, Carpineto et al. <ref type="bibr" target="#b5">[5]</ref>, and Amati et al. <ref type="bibr" target="#b2">[2]</ref>, among others. These studies use the idea of creating multiple subqueries and then examining the nature of the overlap in the documents and/or expansion terms that result from each subquery. Model combination is performed using heuristics. In particular, the studies of Amati et al. and Carpineto et al. investigated combining terms from individual distributional methods using a term-reranking combination heuristic. In a set of TREC topics they found wide average variation in the rank-distance of terms from different expansion methods. Their combination method gave modest positive improvements in average precision.</p><p>The idea of examining the overlap between lists of suggested terms has also been used in early query expansion approaches. Xu and Croft's method of Local Context Analysis (LCA) <ref type="bibr" target="#b20">[19]</ref> includes a factor in the empirically-derived weighting formula that causes expansion terms to be preferred that have connections to multiple query terms.</p><p>On the document side, recent work by Zhou &amp; Croft <ref type="bibr" target="#b22">[21]</ref> explored the idea of adding noise to documents, re-scoring them, and using the stability of the resulting rankings as an estimate of query difficulty. This is related to our use of document sampling to estimate the risk of the feedback model built from the different sets of top-retrieved documents. Sakai et al. <ref type="bibr" target="#b18">[17]</ref> proposed an approach to improving the robustness of pseudo-relevance feedback using a method they call selective sampling. The essence of their method is that they allow skipping of some top-ranked documents, based on a clustering criterion, in order to select a more varied and novel set of documents later in the ranking for use by a traditional pseudo-feedback method. Their study did not find significant improvements in either robustness (RI) or MAP on their corpora.</p><p>Greiff, Morgan and Ponte <ref type="bibr" target="#b8">[8]</ref> explored the role of variance in term weighting. In a series of simulations that simplified the problem to 2-feature documents, they found that average precision degrades as term frequency variance -high noiseincreases. Downweighting terms with high variance resulted in improved average precision. This seems in accord with our own findings for individual feedback models.</p><p>Estimates of output variance have recently been used for improved text classification. Lee et al. <ref type="bibr" target="#b11">[11]</ref> used queryspecific variance estimates of classifier outputs to perform improved model combination. Instead of using sampling, they were able to derive closed-form expressions for classifier variance by assuming base classifiers using simple types of inference networks.</p><p>Ando and Zhang proposed a method that they call structural feedback <ref type="bibr" target="#b3">[3]</ref> and showed how to apply it to query expansion for the TREC Genomics Track. They used r query variations to obtain R different sets Sr of top-ranked documents that have been intersected with the top-ranked documents obtained from the original query qorig. For each Si, the normalized centroid vector ŵi of the documents is calculated. Principal component analysis (PCA) is then applied to the ŵi to obtain the matrix Φ of H left singular vectors φ h that are used to obtain the new, expanded query qexp = qorig + Φ T Φqorig. <ref type="bibr" target="#b7">(7)</ref> In the case H = 1, we have a single left singular vector φ: qexp = qorig + (φ T qorig)φ so that the dot product φ T qorig is a type of dynamic weight on the expanded query that is based on the similarity of the original query to the expanded query. The use of variance as a feedback model quality measure occurs indirectly through the application of PCA. It would be interesting to study the connections between this approach and our own modelfitting method. Finally, in language modeling approaches to feedback, Tao and Zhai <ref type="bibr" target="#b19">[18]</ref> describe a method for more robust feedback that allows each document to have a different feedback α. The feedback weights are derived automatically using regularized EM. A roughly equal balance of query and expansion model is implied by their EM stopping condition. They propose tailoring the stopping parameter η based on a function of some quality measure of feedback documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS</head><p>We have presented a new approach to pseudo-relevance feedback based on document and query sampling. The use of sampling is a very flexible and powerful device and is motivated by our general desire to extend current models of retrieval by estimating the risk or variance associated with the parameters or output of retrieval processes. Such variance estimates, for example, may be naturally used in a Bayesian framework for improved model estimation and combination. Applications such as selective expansion may then be implemented in a principled way.</p><p>While our study uses the language modeling approach as a framework for experiments, we make few assumptions about the actual workings of the feedback algorithm. We believe it is likely that any reasonably effective baseline feedback algorithm would benefit from our approach. Our results on standard TREC collections show that our framework improves the robustness of a strong baseline feedback method across a variety of collections, without sacrificing average precision. It also gives small but consistent gains in top-10 precision. In future work, we envision an investigation into how varying the set of sampling methods used and the number of samples controls the trade-off between robustness, accuracy, and efficiency.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Estimating the uncertainty of the feedback model for a single query.</figDesc><graphic coords="2,45.40,79.06,240.83,72.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The trade-off between robustness and average precision for different corpora. The x-axis gives the change in MAP over using baseline expansion with α = 0.5. The yaxis gives the Robustness Index (RI). Each curve through uncircled points shows the RI/MAP tradeoff using the simple small-α strategy (see text) as α decreases from 0.5 to zero in the direction of the arrow. Circled points represent the tradeoffs obtained by resampling feedback for α = 0.5.</figDesc><graphic coords="5,77.57,464.88,163.56,142.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Histogram showing improved robustness of resampling feedback (RS-FB) over baseline feedback (Base-FB) for all datasets combined. Queries are binned by % change in AP compared to the unexpanded query.</figDesc><graphic coords="6,63.07,84.46,204.10,174.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison of baseline (Base-FB) feedback and feedback using re-sampling (RS-FB). Improvement shown for Base-FB and RS-FB is relative to using no expansion.</figDesc><table><row><cell>Collection</cell><cell></cell><cell>NoExp</cell><cell>Base-FB</cell><cell>RS-FB</cell></row><row><cell>TREC 1&amp;2</cell><cell cols="2">AvgP P10 Recall 15084/37393 0.1818 0.4443</cell><cell cols="2">0.2419 (+33.04%) 0.2406 (+32.24%) 0.4913 (+10.57%) 0.5363 (+17.83%) 19172/37393 15396/37393</cell></row><row><cell></cell><cell>AvgP</cell><cell>0.1890</cell><cell cols="2">0.2175 (+15.07%) 0.2169 (+14.75%)</cell></row><row><cell>TREC 7</cell><cell>P10</cell><cell>0.4200</cell><cell>0.4320 (+2.85%)</cell><cell>0.4480 (+6.67%)</cell></row><row><cell></cell><cell>Recall</cell><cell>2179/4674</cell><cell>2608/4674</cell><cell>2487/4674</cell></row><row><cell></cell><cell>AvgP</cell><cell>0.2031</cell><cell cols="2">0.2361 (+16.25%) 0.2268 (+11.70%)</cell></row><row><cell>TREC 8</cell><cell>P10</cell><cell>0.3960</cell><cell>0.4160 (+5.05%)</cell><cell>0.4340 (+9.59%)</cell></row><row><cell></cell><cell>Recall</cell><cell>2144/4728</cell><cell>2642/4728</cell><cell>2485/4728</cell></row><row><cell></cell><cell>AvgP</cell><cell>0.1741</cell><cell>0.1829 (+5.06%)</cell><cell>0.1946 (+11.78%)</cell></row><row><cell>wt10g</cell><cell>P10</cell><cell>0.2760</cell><cell>0.2630 (-4.71%)</cell><cell>0.2960 (+7.24%)</cell></row><row><cell></cell><cell>Recall</cell><cell>3361/5980</cell><cell>3725/5980</cell><cell>3664/5980</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>Collection</cell><cell>N</cell><cell>Base-FB</cell><cell></cell><cell>RS-FB</cell><cell></cell></row><row><cell></cell><cell></cell><cell>n-</cell><cell>RI</cell><cell>n-</cell><cell>RI</cell></row><row><cell cols="2">TREC 1&amp;2 103</cell><cell>26</cell><cell>+0.495</cell><cell>15</cell><cell>+0.709</cell></row><row><cell>TREC 7</cell><cell>46</cell><cell>14</cell><cell>+0.391</cell><cell>10</cell><cell>+0.565</cell></row><row><cell>TREC 8</cell><cell>44</cell><cell>12</cell><cell>+0.455</cell><cell>12</cell><cell>+0.455</cell></row><row><cell>wt10g</cell><cell>91</cell><cell>48</cell><cell>-0.055</cell><cell>39</cell><cell>+0.143</cell></row><row><cell>Combined</cell><cell>284</cell><cell>100</cell><cell>+0.296</cell><cell>76</cell><cell>+0.465</cell></row></table><note><p>Comparison of robustness index (RI) for baseline feedback (Base-FB) vs. resampling feedback (RS-FB). Also</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison of resampling feedback using document sampling (DS) with (QV) and without (No QV) combining feedback models from multiple query variants.</figDesc><table><row><cell>TREC 1&amp;2</cell><cell>AvgP P10 RI</cell><cell>0.2406 0.5263 0.7087</cell><cell>0.2547 (+5.86%) 0.5362 (+1.88%) 0.6515 (-0.0572)</cell></row><row><cell></cell><cell>AvgP</cell><cell>0.2169</cell><cell>0.2200 (+1.43%)</cell></row><row><cell>TREC 7</cell><cell>P10</cell><cell>0.4480</cell><cell>0.4300 (-4.02%)</cell></row><row><cell></cell><cell>RI</cell><cell>0.5652</cell><cell>0.2609 (-0.3043)</cell></row><row><cell></cell><cell>AvgP</cell><cell>0.2268</cell><cell>0.2257 (-0.49%)</cell></row><row><cell>TREC 8</cell><cell>P10</cell><cell>0.4340</cell><cell>0.4200 (-3.23%)</cell></row><row><cell></cell><cell>RI</cell><cell>0.4545</cell><cell>0.4091 (-0.0454)</cell></row><row><cell></cell><cell>AvgP</cell><cell>0.1946</cell><cell>0.1865 (-4.16%)</cell></row><row><cell>wt10g</cell><cell>P10</cell><cell>0.2960</cell><cell>0.2680 (-9.46%)</cell></row><row><cell></cell><cell>RI</cell><cell>0.1429</cell><cell>0.0220 (-0.1209)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison of uniform and relevance-weighted document sampling. The percentage change compared to uniform sampling is shown in parentheses. QV indicates that query variants were used in both runs.</figDesc><table><row><cell></cell><cell></cell><cell>Collection</cell><cell cols="2">QV + Uniform QV + Relevance-score</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">weighting</cell><cell>weighting</cell></row><row><cell></cell><cell></cell><cell>TREC 1&amp;2</cell><cell>AvgP P10 RI</cell><cell>0.2545 0.5369 0.6212</cell><cell>0.2406 (-5.46%) 0.5263 (-1.97%) 0.7087 (+14.09%)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>AvgP</cell><cell>0.2174</cell><cell>0.2169 (-0.23%)</cell></row><row><cell></cell><cell></cell><cell>TREC 7</cell><cell>P10</cell><cell>0.4320</cell><cell>0.4480 (+3.70%)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>RI</cell><cell>0.4783</cell><cell>0.5652 (+18.17%)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>AvgP</cell><cell>0.2267</cell><cell>0.2268 (+0.04%)</cell></row><row><cell></cell><cell></cell><cell>TREC 8</cell><cell>P10</cell><cell>0.4120</cell><cell>0.4340 (+5.34%)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>RI</cell><cell>0.4545</cell><cell>0.4545 (+0.00%)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>AvgP</cell><cell>0.1808</cell><cell>0.1946 (+7.63%)</cell></row><row><cell></cell><cell></cell><cell>wt10g</cell><cell>P10</cell><cell>0.2680</cell><cell>0.2960 (+10.45%)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>RI</cell><cell>0.0220</cell><cell>0.1099 (+399.5%)</cell></row><row><cell cols="4">Baseline FB p(wi|R) Resampling FB p(wi|R)</cell></row><row><cell>said</cell><cell>0.055</cell><cell>court</cell><cell>0.026</cell></row><row><cell>court</cell><cell>0.055</cell><cell>pay</cell><cell>0.018</cell></row><row><cell>pay</cell><cell>0.034</cell><cell>federal</cell><cell>0.012</cell></row><row><cell>but</cell><cell>0.026</cell><cell>education</cell><cell>0.011</cell></row><row><cell>employees</cell><cell>0.024</cell><cell>teachers</cell><cell>0.010</cell></row><row><cell>their</cell><cell>0.024</cell><cell>employees</cell><cell>0.010</cell></row><row><cell>not</cell><cell>0.023</cell><cell>case</cell><cell>0.010</cell></row><row><cell>federal</cell><cell>0.021</cell><cell>their</cell><cell>0.009</cell></row><row><cell>workers</cell><cell>0.020</cell><cell>appeals</cell><cell>0.008</cell></row><row><cell>education</cell><cell>0.020</cell><cell>union</cell><cell>0.007</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Feedback term quality when a stoplist is not used. Feedback terms for TREC topic 60: merit pay vs seniority.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>For example, an expected parameter vector conditioned on the query observation is formed from top-retrieved documents, which are treated as training strings (see<ref type="bibr" target="#b10">[10]</ref>, p. 62).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Because our points are language models in the multinomial simplex, we extended SOM-PAK to support Jensen-Shannon divergence, a widely-used similarity measure between probability distributions.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>This is sometimes also called the reliability of improvement index and was used in Sakai et al.<ref type="bibr" target="#b18">[17]</ref>.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Paul Bennett for valuable discussions related to this work, which was supported by NSF grants #IIS-0534345 and #CNS-0454018, and U.S. Dept. of Education grant #R305G03123. Any opinions, findings, and conclusions or recommendations expressed in this material are the authors. and do not necessarily reflect those of the sponsors.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<ptr target="http://www.lemurproject.org" />
		<title level="m">The Lemur toolkit for language modeling and retrieval</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Query difficulty, robustness, and selective application of query expansion</title>
		<author>
			<persName><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Carpineto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Romano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 25th European Conf. on Information Retrieval (ECIR 2004)</title>
		<meeting>of the 25th European Conf. on Information Retrieval (ECIR 2004)</meeting>
		<imprint>
			<biblScope unit="page" from="127" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A high-performance semi-supervised learning method for text chunking</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Ando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 43rd Annual Meeting of the ACL</title>
		<meeting>of the 43rd Annual Meeting of the ACL</meeting>
		<imprint>
			<date type="published" when="2005-06">June 2005</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bagging predictors</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="123" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improving retrieval feedback with multiple term-ranking function combination</title>
		<author>
			<persName><forename type="first">C</forename><surname>Carpineto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Giannini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Info. Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="259" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Initial results with structured queries and language models on half a terabyte of text</title>
		<author>
			<persName><forename type="first">K</forename><surname>Collins-Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ogilvie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 2005 Text REtrieval Conference</title>
		<meeting>of 2005 Text REtrieval Conference</meeting>
		<imprint>
			<publisher>NIST Special Publication</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">O</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Stork</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Pattern Classification. Wiley and Sons</publisher>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The role of variance in term weighting for probabilistic information retrieval</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Greiff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Ponte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 11th Intl. Conf. on Info. and Knowledge Mgmt. (CIKM 2002)</title>
		<meeting>of the 11th Intl. Conf. on Info. and Knowledge Mgmt. (CIKM 2002)</meeting>
		<imprint>
			<biblScope unit="page" from="252" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">SOMPAK: The self-organizing map program package</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kohonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hynninen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kangas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Laaksonen</surname></persName>
		</author>
		<idno>A31</idno>
		<ptr target="http://www.cis.hut.fi/research/papers/somtr96.ps.Z" />
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
		<respStmt>
			<orgName>Helsinki University of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A Generative Theory of Relevance</title>
		<author>
			<persName><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Using query-specific variance estimates to combine Bayesian classifiers</title>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Greiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 23rd Intl. Conf. on Machine Learning (ICML 2006)</title>
		<meeting>of the 23rd Intl. Conf. on Machine Learning (ICML 2006)</meeting>
		<imprint>
			<biblScope unit="page" from="529" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Combining the language model and inference network approaches to retrieval</title>
		<author>
			<persName><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Info. Processing and Mgmt</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="735" to="750" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Estimating a Dirichlet distribution</title>
		<author>
			<persName><forename type="first">T</forename><surname>Minka</surname></persName>
		</author>
		<ptr target="http://research.microsoft.com/minka/papers/dirichlet" />
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Advances in Information Retrieval, chapter Language models for relevance feedback</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ponte</surname></persName>
		</author>
		<editor>B. Croft</editor>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="73" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A language modeling approach to information retrieval</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Ponte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 1998 ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>of the 1998 ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<biblScope unit="page" from="275" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The SMART Retrieval System, chapter Relevance Feedback in Information Retrieval</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rocchio</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="313" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Prentice-Hall</forename></persName>
		</author>
		<editor>G. Salton</editor>
		<imprint>
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Flexible pseudo-relevance feedback via selective sampling</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sakai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Manabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Asian Language Information Processing (TALIP)</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="111" to="135" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Regularized estimation of mixture models for robust pseudo-relevance feedback</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2006 ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>of the 2006 ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<biblScope unit="page" from="162" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improving the effectiveness of information retrieval with local context analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="112" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to estimate query difficulty</title>
		<author>
			<persName><forename type="first">E</forename><surname>Yomtov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Carmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Darlow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2005 ACM SIGIR Conf. on Research and Development in Information Retrieval</title>
		<meeting>of the 2005 ACM SIGIR Conf. on Research and Development in Information Retrieval</meeting>
		<imprint>
			<biblScope unit="page" from="512" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ranking robustness: a novel framework to predict query performance</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 15th ACM Intl. Conf. on Information and Knowledge Mgmt. (CIKM 2006)</title>
		<meeting>of the 15th ACM Intl. Conf. on Information and Knowledge Mgmt. (CIKM 2006)</meeting>
		<imprint>
			<biblScope unit="page" from="567" to="574" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
