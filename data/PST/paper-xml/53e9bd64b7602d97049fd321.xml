<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Probabilistic Consolidation of Virtual Machines in Self-Organizing Cloud Data Centers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Carlo</forename><surname>Mastroianni</surname></persName>
							<email>mastroianni@icar.cnr.it</email>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Michela</forename><surname>Meo</surname></persName>
							<email>michela.meo@polito.it..</email>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Giuseppe</forename><surname>Papuzzo</surname></persName>
							<email>papuzzo@eco4cloud.com</email>
						</author>
						<author>
							<persName><forename type="middle">C</forename><surname>Mastroianni</surname></persName>
						</author>
						<author>
							<persName><forename type="middle">M</forename><surname>Meo</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">ICAR-CNR</orgName>
								<address>
									<addrLine>Via P. Bucci 41C</addrLine>
									<postCode>87036</postCode>
									<settlement>Rende</settlement>
									<region>CS</region>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">DET -Politecnico di Torino</orgName>
								<address>
									<addrLine>corso Duca degli Abruzzi 24</addrLine>
									<postCode>10129</postCode>
									<settlement>Torino</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Eco4Cloud</orgName>
								<address>
									<addrLine>Piazza Vermicelli</addrLine>
									<postCode>87036</postCode>
									<settlement>Rende</settlement>
									<region>CS</region>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Probabilistic Consolidation of Virtual Machines in Self-Organizing Cloud Data Centers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B90C594EB6BC342DC185D9D2EDC8F43A</idno>
					<idno type="DOI">10.1109/TCC.2013.17</idno>
					<note type="submission">received 1 Apr. 2013; revised 30 Sept. 2013; accepted 28 Nov. 2013;published online 10 Dec. 2013.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Cloud computing</term>
					<term>VM consolidation</term>
					<term>data center</term>
					<term>energy saving</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Power efficiency is one of the main issues that will drive the design of data centers, especially of those devoted to provide Cloud computing services. In virtualized data centers, consolidation of Virtual Machines (VMs) on the minimum number of physical servers has been recognized as a very efficient approach, as this allows unloaded servers to be switched off or used to accommodate more load, which is clearly a cheaper alternative to buy more resources. The consolidation problem must be solved on multiple dimensions, since in modern data centers CPU is not the only critical resource: depending on the characteristics of the workload other resources, for example, RAM and bandwidth, can become the bottleneck. The problem is so complex that centralized and deterministic solutions are practically useless in large data centers with hundreds or thousands of servers. This paper presents ecoCloud, a selforganizing and adaptive approach for the consolidation of VMs on two resources, namely CPU and RAM. Decisions on the assignment and migration of VMs are driven by probabilistic processes and are based exclusively on local information, which makes the approach very simple to implement. Both a fluid-like mathematical model and experiments on a real data center show that the approach rapidly consolidates the workload, and CPU-bound and RAM-bound VMs are balanced, so that both resources are exploited efficiently.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>A LL main trends in information technology, for example, Cloud Computing and Big Data, are based on large and powerful computing infrastructures. The ever increasing demand for computing resources has led companies and resource providers to build large warehouse-sized data centers, which require a significant amount of power to be operated and hence consume a lot of energy. In 2006, the energy consumed by IT infrastructures in the USA was about 61 billion kWh, corresponding to 1.5 percent of all the produced electricity, and 2 percent of the global carbon emissions, which is equal to the aviation industry, and these figures are expected to double every 5 years <ref type="bibr" target="#b0">[1]</ref>.</p><p>In the past few years important results have been achieved in terms of energy consumption reduction, especially by improving the efficiency of cooling and power supplying facilities in data centers. The Power Usage Effectiveness (PUE) index, defined as the ratio of the overall power entering the data center and the power devoted to computing facilities, had typical values between 2 and 3 only a few years ago, while now big Cloud companies have reached values lower than 1.1. However, much space remains for the optimization of the computing facilities themselves. It has been estimated that most of the time servers operate at 10-50 percent of their full capacity <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>. This low utilization is also caused by the intrinsic variability of VMs' workload: the data center is planned to sustain the peaks of load, while for long periods of time (for example, during nights and weekends), the load is much lower <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. Since an active but idle server consumes between 50 and 70 percent of the power consumed when it is fully utilized <ref type="bibr" target="#b5">[6]</ref>, a large amount of energy is used even at low utilization.</p><p>The virtualization paradigm can be exploited to alleviate the problem, as many Virtual Machine (VM) instances can be executed on the same physical server. This enables the consolidation of the workload, which consists in allocating the maximum number of VMs in the minimum number of physical machines <ref type="bibr" target="#b6">[7]</ref>. Consolidation allows unneeded servers to be put into a low-power state or switched off (leading to energy saving and OpEx reduction), or devoted to the execution of incremental workload (leading to CapEx savings, thanks to the reduced need for additional servers). Unfortunately, efficient VM consolidation is hindered by the inherent complexity of the problem. The optimal assignment of VMs to the servers of a data center is analogous to the NP-hard "Bin Packing Problem," the problem of assigning a given set of items of variable size to the minimum number of bins taken from a given set. The problem is complicated by two circumstances: 1) the assignment of VMs should take into account multiple server resources at the same time, for example, CPU and RAM, therefore it becomes a "multidimensional bin packing problem," much more difficult than the single dimension problem; 2) even when a good assignment has been achieved, the VMs continuously modify their hardware requirements, potentially baffling the previous assignment decisions in a few hours.</p><p>In <ref type="bibr" target="#b7">[8]</ref>, we presented ecoCloud, an approach for consolidating VMs on a single computing resource, i.e., the CPU. Here, the approach is extended to the multidimension problem, and is presented for the specific case in which VMs are consolidated with respect to two resources: CPU and RAM. With ecoCloud, VMs are consolidated using two types of probabilistic procedures, for the assignment and the migration of VMs. Both procedures aim at increasing the utilization of servers and consolidating the workload dynamically, with the twofold objective of saving electrical costs and respecting the Service Level Agreements stipulated with users. All this is done by demanding the key decisions to single servers, while the data center manager is only requested to properly combine such local decisions. The approach is partly inspired by the ant algorithms used first by Deneubourg et al. <ref type="bibr" target="#b8">[9]</ref>, and subsequently by a wide research community, to model the behavior of ant colonies and solve many complex distributed problems. The characteristics inherited by such algorithms make ecoCloud novel and different from other solutions. Among such characteristics: 1) the use of the swarm intelligence paradigm, which allows a complex problem to be solved by combining simple operations performed by many autonomous actors (the single servers in our case); 2) the use of probabilistic procedures, inspired by those that model the operations of real ants; and 3) the self-organizing behavior of the system, which ensures that the assignment of VMs to servers dynamically adapts to the varying workload.</p><p>To evaluate the performance of ecoCloud we use two complementary approaches. We first propose a fluid mathematical model that derives the evolution of the system with time by assuming that the involved variables are continuous. The model allows us to test ecoCloud in a wide range of scenarios by simply changing the value of some parameters. The second approach consists of experiments performed on real data centers. The two approaches complement each other: the analytical model introduces some simplifying assumptions but allows for an easy exploration of a wide range of scenarios; conversely, the real experiments do not suffer from assumptions but are, somehow, less representative. Both the approaches show that ecoCloud achieves very good consolidation, and smoothly adapts to possible changes in the system conditions. Finally, to compare the performance of ecoCloud with those of <ref type="bibr" target="#b0">[1]</ref>, that is, a reference approach, and to perform a scalability study, we use an ad hoc simulator.</p><p>The remainder of this paper is organized as follows: after a general description of the scenario and of performance metrics, given in Section 2, Section 3 defines and illustrates the assignment and migration procedures, generalized for the multiresource consolidation problem. Section 4 analyzes the assignment procedure through a mathematical model based on differential equations and shows that ecoCloud not only consolidates the load but also efficiently balances the available resources between compute-intensive and memory-intensive applications. Section 5 reports the results of the ecoCloud adoption in a real data center of a telecommunications company, extending the assessment to the migration procedure. Section 6 compares ecoCloud to one of the best deterministic algorithms devised recently, and Section 7 focuses on the scalability properties of ecoCloud. Section 8 describes related work and Section 9 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">SCENARIO AND PERFORMANCE METRICS</head><p>The objective of ecoCloud is to dynamically map VMs to servers with the twofold objective of saving electrical costs-through the consolidation of VMs that allows some servers to enter low consuming sleep modes-and respecting the Service Level Agreements stipulated with users, especially concerning the expected quality of service. The scenario is pictured in Fig. <ref type="figure" target="#fig_0">1</ref>: an application request is transmitted from a client to the data center manager, which selects a VM that is appropriate for the application, on the basis of application characteristics such as the amount of required resources (CPU, memory, storage space) and the type of operating system specified by the client. Then, the VM is assigned to one of the available servers through the assignment procedure.</p><p>The main idea underlying the whole approach is that it is up to the single servers to decide whether they should accept or reject a VM. These decisions are based on information available locally-for example, information on the local CPU and RAM utilization-and are founded on Bernoulli trials. The data center manager has only a coordinating role, and it does not need to execute any complex centralized algorithm to optimize the mapping of VMs.</p><p>The workload of each application is dynamic, that is, its demand for computational resources varies with time: for example, the CPU demand of a web server depends on the workload generated by web users. Therefore, the assignment of VMs is monitored continuously and is tuned through the migration procedure. Migrating a VM can be advantageous either when the resources utilization is too low, meaning that the server is highly underutilized, or when it is too high, possibly causing overload situations and service level agreement violations. The migration procedure consists of two steps: in the first step, a server requests the migration of a VM, on the basis of its CPU/RAM utilization. The purpose of the second step is to choose the server that will host the migrating VM, with a technique similar to the one used by the assignment procedure.</p><p>The performance of ecoCloud is assessed through the following metrics:</p><p>. Resource utilization. To foster consolidation and save power, a server should be either highly exploited or in a sleep mode. Analysis of CPU and RAM utilization aims at checking if this objective is fulfilled. . Number of active servers. VMs should be clustered into as few servers as possible. For example, if the overall load of the data center is equal to 30 percent of the total available capacity of servers, the number of active servers should be close to 30 percent of the overall number of servers. . Consumed power. The ultimate objective is to save electrical power, so we compute the power consumed by the whole data center in different load conditions. . Frequency of migrations and server switches. Any VM migration causes a slight performance degradation of the application hosted by the VM. The time needed to transfer the VM memory from the source server to the target server may vary from a few seconds up to two minutes in the worst cases <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. In this interval, the VM is active on the source server. During the actual handover of the VM, the VM experiences a downtime in the order of milliseconds. Analogously, the activation of an off server needs a startup time and additional power. Therefore, though migrations and switches are essential for VM consolidation and power reduction, it is important to limit their frequency. It is even more important to avoid massive migrations of VMs: the asynchronous and gradual migration of a number of VMs is much less detrimental than the concurrent migration of the same number of VMs; for example, concurrent migrations might overload the transmission bandwidth and, hence, increase the downtime duration. . SLA violations. A violation of Service Level Agreements can happen when the workload of some VMs increases and the physical servers that host them become overloaded. Such events can be prevented by timely migrating some VMs to other less loaded servers. We measure the percentage of time in which the VMs allocated to a server demand more resources than what the server can provide. This metric, in accordance with recent studies <ref type="bibr" target="#b0">[1]</ref>, is used to assess the QoS level offered to users. Several studies and experiments (e.g., <ref type="bibr" target="#b5">[6]</ref>  <ref type="bibr" target="#b11">[12]</ref>) have found that an active server with very low CPU utilization consumes between 50 and 70 percent of the power that it consumes when fully utilized. Moreover, as the CPU utilization increases, the consumed power can be assumed, with the error below 10 percent, to increase linearly from the power corresponding to the idle state to the power corresponding to full utilization <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>. Though some studies have derived more accurate nonlinear relations <ref type="bibr" target="#b14">[15]</ref>, such refinements have little practical utility to our purposes. Therefore, in analytical and simulation experiments presented in this study, the power consumed by a single server is expressed as</p><formula xml:id="formula_0">P ðuÞ ¼ P idle þ ðP max À P idle Þu;<label>ð1Þ</label></formula><p>where P max is the power consumed at maximum CPU utilization (u ¼ 1) and P idle is the power consumed when the server is active but idle (u ¼ 0). In experiments on real data centers, the consumed power is directly monitored and measured.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ASSIGNMENT AND MIGRATION PROCEDURES</head><p>In this section, we describe the two main probabilistic procedures that are at the basis of ecoCloud: the assignment and migration procedures. The allocation of VMs is driven by the availability of CPU and RAM on the different servers.</p><p>The assignment procedure is performed when a client asks the data center to execute a new application. Once the application is associated to a compatible VM, the data center manager must assign the VM to one of the servers for execution. Instead of taking the decision on its own, which would require the execution of a complex optimization algorithm, the manager delegates a main part of the procedure to single servers. Specifically, it sends an invitation to all the active servers, or to a subset of them, depending on the data center size and architecture, <ref type="foot" target="#foot_0">1</ref> to check if they are available to accept the new VM. Each server takes its decision whether or not to accept the invitation, trying to contribute to the consolidation of the workload on as few servers as possible. The invitation should be rejected if the server is overutilized or underutilized on either of the two considered resources, CPU and RAM. In the case of overutilization, the rationale is to avoid overload situations that can penalize the quality of service perceived by users, while in the case of underutilization the objective is to put the server in a sleep mode and save energy, so the server should refuse new VMs and try to get rid of those that are currently running. Conversely, a server with intermediate utilization should accept new VMs to foster consolidation.</p><p>The server decision is taken performing a Bernoulli trial. The success probability for this trial is equal to the value of the overall assignment function that, in turn, is defined by evaluating the assignment function on each resource of interest. If x (valued between 0 and 1) is the relative utilization of a resource, CPU or RAM, and T is the maximum allowed utilization (e.g., T ¼ 0:8 means that the resource utilization cannot exceed 80 percent of the server capacity), the assignment function is equal to zero when x &gt; T, otherwise it is defined as</p><formula xml:id="formula_1">fðx; p; T Þ ¼ 1 M p x p ðT À xÞ 0 x T ;<label>ð2Þ</label></formula><p>where p is a shape parameter, and the factor M p is used to normalize the maximum value to 1 and is defined as</p><formula xml:id="formula_2">M p ¼ p p ðp þ 1Þ ðpþ1Þ T ðpþ1Þ :<label>ð3Þ</label></formula><p>Fig. <ref type="figure" target="#fig_1">2</ref> shows the graph of the single-resource assignment function <ref type="bibr" target="#b1">(2)</ref> for some values of the parameter p, and T ¼ 0:9. The value of p can be used to modulate the shape of the function. Indeed, the value of x at which the function reaches its maximum-that is, the value at which assignment attempts succeed with the highest probability-is p=ðp þ 1ÞT , which increases and approaches T as the value of p increases. The value of the function is zero or very low when the resource is overutilized or underutilized.</p><p>If u s and m s are, respectively, the current CPU and RAM utilization at server s, the overall assignment function is obtained by the product of two assignment functions as in <ref type="bibr" target="#b1">(2)</ref>, where x ¼ u s and x ¼ m s are used for CPU and RAM, respectively. Let p u and p m be the shape parameters defined for the two resources, and T u and T m the respective maximum utilizations. The overall assignment function for the server s is denoted as f s and defined as</p><formula xml:id="formula_3">f s ðu s ; m s ; p u ; p m ; T u ; T m Þ ¼ fðu s ; p u ; T u Þ Á fðm s ; p m ; T m Þ:<label>ð4Þ</label></formula><p>The shape of the assignment functions, combined with the definition of function ( <ref type="formula" target="#formula_3">4</ref>), ensures that servers tend to respond positively when they have intermediate utilization values for both CPU and RAM: if one of the resources is under-or overutilized the probability of the Bernoulli trial is low.</p><p>If the Bernoulli trial is successful, the server communicates its availability to the data center manager. Then, the manager selects one of the available servers, and assigns the new VM to it. If none of the contacted servers is available-i.e., all the Bernoulli trials are unsuccessful-it is very likely that in all the servers one of the two resources (CPU or RAM) is close to the utilization threshold. 2 This usually happens when the overall workload is increasing, so that the current number of active servers is not sufficient to sustain the load. In such a case, the manager wakes up an inactive server and requests it to run the new VM. The case in which there is no server to wake up, because all the servers are already active, is a sign that altogether the servers are unable to sustain the load even when consolidating the workload: when this situation occurs, the company should consider the acquisition of new servers.</p><p>The assignment process efficiently consolidates the VMs, as shown later in Section 4, but application workload changes with time. When some VMs terminate or reduce their demand for server resources, it may happen that the server becomes underutilized leading to lower energy efficiency. On the other hand, when the VMs increase their requirements, a server may be overloaded, possibly causing SLA violation events and affecting the dependability of the data center. In both these situations, underutilization and overutilization of servers, some VMs can be profitably migrated to other servers, either to switch off a server, or to alleviate its load.</p><p>The migration procedure is defined as follows: each server monitors its CPU and RAM utilization using the libraries provided by the virtualization infrastructure (e.g., VMWare or Hyper-V) and checks if it is between two specified thresholds, the lower threshold T l and the upper threshold T h . When this condition is violated, 3 the server evaluates the corresponding probability function, f l migrate or f h migrate , and performs a Bernoulli trial whose success probability is set to the value of the function. If the trial is successful the server requests the migration of one of the local VMs. Denoting by x the utilization of a given resource, CPU or RAM, the migration probability functions are defined as follows:</p><formula xml:id="formula_4">f l migrate ¼ 1 À x=T l ð Þ<label>ð5Þ</label></formula><formula xml:id="formula_5">f h migrate ¼ 1 þ x À 1 1 À T h :<label>ð6Þ</label></formula><p>The functions, whose graphs are shown in Fig. <ref type="figure">3</ref>, are defined so as to trigger the migration of VMs when the utilization is below the threshold T l or above the threshold T h , respectively. These two kinds of migrations are also referred to as "low migrations" and "high migrations" in the following. The shape of the functions can be modulated by tuning the parameters and , which can therefore be used to foster or hinder migrations. The same function is applied to CPU and RAM, but the parameters, T l , T h , , and can have different values for the two resources. Whenever a Bernoulli trial is performed with success, the server must choose the VM to consider for migration. In the case of high migration, the server focuses on the overutilized resource (CPU or RAM) and considers the VMs for which the utilization of that resource is larger than the difference between the current server utilization and the threshold T h . Then one of such VMs is randomly selected for migration, as this will allow the utilization to go below 2. The case that all or many servers are not available because underutilized on both resources is very unlikely because the process tends to consolidate the workload on highly utilized servers.</p><p>3. The overutilization of any resource is sufficient to trigger the migration procedure, because the overloaded resource becomes a bottleneck for the server. On the other hand, the under-utilization condition is only checked for the most utilized resource, which is the one that drives consolidation. For example, if RAM is the most utilized resource, some servers can have low values of CPU utilization, but this condition does not trigger migrations. Fig. <ref type="figure">3</ref>. Migration probability functions f l migrate and f h migrate (labeled as f l and f h ) for two different values of the parameters and . In this example, the threshold T l is set to 0.3, T h is set to 0.8. the threshold. <ref type="foot" target="#foot_1">4</ref> In the case of low migration the choice of the VM to migrate is made randomly.</p><p>The choice of the new server that will accommodate the migrating VM is made using a variant of the assignment procedure described previously, with two main differences. The first one concerns the migration from an overloaded server: the threshold T of the assignment function is set to 0.9 times the resource utilization of the server that initiated the procedure, and this value is sent to servers along with the invitation. This ensures that the VM will migrate to a less loaded server, and helps to avoid multiple migrations of the same VM. The second difference concerns the migration from a lightly loaded server. When no server is available to run a migrating VM, it would not be acceptable to switch on a new server to accommodate the VM: one server would be activated to let another one be hibernated. Therefore, when no server is available, the VM is not migrated at all.</p><p>It is worth noting that our approach ensures a gradual and continuous migration process, while most other techniques recently proposed for VM migration (some are discussed in the related work section) require the simultaneous migration of many VMs.</p><p>Finally, the threshold values are generally given as an input by the data center administrator, possibly on the basis of a previous analysis on the variance of VMs workload. Shape parameters offer the data center administrator the chance to choose among different consolidation strategies (e.g., conservative, intermediate, aggressive): a more aggressive strategy allows more servers to be hibernated, but at the expense of more migrations. The choice of the desired strategy is made by tuning the values of the shape parameters. Since this analysis is out of the scope of the paper, in what follows the parameter values set in the experiments are those corresponding to the intermediate strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">MATHEMATICAL ANALYSIS</head><p>This section is devoted to a mathematical analysis of the ecoCloud assignment procedure. The mathematical model is based on a set of differential equations inspired by fluid dynamics problems. Let N s be the number of servers in a data center, N c the number of cores in each server and N v the number of VMs that can be executed in each core. The equations model the evolution with time of the CPU and RAM utilization of the servers, respectively denoted by u s ðtÞ and m s ðtÞ for server s, with s ¼ 0; . . . ; N s À 1. The utilization of both resources is a real number that changes by infinitesimal increments/decrements over the interval ½0; 1. A straightforward extension allows to model the evolution of a larger number of resources.</p><p>It is assumed that two types of VMs are executed on the data center: CPU-bound and RAM-bound VMs, respectively indicated as C-type and M-type. C-type VMs need an amount of CPU that is larger than the amount needed by Mtype VMs of a factor C &gt; 1; conversely, the amount of RAM required by M-type VMs is larger than the one needed by C-type VMs by a factor M &gt; 1. Given the fluid model assumption described above, the VM arrival process is a continuous process that makes it arrive, in a time period Át, an amount of VMs that is ðCÞ ðtÞÁt for C-type VMs and ðMÞ ðtÞÁt for M-type VMs. The rate at which services are completed is denoted by .</p><p>To analyze the two classes of VMs separately, we also define the following state variables: u ðCÞ s ðtÞ and u ðMÞ s ðtÞ are the amount of CPU that in a server s is occupied by C-type and M-type VMs, respectively; while m ðCÞ s ðtÞ and m ðMÞ s ðtÞ are the amounts of RAM occupied by the two types of VMs. The total utilization of CPU and RAM in server s is given by the sum of the utilization of the two classes of VMs, u s ðtÞ ¼ u ðCÞ s ðtÞ þ u ðMÞ s ðtÞ; m s ðtÞ ¼ m ðCÞ s ðtÞ þ m ðMÞ s ðtÞ: Since the probability of assigning a VM to a server increases with the value of the assignment function, in the model the fraction of workload assigned to a server s is proportional to the acceptance probability f s ðu s ðtÞ, m s ðtÞ; p u ; p m ; T u ; T m Þ, as defined in (4). In the following, the acceptance probability is simply denoted as f s ðtÞ.</p><p>The set of differential equations (with server index s ¼ 0; . . . ; N s À 1) is the following: @u ðCÞ s ðtÞ @t ¼ ÀN c N v u ðCÞ s ðtÞ þ K C ðCÞ ðtÞ f s ðtÞ; ð7Þ @u ðMÞ s ðtÞ @t ¼ ÀN c N v u ðMÞ s ðtÞ þ K ðMÞ ðtÞ f s ðtÞ; @m ðCÞ s ðtÞ @t ¼ ÀN c N v m ðCÞ s ðtÞ þ K ðCÞ ðtÞ f s ðtÞ; @m ðMÞ s ðtÞ @t ¼ ÀN c N v m ðMÞ s ðtÞ þ K M ðMÞ ðtÞ f s ðtÞ:</p><p>K is a normalization factor K, defined as</p><formula xml:id="formula_6">K ¼ 1 P N s À1 i¼0 f s<label>ðtÞ</label></formula><formula xml:id="formula_7">:</formula><p>The equations can be solved with the initial conditions that define the state of the system at the time that ecoCloud is executed: u ðCÞ s ð0Þ; u ðMÞ s ð0Þ; m ðCÞ s ð0Þ; m ðMÞ s ð0Þ s ¼ 0; . . . ; N s À 1: ð8Þ To analyze the behavior of the system, we performed an experiment for a data center with N s ¼ 100 servers, each having N c ¼ 6 cores with CPU frequency of 2 GHz and 4-GB RAM. The power consumed at maximum utilization P max is set to 250 W, a typical value for the servers of a data center, while P idle is set to 70 percent of P max , i.e., 175 W. In the experiment, the VMs have nominal CPU frequency of 500 MHz. The average time the VM spends in service, 1=, is set to 100 minutes. The average CPU (memory) load of the data center is defined as the ratio between the total amount of CPU (RAM) required by VMs and the corresponding CPU (RAM) capacity of the data center, is denoted as C ( M ), and is computed as ðCÞ = T ( ðMÞ = T ).</p><p>Here, T is the overall service rate of the data center, obtained as T ¼ N s N c N v , where N v is the number of VMs that can be executed on a single 2-GHz core, in this case 4. To analyze the system with a specified overall CPU or memory load, the arrival rates ðCÞ and ðMÞ must be set accordingly. In the first set of experiments, values of ðCÞ and ðMÞ are set to 9.6. With these values the overall load of the data center, is equal to 0.40 for both CPU and RAM:</p><formula xml:id="formula_8">C ¼ M ¼ 0:4.</formula><p>The experiment started from a nonconsolidated scenario: for each server, initial CPU and RAM utilizations are set using a Gamma probabilistic function having average equal to 40 percent of the server capacity. The parameters of the assignment function were set as follows: maximum utilization threshold T ¼ 0:9, and p ¼ 3. Under normal operation, without using ecoCloud, the data center would tend to a steady condition in which all the servers remain active with CPU and RAM utilization around 40 percent. With ecoCloud, the workload consolidates to only 45 servers, while 55 are switched off. This allows the data center to nearly halve the consumed power, from more than 20 kW to about 11 kW.</p><p>It was assumed that VMs are equally shared between compute-intensive (C-type) and memory-intensive applications (M-type). We considered the values of C and M , i.e., the ratios between the CPU and RAM demanded by the two types of VMs. The values of the two parameters were kept equal to one another, and in different tests were set to: 1.0 (the two kinds of applications coincide), 1.5 (C-type applications need 50 percent more CPU than M-type ones, and M-type applications need 50 percent more RAM than C-type ones), 2.0, and 4.0 as the most extreme case. At the end of the consolidation process, i.e., after about two hours of the modeled time, the 45 active servers show nearly the same distribution of their hardware resources between the two types of applications. This distribution is shown in Fig. <ref type="figure">4</ref> for one of the active servers and for the abovementioned values of C and M . The most interesting outcome of this experiment is that the probabilistic assignment process balances the two kinds of VMs so that neither the CPU nor the RAM becomes a bottleneck. For example, in the most imbalanced scenario ( C and M equal to 4.0), about 71 percent of the CPU is assigned to C-type VMs while about 18 percent is given to M-type VMs, and the opposite occurs for memory. Both CPU and RAM are utilized up to the permitted threshold (90 percent) and the workload is consolidated efficiently, which allows 55 servers to be hibernated and the consumed power to be almost halved.</p><p>Of course, such an efficient consolidation is possible when the relative overall loads of CPU and RAM are comparable (both equal to 40 percent in this case). If one of the two resources undergoes a heavier demand, that resource inevitably limits the consolidation degree. For such a case, it is still interesting to assess the behavior of the assignment algorithm. To this purpose, we run experiments in which the overall CPU load, C , is set to 40 percent of the total CPU capacity of the servers, while the overall RAM load, M , is varied between 20 percent and 60 percent. This is accomplished by appropriately varying the value of ðMÞ , the arrival frequency of M-type VMs. For this set of experiments, the values of C and M are set to 4.0. The CPU and RAM utilizations observed for each server after the consolidation phase are shown in Fig. <ref type="figure">5</ref>. Correspondingly, Figs. <ref type="figure">6</ref> and<ref type="figure">7</ref> report the number of active servers and the average value of consumed power.</p><p>When the overall memory load is lower than 0.4 (cases M ¼ 0:2 and M ¼ 0:3), the CPU is the critical resource and is the one that drives the consolidation process. The number of active servers (45), and the consumed power (about 11 kW) are the same as in the case where CPU and RAM overall loads are comparable. On the other hand, when the most critical resource is the memory, as happens in the cases M ¼ 0:5 and M ¼ 0:6, the consolidation process is driven by the allocation of RAM to the VMs. More active servers and more power are needed to satisfy the increased demand for memory: in the cases that the memory load is equal to 50 and 60 percent of the data center capacity, 56 and 67 servers are kept active, respectively, and corresponding values of consumed power are equal to about 13 kW and about 15 kW. Overall, it may be concluded that the approach is always able to consolidate the load as much as is allowed by the most critical hardware resource.</p><p>The benefit of consolidation depends on how much power is wasted due to server underutilization. In that respect, much research effort is devoted in trying to decrease the power consumed by idle servers, that is, the quantity P idle in (1). This value can be expressed as a fraction F idle of the power consumed at maximum utilization, P idle ¼ F idle P max . To analyze this aspect, we performed tests with different values of F idle in a scenario with C ¼ 0:4, M ¼ 0:4, and a 80-20 imbalance between CPUbound and RAM-bound VMs ( C ¼ M ¼ 4:0). Fig. <ref type="figure">8</ref> reports the overall amount of power consumed in the data center before and after applying the ecoCloud algorithm. The advantage of consolidation decreases as servers become more power efficient. Nevertheless, the consumed power is reduced by about 30 percent even with servers that consume only 40 percent of the power when idle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS ON A REAL DATA CENTER</head><p>In the previous section, we have shown through an analytical model, the effectiveness of ecoCloud in consolidating the load under various scenarios. However, the model relies on some necessary assumptions. To validate the model and prove that ecoCloud is effective in real scenarios, we report in this section the results of the experiments performed in May 2013 on a live data center owned by a major telecommunications operator. The experiment was run on 28 servers virtualized with the platform VMWare vSphere 4.0. Among the servers, 2 are equipped with processor Xeon 32 cores and 256-GB RAM, 8 with processor Xeon 24 cores and 100-GB RAM, 11 with processor Xeon 16 cores and 64-GB RAM and 7 with processor Xeon 8 cores and 32-GB RAM. All the servers have network adapters with bandwidth of 10 Gbps. The servers hosted 447 VMs which were assigned a number of virtual cores varying between 1 and 4 and an amount of RAM varying between 1 GB and 16 GB. The VMs were categorized into CPU-bound (C-type) and memory-bound (M-type) depending on their usage of the two resources. We took as a reference the overall CPU and memory capacity of the data center that were equal, respectively, to 1,171 GHz and 2,334 GBytes. A VM was classified as CPU-bound if, at the end of the analyzed period, the average ratio between its CPU and memory utilization was higher than the ratio between the CPU and memory capacity of the data center. In the opposite case, it was classified as memory-bound. In this data center, 80 percent of the VMs, 358, were memory-bound, with an average usage of CPU and RAM of 0.345 GHz and 3.571 GB, respectively. The remaining 88 CPU-bound VMs had average values of CPU and RAM of 1.971 GHz and 1.633 GB, respectively. The M-type VMs contributed for the 49.44 percent of the overall CPU load and for the 92.15 percent of the overall memory load.</p><p>While the analytical study presented in Section 4 focuses on the assignment procedure, during the real experiments both the assignment and the migration were activated. VMs are migrated either when the CPU or memory load exceeds the high threshold T h , set to 0.95, or when the most utilized resource -the RAM in this case-goes below the low threshold T l , set to 0.5. Values of and , in ( <ref type="formula" target="#formula_4">5</ref>) and ( <ref type="formula" target="#formula_5">6</ref>), were set to 0.25. The parameters of the assignment function were set as follows: T ¼ 0:8 (this value was imposed by the data center administrator), p ¼ 3.</p><p>Fig. <ref type="figure" target="#fig_6">9</ref> shows the number of active servers starting from the time at which ecoCloud is activated and for the following 7 days. Within the first day 11 servers, out of 28, are hibernated thanks to the workload consolidation. In the following days, the number of active servers is stabilized, but daily workload variations allow one or two servers to be hibernated during the night. Fig. <ref type="figure" target="#fig_4">10</ref> shows that the consumed power reduces thanks to consolidation, following the trend of the previous figure. Fig. <ref type="figure" target="#fig_0">11</ref> reports the number of high and low migrations performed during each hour of the analyzed period on the whole data center.</p><p>In the first day, migrations are mostly from low utilized servers, which are first unloaded and then hibernated. As the consolidation process proceeds, active servers tend to be well utilized and some high migrations are needed to prevent overload events, while low migrations allow to improve consolidation during the night. The number of migrations is definitely acceptable: after the first day, only a few migrations per day are performed. The overhead induced by migrations was very low and did not cause a significant impact on the performance of running applications. None of the physical resources (CPU, memory, bandwidth) underwent an overload event, and the responsiveness of virtual machines was never deteriorated. More in detail, the typical effects on the source and target hosts, measured during the time needed to migrate the VM memory (from 30 to 90 seconds in our test) were the following: 1) an increase of CPU utilization up to 2 percent, never sufficient to saturate the CPU; 2) an extra bandwidth utilization equal to no more than 500 Mbps, i.e., only a fraction of the network adapter capacity (equal to 10 Gbps in our case), so that the available bandwidth was never saturated. As for the impact on the migrating VM, the downtime experienced in the final phase of the migration was always between 100 and 300 milliseconds, adding only a small delay to the normal response time of the application. These values are fully compatible with those recently published in a VMWare technical report <ref type="bibr" target="#b15">[16]</ref>. It is useful to recall here that ecoCloud does not impact directly on the migration overhead, as migrations are executed by the virtualization platform, VMWare in this case. However, ecoCloud limits the number of migrations and, even more importantly, migrates the VMs gradually and asynchronously, in this way preventing the occurrence of bandwidth saturation and reducing the migration duration.</p><p>Figs. 12 and 13 offer a snapshot of the data center at the end of the seventh day of ecoCloud operation, when only 17 of 28 servers are active. The first figure reports, for each of the 28 servers, the amount of CPU and RAM utilized by C-type and M-type VMs. Since in this scenario most VMs are memory-bound, the consolidation is driven by RAM: in all active servers the RAM utilization is about 70 percent. The consolidation is made possible by the fact that VMs of the two types are distributed among the servers in a proportion that never diverts too much from the overall proportion observed in the whole data center. This is clear from Fig. <ref type="figure" target="#fig_0">13</ref>, which reports the numbers of VMs of the two types that run on each server. With the exceptions of servers 2 and 3, in which no C-type VM is running, the proportion between the two types of VMs is comparable to the 80-20 proportion observed in the data center. The absolute numbers are different because server capacities are not homogeneous, as detailed at the beginning of this section.   The problem of optimally mapping VMs to servers can be reduced to the bin packing problem. The analogy is indeed exploited in recent research <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b16">[17]</ref>. The problem is very complex -it was proved to be NP-hard-and currently adopted optimization algorithms are time consuming when applied to large data centers. A significant drawback of deterministic algorithms is that any efficient mapping may be valid only for a short period of time, due to the arrival/termination of VMs and to the dynamic nature of the workload. As a consequence, many simultaneous migrations of VMs may be needed to adapt the mapping to these variations.</p><p>A set of experiments were performed to compare ecoCloud to one of these algorithms. In particular, we implemented a variant of the classical Best Fit Decreasing algorithm described and analyzed in <ref type="bibr" target="#b0">[1]</ref>, referred to as BFD in the following. This choice was made because it was proved in <ref type="bibr" target="#b17">[18]</ref> that the Best Fit Decreasing algorithm is the polynomial algorithm that gives the best results in terms of effectiveness. Its consolidation ratio is 11/9, which means that at most (11/9)MINþ1 servers are used, where MIN is the minimum theoretical number of servers. At each execution of BFD, VMs of overutilized and underutilized servers are collected, and then they are sorted in decreasing order of CPU utilization. Respecting this order, each VM is allocated to the server that provides the smallest increase of the power consumption caused by the allocation. A key parameter of BFD is the interval of time between two successive executions of the algorithm; therefore, we performed experiments with four different values of the interval: 1, 5, 15, and 60 minutes.</p><p>So far, we could not install ecoCloud in real data centers having more than 100 servers; thus, we used a home-made Java simulator fed with the logs of real VMs to compare ecoCloud and BFD in a data center with 400 servers. We used workload traces retrieved by the data of the CoMon project, a monitoring infrastructure for PlanetLab <ref type="bibr" target="#b18">[19]</ref>. The traces represent the CPU utilization of 6,000 VMs, monitored in March/April 2012 and updated every 5 minutes. Since the CPU is the only resource considered in <ref type="bibr" target="#b0">[1]</ref>, we also consider this resource only for the experiments reported below.</p><p>A graphical characterization of the traces is provided in the following. Fig. <ref type="figure" target="#fig_7">14</ref> reports the distribution of the average CPU utilization of the VMs, measured as a percentage of the total CPU capacity of the hosting physical machine. The graph shows that the average CPU utilization is under 20 percent for most VMs, even though there are a few VMs with very high CPU requirements. It is clear that this kind of distribution leaves much room for clever consolidation algorithms, since in many cases tens of VMs can be executed on the same physical machine. We then collected, for all the VMs and for all the values of the CPU utilization, the difference-or deviation-between the punctual value and the average value of the same VM. The distribution of the deviations obtained in this way is reported in Fig. <ref type="figure" target="#fig_0">15</ref>. Most values are close to zero, meaning that for most VMs CPU deviations are very small. Specifically, about 94 percent of the deviations are lower than 10, which means that if the average CPU utilization of a VM can be estimated-in most cases this is possible using historical data-and each VM is allocated as much CPU as this average value, only for 6 percent of the times the VM will exceed the allocated CPU by more than one tenth of the CPU capacity. Nevertheless, such deviations can still cause QoS violations, especially when multiple VMs increase their CPU demand at the same time.</p><p>The VM traces are picked randomly during the tests, in a number that depends on the desired overall load. We assigned the VMs to 400 servers, using the ecoCloud and BFD algorithms for assignment and migration of VMs. These servers are all equipped with 2-GHz cores. One third of the servers have four cores, one third have six cores and the remaining third have eight cores. The parameters of the  assignment and migration functions were set as follows: T a ¼ 0:90, T l ¼ 0:50, T h ¼ 0:95, ¼ 0:25, and ¼ 0:25.</p><p>Fig. <ref type="figure" target="#fig_11">16</ref> reports the average number of active servers versus the overall load in ecoCloud and BFD. The curves are close to each other, and also close to the optimal value of the associated bin packing problem. ecoCloud requires a slightly larger number of active servers, mostly because of its behavior in descending load phases, during which the CPU utilization of servers is allowed to decrease by a certain amount before low migrations are triggered, to avoid migrations that are not strictly necessary. Similar observations can be done by analyzing the average consumed power of the two algorithms, shown in Fig. <ref type="figure" target="#fig_10">17</ref>.</p><p>The slightly better consolidation degree of BFD, however, comes at a considerable cost in terms of the number of migrations and the probability of overload events. Fig. <ref type="figure" target="#fig_0">18</ref> shows that the number of migrations is much higher in BFD than in ecoCloud. For example, with load equal to 0.3, less than 400 migrations per hour are needed by ecoCloud, while about 10,000 migrations per hour are needed by BFD in the case that the time interval between two successive executions is set to 1 minute, as in <ref type="bibr" target="#b0">[1]</ref>. This corresponds to more than 150 simultaneous migrations to be performed at each algorithm execution. If the BFD time interval is enlarged the frequency of migrations can be reduced, but the number of required simultaneous migrations increases: for example, about 750 simultaneous migrations are needed when the time interval is set to 60 minutes. Conversely, migrations are executed asynchronously with ecoCloud. Fig. <ref type="figure" target="#fig_6">19</ref> reports the percentage of time of CPU overload. The value of this index is remarkably lower in ecoCloud, due to its capacity of immediately reacting with high migrations each time the CPU utilization exceeds the upper threshold. The probability of overload in BFD comes as the combination of two contrasting phenomena: if the algorithm is executed frequently, the consolidation effort is stressed (cfr. Fig. <ref type="figure" target="#fig_11">16</ref>), which brings the servers closer to their CPU limits and increases the overload probability. This is particularly evident when the overall load is high. When the time interval is larger the consolidation effort is lower, but VM workload variations are not controlled for a longer time, which can also be a cause of overload events. Thus, overload events are present at any load condition. With ecoCloud the index is hardly affected by the value of the overall load.</p><p>A comparison in terms of complexity in also interesting. The complexity of BFD <ref type="bibr" target="#b19">[20]</ref> is n Á m, where n is the number of hosts and m is the number of VMs that need to be assigned or migrated. The complexity of ecoCloud is equal to the number of servers invited during the assignment/migration of a VM. This number is at most n, but in general it is much lower, because in large data centers it is sufficient to invite only a subset of servers, as discussed in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">RESULTS WITH DIFFERENT DATA CENTER SIZES</head><p>One of the most interesting and peculiar features of ecoCloud is its scalability, inherited from the probabilistic, self-organizing and partially distributed nature of the   algorithm. Centralized and deterministic algorithms may be appropriate in data centers with a limited number of servers, but may become inefficient in large and very large data centers, due to the complexity of the problem and the need for the simultaneous migrations of a large number of VMs, as discussed in the previous section. Conversely, ecoCloud is particularly suited for large data centers. To understand why consolidation improves with the number of servers, it is useful to remember that a hibernated server is switched on when a new or migrating VM is rejected by all the active servers. In small systems, it can happen that all the servers-after the execution of Bernoulli trials-reject the VM even when some of them have enough spare CPU to accommodate the VM. The probability of this event becomes negligible in large data centers, where the invitation to accommodate a VM is forwarded to many servers: as a consequence, a server is activated only when strictly needed. This argument also motivates the fact that in large data centers it is not necessary to send invitations to all the servers, but it is sufficient to invite a subset of them. This has two beneficial consequences: 1) the traffic overhead can be limited; and 2) ecoCloud fits well with distributed and multisite data centers, since each invitation can be forwarded to the servers of a specific site, chosen randomly or on the basis of environmental parameters (the cost of energy in different sites, the external temperature, etc.).</p><p>To assess the ecoCloud scalability, we performed simulations with data centers of different size (100, 200, 400, and 3,000 servers), using the VM traces described in the previous section, and keeping the same proportion between the number of VMs and the number of physical servers. Fig. <ref type="figure" target="#fig_12">20</ref> reports the fraction of active servers versus the overall load and shows that this fraction is nearly independent on the system size. We also performed tests for a data center with 3,000 servers in which invitations are forwarded to varying numbers of servers. These tests confirm that there is no advantage to send invitations to more than about 100 servers. The good scalability is confirmed by the other performance metrics. For example, the frequency of migrations experienced by a single server is nearly independent from the system size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">RELATED WORK</head><p>As the Cloud computing paradigm rapidly emerges, a notable amount of studies focus on algorithms and procedures that aim at improving the "green" characteristics of Cloud data centers. An interesting survey is given in <ref type="bibr" target="#b20">[21]</ref>, along with a useful taxonomy of examined methods. Another recent survey <ref type="bibr" target="#b21">[22]</ref> focuses on the categorization of green computing performance metrics in data centers, such as power metrics, thermal metrics and extended performance metrics, i.e., multiple data center indicators. We are experiencing a turning point in this area. So far, most efforts have been devoted to the optimization of the physical infrastructure, commonly evaluated through the PUE index, and results have been notable, as this index is now as low as 1.08 in some modern data centers. Today, focus is switching to the efficiency of the IT infrastructure itself, and is testified by the definition of appropriate indices. Two examples are: 1) eBay has recently defined DSE, the Digital Service Efficiency index <ref type="bibr" target="#b22">[23]</ref>, which computes the useful work (in terms of transactions) performed per kWh; 2) Intel has proposed two new metrics <ref type="bibr" target="#b23">[24]</ref>: IT-power usage effectiveness (ITUE), similar to PUE but "inside" the IT and total-power usage effectiveness (TUE), which combines the two for a total efficiency picture.</p><p>Consolidation is a powerful means to improve IT efficiency and in this way reduce power consumption <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>. Some approaches-for example, <ref type="bibr" target="#b26">[27]</ref> and <ref type="bibr" target="#b12">[13]</ref>-try to forecast the processing load and aim at determining the minimum number of servers that should be switched on to satisfy the demand, so as to reduce energy consumption and maximize data center revenues. However, even a correct setting of this number is only a part of the problem: algorithms are needed to decide how the VMs should be mapped to servers in a dynamic environment, and how live migration of VMs can be exploited to unload servers and switch them off when possible, or to avoid SLA violations.</p><p>The problem of optimally mapping VMs to servers can be reduced to the bin packing problem <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b27">[28]</ref>. Unfortunately, this problem is known to be NP-hard, therefore heuristic approaches can only lead to suboptimal solutions. Live migration of VMs between servers is adopted by the VMWare Distributed Power Management system, using lower and upper utilization thresholds to enact migration procedures <ref type="bibr" target="#b28">[29]</ref>. The heuristic approaches presented in <ref type="bibr" target="#b0">[1]</ref> and in <ref type="bibr" target="#b27">[28]</ref> use techniques derived, respectively, from the Best Fit Decreasing and the First Fit Decreasing algorithms. In both cases, the goal is to place each migrating VM on the server that minimizes the overall power consumption of the data center. The framework presented in <ref type="bibr" target="#b29">[30]</ref> tackles the consolidation problem by exploiting the Constraint Programming paradigm. Rulebased constraints, for example concerning SLA negotiation, are managed by an optimizer that adopts a branching approach: the variables are considered in a priority descending order, and at each step one of the variables is set to the value that is supposed to guide the solver to a good solution. All these approaches represent important steps ahead for the deployment of green-aware data centers, but still they share a couple of notable drawbacks.</p><p>First, they use deterministic and centralized algorithms whose efficiency deteriorates as the size of the data center grows. The second drawback is that mapping strategies may require the concurrent migration of many VMs, which can cause considerable performance degradation during the reassignment process. Conversely, the approach presented here adopts a probabilistic approach, naturally scalable, and uses an asynchronous and smooth migration process, which ensures that VMs are relocated gradually.</p><p>An interesting study is presented in <ref type="bibr" target="#b30">[31]</ref>. The paper confirms that the problem of energy saving in server farms is almost intractable and proposes the Delayed Off strategy (the name derives from the fact that a server is turned off only after a predetermined amount of time in which it has been idle), which is proved to be asymptotically optimal but only under some assumptions, for example stationary Poisson arrival process and homogeneous servers.</p><p>Bioinspired algorithms and protocols are emerging as a useful means to manage distributed systems, and Clouds are not an exception. Assignment and migration procedures presented here are partly inspired by the pick and drop operations performed by some species of ants that cluster items in their environment <ref type="bibr" target="#b8">[9]</ref>. The pick and drop paradigm, though very simple and easy to implement, has already proved surprisingly powerful: for example, it is used to cluster and order resources in P2P networks, to facilitate their discovery <ref type="bibr" target="#b31">[32]</ref>. Another ant-inspired mechanism is proposed in <ref type="bibr" target="#b32">[33]</ref>: in this study, the data center is modeled as a P2P network, and ant-like agents explore the network to collect information that can later be used to migrate VMs and reduce power consumption. The V-MAN system, proposed in <ref type="bibr" target="#b33">[34]</ref>, is also based on the P2P paradigm. Here, a gossip protocol is used by servers to communicate their state to each other, and migrate VMs from servers with low load to servers with higher load, with the aim of switching off the former and save energy. The approach is promising but needs more assessment, as it makes the unrealistic assumption that all VMs are identical. In our opinion, the main problem of pure P2P approaches is that the complete absence of centralized control can be seen as an obstacle by the data center administrator. With ecoCloud, despite the fact that servers can autonomously decide whether or not to migrate or accept a VM, final decisions are still granted to the central manager of the data center, which ensures a better control of the operations.</p><p>Since the mapping of VMs to servers is essentially an optimization problem, evolutionary and genetic algorithms can also represent a valid solution. In <ref type="bibr" target="#b34">[35]</ref>, a genetic algorithm is used to optimize the assignment of VMs, and minimize the number of active servers. The main limitations of this kind of approach are the need of a strong centralized control and the difficulties in the setting of key parameters, such as the population size and the crossover and mutation rates.</p><p>In most studies, CPU is the main component on which energy-efficiency strategies focus to obtain a consistent reduction of consumed power. The reason is that, among hardware components, only CPU supports active lowpower modes, whereas other components can only be completely or partially switched off. Server CPUs can consume less than 30 percent of their peak power in lowactivity modes, leading to dynamic power range of more than 70 percent of peak power <ref type="bibr" target="#b1">[2]</ref>. Dynamic power ranges of other components are much narrower, or even negligible. Nevertheless, important fractions of power are consumed by memory, disk, and power supplies <ref type="bibr" target="#b35">[36]</ref>. Applications hosted by VMs often present complementary resource usage, so it may be profitably to let a server execute, for example, a mix of memory-bound and CPU-bound applications. In <ref type="bibr" target="#b36">[37]</ref>, the mapping of VMs to servers was modeled as a multidimensional bin packing problem, in which servers are represented by bins, and each resource (CPU, disk, memory, and network) was considered as a dimension of the bin. While formally interesting, this problem is even more difficult than the classical bin packing problem, therefore it is hardly applicable in large data centers. The algorithm presented in <ref type="bibr" target="#b37">[38]</ref> is based on the first-fit approximation for the bin packing problem. The algorithm was devised for the single resource problem, but tips are given about the extension to multiple resources. In <ref type="bibr" target="#b38">[39]</ref> the multiresource problem is tackled by using an LP formulation that gives higher priority to virtual machines with more stable workload. ReCon <ref type="bibr" target="#b39">[40]</ref> is a tool that analyzes the resource consumption data of various applications, discovers applications which can be consolidated, and subsequently generates static or dynamic consolidation recommendations. Only CPU utilization is considered, the complete extension to the multiresources problem is left to future research. The Entropy resource manager presented in <ref type="bibr" target="#b40">[41]</ref> performs dynamic consolidation based on constraint programming, where constraints are defined both on CPU and on RAM utilization. When compared to these interesting approaches, our algorithm differentiates for its ability to adaptively consolidate the workload without using any complex centralized algorithm and balance the assignment of CPU-and RAM-intensive applications on each server, which helps to optimize the use of resources.</p><p>Owing to the increased size of data centers, several big companies are adopting a multi-data center infrastructure. This allows companies to balance the load, improve the quality of service and, when sites are distributed over multiple regions or States, save energy by exploiting the different energy costs at different locations and time zones. Distributed solutions for data centers are analyzed in <ref type="bibr" target="#b41">[42]</ref> and <ref type="bibr" target="#b42">[43]</ref>. The solution presented here can be easily tailored to these environments, by splitting the assignment and migration processes into two phases. In the first phase, the system decides on which specific data center an application should be assigned or migrated, on the basis of management, load balancing and energy-efficiency criteria, not differently from other distributed environments. In the second phase, the probabilistic approach is used to decide on which specific server of the selected data center the application should be executed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">CONCLUSION</head><p>This paper tackles the issue of energy-related costs in data centers and Cloud infrastructures, which are the largest contributor to the overall cost of operating such environments. The aim is to consolidate the Virtual Machines on as few physical servers as possible and switch the other servers off, so as to minimize power consumption and carbon emissions while ensuring a good level of the QoS experienced by users. With ecoCloud, the approach proposed in the paper, the mapping of Virtual Machines is based on Bernoulli trials through which single servers decide, on the basis of the local information, whether or not they are available to execute an application. The selforganizing and probabilistic nature of the approach makes ecoCloud particularly efficient in large data centers. This is a notable advantage with respect to other fully deterministic algorithms, which inevitably encounter significant difficulties when the size of the data center grows, since the problem of the optimal assignment of Virtual Machines to servers is known to be very complex.</p><p>Mathematical analysis and experiments performed in a real data center in operation show that the adopted techniques succeed in the objectives of reducing power consumption, avoiding overload events that could cause SLA violations, limiting the number of VM migrations and server switches, and balancing CPU-bound and memorybound applications. Simulation experiments prove that these achievements can be obtained for any system load and system size and that ecoCloud performance is competitive with other approaches based on more traditional algorithms.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Assignment and migration of VMs in a data center.</figDesc><graphic coords="2,293.39,51.17,242.70,88.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Assignment probability function fðx; p; T Þ for three different values of the parameter p, and T equal to 0.9.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .Fig. 5 .Fig. 6 .</head><label>456</label><figDesc>Fig. 4. CPU and RAM utilization of active servers, with C ¼ 0:4, M ¼ 0:4, and different values of C and M . Fig. 5. CPU and RAM utilization of active servers, with different values of M , and C ¼ 0:4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 7 .Fig. 8 .</head><label>78</label><figDesc>Fig. 7. Consumed power with different values of M and C ¼ 0:4. Fig. 8. Consumed power with different values of F idle , before and after applying ecoCloud.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 10 .</head><label>10</label><figDesc>Fig.10. Consumed power after activation of ecoCloud.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 11 .Fig. 12 .</head><label>1112</label><figDesc>Fig. 11. Number of VM migrations after activation of ecoCloud.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Number of active servers after activation of ecoCloud.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. Distribution of the average CPU utilization of the VMs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 13 .Fig. 15 .</head><label>1315</label><figDesc>Fig. 13. Number of C-type and M-type VMs running on the 28 servers. Values are taken at the end of the seventh day of operation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 18 .Fig. 19 .</head><label>1819</label><figDesc>Fig. 18. Number of migrations per hour in the data center versus load: comparison between ecoCloud and BFD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 17 .</head><label>17</label><figDesc>Fig.<ref type="bibr" target="#b16">17</ref>. Power consumed by the data center versus load: comparison between ecoCloud and BFD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 16 .</head><label>16</label><figDesc>Fig.<ref type="bibr" target="#b15">16</ref>. Number of active servers versus load: comparison between ecoCloud and BFD. For BFD, the legend reports the time interval between two successive executions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 20 .</head><label>20</label><figDesc>Fig. 20. Scalability test. Fraction of active servers in data centers with different size.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Data centers are equipped with high-bandwidth networks that naturally support broadcast messaging. In very large data centers, the servers may be distributed among several groups of servers: in this case, the invitation message may be broadcast to one of such groups only.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>If no VM matches the condition, the largest VM will be chosen and a new Bernoulli trial will be executed to trigger another migration.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The research leading to these results has received funding from the European Union Seventh Framework Programme (FP7/2007-2013) under grant agreement no. 257740 (Network of Excellence TREND).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Energy-Aware Resource Allocation Heuristics for Efficient Management of Data Centers for Cloud Computing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Beloglazov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Abawajy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Buyya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Generation Computer Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="755" to="768" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The Case for Energy-Proportional Computing</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Barroso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Ho ¨lzle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="33" to="37" />
			<date type="published" when="2007-12">Dec. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Workload Management for Power Efficiency in Virtualized Data Centers</title>
		<author>
			<persName><forename type="first">G</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kothari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comm. ACM</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="131" to="141" />
			<date type="published" when="2011-07">July 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Solar-Powered Cloud Computing Datacenters</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hosman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Baikie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IT Professional</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="15" to="21" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Developers, Developers, Developers: Engaging the Missing Link in It Resource Efficiency</title>
		<author>
			<persName><forename type="first">M</forename><surname>Aggar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013-03">Mar. 2013</date>
			<publisher>The Green Grid</publisher>
		</imprint>
	</monogr>
	<note type="report_type">technical report</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The Cost of a Cloud: Research Problems in Data Center Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Maltz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. ACM SIGCOMM Computer Comm. Rev</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="68" to="73" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Shares and Utilities Based Power Consolidation in Virtualized Server Environments</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cardosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Korupolu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th IFIP/IEEE Integrated Network Management (IM &apos;09)</title>
		<meeting>11th IFIP/IEEE Integrated Network Management (IM &apos;09)</meeting>
		<imprint>
			<date type="published" when="2009-06">June 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Self-Economy in Cloud Data Centers: Statistical Assignment and Migration of Virtual Machines</title>
		<author>
			<persName><forename type="first">C</forename><surname>Mastroianni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Meo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papuzzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 17th Int&apos;l European Conf. Parallel Processing</title>
		<meeting>17th Int&apos;l European Conf. Parallel essing</meeting>
		<imprint>
			<date type="published" when="2011-09">Sept. 2011</date>
			<biblScope unit="page" from="407" to="418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The Dynamics of Collective Sorting: Robot-Like Ants and Ant-Like Robots</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Deneubourg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Franks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sendova-Franks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Detrain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chre ´tien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. First Int&apos;l Conf. Simulation of Adaptive Behavior on from Animals to Animats</title>
		<meeting>First Int&apos;l Conf. Simulation of Adaptive Behavior on from Animals to Animats</meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="356" to="363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A Live Storage Migration Mechanism over Wan for Relocatable Virtual Machine Services on Clouds</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hirofuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nakada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Itoh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sekiguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Ninth IEEE/ACM Int&apos;l Symp. Cluster Computing and the Grid (CCGrid &apos;09)</title>
		<meeting>Ninth IEEE/ACM Int&apos;l Symp. Cluster Computing and the Grid (CCGrid &apos;09)</meeting>
		<imprint>
			<date type="published" when="2009-05">May 2009</date>
			<biblScope unit="page" from="460" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Performance and Energy Modeling for Live Migration of Virtual Machines</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 20th Int&apos;l Symp. High Performance Distributed Computing (HPDC &apos;11)</title>
		<meeting>20th Int&apos;l Symp. High Performance Distributed Computing (HPDC &apos;11)</meeting>
		<imprint>
			<date type="published" when="2011-06">June 2011</date>
			<biblScope unit="page" from="171" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Energy and Carbon-Efficient Placement of Virtual Machines in Distributed Cloud Data Centers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Khosravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Buyya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 19th Int&apos;l Conf. Parallel Processing (Euro-Par &apos;13)</title>
		<meeting>19th Int&apos;l Conf. Parallel essing (Euro-Par &apos;13)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Maximizing Cloud Providers&quot; Revenues via Energy Aware Allocation Policies</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mazzucco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dyachuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Deters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 10th IEEE/ACM Int&apos;l Symp. Cluster Computing and the Grid (CCGrid &apos;10)</title>
		<meeting>10th IEEE/ACM Int&apos;l Symp. Cluster Computing and the Grid (CCGrid &apos;10)</meeting>
		<imprint>
			<date type="published" when="2010-05">May 2010</date>
			<biblScope unit="page" from="131" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A Comparison of High-Level Full-System Power Models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rivoire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Power Aware Computing and Systems (HotPower &apos;08)</title>
		<meeting>Conf. Power Aware Computing and Systems (HotPower &apos;08)</meeting>
		<imprint>
			<date type="published" when="2008-12">Dec. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Power Provisioning for a Warehouse-Sized Computer</title>
		<author>
			<persName><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-D</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Barroso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 34th Ann. Int&apos;l Symp. Computer Architecture (ISCA &apos;07)</title>
		<meeting>34th Ann. Int&apos;l Symp. Computer Architecture (ISCA &apos;07)</meeting>
		<imprint>
			<date type="published" when="2007-06">June 2007</date>
			<biblScope unit="page" from="13" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">VMware vSphere 5.1 vMotion Architecture, Performance and Best Practices</title>
		<author>
			<persName><surname>Vmware</surname></persName>
		</author>
		<ptr target="http://www.vmware.com/resources/techresources/10305" />
	</analytic>
	<monogr>
		<title level="m">technical report, VMWare tech. papers</title>
		<imprint>
			<date type="published" when="2012-08">Aug. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">pMapper: Power and Migration Cost Aware Application Placement in Virtualized Systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM/IFIP/USENIX Ninth Int&apos;l Middleware Conf. (Middleware &apos;08)</title>
		<meeting>ACM/IFIP/USENIX Ninth Int&apos;l Middleware Conf. (Middleware &apos;08)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="243" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A Simple Proof of the Inequality FFD (L) 11/9 OPT (L) + 1, for All L for the FFD Bin-Packing Algorithm</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Mathematicae Applicatae Sinica</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="321" to="331" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">CoMon: A Mostly-Scalable Monitoring System for Planetlab</title>
		<author>
			<persName><forename type="first">K</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Pai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGOPS Operating Systems Rev</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="65" to="74" />
			<date type="published" when="2006-01">Jan. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Energy Efficient Allocation of Virtual Machines in Cloud Data Centers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Beloglazov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Buyya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 10th IEEE/ACM Int&apos;l Symp. Cluster Computing and the Grid (CCGrid &apos;10)</title>
		<meeting>10th IEEE/ACM Int&apos;l Symp. Cluster Computing and the Grid (CCGrid &apos;10)</meeting>
		<imprint>
			<date type="published" when="2010-05">May 2010</date>
			<biblScope unit="page" from="577" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A Taxonomy and Survey of Energy-Efficient Data Centers and Cloud Computing Systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Beloglazov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Buyya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Zomaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Computers</title>
		<meeting>Advances in Computers</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="47" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Review of Performance Metrics for Green Data Centers: A Taxonomy Study</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">U</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The J. Supercomputing</title>
		<imprint>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2011-10">Oct. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">White Paper on Digital Service Efficiency</title>
		<author>
			<persName><forename type="first">N</forename><surname>Greene</surname></persName>
		</author>
		<ptr target="http://dse.ebay.com/sites/default/files/eBay-DSE-130305.pdf" />
	</analytic>
	<monogr>
		<title level="j">technical report, eBay Inc</title>
		<imprint>
			<date type="published" when="2013-03">Mar. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">TUE, a New Energy-Efficiency Metric Applied at ORNL&apos;s Jaguar</title>
		<author>
			<persName><forename type="first">M</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Tschudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Coles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Int&apos;l Supercomputing Conf</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Energy-Efficient Virtual Machine Consolidation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Graubner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Freisleben</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IT Professional</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="28" to="34" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Behavioral Model for Cloud Aware Load and Power Management</title>
		<author>
			<persName><forename type="first">K</forename><surname>Schro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Nebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Workshop Hot Topics in Cloud Services (HotTopiCS &apos;13)</title>
		<meeting>Int&apos;l Workshop Hot Topics in Cloud Services (HotTopiCS &apos;13)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Managing Server Energy and Operational Costs in Hosting Centers</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sivasubramaniam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gautam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGMETRICS Performance Evaluation Rev</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="303" to="314" />
			<date type="published" when="2005-06">June 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Energy Efficient Resource Allocation Strategy for Cloud Data Centres</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Basmadjian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>De Meer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mahmoodi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sannelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mezza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Telesca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dupont</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 26th Int&apos;l Symp. Computer and Information Sciences (ISCIS &apos;11)</title>
		<meeting>26th Int&apos;l Symp. Computer and Information Sciences (ISCIS &apos;11)</meeting>
		<imprint>
			<date type="published" when="2011-09">Sept. 2011</date>
			<biblScope unit="page" from="133" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Vmware Distributed Resource Management: Design, Implementation, and Lessons Learned</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Holler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shanmuganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Waldspurger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<ptr target="https://labs.vmware.com/academic/publications/gulati-vmtj-spring2012" />
	</analytic>
	<monogr>
		<title level="j">VMware Technical J</title>
		<imprint>
			<date type="published" when="2012">Spring 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A Constraint Programming Approach for the Service Consolidation Problem</title>
		<author>
			<persName><forename type="first">K</forename><surname>Dhyani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gualandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cremonesi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. Integration of AI and OR Techniques in Constraint Programming (CPAIOR &apos;10)</title>
		<meeting>Int&apos;l Conf. Integration of AI and OR Techniques in Constraint Programming (CPAIOR &apos;10)</meeting>
		<imprint>
			<date type="published" when="2010-06">June 2010</date>
			<biblScope unit="page" from="97" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Optimality Analysis of Energy-Performance Trade-Off for Server Farm Management</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Harchol-Balter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Kozuch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Performance Evaluation</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1155" to="1171" />
			<date type="published" when="2010-11">Nov. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">So-Grid: A Self-Organizing Grid Featuring Bio-Inspired Algorithms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Forestiero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mastroianni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Spezzano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Autonomous and Adaptive Systems</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2008-05">May 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A Bio-Inspired Algorithm for Energy Optimization in a Self-Organizing Data Center</title>
		<author>
			<persName><forename type="first">D</forename><surname>Barbagallo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Di Nitto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mirandola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. First Int&apos;l Conf. Self-Organizing Architectures (SOAR &apos;09)</title>
		<meeting>First Int&apos;l Conf. Self-Organizing Architectures (SOAR &apos;09)</meeting>
		<imprint>
			<date type="published" when="2009-09">Sept. 2009</date>
			<biblScope unit="page" from="127" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Server Consolidation in Clouds through Gossiping</title>
		<author>
			<persName><forename type="first">M</forename><surname>Marzolla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Babaoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Panzieri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 12th Int&apos;l Symp. a World of Wireless, Mobile and Multimedia Networks</title>
		<meeting>IEEE 12th Int&apos;l Symp. a World of Wireless, Mobile and Multimedia Networks</meeting>
		<imprint>
			<date type="published" when="2011-06">June 2011</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Online Self-Reconfiguration with Performance Guarantee for Energy-Efficient Large-Scale Cloud Computing Data Centers</title>
		<author>
			<persName><forename type="first">H</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. Services Computing (SCC &apos;10)</title>
		<meeting>IEEE Int&apos;l Conf. Services Computing (SCC &apos;10)</meeting>
		<imprint>
			<date type="published" when="2010-07">July 2010</date>
			<biblScope unit="page" from="514" to="521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Energy Efficiency for Information Technology: How to Reduce Power Consumption in Servers and Data Centers</title>
		<author>
			<persName><forename type="first">L</forename><surname>Minas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ellison</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Intel Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Energy Aware Consolidation for Cloud Computing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Srikantaiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. USENIX Workshop Power Aware Computing and Systems</title>
		<meeting>USENIX Workshop Power Aware Computing and Systems</meeting>
		<imprint>
			<date type="published" when="2008-12">Dec. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Dynamic Placement of Virtual Machines for Managing SLA Violations</title>
		<author>
			<persName><forename type="first">N</forename><surname>Bobroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kochut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Beaty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 10th IFIP/ IEEE Int&apos;l Symp. Integrated Network Management (IM &apos;07)</title>
		<meeting>10th IFIP/ IEEE Int&apos;l Symp. Integrated Network Management (IM &apos;07)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Server Consolidation with Migration Control for Virtualized Data Centers</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ferreto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Netto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Calheiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">De</forename><surname>Rose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Generation Computer Systems</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1027" to="1034" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">ReCon: A Tool to Recommend Dynamic Server Consolidation in Multi-Cluster Data Centers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Network Operations and Management Symp. (NOMS)</title>
		<meeting>IEEE Network Operations and Management Symp. (NOMS)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Entropy: A Consolidation Manager for Clusters</title>
		<author>
			<persName><forename type="first">F</forename><surname>Hermenier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lorca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Menaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lawall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGPLAN/SIGOPS Int&apos;l Conf. Virtual Execution Environments</title>
		<meeting>ACM SIGPLAN/SIGOPS Int&apos;l Conf. Virtual Execution Environments</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Managing the Cost, Energy Consumption, and Carbon Footprint of Internet Services</title>
		<author>
			<persName><forename type="first">K</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bilgir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bianchini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martonosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGMETRICS Performance Evaluation Rev</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="357" to="358" />
			<date type="published" when="2010-06">June 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">He is a member of the IEEE. Michela Meo received the Laurea degree in electronics engineering in 1993, and the PhD degree in electronic and telecommunications engineering in 1997, both from the Politecnico di Torino, Italy. Since November 1999, she is an assistant professor at Politecnico di Torino. She coauthored more than 150 papers, about 50 of which are in international journals. She edited six special issues of international journals, including ACM Monet, Performance Evaluation, and Journal and Computer Networks. She was program cochair of two editions of ACM MSWiM, general chair of another edition of ACM MSWiM, program cochair of the IEEE QoS-IP, IEEE MoVeNet</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Yeo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anandasivam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Buyya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ISCC 2009, and she was in the program committee of about 50 international conferences, including SIGMETRICS, INFOCOM, ICC, and GLOBECOM. Her research interests include the field of performance evaluation and modeling, traffic classification and characterization</title>
		<meeting><address><addrLine>Cosenza, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">June 2011. 1995 and 1999. 2007</date>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="732" to="749" />
		</imprint>
	</monogr>
	<note>IEEE Transactions on Evolutionary Computation and ACM Transactions on Autonomous and Adaptive Systems, and conference proceedings. She is a member of the IEEE. Giuseppe Papuzzo received the Laurea degree in computer engineering from the University of Calabria. in 2004. Since 2004, he collaborates with the Institute of High Performance Computing and Networks of the Italian National Research Council (ICAR-CNR</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">He coauthored scientific papers published in international conferences and journals like Future Generation Computing Systems and Transactions on Computational Systems Biology</title>
	</analytic>
	<monogr>
		<title level="m">His research interests include workflow management, P2P networks, Grid and Cloud Computing, and data streaming</title>
		<meeting><address><addrLine>Cosenza, Italy</addrLine></address></meeting>
		<imprint/>
	</monogr>
	<note>For more information on this or any other computing topic. please visit our Digital Library at www.computer.org/publications/dlib</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
