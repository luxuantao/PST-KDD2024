<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Expressing an Image Stream with a Sequence of Natural Sentences</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Cesc</forename><surname>Chunseong</surname></persName>
							<email>park.chunseong@snu.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Park</forename><forename type="middle">Gunhee</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Expressing an Image Stream with a Sequence of Natural Sentences</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A607EAEE303D4603DFE874DDB13A2B05</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T04:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose an approach for retrieving a sequence of natural sentences for an image stream. Since general users often take a series of pictures on their special moments, it would better take into consideration of the whole image stream to produce natural language descriptions. While almost all previous studies have dealt with the relation between a single image and a single natural sentence, our work extends both input and output dimension to a sequence of images and a sequence of sentences. To this end, we design a multimodal architecture called coherence recurrent convolutional network (CRCN), which consists of convolutional neural networks, bidirectional recurrent neural networks, and an entity-based local coherence model. Our approach directly learns from vast user-generated resource of blog posts as text-image parallel training data. We demonstrate that our approach outperforms other state-of-the-art candidate methods, using both quantitative measures (e.g. BLEU and top-K recall) and user studies via Amazon Mechanical Turk.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently there has been a hike of interest in automatically generating natural language descriptions for images in the research of computer vision, natural language processing, and machine learning (e.g. <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b29">30]</ref>). While most of existing work aims at discovering the relation between a single image and a single natural sentence, we extend both input and output dimension to a sequence of images and a sequence of sentences, which may be an obvious next step toward joint understanding of the visual content of images and language descriptions, albeit under-addressed in current literature. Our problem setup is motivated by that general users often take a series of pictures on their memorable moments. For example, many people who visit New York City (NYC) would capture their experiences with large image streams, and thus it would better take the whole photo stream into consideration for the translation to a natural language description. Fig. <ref type="figure" target="#fig_0">1</ref> illustrates an intuition of our problem statement with an example of visiting NYC. Our objective is, given a photo stream, to automatically produce a sequence of natural language sentences that best describe the essence of the input image set. We propose a novel multimodal architecture named coherence recurrent convolutional networks (CRCN) that integrate convolutional neural networks for image description <ref type="bibr" target="#b12">[13]</ref>, bidirectional recurrent neural networks for the language model <ref type="bibr" target="#b19">[20]</ref>, and the local coherence model <ref type="bibr" target="#b0">[1]</ref> for a smooth flow of multiple sentences. Since our problem deals with learning the semantic relations between long streams of images and text, it is more challenging to obtain appropriate text-image parallel corpus than previous research of single sentence generation. Our idea to this issue is to directly leverage online natural blog posts as text-image parallel training data, because usually a blog consists of a sequence of informative text and multiple representative images that are carefully selected by authors in a way of storytelling. See an example in Fig. <ref type="figure" target="#fig_0">1</ref>.(a).</p><p>We evaluate our approach with the blog datasets of the NYC and Disneyland, consisting of more than 20K blog posts with 140K associated images. Although we focus on the tourism topics in our experiments, our approach is completely unsupervised and thus applicable to any domain that has a large set of blog posts with images. We demonstrate the superior performance of our approach by comparing with other state-of-the-art alternatives, including <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b20">21]</ref>. We evaluate with quantitative measures (e.g. BLEU and Top-K recall) and user studies via Amazon Mechanical Turk (AMT).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related work.</head><p>Due to a recent surge of volume of literature on this subject of generating natural language descriptions for image data, here we discuss a representative selection of ideas that are closely related to our work. One of the most popular approaches is to pose the text generation as a retrieval problem that learns ranking and embedding, in which the caption of a test image is transferred from the sentences of its most similar training images <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b25">26]</ref>. Our approach partly involves the text retrieval, because we search for candidate sentences for each image of a query sequence from training database. However, we then create a final paragraph by considering both compatibilities between individual images and text, and the coherence that captures text relatedness at the level of sentence-to-sentence transitions. There have been also video-sentence works (e.g. <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b31">32]</ref>); our key novelty is that we explicitly include the coherence model. Unlike videos, consecutive images in the streams may show sharp changes of visual content, which cause the abrupt discontinuity between consecutive sentences. Thus the coherence model is more demanded to make output passages fluent.</p><p>Many recent works have exploited multimodal networks that combine deep convolutional neural networks (CNN) <ref type="bibr" target="#b12">[13]</ref> and recurrent neural network (RNN) <ref type="bibr" target="#b19">[20]</ref>. Notable architectures in this category integrate the CNN with bidirectional RNNs <ref type="bibr" target="#b8">[9]</ref>, long-term recurrent convolutional nets <ref type="bibr" target="#b4">[5]</ref>, longshort term memory nets <ref type="bibr" target="#b29">[30]</ref>, deep Boltzmann machines <ref type="bibr" target="#b26">[27]</ref>, dependency-tree RNN <ref type="bibr" target="#b25">[26]</ref>, and other variants of multimodal RNNs <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b18">19]</ref>. Although our method partly take advantage of such recent progress of multimodal neural networks, our major novelty is that we integrate it with the coherence model as a unified end-to-end architecture to retrieve fluent sequential multiple sentences.</p><p>In the following, we compare more previous work that bears a particular resemblance to ours. Among multimodal neural network models, the long-term recurrent convolutional net <ref type="bibr" target="#b4">[5]</ref> is related to our objective because their framework explicitly models the relations between sequential inputs and outputs. However, the model is applied to a video description task of creating a sentence for a given short video clip and does not address the generation of multiple sequential sentences. Hence, unlike ours, there is no mechanism for the coherence between sentences. The work of <ref type="bibr" target="#b10">[11]</ref> addresses the retrieval of image sequences for a query paragraph, which is the opposite direction of our problem. They propose a latent structural SVM framework to learn the semantic relevance relations from text to image sequences. However, their model is specialized only for the image sequence retrieval, and thus not applicable to the natural sentence generation.</p><p>Contributions. We highlight main contributions of this paper as follows. <ref type="bibr" target="#b0">(1)</ref> To the best of our knowledge, this work is the first to address the problem of expressing image streams with sentence sequences. We extend both input and output to more elaborate forms with respect to a whole body of existing methods: image streams instead of individual images and sentence sequences instead of individual sentences. <ref type="bibr" target="#b1">(2)</ref> We develop a multimodal architecture of coherence recurrent convolutional networks (CRCN), which integrates convolutional networks for image representation, recurrent networks for sentence modeling, and the local coherence model for fluent transitions of sentences. <ref type="bibr" target="#b2">(3)</ref> We evaluate our method with large datasets of unstructured blog posts, consisting of 20K blog posts with 140K associated images. With both quantitative evaluation and user studies, we show that our approach is more successful than other state-of-the-art alternatives in verbalizing an image stream.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Text-Image Parallel Dataset from Blog Posts</head><p>We discuss how to transform blog posts to a training set B of image-text parallel data streams, each of which is a sequence of image-sentence pairs:</p><formula xml:id="formula_0">B l = {(I l 1 , T l 1 ),• • •, (I l N l , T l N l )} ∈ B.</formula><p>The training set size is denoted by L = |B|. Fig. <ref type="figure" target="#fig_1">2</ref>.(a) shows the summary of pre-processing steps for blog posts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Blog Pre-processing</head><p>We assume that blog authors augment their text with multiple images in a semantically meaningful manner. In order to decompose each blog into a sequence of images and associated text, we first perform text segmentation and then text summarization. The purpose of text segmentation is to divide the input blog text into a set of text segments, each of which is associated with a single image. Thus, the number of segments is identical to the number of images in the blog. The objective of text summarization is to reduce each text segment into a single key sentence. As a result of these two processes, we can transform each blog into a form of</p><formula xml:id="formula_1">B l = {(I l 1 , T l 1 ), • • • , (I l N l , T l N l )}.</formula><p>Text segmentation. We first divide the blog passage into text blocks according to paragraphs. We apply a standard paragraph tokenizer of NLTK <ref type="bibr" target="#b1">[2]</ref> that uses rule-based regular expressions to detect paragraph divisions. We then use the heuristics based on the image-to-text block distances proposed in <ref type="bibr" target="#b9">[10]</ref>. Simply, we assign each text block to the image that has the minimum index distance where each text block and image is counted as a single index distance in the blog.</p><p>Text summarization. We summarize each text segment into a single key sentence. We apply the Latent Semantic Analysis (LSA)-based summarization method <ref type="bibr" target="#b3">[4]</ref>, which uses the singular value decomposition to obtain the concept dimension of sentences, and then recursively finds the most representative sentences that maximize the inter-sentence similarity for each topic in a text segment.</p><p>Data augmentation. The data augmentation is a well-known technique for convolutional neural networks to improve image classification accuracies <ref type="bibr" target="#b12">[13]</ref>. Its basic idea is to artificially increase the number of training examples by applying transformations, horizontal reflection or adding noise to training images. We empirically observe that this idea leads better performance in our problem as well. For each image-sentence sequence</p><formula xml:id="formula_2">B l = {(I l 1 , T l 1 ), • • • , (I l N l , T l N l</formula><p>)}, we augment each sentence T l n with multiple sentences for training. That is, when we perform the LSA-based text summarization, we select top-κ highest ranked summary sentences, among which the top-ranked one becomes the summary sentence for the associated image, and all the top-κ ones are used for training in our model. With a slight abuse of notation, we let T l n to denote both the single summary sentence and κ augmented sentences. We choose κ = 3 after thorough empirical tests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Text Description</head><p>Once we represent each text segment with κ sentences, we extract the paragraph vector <ref type="bibr" target="#b16">[17]</ref> to represent the content of text. The paragraph vector is a neural-network based unsupervised algorithm that learns fixed-length feature representation from variable-length pieces of passage. We learn 300dimensional dense vector representation separately from the two classes of the blog dataset using the gensim doc2vec code. We use p n to denote the paragraph vector representation for text T n . We then extract a parsed tree for each T n to identify coreferent entities and grammatical roles of the words. We use the Stanford core NLP library <ref type="bibr" target="#b17">[18]</ref>. The parse trees are used for the local coherence model, which will be discussed in section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Architecture</head><p>Many existing sentence generation models (e.g. <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b18">19]</ref>) combine words or phrases from training data to generate a sentence for a novel image. Our approach is one level higher; we use sentences from training database to author a sequence of sentences for a novel image stream. Although our model can be easily extended to use words or phrases as basic building blocks, such granularity makes sequences too long to train the language model, which may cause several difficulties for learning the RNN models. For example, the vanishing gradient effect is a well-known hardship to backpropagate an error signal through a long-range temporal interval. Therefore, we design our approach that retrieves individual candidate sentences for each query image from training database and crafts a best sentence sequence, considering both the fitness of individual image-to-sentence pairs and coherence between consecutive sentences.   <ref type="bibr" target="#b12">[13]</ref> for image representation, bidirectional recurrent neural networks (BRNN) <ref type="bibr" target="#b23">[24]</ref> for sentence sequence modeling, and the local coherence model <ref type="bibr" target="#b0">[1]</ref> for a smooth flow of multiple sentences. Each data stream is a variable-length sequence denoted by</p><formula xml:id="formula_3">{(I 1 , T 1 ), • • • , (I N , T N )}. We use t ∈ {1, • • • , N } to</formula><p>denote a position of a sentence/image in a sequence. We define the CNN and BRNN model for each position separately, and the coherence model for a whole data stream. For the CNN component, our choice is the VGGNet <ref type="bibr" target="#b24">[25]</ref> that represents images as 4,096-dimensional vectors. We discuss the details of our BRNN and coherence model in section 3.1 and section 3.2 respectively, and finally present how to combine the output of the three components to create a single compatibility score in section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The BRNN Model</head><p>The role of BRNN model is to represent a content flow of text sequences. In our problem, the BRNN is more suitable than the normal RNN, because the BRNN can simultaneously model forward and backward streams, which allow us to consider both previous and next sentences for each sentence to make the content of a whole sequence interact with one another. As shown in Fig. <ref type="figure" target="#fig_1">2</ref>.(b), our BRNN has five layers: input layer, forward/backward layer, output layer, and ReLU activation layer, which are finally merged with that of the coherence model into two fully connected layers. Note that each text is represented by 300-dimensional paragraph vector p t as discussed in section 2.2. The exact form of our BRNN is as follows. See Fig. <ref type="figure" target="#fig_1">2</ref>.(b) together for better understanding.</p><formula xml:id="formula_4">x f t = f (W f i p t + b f i ); x b t = f (W b i p t + b b i );<label>(1)</label></formula><formula xml:id="formula_5">h f t = f (x f t + W f h f t-1 + b f ); h b t = f (x b t + W b h b t+1 + b b ); o t = W o (h f t + h b t ) + b o .</formula><p>The BRNN takes a sequence of text vectors p t as input. We then compute x f t and x b t , which are the activations of input units to forward and backward units. Unlike other BRNN models, we separate the input activation into forward and backward ones with different sets of parameters W f i and W b i , which empirically leads a better performance. We set the activation function f to the Rectified Linear Unit (ReLU), f (x) = max(0, x). Then, we create two independent forward and backward hidden units, denoted by h f t and h b t . The final activation of the BRNN o t can be regarded as a description for the content of the sentence at location t, which also implicitly encodes the flow of the sentence and its surrounding context in the sequence. The parameter sets to learn include weights</p><formula xml:id="formula_6">{W f i , W b i , W f , W b , W o } ∈ R 300×300 and biases {b f i , b b i , b f , b b , b o } ∈ R 300×1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Local Coherence Model</head><p>The BRNN model can capture the flow of text content, but it lacks learning the coherence of passage that reflects distributional, syntactic, and referential information between discourse entities. Thus, we explicitly include a local coherence model based on the work of <ref type="bibr" target="#b0">[1]</ref>, which focuses on resolving the patterns of local transitions of discourse entities (i.e. coreferent noun phrases) in the whole text. As shown in Fig. <ref type="figure" target="#fig_1">2</ref>.(b), we first extract parse trees for every summarized text denoted by Z t and then concatenate all sequenced parse trees into one large one, from which we make an entity grid for the whole sequence. The entity grid is a table where each row corresponds to a discourse entity and each column represents a sentence. Grammatical role are expressed by three categories and one for absent (i.e. not referenced in the sentence): S (subjects), O (objects), X (other than subject or object) and -(absent). After making the entity grid, we enumerate the transitions of the grammatical roles of entities in the whole text. We set the history parameter to three, which means we can obtain 4 3 = 64 transition descriptions (e.g. SOor OOX). By computing the ratio of the occurrence frequency of each transition, we finally create a 64-dimensional representation that captures the coherence of a sequence. Finally, we make this descriptor to a 300-dimensional vector by zero-padding, and forward it to ReLU layer as done for the BRNN output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Combination of CNN, RNN, and Coherence Model</head><p>After the ReLU activation layers of the RNN and the coherence model, their output (i.e. {o t } N t=1 and q) goes through two fully connected (FC) layers, whose role is to decide a proper combination of the BRNN language factors and the coherence factors. We drop the bias terms for the fully-connected layers, and the dimensions of variables are</p><formula xml:id="formula_7">W f 1 ∈ R 512×300 , W f 2 ∈ R 4,096×512 , o t , q ∈ R 300×1 , s t , g ∈ R 4,096×1 , O ∈ R 300×N , and S ∈ R 4,096×N . O = [o 1 |o 2 |..|o N ]; S = [s 1 |s 2 |..|s N ]; W f 2 W f 1 [O|q] = [S|g].<label>(2)</label></formula><p>We use the shared parameters for O and q so that the output mixes well the interaction between the content flows and coherency. In our tests, joint learning outperforms learning the two terms with separate parameters. Note that the multiplication W f 2 W f 1 of the last two FC layers does not reduce to a single linear mapping, thanks to dropout. We assign 0.5 and 0.7 dropout rates to the two layers. Empirically, it improves generalization performance much over a single FC layer with dropout.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training the CRCN</head><p>To train our CRCN model, we first define the compatibility score between an image stream and a paragraph sequence. While our score function is inspired by Karpathy et al. <ref type="bibr" target="#b8">[9]</ref>, there are two major differences. First, the score function of <ref type="bibr" target="#b8">[9]</ref> deals between sentence fragments and image fragments, and thus the algorithm considers all combinations between them to find out the best matching. On the other hand, we define the score by an ordered and paired compatibility between a sentence sequence and an image sequence. Second, we also add the term that measures the relevance relation of coherency between an image sequence and a text sequence. Finally, the score S kl for a sentence sequence k and an image stream l is defined by</p><formula xml:id="formula_8">S kl = t=1...N s k t • v l t + g k • v l t<label>(3)</label></formula><p>where v l t denotes the CNN feature vector for t-th image of stream l. We then define the cost function to train our CRCN model as follows <ref type="bibr" target="#b8">[9]</ref>.</p><formula xml:id="formula_9">C(θ) = k l max(0, 1 + S kl -S kk ) + l max(0, 1 + S lk -S kk ) ,<label>(4)</label></formula><p>where S kk denotes the score between a training pair of corresponding image and sentence sequence. The objective, based on the max-margin structured loss, encourages aligned image-sentence sequence pairs to have a higher score by a margin than misaligned pairs. For each positive training example, we randomly sample 100 ne examples from the training set. Since each contrastive example has a random length, and is sampled from the dataset of a wide range of content, it is extremely unlikely that the negative examples have the same length and the same content order of sentences with positive examples.</p><p>Optimization. We use the backpropagation through time (BPTT) algorithm <ref type="bibr" target="#b30">[31]</ref> to train our model. We apply the stochastic gradient descent (SGD) with mini-batches of 100 data streams. Among many SGD techniques, we select RMSprop optimizer <ref type="bibr" target="#b27">[28]</ref>, which leads the best performance in our experiments. We initialize the weights of our CRCN model using the method of He et al. <ref type="bibr" target="#b6">[7]</ref>, which is robust in deep rectified models. We observe that it is better than a simple Gaussian random initialization, although our model is not extremely deep. We use dropout regularization in all layers except the BRNN, with 0.7 dropout for the last FC layer and 0.5 for the other remaining layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Retrieval of Sentence Sequences</head><p>At test time, the objective is to retrieve a best sentence sequence for a given query image stream {I q1 , • • • , I qN }. First, we select K-nearest images for each query image from training database using the 2 -distance on the CNN VGGNet fc7 features <ref type="bibr" target="#b24">[25]</ref>. In our experiments K = 5 is successful. We then generate a set of sentence sequence candidates C by concatenating the sentences associated with the K-nearest images at each location t. Finally, we use our learned CRCN model to compute the compatibility score between the query image stream and each sequence candidate, according to which we rank the candidates.</p><p>However, one major difficulty of this scenario is that there are exponentially many candidates (i.e. |C| = K N ). To resolve this issue, we use an approximate divide-and-conquer strategy; we recursively halve the problem into subproblems, until the size of the subproblem is manageable. For example, if we halve the search candidate length Q times, then the search space of each subproblem becomes K N/2 Q . Using the beam search idea, we first find the top-M best sequence candidates in the subproblem of the lowest level, and recursively increase the candidate lengths while the maximum candidate size is limited to M . We set M = 50. Though it is an approximate search, our experiments assure that it achieves almost optimal solutions with plausible combinatorial search, mainly because the local fluency and coherence is undoubtedly necessary for the global one. That is, in order for a whole sentence sequence to be fluent and coherent, its any subparts must be as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We compare the performance of our approach with other state-of-the-art candidate methods via quantitative measures and user studies using Amazon Mechanical Turk (AMT). Please refer to the supplementary material for more results and the details of implementation and experimental setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setting</head><p>Dataset. We collect blog datasets of the two topics: NYC and Disneyland. We reuse the blog data of Disneyland from the dataset of <ref type="bibr" target="#b10">[11]</ref>, and newly collect the data of NYC, using the same crawling method with <ref type="bibr" target="#b10">[11]</ref>, in which we first crawl blog posts and their associated pictures from two popular blog publishing sites, BLOGSPOT and WORDPRESS by changing query terms from Google search. Then, we manually select the travelogue posts that describe stories and events with multiple images. Finally, the dataset includes 11,861 unique blog posts and 78,467 images for NYC and 7,717 blog posts and 60,545 images for Disneyland.</p><p>Task. For quantitative evaluation, we randomly split our dataset into 80% as a training set, 10% as a validation, and the others as a test set. For each test post, we use the image sequence as a query I q and the sequence of summarized sentences as groundtruth T G . Each algorithm retrieves the best sequences from training database for a query image sequence, and ideally the retrieved sequences match well with T G . Since the training and test data are disjoint, each algorithm can only retrieve similar (but not identical) sentences at best.</p><p>For quantitative measures, we exploit two types of metrics of language similarity (i.e. BLEU <ref type="bibr" target="#b21">[22]</ref>, CIDEr <ref type="bibr" target="#b28">[29]</ref>, and METEOR <ref type="bibr" target="#b15">[16]</ref> scores) and retrieval accuracies (i.e. top-K recall and median rank), which are popularly used in text generation literature <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b25">26]</ref>. The top-K recall R@K is the recall rate of a groundtruth retrieval given top K candidates, and the median rank indicates the median ranking value of the first retrieved groundtruth. A better performance is indicated by higher BLEU, CIDEr, METEOR, R@K scores, and lower median rank values.</p><p>Baselines. Since the sentence sequence generation from image streams has not been addressed yet in previous research, we instead extend several state-of-the-art single-sentence models that have publicly available codes as baselines, including the log-bilinear multimodal models by Kiros et al. <ref type="bibr" target="#b11">[12]</ref>, and recurrent convolutional models by Karpathy et al. <ref type="bibr" target="#b8">[9]</ref> and Vinyals et al. <ref type="bibr" target="#b29">[30]</ref>. For <ref type="bibr" target="#b11">[12]</ref>, we use the three variants introduced in the paper, which are the standard log-bilinear model (LBL), and two multi-modal extensions: modality-based LBL (MLBL-B) and factored three-way LBL (MLBL-F). We use the NeuralTalk package authored by Karpathy et al. for the baseline of <ref type="bibr" target="#b8">[9]</ref> denoted by (CNN+RNN), and <ref type="bibr" target="#b29">[30]</ref> denoted by (CNN+LSTM). As the simplest baseline, we also compare with the global matching (GloMatch) in <ref type="bibr" target="#b20">[21]</ref>. For all the baselines, we create final sentence sequences by concatenating the sentences generated for each image in the query stream.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Language metrics</head><p>Retrieval metrics B-1 B-2 B-3 B-4 CIDEr METEOR R@1 R@5 R@10 MedRank New York City (CNN+LSTM) <ref type="bibr" target="#b29">[30]</ref>  similarity metrics (BLEU) and retrieval metrics (R@K, median Rank). A better performance is indicated by higher BLEU, CIDEr, METEOR, R@K scores, and lower median rank values.</p><p>We also compare between different variants of our method to validate the contributions of key components of our method. We test the K-nearest search (1NN) without the RNN part as the simplest variant; for each image in a test query, we find its K(= 1) most similar training images and simply concatenate their associated sentences. The second variant is the BRNN-only method denoted by (RCN) that excludes the entity-based coherence model from our approach. Our complete method is denoted by (CRCN), and this comparison quantifies the improvement by the coherence model. To be fair, we use the same VGGNet fc7 feature <ref type="bibr" target="#b24">[25]</ref> for all the algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Quantitative Results</head><p>Table <ref type="table" target="#tab_0">1</ref> shows the quantitative results of experiments using both language and retrieval metrics.</p><p>Our approach (CRCN) and (RCN) outperform, with large margins, other state-of-the-art baselines, which generate passages without consideration of sentence-to-sentence transitions unlike ours. The (MLBL-F) shows the best performance among the three models of <ref type="bibr" target="#b11">[12]</ref> albeit with a small margin, partly because they share the same word dictionary in training. Among mRNN-based models, the (CNN+LSTM) significantly outperforms the (CNN+RNN), because the LSTM units help learn models from irregular and lengthy data of natural blogs more robustly.</p><p>We also observe that (CRCN) outperforms (1NN) and (RCN), especially with the retrieval metrics. It shows that the integration of two key components, the BRNN and the coherence model, indeed contributes the performance improvement. The (CRCN) is only slightly better than the (RCN) in language metrics but significantly better in retrieval metrics. It means that (RCN) is fine with retrieving fairly good solutions, but not good at ranking the only correct solution high compared to (CRCN). The small margins in language metrics are also attributed by their inherent limitation; for example, the BLEU focuses on counting the matches of n-gram words and thus is not good at comparing between sentences, even worse between paragraphs for fully evaluating their fluency and coherency.</p><p>Fig. <ref type="figure" target="#fig_3">3</ref> illustrates several examples of sentence sequence retrieval. In each set, we show a query image stream and text results created by our method and baselines. Except Fig. <ref type="figure" target="#fig_3">3</ref>.(d), we show parts of sequences because they are rather long for illustration. These qualitative examples demonstrate that our approach is more successful to verbalize image sequences that include a variety of content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">User Studies via Amazon Mechanical Turk</head><p>We perform user studies using AMT to observe general users' preferences between text sequences by different algorithms. Since our evaluation involves multiple images and long passages of text, we design our AMT task to be sufficiently simple for general turkers with no background knowledge. We first randomly sample 100 test streams from the two datasets. We first set the maximum number of images per query to 5. If a query is longer than that, we uniformly sample it to 5. In an AMT test, we show a query image stream I q , and a pair of passages generated by our method (CRCN) and one baseline in a random order. We ask turkers to choose more agreed text sequence with I q . We design the test as a pairwise comparison instead of a multiple-choice question to make answering and analysis easier. The questions look very similar to the examples of Fig. <ref type="figure" target="#fig_3">3</ref>. We obtain answers from three different turkers for each query. We compare with four baselines; we choose (MLBL-B) among the three variants of <ref type="bibr" target="#b11">[12]</ref>, and (CNN+LSTM) among mRNN-based methods. We also select (GloMatch), and (RCN) as the variants of our method.</p><p>Table <ref type="table">2</ref> shows the results of AMT tests, which validate that AMT annotators prefer our results to those of baselines. The (GloMatch) is the worst because it uses too weak image representation (i.e. GIST and Tiny images). The differences between (CRCN) and (RCN) (i.e. 4th column of Table <ref type="table">2</ref>) are not as significant as previous quantitative measures, mainly because our query image stream is sampled to relatively short 5. The coherence becomes more critical as the passage is longer. To justify this argument, we run another set of AMT tests in which we use 8-10 images per query. As shown in the last column of Table <ref type="table">2</ref>, the performance margins between (CRCN) and (RCN) become larger as the lengths of query image streams increase. This result assures that as passages are longer, the coherence becomes more important, and thus (CRCN)'s output is more preferred by turkers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We proposed an approach for retrieving sentence sequences for an image stream. We developed coherence recurrent convolutional network (CRCN), which consists of convolutional networks, bidirectional recurrent networks, and entity-based local coherence model. With quantitative evaluation and users studies using AMT on large collections of blog posts, we demonstrated that our CRCN approach outperformed other state-of-the-art candidate methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An intuition of our problem statement with a New York City example. We aim at expressing an image stream with a sequence of natural sentences. (a) We leverage natural blog posts to learn the relation between image streams and sentence sequences. (b) We propose coherence recurrent convolutional networks (CRCN) that integrate convolutional networks, bidirectional recurrent networks, and the entity-based coherence model.</figDesc><graphic coords="1,108.00,593.40,391.18,99.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of (a) pre-processing steps of blog posts, and (b) the proposed CRCN architecture.</figDesc><graphic coords="4,108.00,81.86,391.17,137.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2.(b) illustrates the structure of our CRCN. It consists of three main components, which are convolutional neural networks (CNN)<ref type="bibr" target="#b12">[13]</ref> for image representation, bidirectional recurrent neural networks (BRNN)<ref type="bibr" target="#b23">[24]</ref> for sentence sequence modeling, and the local coherence model<ref type="bibr" target="#b0">[1]</ref> for a smooth flow of multiple sentences. Each data stream is a variable-length sequence denoted by {(I 1 , T 1 ), • • • , (I N , T N )}. We use t ∈ {1, • • • , N } to denote a position of a sentence/image in a sequence. We define the CNN and BRNN model for each position separately, and the coherence model for a whole data stream. For the CNN component, our choice is the VGGNet<ref type="bibr" target="#b24">[25]</ref> that represents images as 4,096-dimensional vectors. We discuss the details of our BRNN and coherence model in section 3.1 and section 3.2 respectively, and finally present how to combine the output of the three components to create a single compatibility score in section 3.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Examples of sentence sequence retrieval for NYC (top) and Disneyland (bottom). In each set, we present a part of a query image stream, and its corresponding text output by our method and a baseline.Baselines (GloMatch) (CNN+LSTM) (MLBL-B) (RCN) (RCN N&gt;=8) NYC 92.7% (139/150) 80.0% (120/150) 69.3% (104/150) 54.0% (81/150) 57.0% (131/230) Disneyland 95.3% (143/150) 82.0% (123/150) 70.7% (106/150) 56.0% (84/150) 60.1% (143/238) Table 2: The results of AMT pairwise preference tests. We present the percentages of responses that turkers vote for our (CRCN) over baselines. The length of query streams is 5 except the last column, which has 8-10.</figDesc><graphic coords="8,109.16,211.88,391.18,112.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Evaluation of sentence generation for the two datasets, New York City and Disneyland, with language</figDesc><table><row><cell></cell><cell>21.31 3.65 0.57 0.14</cell><cell>9.1</cell><cell>5.73</cell><cell>0.95 5.24</cell><cell>8.57</cell><cell>84.5</cell></row><row><cell>(CNN+RNN) [9]</cell><cell>6.21 0.01 0.00 0.00</cell><cell>0.5</cell><cell>1.34</cell><cell>0.48 2.86</cell><cell>4.29</cell><cell>120.5</cell></row><row><cell cols="2">(MLBL-F) [12] 21.03 1.92 0.12 0.01</cell><cell>4.3</cell><cell>6.03</cell><cell>0.71 4.52</cell><cell>7.86</cell><cell>87.0</cell></row><row><cell cols="2">(MLBL-B) [12] 20.43 1.54 0.09 0.01</cell><cell>2.6</cell><cell>5.30</cell><cell>0.48 3.57</cell><cell>5.48</cell><cell>101.5</cell></row><row><cell>(LBL) [12]</cell><cell>20.96 1.68 0.08 0.01</cell><cell>2.6</cell><cell>5.29</cell><cell>1.19 4.52</cell><cell>7.38</cell><cell>100.5</cell></row><row><cell cols="2">(GloMatch) [21] 19.00 1.59 0.04 0.0</cell><cell>2.80</cell><cell>5.17</cell><cell>0.24 2.62</cell><cell>4.05</cell><cell>95.00</cell></row><row><cell>(1NN)</cell><cell cols="2">25.97 3.42 0.60 0.22 15.9</cell><cell>7.06</cell><cell cols="2">5.95 13.57 20.71</cell><cell>63.50</cell></row><row><cell>(RCN)</cell><cell cols="2">27.09 5.45 2.56 2.10 33.5</cell><cell>7.87</cell><cell cols="2">3.80 18.33 30.24</cell><cell>29.00</cell></row><row><cell>(CRCN)</cell><cell cols="2">26.83 5.37 2.57 2.08 30.9</cell><cell>7.69</cell><cell cols="2">11.67 31.19 43.57</cell><cell>14.00</cell></row><row><cell></cell><cell cols="2">Disneyland</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">(CNN+LSTM) [30] 27.99 3.55 0.38 0.08 10.0</cell><cell>4.51</cell><cell cols="2">3.06 8.16 14.29</cell><cell>65.0</cell></row><row><cell>(CNN+RNN) [9]</cell><cell>6.04 0.00 0.00 0.00</cell><cell>0.4</cell><cell>1.34</cell><cell>1.02 3.40</cell><cell>5.78</cell><cell>88.0</cell></row><row><cell cols="2">(MLBL-F) [12] 15.75 1.61 0.07 0.01</cell><cell>4.9</cell><cell>7.12</cell><cell cols="2">0.68 4.08 10.54</cell><cell>63.0</cell></row><row><cell cols="2">(MLBL-B) [12] 15.65 1.32 0.05 0.00</cell><cell>3.8</cell><cell>5.83</cell><cell>0.34 2.72</cell><cell>6.80</cell><cell>69.0</cell></row><row><cell>(LBL) [12]</cell><cell>18.94 1.70 0.06 0.01</cell><cell>3.4</cell><cell>4.99</cell><cell>1.02 4.08</cell><cell>7.82</cell><cell>62.0</cell></row><row><cell cols="2">(GloMatch) [21] 11.94 0.37 0.01 0.00</cell><cell>2.2</cell><cell>4.31</cell><cell>2.04 5.78</cell><cell>7.48</cell><cell>73.0</cell></row><row><cell>(1NN)</cell><cell cols="2">25.92 3.34 0.71 0.38 19.5</cell><cell>7.46</cell><cell cols="2">9.18 19.05 27.21</cell><cell>45.0</cell></row><row><cell>(RCN)</cell><cell cols="2">28.15 6.84 4.11 3.52 51.3</cell><cell>8.87</cell><cell cols="2">5.10 20.07 28.57</cell><cell>29.5</cell></row><row><cell>(CRCN)</cell><cell cols="2">28.40 6.88 4.11 3.49 52.7</cell><cell>8.78</cell><cell cols="2">14.29 31.29 43.20</cell><cell>16.0</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This research is partially supported by Hancom and Basic Science Research Program through National Research Foundation of Korea (2015R1C1A1A02036562).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Modeling Local Coherence: An Entity-Based Approach</title>
		<author>
			<persName><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Klein</surname></persName>
		</author>
		<title level="m">Natural Language Processing with Python</title>
		<imprint>
			<publisher>O&apos;Reilly Media Inc</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mind&apos;s Eye: A Recurrent Visual Representation for Image Caption Generation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Latent Semantic Analysis for Text Segmentation</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">Y Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wiemer-Hastings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Long-term Recurrent Convolutional Networks for Visual Recognition and Description</title>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improving Image-Sentence Embeddings Using Large Weakly Annotated Photo Collections</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAIR</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="853" to="899" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep Visual-Semantic Alignments for Generating Image Descriptions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Joint Photo Stream and Blog Post Summarization and Exploration</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ranking and Retrieval of Image Sequences from Multiple Paragraph Queries</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multimodal Neural Language Models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Baby Talk: Understanding and Generating Image Descriptions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Premraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">TreeTalk: Composition and Compression of Trees for Image Descriptions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TACL</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B A</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distributed Representations of Sentences and Documents</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcclosky</surname></persName>
		</author>
		<title level="m">The Stanford CoreNLP Natural Language Processing Toolkit</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Statistical Language Models based on Neural Networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
		<respStmt>
			<orgName>Brno University of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph. D. Thesis</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Im2Text: Describing Images Using 1 Million Captioned Photographs</title>
		<author>
			<persName><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">BLEU: A Method for Automatic Evaluation of Machine Translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Translating Video Content to Natural Language Descriptions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thater</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pinkal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bidirectional Recurrent Neural Networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TSP</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Grounded Compositional Semantics for Finding and Describing Images with Sentences</title>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TACL</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multimodal Learning with Deep Boltzmann Machines</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Lecture 6.5 -RMSProp</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Coursera</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">CIDEr: Consensus-based Image Description Evaluation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.5726</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Show and Tell: A Neural Image Caption Generator</title>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Generalization of Backpropagation with Application to a Recurrent Gas Market Model</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Werbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="339" to="356" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Jointly Modeling Deep Video and Compositional Text to Bridge Vision and Language in a Unified Framework</title>
		<author>
			<persName><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
