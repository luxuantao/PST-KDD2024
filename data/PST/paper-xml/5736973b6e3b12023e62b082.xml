<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Discovery of Meaningful Rules in Time Series</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mohammad</forename><surname>Shokoohi-Yekta</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Riverside</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yanping</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Riverside</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bilson</forename><surname>Campana</surname></persName>
							<email>bcampana@cs.ucr.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Riverside</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bing</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Riverside</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jesin</forename><surname>Zakaria</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Riverside</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Eamonn</forename><surname>Keogh</surname></persName>
							<email>eamonn@cs.ucr.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Riverside</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Discovery of Meaningful Rules in Time Series</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E764E57CC7A51C6881B3A15A7FC253B9</idno>
					<idno type="DOI">10.1145/2783258.2783306</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T05:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.2.8 [Information Systems]: Database Application -Data Mining Rule Discovery</term>
					<term>Prediction</term>
					<term>Motif Discovery</term>
					<term>Time Series</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The ability to make predictions about future events is at the heart of much of science; so, it is not surprising that prediction has been a topic of great interest in the data mining community for the last decade. Most of the previous work has attempted to predict the future based on the current value of a stream. However, for many problems the actual values are irrelevant, whereas the shape of the current time series pattern may foretell the future. The handful of research efforts that consider this variant of the problem have met with limited success. In particular, it is now understood that most of these efforts allow the discovery of spurious rules. We believe the reason why rule discovery in real-valued time series has failed thus far is because most efforts have more or less indiscriminately applied the ideas of symbolic stream rule discovery to real-valued rule discovery. In this work, we show why these ideas are not directly suitable for rule discovery in time series. Beyond our novel definitions/representations, which allow for meaningful and extendable specifications of rules, we further show novel algorithms that allow us to quickly discover high quality rules in very large datasets that accurately predict the occurrence of future events.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Prediction and forecasting have been a topic of great interest in the data mining community for the last decade. Most of the work in the literature has dealt with discrete objects, such as keystrokes (i.e. predictive text), database queries <ref type="bibr" target="#b16">[16]</ref>, medical interventions <ref type="bibr" target="#b28">[28]</ref>, web clicks, etc. However, prediction may also have great utility in real-valued time series. For concreteness we briefly consider two examples:  Researchers in robotic interaction have long noted the importance of short-term prediction of human initiated forces to allow a robot to plan its interaction with a human. For example a recent paper notes the critical "importance of the prediction of motion velocity and the anticipation of future perceived forces</p><p>[to allow the] robot to anticipate the partner's intentions and adapt its motion" <ref type="bibr" target="#b10">[10]</ref>.  Doppler radar technology introduced in the last two decades has increased the mean lead time for tornado warnings from 5.3 to 9.5 minutes, saving countless lives <ref type="bibr" target="#b3">[3]</ref>. But progress seems to have stalled, with 26% of tornados within the US occurring with no warning. McGovern et al. argue that further improvements will come not from new sensors, but from yet-to-be-invented algorithms that "examine existing (time series) data for predictive rules" <ref type="bibr" target="#b19">[19]</ref>. Most of the current work has attempted to predict the future based on the current value of a stream <ref type="bibr" target="#b18">[18]</ref>. However, for many problems the actual values are irrelevant, but the shape of the current pattern may foretell the future. For clarity we call the former forecasting, and the latter, the subject of this paper, rulebased prediction (although the literature is inconsistent on this convention). There is an additional critical distinction between forecasting and rule-based prediction. Time series forecasting is typically always-on; it predicts values at every time step. In contrast, rule-based prediction monitors the incoming data at each time step, but only occasionally makes a prediction about an imminent occurrence of a pattern. While forecasting is mature enough to have its own conferences and commercial software (SAS, IBM Cognos, etc.), the handful of research efforts to consider time series rule-based prediction have met with limited success. In particular, it is widely accepted that these efforts allow the discovery of spurious rules <ref type="bibr" target="#b12">[12]</ref>, including finding high confidence "rules" in random walk data. We believe that the reason why rule discovery in real-valued time series has failed thus far is that most efforts have more or less indiscriminately applied the ideas of symbolic stream rule discovery to real-valued rule discovery. In this work, we argue that such ideas are not directly transferable to rule discovery in real-valued time series. Instead, we formulate a rule representation and a Minimum Description Length (MDL) inspired search strategy that evaluates candidate rules based on how well they can compress the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">BACKGROUND AND RELATED WORK</head><p>In a sequence of papers culminating in <ref type="bibr" target="#b21">[21]</ref>, Park and Chu investigate a rule finding mechanism for time series. However, the algorithm is only evaluated for speed and then only on random walk data. No evidence was presented that the algorithm could actually find generalizable rules in time series. Work by Wu and colleagues also use a piecewise linear representation to support rule discovery in time series. They tested their algorithm on real (financial) data, reporting approximately 68% "correctness of trend prediction" <ref type="bibr" target="#b29">[29]</ref>. However, the authors graciously ran their algorithm on data provided by others and when they ran their algorithms on pure random walk data, they again achieved approximately 68% correctness of trend prediction <ref type="bibr" target="#b30">[30]</ref>. This suggests their original results did not outperform random guessing. The most referenced time series rule-finding method in the literature is <ref type="bibr" target="#b5">[5]</ref>, which quantizes the data with K-means clustering of the entire training dataset and passes the (now) symbolic data over to a classic association rule discovery algorithm. The success of a rule is measured with a score called the J-measure. The method was used in several papers before it was shown that the Jmeasure gave the same significance to rules found in completely random data as to rules found in real data <ref type="bibr" target="#b12">[12]</ref>. Later analyses by more than a dozen follow-up papers suggest that the problem is with the quantization step; in essence any technique that involves clustering all subsequences is doomed to produce cluster centers that are independent of the data <ref type="bibr" target="#b12">[12]</ref>. In Section 7.7 we have compared our algorithm to the three highly cited rival methods. For brevity, we forgo an in-depth review related work here, referring the reader to an expanded version of this paper <ref type="bibr">[31]</ref>. For more background on MDL we direct the interested reader to <ref type="bibr" target="#b1">[1]</ref> <ref type="bibr" target="#b15">[15]</ref>. We will discuss our use of MDL in Section 5.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">THE INTUITION OF RULE DISCOVERY</head><p>It may be instructive to first consider the analogue problem of rule discovery in symbolic strings. Let us consider "The Raven", by Edgar Allan Poe. It begins:</p><p>Once upon a midnight dreary, while I pondered weak and … What are the possible rules we might discover in this text? One possible rule is that the word "door" often follows the word "chamber," a rule we can denote as:</p><formula xml:id="formula_0">chamber → door</formula><p>The left side of the rule is the antecedent and the right side is the consequent. This rule is based on our observation that we see the phrase "chamber door" ten times in the text. We note that this is not a perfect rule; the word "chamber" appears once without been followed by "door" ("...into the chamber turning..."). Furthermore, it is important to note that the rule does not make the claim that all, or even many, occurrences of "door" are preceded by "chamber". In fact, there are four more examples of the word "door" in the text. A major difference between text and time series is that the latter does not have a natural segmentation (i.e. spaces or periods), thus we are facing data that is more like this: onceuponamidnightdrearywhileIponderedweak.... Given such a text, there are (language agnostic) algorithms that can segment the string into the original words <ref type="bibr" target="#b4">[4]</ref>, with varying degrees of accuracy. However, segmenting a real-valued time series into meaningful episodes is much more difficult. Furthermore, the problem is further complicated by the fact that, in most cases, the time series does not consist solely of discretely concatenated events. Rather, the events may be interspersed with filler symbols. For example, if we examine a motion capture of a sign language version of this poem there will be locations that do not correspond to discrete signs, but rather to transitions between signs. This will produce something rather like this: oncexauponwamidnightmtdrearydwhileuIpponderediweak... Finally, time series are inherently real-valued and as such, tests for equality are meaningless. This would be equivalent to our text string having some misspellings: qncexauponwamidmightmtdreerydwgileuIpponderediweek... The problem is now significantly more difficult than the original statement. We must generalize the antecedent to allow flexibility, perhaps by triggering the occurrence of a pattern that is within a certain threshold t distance under some suitable distance measure: dist("chamber", substring) ≤ t → door However, we are not done generalizing the rule model. The existence of misspellings in our data means that we may wish to accept similar consequents such as poor or dooor as successful predictions. Furthermore, we originally assumed that the consequent immediately followed the antecedent. However there may be some additional symbols between words. Thus we need to define a parameter, maxlag, which is the maximum number of characters between the end of the antecedent and the beginning of the consequent. For example, if maxlag, is set to two, then any of the below would be considered successful predictions:</p><p>...chamberdoor..., ...chamberzdoor..., ...chamberxydoor... but the following:</p><p>...chamberxzuvdoor... is not a successful prediction because the lag between the antecedent and consequent is too long. The maxlag parameter allows for meaningful falsifiable predictions. The prediction that "this consequent will eventually occur" is paradoxically both unfalsifiable and almost certainly true (if we wait long enough). We can now show our final rule format:</p><formula xml:id="formula_1">dist(chamber, substri,) ≤ t1 → dist(door, substrj) ≤ t2, j -(i + ρ -1) ≤ maxlag</formula><p>This can be read as follows: "If we see a substring of length ρ that is within distance t1 of the word chamber, then we fire the rule and expect to see a similar substring to word door, within a learned distance t2, in the next maxlag time steps."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Moving to Real-Valued Data</head><p>We are now ready to begin to "port" our ideas to the real-valued time series that are of interest in this work. We will start with an example for which we know the ground truth and for which the reader has already developed some intuition. However, we note that we are not using external knowledge to help our algorithm, only to validate and explain it. As shown in Figure <ref type="figure" target="#fig_0">1</ref>, we took an audio recording of the first four verses of "The Raven" (performed by an American male actor), and converted it to Melfrequency cepstrum coefficient (MFCC) space, keeping just the second coefficient. Using just the first 2,000 data points, which corresponds to the first verse of the poem, we found the pair of non-overlapping subsequences of length 100 (one second length in the original data) that had the minimum distance to each other. Such a pair of subsequences is referred to as a time series motif and extensively studied in the literature <ref type="bibr">[19][20]</ref>.</p><p>The occurrence of such a highly conserved motif suggests one possible method for specifying rules. We could simply split the motif pattern in two, let the average of the left side be the antecedent, and let the average of the right side be the consequent. We need to set the maxlag and the threshold, t1, parameters. For the moment, let us set the former to zero and the later to twice the  We can immediately test this rule by running it on the remainder of The Raven data. The rule fires exactly three times and in every case it maps to an utterance of "door." In this simple example, hard-coding the maxlag to zero is intuitive; however, we can easily imagine examples that need the flexibility of a larger maxlag constraint. Consider Figure <ref type="figure">3</ref> which shows accelerometer data collected from a device worn by a student at USC as he went about daily activities <ref type="bibr" target="#b22">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 3. left)</head><p>A rule for an accelerometer dataset encodes the fact that the initial acceleration "bump" of going up in an elevator must be eventually be matched by the elevator stopping at a floor. right) Real data from which this rule was learned <ref type="bibr" target="#b22">[22]</ref>.</p><p>This example shows a very easy rule to spot. The semicircular bump created by an elevator accelerating must eventually be matched by a bump in the opposite direction when the elevator brakes (the rule for elevators going down is similar, but with the consequent and antecedent swapped). The time lag between these two events is highly variable and depends on the number of floors serviced by the elevator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">THE RULE FRAMEWORK</head><p>We are now in a position to present the definitions necessary to rigorously define our rule framework. First, we need to define a distance measure between two subsequences. While there are dozens of measures in the literature, recent empirical evidence suggests that Euclidean distance is very difficult to beat <ref type="bibr" target="#b7">[7]</ref>. Furthermore, Euclidean distance is parameter-free, fast to compute, and is amiable to various data mining optimizations such as indexing and early abandoning computation <ref type="bibr" target="#b20">[20]</ref>. We empirically considered other distance measures including DTW, Swale, Spade and EPR <ref type="bibr" target="#b6">[6]</ref>, none improved the accuracy of the rules (a finding consistent with <ref type="bibr" target="#b6">[6]</ref>) and all required at least an order of magnitude more time. We formally define a time series antecedent as a subsequence used to trigger a rule if it is similar to the current sliding window:</p><p>Definition 1: Assume we are monitoring a time series by continuously extracting the sliding window, W. Given a positive constant t (threshold), and an antecedent time series Ra, a binary flag fired is set to TRUE if D(Ra, W) &lt; t. Note that in order for a candidate antecedent to be even considered as a rule precursor, it must occur at least twice; we cannot generalize from single exemplars. This is essentially the definition of a time series motif <ref type="bibr" target="#b20">[20]</ref>. In Section 6, we will exploit this in order to reduce our search space of antecedents and consequents.</p><p>In principle, the threshold, maxlag, and antecedent could be hand chosen by a domain expert. However, as we show later it is possible to find them automatically. As an antecedent is a precursor to an event, a predicted subsequence shape which follows an antecedent within a specified time (the maxlag) is called the antecedent's consequent: Definition 2: A consequent, , is a time series subsequence that is predicted to follow the detection of an antecedent within maxlag time steps. The maxlag parameter encodes the fact that for a time series subsequence to be a meaningful consequent in a rule, it must occur within some acceptable time after the rule's antecedent has been detected. Without such a constraint on time, a consequent's occurrence may be coincidental.</p><p>Definition 3: The maxlag is the maximum number of time steps allowed between a detected antecedent and its consequent. In particular, if tk is the last value in W, the moment the rule is triggered, then the consequent must be derived from a subsequence of T, Ti, such that 0 ≤ --1 ≤ . With an antecedent, its consequent, the maximum expected maxlag delay between the two, and the threshold distance used to trigger a subsequence match, we have all the necessary components to specify a single time series rule: Definition 4: A time series rule, R, is a 4-tuple of { , , maxlag, t}. One obvious way to obtain an antecedent and consequent with a zero maxlag is to take a subsequence and split it:</p><p>Definition 5: The Split Point is a ratio in the range (0, 1) which indicates the end point of the antecedent and the beginning of the consequent. Having defined time series rules and all supporting notation, we have just two more tasks. We need to formalize a scoring function to tell us how good a candidate rule is, and design an efficient search strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">DATA DISCRETIZATION</head><p>Because of our intention to use MDL to measure the relative merits of candidate rules, we must transform our real-valued time series into a discretized space <ref type="bibr" target="#b15">[15]</ref>. After consideration of the many quantization options, we quantize the time series' real values into uniformly sized bins. For a subsequence length ρ, we z-normalize all possible subsequences of that length and record the minimum and maximum values across the normalized subsequences. After attaining the global minimum value, min, and global maximum value, max, across all subsequences, we set bin boundaries that are uniformly sized between min and max. The resulting bin width is then: (max -min) / cardinality. We can show the (lack of) effect that discretization has on time series with classification experiments, since the rule triggering step is essentially a classification problem. We conducted empirical tests on data from the UCR Archive <ref type="bibr" target="#b26">[26]</ref>. For each dataset, we ran leave-one-out one-nearest-neighbor classification tests using uniform quantization with varying cardinalities. Table <ref type="table" target="#tab_0">1</ref> provides a snapshot of the results. It is demonstrated that a real-valued time series can be drastically reduced through discretization without significantly affecting the intrinsic information available. In fact, because cardinality reduction of the original data can reduce the effects of noise and outliers, we can sometimes see tiny improvements in accuracy.  These results allow us to use MDL with little fear that we are throwing away valuable information. Which value of cardinality should we use? Empirically, if the value is anywhere in the range of <ref type="bibr" target="#b16">[16,</ref><ref type="bibr">65536]</ref>, it makes no significant difference; we therefore use a cardinality of 16 throughout this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">MDL Scoring</head><p>We begin with an important disclaimer. We claim only that our work in this section is inspired by, and in the spirit of MDL (and MML <ref type="bibr" target="#b27">[27]</ref>). In particular, we have adopted (cf. <ref type="bibr" target="#b11">[11]</ref>) and extended ideas may deviate slightly from the absolute purist's interpretation of MDL. Our goal here is to produce a pragmatic scoring function that works in the real world. We thus defer theoretical and philosophical discussions to an appropriate venue.</p><p>The intuition behind our scoring function is that if we make a good prediction, the consequent shape we predict will be similar to a subsequence that occurs within maxlag steps. We could quantify this similarity with Euclidean distance (essentially the mean squared prediction error used in forecasting <ref type="bibr" target="#b18">[18]</ref>), however, the Euclidean distance does not allow us to compare the quality of consequents with different lengths. To make this clearer, let us return to our expository text example. Suppose we have to evaluate the following candidate rule: dist("chamber", substring) ≤ t → door, which when fired makes a prediction of length four. When encountering this string:</p><p>... bustabovehischamberdoorwithsuchnameasnevermore… it achieves a hamming distance (a good analogue of Euclidean distance) of 0. Contrast this result with the following rule: dist("chamber", substring) ≤ t → doorwithlikename, which when fired makes a prediction of length sixteen. While encountering the same string:</p><p>... bustabovehischamberdoorwithsuchnameasnevermore… it achieves a hamming distance of four. Which of these two rules is better?</p><p>The former is an exact but short prediction; the latter is an approximate but longer and arguably more informative prediction. Unfortunately, simply normalizing for length does not work here; while it is not commonly understood, the Euclidean distance between two subsequences of length ρ can actually decrease when we expand to length ρ + 1 due to the (re)normalization of the data. So not only is the effect of length not linear, it is not even monotonic.</p><p>Our solution to this problem, and the reason for the earlier digression into discretization of time series, is MDL <ref type="bibr">[1][15]</ref>. For several decades MDL has been used to solve very similar problems in intrinsically discrete domains such as text, DNA, MIDI, etc. However, this application to time series rules is novel. The intuition behind our use of MDL is to consider a candidate subsequence as a hypothesis, H, about a future event. This hypothesis (the bold/green line in Figure <ref type="figure" target="#fig_4">4</ref>) has some cost, the number of bits it takes to store it. We denote this cost as the Description Length, DL. If we store the subsequences as simple integer arrays, we have DL(H) = length(H)  log2(cardinality). We then want to evaluate the quality of a predicted consequent by asking how well the prediction matched the future. We do this by asking, "Given our consequent H, what is the cost to encode the error of the actual match m?" We denote this as DL(m H), that is, the description length of a matching subsequence m, given our hypothesized consequent H. We can measure this encoding cost by simply subtracting the consequent from the matching time series and encoding the difference vector efficiently. Thus the score of a candidate subsequence, m, with a hypothesis, H, is:</p><formula xml:id="formula_2">(1) - ( , ) = ( ) -( | ).</formula><p>This idea is illustrated in Figure <ref type="figure" target="#fig_4">4</ref>. Here we use a cardinality of just eight values for visual clarity. This equation allows us to measure the relative predictive power of subsequences, independent of their length. In order to find rules in a training set, we must have at least two firings. This means that to evaluate the hypothesis we must measure how well it encodes a set, M, of at least two consequents.</p><p>(</p><formula xml:id="formula_3">) total-bit-save(M,H)= -( ) + ∑ - ( , ) ∈ ,<label>2</label></formula><p>where the set M consists of all subsequences to be compressed with the consequent H.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">RULE DISCOVERY ALGORITHM</head><p>We are finally in a position to introduce our rule finding algorithm. In essence, it has two parts: 1) a scoring function and 2) a search algorithm which repeatedly invokes this scoring function while searching for high quality rules. As the scoring function is at the heart of our ideas, we will detail the intuition behind it next and then in Section 6.2 we will present our rule search method which utilizes this function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Rule Scoring</head><p>For clarity of presentation we begin by considering the case in which maxlag is constrained to be zero. Our MDL scoring function is given two inputs: a candidate time series (like either one of the two time series in Figure <ref type="figure" target="#fig_0">1</ref>) and an expected maxlag value (recall for the moment, it is hardcoded to zero). The function then returns three things: an antecedent, a consequent and the quality score of the resulting rule. Note that the antecedent concatenated with the consequent are simply the input time series, R, (much like Figure <ref type="figure" target="#fig_2">2</ref>), however the split point is not known in advance. Table <ref type="table" target="#tab_1">2</ref> illustrates the algorithm. Note that we propose a parameter-lite algorithm (Table <ref type="table" target="#tab_1">2</ref>), which is described by hierarchical functions that automatically generate most of the required inputs in Tables <ref type="table" target="#tab_5">3 to 6</ref>.  </p><formula xml:id="formula_4">-7 -6 -5 -1 -1 1 1 0 1 1 -4 -4 -1 0 0 0 0 0 -1 -1 -1 -3 -1 -1 -1 -1 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 -1 -1 0 0 -1 -1 0 -1 -1 0 -2 -1 0 0 -1 0 1 0 -1 0 0 1 0 -1 -1</formula><formula xml:id="formula_5">× Length(R)) c ← R (sp × Length(R) + 1: end) Return a, c, s</formula><p>In lines 1 to 4 the algorithm iterates on all possible split points for the candidate time series, R, and calculates the quality score described in Table <ref type="table" target="#tab_2">3</ref>. In line 5 we find the maximum quality score (s). In lines 6 to 8 we find the split point corresponding to the maximum quality score and we split R to the antecedent (a) and the consequent (c). The procedure returns a, c, and s.</p><p>In Table <ref type="table" target="#tab_2">3</ref> we describe how the rules are scored. For every antecedent that can be produced by R, we search for locations in T in which that rule would have fired. Given each firing, we "predict" the relevant consequent as a hypothesis H to explain the next |c| datapoints in T. We then calculate how many bits MDL could save using this prediction. If, as in Figure <ref type="figure" target="#fig_4">4</ref>.left, our "prediction" was accurate we will save many bits. A less accurate prediction (Figure <ref type="figure" target="#fig_4">4</ref>.right) will save fewer bits. The number of bits saved; summed over all firings (i.e. Eq. 2) is the score returned for the tentative rule. Concretely, in line 1 we find the set of subsequences similar to the antecedent of R. In line 2 we learn a threshold for the distance that leads to the largest quality score for R. In line 3 the algorithm calculates the largest number of bits saved for the rule instances and finally returns that value as a quality score for the rule. We find the set of subsequences similar to the antecedent of R in the subroutine described in Table <ref type="table" target="#tab_3">4</ref>. The first element of the set is the antecedent itself, the second element is the most similar subsequence to the antecedent, the next subsequence is the second closest and so on. This set includes all firings of R which need to be tested in Table <ref type="table" target="#tab_4">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 4. Algorithm to find a set of antecedent candidates</head><p>Procedure find_Antecedent_Candidates (T, R, sp) Input: A time series subsequence, R, extracted from a time series, T;</p><p>Split point for the antecedent/consequent, a number between zero and one, sp; Output: locations of antecedents in T ordered by distances from R's antecedent, ac; In lines 1 and 2 we find the antecedent of the rule R. In line 3 we slide the antecedent across the entire time series, T, and calculate the Euclidean distances for each subsequence of the same length.</p><p>In line 4 the algorithm finds the local minimums and sorts them according to their distances (ignoring trivial matches <ref type="bibr" target="#b20">[20]</ref>). In line 5 we find the locations of the sorted distances in the time series T and finally the procedure returns a set of antecedent candidates sorted by their distances to the antecedent of R.</p><p>Recall that in In lines 3 to 6 we iterate on the number of rule instances and each time calculate the total number of bit-saves as in Eq. 2. The loop terminates when the totalBitSaves starts decreasing. This use of a greedy approach to avoid searching all possible rules produces several orders of magnitude speedup, with little chance of missing a useful rule. In Section 6.2 we show that we can use the Euclidean distance as a heuristic to both guide the "rule test" order, and to tell us when we can abandon the rule test with a small, user-defined probability of missing the optimal answer (cf. Figure <ref type="figure" target="#fig_7">5</ref>). We further justify a probabilistic early abandoning approach in our supporting webpage <ref type="bibr">[31]</ref>. In line 7 we calculate the maximum number of total bit-saves and in line 8 we find the corresponding number of rule instances picked during the iteration in lines 3 to 6. In the subroutine in Table <ref type="table" target="#tab_4">5</ref> (and its invoking functions) we used Euclidean distance to process the data and create a large set of candidate rules with their observed outcomes on the training data. In Table <ref type="table" target="#tab_5">6</ref> we move from Euclidean distance to MDL to score these rules. We consider the consequent of R as a model/hypothesis and calculate the total number of bit-saves in order to predict other consequents. A larger number of bit-saves indicates more accurate predictions. After discovering antecedent candidates, we consider their following subsequences as consequents. The procedure then calculates the number of bits required to record the differences of the consequent saved as a model and the subsequences following antecedent candidates. In lines 1 and 2 the algorithm finds the consequent of R. In line 3 we discretize the consequent into 16 values and we z-normalize it. We will use this consequent as the hypothesis therefore we exclude the antecedent of R in line 6. In lines 7 to 10 for all n-1 AntecedentCandidates we find their corresponding consequents. In line 11 the algorithm discretizes and z-normalizes the corresponding consequents. In line 12 we calculate the number of bits required to record the consequents by using Huffman coding. In line 13 we use the consequent of R as a hypothesis and calculate the number of bits to save all other consequents by using MDL. In lines 14 and 15 our algorithm calculates the total number of bitsaves by subtracting the number of bits to record the consequents by using MDL from the number of bits to save the consequents by using Huffman coding (i.e. Eq. 2) and finally returns the total number of bit-saves, which tells us the quality of the rule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Motif-Based Rule Searching</head><p>The previous section explained the rule scoring operator algorithm (Table <ref type="table" target="#tab_1">2</ref>), all that remains is to explain the search algorithm that uses this operator. In principle we could use a brute force search, testing all subsequences of T. However this would be intractable. Fortunately we have an exploitable observation, a good rule candidate must be a time series motif in T, and efficient algorithms for discovering the top K motifs in a time series are well-known <ref type="bibr" target="#b19">[19]</ref> <ref type="bibr" target="#b20">[20]</ref>. Thus as illustrated in Table <ref type="table" target="#tab_6">7</ref>, we simply evaluate motifs from T, until we can claim the probability of finding a better rule is less than some small user-supplied threshold. In lines 3 to 8 we iterate over the motifs to discover the best rule. In line 4 our procedure calls the subroutine motif_Discovery (T, L, K), which uses the MK motif discovery algorithm <ref type="bibr" target="#b20">[20]</ref> to return the K th best motif of length L in the time series T. Note that by definition, the distance between each pair of motifs is non-decreasing in K <ref type="bibr" target="#b20">[20]</ref>. We exploit this to create an earlyAbandoning function, described later in this section, to terminate the loop. In line 5 we pass the discovered motif to the rule scoring function (Table <ref type="table" target="#tab_1">2</ref>) which finds the best rule that can be derived from that motif. In line 6 we add the discovered rule to the set of existing rules and finally we return the rule which has the largest score s in line 9. We have glossed over the termination condition for our algorithm (line 3). Here we describe it in more detail. Note that there is a strong relationship between Euclidean Distance (which motif discovery is minimizing) and bit-saves defined in Eq. 1 (which Table <ref type="table" target="#tab_1">2</ref> is maximizing). To illustrate this we performed the following experiment. From the MFCC time series of a recitation of the poem "The Dream within a Dream", we randomly sampled 20,000 subsequence pairs of length 100 (1 sec of audio), denoting one subsequence H and the other m. We measured ( -) and the ( ) -( | ) (Eq. 1), and use the two values to create the scatterplot shown in Figure <ref type="figure" target="#fig_7">5</ref>. The figure suggests we can use the Euclidean distance between motifs as a heuristic to tell us when we can abandon the motif The area under red curve to the right of green bar is the probability there exists an untested pair of subsequences with bit-saves greater than best-so-far discovery with a small, user-provided probability of missing the optimal answer. In essence, we propose to allow rule search in the form "stop searching when there is only a one in a thousand chance that the current best-so-far is not the best rule." Let Pbit-save(best-so-far) be the probability that the remaining pairs of subsequences in the Euclidean searching order (the x-axis ordering of Figure <ref type="figure" target="#fig_7">5</ref>.bottom) contains a better rule than the rule represented by the current best-so-far. Concretely, we compute the bit-saves (Eq. 1) for the subsequences on the left side of the dash-line (a) in Figure <ref type="figure" target="#fig_7">5</ref>.bottom to form the histogram shown at the top left of Figure <ref type="figure" target="#fig_7">5</ref>.</p><p>The bit-saves property of the distribution can be realized by a Gaussian process (GP) <ref type="bibr" target="#b8">[8]</ref>. The probability vector {φk} is drawn from a GP as φk ~ N( k, k 2 ), where μk is the mean and k 2 is the variance shown as the red "bell" curve. For example, the best-sofar bit-saves is 99 bits for both histograms in Figure <ref type="figure" target="#fig_7">5</ref>.top and the Pbit-save(best-so-far) of the distribution changes from 0.040 to 0.005 from left to right as shown in Figure <ref type="figure" target="#fig_7">5</ref>.top. The area below the red curve to the right of the best-so-far marker, is the probability that there exists an untested pair of subsequences with bit-saves greater than the best-so-far bit-saves. If Pbit-save(best-sofar) is less than the user threshold then we simply set the earlyAbandoning flag to be True, and the invoking search algorithm will terminate. This method has a few assumptions; for example that a thin vertical "slice" of the scatterplot is Gaussian. These assumptions are empirically observed on most datasets (see <ref type="bibr">[31]</ref>), and violations tend to result in a more conservative algorithm. That is the say, the algorithm may run a little longer, but will over-deliver on the requested probability of a true positive. Our illustration in Figure <ref type="figure" target="#fig_7">5</ref> makes one assumption that is unwarranted, that total bit-saves come from exactly two of subsequences. Recall from Table <ref type="table" target="#tab_5">6</ref> that in fact the total bit-saves come from at least two subsequences. We can easily generalize the earlyAbandoning function to account for this, but as it makes no empirical difference on the datasets we considered, for simplicity we ignore this idea in this work. We conclude this section with a simple experiment to reinforce the intuition that motif distances are a good proxy for rules. We took every subsequence of length 100 (one sec) of the MFCC version of "The Raven" and recorded its distance to its nearest neighbor. The distribution of these distances is shown in Figure <ref type="figure" target="#fig_9">6</ref> with a few annotated examples. Note that one occurrence of the phrase "...chamber door..." has a very small distance to its nearest neighbor, which is naturally just another occurrence of the phrase. Similarly, the repeated phrases such as "...the raven....", "…on the floor…", etc., also have small distances to their nearest neighbors. In contrast, phrases featuring hapax legomena 1 such as "caught" or "crest" have a huge distance to their nearest neighbor. If we were attempting to find rules in the text space, unique words or phrases do not need to be considered since we clearly cannot generalize rules from a single example. Moreover, Zipf's law tells us that about half the words in an English text are hapax legomena <ref type="bibr" target="#b14">[14]</ref>, and an even larger proportion of phrases must be unique. This observation is for text and, as Figure <ref type="figure" target="#fig_9">6</ref> hints, it is also true for most real-valued time series. 1 A hapax legomena is a word that appears only once in a body of text. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Generalizing to Allow a Maxlag</head><p>Our rule discovery algorithm described in Section 6.1 assumes a zero maxlag. To generalize to the arbitrary maxlag value case, we just need to slightly modify the "algorithm to score rule instances based on MDL" (Table <ref type="table" target="#tab_5">6</ref>). All other algorithms in our approach (Tables 2, 3, 4, 5 and 7) remain unchanged. While maxlag is allowed, we should consider a maxlag interval to search the consequent after the split point. The highlighted section of Table <ref type="table" target="#tab_7">8</ref> shows the modifications of Table <ref type="table" target="#tab_5">6</ref> which allows a non-zero maxlag in our rule discovery algorithm.</p><p>In line 10 we allow a maxlag value after the split point (subConsequent) to search for a subsequence in T closest to the consequent. In lines 11 to 14 we slide the consequent through each subsequence of the subConsequent and find the closest subsequence to the consequent. The remainder of the algorithm is the same as Table <ref type="table" target="#tab_5">6</ref>. In Section 7.2 we conduct an experiment which requires a non-zero maxlag. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">EXPERIMENTAL EVALUATION</head><p>To ensure that our experiments are reproducible, we have built a website which contains all data/code/raw spreadsheets for the results, in addition to many experiments that are omitted here for brevity <ref type="bibr">[31]</ref>. The visualization of the rules suffers from space limitations/BW formatting; we encourage the reader to view highresolution color versions at <ref type="bibr">[31]</ref>. We provide two sources of evaluation for quality. In some cases, as in "The Raven" example above, we show the rules are meaningful by considering the annotation available by external labels of some kind. In the more general case we use the Euclidean distance between our predicted consequent and the F matching locations where the rule fired, a value we denote as Ferror (this is essentially the root-mean-squared error). Because this number is difficult to interpret by itself, we do the following: On the same testing set, using the same consequent, we fire the rule randomly F times and measure the Euclidean distance between our predicted consequent and the F random locations. We denote this value as Rerror (which is averaged over 1,000 random runs). Our reported measure of quality then is just =</p><p>. Values close to one suggest our rules are no better than random guessing and values significantly less than one indicate that we are finding true structure in the data. For all experiments, except where otherwise stated, the maxlag parameter is set to zero. We compared our work to the three most obvious and widely cited rival methods. None perform above chance levels, therefore for brevity and clarity we push the details of these comparisons to the expanded version of our paper in [31].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Finding Rules in Bird Vocalization</head><p>We consider the task of finding rules in Zebra finch vocalizations (in MFCC space). Such rules may help weigh in on the "nature vs. nurture" debate <ref type="bibr" target="#b13">[13]</ref>, but here, simply show that we can learn robust accurate rules from complex and noisy datasets. The vocal learning lab at Hunter College provided recordings of Zebra finches singing (~one minute) every ten days, from day 40 to 100 (post hatching). Starting from day 40, we split data into a train (first 30-sec) and test set. Our algorithm finds several high quality rules, one of them shown in Figure <ref type="figure" target="#fig_10">7</ref>.  It is interesting to ask if the discovered rule generalizes over time, as the bird's song evolves (i.e. concept drift). To test this we apply the learned rule in Figure <ref type="figure" target="#fig_10">7</ref> to the zebra finch song from day 50. The low Q-scores in Figure <ref type="figure" target="#fig_12">9</ref> indicate that the rule discovered on day 40 (Figure <ref type="figure" target="#fig_10">7</ref>) still generalizes. In Figure <ref type="figure" target="#fig_13">10</ref> we repeat the same experiment for day 100. Here we find that while the rule does predict the future much better than chance, the song seems to have undergone some modifications. This finding is consistent with the literature which suggests that young birds vocally improvise until about 90 days, after which the song "crystallizes" <ref type="bibr" target="#b2">[2]</ref>. In <ref type="bibr">[31]</ref> we show many addition experiments in this domain, and allow the reader to actually hear the data/rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Finding Rules in Energy Disaggregation</head><p>A home-based intelligent energy conservation system needs to know what appliances (or loads) are being used in the home and when they are being used in order to provide intelligent feedback or to make decisions that can reduce costs. The AMPds, Almanac of Minutely Power dataset, contains one year of such data that includes eleven measurements at one-minute intervals for twentyone sub-meters <ref type="bibr" target="#b17">[17]</ref>.</p><p>For our experiments we consider a single meter into which the Fridge, Dishwasher, Clothes Washer and Clothes Dryer are plugged in. We run our rule discovery algorithm on the first month of the data and find the rule shown in Figure <ref type="figure" target="#fig_14">11</ref>.right as one of the top rules. Then we apply it to our test set and show one of the firings in Figure <ref type="figure" target="#fig_14">11</ref>.left. The antecedent and consequent in Figure <ref type="figure" target="#fig_14">11</ref>.right correspond to the Clothes Washer and Clothes Dryer respectively. The discovered rule in Figure <ref type="figure" target="#fig_14">11</ref>.right can be interpreted as: when the tenant uses the Clothes Washer, after a while they will run the Clothes Dryer. It is obvious that the tenant may immediately run the dryer or may spend some time doing something else first. Therefore in this case a non-zero maxlag must be allowed. For example the rule fired in Figure <ref type="figure" target="#fig_14">11</ref>.left contains a 20 minute gap between its antecedent and consequent. Other firings of the rule contained different values. In order to assure we capture most of the firings, we allowed a 100 minute maxlag. Our algorithm to allow maxlag is described in Section 6.3. An examination of the data suggests that our algorithm did not report any false positives for this experiment; however we did observe some true negatives. Most of these omissions may be attributed to the fact (as visually hinted at in Figure <ref type="figure" target="#fig_14">11</ref>.right) that the Clothes Washer patterns are complicated and polymorphic. That is to say, the patterns depend on many settings of the washing machine (whites/colors, rinse/spin etc). Automatically generalizing to handle such situations is ongoing work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Finding Rules in an Activity Data Set</head><p>We consider a benchmark data set that contains daily activity telemetry <ref type="bibr" target="#b23">[23]</ref>, of four subjects wearing seven inertial measurement units (IMUs). Each subject created five recordings, which we randomly divided into disjoint train/test partitions. We consider only the data from the right upper arm. Figure <ref type="figure" target="#fig_15">12</ref> shows one of the top rules learned from a recording of a subject. To test if the discovered rule generalizes to other instances of the activity, we applied the discovered rule in Figure <ref type="figure" target="#fig_15">12</ref> to other recordings of the same subject. Figure <ref type="figure" target="#fig_16">13</ref> shows the rule firings found in the test recording. According to the labels provided with the dataset, the rule discovered is a part of the activity: drinking from a cup while standing. To better understand the rule, we reproduced the data by having an actor wear an IMU on the same part of the body. We recorded the actor drinking from a cup using both the IMU (at 100Hz) and a camera to capture simultaneous video. Figure <ref type="figure" target="#fig_17">14</ref>.top shows some stills from the video. The time series of the complete activity from the IMU is shown in Figure <ref type="figure" target="#fig_17">14</ref>.bottom. Our data is very similar to the original benchmark data (our actor may have a different physique, mannerisms etc), and gives us some hints to understand the rule we discovered. As shown in Figure <ref type="figure" target="#fig_17">14</ref>, the rule appears to describe the first half part of the drinking activity: a lifting of the cup to the mouth is immediately followed by a slightly tilting head to drink from the cup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Finding Rules in NASA Telemetry Data</head><p>The NASA valve data set consists of 36 events of interleaved nominal and erroneous solenoid voltage measurements recorded from Marrotta series MPV-41 valves as they are tested in a laboratory <ref type="bibr">[9]</ref>. Figure <ref type="figure" target="#fig_18">15</ref> shows one of the top rules learned from this time series where the first peak in Figure <ref type="figure" target="#fig_18">15</ref>.left is that of a failed solenoid. We applied the discovered rule in Figure <ref type="figure" target="#fig_18">15</ref> to the test set and found the rule fires four times. Figure <ref type="figure" target="#fig_19">16</ref> shows the rule instances fired on the test set. This rule appears to describe a normal solenoid discharge event: a rapid decrease in the current is immediately followed by a slight ramp and gradual, complete discharge.</p><p>Because of the variety of malfunction events in contrast to the homogeneity of normal solenoid readings in this data set, this rule learned from successful tests achieves the highest MDL score as well as very low average Q-value of 0.10. Note that the rule fails to fire in several locations in Figure <ref type="figure" target="#fig_19">16</ref>. According to the domain experts <ref type="bibr">[9]</ref>, most of the non-firing locations correspond to a valve assembly miss-cycled for which the solenoid still experienced a nominal discharge. Apart from these outliers, all normal solenoid trials were detected with this rule. Thus, we can imagine using the negation of the rule firing as an anomaly detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">An Important Sanity Check</head><p>We conducted a sanity check experiment that is very simple, but would nevertheless had demonstrated the problems with the approaches in <ref type="bibr" target="#b5">[5]</ref>[29] (as <ref type="bibr" target="#b12">[12]</ref> also demonstrated, but in a different context). We reran all the experiments above, making a single change, which was to replace the data with random walk data. In no such case does our algorithm find any rules. This finding bolsters our confidence that our scoring function is valid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.6">Time Complexity</head><p>If maxlag is set to zero, then the time complexity for our algorithm is O(nlogn). Allowing a maxlag increases this to O(nlogn×|maxlag|). In essence, the time required by our algorithm is dominated by the speed of motif discovery, which fortunately has received a lot of attention in recent years <ref type="bibr">[20][25]</ref>.</p><p>We do not include explicit timing experiments because in general the time needed for rule discovery is inconsequential. For example, the insect EPG data took several months to collect, so the few minutes our algorithm needed to find rules is not likely to be a burden. Likewise, the Zebra finch data reflects years of painstaking work, so the few minutes our algorithm needs is simply negligible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.7">On Comparisons to Rival Methods</head><p>We compared our algorithm to the two (very different) Piecewise Linear Approximation (PLA) based approaches in <ref type="bibr" target="#b21">[21]</ref> and <ref type="bibr" target="#b29">[29]</ref>, and also the highly cited paper <ref type="bibr" target="#b5">[5]</ref>. To be as fair to them as possible we tested over many combinations of reasonable parameters, using both human-guided and brute force search for the best parameters. For all data sets in Section 7, the best results for all approaches had Q values, measure of quality, at the default rate (consistent with random guessing). Due to space limitations, we push the details of these comparisons to the expanded version of our paper in [31].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">CONCLUSIONS</head><p>We have introduced a technique for finding rules in time series which leverages of recent advances in time series motifs discovery to provide a tractable search. Our novel application of MDL to time series rule discovery allows us to meaningfully rank and compare varied length rules, and rules with different levels of "support". Our rule representation is expressive enough to allow rules with different length antecedents/consequents/lags/firing thresholds, but at the same time does not require extensive human intervention or tweaking. We have also demonstrated our method by comparing it to the three most widely cited rival methods and show how we make much more accurate predictions <ref type="bibr">[31]</ref>.</p><p>There are many avenues for future work. On some datasets, Dynamic Time Warping, in single or multi-dimensional cases, may be more robust than the Euclidean distance, adapting to the concept drift that will be inevitable in some applications <ref type="bibr" target="#b24">[24]</ref>, and for some domains scalability to massive datasets remains an issue. It may be possible to generalize the rule representation to allow more expressive logical connectives, i.e.</p><p>D(Ra, W1) &lt; t1 AND D(Rb, W2) &lt; t2 → Rc However this would require significantly more training data to guard against overfitting. Such flexibility would allow a rule to consider antecedents from two different sources. Finally, unlike time series classification <ref type="bibr" target="#b26">[26]</ref>, there are currently no standard benchmarks for time series rule discovery. We plan to repair this omission, and invite the community to donate challenging datasets for which the ground truth is known, and archiving them [31].</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. The motif pair discovered in the first 2,000 data points (20 seconds) of "The Raven". The shape corresponds to the utterance "...at my chamber door".Using just the first 2,000 data points, which corresponds to the first verse of the poem, we found the pair of non-overlapping subsequences of length 100 (one second length in the original data) that had the minimum distance to each other. Such a pair of subsequences is referred to as a time series motif and extensively studied in the literature<ref type="bibr" target="#b19">[19]</ref><ref type="bibr" target="#b20">[20]</ref>. The occurrence of such a highly conserved motif suggests one possible method for specifying rules. We could simply split the motif pattern in two, let the average of the left side be the antecedent, and let the average of the right side be the consequent. We need to set the maxlag and the threshold, t1, parameters. For the moment, let us set the former to zero and the later to twice the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>antecedent motif prefixes. Figure2shows the rule.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. A rule learned (Figure 1) from the first 2,000 data points of the "The Raven". If the antecedent pattern (left) is matched to a subsequence in a stream that is within Euclidean distance of 7.58 to it, we predict the immediate occurrence of the consequent pattern (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. A hypothesis (green/bold) can be used to score subsequences by subtracting it from them (producing the small integers shown top) and encoding the difference vector with Huffman encoding. Intuitively, here the left sequence requires 57 bits, whereas the right sequence requires 84.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>antecedentLength←</head><label></label><figDesc>Length(R) × sp antecedent ← R (1:antecedentLength) Distances ← Euclidean (antecedent, each subsequence in T) AntecedentDistances ← sort (localMinimums (Distances)) AntecedentCandidates ← Locations (AntecedentDistances) ac ← AntecedentCandidates Return ac</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. bottom) The empirical relationship between Euclidean and bit-save. top) As we search in Euclidean order (the x-axis order) from left to right, the expected value of the bit-save (the mean of the Gaussians) decreases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Distribution of nearest neighbor distances for one second snippets of the audio (in MFCC space) of "The Raven."</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. left) 25 seconds of zebra finch vocalization from the day-40 training data set. The discovered locations (orange/bold) are used to find a rule (right).The rule shown in Figure7.right looks plausible, but does it generalize to the test set? In Figure8we show one rule firing on an excerpt of the test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. The rule learned in Figure 7 fires (bold/orange) on the test set of day 40 (only an excerpt is shown). The Q (cf. Section 7) for the fired rule is 0.33, suggesting high accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. The rule learned in Figure 7 is applied to the same Zebra finch ten days later. The Q for the left and right instances are 0.19 and 0.40 respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. The rule learned in Figure 7 is applied to the singing of the same Zebra finch sixty days later.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. right) One of the top rules learned from the first month of the AMPds data set. left) One firing of the learned rule in the right which contains a 20 minute lag between its antecedent and consequent.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 12 .</head><label>12</label><figDesc>Figure 12. left) A subsequence from a recording of a subject that contains a rule discovered by our algorithm. The discovered locations (orange/bold) are generalized by our algorithm into a rule (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 13 .</head><label>13</label><figDesc>Figure 13. Three instances of the rule shown in Figure 12 discovered in the test set. The Q for the three instances from left to right are 0.22, 0.10 and 0.41 respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 14 .</head><label>14</label><figDesc>Figure 14. bottom) The IMU data of an actor drinking from a cup. top) Stills from a video aligned with the IMU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 15 .</head><label>15</label><figDesc>Figure 15. left) A snippet of the NASA data. right) The first ranked rule learned which characterizes a nominal discharge.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 16 .</head><label>16</label><figDesc>Figure 16. The rule discovered in Figure 15 on the test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 . One-nearest-neighbor leave-one-out accuracy results on UCR datasets for various cardinalities</head><label>1</label><figDesc></figDesc><table><row><cell>Dataset</cell><cell>64-bit (raw) Cardinality: 2 64</cell><cell>16-bit Cardinality: 65536</cell><cell>6-bit Cardinality: 64</cell></row><row><cell>50words</cell><cell>63.1%</cell><cell>63.1%</cell><cell>63.3%</cell></row><row><cell>CBF</cell><cell>85.2%</cell><cell>85.2%</cell><cell>85.2%</cell></row><row><cell>Beef</cell><cell>66.7%</cell><cell>66.7%</cell><cell>66.7%</cell></row><row><cell>ECG</cell><cell>88.0%</cell><cell>88.0%</cell><cell>88.0%</cell></row><row><cell>FaceAll</cell><cell>71.4%</cell><cell>69.6%</cell><cell>69.6%</cell></row><row><cell>Fish</cell><cell>78.3%</cell><cell>78.3%</cell><cell>77.7%</cell></row><row><cell>Lightning2</cell><cell>75.4%</cell><cell>75.4%</cell><cell>77.1%</cell></row><row><cell>OSULeaf</cell><cell>52.1%</cell><cell>52.1%</cell><cell>52.1%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 . Algorithm to score all rules that can be created from a single time series subsequence R, returning the antecedent, consequent and quality of the best rule derived from R Procedure</head><label>2</label><figDesc>Output: The antecedent (a), consequent (c) and quality score (s) of the best rule that can be derived from R;</figDesc><table><row><cell>1 2 3 4 5 6</cell><cell>for i ← 1 to 99 do splitPoint ← i / 100 ruleScore(i) ← Best_Rule_Score (T, R, splitPoint) //Table 3 //Test over all splitting points end for s ← max (ruleScore) sp ← find (ruleScore == s) / 100</cell></row><row><cell>7</cell><cell>a ← R (1 : sp</cell></row><row><cell>8</cell><cell></cell></row><row><cell>9</cell><cell></cell></row></table><note><p>find_Best_Rule (T, R) Input: A time series subsequence, R, extracted from a time series, T;</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 . Algorithm to find the best instances of a rule Procedure</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">Split point for the antecedent/consequent, a number between zero and one, sp;</cell></row><row><cell cols="3">Output: Greatest possible bit-saves by predicting rule R in the time series T, bestTotalBitSave;</cell></row><row><cell>1</cell><cell>ac ← find_Antecedent_Candidates (T, R, sp)</cell><cell>//</cell></row><row><cell>2</cell><cell></cell><cell></cell></row><row><cell>3</cell><cell></cell><cell></cell></row><row><cell>4</cell><cell></cell><cell></cell></row></table><note><p><p>Best_Rule_Score (T, R, sp)</p>Input: A time series subsequence, R, extracted from a time series, T;</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 n</head><label>4</label><figDesc>← find_Best_Number_of_Rule_Instances (T, R, sp, ac) // Table5bestTotalBitSave ← Rule_Bit_Saves (T, R, sp, n, ac) // Table6Return bestTotalBitSave</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 . Algorithm to discover the best number of rule instances to maximize the total number of bit-saves Procedure</head><label>5</label><figDesc>Table 3 we search for locations in T in which that rule would have fired. However the number of firings clearly depends on the distance threshold we have chosen. A conservative (small) threshold is more likely to produce an accurate rule, but may miss opportunities when it could have fired and produce predictions that are at least better than random. We generally have no idea what a suitable threshold could be, fortunately we only have to test |AntecedentCandidates| different values. In particular we just need to test all the values in the sorted list AntecedentDistances, as each new value ensures exactly one additional firing of the rule on our training data, this occurs in Table 5. Output: Best Number of instances of R to pick in the time series, n;</figDesc><table><row><cell>1</cell><cell>totalBitSaves (1) ← 0</cell></row><row><cell>2</cell><cell>instances ← 1</cell></row><row><cell>3</cell><cell>while (totalBitSaves is monotonically increasing) do</cell></row><row><cell>4</cell><cell>instances ← instances +1</cell></row><row><cell>5</cell><cell>totalBitSaves(instances)←Rule_Bit_Saves(T,R,sp,instances,ac)</cell></row><row><cell>6</cell><cell>end while</cell></row><row><cell>7</cell><cell>bestBitSaves ← max (totalBitSaves)</cell></row><row><cell>8</cell><cell>n ← find (totalBitSaves == bestBitSaves)</cell></row><row><cell>9</cell><cell>Return n</cell></row></table><note><p><p>find_Best_Number_of_Rule_Instances (T, R, sp, ac) Input: One instance of a rule, R, extracted from a time series, T;</p>Split point for the antecedent/consequent, between zero and one, sp; Locations of antecedents in T ordered by distances from R's antecedent, ac; (i. e. AntecedentCandidates)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 . Algorithm to score rule instances based on MDL Procedure</head><label>6</label><figDesc></figDesc><table><row><cell cols="2">Output: totalBitSave;</cell></row><row><cell>1</cell><cell>antecedentLength ← Length(R) × sp</cell></row><row><cell>2</cell><cell>consequent ← R (antecedentLength:end)</cell></row><row><cell>3</cell><cell>Discretize and z-normalize (consequent)</cell></row><row><cell>4</cell><cell>AntecedentCandidates ← ac</cell></row><row><cell>5</cell><cell>totalBitSave ← 0</cell></row><row><cell>6</cell><cell>antecedentsSelected ← AntecedentCandidates (2 : n)</cell></row><row><cell>7</cell><cell>for i ← 1 to Length(antecedentsSelected) do</cell></row><row><cell>8 9 10 11</cell><cell>s 1 ← AntecedentCandidates(i) + antecedentLength +1 s 2 ← AntecedentCandidates(i) + Length(R) subConsequent ← T (s 1 : s 2 ) Discretize and z-normalize (subConsequent)</cell></row><row><cell>12</cell><cell>subConsequentBits ← Huffman (subConsequent)</cell></row><row><cell>13</cell><cell>subConsequentMDLbits←MDL(subConsequent, consequent)</cell></row><row><cell>14</cell><cell>totalBitSave ← totalBitSave + subConsequentBits -</cell></row><row><cell>15</cell><cell>subConsequentMDLbits</cell></row><row><cell>16</cell><cell>end for</cell></row><row><cell>17</cell><cell>Return totalBitSave -Huffman (consequent) // Eq. 2</cell></row></table><note><p><p>Rule_Bit_Saves (T, R, sp, n, ac)</p>Input: A time series subsequence, R, extracted from a time series, T; Split point for the antecedent/consequent, between zero and one, sp; The Number of instances of R to pick in the time series, n; locations of antecedents in T ordered by distances from R's antecedent, ac;</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 . Algorithm to discover rules in a time series</head><label>7</label><figDesc></figDesc><table><row><cell cols="3">Procedure Discover_Rules (T, L)</cell></row><row><cell cols="4">Input: A user provided time series, T, and the rule length, L;</cell></row><row><cell cols="3">Output: A set of discovered rules, Rules;</cell></row><row><cell>1</cell><cell>Rules ← [ ]</cell><cell cols="2">// Initialize rules to empty set</cell></row><row><cell>2</cell><cell>K ← 1</cell><cell cols="2">// Initialize which motif to consider</cell></row><row><cell>3 4 5 6 7 8</cell><cell cols="2">while (earlyAbandoning is False) do motif ← motif_Discovery (T, L, K) [a, c, s] ← find_Best_Rule(T, motif) Rules. add([a, c, s]) K ← K + 1 end while</cell><cell>// MK algorithm // Table 2</cell></row><row><cell>9</cell><cell cols="3">Return best(Rules) //returns the rule with the maximum score, s</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 . Algorithm to score rule instances based on MDL (allowing maxlag) Differs from Table 6, only in lines 10 to 14 Procedure</head><label>8</label><figDesc>Split point for the antecedent/consequent, between zero and one, sp; The Number of instances of R to pick in the time series, n; locations of antecedents in T ordered by distances from R's antecedent, ac;</figDesc><table><row><cell>Maxlag allowed between the antecedent and consequent, mlag; Output: totalBitSave; 1 2 3 4 5 6 7 8 9 10 antecedentLength ← Length(R) × sp consequent ← R (antecedentLength:end) Discretize and z-normalize (consequent) AntecedentCandidates ← ac totalBitSave ← 0 antecedentsSelected ← AntecedentCandidates (2 : n) for i ← 1 to Length(antecedentsSelected) do s 1 ← AntecedentCandidates(i) + antecedentLength +1 s 2 ← AntecedentCandidates(i) + Length(R) subConsequent ← T</cell></row><row><cell>11</cell></row><row><cell>12</cell></row><row><cell>13</cell></row><row><cell>14</cell></row><row><cell>15</cell></row><row><cell>16</cell></row><row><cell>17</cell></row><row><cell>18</cell></row><row><cell>19</cell></row><row><cell>20</cell></row><row><cell>21</cell></row></table><note><p><p>Rule_Bit_Saves (T, R, sp, n, ac, mlag)</p>Input: A time series subsequence, R, extracted from a time series, T;</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>(s1 : s1 + mlag) // considering maxlag consequentDist ← Euclidean (consequent, each subsequence o f subConsequent) conseqLoc ← find (consequentDist == min(consequentDist)) subConsequent ← T (s1 + conseqLoc : s2 + conseqLoc)</head><label></label><figDesc></figDesc><table><row><cell>Discretize and z-normalize (subConsequent) subConsequentBits ← Huffman (subConsequent) subConsequentMDLbits←MDL(subConsequent, consequent) totalBitSave ← totalBitSave + subConsequentBits -subConsequentMDLbits end for Return totalBitSave -Huffman (consequent) // Eq. 2</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank all the donors of the datasets. We further wish to acknowledge funding from NSF IIS-1161997 II and a gift from Samsung Research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The minimum description length principle in coding and modeling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rissanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Information Theory</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Auditory feedback in learning and maintenance of vocal behaviour</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Brainard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Doupe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Rev. Neurosci</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="31" to="40" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Tornadoes without NWS warning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Brotzge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Erickson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Weather Forecasting</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="159" to="172" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An Algorithm for Segmenting Categorical Time Series into Meaningful Episodes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICAIDA</title>
		<imprint>
			<biblScope unit="page" from="198" to="207" />
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">G</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mannila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Renganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KDD</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pattern-based timeseries subsequence clustering using radial distribution functions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Besemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Dorr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Querying and mining of time series data: experimental comparison of representations and distance measures</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1542" to="1552" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sample Functions of the Gaussian Process</title>
		<author>
			<persName><forename type="first">R</forename><surname>Dudley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Probability</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="66" to="103" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Ferrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Santuro</surname></persName>
		</author>
		<ptr target="http://cs.fit.edu/~pkc/nasa/data/" />
		<title level="m">NASA shuttle valve data</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Motion Learning and Adaptive Impedance for Robot Control during Physical Interaction with Humans</title>
		<author>
			<persName><forename type="first">E</forename><surname>Gribovskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kheddar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Billard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICRA</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Discovering the Intrinsic Cardinality and Dimensionality of Time Series Using MDL</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2011</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Clustering of time-series subsequences is meaningless: implications for previous and future research</title>
		<author>
			<persName><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="177" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Social performance reveals unexpected vocal competency in young songbirds</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kojima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Doupe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc Natl Acad Sci</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="1687" to="1692" />
			<date type="published" when="2011">2011</date>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Kornai</surname></persName>
		</author>
		<title level="m">Mathematical Linguistics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Advances in Minimum Description Length Theory and Applications</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Grunwald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Myung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient type-ahead search on relational data: a TASTIER approach</title>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD Conference</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="695" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">AMPds: A Public Dataset for Load Disaggregation and Eco-Feedback Research</title>
		<author>
			<persName><forename type="first">S</forename><surname>Makonin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Popowich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bartram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Baijic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Electrical Power and Energy Conference (EPEC)</title>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Forecasting: methods and applications</title>
		<author>
			<persName><forename type="first">S</forename><surname>Makridakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wheelwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Hyndman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>John Wiley &amp; Sons</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Identifying Predictive Multi-Dimensional Time Series Motifs: An application to severe weather prediction</title>
		<author>
			<persName><surname>Mcgovern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Mueen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Westover</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Exact Discovery of Time Series Motif. SDM</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Discovering and Matching Elastic Rules from Sequence Databases</title>
		<author>
			<persName><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fundam. Inform</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Coarse In-building Localization with Smartphones</title>
		<author>
			<persName><forename type="first">A</forename><surname>Parnandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vaghela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dantu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Poduri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sukhatme</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The Opportunity challenge: A benchmark database for on-body sensor-based activity recognition</title>
		<author>
			<persName><forename type="first">Ricardo</forename><forename type="middle">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">On the Non-Trivial Generalization of Dynamic Time Warping to the Multi-Dimensional Case</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shokoohi-Yekta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Discovery of timeseries motif from multi-dimensional data based on MDL principle</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Iwamoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Uehara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<ptr target="http://www.cs.ucr.edu/~eamonn/time_series_data/" />
		<title level="m">UCR Time Series, Classification and Clustering</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Minimum message length and Kolmogorov complexity</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Journal</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Predictive Rule Discovery from Electronic Health Records</title>
		<author>
			<persName><forename type="first">S</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Indurkhya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Apte</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>ACM IHI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Online Event-driven Subsequence Matching over Financial Data Streams</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Salzberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD Conference</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="23" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName><forename type="middle">H</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note>Personal email communication</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
