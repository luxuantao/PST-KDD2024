<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Detecting Texts of Arbitrary Orientations in Natural Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Cong</forename><surname>Yao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiang</forename><surname>Bai</surname></persName>
							<email>xbai@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yi</forename><surname>Ma</surname></persName>
							<email>mayi@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research Asia</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Lab of Neuro Imaging</orgName>
								<orgName type="institution">UCLA</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Detecting Texts of Arbitrary Orientations in Natural Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9686FEE428980826412ABFB08603413A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T04:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the increasing popularity of practical vision systems and smart phones, text detection in natural scenes becomes a critical yet challenging task. Most existing methods have focused on detecting horizontal or near-horizontal texts. In this paper, we propose a system which detects texts of arbitrary orientations in natural images. Our algorithm is equipped with a two-level classification scheme and two sets of features specially designed for capturing both the intrinsic characteristics of texts. To better evaluate our algorithm and compare it with other competing algorithms, we generate a new dataset, which includes various texts in diverse real-world scenarios; we also propose a protocol for performance evaluation. Experiments on benchmark datasets and the proposed dataset demonstrate that our algorithm compares favorably with the state-of-the-art algorithms when handling horizontal texts and achieves significantly enhanced performance on texts of arbitrary orientations in complex natural scenes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The great success of smart phones and large demands in content-based image search/understanding have made text detection a crucial task in human computer interaction. It is desirable to build practical systems that are robust and fast enough to deal with natural scenes of various conditions; as shown in Fig. <ref type="figure" target="#fig_0">1</ref>, we want to detect texts of large variations in language, font, color, scale and orientation in complex scenes. Although text detection has been studied extensively in the past <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b14">15]</ref>, the problem remains unsolved. The difficulties mainly come from two aspects: <ref type="bibr" target="#b0">(1)</ref> the diversity of the texts and (2) the complexity of the backgrounds. On one hand, text is a high level concept but better defined than the generic objects <ref type="bibr" target="#b7">[8]</ref>; on the other hand, repeated patterns (such as windows and barriers) and random clutters (such as grasses and leaves) may be similar to texts, and thus lead to potential false positives.</p><p>As our survey of related work shows below, most ex- isting methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b21">22]</ref> have focused on detecting horizontal or near-horizontal texts. Detecting texts of arbitrary orientations in complex natural images has received much less attentions and remains a challenge for most practical systems. In this work, we make an effort to build an effective and practical detection system for texts of arbitrary orientations in complex natural scenes. When directly applied to detect texts of arbitrary orientations, conventional features (such as SWT used in <ref type="bibr" target="#b6">[7]</ref>) that are primarily designed for horizontal texts would lead to significant false positives. In this paper, we introduce two additional sets of rotation invariant features for text detection. To further reduce false positives produced by these low-level features, we have also designed a two-level classification scheme that can effectively discriminate texts from non-texts. Hence, combining the strengths of specially designed features and discriminatively trained classifiers, our system is able to effectively detect texts of arbitrary orientations but produce fewer false positives.</p><p>To evaluate the effectiveness of our system, we have conducted extensive experiments on both conventional and new image datasets. Compared with the state-of-the-art text detection algorithms, our system performs competitively in the conventional setting of horizontal texts. We have also tested our system on a very challenging large dataset of 500 natural images containing texts of various orientations in complex backgrounds (see Fig. <ref type="figure" target="#fig_10">8 (a)</ref>). On this dataset, our system works significantly better than any of the existing systems, with an F-measure about 0.6, more than twice that of the closest competitor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>There have been a large number of methods dealing with text detection in natural images and videos <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26]</ref>. Comprehensive surveys can be found in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b18">19]</ref>.</p><p>Existing approaches to text detection can be roughly divided into three categories: texture-based methods, regionbased methods, and hybrid methods. Texture-based methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b9">10]</ref> treat texts as a special type of texture and make use of their properties, such as local intensities, filter responses and wavelet coefficients. These methods are computation demanding as all locations and scales are exhaustively scanned. Moreover, these algorithms mostly detect horizontal texts. Region-based methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b21">22]</ref> first extract candidate text regions through edge detection or clustering and then eliminate non-text regions using various heuristic rules. The third category, hybrid methods <ref type="bibr" target="#b22">[23]</ref>, is a mixture of texture-based and region-based methods.</p><p>Most existing algorithms, e.g. <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b6">7]</ref>, have focused on detecting horizontal texts. In this paper, we address the problem of detecting texts of large variations in natural images, which has great practical importance but has not been well studied. In <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b25">26]</ref>, methods that can detect text strings of arbitrary directions are proposed but they have a large set of rules and parameters; how general and applicable they are in dealing with scenes of large variation is unclear.</p><p>We observe two-sides aspects about the current text detection algorithms: (1) methods built on heavy learning (nearly black-box) <ref type="bibr" target="#b5">[6]</ref> by training classifiers on a large amount of data reach certain but limited level of success (system <ref type="bibr" target="#b5">[6]</ref> obtained from the authors produces reasonable results on horizontal English texts but has poor performances in the general cases); (2) systems based on smart features, such as Stroke Width Transform (SWT) <ref type="bibr" target="#b6">[7]</ref>, are robust to variations in texts but they involve many hand tunings and are still far from producing all satisfactory results, especially for non-horizontal texts.</p><p>In this paper, we adopt SWT and also design various features that are intrinsic to texts and robust to variations; a two-level classification scheme is devised to moderately utilize training to remove sensitive manual parameter tuning. We observe significant improvement over the existing approaches in dealing with real-world scenes.</p><p>Though widely used in the community, the ICDAR datasets <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b23">24]</ref> only contain horizontal English texts. In <ref type="bibr" target="#b28">[29]</ref>, a dataset with texts of different directions is released, but it includes only 89 images without enough diversity in the texts and backgrounds. Here we collect a new dataset with 500 images of indoor and outdoor scenes. In addition, the evaluation methods used in <ref type="bibr" target="#b12">[13]</ref> and the IC-DAR competitions <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b19">20]</ref> are designed for horizontal texts only. Hence, we use a different protocol that is suitable to handle texts of arbitrary orientations (see Sec. 4). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In this section, we present the details of the proposed algorithm. Specifically, the pipeline of the algorithm will be presented in Sec. 3.1 and the details of the features will be described in Sec. 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Algorithm Pipeline</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">overview</head><p>The proposed algorithm consists of four stages: (1) component extraction, (2) component analysis, (3) candidate linking, and (4) chain analysis, which can be further categorized into two procedures, bottom-up grouping and topdown pruning, as shown in Fig. <ref type="figure" target="#fig_1">2</ref>. In the bottom-up grouping procedure, pixels first form connected components and later these connected components are aggregated to form chains; in the top-down pruning procedure non-text components and chains are successively identified and eliminated. Component extraction: At this stage, edge detection is performed on the original image and the edge map is input to SWT <ref type="bibr" target="#b6">[7]</ref> module to produce an SWT image. Neighboring pixels in the SWT image are grouped together to form connected components using a simple association rule. Component analysis: Many components extracted at the component extraction stage are not parts of texts. The component analysis stage therefore identifies and filters out those non-text components by a trained classifier. Candidate linking: The remaining components are taken as character candidates <ref type="foot" target="#foot_0">1</ref> . The first step of the candidate linking stage is to link the character candidates into pairs. Two adjacent candidates are grouped into a pair if they have similar geometric properties and colors. The candidate pairs are then aggregated into chains in a recursive fashion. Chain analysis: At the chain analysis stage, the chains determined at the former stage are verified by a chain level Table <ref type="table">1</ref>. Basic component properties and their valid ranges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Property</head><p>Definition Range</p><formula xml:id="formula_0">width variation W V (c) = σ(c) µ(c) [0, 1] aspect ratio AR(c) = min{ w(c) h(c) , h(c) w(c) } [0.1, 1] occupation ratio OR(c) = q w(c) * h(c) [0.1, 1]</formula><p>classifier. The chains with low classification scores (probabilities) are discarded. The chains may be in any direction, so a candidate might belong to multiple chains; the interpretation step is aimed to dispel this ambiguity. The chains that pass this stage are the final detected texts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Component Extraction</head><p>To extract connected components from the image, SWT <ref type="bibr" target="#b6">[7]</ref> is adopted for its effectiveness and efficiency. In addition, it provides a way to discover connected components from edge map directly. We use Canny edge detector <ref type="bibr" target="#b4">[5]</ref> to produce an edge map (Fig. <ref type="figure" target="#fig_4">3 (b)</ref>) from the original image (Fig. <ref type="figure" target="#fig_4">3 (a)</ref>). SWT is a local image operator which computes per pixel width of the most likely stroke containing the pixel. See <ref type="bibr" target="#b6">[7]</ref> for details. The resulting SWT image is shown in Fig. <ref type="figure" target="#fig_4">3 (c</ref>). The next step of this stage is to group the pixels in the SWT image into connected components. The pixels are associated using a simple rule that the ratio of SWT values of neighboring pixels is less than 3.0. The connected components are shown in Fig. <ref type="figure" target="#fig_4">3 (d)</ref>. Note the red rectangles in the image. Each rectangle contains a connected component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Component Analysis</head><p>The purpose of component analysis is to identify and eliminate the connected components that are unlikely parts of texts. Towards this end, we devise a two-layer filtering mechanism. The first layer is a filter consists of a set of heuristic rules. This filter runs on a collection of statistical and geometric properties of components, which are very fast to compute. For a connected component c with q foreground pixels (black pixels in the SWT image), we first compute its bounding box bb(c) (its width and height are denoted by w(c) and h(c), respectively) and the mean as well as standard deviation of the stroke widths, µ(c) and σ(c). The definitions of these basic properties and the corresponding valid ranges are summarized in Tab. 1.</p><p>The components with one or more invalid properties will be taken as non-text regions and discarded. This preliminary filter proves to be both effective and efficient. A large portion of obvious non-text regions are eliminated after this step. Notice the difference between Fig. <ref type="figure" target="#fig_4">3 (d</ref>) and Fig. <ref type="figure" target="#fig_4">3 (e)</ref>.</p><p>The second layer is a classifier trained to identify and reject the non-text components that are hard to remove with the preliminary filter. A collection of component level features, which capture the differences of geometric and textural properties between text components and non-text com- ponents, are used to train this classifier. The criteria for feature design are: scale invariance, rotation invariance and low computational cost. To meet these criteria, we propose to estimate the center, characteristic scale and major orientation of each component (Fig. <ref type="figure" target="#fig_2">4</ref>) before computing the component level features. Based on these characteristics, features that are both effective and computational efficient can be obtained. The details of these component level features are discussed in Sec. These characteristics are invariant to translation, scale and rotation to some degree (Fig. <ref type="figure" target="#fig_2">4</ref>). As we will explain in Sec. 3.2.1, this is the key to the scale and rotation invariance of the component level features.</p><p>We train a component level classifier using the component level features. Random Forest <ref type="bibr" target="#b3">[4]</ref> is chosen as the strong classifier. The component level classifier is the first level of the two-level classification scheme. The probability of component c, p 1 (c), is the fraction of votes for the positive class (text) from the trees. The components whose probabilities are lower than a threshold T 1 are eliminated and the remaining components are considered as character candidates (Fig. <ref type="figure" target="#fig_4">3</ref> (f)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Candidate Linking</head><p>The character candidates are aggregated into chains at this stage. This stage also serves as a filtering step because the candidate characters cannot be linked into chains are taken as components casually formed by noises or background clutters, and thus are discarded.</p><p>Firstly, character candidates are linked into pairs. In <ref type="bibr" target="#b6">[7]</ref>, whether two candidates can be linked into a pair is determined based on the heights and widths of their bounding boxes. However, bounding boxes of candidates are not rotation invariant, so we use their characteristic scales instead. If two candidates have similar stroke widths (ratio between the mean stroke widths is less than 2.0), similar sizes (ratio between their characteristic scales does not exceed 2.5), similar colors and are close enough (distance between them is less than two times the sum of their characteristic scales), they are labeled as a pair. Unlike <ref type="bibr" target="#b6">[7]</ref>, which only considers horizontal linkings, the proposed algorithm allows linkings of arbitrary directions. This endows the system with the ability of detecting texts of arbitrary orientations, not limited to horizontal texts (see Fig. <ref type="figure" target="#fig_0">1</ref>). Note that a character candidate may belong to several pairs. Next, a greedy hierarchical agglomerative clustering <ref type="bibr" target="#b11">[12]</ref> method is applied to aggregate the pairs into candidate chains. Initially, each pair constitutes a chain. Then the similarity between each couple of chains that share at least one common candidate and have similar orientations is computed; chains with the highest similarity are merged together to form a new chain. The orientation consistency s o (C 1 , C 2 ) and population consistency s p (C 1 , C 2 ) between two chains C 1 and C 2 , which share at least one common candidate, are defined as:</p><formula xml:id="formula_1">s o (C 1 , C 2 ) = 1 - γ(C 1 , C 2 ) π/2 if γ(C 1 , C 2 ) ≤ π 8 0 otherwise ,<label>(1)</label></formula><p>and</p><formula xml:id="formula_2">s p (C 1 , C 2 ) = 1 - |n C 1 -n C 2 | |n C 1 + n C 2 | if γ(C 1 , C 2 ) ≤ π 8 0 otherwise ,<label>(2)</label></formula><p>where γ(C 1 , C 2 ) is the included angle of C 1 and C 2 while n C1 and n C2 are the candidate numbers of C 1 and C 2 . The similarity between two chains C 1 and C 2 is:</p><formula xml:id="formula_3">s(C 1 , C 2 ) = ω • s o (C 1 , C 2 ) + (1 -ω) • s p (C 1 , C 2 ),<label>(3)</label></formula><p>where ω ∈ [0, 1] is a control parameter. ω is set to 0.5 to give equal weights to s o (C 1 , C 2 ) and s p (C 1 , C 2 ). According to this similarity definition, the chains with proximal sizes and orientations are merged with priority. This merging process proceeds until no chains can be merged.</p><p>At last, the character candidates not belonging to any chain are discarded. The candidate chains after aggregation are shown in Fig. <ref type="figure" target="#fig_4">3 (g</ref>). Each green line represents a candidate chain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.5">Chain Analysis</head><p>The candidate chains formed at the previous stage might include false positives that are random combinations of scattered background clutters (such as leaves and grasses) and repeated patterns (such as bricks and windows). To eliminate these false positives, a chain level classifier is trained using the chain level features (Sec. 3.2.2). Random Forest <ref type="bibr" target="#b3">[4]</ref> is again used. The chain level classifier is the second level of the two-level classification scheme. The probability of chain C, p 2 (C), is the fraction of votes for the positive class (text) from the trees. The chains with probabilities lower than a threshold T 2 are eliminated.</p><p>To make better decisions, the total probability of each chain is also calculated. For a chain C with n candidates c i , i = 1, 2, • • • , n, the total probability is defined as:</p><formula xml:id="formula_4">p(C) = ( n i=1 p 1 (c i ) n + p 2 (C))/2.</formula><p>The chains whose total probabilities are lower than a threshold T are discarded.</p><p>As texts of arbitrary orientations are considered, the remaining chains may be in any direction. Therefore, a candidate might belong to multiple chains. For example, in Fig. <ref type="figure" target="#fig_4">3</ref> (h) the character 'P' in the first line is linked in three chains (note the green lines). In reality, however, a character is unlikely to belong to multiple text lines. If several chains compete for the same candidate, only the chain with the highest total probability will survive (note the difference between Fig. <ref type="figure" target="#fig_4">3</ref> (h) and Fig. <ref type="figure" target="#fig_4">3 (i)</ref>).</p><p>The survived chains are outputted by the system as detected texts (Fig. <ref type="figure" target="#fig_4">3 (j)</ref>). For each detected text, its orientation is calculated through linear least squares <ref type="bibr" target="#b11">[12]</ref> using the centers of the characters; its minimum area rectangle <ref type="bibr" target="#b8">[9]</ref> is estimated using the orientation and the bounding boxes of the characters. Word partition, which divides text lines into separate words, is also implemented in the proposed algorithm; but it is not shown in Fig. <ref type="figure" target="#fig_4">3</ref> since the general task of text detection does not require this step.</p><p>The whole algorithm is performed twice to handle both bright text on dark background and dark text on bright background, once along the gradient direction and once along the inverse direction. The results of two passes are fused to make final decisions. For clarity, only the results of one pass are presented in Fig. <ref type="figure" target="#fig_4">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Feature Design</head><p>We design two collections of features, component level features and chain level features, for classifying text and non-text, based on the observation that it is the median degree of regularities of text rather than particular color or shape that distinguish it from non-text, which usually has either low degree (random clutters) or high degree (repeated patterns) of regularities. At character level, the regularities of text come from nearly constant width and texturelessness of strokes, and piecewise smoothness of stroke boundaries; at line level, the regularities of text are similar colors, sizes, orientations and structures of characters, and nearly constant spacing between consecutive characters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Component Level Features</head><p>Inspired by Shape Context <ref type="bibr" target="#b0">[1]</ref> and Feature Context <ref type="bibr" target="#b27">[28]</ref>, we devise two templates (Fig. <ref type="figure" target="#fig_5">5 (a)</ref>) to capture the regularities of each component in coarse and fine granularity, respectively. The radius and orientation of the templates are not stationary, but adaptive to the component. When computing descriptors for a component, each template is placed at the center and rotated to align with the major orientation of the component; the radius is set to the characteristic scale of the component. Different cues from the sectors are encoded and concatenated into histograms. In this paper, the following cues are considered for each sector:</p><p>-Contour shape <ref type="bibr" target="#b10">[11]</ref>. Contour shape is a histogram of oriented gradients. The gradients are computed on the component contour (Fig. <ref type="figure" target="#fig_5">5 (c)</ref>).</p><p>-Edge shape <ref type="bibr" target="#b10">[11]</ref>. Edge shape is also a histogram of oriented gradients; but the gradients are computed at all the pixels in the sector (Fig. <ref type="figure" target="#fig_5">5 (d)</ref>).</p><p>-Occupation ratio. Occupation ratio is defined as the ratio between the number of the foreground pixels of the component within the sector and the sector area (Fig. <ref type="figure" target="#fig_5">5 (e)</ref>).</p><p>To achieve rotation invariance, the gradient orientations are rotated by an angle Θ(c), before computing contour shape and edge shape. Then, the gradient orientations are normalized to the range [0, π]. 6 orientation bins are used for computing histograms of contour shape and edge shape, to cope with different fonts and local deformations. For each cue, the signals computed in all the sectors of all the templates are concatenated to form a descriptor. We call these descriptors scalable rotative descriptors, because they are computed based on templates that are scalable and rotative. Scalable rotative descriptors are similar to PHOG <ref type="bibr" target="#b1">[2]</ref>, as they both adopt spatial pyramid representation <ref type="bibr" target="#b16">[17]</ref>.</p><p>Different from the templates used for computing PHOG, our templates are circular and their scale and orientation are adaptive to the component being described. This is the key to the scale and rotation invariance of these descriptors. We found through experiments (not shown in this paper) that using finer templates can slightly improve the performance, but will largely increase the computational burden.</p><p>Another three types of features are also considered: -Axial ratio. Axial ratio is computed by dividing the major axis of the component c with its minor axis: XR(c) = L(c)/l(c).</p><p>-Width variation. This feature is the same as defined in Tab. 1.</p><p>-Density. The density of component c is defined as the ratio between its pixel number q and characteristic area (here the characteristic area is π • S 2 (c), not the area of the bounding box): D(c) = q/(π • S 2 (c)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Chain Level Features</head><p>Eleven types of chain level features are designed to discriminate text lines from false positives (mostly repeated patterns and random clutters) that cannot be distinguished by the component level features.</p><p>For a candidate chain C with n (n ≥ 2) candidates c i , i = 1, 2, . . . , n, the features are defined as below:</p><p>-Candidate count. This feature is adopted based on the observation that false positives usually have very few (random clutters) or too many (repeated patterns) candidates.</p><p>-Average probability. The probabilities given by the component level classifier are reliable. This feature is the average of all the probabilities (p 1 (c i ), i = 1, 2, . . . , n) of the candidates belonging to C.</p><p>-Average turning angle. Most texts present in linear form, so for a text line the mean of the turning angles at the interior characters (τ (c i ), i = 2, 3, . . . , n -1) is very small; however, for random clutters this property will not hold. τ (c i ) is the included angle between the line O(c i-1 )O(c i ) and O(c i )O(c i+1 ).</p><p>-Size variation. In most cases characters in a text line have approximately equal sizes; but it's not that case for random clutters. The size of each component is measured by its characteristic scale S(c i ).</p><p>-Distance variation. Another property of text is that characters in a text line are distributed uniformly, i.e. the distances between consecutive characters have small deviation. The distance between two consecutive components is the distance of their centers O(c i-1 ) and O(c i ).</p><p>-Average direction bias. For most text lines, the major orientations of the characters are nearly perpendicular to the major orientation of the text line.</p><p>-Average axial ratio. Some repeated patterns (e.g. barriers) that are not texts consist of long and thin components, this feature can help differentiate them from true texts.</p><p>-Average density. On the contrary, other repeated patterns (e.g. bricks) consist of short and fat components, this feature can be used to eliminate this kind of false positives.</p><p>-Average width variation. False positives formed by foliage usually have varying widths while texts have constant widths. This feature is defined as the mean of all the width variation values of the candidates.</p><p>-Average color self-similarity. Characters in a text line usually have similar but not identical color distributions with each other; yet in false positive chains, color self-similarities <ref type="bibr" target="#b24">[25]</ref> of the candidates are either too high (repeated patterns) or too low (random clutters). The color similarity cs(x, y) is defined as the cosine similarity of the color histograms of the two candidates x and y.</p><p>-Average structure self-similarity. Likewise, characters in a text line have similar structure with each other while false positives usually have almost the same structure (repeated patterns) or diverse structures (random clutters). The structure similarity ss(x, y) is defined as the cosine similarity of the edge shape descriptors of the two components x and y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Dataset and Evaluation Protocol</head><p>In this section, we introduce a dataset for evaluating text detection algorithms, which contains images of real-world complexity; a new evaluation method is also proposed.</p><p>Although widely used in the community, the ICDAR dataset <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b19">20]</ref> has two major drawbacks. First, most of the text lines (or single characters) in the ICDAR dataset are horizontal. In real scenarios, however, text may appear in any orientation. The second drawback is that all the text lines or characters in this dataset are in English. These two shortcomings are also pointed out in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b28">29]</ref>. In this work, we generate a new multilingual image dataset with horizontal as well as skewed and slant texts. We name this dataset MSRA Text Detection 500 Database (MSRA-TD500) <ref type="foot" target="#foot_1">2</ref> , because it contains 500 natural images in total. These images are taken from indoor (office and mall) and outdoor (street) scenes using a packet camera. The indoor images are mainly signs, doorplates and caution plates while the outdoor images are mostly guide boards and billboards in complex background. The resolutions of the images vary from 1296 × 864 to 1920 × 1280. Some typical images from this dataset are shown in Fig. <ref type="figure" target="#fig_10">8 (a)</ref>.</p><p>This dataset is very challenging because of both the diversity of the texts and the complexity of the backgrounds in the images. The texts may be in different languages (Chinese, English or mixture of both), fonts, sizes, colors and orientations. The backgrounds may contain vegetation (e.g. trees and grasses) and repeated patterns (e.g. windows and bricks), which are not so distinguishable from text.</p><p>The dataset is divided into two parts: training set and test set. The training set contains 300 images randomly selected from the original dataset and the rest 200 images constitute the test set. All the images in this dataset are fully annotated. The basic unit in this dataset is text line rather than word, which is used in the ICDAR dataset, because it is hard to partition Chinese text lines into individual words based on their spacings; even for English text lines, it is non-trivial to perform word partition without high level information. The procedure of ground truth generation is shown in Fig. <ref type="figure" target="#fig_6">6</ref> (a) and (b).</p><p>Minimum area rectangles <ref type="bibr" target="#b8">[9]</ref> are used in our protocol because they (green rectangles in Fig. <ref type="figure" target="#fig_6">6 (b)</ref>) are much tighter than axis-aligned rectangles (red rectangles in Fig. <ref type="figure" target="#fig_6">6 (b)</ref>). However, a problem imposed by using minimum area rectangles is that it is difficult to judge whether a text line is correctly detected. As shown in Fig. <ref type="figure" target="#fig_6">6</ref> (c), it is not trivial to directly compute the overlap ratio between the estimated rectangle D and the ground truth rectangle G. Instead, we compute the overlap ratio using axis-aligned rectangles G   In order to compare the proposed algorithm with existing methods, we evaluated the algorithm on the standard benchmark ICDAR dataset <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b19">20]</ref>. The ICDAR dataset contains 509 fully annotated text images. 258 images from the dataset are used for training and 251 for testing. Some text detection examples of the proposed algorithm are presented in Fig. <ref type="figure" target="#fig_8">7</ref>. The algorithm can handle several types of challenging scenarios, e.g. variations in text font, color and size, as well as repeated patterns and background clutters. The quantitative comparisons of different methods evaluated on the ICDAR test set are shown in Tab. 2. Our algorithm compares favorably with the state-of-the-art algorithms when dealing with horizontal texts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>Besides the ICDAR dataset, we also tested the proposed algorithm and the systems of Chen et al. <ref type="bibr" target="#b5">[6]</ref> and Epshtein et al. <ref type="bibr" target="#b6">[7]</ref> on the proposed dataset. Examples of our algorithm on this dataset are shown in Fig. <ref type="figure" target="#fig_10">8</ref> (a). Our algorithm is able to detect texts of large variation in natural scenes,  with the presence of vegetation and buildings. The images in the last row of Fig. <ref type="figure" target="#fig_10">8</ref> (a) are some typical cases where our algorithms failed to detect the texts or gave false positives. The misses (pink rectangles) are mainly due to strong highlights, blur and low resolution; the false positives (red rectangles) are usually caused by windows, trees, or signs that are very alike text. The performances are measured using the proposed evaluation protocol and shown in Tab. 3. Our algorithm achieves significantly enhanced performance when detecting texts of arbitrary orientations. The performances of other competing algorithms are not presented because of unavailability of their executables. The average processing time of our algorithm on this dataset is 7.2s and that of Epshtein et al. is 6s (both tested on a 2.53GHz CPU without optimization). Our algorithm is a bit slower, but with the advantage of being able to detect multi-oriented texts.</p><p>In <ref type="bibr" target="#b28">[29]</ref>, a dataset called Oriented Scene Text Database (OSTD), which contains texts of various orientations, is released. This dataset contains 89 images of logos, indoor scenes and street views. We perform text detection on all the images in this dataset. The quantitative results are presented in Tab. 4. Our method outperforms <ref type="bibr" target="#b28">[29]</ref> on the Oriented Scene Text Database (OSTD), with an improvement of 0.19 in F-measure. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions and Future Work</head><p>We have presented a text detection system that detects texts of arbitrary directions in natural images. Our system compares favorably with the state-of-the-art algorithms when handling horizontal texts and achieves significantly enhanced performance on texts of arbitrary orientations in complex natural scenes.</p><p>The component level features are actually character descriptors that can distinguish among different characters, thus they can be adopted to recognize characters. We plan to make use of this property and develop an unified framework for text detection and character recognition in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Detected texts in natural images.</figDesc><graphic coords="1,308.91,241.78,236.01,88.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Pipeline of the proposed approach.</figDesc><graphic coords="2,332.59,72.05,188.86,164.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Component characteristics. The green points are the centers of the components. The radii of the pink circles represent their characteristic scales while the yellow lines indicate the major orientations. The two images, which contain the same text line, are taken from different viewpoints and distances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>3.2.1. For a component c, the barycenter o(c), major axis L(c), minor axis l(c), and orientation θ(c) are estimated using Camshift algorithm [3] by taking the SWT image of component c as distribution map. The center, characteristic scale and major orientation of component c are defined as: O(c) = o(c), S(c) = L(c) + l(c), and Θ(c) = θ(c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Text detection process. See text for details.</figDesc><graphic coords="4,99.65,72.24,395.80,171.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Templates and calculation of scalable rotative descriptors. (a) Two templates used for computing the descriptors. The radius space and angle space are partitioned evenly in a coarse-to-fine manner. The red arrows indicate the reference orientations of the templates. (b) Component and its characteristics. (c)(d)(e) Calculation of contour shape, edge shape and occupation ratio. See text for details.</figDesc><graphic coords="5,73.72,71.96,188.89,97.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Ground truth generation and overlap ratio calculation. (a) Human annotations. The annotators are required to bound each text line using a four-vertex polygon (red dots and yellow lines). (b) Ground truth rectangles (green). The ground truth rectangle is generated automatically by fitting a minimum area rectangle using the polygon. (c) Calculation of overlap ratio between detection rectangle and ground truth rectangle.</figDesc><graphic coords="6,319.48,72.12,212.38,67.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>′ and D ′ , which are obtained by rotating G and D round their centers C G and C D , respectively. The overlap ratio between G and D is defined as: m(G, D) = A(G ′ ∩ D ′ ) A(G ′ ∪ D ′ ) where A(G ′ ∩ D ′ ) and A(G ′ ∪ D ′ ) denote the areas of the intersection and union of G ′ and D ′ . Similar to the evalu-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Detected texts in images from the ICDAR test set. ation method for the PASCAL object detection task [8], in our protocol detections are considered true or false positives based on the overlap ratio between the estimated minimum area rectangles and the ground truth rectangles. If the included angle of the estimated rectangle and the ground truth rectangle is less than π/8 and their overlap ratio exceeds 0.5, the estimated rectangle is considered a correct detection. Multiple detections of the same text line are taken as false positives. The definitions of precision and recall are: precision = |T P |/|E|, recall = |T P |/|T | where T P is the set of true positive detections while E and T are the sets of estimated rectangles and ground truth rectangles. The F-measure, which is a single measure of algorithm performance, is a combination of the two above measures: f = 2 • precision • recall/(precision + recall).</figDesc><graphic coords="7,97.36,69.25,141.54,87.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>We implemented the proposed algorithm and trained two text detectors, one on the mixture of the ICDAR training set and the training set of the proposed dataset, and the other only on the ICDAR training set. These two text detectors are denoted by TD-Mixture and TD-ICDAR, respectively. 200 trees are used for training the component level classifier and 100 trees for the chain level classifier. The threshold values are: T 1 = 0.1, T 2 = 0.3 and T = 0.4. We found empirically that the text detectors under this parameter setting work well for all the datasets used in this paper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. (a) Detected texts in images from the proposed dataset. Yellow rectangles: true positives, pink rectangles: false negatives, red rectangles: false positives. Best viewed in color. (b) Detected texts in various languages in images collected from the internet. Note that the texts are detected in full images. We only show cropped sub images because of space limitation. From Tab. 3 and Tab. 4, we observe that even TD-ICDAR (only trained on horizontal texts) achieves much better performance than other methods on non-horizontal texts. It demonstrates the effectiveness of the proposed features. Fig. 8 (b) shows some detected texts in various languages, including both oriental and western languages, such as Japanese, Korean, Arabic, Greek, and Russian. Though our text detector is only trained on Chinese and English texts, it can effortlessly generalize to texts in different languages. It indicates that the proposed algorithm is quite general and it can serve as a multilingual text detector if sufficient training examples are available.</figDesc><graphic coords="8,73.61,72.20,445.37,116.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>Performances of different text detection methods evaluated on the ICDAR test set.</figDesc><table><row><cell>Algorithm</cell><cell>Precision</cell><cell>Recall</cell><cell>F-measure</cell></row><row><cell>TD-Mixture</cell><cell>0.69</cell><cell>0.66</cell><cell>0.67</cell></row><row><cell>TD-ICDAR</cell><cell>0.68</cell><cell>0.66</cell><cell>0.66</cell></row><row><cell>Epshtein et al. [7]</cell><cell>0.73</cell><cell>0.60</cell><cell>0.66</cell></row><row><cell>Yi et al. [29]</cell><cell>0.71</cell><cell>0.62</cell><cell>0.62</cell></row><row><cell>Becker et al. [20]</cell><cell>0.62</cell><cell>0.67</cell><cell>0.62</cell></row><row><cell>Chen et al. [6]</cell><cell>0.60</cell><cell>0.60</cell><cell>0.58</cell></row><row><cell>Zhu et al. [20]</cell><cell>0.33</cell><cell>0.40</cell><cell>0.33</cell></row><row><cell>Kim et al. [20]</cell><cell>0.22</cell><cell>0.28</cell><cell>0.22</cell></row><row><cell>Ezaki et al. [20]</cell><cell>0.18</cell><cell>0.36</cell><cell>0.22</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Performances of different text detection methods evaluated on the proposed dataset.</figDesc><table><row><cell>Algorithm</cell><cell>Precision</cell><cell>Recall</cell><cell>F-measure</cell></row><row><cell>TD-Mixture</cell><cell>0.63</cell><cell>0.63</cell><cell>0.60</cell></row><row><cell>TD-ICDAR</cell><cell>0.53</cell><cell>0.52</cell><cell>0.50</cell></row><row><cell>Epshtein et al. [7]</cell><cell>0.25</cell><cell>0.25</cell><cell>0.25</cell></row><row><cell>Chen et al. [6]</cell><cell>0.05</cell><cell>0.05</cell><cell>0.05</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Performances of different text detection methods evaluated onthe Oriented Scene Text Database (OSTD)<ref type="bibr" target="#b28">[29]</ref>.</figDesc><table><row><cell>Algorithm</cell><cell>Precision</cell><cell>Recall</cell><cell>F-measure</cell></row><row><cell>TD-Mixture</cell><cell>0.77</cell><cell>0.73</cell><cell>0.74</cell></row><row><cell>TD-ICDAR</cell><cell>0.71</cell><cell>0.69</cell><cell>0.68</cell></row><row><cell>Yi et al. [29]</cell><cell>0.56</cell><cell>0.64</cell><cell>0.55</cell></row><row><cell>Epshtein et al. [7]</cell><cell>0.37</cell><cell>0.32</cell><cell>0.32</cell></row><row><cell>Chen et al. [6]</cell><cell>0.07</cell><cell>0.06</cell><cell>0.06</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>In fact, components do not necessarily correspond to characters, because a single character in some languages may consist of several strokes; however, we still call them characters (or character candidates) hereafter for simplicity.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>http : //users.loni.ucla.edu/∼ztu/Download f ront.htm</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work was supported by National Natural Science Foundation of China (grant No. 61173120 and 60903096), Office of Naval Research Award N000140910099 and NSF CAREER award IIS-0844566.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Shape matching and object recognition using shape contexts</title>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Puzicha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>IEEE Trans. PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Representing shape with a spatial pyramid kernel</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bosch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Munoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CIVR</title>
		<meeting>CIVR</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Real time face and object tracking as a component of a perceptual user interface</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Bradski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Workshop on Applications of Computer Vision</title>
		<meeting>IEEE Workshop on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<title level="m">Random forests. Machine Learning</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A computational approach to edge detection</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Trans. PAMI</title>
		<imprint>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Detecting and reading text in natural scenes</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Detecting text in natural scenes with stroke width transform</title>
		<author>
			<persName><forename type="first">B</forename><surname>Epshtein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ofek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wexler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Determining the minimum-area encasing rectangle for an arbitrary closed curve</title>
		<author>
			<persName><forename type="first">H</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shapira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comm. ACM</title>
		<imprint>
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Text detection in images based on unsupervised classification of high-frequency wavelet coefficients</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gllavata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ewerth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Freisleben</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICPR</title>
		<meeting>ICPR</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Recognition using regions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The elements of statistical learning: Data mining, inference, and prediction</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note>second edition</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An automatic performance evaluation protocol for video text detection algorithms</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. CSVT</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automatic text location in images and video frames</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PR</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Text information extraction in images and video: a survey</title>
		<author>
			<persName><forename type="first">K</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PR</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Texture-based approach for text detection in images using support vector machines and continuously adaptive mean shift algorithm</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">I</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>IEEE Trans. PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Automatic text detection and tracking in digital video</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. IP</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Camera-based analysis of text and documents: a survey</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJDAR</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Icdar 2005 text locating competition results</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Lucas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICDAR</title>
		<meeting>ICDAR</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Icdar 2003 robust reading competitions</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Panaretos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICDAR</title>
		<meeting>ICDAR</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A method for text localization and recognition in real-world images</title>
		<author>
			<persName><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACCV</title>
		<meeting>of ACCV</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A hybrid approach to detect and localize texts in natural scene images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>IEEE Trans. IP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Icdar 2011 robust reading competition challenge 2: Reading text in scene images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shahab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICDAR</title>
		<meeting>ICDAR</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Matching local self-similarities across images and videos</title>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">A laplacian approach to multioriented text detection in video</title>
		<author>
			<persName><forename type="first">P</forename><surname>Shivakumara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Q</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Tan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>IEEE Trans. PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Word spotting in the wild</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Feature context for image classification and object detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Latecki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Text string detection from natural scenes by structure-based partition and grouping</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>IEEE Trans. IP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Text detection in images using sparse representation with discriminative dictionaries</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IVC</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
