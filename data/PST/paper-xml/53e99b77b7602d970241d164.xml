<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Video Data Mining: Semantic Indexing and Event Detection from the Association Perspective</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Xingquan</forename><surname>Zhu</surname></persName>
							<email>xqzhu@cs.uvm.edu</email>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Xindong</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Ahmed</forename><forename type="middle">K</forename><surname>Elmagarmid</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhe</forename><surname>Feng</surname></persName>
							<email>zhfeng@fudan.edu</email>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Lide</forename><surname>Wu</surname></persName>
							<email>ldwu@fudan.edu</email>
						</author>
						<author>
							<persName><forename type="middle">X</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName><forename type="middle">X</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Vermont</orgName>
								<address>
									<addrLine>33 Colchester Ave., Votey 377</addrLine>
									<postCode>05401</postCode>
									<settlement>Burlington</settlement>
									<region>VT</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Vermont</orgName>
								<address>
									<addrLine>33 Colchester Ave., Votey 351</addrLine>
									<postCode>05401</postCode>
									<settlement>Burlington</settlement>
									<region>VT</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Purdue University</orgName>
								<address>
									<addrLine>250 N. University Street</addrLine>
									<postCode>47907</postCode>
									<settlement>West Lafayette</settlement>
									<region>IN</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<addrLine>220 Handan Road</addrLine>
									<postCode>200433</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Video Data Mining: Semantic Indexing and Event Detection from the Association Perspective</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C98A224F3656B897E60246035DA44C28</idno>
					<note type="submission">received 13 Oct. 2003; revised 15 Apr. 2004; accepted 20 Oct. 2004; published online 17 Mar. 2005.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Video mining</term>
					<term>multimedia systems</term>
					<term>database management</term>
					<term>knowledge-based systems</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Advances in the media and entertainment industries, including streaming audio and digital TV, present new challenges for managing and accessing large audio-visual collections. Current content management systems support retrieval using low-level features, such as motion, color, and texture. However, low-level features often have little meaning for naive users, who much prefer to identify content using high-level semantics or concepts. This creates a gap between systems and their users that must be bridged for these systems to be used effectively. To this end, in this paper, we first present a knowledge-based video indexing and content management framework for domain specific videos (using basketball video as an example). We will provide a solution to explore video knowledge by mining associations from video data. The explicit definitions and evaluation measures (e.g., temporal support and confidence) for video associations are proposed by integrating the distinct feature of video data. Our approach uses video processing techniques to find visual and audio cues (e.g., court field, camera motion activities, and applause), introduces multilevel sequential association mining to explore associations among the audio and visual cues, classifies the associations by assigning each of them with a class label, and uses their appearances in the video to construct video indices. Our experimental results demonstrate the performance of the proposed approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>O RGANIZATIONS with large digital assets have a need to retrieve meaningful information from their digital collections. Applications such as digital libraries, video-ondemand systems, and interactive video applications introduce new challenges in managing large collections of audiovisual content. To help users find and retrieve relevant video more effectively and to facilitate new and better ways of entertainment, advanced technologies must be developed for indexing, filtering, searching, and mining the vast amount of videos. Motivated by these demands, many video research efforts have been made on exploring more efficient content management systems. A simple framework is to partition continuous video frames into discrete physical shots and extract low-level features from video shots to support activities like searching, indexing <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>, or retrieval <ref type="bibr" target="#b0">[1]</ref>. Unfortunately, a single shot which is separated from its context has less capability of conveying semantics.</p><p>Moreover, the index considering only visual similarities ignores the temporal information among shots. Consequently, the constructed cluster nodes may contain shots that have considerable variances both in semantics and visual content and, therefore, do not make much sense to human perception. The solution to this problem is to explore video knowledge to construct a database indexing structure which can facilitate database management and access. However, despite the fact that video was invented for more than 50 years and has been widely accepted as an excellent and popular tool to represent information, one can find that it has never been an easy operation to extract or explore knowledge from video data <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>.</p><p>Recently, there has been a trend of employing various data mining techniques <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref> in exploring knowledge from large video sets. These efforts are motivated by successful data mining algorithms and by the tremendous appeal of efficient video database management. Consequently, many video mining approaches have been proposed, which can be roughly classified into three categories:</p><p>1. Special pattern detection <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, which detects special patterns that have been modeled in advance, and these patterns are usually characterized as video events (e.g., dialog, or presentation). 2. Video clustering and classification <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b18">[19]</ref>, which clusters and classifies video units into different categories. For example, in <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, video clips are grouped into different topic groups, where the topic information is extracted from the transcripts of the video.</p><p>3. Video association mining, where associations from video units are used to explore video knowledge <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>. An intuitive solution for video mining is to apply existing data mining techniques <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref> to video data directly. Nevertheless, as we can see from the three types of video mining techniques above, except <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref> which have integrated traditional sequential association mining techniques, most others provided their own mining algorithms. The reason is that almost all existing data mining approaches deal with various databases (like transaction data sets) in which the relationship between data items is explicitly given. Video and image databases (or other multimedia data) are different from them. The greatest distinction between video and image databases is that the relationship between any two of their items cannot be explicitly (or precisely) figured out. Although we may now retrieve video frames (and even physical shots) with satisfactory results, acquiring relationships among video frames (or shots) is still an open problem. This inherent complexity has suggested that mining knowledge from multimedia materials is even harder than from general databases <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b43">[44]</ref>.</p><p>In this paper, we first introduce a knowledge-based video indexing framework to facilitate video database management and access. To explore video knowledge in supporting this framework, we propose a solution for a new research topic, video association mining, in which video processing and existing data mining algorithms are seamlessly integrated to mine video knowledge. We will systematically address the definitions and evaluation measures (temporal distance, temporal support, and confidence) for video associations by taking the distinct features of video data into consideration, and then proposing a solution in mining sequential patterns from the video stream that usually consists of multiple information sources (e.g., image, audio, and caption text). We use basketball videos as our test bed because sports video generates large interest and high impact worldwide.</p><p>The paper is organized as follows: In Section 2, we present a knowledge-based video indexing framework and introduce the system architecture for video association mining. We provide several techniques in Section 3 to explore visual and audio cues that can help us bridge the semantic gap between low-level features and video content. In Section 4, we present a video association mining scheme. We discuss algorithms to classify video associations and construct video indexing in Section 5. Section 6 presents the results of our performance evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">KNOWLEDGE-BASED VIDEO INDEXING AND SYSTEM ARCHITECTURE</head><p>There are two widely accepted approaches for accessing video in databases: shot-based and object-based. In this paper, we focus on the shot-based approach. In comparison with traditional video database systems that use low-level similarities among shots to construct indices, a semantic video database management framework has been proposed in Fig. <ref type="figure" target="#fig_0">1</ref>, where video semantic units (scenes or story units) are used to construct database indices <ref type="bibr" target="#b6">[7]</ref>. However, this scheme works on videos with content structure, e.g., movies and news, where video scenes are used to convey scenarios and content evolution. For many other videos, such as sports videos, there are no such story units. Instead, they contain various interesting events, e.g., a goal or a fast break, which could be taken as highlights and important semantics. Accordingly, by integrating the existing framework in Fig. <ref type="figure" target="#fig_0">1</ref>, we propose a knowledge-based video indexing framework for basketball videos, as shown in Fig. <ref type="figure">2</ref>. To support efficient video indexing, we need to address the following three key problems before we can actually adopt the framework in Fig. <ref type="figure">2</ref>: 1) How many levels should be included in the model? 2) Which kinds of decision rules should be used at each node? and 3) Do these nodes make sense to human beings? We solve the first and third problems by deriving knowledge from domain experts (or from extensive observations) and from the video concept hierarchy. For basketball videos, we first classify them into a two-level hierarchy. The first level is the host association of the games, e.g., NBA, NCAA, and CBA, and the second level consists of teams of each association, such as LA_Lake and Houston, where each video can be explicitly classified into one node. Then, we integrate the structure of video content to construct lower-level indices. As we have stated above, extensive observations and existing research efforts suggest that there are many interesting events in sports videos that can be used as highlights <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b28">[29]</ref>. For basketball videos, the events that likely attract most viewers' interests are goals, fast breaks, and free throws, etc. We can therefore use these events as nodes at the third level of our indexing structure. At the lowest level, we use the video shots as index nodes, as shown in Fig. <ref type="figure">2</ref>, where each shot may have more than one parent node because some shots contain several events.</p><p>To solve the second problem, we find that the decision rules for the first two levels (cluster and subcluster) and the lowest level (shots and frames) are relatively easy and we can employ domain knowledge and some video shot segmentation algorithms <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b26">[27]</ref> to get satisfactory results. Our analysis in Section 3.2 also indicates that, by using the caption text in basketball videos, we can recognize team names and their scores. Hence, the decision rules for the second level can also be accomplished automatically. Nevertheless, the most challenging task comes from the decision rules of the third level (events), i.e., mapping physical shots to various event nodes. In this paper, we will adopt video association mining to detect sports events. Our system architecture is given in Fig. <ref type="figure" target="#fig_2">4</ref>, where various features are outlined below:</p><p>1. A video association mining algorithm to discover video knowledge. It also explores a new research area in video mining, where existing video processing techniques and data mining algorithms are seamlessly integrated to explore video content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">An association-based video event detection scheme</head><p>to detect various sports events for database indexing. In comparison with other video event detection techniques, e.g., special pattern detection <ref type="bibr" target="#b24">[25]</ref>, the Hidden Markov Models <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b28">[29]</ref>, and classification rules <ref type="bibr" target="#b27">[28]</ref>, the association-based technique does not need to define event models in advance. Instead, the association mining will help us explore models (associated patterns) from video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">A knowledge-based sports video management</head><p>framework to support effective video access. The inherent hierarchical video classification and indexing structure can support a wide range of granularity levels. The organization of visual summaries is also inherently supported. Hence, a naive user can browse only a portion of highlights (events) to get a concise summary. By integrating the video knowledge in the indexing structure, the constructed video database system will make more sense in supporting the retrieval and browsing for naive users. As shown in Fig. <ref type="figure" target="#fig_1">3</ref>, where we provide four examples of "foul shots," it can be seen that the visual perception of these four shots vary a lot (especially for Fig. <ref type="figure" target="#fig_1">3a</ref> and all others), but Fig. <ref type="figure" target="#fig_1">3a</ref> and Fig. <ref type="figure" target="#fig_1">3b</ref> both cover the same event of the same player, which are captured from different angles. With traditional video indexing mechanisms, these four shots will be indexed at different nodes (because they have different visual perceptions) and providing Fig. <ref type="figure" target="#fig_1">3a</ref> as a query example may never work out results, like Fig. <ref type="figure" target="#fig_1">3b</ref> (even if they do match with each other in semantics). With knowledge-based indexing, we can index them as one node (as long as we can detect this type of event), so the retrieval, browsing, and database management can be facilitated. When searching from a database constructed with the proposed indexing structure, the search engine can either include or exclude any index level to facilitate different types of queries. For example, if a user wants to query for a foul shot, regardless of the team names or the host association of the games (NBA, NCAA, etc.), the search engine can inherently attain this goal by ignoring the first two levels of indexing (cluster and subcluster in Fig. <ref type="figure">2</ref>) at the search stage.</p><p>In the system architecture in Fig. <ref type="figure" target="#fig_2">4</ref>, we first parse a video sequence into physical shots and use a clustering algorithm to merge visually similar shots into groups. We then use dominant color detection to identify video groups that consist of court field shots and classify video shots into two categories: court and noncourt. We also perform camera motion extraction, audio signal analysis, and video text detection and recognition to detect visual and audio cues. A hybrid sequence is constructed by integrating the temporal order and the audio and visual cues of each shot. An association mining scheme is designed to mine sequential associations from the sequence. Finally, we classify all mined associations and use them to construct video indexing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">VIDEO PREPROCESSING</head><p>To apply existing data mining techniques on video data, one of the most important steps is to transform video from nonrelational data into a relational data set. To facilitate this goal, we adopt a series of algorithms to explore audio and visual cues. We start with a raw video sequence and output symbolic sequences that indicate where and what types of cues appear in the video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Video Shot Detection and Classification</head><p>Physical video shots that are implicitly related to content changes among frames are widely used in various video database systems <ref type="bibr" target="#b0">[1]</ref>. To support shot-based video content access, we have developed a shot cut detection technique <ref type="bibr" target="#b26">[27]</ref>, which uses color features in each frame to characterize content changes among frames. The boundaries of shots are then determined by a threshold that is adjusted adaptively by using a small window (30 frames in our current work).</p><p>After shot segmentation, we try to classify each shot into two categories: court and noncourt. We first cluster visually similar shots into groups and then use the dominant color to identify groups which consist of court field shots because the court field in most sports can be described by one distinct dominant color <ref type="bibr" target="#b28">[29]</ref>. To facilitate this goal, we use  the 10th frame of each shot as its representative frame (keyframe) 1 and then extract two visual features from each keyframe (a 3D HSV color histogram and a 10-dimensional tamura coarseness texture <ref type="bibr" target="#b30">[31]</ref>). When constructing a color histogram, we quantize H, S, and V into 16, 4, and 4 bins, respectively, so that the histogram of each image is characterized by a 256-dimensional vector and the total number of feature dimensions is 266. Given a video in the database, we assume it contains N shots S 1 ; S 2 ; . . . ; S N and denote the key-frame of S i by K i . Suppose H i;l , l 2 ½0; 255, and T C i;n , n 2 ½0; 9 are the normalized color histogram and texture of K i . The distance between shots S i and S j is defined by <ref type="bibr" target="#b0">(1)</ref>, where W C and W T indicate the weight of each feature:</p><formula xml:id="formula_0">DisðS i ; S j Þ ¼ W c 1 À X 255 l¼0 minðH i;l ; H j;l Þ ( ) þ W T ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi X 9 n¼0 ðT C i;n À T C j;n Þ 2 v u u t :<label>ð1Þ</label></formula><p>We want to group shots that are similar into a cluster. In addition, different clusters should have sufficiently different characteristics. Hence, we adopt a modified split-and-merge clustering algorithm <ref type="bibr" target="#b31">[32]</ref> by sequentially executing two major procedures: merging and splitting. In the merging procedure, we iteratively merge the most similar clusters (defined by ( <ref type="formula" target="#formula_1">2</ref>)) until the distance between the most similar clusters is larger than a given threshold. Nevertheless, this merging procedure may generate clusters with a large intracluster distance (defined by ( <ref type="formula" target="#formula_2">3</ref>)). Accordingly, after the merging procedure, we turn to the splitting procedure to split clusters with large visual variances. We iteratively calculate the intracluster distance for any cluster C i , the cluster with its intracluster distance larger than a given threshold is separated into two clusters until all clusters have their intracluster distance less than the given threshold.</p><p>Let's denote the ith cluster by C i and the number of members in C i by N i , where each element ðS l i ; l ¼ 1; . . . ; N i Þ in the cluster is a shot. The intercluster distance between C i and C j is defined by (2):</p><formula xml:id="formula_1">d min ðC i ; C j Þ ¼ min S l i 2C i ;S k j 2C j ;l¼1;...;N i ;k¼1;...;N j DisðS l i ; S k j Þ:<label>ð2Þ</label></formula><p>We then define the intracluster distance of C i by (3):</p><formula xml:id="formula_2">dðC i Þ ¼ max S l i 2C i ;S k i 2C i ; l6 ¼k; l¼1;...;N i ;k¼1;...;N i DisðS l i ; S k i Þ:<label>ð3Þ</label></formula><p>After we have clustered visually distinct shots into groups, we can use the dominant color (usually, a tone of yellow) to identify groups that consist of court field shots. However, even though the color of the court field is likely a tone of yellow, the actual color may vary from stadium to stadium and also change with lighting conditions. Therefore, we cannot assume any specific value for this dominant color, but learn it adaptively. We randomly sample N frames from video sequences (in our system, we set N ¼ 50). Because sports videos usually focus on the court field, most of these N frames will contain the court field. We then calculate the histogram of the hue component of each frame (in HSV color space). The histogram of the hue component is added up over these N frames. We pick up the peak of this cumulated hue histogram and use the corresponding hue value as the court field hue color. Assuming this hue color is denoted by " H H, we calculate the average saturation and intensity value of the pixels in these N frames, where the hue color of the pixels is " H H. We denote the average saturation and intensity by " S S and " I I. For each group G i (acquired from the former clustering algorithm), we calculate the dominant hue color of all key-frames in G i , denote it by " H H i , and the average saturation and intensity of the pixels with their hue color equal to " H H i are denoted by " S S i and " I I i . Then, we use (4) to calculate the distance between G i and the template. After we get the distances from all video groups, we use a simple thresholding method to classify each group into two exclusive categories: a group consisting of court filed shots or not:</p><formula xml:id="formula_3">HsvDisðiÞ ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ð " I I i À " I IÞ 2 þ ð " S S i Þ 2 þ ð " S SÞ 2 À 2 " S S i Á " S S Á cosðÞ q ; ð4Þ ¼ j " H H i À " H Hj if j " H H i À " H Hj &lt; 180 360 À j " H H i À " H Hj if j " H H i À " H Hj &gt; 180 :<label>ð5Þ</label></formula><p>Generally, since one sports video is captured from one place, both shot clustering and classification can acquire relatively good performances. As shown in Fig. <ref type="figure" target="#fig_3">5</ref>, we can find that the shots containing the court are successfully clustered into groups (and likely characterized by cameras with different angles or views) because the court field color plays an important role in similarity evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Video Text Detection and Recognition</head><p>There are two types of video text: the first is the text shown in video scenes, referred to as scene text hereafter, and the second is the text postprocessed and added into the video, such as team names and their scores, which we call caption text. For sports videos, caption text is much more important than scene text because the former directly conveys video semantics. With caption text, we can acquire the name of each team and use it to construct the second level index in Fig. <ref type="figure">2</ref>. Moreover, as long as we can detect the team scores, the score change is directly associated to the "goal" events. 1. For the sake of simplicity, we use this simplest key-frame selection mechanism. One can also adopt other complicated approaches <ref type="bibr" target="#b29">[30]</ref>. Nevertheless, because our purpose is not to characterize the content change in the video shots, but to classify video shots into different categories, we find the performance of this simple mechanism works reasonably well.</p><p>In comparison with scene text, the caption text has one distinct feature: It rarely moves. This distinct feature inspires us to develop a simple but efficient caption text detection algorithm:</p><p>1. Calculate the edge of the current frame F i , denote the edge frame by E i , and then calculate the edge of the succeeding frame with a step (in our system, we set ¼ 10), i.e, frame F iþ and its edge frame E iþ . 2. Compare edge pixels in E i and E iþ . If the edge pixel in E i is still the edge pixel in E iþ , the current pixel is a candidate of caption text pixel. 3. After all edge pixels in E i have been processed, use a median filter to eliminate noise and all remaining pixels to form the caption text regions. If the camera motion were still, we take the locations of the text regions detected from the most recent moving frame as the caption text regions in the current frame because, without moving the camera, all edge pixels in E i and E iþ are the same and the proposed method may not work. Meanwhile, we add another constraint: The detected caption text region should appear in either the top 1  4 or bottom 1  4 of the frame region. We have observed various basketball videos from ESPN, FOX, etc., and found that, in almost all situations, the team names and their scores appear in the top or the bottom regions of the frame because it has less impact on the viewers.</p><p>After candidate text regions have been detected, we need to prune some false candidates and handle the scale problem. The regions with their height and width less than given thresholds are eliminated and the horizontal-vertical ratio of the regions should also be in a certain range. After that, we use the Bilinear Interpolation algorithm to resize each candidate region into a certain size of box and transform the pixels into binary values (black or white) for recognition.</p><p>To recognize caption text, we adopt an existing OCR (Optical Character Recognition) engine, WOCAR <ref type="bibr" target="#b32">[33]</ref>, which takes a binarized image as the input and yields an ASCII string result. This engine has many function calls to support applications. More details about video text detection can be found in <ref type="bibr" target="#b33">[34]</ref>. Fig. <ref type="figure">6</ref> gives an example of our caption text detection results. Meanwhile, since we only detect team names and score numbers, we can develop a small vocabulary for the OCR engine to improve the recognition accuracy. We perform the algorithm on every ð ¼ 10Þ frames and use detected team names to construct the second level index. Once we detect a score change, we add a symbolic tag at the corresponding place.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Camera Motion Characterization</head><p>Given a shot S i , the camera motions in the shot can also imply some knowledge. For example, a fast break usually happens when the camera is still, or pans slowly, then suddenly speeds up and pans quickly. Hence, we can explore semantic cues from camera motions in each shot. However, the camera motions in noncourt field shots have less knowledge or can even be meaningless. We therefore only analyze camera motions from court field shots.</p><p>To extract camera motions, we have recently developed a qualitative camera motion extraction method <ref type="bibr" target="#b34">[35]</ref>. This method works on compressed MPEG streams and uses motion vectors from P-frames to characterize camera motions. For any two motion vectors in each P-frame, we first classify their mutual relationship into four categories: approaching, parallel, diverging, and rotation, as shown in Fig. <ref type="figure">7</ref>. Generally, if the camera pans or tilts, the mutual relationship between any two motion vectors is likely parallel, as shown in Fig. <ref type="figure" target="#fig_6">9</ref> and, if the camera zooms, the mutual relationship is likely to be approaching or diverging (depending on whether the actual motion is zoom-in or zoom-out). We then construct a 14-bin motion feature vector to characterize the camera motion in each P-frame. More details related to the camera motion classification can be found in <ref type="bibr" target="#b34">[35]</ref>. Only certain types of camera motions in basketball videos could possibly imply useful information and we therefore classify the camera motion of each P-frame into the following six categories: Still, Pan (left and right), Zoom (in and out), and others. A motion description hierarchy is given in Fig. <ref type="figure" target="#fig_5">8</ref>.</p><p>In addition to classifying the camera motion, we also calculate the average motion magnitude of each P-frame by <ref type="bibr" target="#b5">(6)</ref>, where MV i is the number of valid motion vectors in the P-frame i, x i ðmÞ and y i ðmÞ are the x and y components of the motion vector m in the frame i. Our objective is to characterize the speed of motion activities. We roughly classify the motion magnitude into three categories: slow, medium, and fast, by specifying a numeric range for each category. Finally, a temporal filter is adopted to eliminate falsely detected camera motions. For the MPEG videos used in our test bed, there are eight P-frames in each second of stream. So, we use the dominant motion of these eight P-frames and its magnitude as the camera motion in this range and collect camera motions and magnitude (in the original temporal order) to form a symbolic sequence. For MPEG videos encoded with fewer P-frames, one can use a longer time span for temporal filtering, because the dominant camera motion in sports video usually lasts several seconds. With the proposed approach, we can identify three typical camera motions: Pan, Tilt, and Zoom. All other camera motions are marked as "Others." For mining purposes, after camera motion detection all "Others" tags will be removed from the sequence:</p><formula xml:id="formula_4">MðiÞ ¼ X MV i m¼1 ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi x i ðmÞ 2 þ y i ðmÞ 2 q MV i :<label>ð6Þ</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Salient Audio Event Detection</head><p>In sports videos, some special audio events, e.g., audience applause and a referee's whistle, will help us acquire some semantic cues. Generally, audience applause occurs when exciting events happen, e.g., shooting and/or a goal, and a referee's whistle may imply an interruption or another special event.</p><p>To detect audience cheering, we use the pitch of audio signal. Basically, pitch is the fundamental frequency that reveals harmonic properties of audio and is an important parameter in the analysis and synthesis of speech signals. In comparison with voice and music, the pitch value of audience applause is very small. In most cases, this value in sports videos is zero because, when cheering happens, the audio signal exhibits a constant high value noise that likely drowns out other audio signals, e.g., the voice of the anchorperson or the music. We therefore extract the pitch for each audio frame. In our system, the audio frame length is 20ms and the frame shift is 0ms. Because the duration of cheering usually exceeds 1 second, we apply cheering detection on each 1-second segment. For each segment, we calculate the NonZero Pitch Ratio (NZPR), which is defined as the ratio between the number of frames whose pitch is not zero and the total number of frames in a segment. For a cheering segment, its NZPR value likely exhibits a small value, and a simple threshold scheme can distinguish cheering segments from others. Fig. <ref type="figure" target="#fig_0">10</ref> shows the results of NZPR values from a test sports video with one minute duration, where four cheering events appear at 3s-9s, 20s-25s, 41s-44s, and 54s-57s.</p><p>To detect a referee's whistle, we use spectrum domain features. Fig. <ref type="figure" target="#fig_0">11</ref> demonstrates the spectrum of an audio segment that contains two whistles. The regions with a circle margin correspond to the spectrum when the referee whistles. One can find that, in frequency regions between 3500Hz to 4500Hz, the energy of a whistle is much higher than others. We then calculate the energy ratio between frequency 3500Hz and 4500Hz for each audio frame to detect whistles. We split the whole frequency into B subbands. Given audio frame i and subband j, we define the band energy ratio (BER) by <ref type="bibr" target="#b6">(7)</ref>, where DF T i;k is the Discrete Fourier Transformation of the audio frame i and E is the order of DFT coefficients. In our system, the sampling rate for audio signals is 22050Hz and B is 12. Thus, the frequency of the fifth subband is 3675 $ 4594Hz. Then, we calculate the segment band energy ratio of the fifth subband ðSBER 5 Þ during a short time period (0.5s) by <ref type="bibr" target="#b7">(8)</ref>, where AF is the total number of audio frames in this period. Fig. <ref type="figure" target="#fig_8">12</ref> shows the results of SBER 5 values from a test sports video of about 200 seconds in length. The regions with a circle margin correspond to whistle events. We can then involve some thresholding mechanisms to find out the location of those whistle events.    Generally, there are two types of videos in our daily life: videos with some content structure and videos without any content structure. The former are videos such as movies and news where scenarios are used to convey video content. In <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, we have proposed techniques to mine associations from this type of videos. For videos without content structure, e.g., sports videos, associations may still exist where the associations could be characterized as a series of sequentially related actions. For example, in basketball videos, a series of actions, such as Camera pan ! Camera still ! Camera zoom-in ! Applause ! Scoreboard change, likely appear sequentially, because they usually accompany a goal event. Mining associations from these videos, which do not have content structure, will not only facilitate knowledge acquisition, but also help us in realizing intelligent video management. In this section, we discuss techniques for video association mining, where the definitions and measures for video associations, and the sequential pattern search procedure are extensively studied.</p><formula xml:id="formula_5">BER i;j ¼ XE B j e¼ E B ðjÀ1Þ DF T i;e X E e¼1 DF T i;e ;<label>ð7Þ</label></formula><formula xml:id="formula_6">SBER 5 ¼ 1 AF X AF i¼1 BER i;5 :<label>ð8Þ</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Video Data Transformation</head><p>With techniques in Section 3, the original video sequence is transformed into four separated symbolic streams: court field (CF), camera motion (CM), scoreboard (SB), and audio events (AE), as shown in Fig. <ref type="figure" target="#fig_9">13</ref>. Our next step is to conduct mining activities on these streams. To this end, there are two solutions: Treat data streams separately or combine them together as a single stream. Oates and Cohen <ref type="bibr" target="#b35">[36]</ref> proposed a mechanism which treats multiple streams separately when conducting the mining activity, where the objective is to find the cooccurrence of the patterns that appear in the multiple streams. However, this method requires that the streams which take part in the mining activity be synchronized, where each stream produces the same amount of symbols in the same amount of time. In our situation, the multiple streams extracted from video data obviously do not satisfy this requirement. Intuitively, combining multiple streams into a single stream appears to be an easier way for data mining purposes because mining from one stream is obviously easier than mining from multiple sources and many research efforts have been conducted to find patterns, e.g., periodic patterns <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b44">[45]</ref>, from a data stream. However, we need to guarantee that there is no information loss when combining multiple streams, which means that, after the data combination, we should maintain the original temporal order information of each separate stream in the combined stream. To this end, we adopt the following approach to combine multiple symbolic steams into a single hybrid (HB) stream: 1) For video and audio cues which happen at different time slots, we put all their tags together, with each tag placed at a corresponding place in its original stream. 2) If multiple tags happen at the same time, e.g., a scoreboard change and a camera motion happen at the same time, we use the same order to combine them in all situations, e.g., a scoreboard change always precedes a camera motion. An example of video data transformation is shown in Fig. <ref type="figure" target="#fig_9">13</ref>, where information from four separate streams is combined to form a hybrid stream. <ref type="foot" target="#foot_0">2</ref> With such a mechanism, the temporal order information in each separate stream is well maintained in the transferred hybrid stream and combining multiple streams into a single stream will not lose information for effective association mining from data streams.</p><p>We have adopted a hierarchical camera motion description in Fig. <ref type="figure" target="#fig_5">8</ref>, so we have to generalize an HB stream for multilevel association mining. Our generalization is accomplished by assigning a symbol to each type of tag, as shown in Table <ref type="table" target="#tab_0">1</ref>. For events with a hierarchy, we generalize them into a set of characters with each character indicating a state. For example, for "E12," "E" denotes camera pan, "1"  indicates the panning direction, and "2" represents the motion magnitude. In Fig. <ref type="figure" target="#fig_9">13</ref>, the last row gives a generalized HB stream.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Definitions and Terminology</head><p>Based on the above observations, we define a video association as a sequential pattern with {X 1 ::X i ::X L ; X t i &lt; X t j for any i &lt; j}, where X i is a video item (see Definition 1 below), L denotes the length of the association, X 1 \ :: \ X i :: \ X L ¼ , X t i denotes the temporal order of X i , and X t i &lt; X t j indicates that X i happens before X j . For simplicity, we use fXg as the abbreviation for a video association.</p><p>Generally, two measures (support and confidence) have been used to evaluate the quality of an association. However, these measures do not consider temporal information of the items in the association. For video associations, the temporal distance (see Definition 5 below) between neighboring items implies some useful information: The smaller the temporal distance between neighboring items, the larger is their correlation. For example, if two neighboring shots contain applause and scoreboard change, respectively, we naturally believe that they are correlated. However, the applause that happens several shots (e.g., three more shots) before the scoreboard change rarely indicates any correlation between them. That is, for associations with a large temporal distance between neighboring items, their items usually have a weaker correlation and, therefore, can imply almost no knowledge. Accordingly, instead of using the traditional support measure, we adopt a temporal support (TS) to evaluate the video association. Moreover, several other definitions are also given below:</p><p>1. A video item is a basic unit in association mining. In this paper, it denotes a symbolic tag acquired from video processing techniques, i.e., a symbolic unit in the hybrid video stream. 2. An L-ItemAssociation is an association that consists of L sequential items. For example, "AB" is a 2-Item-Association and "ABC" is a 3-ItemAssociation. 3. An ItemSet is an aggregation which consists of video associations. More specifically, an L-ItemSet is an aggregation of all L-ItemAssociations, each of which is an L-ItemAssociation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">L-LItemSet is an aggregation of all L-ItemAssociations</head><p>whose temporal support (see Definition 7 below) is no less than a given threshold. 5. Given a transformed hybrid video stream, the temporal distance (TD) between two items is the temporal identification difference of the shots that contain these two items. For example, in the hybrid stream demonstrated in Fig. <ref type="figure" target="#fig_10">14</ref>, the first time the pattern fABg appears, their temporal distance T DfABg is 0 because they happen in the same shot.</p><p>The second time fABg appears, T DfABg equals 1 because A and B happen in two neighboring shots and the temporal identification difference between the neighboring shots is 1. 6. The temporal distance threshold (TDT) specifies the upper bound that the temporal distance must comply with, i.e., no larger than this threshold. Take the pattern fABg in Fig. <ref type="figure" target="#fig_10">14</ref>, for example, when T DT ¼ 1, T DfABg ¼ 2 will not satisfy because T DfABg ¼ 2 is larger than the given TDT value.</p><p>7. Given a temporal distance threshold (TDT) T DT ¼ T , the temporal support (TS) of an association fX 1 . . . X L g is defined as the number of times this association appears sequentially in the sequence. In addition, each time this association appears, the temporal distance between any two neighboring items of the association should satisfy the given T DT (i.e., no more than T shots). In Fig. <ref type="figure" target="#fig_10">14</ref>, when T DT ¼ 1 (i.e., ignoring the temporal distance), the temporal support for fABg is T SfABg ¼ 3. However, when we set T DT ¼ 1, T SfABg becomes 2 because the last time fABg appears, its temporal distance ðT DfABg ¼ 2Þ does not satisfy the given T DT . It is obvious that the smaller the T DT , the stronger the semantic correlations among the mined associations are. 8. Given T DT ¼ T , the confidence of an association fX 1 ; :; X L g is defined as the ratio between the temporal support of fXg (when T DT ¼ T ) and the maximal number of possible occurrences of the association fXg. Because the maximal possible occurrences of the association are determined by the number of occurrences of the item with the minimal support, the confidence of the association is defined by <ref type="bibr" target="#b8">(9)</ref>. The examples of the confidence evaluation have been provided in Fig. <ref type="figure" target="#fig_10">14</ref>, where different T DT values result in different confidences for the same association. The larger the confidence value, the more confidently the association holds:</p><formula xml:id="formula_7">ConffXg T DT ¼T ¼T SfXg T DT¼T MinðT SðX 1 Þ; ::; T SðX L ÞÞ:<label>ð9Þ</label></formula><formula xml:id="formula_8">4.3 Video Association Mining 4.3.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Multilevel Associations</head><p>We have introduced a hierarchy in Fig. <ref type="figure" target="#fig_5">8</ref> (which can also be interpreted as a taxonomy) to characterize camera motions. When a taxonomy exists, the supports of associations at lower levels are lower than associations at higher levels. Accordingly, some solutions have been proposed to mine multilevel associations <ref type="bibr" target="#b37">[38]</ref>. The motivation behind these algorithms is simple and intuitive: For all hierarchical items, their ancestors at higher levels are added into data sets and a data mining algorithm is executed on new data sets for multiple times to mine multilevel associations. As shown in Table <ref type="table" target="#tab_1">2</ref>, given a generalized HB sequence in Tables 2a, 2b, and 2c, show the 1-ItemSet at level 1 and level 2, respectively. As we can see, only the descendants of the large ItemSet at level 1 are considered as candidates for level 2 large 1-ItemSet. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">The Mining Algorithm</head><p>Our video association mining algorithm consists of the following phases:</p><p>1. Transform. This phase adopts various techniques to explore visual and audio cues and transforms video data into a relational data set D. 2. L-LItemSet. In this phase, we mine video associations with various levels and lengths. We first find an L-ItemSet and then use the L-ItemSet and userspecified thresholds to find L-LItemSet. We iteratively execute this phase until no more nonempty L-LItemSet can be found.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Collection and Postprocessing. This phase collects</head><p>and postprocesses video associations for different applications. We have discussed techniques for Phase 1 and Phase 3 directly relates to applications of video associations, which is trivial from the data mining point of view. Therefore, we focus on Phase 2 only, where its main procedure is shown in Fig. <ref type="figure" target="#fig_3">15</ref>. Throughout this section, we use the notions that l denotes the level of associations (the maximal level of associations max_level is 3 in our system) and D½l represents the filtered data set at level l. I½l; k and L½l; k are the aggregations of k-ItemSet and k-LItemSet at level l, respectively. fXg:Item k means the kth item of the association fXg.</p><p>Basically, Phase 2 consists of two stages: 1) In the first stage, the algorithm filters the data set at level l and uses the filtered data set D½l to construct 1-ItemSet and 1-LItemSet at level l, as shown on lines 2 to 4 in Fig. <ref type="figure" target="#fig_3">15</ref>. 2) Then, the algorithm uses the constructed 1-LItemSet and the candidate generation procedure (Fig. <ref type="figure" target="#fig_12">16</ref>) to progressively mine k-ItemAssociations, k ¼ 2; 3; . . . ; at level l, until the constructed k-LItemSet at level l is empty. Then, the algorithm turns to next level l þ 1 and mines associations at this level.</p><p>As shown in Fig. <ref type="figure" target="#fig_3">15</ref>, for each level l, we first filter the data set D, F ilter DatasetðD; lÞ, to process items that are no larger than level l. For example, when l ¼ 2, this procedure filters items fE13; E12g as fE1; E1g and the higher the level, the more subtle the filtered item is. The filtered sequence is put into a new data set D½l. We then use D½l to generate 1-ItemAssociations at level l (denoted by I½l; 1) by using function Get 1 ItemSetðD½l; lÞ. We use the generated 1-ItemSet and the user specified minimal support minSup½l to generate 1-LItemSet at level l (denoted by L½l; 1) with procedure Get 1 LItemSetðD½l; I½l; 1; minSup½lÞ. The generated 1-LItemSet consists of associations in 1-ItemSet which satisfy the user-specified minimal support minSup½l. Because 1-ItemAssocitions do not involve any temporal distance, we ignore TDT when constructing the 1-LItemSet. We then use the generated 1-LItemSet at level l to mine associations with larger lengths. This is facilitated by adopting an Apriori-like algorithm which uses multiple passes to generate candidates and evaluate their supports.</p><p>In each pass, we use the LItemSet from the previous pass to generate the candidate ItemSet and then measure the temporal support of generated candidates by making a pass over the database D½l. At the end of the pass, the support of each candidate is used to determine the frequent ItemSet.</p><p>Candidate generation for each pass is similar to the method in <ref type="bibr" target="#b11">[12]</ref>. It takes the set of all k À 1-ItemAssociations in L½l; k À 1 and all their items as input and works as shown in Fig. <ref type="figure" target="#fig_12">16</ref>. The items in L½l; k À 1 first join together to form new candidates. To this end, for any two distinct k À 1-ItemAssociations fpg and fqg in L½l; k À 1, if their first k À 2 items are the same (as shown on line 3 in Fig. <ref type="figure" target="#fig_12">16</ref>), we will generate a new k-ItemAssociation fXg. The first k À 2 items of fXg are the same as that of fpg and the k À 1th and kth items of fXg are the k À 1th item of fpg and fqg, respectively (as shown on line 5 in Fig. <ref type="figure" target="#fig_12">16</ref>). Then, fXg is taken as a candidate and put in I½l; k. We iteratively repeat the same procedure until all elements in L½l; k À 1 have been evaluated. After that, we prune out the candidates in I½l; k whose subsequences are not in L½l; k À 1 because, if a  subsequence of an association is not frequent, this association will not be frequent neither. All remaining candidates are taken as associations in I½l; k. Table <ref type="table">3</ref> provides an example of candidate generation, where the fourth column gives the 3-LItemSet and the fifth column is the join results (candidates) from the 3-LItemSet. After pruning out sequences whose subsequences are not in the 3-LItemSet, the sequences shown in the sixth column will be left. For example, fABDCg is pruned out because its subsequence fBDCg is not in the 3-LItemSet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Search Patterns from Hybrid Stream with Constraints</head><p>To mine video associations, the most important procedure is to search the appearances of the candidate pattern in the data stream, and this problem is complicated by users' constraint on the temporal distance (T DT ) between items of the pattern. For example, with the HB stream in Fig. <ref type="figure" target="#fig_13">17a</ref>, when searching the appearance for pattern fAEF BGg, many other approaches <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b38">[39]</ref> usually adopt a sliding window (e.g., w 1 and w 2 in Fig. <ref type="figure" target="#fig_13">17a</ref>) to evaluate whether the pattern appears in the window or not. Such a windowing procedure has two obvious disadvantages: 1) Users have no control with the temporal distance between the items of the pattern, i.e., this approach ignores the temporal distances in the pattern, and 2) users have to well define the width of the window, otherwise the pattern may never fall into any window. Accordingly, we need to design a new search mechanism by considering the temporal distance between neighboring items of the pattern. The simplest solution for this problem is to adopt a waiting-and-matching <ref type="bibr" target="#b45">[46]</ref> method: We start from the first item of the pattern fAEF BGg and scan the data stream until the certain item appears; at any state, if the temporal distance between items violates the T DT , the search procedure restarts. In Fig. <ref type="figure" target="#fig_13">17a</ref>, "choice 1" of fAEF BGg represents the results from this method. This approach, however, could miss targets if the user specifies a relatively small T DT . In Fig. <ref type="figure" target="#fig_13">17a</ref>, if we set T DT ¼ 2, the waiting-and-matching mechanism will fail to find the pattern because the temporal support between "BG" in "choice 1" is 3, which is larger than T DT ¼ 2. However, there are other choices that fAEF BGg actually satisfies T DT ¼ 2, e.g., "choice 2" and "choice3." Motivated by the above observations, we propose a new algorithm for searching sequential patterns from data stream with constraints. The intuitive idea behind this scheme is to push an item backward as much as we can  (without violating the T DT ), so we can maximize the possibility that, under the constraint of T DT , the pattern may appear in the stream. The algorithm consists of following major steps:</p><p>1. Given a pattern fX 1 ; X 2 ::; X L g, a hybrid stream D, and a user-specified T DT , we call D fX 1 ; X 2 ::; X L g the objective pattern in. For each item X i in the objective pattern, we construct a list O i to record the appearances of X i in D and initialize the list with O 1 È; ::; O i È; ; ::; O L È; . 2. Starting from the first item of the objective pattern, for each item X i ; i ¼ 1; ::L, we search the first appearance of X i from D (and ignore the appearance of any other item X j , j &gt; i). If item X i appears in D, we put the index (sequence index and shot index) of the appearance into the list O i .</p><p>As demonstrated in Fig. <ref type="figure" target="#fig_13">17b</ref>,</p><formula xml:id="formula_9">O 1 O 1 [ A 1 1 ; O 2 O 2 [ E 1 3</formula><p>, and so on. This procedure continues until all items X i ; i ¼ 1; ::; L, have at least one member in their list O i ; i ¼ 1; ::; L. 3. When searching the appearance of the current item X i , if any former item (including X i itself) X j , j i appears in D again, we put the index of X j in the list O j as long as the appearance of X j satisfies the T DT . As shown at "status in B 2 6 " in Fig. <ref type="figure" target="#fig_13">17b</ref>, when searching for the appearance of "G," another "B" comes. Denote O k i by the kth member in the list O i , and T i by the number of members in O i , so O Ti i is the last member in O i . To evaluate whether the appearance of X j satisfies the constraint of T DT , we calculate two measures: a) the temporal distance between X j and the latest appearance of its neighboring item O jÀ1 Þ T DT , we add the index of X j into its list O j and continue the procedure; otherwise, O j remains unchanged. Meanwhile, if T DðX j ; O TiÀ1 iÀ1 Þ &gt; TDT, it will indicate that, even if we assume the item X i does appear at the current location of X j , the temporal distance with its neighboring item X iÀ1 still violates the constraint of T DT , so there is no need to search the appearance of X i any further. We will restart searching the appearance of the objective pattern from the location of the last member in O  <ref type="figure" target="#fig_13">17a</ref>, no matter what T DT value (1, 2, or 3) users specify, our algorithm will exactly locate only one location for fAEF BGg, which is "choice 3." However, with the waiting-and-matching approach, only "choice 1" could be found and, if we set T DT ¼ 1, it will miss the appearance of the pattern because, in this case, "choice 1" does not satisfy the T DT . Therefore, our algorithm has a higher accuracy than the waiting-and-matching mechanism. And, because we only scan stream D once, the complexity of the algorithm is OðNÞ for one objective pattern, where N is the length of D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">VIDEO ASSOCIATION CLASSIFICATION</head><p>To apply video associations in video indexing, we need to classify each association into a corresponding category (event) and use detected events to construct video indices. Some research efforts have addressed the problem of association rule classification, but little literature has been found on classifying sequential associations. We adopt the nearest neighbor search-based strategy as follows: We first mine associations from training videos. For each association, we manually go through the training data to evaluate what types of events associate with the appearance of this association. We count the number and the types of events from all appearances and select the event with the largest number to label the association. Accordingly, each association will receive one class label. For each association, fXg, in the test set, we calculate its distance with associations in the training set and the class label of the association in the training set which has the smallest distance with fXg is used to label fXg. In the case that multiple associations have the same smallest distance with fXg, all their class labels are used to label fXg. To calculate the distance between sequential associations, we take the temporal order and the length of the associations into consideration and use the Longest Common Subsequence (LCS) <ref type="bibr" target="#b39">[40]</ref> between two associations to evaluate the association distances.</p><p>Given two associations, assuming fXg 1 ¼ fX 1 ; . . . ; X P g denotes the association with a length, P , and the other association is denoted by fXg 2 ¼ fX 1 ; . . . X Q g with length Q. For example, fXg 1 ¼ fA; B; E; F; Gg and fXg 2 ¼ fB; A; E; G; A; Dg. The Dynamic Programming <ref type="bibr" target="#b39">[40]</ref> has OðP QÞ time complexity and space requirement to find the largest common subsequence between fXg 1 and fXg 2 . <ref type="foot" target="#foot_2">3</ref> Then, the distance between fXg 1 and fXg 2 is defined by <ref type="bibr" target="#b9">(10)</ref>, where jLCSffXg 1 ; fXg 2 gj represents the length of the largest common subsequence:</p><formula xml:id="formula_10">SeqAssocDffXg 1 ; fXg 2 g ¼ 1 À jLCSffXg 1 ; fXg 2 gj MinðP ; QÞ ;<label>ð10Þ</label></formula><p>Actually, this distance is determined by the maximal number of sequentially matched items between the associations fXg 1 and fXg 2 . The larger the number, the smaller their distance is.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTAL RESULTS</head><p>The results of an extensive performance analysis conducted to 1) evaluate the video processing techniques in Section 3, 2) evaluate the video association mining and association-based indexing algorithms in Sections 4 and 5, and 3) analyze the performance of our knowledge-based indexing framework are located in the Appendix which can be found on the Computer Society Digital Library at http://computer.org/tkde/archives.htm. Our algorithms were evaluated with eight basketball videos (NBA and NCAA) captured from ESPN and Fox and all commercials in the videos are removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS AND REMARKS</head><p>In this paper, we have proposed a solution for a new research area of video mining-video association mining. We have used video associations to construct a knowledgebased video indexing structure to support efficient video database management and access. We have introduced various techniques to extract visual and audio semantic cues and combined them into one hybrid stream by considering their original temporal order in the video. Consequently, the video data is transformed into a relational data set. We have employed a sequential multi level association mining strategy to mine associated video items and take them as video associations. We have adopted a scheme to classify associations into different categories, where each association can possibly indicate the happening of one type of event. The knowledge-based video indexing structure is accomplished by mining and classifying associations from video data. We have presented experimental results to demonstrate the performance of the proposed schemes. We believe we have explored a new research area to discover video knowledge for efficient video database management.</p><p>While the strategies presented in this paper are specific to basketball videos, mining associations for video knowledge exploration is an essential idea we want to convey here. From this point of view, further research could be conducted on the following aspects: 1) Extend the current framework to other domains and evaluate the performance of the video mining algorithm in environments containing more events. We believe the most promising domain is the surveillance video, where the routine vehicles in security areas normally comply with some associations like enter ! stop ! drop off ! leave and a vehicle which does not comply with this association might be problematic and deserves further investigation. However, due to the inherent differences between different video domains (e.g., the concept of shot and video text do not exist in surveillance videos), we may need more efforts to analyze the video content details for association mining, e.g., extract trails and status of moving objects to characterize associations. 2) We have adopted various video processing techniques to explore visual and audio cues for association mining and it will inevitably incur information loss from the original video sequences to transferred symbolic streams; more studies are needed to address this issue in the mining activities. 3) The mining algorithms in this paper are mainly derived from the existing data mining schemes (with some extensions for video mining scenarios); extensive studies are needed to explore efficient mining algorithms which are unique for mining knowledge from video data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The proposed hierarchical video database model. Fig. 2. Knowledge-based basketball video database management.</figDesc><graphic coords="2,62.08,69.17,179.37,111.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Examples of the free throws of "foul shots," where shot (b) is captured right after shot (a).</figDesc><graphic coords="3,31.80,69.17,239.95,62.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The architecture of association-based video indexing.</figDesc><graphic coords="3,306.60,69.17,216.46,207.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Video shot clustering results where each icon image represents one shot (the first row represents the first shot of each group and all other rows represent each clustered group)</figDesc><graphic coords="4,306.60,69.17,216.46,139.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .Fig. 7 .</head><label>67</label><figDesc>Fig. 6. Video caption text region detection, (a) frame F i , (b) edge frame E i , (c) frame F iþ , (d), edge frame E iþ , (e) the edge pixels which appear in both E i and E iþ , and (f) detected caption text regions.</figDesc><graphic coords="5,57.88,69.17,450.87,73.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Camera motion description hierarchy.</figDesc><graphic coords="6,34.19,178.30,235.09,71.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Camera pan operation between two frames (a) and (b), and (c) the corresponding motion vectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 10 .Fig. 11 .</head><label>1011</label><figDesc>Fig. 10. Nonzero pitch ratio from an audio signal.</figDesc><graphic coords="6,303.19,183.97,223.21,99.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Segment band energy ratio of the fifth subband from an audio with multiple whistle events.</figDesc><graphic coords="6,294.07,307.11,241.38,108.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Video data transformation and generalization.</figDesc><graphic coords="7,72.17,69.17,422.24,137.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. Example of video association evaluation in terms of temporal support and confidence</figDesc><graphic coords="8,294.80,69.17,239.95,84.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>2 Fig. 15 .</head><label>215</label><figDesc>Fig. 15. Pseudocode for multilevel video association mining.</figDesc><graphic coords="9,39.86,546.92,486.98,197.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 16 .</head><label>16</label><figDesc>Fig. 16. Pseudocode for candidate generation.TABLE 3 An Example of Video Association Mining, where fXg C S Indicates an Association</figDesc><graphic coords="10,43.09,258.52,480.41,77.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 17 .</head><label>17</label><figDesc>Fig. 17. Search candidates from a hybrid stream, where X j i represents the index information of the item X (j means in which shot the item appears and i indicates the order of the item in the stream). (a) An example of hybrid stream. (b) Search procedure (TDT=2).</figDesc><graphic coords="10,305.63,499.69,218.35,219.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>b) the temporal distance between the current location of X j and the last member in O iÀ1 , T DðX j ; O T iÀ1 iÀ1 Þ. If T DðX j ; O T jÀ1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1 A</head><label>1</label><figDesc>Mapping Table to Generalize Video Data</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2 Multilevel</head><label>2</label><figDesc>Association Mining:</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>As long as the lists of all items ðO 1 ; O 2 ; ::; O L Þ have at least one member, we cease the current search procedure because an appearance of the pattern have been located so far. As shown at "status at G 4 14 . . . " in Fig.17b, we will start from the last member in O L (actually, there is only one member in O L ), denote it by O Ã L , and check all members in O LÀ1 in an inverse order (backward) to find the member that appears before O Ã L and has the smallest temporal distance with O Ã</figDesc><table><row><cell>1 . Meanwhile, all lists should be initialized with O 1 È; ::; O i È; ::O L È. 4. the pattern, as shown in Fig. 17b. Then, we initialize all lists with O 1 È; ::; O i È; ::O L È and restart to locate the next appearance of the pattern from the location next to O Ã L . As shown in Fig.</cell></row></table><note><p><p><p>L . We denote this member by O Ã</p>LÀ1</p>and then find the member from O LÀ2 that appears before O Ã LÀ1 and has the smallest temporal distance with O Ã LÀ1 . We repeat the same procedure until the appearances of all items have been located. The sequence fO Ã 1 ; ::; O Ã L g will provide actual locations of</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>We mark only one CF tag for each video shot, which is placed at the beginning of the shot, because a shot either belongs to the court field or not. Inside each shot, we will analyze its content and explore other video and audio cues. This is the reason that some video shots receive several tags, as shown in shot 2 of Fig.13. This is different from the statements in Section</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>3.1, where only one key-frame is extracted from each shot to classify a video shot into a noncourt shot or a court shot.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>In the example above, there are two LCS subsequences LCSffXg 1 ; fXg 2 g ¼ ffA; E; Gg; fB; E; Ggg.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank the anonymous reviewers for their constructive comments on two earlier versions of this paper. This work was supported by the US Army Research Laboratory and the US Army Research Office under grant number DAAD19-02-1-0178, the US National Science Foundation under grants 0093116-IIS and 9972883-EIA, and the NSF under contract 69935010.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automatic Partitioning of Full-Motion Video</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kantankanhalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Smoliar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Multimedia Systems</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="10" to="28" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A Survey on Content-Based Retrieval for Multimedia Databases</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yoshitaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ichikawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowledge and Data Eng</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="81" to="93" />
			<date type="published" when="1999-02">Jan./Feb. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">WVTDB-A Semantic Content-Based Video Database System on the World Wide Web</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Elmagarmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowledge and Data Eng</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="947" to="966" />
			<date type="published" when="1998-12">Nov./ Dec. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multimodal Video Indexing: A Review of the State-of-the-Art</title>
		<author>
			<persName><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia Tools and Applications</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Smart VideoText: A Video Data Model Based on Conceptual Graphs</title>
		<author>
			<persName><forename type="first">F</forename><surname>Kokkoras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Vlahavas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elmagarmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Houstis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Aref</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM/Springer Multimedia Systems</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="328" to="338" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">ClassMiner: Mining Medical Video Content Structure and Events Towards Efficient Access and Scalable Skimming</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">G</forename><surname>Aref</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Elmagarmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGMOD Workshop</title>
		<meeting>ACM SIGMOD Workshop</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Medical Video Mining for Efficient Database Indexing, Management and Access</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Aref</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Catlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elmagarmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 19th Int&apos;l Conf. Data Eng</title>
		<meeting>19th Int&apos;l Conf. Data Eng</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="569" to="580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Video Data Mining: Extracting Cinematic Rules from Movie</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Matsuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shirahama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Uehara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Workshop Multimedia Data Management (MDM-KDD)</title>
		<meeting>Int&apos;l Workshop Multimedia Data Management (MDM-KDD)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A Framework of Human Motion Tracking and Event Detection for Video Indexing and Mining</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. DIMACS Workshop Video Mining</title>
		<meeting>DIMACS Workshop Video Mining</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multimedia Data Mining Framework for Raw Video Sequence</title>
		<author>
			<persName><forename type="first">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Workshop Multimedia Data Management</title>
		<meeting>Int&apos;l Workshop Multimedia Data Management</meeting>
		<imprint>
			<publisher>MDM-KDD</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">VideoCube: A Novel Tool for Video Mining and Classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. Asian Digital Libraries (ICADL)</title>
		<meeting>Int&apos;l Conf. Asian Digital Libraries (ICADL)</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="194" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">GeoPlot: Spatial Data Mining on Video Libraries</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. Information and Knowledge Management</title>
		<meeting>Int&apos;l Conf. Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="405" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mining Video Association for Efficient Database Management</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Joint Conf</title>
		<meeting>Int&apos;l Joint Conf</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="1422" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sequential Association Mining for Video Summarization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. Multimedia and Expo</title>
		<meeting>IEEE Int&apos;l Conf. Multimedia and Expo</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="333" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mining of Video Database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia Data Mining</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised Mining of Statistical Temporal Structures in Video</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Divakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Video Mining</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Doremann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Dementhon Eds</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Academic</forename><surname>Kluwer</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mining Cinematic Knowledge: Work in Progress</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wijesekera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Barbara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Workshop Multimedia Data Management</title>
		<meeting>Int&apos;l Workshop Multimedia Data Management</meeting>
		<imprint>
			<publisher>MDM-KDD</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Content-Based Video Indexing for the Support of Digital Library Search</title>
		<author>
			<persName><forename type="first">M</forename><surname>Windhouwer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zwol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Blok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jonker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kersten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Apers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. Data Eng</title>
		<meeting>Int&apos;l Conf. Data Eng</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="494" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mining Images and Video</title>
		<author>
			<persName><forename type="first">S</forename><surname>Newsam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tesic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Manjunath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. DIMACS Workshop Video Mining</title>
		<meeting>DIMACS Workshop Video Mining</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fast Algorithms for Mining Association Rules</title>
		<author>
			<persName><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Very Large Data Bases Conf</title>
		<meeting>Very Large Data Bases Conf</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="487" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Mining Sequential Patterns</title>
		<author>
			<persName><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th Int&apos;l Conf. Data Eng</title>
		<meeting>11th Int&apos;l Conf. Data Eng</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kamber</surname></persName>
		</author>
		<title level="m">Data Mining: Concepts and Techniques</title>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Thuraisingham</surname></persName>
		</author>
		<title level="m">Managing and Mining Multimedia Database</title>
		<imprint>
			<publisher>CRC Press</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multi-mediaMiner: A System Prototype for Multimedia Data Mining</title>
		<author>
			<persName><forename type="first">O</forename><surname>Zaiane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGMOD</title>
		<meeting>ACM SIGMOD</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="581" to="583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Automatic Detection of &apos;Goal&apos; Segments in Basketball Videos</title>
		<author>
			<persName><forename type="first">S</forename><surname>Nepal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Reynolds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Ninth ACM Multimedia Conf</title>
		<meeting>Ninth ACM Multimedia Conf</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="261" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A Mid-Level Representation Framework for Semantic Sports Video Analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 11th ACM Multimedia Conf</title>
		<meeting>of 11th ACM Multimedia Conf</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="33" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">MultiView: Multi-Level Video Content Representation and Retrieval</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">G</forename><surname>Aref</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Elmagarmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hacid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marzouk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="895" to="908" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rule-Based Video Classification System for Basketball Video Indexing</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vellaikal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Multimedia Workshops</title>
		<meeting>ACM Multimedia Workshops</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="213" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Structure Analysis of Soccer Video with Hidden Markov Models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Divakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. Acoustics, Speech, and Signal Processing</title>
		<meeting>IEEE Int&apos;l Conf. Acoustics, Speech, and Signal essing</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Key Frame Selection by Motion Analysis</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. Acoustics, Speech, and Signal Processing</title>
		<meeting>IEEE Int&apos;l Conf. Acoustics, Speech, and Signal essing</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="1228" to="1231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Texture Features Corresponding to Visual Perception</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yamawaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="460" to="473" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Picture Segmentation by a Directed Split-and-Merge Procedure</title>
		<author>
			<persName><forename type="first">S</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pavlidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Joint Conf. Pattern Recognition</title>
		<meeting>Int&apos;l Joint Conf. Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1974">1974</date>
			<biblScope unit="page" from="424" to="433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<ptr target="http://ccambien.free.fr/wocar/" />
		<title level="m">WOCAR Engine 2.5</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Automatic Text Location in Images and Video Frames</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2055" to="2076" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">InsightVide: Towards Hierarchical Video Content Organization for Efficient Browsing, Summarization, and Retrieval</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Elmagarmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Catlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Searching for Structure in Multiple Streams of Data</title>
		<author>
			<persName><forename type="first">T</forename><surname>Oates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 13th Int&apos;l Conf. Machine Learning</title>
		<meeting>13th Int&apos;l Conf. Machine Learning</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="346" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Efficient Mining Partial Periodic Patterns in Time Series Database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. Data Eng</title>
		<meeting>Int&apos;l Conf. Data Eng</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="106" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mining Generalized Association Rules</title>
		<author>
			<persName><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 21th Very Large Data Bases Conf</title>
		<meeting>21th Very Large Data Bases Conf</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Reliable Detection of Episodes in Event Sequences</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gwadera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Atallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Szpankowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Third Int&apos;l Conf. Data Mining</title>
		<meeting>Third Int&apos;l Conf. Data Mining</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="67" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Cormen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leiserson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rivest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stein</surname></persName>
		</author>
		<title level="m">Introduction to Algorithms</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Contorting High Dimensional Data for Efficient Main Memory Processing</title>
		<author>
			<persName><forename type="first">B</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ooi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGMOD Conf</title>
		<meeting>SIGMOD Conf</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="479" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">R-Trees: A Dynamic Index Structure for Spatial Searching</title>
		<author>
			<persName><forename type="first">A</forename><surname>Guttman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGMOD Conf</title>
		<meeting>SIGMOD Conf</meeting>
		<imprint>
			<date type="published" when="1984">1984</date>
			<biblScope unit="page" from="47" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The SR-Tree: An Index Structure for High-Dimensional Nearest Neighbor Queries</title>
		<author>
			<persName><forename type="first">N</forename><surname>Katayama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGMOD Conf</title>
		<meeting>SIGMOD Conf</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="369" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Mining Viewpoint Patterns in Image Databases</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGKDD</title>
		<meeting>SIGKDD</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="553" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Discovery of Frequent Episodes in Event Sequences</title>
		<author>
			<persName><forename type="first">H</forename><surname>Mannila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Toivonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Verkamo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="259" to="289" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Mining Sequential Patterns: Generalizations and Performance Improvements</title>
		<author>
			<persName><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Fifth Int&apos;l Conf. Extending Database Technology (EDBT)</title>
		<meeting>Fifth Int&apos;l Conf. Extending Database Technology (EDBT)</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
