<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Video-based raindrop detection for improved image registration</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Martin</forename><surname>Roser</surname></persName>
							<email>martin.roser@kit.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Measurement and Control</orgName>
								<orgName type="institution">Karlsruhe Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
							<email>geiger@kit.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Measurement and Control</orgName>
								<orgName type="institution">Karlsruhe Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Video-based raindrop detection for improved image registration</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CEBD78F69E4A55585E036FC9EE95C522</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we present a novel approach to improved image registration in rainy weather situations. To this end, we perform monocular raindrop detection in single images based on a photometric raindrop model. Our method is capable of detecting raindrops precisely, even in front of complex backgrounds. The effectiveness is demonstrated by a significant increase in image registration accuracy which also allows for successful image restoration. Experiments on video sequences taken from within a moving vehicle prove the applicability to real-world scenarios.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Many state-of-the-art computer vision algorithms are designed to work in well-posed visibility conditions. Though this assumption may hold for most indoor and simulation scenarios, environmental noise can badly influence their performance in outdoor settings, e.g. in surveillance applications or in driver assistance systems where a camera is mounted behind the windshield of a moving vehicle.</p><p>However, proper operation in the presence of rain is a security-relevant prerequisite to many applications, particularly on board mobile vehicles. For example image registration accuracy declines in the presence of raindrops on the windshield due to mismatched features. In this paper we propose a novel approach to video-based raindrop detection with an application to monocular image registration improvement. Detecting raindrops using a photometric raindrop model also allows for restoring occluded regions by fusing intensity information from nearby image frames as illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>.</p><p>Traditional image mosaicing approaches <ref type="bibr" target="#b1">[2]</ref> fail in registering images from in-vehicle cameras correctly, since they either assume rotations around the yaw axis merely or insignificant scene depth. Our image sequence, however, is recorded at very small frame rates (10 Hz) from within a vehicle moving at 15 -25 m s which changes scene depth significantly. Furthermore, feature localization noise is high due to the tilted orientation of the camera with respect to the road and the presence of raindrops on the windshield. This makes direct estimation of the fundamental matrix between consecutive frames difficult <ref type="bibr" target="#b9">[10]</ref>.</p><p>On the other hand, visual SLAM methods <ref type="bibr" target="#b2">[3]</ref> are hard to employ due to road homogeneities, pattern recurrences (e.g. lane markings or guardrails) and a wide baseline between frames. Since we are using only monocular sequences, stereo information <ref type="bibr" target="#b14">[15]</ref> is not available to us.</p><p>For registration we therefore extended the approach proposed in <ref type="bibr" target="#b7">[8]</ref>, which uses prior knowledge of the vehicle dynamics and camera setup in combination with homography constraints. However, this only works reliably, if enough feature correspondences on the road plane can be matched between consecutive frames. Due to mismatches and occluded image regions this is not the case in the presence of raindrops in front of the camera. In this paper we show that automatic raindrop detection helps alleviating this problem.</p><p>However, raindrop detection is a challenging problem due to several reasons: Raindrops on windshields exhibit a large variety in shape and size. Their short distance to the camera results in out-of-focus blur, hereby decreasing distinctiveness of raindrop features. Moreover, transparency and light refraction makes the raindrops appearance highly dependent on the image background. Recently, several approaches have been proposed to overcome these problems:</p><p>Garg and Nayar <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref> proposed an accurate photometric model for stationary spherical raindrops and determined the effect of camera parameters on image disturbance. However, a static observer or high camera exposure times are assumed making the approach not applicable in scenarios where the camera is moving. Kurihata et al. <ref type="bibr" target="#b11">[12]</ref> used a machine learning approach with raindrop templates, so called eigendrops, to detect raindrops on windshields. Results within the sky area were promising, whereas the proposed method produced a large number of false positives within the non-sky regions of the image where raindrop appearance modeling becomes more challenging.</p><p>Halimeh and Roser <ref type="bibr" target="#b8">[9]</ref> developed the basics of a photometric raindrop model to predict the appearance of a raindrop on the windshield. However, they only give experimental validation in a well-defined laboratory setting. They presently disregard the fact that raindrops appear blurred since they are outside the scene focus of the camera. Yamashita et al. <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b16">17]</ref> proposed several approaches for detecting and removing waterdrops from images. They detect noisy regions using difference images <ref type="bibr" target="#b18">[19]</ref> and replace contaminated image parts with patterns from a second camera, assuming a distant scene. Yamashita et al. further developed a spatio-temporal approach for detecting adherent noise by image sequence analysis <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b16">17]</ref>, supposing a camera with constant and known yaw rate.</p><p>In this paper, we present an integrated concept for raindrop detection (Section 2) that contains the generation of artificial raindrop patterns at regions of interest (ROIs), ROI initialization, and raindrop verification by intensity-based correlation. Then we omit disturbing image regions to improve registration accuracy. Thereto we use prior knowledge of the vehicle dynamics and camera setup in combination with homography constraints (Section 3). Finally, we make use of the image registration results and the accurately detected raindrop positions for image enhancement by restoring occluded image areas with intensity information from neighboring image frames. In section 4 we compare the different raindrop detection algorithms in terms of Precision-Recall curves and demonstrate improved image registration performance. Finally, we conclude this paper with an outlook on future work (Section 5).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Raindrop Detection</head><p>Fig. <ref type="figure" target="#fig_1">2</ref> shows an overview over the proposed raindrop detection approach. First, we extract ROIs from the image via interest point detection. In order to check if a region contains a raindrop, we use a photometric raindrop model which renders artificial raindrop patterns for each ROI. These patterns show the raindrop appearance at this location and scale in the image (See Fig. <ref type="figure" target="#fig_5">6</ref>). In a verification step, we compare the extracted region with the artificial raindrop pattern.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Photometric raindrop model</head><p>In this section, we give a brief overview of the raindrop model we used, called Raindrop Intelligent Geometric Scanner and Environment Constructor (RIGSEC) and present all necessary extensions. Our raindrop model is inspired by the model of <ref type="bibr" target="#b8">[9]</ref>, but we enhance the model by realistic out-of-focus blur and validate the model in realworld scenarios. Furthermore we compare several strategies for finding suitable regions of interest.</p><p>Given a potential raindrop, our algorithm models the geometric shape of that droplet on the car windshield. It utilizes its photometric properties and renders an artificial raindrop pattern from points in the environment.</p><p>Drop modeling can be performed with varying parametrizations of raising complexity like spherical, ellipsoidal or even more complex models. We use a sphere section, since for most raindrops, it models the shape sufficiently well (see Fig. <ref type="figure" target="#fig_0">1(a)</ref>).</p><p>Assuming a potential raindrop with radius r at position P = (x, y) on the image plane, we determine the part of the scene that will be observed through this raindrop. As depicted in Fig. <ref type="figure" target="#fig_2">3</ref>, a light ray emanating from a point in the environment will be refracted by the raindrop and the windshield and finally reaches point P on the image plane.</p><p>The exactly same point can also be seen directly at point P . By tracing the light rays passing through the raindrop, we are able to virtually render the appearance of any raindrop at any location. Snell's law describes the transition of light rays from medium 1 to medium 2. The angle of refraction μ 2 is a function of the inclination angle μ 1 and the refractive indices n 1 and n 2 . Hence, for all transitions A -E, the angle of refraction μ 2 can be calculated as</p><formula xml:id="formula_0">μ 2 = arcsin n 1 sin μ 1 n 2 . (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>Thus, an accurate geometric relationship between P and P can be derived. Fig. <ref type="figure" target="#fig_5">6</ref>(a) shows the sampling of points in the world for rendering an artificial raindrop as determined by our refraction model.</p><p>However, in order to compare the intensity values of the mapped environment to the potential raindrop, a prediction of the raindrop photometry is equally essential as the geometric ray tracing discussed above. According to <ref type="bibr" target="#b10">[11]</ref>, Fresnel's reflectivity coefficients for partly polarized sunlight in the atmosphere going from medium 1 to medium 2 can be expressed as</p><formula xml:id="formula_2">R 12 = 1 2 r 2 12 + r 2 12 ⊥<label>(2)</label></formula><formula xml:id="formula_3">r 12 ⊥ = n 1 cos μ 1 -n 2 cos μ 2 n 1 cos μ 1 + n 2 cos μ 2<label>(3)</label></formula><formula xml:id="formula_4">r 12 = n 1 cos μ 2 -n 2 cos μ 1 n 1 cos μ 2 + n 2 cos μ 1<label>(4)</label></formula><p>where μ 1 , μ 2 , n 1 , and n 2 are the angles and refractive indices of media 1 and 2 respectively. Since the pixel intensity I P is known from the camera image, the estimated raindrop intensity at point P can be determined as</p><formula xml:id="formula_5">I P = I P i (1 -R i ) j .<label>(5)</label></formula><p>Here R i denotes Fresnel's reflectivity coefficients at points A -E and j = ±1 stands for the direction of the intensity prediction. The geometric relations and the photometric properties allows an artificial raindrop pattern generation as depicted in Fig. <ref type="figure" target="#fig_5">6(c</ref>). The sharp optical imaging of an object onto a camera sensor by means of a thin lens can be described by</p><formula xml:id="formula_6">1 f = 1 b + 1 g (<label>6</label></formula><formula xml:id="formula_7">)</formula><p>where f is the focal length of the camera, g is the object distance and b is the distance between lens and image plane. Since video sensors for environment perception are adjusted for sharp optical imaging of distant objects, close scene points like raindrops on the windshield violate this assumption. They are not imaged sharply, but rather projected onto a disc with diameter in the image plane (see Fig. <ref type="figure">4</ref>). We model this effect by applying an out-of-focus blur using a disc kernel. According to Fig. <ref type="figure">4</ref>, the lens equation for out-of-focus objects is given by</p><formula xml:id="formula_8">1 f = 1 b + Δb + 1 g -Δg (<label>7</label></formula><formula xml:id="formula_9">)</formula><p>where Δg is known as depth of field. Using the geometric relation</p><formula xml:id="formula_10">D = Δb b + Δb<label>(8)</label></formula><p>with lens diameter D, equation 6 and 7 we get an expression for the disc kernel diameter</p><formula xml:id="formula_11">= Δgf 2 O(g -Δg)(g -f )<label>(9)</label></formula><p>where O = f D is the camera aperture size. Since the distance and relative orientation of the windshield with respect to the camera is known, can easily be computed for each point on the windshield as shown in Fig. <ref type="figure">5</ref>. The blurred raindrop pattern is illustrated in Fig. <ref type="figure" target="#fig_5">6(d)</ref>. Comparison to the original raindrop (Fig. <ref type="figure" target="#fig_5">6(b</ref>)) shows good visual agreement.</p><p>Rendering raindrops at multiple scales and all image locations is computationally expensive. On the other hand,   raindrop appearance is highly dependent from its backgound scenario, which lets simple filter masks or eigendrops as discussed in <ref type="bibr" target="#b11">[12]</ref> only perform insufficiently. For this reason, we developed a fast RIGSEC implementation that synthesizes a limited number of equally spaced raindrop temlates and correlates them in a surrounding region. Fig. <ref type="figure" target="#fig_6">7</ref> shows artificial raindrop patterns and the correlation areas for a 4 × 3 grid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Initialization</head><p>In order to further narrow the search space we first find ROIs using two different interest point detectors described in this section. In this paper we choose the SURF feature detector <ref type="bibr" target="#b0">[1]</ref> as a baseline, which approximates the second order image derivatives with box filters. By looking at the scale of the SURF filter response we also get an estimate for the raindrop size in addition to its location.</p><p>Gradient based feature detectors like SURF are not performing well for detecting raindrops, because blurred regions only respond weakly. On the other hand, decreasing the detection threshold leads to many false detections that must be rejected by the raindrop model. For this reason, we present a novel method for detecting blurred image regions by adaptive bandpass filtering. The intuition behind this is that raindrops can be located at specific spatial frequencies which are upper bounded by their out-of-focus blur.</p><p>We create a bandpass image B for each image I by computing the Difference of Gaussians (DoG) for each pixel location according to B(x, y) = (I ⊗ g σ-Δσ )(x, y) -(I ⊗ g σ+Δσ )(x, y)</p><p>where ⊗ denotes the convolution operator and g is a 2D-Gaussian. Here σ is taken from the out-of-focus blur map in Fig. <ref type="figure">5(b</ref>) and Δσ has experimentally be determined as 5px. ROIs are extracted by thresholding the bandpass image and finding segments using a connected component algorithm with a 4-neighborhood. Finally, an eigenvalue decomposition of the covariance matrix of all pixel positions belonging to a segment is performed. Convexity ratio (segment area to area of convex hull), dominant orientation (first eigenvector), and aspect ratio (first eigenvalue to second one) are extracted to distinguish raindrops from background features. In our experiments typical raindrops exhibit a convexity ratio of &gt; 0.9, possess an orientation difference of &lt; 15 • to the horizontal and have an aspect ratio between 1 and 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Raindrop verification</head><p>Verification of a raindrop candidate is performed by comparing the observed image region with the artificial created raindrop pattern in a small surrounding. We maximize the correlation coefficient of their intensity values CC intensity as well as the correlation coefficient of their first derivative (gradient map) CC gradient . In order to take different raindrop sizes into account, we slightly vary the scale of the artificial raindrop by a factor of 1 -1.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Registration and restoration</head><p>In this section we show how raindrop detections can be used to improve image registration accuracy. Furthermore, we fuse intensity information from multiple views into one single frame to restore image areas occluded by raindrops.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Geometric scene description</head><p>Approximating the road surface by planes enables us to describe perspective mappings via homographies <ref type="bibr" target="#b9">[10]</ref>.</p><p>Fig. <ref type="figure" target="#fig_7">8</ref> illustrates the geometric scene description. We keep the road-to-camera transformation matrix T RC and the calibration matrix K fixed in all frames and estimate the parameters of the road-to-road mappings T RR (Θ i ). The 6D-vector Θ i = (r x (i), r y (i), r z (i), t x (i), t y (i), t z (i)) represents the rotational and translational parameters between frame i and frame i + 1. The extrinsic and intrinsic parameters are determining the transformation from the road to the camera coordinate system T RC and the calibration matrix K. They are estimated using standard calibration techniques <ref type="bibr" target="#b19">[20]</ref> and kept constant over time.</p><p>However, only small parts of the scene (e.g. the road) can be represented well by a single plane. We tackle this problem by approximating the world as a box using five homographies, namely the ground plane, the left wall, the right wall, the sky and the background plane. Please note that depth is not represented accurately due to our simplified assumption. However, we model the distance to the left and right wall such that objects of interest (e.g. closeby trees and guardrails) are represented sufficiently well. We achieve satisfactory results in image restoration since only nearby image frames are considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Registration</head><p>We initially warp the road plane from frame i+1 to frame i using a prior estimate for Θ i (by means of a first order Markov model) and bilinear interpolation. This projection ensures feature similarity in appearance between frames. We then compute Harris corners in frame i + 1 that do not intersect with detected raindrops and search for correspondences in frame i using normalized cross-correlation. Because of low saliency in our test sequences we compute center-surround feature responses for the maxima in correlation space which are weighted by the template variance. Thereof we only keep the 50% best matches. Further outliers are removed by applying the Direct Linear Transform (DLT) algorithm <ref type="bibr" target="#b9">[10]</ref> in combination with RANSAC <ref type="bibr" target="#b3">[4]</ref> which is especially important in situations where other dynamic objects (e.g. vehicles) are present in the scene.</p><p>Refining the final parameter set Θ = {Θ i } N -1 i=1 is done by searching for the MAP solution of</p><formula xml:id="formula_12">P (Θ|Z 1 , ..., Z N ) ∝ P (Z 1 , ..., Z N |Θ)P (Θ)<label>(10)</label></formula><p>with respect to Θ, where N denotes the total number of frames. We assume independence of non-consecutive frames and therefore the likelihood P (Z 1 , ..., Z N |Θ) factorizes to</p><formula xml:id="formula_13">N i=1 P (Z i |Θ i )</formula><p>where the observation probabil-ity P (Z i |Θ i ) is assumed to be normally distributed</p><formula xml:id="formula_14">P (Z i |Θ) ∝ exp(- 1 2 d T i Σ -1 d i )</formula><p>with covariance matrix Σ = diag(σ 2 d , ..., σ 2 d ) T and d i containing the euclidean reprojection errors between the features connecting frame i and frame i + 1.</p><p>Further assuming independence in the translational and rotational parameters (r x , r y , r z , t x , t y , t z ) over the whole sequence gives P (Θ) = P (r x )P (r y )P (r z )P (t x )P (t y )P (t z ). <ref type="bibr" target="#b10">(11)</ref> Here the vector r x ∈ R N contains the pitch angle rate for all frames of the sequence, for example. Ideally one could use a uniform prior for the parameters. However, since in our setting the camera is highly tilted with respect to the registration surface, small registration errors have a large impact on the parameters. Thus we encourage smoothness by putting Gaussian process priors <ref type="bibr" target="#b13">[14]</ref> on the function space of the parameters</p><formula xml:id="formula_15">f (i) ∼ GP(μ f (i), σ f (i, i ))<label>(12)</label></formula><p>with f (i) ∈ {r x (i), r y (i), r z (i), t x (i), t y (i), t z (i)} where i denotes the frame number and r x (i) is the i'th entry of r x . For all f we fix the mean function to an initial estimate of the parameter μ f (i) ≡ f 0 and model the covariance function σ f (i, i ) using the squared exponential kernel.</p><p>Integrating the prior <ref type="bibr" target="#b10">(11)</ref> and the likelihood into <ref type="bibr" target="#b9">(10)</ref> and taking the logarithm yields the log-posterior log P (Θ|Z 1 , ..., Z N ) which we maximize using standard gradient descent techniques <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Multi-band blending</head><p>After estimating the parameters Θ, reconstruction of image areas covered by raindrops is possible using intensity information from nearby frames. Since simple merging of images leads to unsatisfactory results due to differences in gain, vignetting, object shadows and registration errors we use multiresolution splines <ref type="bibr" target="#b1">[2]</ref> which are outlined in the following section.</p><p>For each frame one base image and one base mask are generated. The base image contains all visible pixels warped into the target coordinate system using bilinear interpolation and the homography model from section 3.1, whereas the base mask contains the warped binary raindrop detection images. Combining base images is done by generating difference masks and applying multi-band blending <ref type="bibr" target="#b1">[2]</ref>. We compensate the gain in each contiguous area independently. This is done by comparing pixels around the area of interest and adjusting the mean intensity value.</p><p>Bandpass images B k i are generated by differencing the previously smoothed base images. Here the index i denotes that the image and its mask has been projected from frame i into the current frame. Please note that only neighboring frames (we typically used a neighborhood size of 6) are considered. For k ≥ 1 we have</p><formula xml:id="formula_16">W k i = W (k-1) i ⊗ g σ(k) I k i = I (k-1) i ⊗ g σ(k) B k i = I (k-1) i -I k i .</formula><p>where the standard deviation of the Gaussian kernel g σ(k) is set to σ(k) = √ 2k + 1σ. Combining overlapping regions <ref type="figure">,</ref><ref type="figure">y</ref>) finally yields the mosaic</p><formula xml:id="formula_17">I k Σ (x, y) = i W k i (x, y)I k i (x, y) i W k i (x, y) B k Σ (x, y) = i W k i (x, y)B k i (x, y) i W k i (x</formula><formula xml:id="formula_18">I mosaic = I Kσ Σ + K k=1 B k Σ</formula><p>with K the total number of bands.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>In this section we evaluate our approach using two types of experiments: Raindrop detection rates in terms of Precision-Recall and image registration errors with and without raindrop detection. The results are generated by applying our algorithm to an outdoor sequence of 302 frames recorded in a rural environment. We used monochrome images with a resolution of 1024 × 768. The camera system was mounted at a distance of ≈ 14cm from the windshield with a focal length of 6mm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Raindrop detection experiments</head><p>First, we perform experiments on raindrop detection for the different methods described in section 2. To this end we asked several persons for labeling elliptical regions which appear to contain raindrops within our 302 test frames. Detection results for the different methods are illustrated in figure <ref type="figure">9</ref>. We present results in terms of Precision-Recall for varying detection thresholds. Precision and recall are defined as precision = T P T P + F P recall = T P T P + F N <ref type="bibr" target="#b12">(13)</ref> where TP = true positives is the number of correctly discovered raindrops, FP = false positives is the number of detections which are no raindrops and FN = false negatives is the number of raindrops which are not detected. A detection is considered to be correct if its center is within the true raindrop. Other closeby detections are removed to avoid multiple true positives for one drop. The ground truth and two exemplary detection results for the image sequence are presented in Fig. <ref type="figure" target="#fig_0">10</ref>(a). We further position each frame in Precision-Recall space (Fig. <ref type="figure" target="#fig_0">10(b)</ref>) and take the mean value for each configuration to generate the Precision-Recall plots shown in Fig. <ref type="figure" target="#fig_0">10(c</ref>). Fig. <ref type="figure" target="#fig_0">10(c</ref>) compares the performance of the proposed algorithms SURF, BLUR, fastRIGSEC and a combination of BLUR and RIGSEC for varying parameters. SURF is tested with varying filter response thresholds. A recall close to 1 is achieved when keeping the threshold low. However, up to 2000 detected interest points lead to very low precision. Higher thresholds increase precision slowly but at the expense of decreasing recall. In contrast, BLUR shows a non-monotonic behaviour for varying parameters. The curve reaches its best value at a threshold of 0.05. Lower thresholds lead to an insufficient segmentation of raindrops in the binary image and hence show low precision and recall. Higher thresholds omit raindrops that are less distinctive. Recall rates up to 0.67 are achievable with this model. The fastRIGSEC algorithm uses a 4 × 3 grid and we vary the threshold for the intensity correlation coefficient CC intensity between 0.1 and 0.95. The best compromise of recall and precision with regards to subsequent image registration was found at values of 0.85 -0.9. As can be seen, fastRIGSEC dominates SURF and BLUR in performance, verifying the validity of the raindrop model. As depicted in Fig. <ref type="figure" target="#fig_0">10(c</ref>), we further combined BLUR with RIGSEC and varied the threshold for CC intensity , and CC gradient respectively. The presented results reveal that BLUR+RIGSEC clearly outperforms all other proposed methods. However, in terms of recall, fastRIGSEC can exceed BLUR+RIGSEC (at the cost of low precision). This is caused by the fact that RIGSEC only validates interest points. Hence, the recall of BLUR+RIGSEC is bounded by the recall of the BLUR model.</p><p>Please also note that neither temporal nor stereo information are used in our system. All tested algorithms can be applied to single monocular grayscale images in contrast to methods using multiple cameras <ref type="bibr" target="#b18">[19]</ref> or image sequence analysis <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b16">17]</ref> for raindrop detection. We believe that further improvements can be made by temporally integrating the respective methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Image registration experiments</head><p>In this experiment we compare different raindrop detection methods and their impact on image registration. For this purpose we remove all feature detections which are hidden by drops detected with one of the methods described in section 2 and perform image registration as described in section 3.2. We further gather ground truth information visually by maximizing equation 10 using hand selected fea-ror for all test scenarios. We observed that combining the BLUR detector with RIGSEC performed best, which is confirmed by the results in section 4.1. The ground truth acts as an approximate lower bound on the attainable registration error. It is nearly reached using the combined method which shows the effectiveness of our raindrop detection model on real data. We used the successful image registration provided by this method to reconstruct occluded image areas. An example is shown in Fig. <ref type="figure" target="#fig_0">1(d)</ref>. Please note that some artifacts have been left mainly in the image center. This stems from the fact that regions close to the vanishing point of the image are unobservable since no intensity information from neighboring frames can be used for reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Work</head><p>In this paper we have presented a novel approach that improves image registration accuracy in rainy weather conditions by considering raindrop detections. We compared different detectors and combined the complementary BLUR detector with the presented raindrop model. This combination revealed superior performance compared to the SURF baseline. We further showed that restoring occluded areas is possible by considering information from neighboring image frames and leads to improved visibility.</p><p>Since the sphere section assumption of our model does not cover all raindrop shapes, we plan to extend the raindrop model flexibility. Making use of temporal dependencies in raindrop location and appearance will further improve raindrop detection rates and hence registration results. We believe that this could pave the path for applying many vision algorithms even in rainy weather conditions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Raindrop model and image restoration. (a) shows a typical raindrop's appearance on the windshield. Modeling the refraction of light rays (b) allows for precisely detecting raindrops (c). After registering consecutive frames, occluded areas can be restored using the intensity from neighboring image frames (d).</figDesc><graphic coords="1,426.83,314.24,116.05,77.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Flowchart of raindrop detection with RIGSEC. Artificial raindrop patterns are compared to potential raindrops using intensity-based correlation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Refraction model. Light ray tracing allows for accurate reconstruction of raindrops on transparent surfaces from background scene information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .Figure 5 .</head><label>45</label><figDesc>Figure 4. Optical path for out-of-focus imaging. Objects that are out-of-focus are imaged blurred. If the camera is focusing point A, point B is imaged onto a disc with diameter in the image plane.</figDesc><graphic coords="3,417.11,193.16,99.55,60.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Artificial raindrop generation. (a) all pixels of a raindrop at a specified location and scale (red circle) are projected to the environment by our raindrop model (green dots). In (b) the original raindrop is shown, while (c) depicts our reconstruction using all points from (a) and photometric constraints. The result of applying out-of-focus blur to our reconstruction is shown in (d).</figDesc><graphic coords="4,57.60,302.96,218.52,102.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Fast RIGSEC algorithm with 4 × 3 artifical raindrops.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Geometric scene description. Transformations used for projecting between different coordinate systems.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2009" xml:id="foot_0"><p>IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops 978-1-4244-4441-0/09/$25.00 ©2009 IEEE</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgment</head><p>This work is partly supported by Honda R&amp;D Europe. The authors would also like to thank the Karlsruhe School of Optics and Photonics (KSOP) and the Collaborative Research Center (SFB/Tr28) "Cognitive Automobiles" granted by the Deutsche Forschungsgemeinschaft.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>tures and prior knowledge about the camera setup and the vehicle speed. Results are presented in terms of the mean translational and rotational error between frames.</p><p>Fig. <ref type="figure">10</ref>(d+e) show error histograms over the percentage of occluded road area for the respective methods. Both error measures increase with the number of raindrops when no raindrop detection is performed. This is due to the fact that raindrops are matched with high reliability in consecutive frames. Please note that location and appearance of raindrops are not reflected by this figure, leading to nonmonotonic curve characteristics. However, using our raindrop detection approach lowers the mean registration er-</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Surf</surname></persName>
		</author>
		<title level="m">Speeded up robust features. In 9th European Conference on Computer Vision</title>
		<meeting><address><addrLine>Graz Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-05">May 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A multiresolution spline with application to image mosaics</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Burt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="217" to="236" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Monoslam: Real-time single camera slam. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Molton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Stasse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1052" to="1067" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Readings in computer vision: issues, problems, principles, and paradigms</title>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="726" to="740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Detection and removal of rain from videos</title>
		<author>
			<persName><forename type="first">K</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2004-06">Jun 2004</date>
			<biblScope unit="volume">I</biblScope>
			<biblScope unit="page" from="528" to="535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">When does a camera see rain</title>
		<author>
			<persName><forename type="first">K</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2005-10">Oct 2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1067" to="1074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Vision and rain. Internatl</title>
		<author>
			<persName><forename type="first">K</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="27" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Monocular road masaicing for urban environments</title>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Vehicle Symposium</title>
		<meeting><address><addrLine>Xi&apos;an, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Raindrop detection on car windshields using geometric-photometric environment construction and intensity-based correlation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Halimeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Roser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Vehicle Symposium</title>
		<meeting><address><addrLine>Xi&apos;an, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Multiple View Geometry in Computer Vision</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">I</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
	<note>second edition</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Semiconductor Optics</title>
		<author>
			<persName><forename type="first">C</forename><surname>Klingshirn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
	<note>3. ed. edition</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rainy weather recognition from in-vehicle camera images for driver assistance</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kurihata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mekade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M Y</forename><surname>Tamatsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Miyahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="205" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A scaled conjugate gradient algorithm for fast supervised learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Moeller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="525" to="533" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<title level="m">Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning)</title>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A taxonomy and evaluation of dense two-frame stereo correspondence algorithms</title>
		<author>
			<persName><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="7" to="42" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Removal of adherent waterdrops from images acquired with a stereo camera system</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yamashita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kaneko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Miura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEICE -Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2021" to="2027" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>E89-D</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Removal of adherent noises from image sequences by spatio-temporal image processing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yamashita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fukuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kaneko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Miura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2008-05">2008. 2008. May 2008</date>
			<biblScope unit="page" from="2386" to="2391" />
		</imprint>
	</monogr>
	<note>ICRA</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Removal of adherent noises from images of dynamic scenes by using a pan-tilt camera. Intelligent Robots and Systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yamashita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kaneko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Miura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2004-10-02">2004. 2004. 2004. Sept.-2 Oct. 2004</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="437" to="442" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A virtual wiper -restoration of deteriorated images by using multiple cameras. Intelligent Robots and Systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yamashita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kuramoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kaneko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Miura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 2003 IEEE/RSJ International Conference on</title>
		<meeting>2003 IEEE/RSJ International Conference on</meeting>
		<imprint>
			<date type="published" when="2003-10">2003. 2003. Oct. 2003</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="3126" to="3131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Flexible camera calibration by viewing a plane from unknown orientations</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="666" to="673" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
