<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PESTO: PITCH ESTIMATION WITH SELF-SUPERVISED TRANSPOSITION-EQUIVARIANT OBJECTIVE</title>
				<funder>
					<orgName type="full">Sony France</orgName>
				</funder>
				<funder>
					<orgName type="full">ANRT</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Alain</forename><surname>Riou</surname></persName>
							<email>alain.riou@sony.com</email>
							<affiliation key="aff0">
								<orgName type="institution">LTCI</orgName>
								<address>
									<settlement>T?l?com-Paris</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Institut Polytechnique de Paris</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Sony Computer Science Laboratories -Paris</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Stefan</forename><surname>Lattner</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Sony Computer Science Laboratories -Paris</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ga?tan</forename><surname>Hadjeres</surname></persName>
							<affiliation key="aff3">
								<address>
									<settlement>Sony</settlement>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Geoffroy</forename><surname>Peeters</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">LTCI</orgName>
								<address>
									<settlement>T?l?com-Paris</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Institut Polytechnique de Paris</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PESTO: PITCH ESTIMATION WITH SELF-SUPERVISED TRANSPOSITION-EQUIVARIANT OBJECTIVE</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>? A. Riou</term>
					<term>S. Lattner</term>
					<term>G. Hadjeres and G. Peeters. Licensed under a Creative Commons Attribution 4.0 International License (CC BY 4.0). Attribution: A. Riou</term>
					<term>S. Lattner</term>
					<term>G. Hadjeres and G. Peeters</term>
					<term>&quot;PESTO: Pitch Estimation with Self-supervised Transpositionequivariant Objective&quot;</term>
					<term>in Proc. of the 24th Int. Society for Music Information Retrieval Conf.</term>
					<term>Milan</term>
					<term>Italy</term>
					<term>2023</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we address the problem of pitch estimation using Self Supervised Learning (SSL). The SSL paradigm we use is equivariance to pitch transposition, which enables our model to accurately perform pitch estimation on monophonic audio after being trained only on a small unlabeled dataset. We use a lightweight (&lt; 30k parameters) Siamese neural network that takes as inputs two different pitch-shifted versions of the same audio represented by its Constant-Q Transform. To prevent the model from collapsing in an encoder-only setting, we propose a novel class-based transposition-equivariant objective which captures pitch information. Furthermore, we design the architecture of our network to be transposition-preserving by introducing learnable Toeplitz matrices.</p><p>We evaluate our model for the two tasks of singing voice and musical instrument pitch estimation and show that our model is able to generalize across tasks and datasets while being lightweight, hence remaining compatible with lowresource devices and suitable for real-time applications. In particular, our results surpass self-supervised baselines and narrow the performance gap between self-supervised and supervised methods for pitch estimation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Pitch estimation is a fundamental task in audio analysis, with numerous applications, e.g. in Music Information Retrieval (MIR) and speech processing. It involves estimating the fundamental frequency of a sound, which allows to estimate its perceived pitch. Over the years, various techniques have been developed for pitch estimation, ranging from classical methods (based on signal processing) <ref type="bibr" target="#b1">[1]</ref><ref type="bibr" target="#b2">[2]</ref><ref type="bibr" target="#b3">[3]</ref><ref type="bibr" target="#b4">[4]</ref> to machine learning approaches <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b6">6]</ref>.</p><p>In recent years, deep learning has emerged as a powerful tool for a wide range of applications, outperforming classical methods in many domains. This is notably true in MIR, where deep learning has led to significant advances in tasks such as music transcription <ref type="bibr">[7]</ref><ref type="bibr" target="#b8">[8]</ref><ref type="bibr" target="#b9">[9]</ref>, genre classification <ref type="bibr" target="#b10">[10]</ref><ref type="bibr" target="#b11">[11]</ref><ref type="bibr" target="#b12">[12]</ref>, and instrument recognition <ref type="bibr" target="#b13">[13]</ref><ref type="bibr" target="#b14">[14]</ref><ref type="bibr" target="#b15">[15]</ref>. Pitch estimation has also benefited greatly from deep learning techniques <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b17">17]</ref>. However, these deep learning models often require a large amount of labelled data to be trained, and can be computationally expensive, hindering their practical applications in devices with limited computing power and memory capabilities. Additionally, these models are often task-specific and may not generalize well to different datasets or tasks <ref type="bibr" target="#b18">[18]</ref>. Therefore, there is a need for a lightweight and generic model that does not require labelled data to be trained. We address this here.</p><p>We take inspiration from the equivariant pitch estimation <ref type="bibr" target="#b19">[19]</ref> and the equivariant tempo estimation <ref type="bibr" target="#b20">[20]</ref> algorithms which we describe in part 2. As those, we use a SSL paradigm based on Siamese networks and equivariance to pitch transpositions (comparing two versions of the same sound that have been transposed by a random but known pitch shift). We introduce a new equivariance loss that enforces the model to capture pitch information specifically.</p><p>This work has the following contributions:</p><p>? we formulate pitch estimation as a multi-class problem (part 3.1); while <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b20">20]</ref> model pitch/tempo estimation as a regression problem,</p><p>? we propose a novel class-based equivariance loss (part 3.1) which prevents collapse; while <ref type="bibr" target="#b19">[19]</ref> necessitates a decoder,</p><p>? the architecture of our model is lightweight and transposition-equivariant by design. For this, we introduce Toeplitz fully-connected layers (part 3.4). We evaluate our method on several datasets and show that it outperforms self-supervised baselines on single pitch estimation (part 4.4.1). We demonstrate the robustness of our method to domain-shift and background music, highlighting its potential for real-world applications (part 4.4.2).</p><p>Our proposed method requires minimal computation resources and is thus accessible to a wide range of users for both research and musical applications. In consideration of accessibility and reproducibility, we make our code and pretrained models publicly available 1 .  <ref type="bibr" target="#b21">[21]</ref>. See section 2. The underlying idea is to generate two views of an input, feed them to a neural network, and train the network by applying a criterion between the output embeddings. Various techniques have been developed for generating views <ref type="foot" target="#foot_0">2</ref> .</p><p>Collapse. However, a major issue with these methods is "collapse", when all inputs are mapped to the same embedding. To address this, various techniques have been proposed. One of the most common is SimCLR <ref type="bibr" target="#b22">[22]</ref> which also uses negative samples to ensure that embeddings are far apart through a contrastive loss. Additionally, several regularization techniques have been developed that minimize a loss over the whole batch. Barlow Twins <ref type="bibr" target="#b23">[23]</ref> force the cross-correlation between embeddings to be identity, while VICReg <ref type="bibr" target="#b24">[24]</ref> add loss terms on the statistics of a batch to ensure that dimensions of the embeddings have high enough variance while remaining independent of each other. On the other hand, <ref type="bibr" target="#b25">[25]</ref> explicitly minimize a loss over the hypersphere to distribute embeddings uniformly. Furthermore, incorporating asymmetry between inputs has been shown to improve performance. <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b27">27]</ref> uses a momentum encoder, while <ref type="bibr" target="#b28">[28]</ref> and <ref type="bibr" target="#b29">[29]</ref> add a projection head and a stop-gradient operator on top of the network, with <ref type="bibr" target="#b28">[28]</ref> also using a teacher network. Finally, <ref type="bibr" target="#b30">[30]</ref> incorporates asymmetry to contrastive-and clustering-based representation learning.</p><p>Application to audio. While originally proposed for computer vision, these methods have been successfully adapted to audio and music as well. For example, <ref type="bibr" target="#b31">[31]</ref>, <ref type="bibr" target="#b32">[32]</ref>, and <ref type="bibr" target="#b33">[33]</ref> respectively adapted <ref type="bibr" target="#b22">[22]</ref>, <ref type="bibr" target="#b23">[23]</ref>, and <ref type="bibr" target="#b28">[28]</ref> to the audio domain. By training their large models on Au-dioSet <ref type="bibr" target="#b34">[34]</ref>, they aim at learning general audio representations that are suited for many downstream tasks. More specifically, <ref type="bibr" target="#b35">[35]</ref> successfully adapts contrastive learning to the task of music tagging by proposing more musicallyrelevant data augmentations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">SSL to learn equivariant representations.</head><p>The purpose of the methods described above is to learn a mapping f : X ? Y that is invariant to a set of transforms T X , i.e. so that for any input x ? X and transform t ? T X</p><formula xml:id="formula_0">f (t(x)) ? f (x)<label>(1)</label></formula><p>However, recent approaches <ref type="bibr" target="#b36">[36]</ref><ref type="bibr" target="#b37">[37]</ref><ref type="bibr" target="#b38">[38]</ref> try instead to learn a mapping f that is equivariant to T X , i.e. that satisfies</p><formula xml:id="formula_1">f (t(x)) ? t ? (f (x))<label>(2)</label></formula><p>where t ? ? T Y with T Y a set of transforms that acts on the output space Y. In other words, if the input is transformed, the output should be transformed accordingly. Representation collapse is hence prevented by design.</p><p>Equivariant representation learning has mostly been applied to computer vision and usually combines an invariance and an equivariance criterion. E-SSL <ref type="bibr" target="#b36">[36]</ref> trains two projection heads on top of an encoder, one to return projections invariant to data augmentations while the other predicts the parameters of the applied data augmentations. <ref type="bibr" target="#b37">[37]</ref> predicts separately a semantic representation and a rotation angle of a given input and optimizes the network with a reconstruction loss applied to the decoded content representation rotated by the predicted angle. Finally, SIE <ref type="bibr" target="#b38">[38]</ref> creates a pair of inputs by augmenting an input and learns equivariant representations by training a hypernetwork conditioned on the parameters of the augmentation to predict one embedding of the pair from the other.</p><p>Application to audio. Finally, a few successful examples of equivariant learning for solving MIR tasks recently emerged <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b20">20]</ref>. In particular, <ref type="bibr" target="#b20">[20]</ref> introduces a simple yet effective equivariance criterion for tempo estimation while preventing collapse without any decoder or regularization: pairs are created by time-stretching an input with two different ratios, then the output embeddings are linearly projected onto scalars and the network is optimized to make the ratio of the scalar projections match the time-stretching ratio within a pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Pitch estimation.</head><p>Monophonic pitch estimation has been a subject of interest for over fifty years <ref type="bibr" target="#b39">[39]</ref>. The earlier methods typically obtain a pitch curve by processing a candidate-generating function such as cepstrum <ref type="bibr" target="#b39">[39]</ref>, autocorrelation function (ACF) <ref type="bibr" target="#b40">[40]</ref>, and average magnitude difference function (AMDF) <ref type="bibr" target="#b41">[41]</ref>. Other functions, such as the normalized cross-correlation function (NCCF) <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b2">2]</ref> and the cumulative mean normalized difference function <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b42">42]</ref>, have also been proposed. On the other hand, <ref type="bibr" target="#b4">[4]</ref> performs pitch estimation by predicting the pitch of the sawtooth waveform whose spectrum best matches the one of the input signal.</p><p>Recently, methods involving machine learning techniques have been proposed <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b6">6]</ref>.</p><p>In particular, CREPE <ref type="bibr" target="#b16">[16]</ref> is a deep convolutional network trained on a large corpus to predict pitch from raw audio waveforms. SPICE <ref type="bibr" target="#b19">[19]</ref> is a self-supervised method that takes as inputs individual Constant-Q Transform (CQT) frames of pitchshifted inputs and learns the transposition between these inputs. It achieves quite decent results thanks to a decoder that takes as input the predicted pitch and tries to reconstruct the original CQT frame from it.</p><p>Finally, some works <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b44">44]</ref> aim at disentangling the pitch and timbre of an input audio, thus predicting pitch as a side effect. In particular, DDSP-inv <ref type="bibr" target="#b45">[45]</ref> is a DDSPbased approach <ref type="bibr" target="#b46">[46]</ref> that relies on inverse synthesis to infer pitch in a self-supervised way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">SELF-SUPERVISED PITCH ESTIMATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Transposition-equivariant objective</head><p>We focus on the problem of monophonic pitch estimation and model it as a classification task. Our model is com- posed of a neural network f ? that takes as input an audio signal x and returns a vector y = (y 0 , . . . , y i , . . . , y d-1 ) ? [0, 1] d , which represents the probability distribution of each pitch i. y i represents the probability that i is the pitch of x. We propose here to train f ? in a SSL way. For this, similarly to <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b29">29]</ref>, we use data augmentations and Siamese networks.</p><p>Given x, we first generate x (k) by pitch-shifting x by a known number k of semitones. Then, both x and x (k) are fed to f ? which is trained to minimize a loss function between y = f ? (x) and y (k) = f ? (x (k) ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition. For two vectors y, y</head><formula xml:id="formula_2">? ? R d and 0 ? k &lt; d, y ? is a k-transposition of y if and only if for all 0 ? i &lt; d ? ? ? ? ? y ? i+k = y i when 0 ? i &lt; d -k y ? i = 0 when i &lt; k y i = 0 when i ? d -k -1<label>(3)</label></formula><p>Similarly, for -d &lt; k ? 0, y ? is a k-transposition of y if and only if y is a -k-transposition of y ? .</p><p>The concept of k-transposition is illustrated in Figure <ref type="figure" target="#fig_0">1</ref>. Note also that for a vector y ? R d , exists at most one vector y ? ? R d that is a k-transposition of y. We can therefore refer to y ? as the k-transposition of this vector y.</p><p>Equivariance loss. We then design our criterion based on the following assumption: the probability of x to have pitch i is equal to the probability of x (k) to have pitch i+k, i.e. y i should be equal to y (k) i+k <ref type="foot" target="#foot_1">3</ref> . In other words, if x (k) is a pitch-shifted version of x, their respective pitch probability distributions should be shifted accordingly, i.e. y (k) should be the k-transposition of y.</p><p>We take inspiration from <ref type="bibr" target="#b20">[20]</ref> to design our equivariance loss. However, in our case, the output of our network f ? is not a generic representation but a probability distribution. We therefore adapt our criterion by replacing the learnable linear projection head from <ref type="bibr" target="#b20">[20]</ref> by the following deterministic linear form:</p><formula xml:id="formula_3">? : R d ? R y ? (?, ? 2 , . . . , ? d )y (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>where ? is a fixed hyperparameter <ref type="foot" target="#foot_2">4</ref> .</p><p>Indeed, with this formulation, for any k if y ? is a ktransposition of y then ?(y ? ) = ? k ?(y). Hence we define our loss as</p><formula xml:id="formula_5">L equiv (y, y (k) , k) = h ? ?(y (k) ) ?(y) -? k (5)</formula><p>where h ? is the Huber loss function <ref type="bibr" target="#b47">[47]</ref>, defined by</p><formula xml:id="formula_6">h ? (x) = x 2 2 if |x| ? ? ? 2 2 + ? (|x| -? ) otherwise<label>(6)</label></formula><p>Regularization loss. Note that if y (k) is the ktransposition of y then L equiv (y, y (k) , k) is minimal. However, the converse is not always true. In order to actually enforce pitch-shifted pairs of inputs to lead to ktranspositions, we further add a regularization term which is simply the shifted cross-entropy (SCE) between y and y (k) , i.e. the cross-entropy between the k-transposition of y and y (k) :</p><formula xml:id="formula_7">L SCE (y, y (k) , k) = d-1 i=0 y i log y (k) i+k (7)</formula><p>with the out-of-bounds indices replaced by 0. The respective contribution of L equiv and L SCE is studied in part 4.4.3.</p><p>Invariance loss. L equiv and L SCE allow our model to learn relative transpositions between different inputs and learn to output probability distributions y and y (k) that satisfy the equivariance constraints. However, these distributions may still depend on the timbre of the signal. This is because our model actually never observed at the same time two different samples with the same pitch.</p><p>To circumvent this, we rely on a set T of data augmentations that preserve pitch (such as gain or additive white noise). We create augmented views x = t(x) of our inputs x by applying random transforms t ? T .</p><p>Similarly to <ref type="bibr" target="#b35">[35]</ref>, we then train our model to be invariant to those transforms by minimizing the cross-entropy between y = f ? (x) and ? = f ? (x).</p><formula xml:id="formula_8">L inv (y, ?) = CrossEntropy(y, ?)<label>(8)</label></formula><p>Combining the losses. For a given input sample x and a given set of augmentations T ,</p><p>? we first compute x (k) by pitch-shifting x by a random number of bins k (the precise procedure is described in section 3.2); ? we then generate two augmented views x = t 1 (x) and</p><formula xml:id="formula_9">x(k) = t 2 (x (k) ), where t 1 , t 2 ? T ; ? we compute y = f ? (x), ? = f ? (x) and ?(k) = f ? (x (k) ).</formula><p>Our final objective loss is then:</p><formula xml:id="formula_10">L(y, ?, ?(k) , k) = ? inv L inv (y, ?) + ? equiv L equiv (?, ?(k) , k) + ? SCE L SCE (?, ?(k) , k)<label>(9)</label></formula><p>We illustrate this in Figure <ref type="figure" target="#fig_1">2</ref>. To set the weights ? * we use the gradient-based method proposed by <ref type="bibr" target="#b48">[48]</ref><ref type="bibr" target="#b49">[49]</ref><ref type="bibr" target="#b50">[50]</ref>. ). Then we compute x and x(k) by randomly applying pitch-preserving transforms to the pair. We finally pass x, x and x(k) through the network f ? and optimize the loss between the predicted probability distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Audio-frontend</head><p>The inputs x are the individual frames of the CQT. We have chosen the CQT as input since its logarithmic frequency scale, in which bins of the CQT exactly correspond to a fixed fraction b of pitch semitones, naturally leads to pitch-shifting by translation. CQT is also a common choice made for pitch estimation <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b51">51]</ref>.</p><p>To compute the CQT, we use the implementation provided in the nnAudio library <ref type="bibr" target="#b52">[52]</ref> since it supports parallel GPU computation. We choose f min = 27.5 Hz, which is the frequency of A0 the lowest key of the piano and select a resolution of b = 3 bins per semitone. Our CQT has in total K = 99b log-frequency bins, which corresponds to the maximal number of bins for a 16kHz signal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Simulating translations.</head><p>To avoid any boundary effects, we perform pitch-shift by cropping shifted slices of the original CQT input frame as in <ref type="bibr" target="#b19">[19]</ref>  5 . From a computational point of view, it is indeed significantly faster than applying classical pitch shift algorithms based on phase vocoder and resampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Transpostion-preserving architecture</head><p>The architecture of f ? is illustrated in Figure <ref type="figure" target="#fig_2">3</ref>. It is inspired by <ref type="bibr" target="#b17">[17]</ref>. Each input CQT frame is processed independently: first layer-normed <ref type="bibr" target="#b53">[53]</ref> then preprocessed by two 1D-Conv (convolution in the log-frequency dimension) with skip-connections <ref type="bibr" target="#b54">[54]</ref>, followed by four 1D-Conv layers. As in <ref type="bibr" target="#b17">[17]</ref>, we apply a non-linear leaky-ReLU (slope 0.3) <ref type="bibr" target="#b55">[55]</ref> and dropout (rate 0.2) <ref type="bibr" target="#b56">[56]</ref> between each convolutional layer. Importantly, the kernel size and padding of each of these layers are chosen so that the frequency resolution is never reduced. We found in practice that it helps the model to distinguish close but different 5 Specifically, we sample an integer k uniformly from the range {-kmax, . . . , kmax}, then generate two CQT outputs, denoted as x and x (k) , where x is obtained by cropping the input CQT at indices [kmax, K -kmax -1], and x (k) is obtained by cropping the input CQT at indices [kmax -k, K -kmax + k -1], with K the total number of bins of the original CQT frame and kmax = 16 in practice (see Figure <ref type="figure" target="#fig_1">2</ref>). pitches. The output is then flattened, fed to a final fullyconnected layer and normalized by a softmax layer to become a probability distribution of the desired shape. Note that all layers (convolutions <ref type="foot" target="#foot_3">6</ref> , elementwise nonlinearities, layer-norm and softmax), except the last final fully-connected layer, preserve transpositions. To make the final fully-connected layer also transposition-equivariant, we propose to use Toeplitz fully-connected layers. It simply consists of a standard linear layer without bias but whose weights matrix A is a Toeplitz matrix, i.e. each of its diagonals is constant. For DDSP-inv, we report the results when training and evaluating on the same dataset.</p><formula xml:id="formula_11">A = ? ? ? ? ? ? ? ? ? a 0 a -1 a -2 ? ? ? a -n+2 a -n+1 a 1 a 0 a -1 . . . . . . a -n+2 a 2 a 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . a m-1 ? ? ? ? ? ? ? ? ? ? ? ? a m-n ? ? ? ? ? ? ? ? ?<label>(</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Absolute pitch inference from y</head><p>Our encoder f ? returns a probability distribution over (quantized) pitches. From an input CQT frame x, we first compute the probability distribution f ? (x), then we infer the absolute pitch p by applying the affine mapping:</p><formula xml:id="formula_12">p(x) = 1 b (arg max f ? (x) + p 0 )<label>(11)</label></formula><p>where b = 3 is the number of bins per semitones in the CQT and p 0 is a fixed integer shift that only depends on f ? . As in <ref type="bibr" target="#b19">[19]</ref>, we set the integer shift p 0 by relying on a set of synthetic data 7 with known pitch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS 4.1 Datasets</head><p>To evaluate the performance of our approach, we consider the two following datasets: 1. MIR-1K <ref type="bibr" target="#b57">[57]</ref> contains 1000 tracks (about two hours) of people singing Chinese pop songs, with separate vocal and background music tracks provided. 2. MDB-stem-synth <ref type="bibr" target="#b58">[58]</ref> contains re-synthesized monophonic music played by various instruments. The pitch range of the MDB-stem-synth dataset is wider than the one of MIR-1K. The two datasets have different sampling rates and granularity for the annotations.</p><p>We conduct separate model training and evaluation on both datasets to measure overfitting and generalization performance. In fact, given that our model is lightweight and does not require labelled data, overfitting performance is particularly relevant for real-world scenarios, as it is easy for someone to train on their own dataset, e.g. their own voice. However, we also examine generalization performance through cross-evaluation to ensure that the model truly captures the underlying concept of pitch and does not merely memorize the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training details</head><p>From an input CQT (see part 3.2), we first compute the pitch-shifted CQT (see part 3.3). Then two random data augmentations t 1 , t 2 ? T are applied with a probability of 0.7. We used white noise with a random standard deviation between 0.1 and 2, and gain with a random value 7 synthetic harmonic signals with random amplitudes and pitch picked uniformly between -6 and 3 dB. The overall architecture of f ? (see part 3.4) is implemented in PyTorch <ref type="bibr" target="#b59">[59]</ref>.</p><p>For training, we use a batch size of 256 and the Adam optimizer <ref type="bibr" target="#b60">[60]</ref> with a learning rate of 10 -4 and default parameters. The model is trained for 50 epochs using a cosine annealing learning rate scheduler. Our architecture being extremely lightweight, training requires only 545MB of GPU memory and can be performed on a single GTX 1080Ti.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Performance metrics</head><p>We measure the performances using the following metrics.</p><p>1. Raw Pitch Accuracy (RPA): corresponds to the percentage of voiced frames whose pitch error<ref type="foot" target="#foot_4">8</ref> is less than 0.5 semitone <ref type="bibr" target="#b61">[61]</ref>. 2. Raw Chroma Accuracy (RCA): same as RPA but considering the mapping to Chroma (hence allowing octave errors) <ref type="bibr" target="#b61">[61]</ref>. RCA is only used in our ablation studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results and discussions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Clean signals</head><p>We compare our results with three baselines: CREPE <ref type="bibr" target="#b16">[16]</ref>, SPICE <ref type="bibr" target="#b19">[19]</ref> and DDSP-inv <ref type="bibr" target="#b45">[45]</ref>.</p><p>CREPE is fullysupervised while SPICE and DDSP-inv are two SSL approaches. To measure the influence of the training set, we train PESTO on the two datasets (MIR-1K and MDB-stemsynth) and also evaluate on the two. This allows to test model generalization.</p><p>We indicate the results in Table <ref type="table" target="#tab_1">1</ref>. We see that PESTO significantly outperforms the two SSL baselines (SPICE and DDSP-inv) even in the cross-dataset scenario (93.5% and 94.6%). Moreover, it is competitive with CREPE (-1.7% and -1.2%) which has 750 times more parameters and is trained in a supervised way on the same datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Robustness to background music</head><p>Background noise and music can severely impact pitch estimation algorithms, making it imperative to develop robust methods that can handle real-world scenarios where background noise is often unavoidable.</p><p>We therefore test the robustness of PESTO to background music. For this, we use the MIR-1K dataset, which contains separated vocals and background tracks and allows testing various signal-to-noise (here vocal-tobackground) ratios (SNRs).</p><p>We indicate the results in Table <ref type="table" target="#tab_2">2</ref>. As foreseen, the performance of PESTO when trained on clean vocals (row ? = 0) and applied to vocal-with-background considerably drop: from 94.8% (clean) to 50.0% (SNR = 0 dB) <ref type="foot" target="#foot_5">9</ref> .</p><p>To improve the robustness to background music, we slightly modify our method to train our model on mixed sources. Instead of using gain and white noise as data augmentations, we create an augmented view of our original vocals signal x vocals by mixing it (in the complex-CQT domain) with its corresponding background track x background :</p><formula xml:id="formula_13">x = x vocals + ?x background<label>(12)</label></formula><p>Then, thanks to L inv , the model is trained to ignore the background music for making its predictions. The background level ? is randomly sampled for each CQT frame. The influence of the distribution we sample ? from is depicted in Table <ref type="table" target="#tab_2">2</ref>. This method significantly limits the drop in performances observed previously and also makes PESTO outperform SPICE in noisy conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Ablation study</head><p>Table <ref type="table" target="#tab_3">3</ref> depicts the influence of our different design choices. First, we observe that the equivariance loss L equiv and the final Toeplitz fully-connected layer (eq.( <ref type="formula">10</ref>)) are absolutely essential for our model not to collapse. Moreover, data augmentations seem to have a negligible influence on out-of-domain RPA (-0.2%) but slightly help when training and evaluating on the same dataset (+1.2%).</p><p>On the other hand, it appears that both L inv and L SCE do not improve in-domain performances but help the model to generalize better. This is especially true for L SCE , whose addition enables to improve RPA from 86.9% to 94.6% on MDB-stem-synth.</p><p>Finally, according to the drop of performances in RPA and RCA when removing L inv , it seems that the invariance loss prevents octave errors on the out-of-domain dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>In this paper, we presented a novel self-supervised learning method for pitch estimation that leverages equivariance to musical transpositions. We propose a class-based equivariant objective that enables Siamese networks to capture pitch information from pairs of transposed inputs accurately. We also introduce a Toeplitz fully-connected layer to the architecture of our model to facilitate the optimization of this objective. Our method is evaluated on two standard benchmarks, and the results show that it outperforms self-supervised baselines and is robust to background music and domain shift. From a musical perspective, our lightweight model is well-suited for real-world scenarios, as it can run on resource-limited devices without sacrificing performance. Moreover, its SSL training procedure makes it convenient to fine-tune on a small unlabeled dataset, such as a specific voice or instrument. Additionally, the resolution of the model is a sixth of a tone but could eventually be increased by changing the resolution of the CQT. Moreover, despite modelling pitch estimation as a classification problem, we make no assumption about scale or temperament.</p><p>These features make our method still a viable solution, e.g. for instruments that use quartertones and/or for which no annotated dataset exists. We therefore believe that it has many applications even beyond the limitations of Western music.</p><p>Overall, the idea of using equivariance to solve a classification problem is a novel and promising approach that enables the direct return of a probability distribution over the classes with a single, potentially synthetic, labelled element. While our paper applies this approach to pitch estimation, there are other applications where this technique could be useful, such as tempo estimation.</p><p>Moreover, modelling a regression task as a classification problem can offer greater interpretability as the output of the network is not a single scalar but a whole probability distribution. Finally, it can generalize better to multi-label scenarios.</p><p>Our proposed method hence demonstrates the potential of using equivariance to solve problems that are beyond the scope of our current work. In particular, it paves the way towards self-supervised multi-pitch estimation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Example of k-transpositions. Visually, y and y ? are just translated versions of each other. The sign of k and its absolute value respectively indicate the direction and the distance of the translation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Overview of the PESTO method. The input CQT frame (log-frequencies) is first cropped to produce a pair of pitch-shifted inputs (x, x (k)). Then we compute x and x(k) by randomly applying pitch-preserving transforms to the pair. We finally pass x, x and x(k) through the network f ? and optimize the loss between the predicted probability distributions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Architecture of our network f ? . The number of channels varies between the intermediate layers, however the frequency resolution remains unchanged until the final Toeplitz fully-connected layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Evaluation results of PESTO compared to supervised and self-supervised baselines. CREPE has been trained in a supervised way on a huge dataset containing in particular MIR-1K and MDB-stem-synth. It is grayed out as a reference.</figDesc><table><row><cell>10)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Robustness of PESTO and other baselines to background music with various Signal-to-Noise ratios. Adding background music to training samples significantly improves the robustness of PESTO (see section 4.4.2).</figDesc><table><row><cell></cell><cell cols="4">Raw Pitch Accuracy (MIR-1K)</cell></row><row><cell>Model</cell><cell>clean</cell><cell>20 dB</cell><cell>10 dB</cell><cell>0 dB</cell></row><row><cell>SPICE [19]</cell><cell cols="4">91.4% 91.2% 90.0% 81.6%</cell></row><row><cell>PESTO</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>? = 0</cell><cell cols="4">94.8% 90.7% 79.2% 50.0%</cell></row><row><cell>? = 1</cell><cell cols="4">94.5% 94.2% 92.9% 83.1%</cell></row><row><cell>? ? U(0, 1)</cell><cell cols="4">94.7% 94.4% 92.9% 81.7%</cell></row><row><cell cols="5">? ? N (0, 1) 94.8% 94.5% 93.0% 82.6%</cell></row><row><cell cols="5">? ? N (0, 1 2 ) 94.8% 94.5% 92.9% 81.0%</cell></row><row><cell cols="5">CREPE [16] 97.8% 97.3% 95.3% 84.8%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Respective contribution of various design choices of PESTO for a model trained on MIR-1K.</figDesc><table><row><cell></cell><cell cols="2">MIR-1K</cell><cell cols="2">MDB</cell></row><row><cell></cell><cell>RPA</cell><cell>RCA</cell><cell>RPA</cell><cell>RCA</cell></row><row><cell>PESTO baseline</cell><cell cols="4">96.1% 96.4% 94.6% 95.0%</cell></row><row><cell>Loss ablations</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>w/o L equiv</cell><cell>5.8%</cell><cell>8.6%</cell><cell>1.3%</cell><cell>6.1%</cell></row><row><cell>w/o L inv</cell><cell cols="4">96.1% 96.4% 92.5% 94.5%</cell></row><row><cell>w/o L SCE</cell><cell cols="4">96.1% 96.5% 86.9% 93.8%</cell></row><row><cell>Miscellaneous</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">no augmentations 94.8% 95.4% 94.8% 95.2%</cell></row><row><cell>non-Toeplitz fc</cell><cell>5.7%</cell><cell>8.7%</cell><cell>1.2%</cell><cell>6.1%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>The most common technique involves randomly applying data augmentations to inputs to create pairs of inputs that share semantic content.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>For example, if k = 2 semitones, the probability of x to be C4 is exactly the probability of x (k) to be a D4, and the same holds for any pitch independently of the actual pitch of x.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>We found ? = 2 1/36 to work well in practice.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3"><p>Convolutions roughly preserve transpositions since the kernels are applied locally, meaning that if two transposed inputs are convolved by the same kernel, then the output results will be almost transpositions of each other as well</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_4"><p>i.e. distance between the predicted pitch and the actual one</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_5"><p>It should be noted that the difference between the 96.1% of Table1and the 94.8% of Table2is due to the fact that we do not apply any data augmentation (gain or additive white noise) when ? = 0.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6.">ACKNOWLEDGEMENTS</head><p>This work has been funded by the <rs type="funder">ANRT</rs> CIFRE convention n?2021/1537 and <rs type="funder">Sony France</rs>. This work was granted access to the <rs type="institution">HPC/AI resources of IDRIS</rs> under the allocation 2022-AD011013842 made by <rs type="institution">GENCI</rs>. We would like to thank the reviewers and meta-reviewer for their valuable and insightful comments.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A robust algorithm for pitch tracking (RAPT)</title>
		<author>
			<persName><forename type="first">D</forename><surname>Talkin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="495" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Acurate short-term analysis of the fundamental frequency and the harmonics-to-noise ratio of a sampled sound</title>
		<author>
			<persName><forename type="first">P</forename><surname>Boersma</surname></persName>
		</author>
		<ptr target="http://www.fon.hum.uva.nl/paul/papers/Proceedings_1993.pdf" />
	</analytic>
	<monogr>
		<title level="j">IFA Proceedings</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="97" to="110" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">PYIN: A fundamental frequency estimator using probabilistic threshold distributions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mauch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dixon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="659" to="663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A sawtooth waveform inspired pitch estimator for speech and music</title>
		<author>
			<persName><forename type="first">A</forename><surname>Camacho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Harris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1638" to="1652" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Noise robust pitch tracking by subband autocorrelation classification</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P W</forename><surname>Ellis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="707" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural Network Based Pitch Tracking in Very Noisy Speech</title>
		<author>
			<persName><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2158" to="2168" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Onsets and frames: Dual-objective piano transcription</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hawthorne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eck</surname></persName>
		</author>
		<ptr target="https://archives.ismir.net/ismir2018/paper/000019.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018. International Society for Music Information Retrieval</title>
		<meeting>the 19th International Society for Music Information Retrieval Conference, ISMIR 2018. International Society for Music Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2018-10">oct 2018</date>
			<biblScope unit="page" from="50" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adversarial learning for improved onsets and frames music transcription</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Bello</surname></persName>
		</author>
		<ptr target="https://archives.ismir.net/ismir2019/paper/000081.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019. International Society for Music Information Retrieval</title>
		<meeting>the 20th International Society for Music Information Retrieval Conference, ISMIR 2019. International Society for Music Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2019-06">jun 2019</date>
			<biblScope unit="page" from="670" to="677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">High-Resolution Piano Transcription with Pedals by Regressing Onset and Offset Times</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2010.01815v3" />
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3707" to="3717" />
			<date type="published" when="2021-10">oct 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Transfer learning for music genre classification</title>
		<author>
			<persName><forename type="first">G</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IFIP Advances in Information and Communication Technology</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">510</biblScope>
			<biblScope unit="page" from="183" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multimodal deep learning for music genre classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Oramas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Barbieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">Nieto</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Serra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the International Society for Music Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="21" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Music Genre Classification: A Review of Deep-Learning and Traditional Machine-Learning Approaches</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ndou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ajoodha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jadhav</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International IOT, Electronics and Mechatronics Conference (IEMTRONICS)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep convolutional networks on the pitch spiral for music instrument recognition</title>
		<author>
			<persName><forename type="first">V</forename><surname>Lostanlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Cella</surname></persName>
		</author>
		<ptr target="https://archives.ismir.net/ismir2016/paper/000093.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016. International Society for Music Information Retrieval</title>
		<meeting>the 17th International Society for Music Information Retrieval Conference, ISMIR 2016. International Society for Music Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2016-05">may 2016</date>
			<biblScope unit="page" from="612" to="618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep Convolutional Neural Networks for Predominant Instrument Recognition in Polyphonic Music</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="208" to="221" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Music instrument recognition using deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Solanki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pandey</surname></persName>
		</author>
		<idno type="DOI">10.1007/s41870-019-00285-y</idno>
		<ptr target="https://doi.org/10.1007/s41870-019-00285-y" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Information Technology</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1659" to="1668" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Crepe: A Convolutional Representation for Pitch Estimation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Bello</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1802.06182" />
	</analytic>
	<monogr>
		<title level="m">ICASSP, IEEE International Conference on Acoustics</title>
		<title level="s">Speech and Signal Processing -Proceedings</title>
		<imprint>
			<date type="published" when="2018-02">2018-April, feb 2018</date>
			<biblScope unit="page" from="161" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Deep-Learning Architectures for Multi-Pitch Estimation: Towards Reliable Evaluation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wei?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Peeters</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2202.09198" />
		<imprint>
			<date type="published" when="2022-02">feb 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Shortcut learning in deep neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
		</author>
		<ptr target="https://www.nature.com/articles/s42256-020-00257-z" />
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="665" to="673" />
			<date type="published" when="2020-04">apr 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">SPICE: Self-Supervised Pitch Estimation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gfeller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roblek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sharifi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Velimirovic</surname></persName>
		</author>
		<idno type="DOI">10.1109/TASLP.2020.2982285</idno>
		<ptr target="https://dl.acm.org/doi/pdf/10.1109/TASLP.2020.2982285" />
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1118" to="1128" />
			<date type="published" when="2020-10">oct 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Equivariant Self-Supervision for Musical Tempo Estimation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Quinton</surname></persName>
		</author>
		<ptr target="https://archives.ismir.net/ismir2022/paper/000009.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Society for Music Information Retrieval Conference, ISMIR 2022</title>
		<meeting>the 23rd International Society for Music Information Retrieval Conference, ISMIR 2022</meeting>
		<imprint>
			<date type="published" when="2022-09">sep 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v119/chen20j/chen20j.pdf" />
	</analytic>
	<monogr>
		<title level="m">37th International Conference on Machine Learning, ICML 2020</title>
		<imprint>
			<date type="published" when="2020-02">feb 2020</date>
			<biblScope unit="page" from="1575" to="1585" />
		</imprint>
	</monogr>
	<note>PartF16814. International Machine Learning Society (IMLS)</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Barlow Twins: Self-Supervised Learning via Redundancy Reduction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Deny</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v139/zbontar21a/zbontar21a.pdf" />
	</analytic>
	<monogr>
		<title level="m">38th International Conference on Machine Learning, ICML 2021</title>
		<imprint>
			<date type="published" when="2021-03">mar 2021</date>
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">VI-CReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bardes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=xm6YD62D1Ub" />
	</analytic>
	<monogr>
		<title level="m">The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event</title>
		<imprint>
			<date type="published" when="2022">April 25-29, 2022. OpenReview.net, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Understanding contrastive representation learning through alignment and uniformity on the hypersphere</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v119/wang20k/wang20k.pdf" />
	</analytic>
	<monogr>
		<title level="m">37th International Conference on Machine Learning, ICML 2020</title>
		<imprint>
			<date type="published" when="2020-05">may 2020</date>
			<biblScope unit="page" from="9871" to="9881" />
		</imprint>
	</monogr>
	<note>PartF16814. International Machine Learning Society (IMLS)</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Momentum Contrast for Unsupervised Visual Representation Learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<ptr target="https://openaccess.thecvf.com/content_CVPR_2020/papers/He_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2020-11">nov 2020</date>
			<biblScope unit="page" from="9726" to="9735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Improved Baselines with Momentum Contrastive Learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2003.04297" />
		<imprint>
			<date type="published" when="2020-03">mar 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent a new approach to self-supervised learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valko</surname></persName>
		</author>
		<ptr target="https://papers.nips.cc/paper/2020/file/f" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020-06">jun 2020</date>
		</imprint>
	</monogr>
	<note>2020-Decem. Neural information processing systems foundation. Online. 3ada80d5c4ee70142b17b8192b2958e-Paper.pdf</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Exploring simple Siamese representation learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<ptr target="https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Exploring_Simple_Siamese_Representation_Learning_CVPR_2021_paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021-11">nov 2021</date>
			<biblScope unit="page" from="15" to="745" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Improving Self-Supervised Learning by Characterizing Idealized Representations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<ptr target="https://openreview.net/pdf?id=agQGDz6gPOo" />
		<imprint>
			<date type="published" when="2022-09">sep 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Contrastive learning of general-purpose audio representations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Saeed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zeghidour</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2010.10915v1" />
	</analytic>
	<monogr>
		<title level="m">ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing -Proceedings</title>
		<imprint>
			<publisher>Institute of Electrical and Electronics Engineers Inc</publisher>
			<date type="published" when="2021-10">June. oct 2021</date>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="page" from="3875" to="3879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Audio Barlow Twins: Self-Supervised Audio Representation Learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Anton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Coppock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shukla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Schuller</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2209.14345" />
		<imprint>
			<date type="published" when="2022-09">sep 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">BYOL for Audio: Exploring Pre-Trained General-Purpose Audio Representations</title>
		<author>
			<persName><forename type="first">D</forename><surname>Niizumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Takeuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ohishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kashino</surname></persName>
		</author>
		<idno type="DOI">10.1109/TASLP.2022.3221007</idno>
		<ptr target="https://dl.acm.org/doi/pdf/10.1109/TASLP.2022.3221007" />
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2022-04">apr 2022</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Audio Set: An ontology and human-labeled dataset for audio events</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="776" to="780" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Contrastive Learning of Musical Representations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Spijkervet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Burgoyne</surname></persName>
		</author>
		<idno>ISMIR 2021</idno>
		<ptr target="https://archives.ismir.net/ismir2021/paper/000084.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Society for Music Information Retrieval Conference</title>
		<meeting>the 22nd International Society for Music Information Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2021-03">mar 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Equivariant Contrastive Learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Dangovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Solja?i?</surname></persName>
		</author>
		<ptr target="https://openreview.net/pdf?id=gKLAAfiytI" />
	</analytic>
	<monogr>
		<title level="m">ICLR 2022 -10th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022-10">oct 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Unsupervised Learning of Group Invariant and Equivariant Representations</title>
		<author>
			<persName><forename type="first">R</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bertolini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>No?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-A</forename><surname>Clevert</surname></persName>
		</author>
		<ptr target="https://openreview.net/pdf?id=47lpv23LDPr" />
		<imprint>
			<date type="published" when="2022-02">feb 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Selfsupervised learning of Split Invariant Equivariant representations</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<ptr target="https://openreview.net/pdf?id=2sIVxJ9Hp0" />
		<imprint>
			<date type="published" when="2023-02">feb 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Cepstrum pitch determination</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Noll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Journal of the Acoustical Society of America</title>
		<imprint>
			<date type="published" when="1967">1967</date>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="293" to="309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Real-Time Digital Hardware Pitch Detector</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Dubnowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Schafer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Rabiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2" to="8" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Average magnitude difference function pitch extractor</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Shaffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Freudberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Manley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="353" to="362" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">YIN, a fundamental frequency estimator for speech and music</title>
		<author>
			<persName><forename type="first">A</forename><surname>De Cheveign?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kawahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1917" to="1930" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Unsupervised disentanglement of pitch and timbre for isolated musical instrument sounds</title>
		<author>
			<persName><forename type="first">Y.-J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Cheuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nakano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Goto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Herremans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Society for Music Information Retrieval Conference</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unsupervised disentanglement of timbral, pitch, and variation features from musical instrument sounds with random perturbation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yoshii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Morishima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="709" to="716" />
		</imprint>
		<respStmt>
			<orgName>APSIPA ASC</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Self-Supervised Pitch Detection by Inverse Audio Synthesis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Swavely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hanoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hantrakul</surname></persName>
		</author>
		<author>
			<persName><surname>Hawthorne</surname></persName>
		</author>
		<ptr target="https://goo.gl/magenta/ddsp-inv" />
	</analytic>
	<monogr>
		<title level="m">Workshop on Self-Supervision in Audio and Speech at the 37th International Conference on Machine Learning (ICML 2020)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">DDSP: Differentiable Digital Signal Processing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hantrakul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<ptr target="https://openreview.net/pdf?id=B1x1ma4tDr" />
	</analytic>
	<monogr>
		<title level="m">The Eighth International Conference on Learning Representations, ICLR 2020</title>
		<imprint>
			<date type="published" when="2020-01">jan 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Robust Estimation of a Location Parameter</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Huber</surname></persName>
		</author>
		<ptr target="http://www.jstor.org/stable/2238020" />
	</analytic>
	<monogr>
		<title level="m">The Annals of Mathematical Statistics</title>
		<imprint>
			<date type="published" when="1964">1964</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="73" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">GradNorm: Gradient normalization for adaptive loss balancing in deep multitask networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v80/chen18a/chen18a.pdf" />
	</analytic>
	<monogr>
		<title level="m">35th International Conference on Machine Learning, ICML 2018</title>
		<imprint>
			<date type="published" when="2018-11">nov 2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1240" to="1251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Taming transformers for high-resolution image synthesis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
		<ptr target="https://openaccess.thecvf.com/content/CVPR2021/papers/Esser_Taming_Transformers_for_High-Resolution_Image_Synthesis_CVPR_2021_paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021-12">dec 2021</date>
			<biblScope unit="volume">878</biblScope>
			<biblScope unit="page" from="12" to="868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Value Function Decomposition for Iterative Design of Reinforcement Learning Agents</title>
		<author>
			<persName><forename type="first">J</forename><surname>Macglashan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Archer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Devlic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Seno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sherstan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Wurman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stone</surname></persName>
		</author>
		<ptr target="https://openreview.net/pdf?id=pNEisJqGuei" />
		<imprint>
			<date type="published" when="2022-06">jun 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A Lightweight Instrument-Agnostic Model for Polyphonic Note Transcription and Multipitch Estimation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Bittner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Bosch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Meseguer-Brocal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ewert</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2203.09893v2" />
	</analytic>
	<monogr>
		<title level="m">ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing -Proceedings</title>
		<imprint>
			<publisher>Institute of Electrical and Electronics Engineers Inc</publisher>
			<date type="published" when="2022-03">mar 2022</date>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="781" to="785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Herremans, &quot;nnAudio: An on-the-Fly GPU Audio to Spectrogram Conversion Toolbox Using 1D Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Cheuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Agres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="161" to="981" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Layer Normalization</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1607.06450" />
		<imprint>
			<date type="published" when="2016-07">jul 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<ptr target="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016-12">2016-Decem, dec 2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Empirical Evaluation of Rectified Activations in Convolutional Network</title>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1505.00853" />
		<imprint>
			<date type="published" when="2015-05">may 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">On the Improvement of Singing Voice Separation for Monaural Recordings Using the MIR-1K Dataset</title>
		<author>
			<persName><forename type="first">C.-L</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-S</forename><forename type="middle">R</forename><surname>Jang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="310" to="319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">An analysis/synthesis framework for automatic f0 annotation of multitrack datasets</title>
		<author>
			<persName><forename type="first">J</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Bittner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bonada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Bosch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>G?mez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Bello</surname></persName>
		</author>
		<idno>ISMIR 2017</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Society for Music Information Retrieval Conference</title>
		<meeting>the 18th International Society for Music Information Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="71" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">PyTorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>K?pf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<ptr target="https://pytorch.org/" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019-12">dec 2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
	<note>Neural information processing systems foundation</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6980" />
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR 2015 -Conference Track Proceedings</title>
		<imprint>
			<date type="published" when="2015-12">dec 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Melody transcription from music audio: Approaches and evaluation</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Poliner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Ehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>G?mez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Streich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1247" to="1256" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
