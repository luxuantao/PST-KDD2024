<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Human Parity on CommonsenseQA: Augmenting Self-Attention with External Attention</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yichong</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
							<email>chezhu@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Siqi</forename><surname>Sun</surname></persName>
							<email>siqi.sun@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hao</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
							<email>jfgao@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Zeng</surname></persName>
							<email>nzeng@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xuedong</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Human Parity on CommonsenseQA: Augmenting Self-Attention with External Attention</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most of today's AI systems focus on using selfattention mechanisms and transformer architectures on large amounts of diverse data to achieve impressive performance gains. In this paper, we propose to augment the transformer architecture with an external attention mechanism to bring external knowledge and context to bear. By integrating external information into the prediction process, we hope to reduce the need for ever-larger models and increase the democratization of AI systems. We find that the proposed external attention mechanism can significantly improve the performance of existing AI systems, allowing practitioners to easily customize foundation AI models to many diverse downstream applications. In particular, we focus on the task of Commonsense Reasoning, demonstrating that the proposed external attention mechanism can augment existing transformer models and significantly improve the model's reasoning capabilities. The proposed system, Knowledgeable External Attention for commonsense Reasoning (KEAR), reaches human parity on the open CommonsenseQA research benchmark with an accuracy of 89.4% in comparison to the human accuracy of 88.9%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Transformers <ref type="bibr" target="#b15">[Vaswani et al., 2017]</ref> have revolutionized many areas of AI with state-of-the-art performance in a wide range of tasks <ref type="bibr" target="#b4">[Devlin et al., 2018;</ref><ref type="bibr" target="#b4">Dosovitskiy et al., 2020]</ref>. The most notable and effective component in a Transformer model is the self-attention mechanism, which enables the model to dynamically leverage different parts of the input for computation, with no information loss for even the most distant parts of the input. With the success of pre-trained models <ref type="bibr" target="#b4">[Devlin et al., 2018;</ref><ref type="bibr">Liu et al., 2019]</ref>, the Transformer and its self-attention mechanism have been widely adopted as the cornerstone of foundation models trained on huge amounts of data <ref type="bibr" target="#b1">[Bommasani et al., 2021]</ref>.</p><p>One phenomenon found during the development of Transformer models is that models with larger sizes tend to have better learning abilities, especially when combined with largescale data. This has prompted the recent boom of super large Transformer models, ranging from BERT <ref type="bibr" target="#b4">[Devlin et al., 2018]</ref> with 110 million parameters, to <ref type="bibr">GPT-3 [Brown et al., 2020]</ref> with 175 billion parameters. Nevertheless, numerous studies have shown that the corresponding understanding and generation capabilities of these huge models are still behind humans <ref type="bibr" target="#b1">[Bommasani et al., 2021]</ref>. Furthermore, the sheer size of these models already poses serious practical challenges in utilization, deployment, interpretation, and environmental impact <ref type="bibr" target="#b10">[Patterson et al., 2021]</ref>. Thus, the recent "scaling-up" approach to Transformer-based NLP modeling is unsustainable and has been questioned in recent studies <ref type="bibr" target="#b1">[Bommasani et al., 2021]</ref>.</p><p>In this paper, we take a step back and examine the mechanism of current Transformer-based models. Self-attention was designed to allow the model to better analyze the inner structure of input data, and the model is trained to have its parameters grasp and memorize all the content and patterns of the training data. When the model is given a novel input X, the implicitly stored knowledge in the parameters about related information is activated to facilitate the analysis of X. This could partly explain why larger models pre-trained with more data have an advantage in performance.</p><p>While Transformer models process input by looking inward via self-attention, we propose to make the model look outward by providing it with related context and knowledge from various sources. We then let the model conduct selfattention on the input while also computing external attention to the knowledge (Figure <ref type="figure" target="#fig_0">1</ref>). As the context and knowledge can usually be stored in a non-parametric and symbolic way (e.g., plain text, knowledge graph and dictionary entries), even moderately-sized Transformer models can perform exceptionally well on NLP tasks. This approach allows one to shrink the size of Transformer-based foundation models, which is critical to the accessibility and democratization of AI technology. This approach is also analogous to the way humans conduct intelligence; we often resort to search engines, dictionaries, or information from other people to navigate the world.</p><p>Another benefit of external attention is that, as the related knowledge is stored outside of the model, practitioners can easily update the knowledge source to change the behavior of their models. For example, one could add or delete entries from a knowledge graph or rewrite certain paragraphs in Wikipedia. By explicitly representing knowledge, the decision process of the model becomes much more transparent and explainable.</p><p>In this paper, we use the commonsense reasoning task Com-monsenseQA <ref type="bibr" target="#b14">[Talmor et al., 2019]</ref> as a case study in lever-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question &amp; Candidate</head><p>ùë¨ <ref type="bibr">[ùë™ùë≥ùë∫]</ref> ùë¨ ùüé ùë¨ ùëµ ‚Ä¶  aging external attention to obtain and integrate information related to the input. Given a commonsense question and a choice, we retrieve knowledge from three external sources: a knowledge graph (ConceptNet), a dictionary (Wiktionary), and labeled training data (CommonsenseQA and 16 related QA datasets). The retrieved knowledge is directly appended to the input and sent to the language model with no revision to the underlying architecture. We show that with the proposed external attention, the accuracy of commonsense reasoning using a DeBERTa-xxlarge model <ref type="bibr" target="#b7">[He et al., 2020]</ref> can be significantly boosted from 83.8% to 90.8% on the dev set, while fine-tuned large-scale models like GPT-3 can only achieve 73.0%. The ensembled version of our model, Knowledgeable External Attention for commonsense Reasoning (KEAR), reaches an accuracy of 93.4% on the dev set and 89.4% on the test set, surpassing human performance (88.9%) for the first time <ref type="bibr" target="#b14">[Talmor et al., 2019]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Attention</head><p>The benefits of our approach extend beyond commonsense reasoning. First, the external attention dramatically reduces our system's dependence on large-scale models, i.e., achieving human parity with models up to 1.5B parameters. Second, the external information is obtained via computationally efficient methods, such as information retrieval and word matching, adding little computational cost to the main model. Third, the text-level concatenation of input and knowledge leads no change to the Transformer model, enabling existing systems to easily adopt this new external attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>We first describe our external attention framework in Sec 2.1. Next, we describe our external knowledge sources in Sec 2.2. Last, we present additional modeling techniques for improving commonsense reasoning in Sec 2.3.</p><p>Problem Formulation. We focus on the multiple-choice question answering task in this paper, where the goal is to select the correct answer from a given list c 1 , c 2 , ..., c n for a commonsense question q. The output of the model is a distribution P on {1, 2, ..., n}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">External Attention</head><p>Self Attention. The majority of recent language models are based on the Transformer architecture <ref type="bibr" target="#b15">[Vaswani et al., 2017]</ref>. One of the most important components in Transformer is the self-attention mechanism, which can be formulated as</p><formula xml:id="formula_0">Q = H l W q , K = H l W k , V = H l W v , A = QK T ‚àö d , H l+1 = softmax(A)V,<label>(1)</label></formula><p>where H l ‚àà R N √ód is the input hidden vectors to the l-th Transformer layer, W q , W k , W v ‚àà R d√ód are projection matrices, N is the input length and d is the hidden vector's dimension. The inputs to the first Transformer layer are usually the embeddings of the tokenized input text, denoted as</p><formula xml:id="formula_1">H 0 = X = [x 1 , x 2 , ..., x N ] 1 .</formula><p>In the multi-choice question answering context, the input text is a concatenation of the question and a specific choice.</p><p>External Attention. For commonsense question answering, the required information needed to answer the question is usually absent from the input. Thus, we need to integrate external knowledge into the model. In this work, we denote the extra knowledge in text format as</p><formula xml:id="formula_2">K = [x K 1 , x K 2 , ..., x K N k ].</formula><p>There are many ways to integrate the external knowledge into the model, such as using graph neural networks <ref type="bibr" target="#b9">[Lin et al., 2019]</ref>. In this paper we simply concatenate the knowledge to the input text:</p><formula xml:id="formula_3">H 0 = [X; K] = [x 1 , ..., x N , x K 1 , ..., x K N k ].</formula><p>The advantage of this input-level integration is that the existing model architecture does not need to be modified. Then, applying self-attention on H 0 can make the model freely reason between the knowledge text and the question/choices, therefore equipping the model with enhanced reasoning capacity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">External Knowledge Sources</head><p>The knowledge to append to the input for external attention is crucial for getting the correct prediction. For commonsense reasoning, we collect three external knowledge sources to complement the input questions and choices.</p><p>Knowledge Graph. Knowledge graphs (KG) contain curated facts that can help with commonsense reasoning which might not appear in a general corpus. We follow KCR<ref type="foot" target="#foot_2">2</ref> to retrieve relevant relation triples in the ConceptNet graph <ref type="bibr" target="#b13">[Speer et al., 2017]</ref>. Suppose the question entity is e q and the choice contains entity e c<ref type="foot" target="#foot_3">3</ref> . If there is a direct edge r from e q to e c in ConceptNet, we choose this triple (e q , r, e c ). Otherwise, we retrieve all the triples originating from e c . We score each triple j by the product of its confidence w j (provided by ConceptNet) and the defined relation type weight</p><formula xml:id="formula_4">t rj : s j = w j ‚Ä¢ t rj = w j ‚Ä¢ N Nr j</formula><p>, where r j is the relation type of j, N is the total number of triples originating from e c , N rj is the number of triples with relation r j among these triples. We then choose the triple with highest weight. Finally, if the selected triple is (e 1 , r, e 2 ), we denote the knowledge from the KG as</p><formula xml:id="formula_5">K KG (q, c) = [e 1 r e 2 ].</formula><p>Dictionary. Although pre-trained language models are exposed to large-scale text data, the long tail distribution of words means that the quality of a word's representation is highly dependent on that word's frequency in the pre-training corpus. Dictionaries, on the other hand, can provide accurate semantic explanation of words regardless of their frequency in datasets. To help understand key concepts in the question and answer, we follow DEKCOR <ref type="bibr">[Xu et al., 2021]</ref> to use the Wiktionary definitions of the question and answer concepts as external knowledge. For every concept, we fetch the first (most frequent) definition from Wiktionary using its closest lexical match. Let d q be the definition text for e q and d c be the definition text for e c , we denote the dictionary knowledge as K dict (q, c) = [e q : d q ; e c :</p><formula xml:id="formula_6">d c ].</formula><p>Training Data. Although recent language models are giant in terms of the number of parameters, recent studies show that they cannot perfectly memorize all the details of their training data <ref type="bibr" target="#b16">[Wang et al., 2022]</ref>.</p><p>To tackle this challenge, we propose to retrieve relevant questions and answers from the training data as additional knowledge. We use BM25 <ref type="bibr" target="#b12">[Sch√ºtze et al., 2008]</ref> to retrieve top M relevant questions and answers from the training data. For each question q, we index the concatenation of the question text, the ground-truth choice c * , ConceptNet triples and Wiktionary definitions:</p><formula xml:id="formula_7">[q; c * ; K KG (q, c * ); K dict (q, c * )].</formula><p>For a new question q ‚Ä≤ and a potential choice c ‚Ä≤ , we similarly build a query [q ‚Ä≤ ; c ‚Ä≤ ; K KG (q ‚Ä≤ , c ‚Ä≤ ); K dict (q ‚Ä≤ , c ‚Ä≤ )] to retrieve most similar questions from the training set. For each retrieved question from the training data, we drop the knowledge part and employ the retrieved question and the ground-truth answer as external knowledge. Suppose the retrieved questions and (correct) answers are {(q 1 , c * 1 ), (q 2 , c * 2 ), ..., (q M , c * M )}, we denote the knowledge from training data as</p><formula xml:id="formula_8">K train = [q 1 c * 1 ; q 2 c * 2 ; ‚Ä¢ ‚Ä¢ ‚Ä¢ ; q M c * M ].</formula><p>During training, for each question q we filter itself from the retrieved results to avoid data leakage.</p><p>Different from <ref type="bibr" target="#b16">Wang et al. [2022]</ref> where the retrieval questions are only obtained from the same dataset, we experiment with three sources of training data for retrieval: i) CSQA training data, ii) CSQA+OBQA+RiddleSense, a small collection of datasets focusing on ConceptNet knowledge, and iii) a pool of 17 datasets focusing on commonsense reasoning (we describe details of these 17 datasets in the Appendix). Since most datasets do not provide the question and choice entity e q , e c for every question-choice pair, we use entity linking to find all entities E q = {e (1) q , ..., e</p><formula xml:id="formula_9">(nq) q }, E c = {e (1) c , ..., e (nc) c</formula><p>} appearing in the question and choice text respectively. We select the entity with the maximum length in E q and E c as the question and choice entity for Wiktionary definitions. For ConceptNet triples, we find edges between E q and E c and choose the one with the maximum total length.</p><p>Finally, we concatenate the retrieved knowledge from our three sources to form a final knowledge input: K = [K KG ; K dict ; K train ]. In practice, the semicolon is replaced by the separator token (e.g., <ref type="bibr">[SEP]</ref>). We name our knowledge retrieval and integration technology as Knowledgeable External Attention for commonsense Reasoning (KEAR), shown in Figure <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">General Methods to Improve Commonsense Reasoning</head><p>Prior works have proposed other methods to improve general NLU performance, and it is therefore natural to wonder if these methods also work for commonsense reasoning. Here, we explore two general methods for improving commonsense reasoning performance: i) using different text encoders and ii) virtual adversarial learning.</p><p>Text Encoders. Previous methods for natural language understanding (NLU) have tried using BERT <ref type="bibr" target="#b4">[Devlin et al., 2018]</ref>, RoBERTa <ref type="bibr">[Liu et al., 2019]</ref>, <ref type="bibr">ALBERT [Lan et al., 2019]</ref>, T5 <ref type="bibr" target="#b11">[Raffel et al., 2019]</ref>, ELECTRA <ref type="bibr" target="#b3">[Clark et al., 2020]</ref> and DeBERTa <ref type="bibr" target="#b7">[He et al., 2020]</ref> as the text encoder, achieving state-of-the-art performance on the GLUE benchmark <ref type="bibr" target="#b16">[Wang et al., 2019]</ref>. Thus, we evaluate these models as encoders for the commonsense reasoning task.</p><p>Virtual Adversarial Training (VAT). Previous works show that virtual adversarial training (VAT) can improve the performance for general NLU and question answering tasks <ref type="bibr" target="#b7">[Jiang et al., 2020]</ref>. In the multiple-choice commonsense reasoning task, the goal is to minimize the cross-entropy loss:</p><formula xml:id="formula_10">min Œ∏ E (x,y)‚àºD [CE(f (x; Œ∏), y)]<label>(2)</label></formula><p>where f produces the model prediction (distribution P on the choices), Œ∏ represents the model parameters, y is the one-hot ground-truth answer vector, CE is cross-entropy, and D is the empirical data distribution. VAT first finds the update Œ¥ that leads to the largest change in the predicted distribution, subject to a L 2 -norm constraint. Then, a consistency regularization loss term is added to minimize the difference in the function's output when compared to the input variation Œ¥:</p><formula xml:id="formula_11">min Œ∏ E (x,y)‚àºD [CE(f (x; Œ∏), y)+ (3) Œ± max ‚à•Œ¥‚à•2‚â§Œµ CE(f (x; Œ∏), f (x + Œ¥; Œ∏))],</formula><p>where Œ± and Œµ are hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We present our empirical results in this section. Each of our three external knowledge sources can boost the commonsense reasoning performance, and combining all the three techniques helps us reach the human parity on the CommonsenseQA benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Setup</head><p>Data. We focus on the CommonsenseQA (CSQA, Model Setup. We feed the input text into a pretrained text encoder (e.g., DeBERTa) and take the representation v ‚àà R d of the [CLS] token, where d is the dimension of the encoder.</p><p>We set the segment id as 0 for the question and answer text, and 1 for the appended knowledge text. The position embedding simply runs from 1 to l, where l is the total input length including the external knowledge. The embedding of [CLS] is projected to a scalar with learned weights and the final prediction is computed via a softmax over all five choices for a question. We then minimize the cross entropy error during training. Implementation Details. We finetune the model using the AdamW optimizer <ref type="bibr">[Loshchilov and Hutter, 2017]</ref>. The batch size is set to 48 or smaller to fit the batch onto a single GPU. We train the model for 10 epochs and take the best result on the dev set. We choose the weight decay in {0, 0.01, 0.1}. The learning rate are chosen from {1e ‚àí 5, 2e ‚àí 5, 3e ‚àí 6} for all encoders except for DeBERTa; following the DeBERTa paper <ref type="bibr" target="#b7">[He et al., 2020]</ref> we use a smaller learning rate, chosen from {4e ‚àí 6, 6e ‚àí 6, 9e ‚àí 6}. We use the DeBERTa v2 model and choose from the pretrained model or model finetuned on MNLI. We also try out the recent DeBERTa V3 model <ref type="bibr" target="#b6">[He et al., 2021]</ref> which combines DeBERTa with adversarial pretraining. For VAT, we choose the weight multiplier Œ± ‚àà {0.1, 1.0, 10.0} and set input variation norm Œµ = 1e ‚àí 5 (see Eqn. 3). For retrieving from training data, we choose the data source with the best validation set performance from the three retrieval source datasets. We set number of retrieved questions M = 10. We run each experiment with 3 different seeds and present results from the best run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Effects of Individual Components</head><p>General Methods. As shown in Table <ref type="table" target="#tab_2">2</ref>, there is a positive correlation between general performance on NLI tasks and commonsense reasoning abilities on CommonsenseQA.</p><p>Notice that the fine-tuned GPT-3 model with 175 billion parameters could only achieve 73.0% on the dev set of Common-senseQA. Based on these results, we choose ELECTRA-large and DeBERTa variants <ref type="bibr" target="#b7">[He et al., 2020]</ref> as the encoders for subsequent experimentation.</p><p>For virtual adversarial training, we find that VAT can improve commonsense reasoning accuracy for ELECTRA, improving the result from 81.3% to 82.1%. It does not show much improvement for DeBERTa models in our experiment. Therefore, we apply VAT to ELECTRA in subsequent experiments. Effect of External Attention. As shown in Table <ref type="table" target="#tab_3">3</ref>, all of the proposed knowledge sources bring gains in commonsense reasoning accuracy across all base encoder models. The dictionary, knowledge graph and training data bring 0.5%, 2.1%, and 2.5% improvement, respectively, when DeBERTaV3-large <ref type="bibr" target="#b6">[He et al., 2021]</ref> is the base encoder model. We find that the best training data retrieval source depends on the exact encoders and the techniques applied, and we show a comparison in Table <ref type="table">4</ref>. In general, the 17-dataset pool achieves the best performance for DeBERTa, but for ELEC-TRA retrieving from the CSQA training set alone can get the best performance.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Combining the Techniques</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question</head><p>What is a treat that your dog will enjoy? Choices A) salad, B) petted, C) affection, D) bone, E) lots of attention KG dog Desires {petted, affection, bone, lots of attention} Dictionary dog: A mammal that has been domesticated for thousands of years. bone: A composite material making up the skeleton of most vertebrates. Training Data What do dogs like to eat? bones.</p><p>Table <ref type="table">7</ref>: Case study for the effect of external attention. We list two questions from the CSQA dataset with our retrieved knowledge (both retrieved training data questions are from CSQA). The correct answer is in bold, and our KEAR model selected the correct choice for both questions. We highlight the wrong choice that a DeBERTa 1.5B model chooses in italic. 0.5%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Case Study</head><p>We present two examples from CSQA in Table <ref type="table">7</ref> to illustrate how the model can reason between all retrieved knowledge sources to get the correct answer. For the first question, the knowledge graph helps rule out the wrong answer triangle since it does not have a surface. The dictionary and training data attention further confirms that a tetrahedron has four sides/faces, which is the correct answer. For the second question, again knowledge graph rules out "salad" since a dog does not desire salads. The dictionary attention results suggest that bones are important for a dog, and training data attention suggests that bones are good food for a dog. This leads to the correct answer (bone). This suggests that all three knowledge sources are critical for getting the correct answer. Having access to all three knowledge sources makes it easier for model reasoning to get the correct answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Many previous works have proposed ways of incorporating external knowledge sources into Transformer architectures. For commonsense question answering, specialized knowledge graphs like <ref type="bibr">ConceptNet [Speer et al., 2017]</ref> and ATOMIC <ref type="bibr">[Sap et al., 2019]</ref> are the most popular choices for external knowledge <ref type="bibr" target="#b2">[Chang et al., 2021;</ref><ref type="bibr" target="#b16">Yao et al., 2022;</ref><ref type="bibr" target="#b13">Song et al., 2021]</ref>. <ref type="bibr" target="#b9">Lin et al. [2019]</ref>construct a scheme graph from concepts in the question and choices and uses an LSTM to reason on paths between question and choice concepts. <ref type="bibr">Yasunaga et al. [2021]</ref> construct a joint graph containing the QA context and KG, then use graph neural networks to reason over the two knowledge sources.</p><p>Another line of work explores less structured knowledge such as Wikipedia and dictionaries for commonsense reasoning <ref type="bibr">[Xu et al., 2021;</ref><ref type="bibr" target="#b5">Guu et al., 2020]</ref>. <ref type="bibr" target="#b0">Bhakthavatsalam et al. [2020]</ref> combine the knowledge from ConceptNet, Word-Net, and other corpora to form 3.5M generic statements and show that this knowledge can help boost accuracy and explanation quality. <ref type="bibr" target="#b10">[Mitra et al., 2019]</ref> compares several ways of incorporating external knowledge from a relevant corpus for commensense question answering.</p><p>Recently, there are approaches to generate facts from pretrained language models to complement missing facts in the external knowledge source. <ref type="bibr" target="#b1">Bosselut et al. [2019]</ref> finetune a pretrained model on ATOMIC for commonsense knowledge graph completion. <ref type="bibr">Liu et al. [2021]</ref> directly prompt the <ref type="bibr">GPT-3 model [Brown et al., 2020]</ref> to get knowledge for reasoning.</p><p>Beyond commonsense reasoning, external knowledge can also help boost performance on other language processing tasks like open domain question answering <ref type="bibr" target="#b17">[Yu et al., 2021]</ref>, conversational <ref type="bibr">QA [Qin et al., 2019]</ref>, and text generation <ref type="bibr" target="#b16">[Yu et al., 2020]</ref>. Compared with prior work that uses extra modules (e.g., GNNs) or extra models (e.g., GPT-3), our external attention framework is extremely lightweight. It operates via a combination of non-parametric retrieval and text concatenation, which we show is highly effective, able to surpass human parity on the CommonsenseQA task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We propose external attention as a lightweight framework for retrieving and integrating external knowledge for language understanding. Compared with self-attention which benefits from ever-increasing model sizes, external attention can bring related information from external sources to supplement the input. We demonstrate that this strategy can lead to considerable gains in performance with little additional computational cost. By leveraging knowledge from knowledge graphs, dictionaries, and training data, we show that our technology, KEAR, achieves human parity on the CommonsenseQA benchmark for the first time. For future work, we will apply the technique to other NLP tasks to improve language model performance with external knowledge.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Our proposed method of Knowledgeable External Attention for commonsense Reasoning (KEAR). Related knowledge is retrieved from external sources, e.g., knowledge graph, dictionary and training data, using the input as the key and then integrated with the input. While additional external attention layers can be added to the Transformer blocks, we adopt text-level concatenation for external attention, incurring no structural change to the model architecture.</figDesc><graphic url="image-1.png" coords="2,252.17,299.88,63.53,77.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>External-Attention ConceptNet Definition Training Data ConceptNet Knowledge Retrieval OpenBookQA CommonsenseQA RiddleSense, Rainbow ‚Ä¶ Data from multiple datasets</head><label></label><figDesc></figDesc><table><row><cell cols="2">Score Prediction</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Self-Attention</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>+</cell><cell>+</cell><cell></cell><cell>+</cell><cell>ùë¨ ùüé ùíå +</cell><cell>ùë¨ ùüè ùíå +</cell><cell>‚Ä¶</cell><cell>ùë¨ ùíè ùíå +</cell><cell>‚Ä¶</cell><cell>ùíå ùë¨ ùëµ ùíå +</cell></row><row><cell>ùë∫ ùüé</cell><cell>ùë∫ ùüé</cell><cell>‚Ä¶</cell><cell>ùë∫ ùüé</cell><cell>ùë∫ ùüè</cell><cell>ùë∫ ùüè</cell><cell>‚Ä¶</cell><cell>ùë∫ ùüè</cell><cell>‚Ä¶</cell><cell>ùë∫ ùüè</cell></row><row><cell cols="4">What do people do while playing</cell><cell>Playing guitar,</cell><cell>Guitar: A musical</cell><cell></cell><cell cols="3">A man is seen what while</cell></row><row><cell></cell><cell cols="2">guitar? Singing</cell><cell></cell><cell>subevent, singing</cell><cell>instrument</cell><cell></cell><cell cols="3">playing the guitar? Singing.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>concepts, one human-picked concept, and one human-curated answer.We present details of the 17 datasets that we use for training data retrieval in Table1. All the datasets are multiple-choice or classification datasets related to commonsense reasoning, and we include dataset details in the appendix. The datasets used for training data retrieval. NLI stands for natural language inference, MC is multiple choice, MRC is machine reading comprehension, CLF is classification, NSP is next sentence prediction.</figDesc><table><row><cell>Talmor</cell></row></table><note>tion containing the subject and with the object as the correct answer, ii) pick the most distractive answer from the retrieved concepts, and iii) write another distractor for the question. The final question contains five choices, with one correct choice, two random retrieved</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>CSQA dev set accuracy for various encoders. We append the accuracy on MNLI dataset (in-domain) for each encoder as a reference. MNLI scores are from the corresponding GitHub repositories.</figDesc><table><row><cell>Encoder</cell><cell cols="3">CSQA MNLI #Para</cell></row><row><cell>Fine-tuned GPT-3</cell><cell>73.0</cell><cell>82.1</cell><cell>175B</cell></row><row><cell>RoBERTa-large</cell><cell>76.7</cell><cell>90.2</cell><cell>355M</cell></row><row><cell>ALBERT-xxlarge</cell><cell>81.2</cell><cell>90.6</cell><cell>235M</cell></row><row><cell>ELECTRA-base</cell><cell>75.0</cell><cell>88.8</cell><cell>110M</cell></row><row><cell>ELECTRA-large</cell><cell>81.3</cell><cell>90.9</cell><cell>335M</cell></row><row><cell>DeBERTa-xlarge</cell><cell>82.9</cell><cell>91.7</cell><cell>900M</cell></row><row><cell>DeBERTa-xxlarge</cell><cell>83.8</cell><cell>91.7</cell><cell>1.5B</cell></row><row><cell>DeBERTaV3-large</cell><cell>84.6</cell><cell>91.8</cell><cell>418M</cell></row><row><cell>T5-11B</cell><cell>83.5 1</cell><cell>91.3</cell><cell>11B</cell></row></table><note>1 : from Liu et al. [2021].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Applying external attention to different knowledge sources.</figDesc><table><row><cell></cell><cell cols="3">E-l+VAT D-xxl DV3-l</cell></row><row><cell>Base</cell><cell>82.1</cell><cell>83.8</cell><cell>84.6</cell></row><row><cell>+ KG</cell><cell>85.2</cell><cell>86.4</cell><cell>86.7</cell></row><row><cell>+ Dictionary</cell><cell>83.8</cell><cell>84.0</cell><cell>85.1</cell></row><row><cell>+ Training data</cell><cell>84.0</cell><cell>86.4</cell><cell>87.1</cell></row></table><note>E-l+VAT stands for ELECTRA-large with VAT, D-xxl stands for DeBERTa-xxlarge, DV3-l stands for DeBERTaV3-large.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Table 3 and 4 demonstrate the effectiveness of our proposed knowledge retrieval and concatenation methods. CSQA dev set results with different encoders and ensembles.</figDesc><table><row><cell>Model</cell><cell cols="4">CSQA 3-Data 17-Data</cell></row><row><cell>E-l+VAT</cell><cell cols="2">84.0</cell><cell>82.9</cell><cell>82.8</cell></row><row><cell>D-xxl</cell><cell cols="2">86.2</cell><cell>86.1</cell><cell>86.4</cell></row><row><cell>DV3-l</cell><cell cols="2">87.0</cell><cell>87.1</cell><cell>87.1</cell></row><row><cell>E-l+VAT + KG + Dict</cell><cell cols="2">88.5</cell><cell>88.2</cell><cell>87.1</cell></row><row><cell>D-xxl + KG + Dict</cell><cell cols="2">89.8</cell><cell>90.5</cell><cell>90.8</cell></row><row><cell>DV3-l + KG + Dict</cell><cell cols="2">91.0</cell><cell>91.2</cell><cell>91.2</cell></row><row><cell cols="5">Table 4: Performance on CSQA dev set of model w.r.t source of train-</cell></row><row><cell cols="5">ing data retrieval. E-l+VAT stands for ELECTRA-large with VAT,</cell></row><row><cell cols="5">D-xxl stands for DeBERTa-xxlarge, DV3-l stands for DeBERTaV3-</cell></row><row><cell>large.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell></cell><cell></cell><cell cols="2">Dev Acc(%)</cell></row><row><cell cols="3">ELECTRA-large + VAT + KEAR</cell><cell cols="2">88.7</cell></row><row><cell cols="2">DeBERTa-xxlarge + KEAR</cell><cell></cell><cell cols="2">90.8</cell></row><row><cell cols="3">DeBERTaV3-large + KEAR</cell><cell cols="2">91.2</cell></row><row><cell cols="3">Ensemble (39 models w/ KEAR)</cell><cell cols="2">93.4</cell></row><row><cell>Method</cell><cell cols="4">Single Ensemble</cell></row><row><cell>BERT+OMCS</cell><cell></cell><cell>62.5</cell><cell>-</cell></row><row><cell>RoBERTa</cell><cell></cell><cell>72.1</cell><cell>72.5</cell></row><row><cell cols="2">RoBERTa+KEDGN</cell><cell>-</cell><cell>74.4</cell></row><row><cell>ALBERT</cell><cell></cell><cell>-</cell><cell>76.5</cell></row><row><cell cols="2">RoBERTa+MHGRN</cell><cell>75.4</cell><cell>76.5</cell></row><row><cell>ALBERT + HGN</cell><cell></cell><cell>77.3</cell><cell>80.0</cell></row><row><cell>T5</cell><cell></cell><cell>78.1</cell><cell>-</cell></row><row><cell>UnifiedQA</cell><cell></cell><cell>79.1</cell><cell>-</cell></row><row><cell>ALBERT+KCR</cell><cell></cell><cell>79.5</cell><cell>-</cell></row><row><cell>ALBERT + KD</cell><cell></cell><cell>80.3</cell><cell>80.9</cell></row><row><cell>ALBERT + SFR</cell><cell></cell><cell>-</cell><cell>81.8</cell></row><row><cell>DEKCOR</cell><cell></cell><cell>80.7</cell><cell>83.3</cell></row><row><cell>Human</cell><cell></cell><cell>-</cell><cell>88.9</cell></row><row><cell>KEAR (ours)</cell><cell></cell><cell>86.1</cell><cell>89.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Results on test set from the leaderboard. The human performance is ensemble of 5 workers<ref type="bibr" target="#b14">[Talmor et al., 2019]</ref>.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>The overside or up-side of a flat object such as a table, or of a liquid. tetrahedron: A polyhedron with four faces. Training Data The four equal sides were all a smooth surface, he had carved and sanded a perfect what? tetrahedron.</figDesc><table><row><cell>Question</cell><cell>What has a surface with many sides?</cell></row><row><cell>Choices</cell><cell>A) tetrahedron, B) object, C) geometry problem, D) lake, E) triangle</cell></row><row><cell>KG</cell><cell>surface AtLocation {tetrahedron, object, geometry problem, lake}</cell></row><row><cell>Dictionary</cell><cell>surface:</cell></row><row><cell></cell><cell>Table 5 shows the results of KEAR, which combines the best</cell></row><row><cell></cell><cell>techniques in previous experiments, i.e., best encoders and</cell></row><row><cell></cell><cell>external attention to all knowledge sources, to further boost</cell></row><row><cell></cell><cell>the performance. The best single model (DeBERTaV3-large</cell></row><row><cell></cell><cell>+ KEAR) achieves 91.2% accuracy on the dev set. To get</cell></row><row><cell></cell><cell>the best performance, we train KEAR models with ELEC-</cell></row><row><cell></cell><cell>TRA large, DeBERTa xlarge (900M), xxlarge (1.5B) and</cell></row><row><cell></cell><cell>V3 large as encoders with 12 different seeds, resulting in</cell></row><row><cell></cell><cell>48 models in total. We rank the models by their dev set per-</cell></row><row><cell></cell><cell>formance as M 1 , M 2 , ..., M 48 . The ensemble prediction uses</cell></row><row><cell></cell><cell>a majority vote on individual predictions. We picked the first</cell></row><row><cell></cell><cell>N models such that the dev set performance of ensembling</cell></row><row><cell></cell><cell>M 1 , M 2 , ..., M N is the best. We ended up with 39 models with</cell></row><row><cell></cell><cell>12 ELECTRA models, 12 DeBERTaV3 models, 11 DeBERTa-</cell></row><row><cell></cell><cell>xxlarge models and 4 DeBERTa-xlarge models. Our ensemble</cell></row><row><cell></cell><cell>model reaches 93.4% accuracy on the dev set. Table 6 shows</cell></row><row><cell></cell><cell>the official leaderboard result on the hidden test set. Our en-</cell></row><row><cell></cell><cell>semble model exceeds the previously best DEKCOR model</cell></row><row><cell></cell><cell>by over 6% and exceeds the human performance (88.9%) by</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1">We do not differentiate between tokens and their embeddings in the following discussion. Following previous work, we prepend a [CLS] token to the input.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2">https://github.com/jessionlin/csqa</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3">In CommonsenseQA dataset, both eq and ec are provided. Otherwise, we use entity linking to find related knowledge graph nodes to the input text (see "Training Data" part later in this section).</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Genericskb: A knowledge base of generic statements</title>
		<author>
			<persName><surname>Bhakthavatsalam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00660</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Comet: Commonsense transformers for automatic knowledge graph construction</title>
		<author>
			<persName><surname>Bommasani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.07258</idno>
		<idno>arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2019">2021. 2021. 2019. 2019. 2020</date>
			<publisher>Tom B Brown</publisher>
			<pubPlace>Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>On the opportunities and risks of foundation models. Brown et al., 2020. et al. Language models are few-shot learners</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Incorporating commonsense knowledge graph in pretrained models for social commonsense tasks</title>
		<author>
			<persName><forename type="first">Chang</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Electra: Pre-training text encoders as discriminators rather than generators</title>
		<author>
			<persName><forename type="first">Clark</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><surname>Devlin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<idno>arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018. 2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>An image is worth 16x16 words: Transformers for image recognition at scale</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deberta: Decoding-enhanced bert with disentangled attention</title>
		<author>
			<persName><surname>Guu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08909</idno>
	</analytic>
	<monogr>
		<title level="m">REALM: Retrieval-augmented language model pre-training</title>
				<imprint>
			<date type="published" when="2020">2020. 2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>International Conference on Learning Representations</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Debertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing</title>
		<author>
			<persName><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.09543</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Smart: Robust and efficient fine-tuning for pre-trained natural language models through principled regularized optimization</title>
		<author>
			<persName><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="2177" to="2190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName><forename type="first">Lan</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11942</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Lin</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1909.02151</idno>
		<idno>arXiv:1711.05101</idno>
		<title level="m">Knowledge-aware graph networks for commonsense reasoning</title>
				<imprint>
			<publisher>Ilya Loshchilov and Frank Hutter</publisher>
			<date type="published" when="2017">2019. 2019. 2019. 2019. 2021. 2021. 2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Decoupled weight decay regularization</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Conversing by reading: Contentful neural conversation with on-demand machine reading</title>
		<author>
			<persName><forename type="first">Mitra</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08855</idno>
		<idno>arXiv:2104.10350</idno>
	</analytic>
	<monogr>
		<title level="m">Swaroop Mishra, and Chitta Baral. How additional knowledge can improve natural language commonsense question answering?</title>
				<imprint>
			<publisher>Kuntal Kumar Pal</publisher>
			<date type="published" when="2019">2019. 2019. 2021. 2021. 2019</date>
			<biblScope unit="page" from="5427" to="5436" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019. 2019. 2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3027" to="3035" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Atomic: An atlas of machine commonsense for if-then reasoning</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Introduction to information retrieval</title>
		<author>
			<persName><surname>Sch√ºtze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
			<publisher>Cambridge University Press Cambridge</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Knowledge augmented transformer for adversarial multidomain multiclassification multimodal fake news detection</title>
		<author>
			<persName><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI conference on artificial intelligence (AAAI)</title>
				<editor>
			<persName><forename type="first">Robyn</forename><surname>Speer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Joshua</forename><surname>Chin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Catherine</forename><surname>Havasi</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2017">2021. 2021. 2017. 2017</date>
			<biblScope unit="volume">462</biblScope>
			<biblScope unit="page" from="88" to="100" />
		</imprint>
	</monogr>
	<note>Conceptnet 5.5: An open multilingual graph of general knowledge</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Commonsenseqa: A question answering challenge targeting commonsense knowledge</title>
		<author>
			<persName><surname>Talmor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note>North American Chapter of the Association for Computational Linguistics (NAACL</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Training data is more valuable than you think: A simple and effective method by retrieving from training data</title>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2201.05742</idno>
		<idno>arXiv:2010.04389</idno>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
				<editor>
			<persName><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</editor>
		<imprint>
			<publisher>Wenhao Yu</publisher>
			<date type="published" when="2019">2019. 2019. 2022. 2022. 2021. 2021. 2022. 2022. 2021. 2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Qa-gnn: Reasoning with language models and knowledge graphs for question answering. A survey of knowledge-enhanced text generation</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Kg-fid: Infusing knowledge graph in fusionin-decoder for open-domain question answering</title>
		<author>
			<persName><forename type="first">Yu</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2110.04330</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
