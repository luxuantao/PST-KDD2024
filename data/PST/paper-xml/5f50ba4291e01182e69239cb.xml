<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CLOCs: Camera-LiDAR Object Candidates Fusion for 3D Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-09-02">2 Sep 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Su</forename><surname>Pang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Morris</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hayder</forename><surname>Radha</surname></persName>
						</author>
						<title level="a" type="main">CLOCs: Camera-LiDAR Object Candidates Fusion for 3D Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-09-02">2 Sep 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2009.00784v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>There have been significant advances in neural networks for both 3D object detection using LiDAR and 2D object detection using video. However, it has been surprisingly difficult to train networks to effectively use both modalities in a way that demonstrates gain over single-modality networks. In this paper, we propose a novel Camera-LiDAR Object Candidates (CLOCs) fusion network. CLOCs fusion provides a low-complexity multi-modal fusion framework that significantly improves the performance of single-modality detectors. CLOCs operates on the combined output candidates before Non-Maximum Suppression (NMS) of any 2D and any 3D detector, and is trained to leverage their geometric and semantic consistencies to produce more accurate final 3D and 2D detection results. Our experimental evaluation on the challenging KITTI object detection benchmark, including 3D and bird's eye view metrics, shows significant improvements, especially at long distance, over the state-of-the-art fusion based methods. At time of submission, CLOCs ranks the highest among all the fusion-based methods in the official KITTI leaderboard. We will release our code upon acceptance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Autonomous driving systems need accurate 3D perception of vehicles and other objects in their environment. Unlike 2D visual detection, 3D-based object detection enables spatial path planning for object avoidance and navigation. Compared to 2D object detection, which has been well-studied <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, 3D object detection is more challenging with more output parameters needed to specify 3D oriented bounding boxes around targets. In addition, LiDAR methods <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref> are hampered by typically lower input data resolution than video which has a large adverse impact on accuracy at longer ranges. Fig. <ref type="figure" target="#fig_0">1</ref> illustrates the difficulty in detecting vehicles from just a few points and no texture at long range. Human annotators use both the camera images together with the LiDAR point clouds to create the ground truth bounding boxes <ref type="bibr" target="#b9">[10]</ref>. This motivates multi-modal sensor fusion as a way to improve single-modal methods.</p><p>While sensor fusion has potential to address the shortcomings of video-only and LiDAR-only detections, finding an effective approach that improves on the state-of-the-art single modality detectors has been difficult. This is illustrated in the official KITTI 3D object detection benchmark leaderboard, where LiDAR-only based methods outperform most of the fusion based methods. Fusion methods can be divided into three broad classes: early fusion, deep fusion Su Pang, Daniel Morris and Hayder Radha are with the Department of Electrical and Computer Engineering, College of Engineering, Michigan State University, 220 Trowbridge Road, East Lansing, Michigan, 48824, United States. Email: pangsu@msu.edu, dmorris@msu.edu, radha@egr.msu.edu and late fusion, each with their own pros and cons. While early and deep fusion have greatest potential to leverage cross modality information, they suffer from sensitivity to data alignment, often involve complicated architectures <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, and typically require pixel-level correspondences of sensor data. On the other hand, late fusion systems are much simpler to build as they incorporate pre-trained, singlemodality detectors without change, an only need association at the detection level. Our late fusion approach uses muchreduced thresholds for each sensor and combines detection candidates before Non-Maximum Suppression (NMS). By leveraging cross-modality information, it can keep detection candidates that would be mistakenly suppressed by singlemodality methods.</p><p>We propose Camera-LiDAR Object Candidates Fusion (CLOCs) as a way to achieve improved accuracy for 3D object detection. The proposed architecture delivers the following contributions: The rest of the paper is organized as follows. We first review related work in section 2. Then, we introduce the motivation of our work and why we choose to fuse the detection candidates in section 3. In section 4, we illustrate our Camera-LiDAR Object Candidates (CLOCs) Fusion architecture and relevant details of our network. We report and analyse our experimental results on the KITTI dataset in section 5. In section 6, we conclude the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>The three main categories 3D object detection are based on (1) 2D images, (2) 3D point clouds and (3) both images and point clouds. Although 2D image-based methods are attractive for not requiring LiDAR, there is a large gap in 3D performance between these methods and those leveraging point clouds, and so here we focus on the latter two categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. 3D Detection Using 2D Images</head><p>Mousavian et al. <ref type="bibr" target="#b14">[15]</ref> leverage the geometric constraints between 2D and 3D bounding boxes to recover 3D information. <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref> estimate 3D object information by calculating the similarity between 3D objects and CAD models. <ref type="bibr" target="#b17">[18]</ref> and <ref type="bibr" target="#b18">[19]</ref> explore using stereo images to generate dense point cloud and conduct object detection using that cloud. These image-based methods are promising, but when compared to LiDAR-based techniques, they generate much less accurate 3D bounding boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. 3D Detection Using Point Cloud</head><p>Point-cloud techniques currently lead in popularity for 3D object detection. Compared to multi-modal fusion based methods, single sensor setup avoids multi-sensor calibration and synchronization issues. However, object detection performance at longer distance is still relatively poor. Methods vary by how they encode and learn features from raw point cloud. <ref type="bibr" target="#b4">[5]</ref> uses voxels to encode the raw point cloud, and 3D CNNs (Convolutonal Neural Networks) are applied to learn voxel features for classification and bounding box regression. SECOND <ref type="bibr" target="#b5">[6]</ref> is the upgrade version of <ref type="bibr" target="#b4">[5]</ref>, since raw LiDAR point cloud has very sparse data structure, it uses sparse 3D CNNs which reduces the inference time significantly. PointPillars <ref type="bibr" target="#b8">[9]</ref> uses PointNets <ref type="bibr" target="#b6">[7]</ref> in an encoder that represents point clouds organized in vertical columns (pillars) followed with a 2D CNN detection head to perform 3D object detection; it enables inference at 62 Hz; Compared with one-stage methods discussed above, PointRCNN <ref type="bibr" target="#b7">[8]</ref>, Fast PointRCNN <ref type="bibr" target="#b19">[20]</ref> and STD <ref type="bibr" target="#b20">[21]</ref> applies a two-stage architecture that first generate 3D proposals in a bottomup manner and then refines these proposals in a second stage. PV-RCNN <ref type="bibr" target="#b21">[22]</ref> leverages the advantages of both 3D voxel CNN and PointNet-based set abstraction to learn more discriminative features. Besides, Part-A 2 in <ref type="bibr" target="#b22">[23]</ref> explores predicting intra-object part locations (lower left, upper right, etc.) in the first stage, and such part locations can assist accurate 3D bounding box refinement in the second stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. 3D Detection Using Multi-modal Fusion</head><p>We focus on camera-LiDAR fusion methods in this section since this is the most common sensor setup for self-driving cars. Frustum PointNet <ref type="bibr" target="#b23">[24]</ref>, Pointfusion <ref type="bibr" target="#b12">[13]</ref> and Frustum ConvNet <ref type="bibr" target="#b24">[25]</ref> are the representatives of 2D driven 3D detectors, which exploit mature 2D detectors to generate 2D proposals and narrow down the 3D processing domain to the corresponding cropped region in the image. But the 2D image-based proposal generation might fail in some cases that could only be observed from 3D space. MV3D <ref type="bibr" target="#b10">[11]</ref> and AVOD <ref type="bibr" target="#b11">[12]</ref> project the raw point cloud into bird's eye view (BEV) to form a multi-channel BEV image. A deep fusion based 2D CNN is used to extract features from this BEV image as well as the front camera image for 3D bounding box regression. The overall performance of these fusion based methods is worse than LiDAR-only based methods. Possible reasons include: First, transforming raw point cloud into BEV image loses spatial information. Second, the crop and resize operation used in these algorithms in order to fuse feature vectors from different sensor modalities may destroy the feature structure from each sensor. Camera images are high-resolution dense data, while LiDAR point cloud are low-resolution sparse data, fusing these two different types of data structure is not trivial. Forcing feature vectors from 2D images and 3D LiDAR point cloud to have the same size or equal-length, then concatenating, aggregating or averaging them could result in inaccurate correspondence between these feature vectors and therefore is not the optimal way for fusing features. In order to fuse features from different sensor modalities with better correspondence, MMF <ref type="bibr" target="#b25">[26]</ref> adopts continuous convolution <ref type="bibr" target="#b13">[14]</ref> to build dense LiDAR BEV feature maps and do point-wise feature fusion with dense image feature maps. MMF is currently one of the best public multi-modal fusion based 3D detector according to the KITTI 3D/BEV object detection benchmark. However, it is still 2∼4% worse in moderate level than the best LiDARonly based detectors in KITTI leaderboard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. MOTIVATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. 2D and 3D Object Detection</head><p>We first introduce the basic concepts of 2D and 3D object detection used in this paper. 2D detection systems discussed in this paper take RGB images as input, and output classified 2D axis-aligned bounding boxes with confidence scores, as shown in Fig 2 <ref type="figure">.</ref> 3D detection systems generate classified oriented 3D bounding boxes with confidence scores, as shown in Fig 2 <ref type="figure">.</ref> In the KITTI dataset <ref type="bibr" target="#b9">[10]</ref> only rotation in z axis is considered (yaw angle), while rotations in x and y axis is set to zero for simplicity. Using calibration parameters of the camera and LiDAR, the 3D bounding box in the LiDAR coordinate can be accurately projected into the image plane, as shown in  Early fusion has the greatest opportunity for cross-modal interaction, but at the same time inherent data differences between modalities including alignment, representation, and sparsity are not necessarily well-addressed by passing them all through the same network.</p><p>Deep fusion addresses this issue by including separate channels for different modalities while still combining features during processing. This is the most complicated approach, and it is not easy to determine whether or not the complexity actually leads to real improvements; simply showing gain over single-modality methods is insufficient.</p><p>Late fusion has a significant advantage in training; single modality algorithms can be trained using their own sensor data. Hence, the multi-modal data does not need to be synchronized or aligned with other modalities. Only the final fusion step requires jointly aligned and labeled data. Additionally, the detection candidate data that late fusion operates on is compact and simple to encode for a network. Since late fusion prunes rather than creates new detections, it is important that the input detectors be tuned to maximize their recall rate rather than their precision. In practice, this implies that individual modalities (a) avoid the NMS stage, which may mistakenly suppress true detections. and (b) keep thresholds as low as possible.</p><p>In our late fusion framework, we incorporate all detection candidates before NMS in the fusion step to maximize the probability of extracting all potential correct detections. Our approach is data-driven; we train a discriminative network that receives as input the output scores and classifications of individual detection candidates, as well as spatial descriptions of the detection candidates. It learns from data how best to combine input detection candidates for a final output detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CAMERA-LIDAR OBJECT CANDIDATES FUSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Geometric and Semantic Consistencies</head><p>For a given frame of image and LiDAR data there may be many detection candidates of with various confidences in each modality from which we seek a single set of 3D detections and scores. Fusing these detection candidates requires an association between the different modalities (even if the association is not unique). For this we build a geometric association score and apply semantic consistency. These are described in more detail as follows. Geometric consistency An object that is correctly detected by both a 2D and 3D detector will have an identical bounding box in the image plane, see </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic consistency</head><p>Detectors may output multiple categories of objects, but we only associate detections of the same category during fusion. We avoid thresholding detections at this stage (or use very low thresholds), and leave thresholding to the final output based on the final fused score.</p><p>The two types of consistencies illustrated above is the fundamental concept used in our fusion network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Network Architecture</head><p>In this section we explain the preprocessing/encoding of fused data, the fusion network architecture and the loss function used for training. Fig. <ref type="figure">3</ref>: CLOCs Fusion network architecture. First, individual 2D and 3D detection candidates are converted into a set of consistent joint detection candidates (a sparse tensor, the blue box); Then a 2D CNN is used to process the non-empty elements in the sparse input tensor; Finally, this processed tensor is mapped to the desired learning targets, a probability score map, through maxpooling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Sparse Input Tensor Representation:</head><p>The goal of our encoding step is to convert all individual 2D and 3D detection candidates into a set of all consistent joint detection candidates which can be fed into our fusion network. The general output of a 2D object detector are a set of 2D bounding boxes in the image plane and corresponding confident scores. For k 2D detection candidates in one image can be defined as follows:</p><formula xml:id="formula_0">P 2D ={p 2D 1 , p 2D 2 , ...p 2D k }, p 2D i ={[x i1 , y i1 , x i2 , y i2 ] , s 2D i }<label>(1)</label></formula><p>P 2D is the set of all k detection candidates in one image, for i th detection p 2D i , x i1 , y i1 and x i2 , y i2 are the pixel coordinates of the top left and bottom right corner points from the 2D bounding box. s 2D i is the confident score. The output of 3D object are 3D oriented bounding boxes in LiDAR coordinate and confident scores. There are multiple ways to encode the 3D bounding boxes, in KITTI dataset <ref type="bibr" target="#b9">[10]</ref>, a 7-digit vector containing 3D dimension (height, width and length), 3D location (x,y,z) and rotation (yaw angle) is used. For n 3D detection candidates in one LiDAR scan can be defined as follows:</p><formula xml:id="formula_1">P 3D ={p 3D 1 , p 3D 2 , ...p 3D n }, p 3D i ={[h i , w i , l i , x i , y i , z i , θ i ] , s 3D i }<label>(2)</label></formula><p>where P 3D is the set of all n detection candidates in one LiDAR scan, for i th detection p 3D i , [h i , w i , l i , x i , y i , z i , θ i ] is the 7-digit vector for 3D bounding box. s 3D i is the 3D confident score. Note that we take 2D and 3D detections without doing NMS, as discussed in the previous section, some correct detections may be suppressed because of limited information from single sensor modality. Our proposed fusion network would reevaluate all detection candidates from both sensor modalities to make better predictions. For k 2D detections and n 3D detections, we build a k × n × 4 tensor T, as shown in Fig 3 <ref type="figure">.</ref> For each element T i,j , there are 4 channels denoted as follows:</p><formula xml:id="formula_2">T i,j = {IoU i,j , s 2D i , s 3D j , d j }<label>(3)</label></formula><p>where IoU i,j is the IoU between i th 2D detection and j th projected 3D detection, s 2D i and s 3D j are the confident scores for i th 2D detection and j th 3D detection respectively. d j represents the normalized distance between the j th 3D bounding box and the LiDAR in xy plane. Elements T i,j with zero IoU are eliminated as they are geometrically inconsistent.</p><p>The input tensor T is sparse because for each projected 3D detection, only few 2D detections intersect with it and so most elements are empty. The fusion network only needs to learn from these intersected examples. Because we take the raw predictions before NMS, k and n are large numbers, for SECOND <ref type="bibr" target="#b5">[6]</ref>, there are 70400 (200 × 176 × 2) predictions in each frame. It would be impractical to do 1 × 1 convolution on a dense tensor with this shape. We propose an implementation architecture to utilize the sparsity of tensor T and make the calculations much faster and feasible for large k and n values. Only non-empty elements are delivered to the fusion network for processing, shown in Fig. <ref type="figure">3</ref>. As we would discuss later, the indices of the non-empty elements (i, j) are important for further calculations, therefore the indices of these non-empty elements are saved in the cache, as shown in the blue box in Fig. <ref type="figure">3</ref>. Here noted that for projected 3D detection p j that has no 2D detection intersected, we still fill the last element in j th column T k,j in T with the available 3D detection information and set IoU k,j and s 2D k as -1. Because sometimes 3D detector could detect some objects that 2D detector couldn't and we do not want to discard these 3D detections. Setting the IoU and s 2D to -1 rather than 0 enables our network to distinguish this case from other examples with very small IoU and s 2D . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Network Details</head><p>The fusion network is a set of 1×1 2D convolution layers. We use Conv2D(c in , c out , k, s) to represent an 2 dimensional convolution operator where c in and c out are the number of input and output channels, k and s are the kernel size vector and stride respectively. We employ four convolution layers sequentially as Conv2D(4, 18, (1,1), 1), Conv2D(18, 36, (1,1), 1), Conv2D(36, 36, (1,1), 1) and Conv2D(36, 1, (1,1), 1), which yields a tensor of size 1 × p × 1 shown in Fig. <ref type="figure">3</ref>, where p is the number of non-empty elements in the input tensor T. Note that for the first three convolution layers, after each convolution layer applied, ReLU <ref type="bibr" target="#b26">[27]</ref> is used. Since we have saved the indices of these non-empty elements (i, j), as shown in Fig. <ref type="figure">3</ref> now we could build a tensor T out of shape k × n × 1 by filling p outputs based on the indices (i, j) and putting negative infinity elsewhere. Finally, this tensor is mapped to the desired learning targets, a probability score map of size 1 × n, through maxpooling in the first dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Loss</head><p>We use a cross entropy entropy loss for target classification, modified by the focal loss in <ref type="bibr" target="#b3">[4]</ref> with parameters α = 0.25 and γ = 2 to address the large class imbalance between targets and background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Training</head><p>The fusion network is trained using stochastic gradient descent (SGD). We use the Adam optimizer with an initial learning rate of 3 * 10 −3 and decay the learning rate by a factor of 0.8 for 15 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL RESULTS</head><p>In this section we present our experimental setup and results, including dataset, platform, performance results and analyses. For all experiments, we focus on the car class since it has the most training and testing samples in the KITTI <ref type="bibr" target="#b9">[10]</ref> dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset</head><p>Our fusion system is evaluated on the challenging 3D object detection benchmark KITTI dataset <ref type="bibr" target="#b9">[10]</ref> which has both LiDAR point clouds and camera images. There are 7481 training samples and 7518 testing samples. Ground truth labels are only available for training samples. For the evaluation of testing samples, one needs to submit the detection results to KITTI server. For experimental studies, we follow the convention in <ref type="bibr" target="#b27">[28]</ref> to split the original training samples into 3712 training samples and 3769 validation samples. We compare our method with sate-of-the-art multimodal fusion methods of 3D object detection on official test split of KITTI as well as validation split.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. 2D/3D Detector Setup</head><p>We apply our fusion network for a combination of different 2D and 3D detectors to demonstrate the flexibility of our proposed pipeline. The 2D detectors we used are: RRC <ref type="bibr" target="#b28">[29]</ref>, MS-CNN <ref type="bibr" target="#b29">[30]</ref> and Cascade R-CNN <ref type="bibr" target="#b30">[31]</ref>. The 3D detectors we incorporated are: SECOND <ref type="bibr" target="#b5">[6]</ref>, PointPillars <ref type="bibr" target="#b8">[9]</ref>, PointRCNN <ref type="bibr" target="#b7">[8]</ref> and PV-RCNN <ref type="bibr" target="#b21">[22]</ref>. While not the top performers within the KITTI leaderboard, we have selected these methods as they are the best currently-available open-source detectors. Our experiments show that CLOCs improves the performance of these detectors significantly. At the time of submission, CLOCs fusion of PV-RCNN with Cascade R-CNN, is ranked number 4 on KITTI 3D detection leaderboard, number 6 on Bird Eye View detection leaderboard, number 1 on 2D detection leaderboard, and outperforms all other fusion methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation Results</head><p>We evaluate the detection results on the KITTI test server. The IoU threshold for car is 0.7. All the instances are classified into three difficulty levels: easy, moderate and hard, based on their 2D bounding boxes' height, occlusion level and truncation level. Since KITTI has some restrictions on the number of submissions, we only show the results evaluated on the official KITTI test server from three fusion combinations of 2D and 3D detectors, which are SECOND <ref type="bibr" target="#b5">[6]</ref> and Cascade R-CNN <ref type="bibr" target="#b30">[31]</ref>, written as CLOCs SecCas, PointRCNN <ref type="bibr" target="#b7">[8]</ref> and Cascade R-CNN, as CLOCs PointCas, PV-RCNN <ref type="bibr" target="#b21">[22]</ref> and Cascade R-CNN, as CLOCs PVCas. All the other combinations are evaluated on the validation set. Table <ref type="table" target="#tab_1">I</ref> shows the performance of our fusion method on the KITTI test set through server submission.    shown in Table <ref type="table" target="#tab_1">I</ref>, compared to baseline methods SECOND, PointRCNN and PV-RCNN, fusion with Cascade R-CNN through our fusion network increases the performance in 3D and BEV object detection by a large margin. We evaluate the performance of all the combinations of 2D and 3D detectors on car class of KITTI validation set, the results are shown in Table <ref type="table" target="#tab_2">II</ref>. Compared to the corresponding baseline 3D detectors, our fusion methods have better performance in 3D and BEV detection benchmark. These results demonstrate the effectiveness as well as the flexibility of our fusion approach.</p><p>Table <ref type="table" target="#tab_3">III</ref> and Table <ref type="table" target="#tab_4">IV</ref> show the 3D and BEV evaluation results of pedestrian and cyclist on KITTI validation set. The IoU threshold for pedestrian and cyclist is 0.5. Here for 3D detectors, only SECOND <ref type="bibr" target="#b5">[6]</ref> and PointPillars <ref type="bibr" target="#b8">[9]</ref> publish their training configurations for class pedestrian and cyclist; for 2D detectors, only MSCNN <ref type="bibr" target="#b29">[30]</ref> does. Therefore, we only show the evaluation results based on SECOND, PointPillars and MSCNN. As shown in Table <ref type="table" target="#tab_3">III</ref> and Table <ref type="table" target="#tab_4">IV</ref>, our fusion method improves the detection performance by a large margin.   detectors such as SECOND and PointRCNN, while CLOCs could utilize 2D detections to improve the performance. Fig. <ref type="figure" target="#fig_7">5</ref> shows some qualitative results of our proposed fusion method on the KITTI <ref type="bibr" target="#b9">[10]</ref> test set. Red bounding boxes represent wrong detections (false positives) from SECOND that are deleted by our CLOCs, blue bounding boxes stand for missed detections from SECOND that are corrected by our CLOCs, green bounding boxes are correct detections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Score Scales</head><p>There are two common output scores for detectors: the first is a real number approximating the log likelihood ratio between target and clutter, and the second is a sigmoid transformation of this onto the range 0 to 1, so approximating a probability of target. We compare use of these in CLOCs in Table V and find improved performance using the log likelihood score. The primary reason for the poor performance for the normalized score is that it poorly approximates a probability of target (or precision). Using this score forces the fusion network to learn a non-linear correction, whereas the equivalent log likelihood score discrepancy is simple offset that can easily corrected by the fusion layer. If we instead use a fitted sigmoid to obtain better probabilistic outputs from the PointRCNN, then fusion works equally well with either input. In general we believe it is simpler to use a log likelihood output for each single-modality detector and fuse these.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Ablation Study</head><p>We evaluate the contribution of each channel and focal loss in our fusion pipeline. The four channel includes: IoU between 2D detections and projected 3D detections (IoU ), 2D confident score (s 2D ), 3D confident score (s 3D ) and normalized distance (d) between 3D bounding box and the LiDAR in xy plane. The results are shown in Table <ref type="table" target="#tab_6">VI</ref>.</p><p>IoU , as the measure of geometric consistency, is crucial to the fusion network. Without IoU , the association between 2D and 3D detections would be ambiguous and further lead to degrade performance. 2D confident score indicates the certainty of 2D detections, which could provide useful clues for the fusion. 3D confident score (s 3D ) plays the most important role among the four channels, because CLOCs generates new confident scores to all 3D detection candidates through fusion in which original 3D scores are highly important evidences. Closer objects usually are easier detected because there are more hits from LiDAR, the normalized distance (d) could be a useful indicator for this. Because there is a large imbalance between positives and negatives among the detection candidates, focal loss could address this issue and improve the detection accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>In this paper, we propose Camera-LiDAR Object Candidates Fusion (CLOCs), as a fast and simple way to improve performance of just about any 2D and 3D object </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Example #1 of car detection from single modality methods: (a) LiDAR-only detector, and (c) image-only detector, with our CLOCs fusion shown in (b) and (d). A second example, #2, is shown below. Dashed red box shows misses and solid red bounding box shows false positives. Our proposed CLOCs fusion can correct both of these errors.</figDesc><graphic url="image-7.png" coords="1,314.42,458.04,242.36,56.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Fig 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Fig.2: 2D and 3D object detection. An object that is correctly detected by both a 2D and 3D detector will have highly overlapped bounding boxes in the image plane.B. Why Fusion of Detection CandidatesFusion architectures can be categorized based on at what point during their processing features from different modalities are combined. Three general categories are (1) early fusion which combines data at the input, (2) deep fusion which has different networks for different modalities while simultaneously combining intermediate features, and (3) late fusion which processes each modality on a separate path and fuses the outputs in the decision level.Early fusion has the greatest opportunity for cross-modal interaction, but at the same time inherent data differences between modalities including alignment, representation, and sparsity are not necessarily well-addressed by passing them all through the same network.Deep fusion addresses this issue by including separate channels for different modalities while still combining features during processing. This is the most complicated approach, and it is not easy to determine whether or not</figDesc><graphic url="image-8.png" coords="3,78.48,352.96,195.88,116.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Fig 2, whereas false positives are less likely to have identical bounding boxes. Small errors in pose will result in a reduction of overlap. This motivates an image-based Intersection over Union (IoU) of the 2D bounding box and the bounding box of the projected corners of the 3D detection, to quantify geometric consistency between a 2D and a 3D detection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Our methods outperform all multi-modal fusion based works in moderate and hard level at the time of submission. Note that the official open-source code of PV-RCNN performs slightly worse than the private one owned by the PV-RCNN authors shown on the KITTI leaderboard, and our CLOCs PVCas result is based on the open-source PV-RCNN. The baseline PV-RCNN in Table I refers to the open-source PV-RCNN. As</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Average Precision (AP) based on distance. CLOCs SecCas, CLOCs PointCas and their corresponding baselines SECOND and PointRCNN are shown in this figure. Our CLOCs outperforms the baseline by a large margin especially in long distance (40 ∼ 50m).</figDesc><graphic url="image-11.png" coords="6,428.58,178.83,138.11,87.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4</head><label>4</label><figDesc>shows the average precision (AP) on KITTI validation set in different distance ranges. The distance is defined as the Euclidean distance in xy plane between objects and LiDAR. The blue bars are the APs for SECOND detector, the orange bars represent APs for our CLOCs SecCas. The yellow and purple bars show the APs of PointRCNN and CLOCs PointCas respectively. As shown in Fig. 4, APs for CLOCs is higher than the corresponding baselines in all distance ranges on both 3D and BEV detection benchmarks. The largest improvement is in 40 ∼ 50m. This is because the point clouds in long distance are too sparse for LiDAR-only</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Qualitative results of our CLOCs on KITTI test set compared to SECOND [6]. Red and blue bounding boxes are false and missed detections from SECOND respectively that are corrected by our CLOCs. Green bounding boxes are correct detections. The upper row in each image is the 3D detection projected to image, the others are 3D detections in LiDAR point clouds.</figDesc><graphic url="image-12.png" coords="7,54.00,56.06,489.61,222.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-9.png" coords="4,85.68,56.06,440.66,173.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Performance comparison of object detection with state-of-the-art camera-LiDAR fusion methods on car class of KITTI test set (new 40 recall positions metric). CLOCs fusion improves the baselines and outperforms other state-of-the-art fusion-based detectors. 3D and bird's eye view detection are evaluated by Average Precision (AP)</figDesc><table><row><cell>Detector</cell><cell>Input Data</cell><cell>easy</cell><cell>3D AP (%) moderate</cell><cell>hard</cell><cell cols="3">Bird's Eye View AP (%) easy moderate hard</cell></row><row><cell>SECOND (baseline) [6]</cell><cell>LiDAR</cell><cell>83.34</cell><cell>72.55</cell><cell cols="2">65.82 89.39</cell><cell>83.77</cell><cell>78.59</cell></row><row><cell>CLOCs SecCas (SECOND+Cascad R-CNN)</cell><cell cols="2">LiDAR+Img 86.38</cell><cell>78.45</cell><cell cols="2">72.45 91.16</cell><cell>88.23</cell><cell>82.63</cell></row><row><cell>Improvement (CLOCs SecCas over SECOND)</cell><cell>-</cell><cell>+3.04</cell><cell>+5.90</cell><cell cols="2">+6.63 +1.77</cell><cell>+4.46</cell><cell>+4.04</cell></row><row><cell>PointRCNN (baseline) [8]</cell><cell>LiDAR</cell><cell>86.23</cell><cell>75.81</cell><cell cols="2">68.99 92.51</cell><cell>86.52</cell><cell>81.39</cell></row><row><cell>CLOCs PointCas (PointRCNN+Cascad R-CNN)</cell><cell cols="2">LiDAR+Img 87.50</cell><cell>76.68</cell><cell cols="2">71.20 92.60</cell><cell>88.99</cell><cell>81.74</cell></row><row><cell>Improvement (CLOCs PointCas over PointRCNN)</cell><cell>-</cell><cell>+1.27</cell><cell>+1.04</cell><cell cols="2">+2.21 +0.09</cell><cell>+2.47</cell><cell>+0.35</cell></row><row><cell>PV-RCNN (baseline) [22]</cell><cell>LiDAR</cell><cell>87.45</cell><cell>80.28</cell><cell cols="2">76.21 91.91</cell><cell>88.13</cell><cell>85.41</cell></row><row><cell>CLOCs PVCas (PV-RCNN+Cascad R-CNN)</cell><cell cols="2">LiDAR+Img 88.94</cell><cell>80.67</cell><cell cols="2">77.15 93.05</cell><cell>89.80</cell><cell>86.57</cell></row><row><cell>Improvement (CLOCs PVCas over PV-RCNN)</cell><cell>-</cell><cell>+1.49</cell><cell>+0.39</cell><cell cols="2">+0.94 +1.14</cell><cell>+1.67</cell><cell>+1.17</cell></row><row><cell>F-PointNet [24]</cell><cell cols="2">LiDAR+Img 82.19</cell><cell>69.79</cell><cell cols="2">60.59 91.17</cell><cell>84.67</cell><cell>74.77</cell></row><row><cell>AVOD-FPN [12]</cell><cell cols="2">LiDAR+Img 83.07</cell><cell>71.76</cell><cell cols="2">65.73 90.99</cell><cell>84.82</cell><cell>79.62</cell></row><row><cell>F-ConvNet [25]</cell><cell cols="2">LiDAR+Img 87.36</cell><cell>76.39</cell><cell cols="2">66.69 91.51</cell><cell>85.84</cell><cell>76.11</cell></row><row><cell>UberATG-MMF [26]</cell><cell cols="2">LiDAR+Img 88.40</cell><cell>77.43</cell><cell cols="2">70.22 93.67</cell><cell>88.21</cell><cell>81.99</cell></row><row><cell>UberATG-ContFuse [14]</cell><cell cols="2">LiDAR+Img 83.68</cell><cell>68.78</cell><cell cols="2">61.67 94.07</cell><cell>85.35</cell><cell>75.88</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>3D and bird's eye view performance of fusion with different combinations of 2D/3D detectors through CLOCs on car class of KITTI validation set (new 40 recall positions metric). Our CLOCs fusion methods outperform the baseline methods.</figDesc><table><row><cell>Detector</cell><cell cols="2">3D AP (%) easy moderate hard easy moderate hard Bird's Eye View AP (%)</cell></row><row><cell cols="2">SECOND (baseline) 90.97 79.94 77.09 95.61 89.54</cell><cell>86.96</cell></row><row><cell>SECOND+RRC</cell><cell>92.69 82.69 78.20 96.53 92.78</cell><cell>87.74</cell></row><row><cell cols="2">SECOND+MSCNN 92.37 82.36 78.23 96.34 92.59</cell><cell>87.81</cell></row><row><cell cols="2">SECOND+C-RCNN* 92.35 82.73 78.10 96.34 92.57</cell><cell>89.36</cell></row><row><cell cols="2">PointPillars (baseline) 87.37 76.17 72.88 92.40 87.79</cell><cell>86.39</cell></row><row><cell>PointPillars+RRC</cell><cell>88.48 78.50 75.13 93.53 88.87</cell><cell>87.09</cell></row><row><cell cols="2">PointPillars+MSCNN 89.22 77.05 73.16 92.80 88.46</cell><cell>87.26</cell></row><row><cell cols="2">PointPillars+C-RCNN* 89.95 78.99 73.27 93.77 88.27</cell><cell>87.34</cell></row><row><cell cols="2">PointRCNN (baseline) 92.54 82.16 77.88 95.58 88.78</cell><cell>86.34</cell></row><row><cell>PointRCNN+RRC</cell><cell>92.67 84.75 81.82 95.98 90.80</cell><cell>87.96</cell></row><row><cell cols="2">PointRCNN+MSCNN 92.64 83.26 79.88 95.60 90.05</cell><cell>87.05</cell></row><row><cell cols="2">PointRCNN+C-RCNN* 93.09 84.09 80.73 96.13 90.19</cell><cell>87.26</cell></row><row><cell cols="2">PV-RCNN (baseline) 92.10 84.36 82.48 93.02 90.33</cell><cell>88.53</cell></row><row><cell>PV-RCNN+RRC</cell><cell>92.82 85.59 83.00 93.65 92.40</cell><cell>90.19</cell></row><row><cell cols="2">PV-RCNN+MSCNN 92.66 83.89 83.29 93.50 91.63</cell><cell>89.42</cell></row><row><cell cols="2">PV-RCNN+C-RCNN* 92.78 85.94 83.25 93.48 91.98</cell><cell>89.48</cell></row></table><note>*C-RCNN is Cascade R-CNN.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III</head><label>III</label><figDesc></figDesc><table><row><cell cols="3">: 3D and bird's eye view performance of fusion</cell></row><row><cell cols="3">on pedestrian class of KITTI validation set (using new 40</cell></row><row><cell cols="3">recall positions). Our CLOCs fusion methods outperform the</cell></row><row><cell cols="2">corresponding baseline methods</cell></row><row><cell>Detector</cell><cell cols="2">3D AP (%) easy moderate hard easy moderate hard Bird's Eye View AP (%)</cell></row><row><cell cols="2">SECOND (baseline) 58.01 51.88 47.05 61.97 56.77</cell><cell>51.27</cell></row><row><cell cols="2">SECOND+MSCNN 62.54 56.76 52.26 69.35 63.47</cell><cell>58.93</cell></row><row><cell cols="2">PointPillars (baseline) 58.38 51.42 45.20 66.97 59.45</cell><cell>53.42</cell></row><row><cell cols="2">PointPillars+MSCNN 60.33 54.17 46.42 69.29 63.00</cell><cell>54.80</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV :</head><label>IV</label><figDesc>Performance of fusion on cyclist class of KITTI validation set (new 40 recall positions). Our CLOCs fusion methods outperform the corresponding baseline methods</figDesc><table><row><cell>Detector</cell><cell cols="2">3D AP (%) easy moderate hard easy moderate hard Bird's Eye View AP (%)</cell></row><row><cell cols="2">SECOND (baseline) 78.50 56.74 52.83 81.91 59.36</cell><cell>55.53</cell></row><row><cell cols="2">SECOND+MSCNN 85.47 59.47 55.00 88.96 63.40</cell><cell>59.81</cell></row><row><cell cols="2">PointPillars (baseline) 82.31 59.33 55.25 84.65 61.39</cell><cell>57.28</cell></row><row><cell cols="2">PointPillars+MSCNN 90.26 64.84 59.59 92.64 67.97</cell><cell>62.31</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V :</head><label>V</label><figDesc>Performance of CLOCs with PointRCNN using different score scales on car class of KITTI validation set. Because the sigmoid score from PointRCNN poorly approximates probability of a target (or precision), using it for fusion could result in worse performance.</figDesc><table><row><cell>Type of Scores</cell><cell cols="2">3D AP (%) easy moderate hard easy moderate hard Bird's Eye View AP (%)</cell></row><row><cell>log score</cell><cell>93.09 84.09 80.73 96.13 90.19</cell><cell>87.26</cell></row><row><cell>sigmoid score</cell><cell>91.64 82.96 79.13 95.33 89.70</cell><cell>86.36</cell></row><row><cell cols="2">corrected sigmoid score 92.83 83.73 80.12 95.88 90.19</cell><cell>87.08</cell></row><row><cell cols="2">corrected log score 92.88 83.92 80.22 96.07 89.93</cell><cell>87.21</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI :</head><label>VI</label><figDesc>The contribution of each channel and focal loss in our CLOCs fusion pipeline. The results are on the moderate level car class of KITTI val split with AP calculated by 40 recall positions. SECOND and Cascade R-CNN are fused in this experiment, so the baseline model is SECOND detectors when both LiDAR and camera data are available. CLOCs exploits the geometric and semantic consistencies between 2D and 3D detections and automatically learns fusion parameters. The experiments show that our fusion method outperforms previous state-of-the-art methods by a large margin on the challenging 3D detection benchmark of KITTI dataset, especially in long distance detection. As such, CLOCs provides a baseline for other types of fusion including early and deep fusion.</figDesc><table><row><cell>IoU s 2D s 3D</cell><cell>d</cell><cell>focal loss 3D AP BEV AP</cell></row><row><cell></cell><cell></cell><cell>79.94 89.54</cell></row><row><cell></cell><cell></cell><cell>78.95 88.43</cell></row><row><cell></cell><cell></cell><cell>80.96 90.32</cell></row><row><cell></cell><cell></cell><cell>38.64 47.16</cell></row><row><cell></cell><cell></cell><cell>81.96 91.90</cell></row><row><cell></cell><cell></cell><cell>81.01 92.17</cell></row><row><cell></cell><cell></cell><cell>82.73 92.57</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards realtime object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bayesian optimization for learning gaits under uncertainty</title>
		<author>
			<persName><forename type="first">R</forename><surname>Calandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Seyfarth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Deisenroth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annals of Mathematics and Artificial Intelligence (AMAI)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="5" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
				<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4490" to="4499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Second: Sparsely embedded convolutional detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">3337</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pointrcnn: 3d object proposal generation and detection from point cloud</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="770" to="779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pointpillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1907" to="1915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Joint 3d proposal generation and object detection from view aggregation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mozifian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pointfusion: Deep sensor fusion for 3d bounding box estimation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="244" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep continuous fusion for multi-sensor 3d object detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
				<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="641" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">3d bounding box estimation using deep learning and geometry</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kosecka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="7074" to="7082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep manta: A coarse-to-fine many-task network for joint 2d and 3d vehicle analysis from monocular image</title>
		<author>
			<persName><forename type="first">F</forename><surname>Chabot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chaouch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rabarisoa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Teuliere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chateau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2040" to="2049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A coarse-to-fine model for 3d pose estimation and sub-category recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="418" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pseudo-lidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous driving</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8445" to="8453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Pseudo-lidar++: Accurate depth for 3d object detection in autonomous driving</title>
		<author>
			<persName><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.06310</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fast point r-cnn</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision (ICCV)</title>
				<meeting>the IEEE international conference on computer vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">STD: sparse-to-dense 3d object detector for point cloud</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1907.10471" />
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pvrcnn: Point-voxel feature set abstraction for 3d object detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">From points to parts: 3d object detection from point cloud with part-aware and partaggregation network</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Frustum pointnets for 3d object detection from rgb-d data</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="918" to="927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Frustum convnet: Sliding frustums to aggregate local point-wise features for amodal 3d object detection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IROS</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-task multi-sensor fusion for 3d object detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7345" to="7353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th international conference on machine learning (ICML-10)</title>
				<meeting>the 27th international conference on machine learning (ICML-10)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">3d object proposals for accurate object class detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Berneshawi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="424" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Accurate single stage detector using recurrent rolling convolution</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5420" to="5428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A unified multiscale deep convolutional neural network for fast object detection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="354" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Cascade r-cnn: High quality object detection and instance segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.09756</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
