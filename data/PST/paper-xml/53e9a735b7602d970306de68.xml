<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Parametric Bandits: The Generalized Linear Case</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sarah</forename><surname>Filippi</surname></persName>
							<email>filippi@telecom-paristech.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">LTCI Telecom ParisTech et CNRS</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Olivier</forename><surname>Cappé</surname></persName>
							<email>cappe@telecom-paristech.fr</email>
							<affiliation key="aff1">
								<orgName type="institution">LTCI Telecom ParisTech et CNRS</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aurélien</forename><surname>Garivier</surname></persName>
							<email>garivier@telecom-paristech.fr</email>
							<affiliation key="aff2">
								<orgName type="institution">LTCI Telecom ParisTech et CNRS</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Csaba</forename><surname>Szepesvári</surname></persName>
							<email>szepesva@ualberta.ca</email>
							<affiliation key="aff3">
								<orgName type="institution">RLAI Laboratory University of Alberta</orgName>
								<address>
									<settlement>Edmonton</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Parametric Bandits: The Generalized Linear Case</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2A20502E0498FF65AFE623DA59194FB3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T05:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>multi-armed bandit</term>
					<term>parametric bandits</term>
					<term>generalized linear models</term>
					<term>UCB</term>
					<term>regret minimization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider structured multi-armed bandit problems based on the Generalized Linear Model (GLM) framework of statistics. For these bandits, we propose a new algorithm, called GLM-UCB. We derive finite time, high probability bounds on the regret of the algorithm, extending previous analyses developed for the linear bandits to the non-linear case. The analysis highlights a key difficulty in generalizing linear bandit algorithms to the non-linear case, which is solved in GLM-UCB by focusing on the reward space rather than on the parameter space. Moreover, as the actual effectiveness of current parameterized bandit algorithms is often poor in practice, we provide a tuning method based on asymptotic arguments, which leads to significantly better practical performance. We present two numerical experiments on real-world data that illustrate the potential of the GLM-UCB approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the classical K-armed bandit problem, an agent selects at each time step one of the K arms and receives a reward that depends on the chosen action. The aim of the agent is to choose the sequence of arms to be played so as to maximize the cumulated reward. There is a fundamental trade-off between gathering experimental data about the reward distribution (exploration) and exploiting the arm which seems to be the most promising.</p><p>In the basic multi-armed bandit problem, also called the independent bandits problem, the rewards are assumed to be random and distributed independently according to a probability distribution that is specific to each arm -see <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref> and references therein. Recently, structured bandit problems in which the distributions of the rewards pertaining to each arm are connected by a common unknown parameter have received much attention <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>. This model is motivated by the many practical applications where the number of arms is large, but the payoffs are interrelated. Up to know, two different models were studied in the literature along these lines. In one model, in each times step, a side-information, or context, is given to the agent first. The payoffs of the arms depend both on this side information and the index of the arm. Thus the optimal arm changes with the context <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">9]</ref>. In the second, simpler model, that we are also interested in here, there is no side-information, but the agent is given a model that describes the possible relations between the arms' payoffs. In particular, in "linear bandits" <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>, each arm a ∈ A is associated with some d-dimensional vector m a ∈ R d known to the agent. The expected payoffs of the arms are given by the inner product of their associated vector and some fixed, but initially unknown parameter vector θ * . Thus, the expected payoff of arm a is m ′ a θ * , which is linear in θ * . <ref type="foot" target="#foot_0">1</ref>In this article, we study a richer generalized linear model (GLM) in which the expectation of the reward conditionally to the action a is given by µ(m ′ a θ * ), where µ is a real-valued, non-linear function called the (inverse) link function. This generalization allows to consider a wider class of problems, and in particular cases where the rewards are counts or binary variables using, respectively, Poisson or logistic regression. Obviously, this situation is very common in the fields of marketing, social networking, web-mining (see example of Section 5.2 below) or clinical studies.</p><p>Our first contribution is an "optimistic" algorithm, termed GLM-UCB, inspired by the Upper Confidence Bound (UCB) approach <ref type="bibr" target="#b1">[2]</ref>. GLM-UCB generalizes the algorithms studied by <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b11">12]</ref>. Our next contribution are finite-time bounds on the statistical performance of this algorithm. In particular, we show that the performance depends on the dimension of the parameter but not on the number of arms, a result that was previously known in the linear case. Interestingly, the GLM-UCB approach takes advantage of the particular structure of the parameter estimate of generalized linear models and operates only in the reward space. In contrast, the parameter-space confidence region approach adopted by <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12]</ref> appears to be harder to generalize to non-linear regression models. Our second contribution is a tuning method based on asymptotic arguments. This contribution addresses the poor empirical performance of the current algorithms that we have observed for small or moderate sample-sizes when these algorithms are tuned based on finite-sample bounds.</p><p>The paper is organized as follows. The generalized linear bandit model is presented in Section 2, together with a brief survey of needed statistical results. Section 3 is devoted to the description of the GLM-UCB algorithm, which is compared to related approaches. Section 4 presents our regret bounds, as well as a discussion, based on asymptotic arguments, on the optimal tuning of the method. Section 5 reports the results of two experiments on real data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Generalized Linear Bandits, Generalized Linear Models</head><p>We consider a structured bandit model with a finite, but possibly very large, number of arms. At each time t, the agent chooses an arm A t from the set A (we shall denote the cardinality of A by K). The prior knowledge available to the agent consists of a collection of vectors {m a } a∈A of features which are specific to each arm and a so-called (inverse) link function µ : R → R.</p><p>The generalized linear bandit model investigated in this work is based on the assumption that the payoff R t received at time t is conditionally independent of the past payoffs and choices and it satisfies</p><formula xml:id="formula_0">E [ R t | A t ] = µ(m ′ At θ * ) ,<label>(1)</label></formula><p>for some unknown parameter vector θ * ∈ R d . This framework generalizes the linear bandit model considered by <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b11">12]</ref>. Just like the linear bandit model builds on linear regression, our model capitalizes on the well-known statistical framework of Generalized Linear Models (GLMs). The advantage of this framework is that it allows to address various, specific reward structures widely found in applications. For example, when rewards are binary-valued, a suitable choice of µ is µ(x) = exp(x)/(1 + exp(x)), leading to the logistic regression model. For integer valued rewards, the choice µ(x) = exp(x) leads to the Poisson regression model. This can be easily extended to the case of multinomial (or polytomic) logistic regression, which is appropriate to model situations in which the rewards are associated with categorical variables.</p><p>To keep this article self-contained, we briefly review the main properties of GLMs <ref type="bibr" target="#b12">[13]</ref>. A univariate probability distribution is said to belong to a canonical exponential family if its density with respect to a reference measure is given by</p><formula xml:id="formula_1">p β (r) = exp (rβ -b(β) + c(r)) ,<label>(2)</label></formula><p>where β is a real parameter, c(•) is a real function and the function b(•) is assumed to be twice continuously differentiable. This family contains the Gaussian and Gamma distributions when the reference measure is the Lebesgue measure and the Poisson and Bernoulli distributions when the reference measure is the counting measure on the integers. For a random variable R with density defined in (2), E(R) = ḃ(β) and Var(R) = b(β), where ḃ and b denote, respectively, the first and second derivatives of b. In addition, b(β) can also be shown to be equal to the Fisher information matrix for the parameter β. The function b is thus strictly convex. Now, assume that, in addition to the response variable R, we have at hand a vector of covariates X ∈ R d . The canonical GLM associated to <ref type="bibr" target="#b1">(2)</ref> postulates that p θ (r|x) = p x ′ θ (r), where θ ∈ R d is a vector of parameter. Denote by µ = ḃ the so-called inverse link function. From the properties of b, we know that µ is continuously differentiable, strictly increasing, and thus one-to-one. The maximum likelihood estimator θt , based on observations (R 1 , X 1 ), . . . (R t-1 , X t-1 ), is defined as the maximizer of the function</p><formula xml:id="formula_2">t-1 k=1 log p θ (R k |X k ) = t-1 k=1 R k X ′ k θ -b(X ′ k θ) + c(R k ) ,</formula><p>a strictly concave function in θ. <ref type="foot" target="#foot_1">2</ref> Upon differentiating, we obtain that θt is the unique solution of the following estimating equation</p><formula xml:id="formula_3">t-1 k=1 (R k -µ(X ′ k θ)) X k = 0 ,<label>(3)</label></formula><p>where we have used the fact that µ = ḃ. In practice, the solution of (3) may be found efficiently using, for instance, Newton's algorithm.</p><p>A semi-parametric version of the above model is obtained by assuming only that E θ [R|X] = µ(X ′ θ) without (much) further assumptions on the conditional distribution of R given X. In this case, the estimator obtained by solving (3) is referred to as the maximum quasi-likelihood estimator. It is a remarkable fact that this estimator is consistent under very general assumptions as long as the design matrix</p><formula xml:id="formula_4">t-1 k=1 X k X ′</formula><p>k tends to infinity <ref type="bibr" target="#b13">[14]</ref>. As we will see, this matrix also plays a crucial role in the algorithm that we propose for bandit optimization in the generalized linear bandit model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The GLM-UCB Algorithm</head><p>According to (1), the agent receives, upon playing arm a, a random reward whose expected value is µ(m ′ a θ * ), where θ * ∈ Θ is the unknown parameter. The parameter set Θ is an arbitrary closed subset of R d . Any arm with largest expected reward is called optimal. The aim of the agent is to quickly find an optimal arm in order to maximize the received rewards. The greedy action argmax a∈A µ(m ′ a θt ) may lead to an unreliable algorithm which does not sufficiently explore to guarantee the selection of an optimal arm. This issue can be addressed by resorting to an "optimistic approach". As described by <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12]</ref> in the linear case, an optimistic algorithm consists in selecting, at time t, the arm</p><formula xml:id="formula_5">A t = argmax a max θ E θ [ R t | A t = a] s.t. θ -θt Mt ≤ ρ(t) ,<label>(4)</label></formula><p>where ρ is an appropriate, "slowly increasing" function,</p><formula xml:id="formula_6">M t = t-1 k=1 m A k m ′ A k<label>(5)</label></formula><p>is the design matrix corresponding to the first t -1 timesteps and v M = √ v ′ M v denotes the matrix norm induced by the positive semidefinite matrix M . The region θ -θt Mt ≤ ρ(t) is a confidence ellipsoid around the estimated parameter θt . Generalizing this approach beyond the case of linear link functions looks challenging. In particular, in GLMs, the relevant confidence regions may have a more complicated geometry in the parameter space than simple ellipsoids. As a consequence, the benefits of this form of optimistic algorithms appears dubious. <ref type="foot" target="#foot_2">3</ref>An alternative approach consists in directly determining an upper confidence bound for the expected reward of each arm, thus choosing the action a that maximizes</p><formula xml:id="formula_7">E θt [ R t | A t = a] + ρ(t) m a M -1 t .</formula><p>In the linear case the two approaches lead to the same solution <ref type="bibr" target="#b11">[12]</ref>. Interestingly, for non-linear bandits, the second approach looks more appropriate.</p><p>In the rest of this section, we apply this second approach to the GLM bandit model defined in <ref type="bibr" target="#b0">(1)</ref>. According to (3), the maximum quasi-likelihood estimator of the parameter in the GLM is the unique solution of the estimating equation</p><formula xml:id="formula_8">t-1 k=1 R k -µ(m ′ A k θt ) m A k = 0 ,<label>(6)</label></formula><p>where A 1 , . . . , A t-1 denote the arms played so far and R 1 , . . . , R t-1 are the corresponding rewards.</p><formula xml:id="formula_9">Let g t (θ) = t-1 k=1 µ(m ′ A k θ)m A k be the invertible function such that the estimated parameter θt satisfies g t ( θt ) = t-1 k=1 R k m A k .</formula><p>Since θt might be outside of the set of admissible parameters Θ, we "project it" to Θ, to obtain θt :</p><formula xml:id="formula_10">θt = argmin θ∈Θ g t (θ) -g t ( θt ) M -1 t = argmin θ∈Θ g t (θ) - t-1 k=1 R k m A k M -1 t .<label>(7)</label></formula><p>Note that if θt ∈ Θ (which is easy to check and which happened to hold always in the examples we dealt with) then we can let θt = θt . This is important since computing θt is non-trivial and we can save this computation by this simple check. The proposed algorithm, GLM-UCB, is as follows:</p><p>Algorithm 1 GLM-UCB Estimate θt according to <ref type="bibr" target="#b5">(6)</ref> 5:</p><p>if θt ∈ Θ let θt = θt else compute θt according to <ref type="bibr" target="#b6">(7)</ref> 6:</p><p>Play the action A t = argmax a µ(m ′ a θt ) + ρ(t) m a M -1 t , receive R t 7: end for At time t, for each arm a, an upper bound µ(m ′ a θt ) + β a t is computed, where the "exploration bonus"</p><formula xml:id="formula_11">β a t = ρ(t) m a M -1 t</formula><p>is the product of two terms. The quantity ρ(t) is a slowly increasing function; we prove in Section 4 that ρ(t) can be set to guarantee high-probability bounds on the expected regret (for the actual form used, see <ref type="bibr" target="#b7">(8)</ref>). Note that the leading term of β a t is m a M -1 t which decreases to zero as t increases.</p><p>As we are mostly interested in the case when the number of arms K is much larger than the dimension d, the algorithm is simply initialized by playing actions a 1 , . . . , a d such that the vectors m a1 . . . , m a d form a basis of M = span(m a , a ∈ A). Without loss of generality, here and in what follows we assume that the dimension of M is equal to d. Then, by playing a 1 , . . . , a d in the first d steps the agent ensures that M t is invertible for all t. An alternative strategy would be to initialize M 0 = λ 0 I, where I is the d × d identify matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Discussion</head><p>The purpose of this section is to discuss some properties of Algorithm 1, and in particular the interpretation of the role played by m a M -1 t .</p><p>Generalizing UCB The standard UCB algorithm for K arms <ref type="bibr" target="#b1">[2]</ref> can be seen as a special case of GLM-UCB where the vectors of covariates associated with the arms form an orthogonal system and µ(x) = x. Indeed, take d = K, A = {1, . . . , K}, define the vectors {m a } a∈A as the canonical basis {e a } a∈A of R d , and take θ ∈ R d the vector whose component θ a is the expected reward for arm a.</p><p>Then, M t is a diagonal matrix whose a-th diagonal element is the number N t (a) of times the a-th arm has been played up to time t. Therefore, the exploration bonus in GLM-UCB is given by β a t = ρ(t)/ N t (a). Moreover, the maximum quasi-likelihood estimator θt satisfies Ra t = θt (a) for all a ∈ A, where Ra t = 1</p><p>Nt(a)</p><p>t-1 k=1 I {At=a} R k is the empirical mean of the rewards received while playing arm a. Algorithm 1 then reduces to the familiar UCB algorithm. In this case, it is known that the expected cumulated regret can be controlled upon setting the slowly varying function ρ to ρ(t) = 2 log(t), assuming that the range of the rewards is bounded by one <ref type="bibr" target="#b1">[2]</ref>.</p><p>Generalizing linear bandits Obviously, setting µ(x) = x, we obtain a linear bandit model. In this case, assuming that Θ = R d , the algorithm will reduce to those described in the papers <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12]</ref>. In particular, the maximum quasi-likelihood estimator becomes the least-squares estimator and as noted earlier, the algorithm behaves identically to one which chooses the parameter optimistically within the confidence ellipsoid {θ : θ -θt Mt ≤ ρ(t)}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dependence in the Number of Arms</head><p>In contrast to an algorithm such as UCB, Algorithm 1 does not need that all arms be played even once. <ref type="foot" target="#foot_3">4</ref> To understand this phenomenon, observe that, as</p><formula xml:id="formula_12">M t+1 = M t + m At m ′ At , m a 2 M -1 t+1 = m a 2 M -1 t -m ′ a M -1 t m At 2 (1 + m At 2 M -1 t</formula><p>) for any arm a. Thus the exploration bonus β a t+1 decreases for all arms, except those which are exactly orthogonal to m At (in the M -1 t metric). The decrease is most significant for arms that are colinear to m At . This explains why the regret bounds obtained in Theorems 1 and 2 below depend on d but not on K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Theoretical analysis</head><p>In this section we first give our finite sample regret bounds and then show how the algorithm can be tuned based on asymptotic arguments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Regret Bounds</head><p>To quantify the performance of the GLM-UCB algorithm, we consider the cumulated (pseudo) regret defined as the expected difference between the optimal reward obtained by always playing an optimal arm and the reward received following the algorithm:</p><formula xml:id="formula_13">Regret T = T t=1 µ(m ′ a * θ * ) -µ(m ′ At θ * ) .</formula><p>For the sake of the analysis, in this section we shall assume that the following assumptions hold: Assumption 1. The link function µ : R → R is continuously differentiable, Lipschitz with constant k µ and such that c µ = inf θ∈Θ,a∈A μ(m ′ a θ) &gt; 0.</p><p>For the logistic function k µ = 1/4, while the value of c µ depends on sup θ∈Θ,a∈A |m ′ a θ|. Assumption 2. The norm of covariates in {m a : a ∈ A} is bounded: there exists c m &lt; ∞ such that for all a ∈ A, m a 2 ≤ c m .</p><p>Finally, we make the following assumption on the rewards: Assumption 3. There exists R max &gt; 0 such that for any t ≥ 1, 0 ≤ R t ≤ R max holds a.s. Let</p><formula xml:id="formula_14">ǫ t = R t -µ(m ′ At θ * ). For all t ≥ 1, it holds that E [ǫ t |m At , ǫ t-1 , . . . , m A2 , ǫ 1 , m A1 ] = 0 a.s.</formula><p>As for the standard UCB algorithm, the regret can be analyzed in terms of the difference between the expected reward received playing an optimal arm and that of the best sub-optimal arm:</p><formula xml:id="formula_15">∆(θ * ) = min a:µ(m ′ a θ * )&lt;µ(m ′ a * θ * ) µ(m ′ a * θ * ) -µ(m ′ a θ * ) .</formula><p>Theorem 1 establishes a high probability bound on the regret underlying using GLM-UCB with</p><formula xml:id="formula_16">ρ(t) = 2k µ κR max c µ 2d log(t) log(2 d T /δ) , (<label>8</label></formula><formula xml:id="formula_17">)</formula><p>where T is the fixed time horizon, κ = 3 + 2 log(1 + 2c 2 m /λ 0 ) and λ 0 denotes the smallest eigenvalue of d i=1 m ai m ′ ai , which by our previous assumption is positive. Theorem 1 (Problem Dependent Upper Bound). Let s = max(1, c 2 m /λ 0 ). Then, under Assumptions 1-3, for all T ≥ 1, the regret satisfies:</p><formula xml:id="formula_18">P Regret T ≤ (d + 1)R max + C d 2 ∆(θ * ) log 2 [s T ] log 2d T δ ≥ 1 -δ with C = 32κ 2 R 2 max k 2 µ c 2 µ .</formula><p>Note that the above regret bound depends on the true value of θ * through ∆(θ * ). The following theorem provides an upper-bound of the regret independently of the θ * . Theorem 2 (Problem Independent Upper Bound). Let s = max(1, c 2 m /λ 0 ). Then, under Assumptions 1-3, for all T ≥ 1, the regret satisfies</p><formula xml:id="formula_19">P Regret T ≤ (d + 1)R max + Cd log [s T ] T log 2dT δ ≥ 1 -δ with C = 8R max k µ κ c µ .</formula><p>The proofs of Theorems 1-2 can be found in the supplementary material. The main idea is to use the explicit form of the estimator given by ( <ref type="formula" target="#formula_8">6</ref>) to show that</p><formula xml:id="formula_20">µ(m ′ At θ * ) -µ(m ′ At θt ) ≤ k µ c µ m At M -1 t t-1 k=1 m A k ǫ k M -1 t .</formula><p>Bounding the last term on the right-hand side is then carried out following the lines of <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Asymptotic Upper Confidence Bound</head><p>Preliminary experiments carried out using the value of ρ(t) defined equation ( <ref type="formula" target="#formula_16">8</ref>), including the case where µ is the identity function -i.e., using the algorithm described by <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12]</ref>, revealed poor performance for moderate sample sizes. A look into the proof of the regret bound easily explains this observation as the mathematical involvement of the arguments is such that some approximations seem unavoidable, in particular several applications of the Cauchy-Schwarz inequality, leading to pessimistic confidence bounds. We provide here some asymptotic arguments that suggest to choose significantly smaller exploration bonuses, which will in turn be validated by the numerical experiments presented in Section 5.</p><p>Consider the canonical GLM associated with an inverse link function µ and assume that the vectors of covariates X are drawn independently under a fixed distribution. This random design model would for instance describe the situation when the arms are drawn randomly from a fixed distribution. Standard statistical arguments show that the Fisher information matrix pertaining to this model is given by J = E[ μ(X ′ θ * )XX ′ ] and that the maximum likelihood estimate θt is such that t -1/2 ( θtθ * ) D -→ N (0, J -1 ), where D -→ stands for convergence in distribution. Moreover, t -1 M t a.s.</p><p>-→ Σ where Σ = E[XX ′ ]. Hence, using the delta-method and Slutsky's lemma</p><formula xml:id="formula_21">m a -1 M -1 t (µ(m ′ a θt ) -µ(m ′ a θ * )) D -→ N (0, μ(m ′ a θ * ) m ′ a -<label>2</label></formula><formula xml:id="formula_22">Σ -1 m ′ a 2 J -1</formula><p>) . The right-hand variance is smaller than k µ /c µ as J c µ Σ. Hence, for any sampling distribution such that J and Σ are positive definite and sufficiently large t and small δ,</p><formula xml:id="formula_23">P m a -1 M -1 t (µ(m ′ a θt ) -µ(m ′ a θ * )) &gt; 2k µ /c µ log(1/δ)</formula><p>is asymptotically bounded by δ. Based on the above asymptotic argument, we postulate that using ρ(t) = 2k µ /c µ log(t), i.e., inflating the exploration bonus by a factor of k µ /c µ compared to the usual UCB setting, is sufficient. This is the setting used in the simulations below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>To the best of our knowledge, there is currently no public benchmark available to test bandit methods on real world data. On simulated data, the proposed method unsurprisingly outperforms its competitors when the data is indeed simulated from a well-specified generalized linear model.</p><p>In order to evaluate the potential of the method in more challenging scenarios, we thus carried out two experiments using real world datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Forest Cover Type Data</head><p>In this first experiment, we test the performance of the proposed method on a toy problem using the "Forest Cover Type dataset" from the UCI repository. The dataset (centered and normalized with constant covariate added, resulting in 11-dimensional vectors, ignoring all categorical variables) has been partitioned into K = 32 clusters using unsupervised k-means. The values of the response variable for the data points assigned to each cluster are viewed as the outcomes of an arm while the centroid of the cluster is taken as the 11-dimensional vector of covariates characteristic of the arm.</p><p>To cast the problem into the logistic regression framework, each response variable is binarized by associating the first class ("Spruce/Fir") to a response R = 1 and all other six classes to R = 0.</p><p>The proportions of responses equal to 1 in each cluster (or, in other word, the expected reward associated with each arm) ranges from 0.354 to 0.992, while the proportion on the complete set of 581,012 data points is equal to 0.367. In effect, we try to locate as fast as possible the cluster that contains the maximal proportion of trees from a given species. We are faced with a 32-arm problem in a 11-dimensional space with binary rewards. Obviously, the logistic regression model is not satisfied, although we do expect some regularity with respect to the position of the cluster's centroid as the logistic regression trained on all data reaches a 0.293 misclassification rate.  We compare the performance of three algorithms. First, the GLM-UCB algorithm, with parameters tuned as indicated in Section 4.2. Second, the standard UCB algorithm that ignores the covariates. Third, an ǫ-greedy algorithm that performs logistic regression and plays the best estimated action, A t = argmax a µ(m ′ a θt ), with probability 1ǫ (with ǫ = 0.1). We observe in the top graph of Figure <ref type="figure" target="#fig_1">1</ref> that the GLM-UCB algorithm achieves the smallest average regret by a large margin. When the parameter is well estimated, the greedy algorithm may find the best arm in little time and then leads to small regrets. However, the exploration/exploitation tradeoff is not correctly handled by the ǫ-greedy approach causing a large variability in the regret. The lower plot of Figure <ref type="figure" target="#fig_1">1</ref> shows the number of times each of the 20 best arms have been played by the UCB and GLM-UCB algorithms. The arms are sorted in decreasing order of expected reward. It can be observed that GML-UCB only plays a small subset of all possible arms, concentrating on the bests. This behavior is made possible by the predictive power of the covariates: by sharing information between arms, it is possible to obtain sufficiently accurate predictions of the expected rewards of all actions, even for those that have never (or rarely) been played.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Internet Advertisement Data</head><p>In this experiment, we used a large record of the activity of internet users provided by a major ISP. The original dataset logs the visits to a set of 1222 pages over a six days period corresponding to about 5.10 8 page visits. The dataset also contains a record of the users clicks on the ads that were presented on these pages. We worked with a subset of 208 ads and 3.10 5 users. The pages (ads) were partitioned in 10 (respectively, 8) categories using Latent Dirichlet Allocation <ref type="bibr" target="#b14">[15]</ref> applied to their respective textual content (in the case of ads, the textual content was that of the page pointed to by the ad's link). This second experiment is much more challenging, as the predictive power of the sole textual information turns out to be quite limited (for instance, Poisson regression trained on the entire data does not even correctly identify the best arm).</p><p>The action space is composed of the 80 pairs of pages and ads categories: when a pair is chosen, it is presented to a group of 50 users, randomly selected from the database, and the reward is the number of recorded clicks. As the average reward is typically equal to 0.15, we use a logarithmic link function corresponding to Poisson regression. The vector of covariates for each pair is of dimension 19: it is composed of an intercept followed by the concatenation of two vectors of dimension 10 and 8 representing, respectively, the categories of the pages and the ads. In this problem, the covariate vectors do not span the entire space; to address this issue, it is sufficient to consider the pseudo-inverse of M t instead of the inverse.</p><p>On this data, we compared the GLM-UCB algorithm with the two alternatives described in Section 5.1. Figure <ref type="figure" target="#fig_2">2</ref> shows that GLM-UCB once again outperforms its competitors, even though the margin over UCB is now less remarkable. Given the rather limited predictive power of the covariates in this example, this is an encouraging illustration of the potential of techniques which use vectors of covariates in real-life applications. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We have introduced an approach that generalizes the linear regression model studied by <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b11">12]</ref>. As in the original UCB algorithm, the proposed GLM-UCB method operates directly in the reward space. We discussed how to tune the parameters of the algorithm to avoid exaggerated optimism, which would slow down learning. In the numerical simulations, the proposed algorithm was shown to be competitive and sufficiently robust to tackle real-world problems. An interesting open problem (already challenging in the linear case) consists in tightening the theoretical results obtained so far in order to bridge the gap between the existing (pessimistic) confidence bounds and those suggested by the asymptotic arguments presented in Section 4.2, which have been shown to perform satisfactorily in practice.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Top: Regret of the UCB, GLM-UCB and the ǫ-greedy algorithms. Bottom: Frequencies of the 20 best arms draws using the UCB and GLM-UCB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Comparison of the regret of the UCB, GLM-UCB and the ǫ-greedy (ǫ = 0.1) algorithm on the advertisement dataset.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Throughout the paper we use the prime to denote transposition.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Here, and in what follows log denotes the natural logarithm.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Note that maximizing µ(m ′ a θ) over a convex confidence region is equivalent to maximizing m ′ a θ over the same region since µ is strictly increasing. Thus, computationally, this approach is not more difficult than it is for the linear case.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>Of course, the linear bandit algorithms also share this property with our algorithm.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported in part by AICML, AITF, NSERC, PASCAL2 under n o 216886, the DARPA GALE project under n o HR0011-08-C-0110 and Orange Labs under contract n o 289365.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Asymptotically efficient adaptive allocation rules</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Robbins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="22" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Finite-time analysis of the multiarmed bandit problem</title>
		<author>
			<persName><forename type="first">P</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="235" to="256" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Prediction, learning, and games</title>
		<author>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lugosi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
		<respStmt>
			<orgName>Cambridge Univ Pr</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Tuning bandit algorithms in stochastic environments</title>
		<author>
			<persName><forename type="first">J</forename><surname>Audibert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cs</forename><surname>Szepesvári</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">4754</biblScope>
			<biblScope unit="page">150</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bandit problems with side observations</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">V</forename><surname>Poor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="338" to="355" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The epoch-greedy algorithm for multi-armed bandits with side information</title>
		<author>
			<persName><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="817" to="824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-armed bandit problems with dependent arms</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine learning</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="721" to="728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Stochastic linear optimization under bandit feedback</title>
		<author>
			<persName><forename type="first">V</forename><surname>Dani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Efficient bandit algorithms for online multiclass prediction</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine learning</title>
		<meeting>the 25th International Conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="440" to="447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Using confidence bounds for exploitation-exploration trade-offs</title>
		<author>
			<persName><forename type="first">P</forename><surname>Auer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="397" to="422" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Forced-exploration based algorithms for playing in stochastic linear bandits</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Abbasi-Yadkori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Antos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cs</forename><surname>Szepesvári</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT Workshop on On-line Learning with Limited Feedback</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">P</forename><surname>Rusmevichientong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Tsitsiklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linearly parameterized bandits. Mathematics of Operations Research</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="395" to="411" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Generalized Linear Models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Mccullagh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Nelder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Chapman and Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Strong consistency of maximum quasi-likelihood estimators in generalized linear models with fixed and adaptive designs</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1155" to="1163" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Latent Dirichlet allocation</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="601" to="608" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Self-normalized processes: exponential inequalities, moment bounds and iterated logarithm laws</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">H</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Pena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Klass</surname></persName>
		</author>
		<author>
			<persName><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Probability</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1902" to="1933" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Rusmevichientong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Tsitsiklis</surname></persName>
		</author>
		<idno>arXiv:0812</idno>
		<title level="m">Linearly parameterized bandits</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">3465</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Arxiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
