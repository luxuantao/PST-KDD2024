<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Holoportation: Virtual 3D Teleportation in Real-time</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sergio</forename><surname>Orts-Escolano</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Christoph</forename><surname>Rhemann</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sean</forename><surname>Fanello</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Wayne</forename><surname>Chang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Adarsh</forename><surname>Kowdle</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yury</forename><surname>Degtyarev</surname></persName>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Philip</forename><surname>Davidson</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sameh</forename><surname>Khamis</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mingsong</forename><surname>Dou</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Vladimir</forename><surname>Tankovich</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Charles</forename><surname>Loop</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Qin</forename><surname>Cai</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Philip</forename><surname>Chou</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yuliya</forename><surname>Lutchyn</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Cem</forename><surname>Keskin</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Shahram</forename><surname>Izadi</surname></persName>
							<email>shahrami@microsoft.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Sarah Mennicken Julien Valentin Vivek Pradeep Shenlong Wang Sing Bing Kang</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Holoportation: Virtual 3D Teleportation in Real-time</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F5EDD67FA5798B10398BCD0E42CC37DF</idno>
					<idno type="DOI">10.1145/2984511.2984517</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T02:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Depth Cameras</term>
					<term>3D capture</term>
					<term>Telepresence</term>
					<term>Non-rigid reconstruction</term>
					<term>Real-time</term>
					<term>Mixed Reality</term>
					<term>GPU I.4.5 Image Processing and Computer Vision: Reconstruction</term>
					<term>I.3.7 Computer Graphics: Three-Dimensional Graphics and Realism,</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Figure <ref type="figure">1</ref>. Holoportation is a new immersive telepresence system that combines the ability to capture high quality 3D models of people, objects and environments in real-time, with the ability to transmit these and allow remote participants wearing virtual or augmented reality displays to see, hear and interact almost as if they were co-present.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTRODUCTION</head><p>Despite huge innovations in the telecommunication industry over the past decades, from the rise of mobile phones to the emergence of video conferencing, these technologies are far from delivering an experience close to physical co-presence. For example, despite a myriad of telecommunication technologies, we spend over a trillion dollars per year globally on business travel, with over 482 million flights per year in the US alone 1 . This does not count the cost on the environment. Indeed telepresence has been cited as key in battling carbon emissions in the future <ref type="foot" target="#foot_0">2</ref> .</p><p>However, despite the promise of telepresence, clearly we are still spending a great deal of time, money, and CO 2 getting on planes to meet face-to-face. Somehow much of the subtleties of face-to-face co-located communication -eye contact, body language, physical presence -are still lost in even high-end audio and video conferencing. There is still a clear gap between even the highest fidelity telecommunication tools and physically being there.</p><p>In this paper, we describe Holoportation, a step towards addressing these issues of telepresence. Holoportation is a new immersive communication system that leverages consumer augmented reality (AR) and virtual reality (VR) display technologies, and combines these with a state-of-the-art, real-time 3D capture system. This system is capable of capturing in full 360 o the people, objects and motions within a room, using a set of custom depth cameras. This 3D content is captured and transmitted to remote participants in real-time.</p><p>Any person or object entering the instrumented room will be captured in full 3D, and virtually teleported into the remote participants space. Each participant can now see and hear these remote users within their physical space when they wear their AR or VR headsets. From an audio-visual perspective, this gives users an impression that they are co-present in the same physical space as the remote participants.</p><p>Our main contribution is a new end-to-end immersive system for high-quality, real-time capture, transmission and rendering of people, spaces, and objects in full 3D. Apart from the system as a whole, another set of technical contributions are:</p><p>• A new active stereo depth camera technology for real-time high-quality capture.</p><p>• A novel two GPU pipeline for higher speed temporally consistent reconstruction.</p><p>• A real-time texturing pipeline that creates consistent colored reconstructions.</p><p>• Low latency pipeline for high quality remote rendering.</p><p>• Spatial audio that captures user position and orientation.</p><p>• A prototype for headset removal using wearable cameras.</p><p>Furthermore, this paper also contributes key interactive capabilities, new application scenarios, and an initial qualitative study of using this new communication medium.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RELATED WORK</head><p>There has been a huge amount of work on immersive 3D telepresence systems (see literature reviews in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b31">32]</ref>).</p><p>Given the many challenges around real-time capture, transmission and display, many systems have focused on one specific aspect. Others have constrained the scenarios, for example limiting user motion, focusing on upper body reconstructions, or trading quality for real-time performance.</p><p>Early seminal work on telepresence focused on capturing dynamic scenes using an array of cameras <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b26">27]</ref>. Given both the hardware and computational limitations of these early systems, only low resolution 3D models could be captured and transmitted to remote viewers. Since this early work, we have seen improvements in the real-time capture of 3D models using multiple cameras <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b30">31]</ref>. Whilst results are impressive, given the real-time constraint and hardware limits of the time, the 3D models are still far from high quality. <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b35">36]</ref> demonstrate compelling real-time multi-view capture, but focus on visual hull-based reconstruction -only modeling silhouette boundaries. With the advent of Kinect and other consumer depth cameras, other real-time room-scale capture systems emerged <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b46">47]</ref>. However, these systems again lacked visual quality due to lack of temporal consistency of captured 3D models, sensor noise, interference, and lack of camera synchronization. Conversely, high quality offline reconstruction of human models has been demonstrated using both consumer, e.g. <ref type="bibr" target="#b12">[13]</ref>, and custom depth cameras, e.g. <ref type="bibr" target="#b10">[11]</ref>. Our aim with Holoportation is to achieve a level of visual 3D quality that steps closer to these offline systems. By doing so, we aim to achieve a level of presence that has yet to be achieved in previous real-time systems.</p><p>Beyond high quality real-time capture, another key differentiator of our work is our ability to support true motion within the scene. This is broken down into two parts -remote and local motion. Many telepresence systems limit the local user to a constrained seating/viewing position, including the seminal TELEPORT system <ref type="bibr" target="#b18">[19]</ref>, and more recent systems <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b61">62]</ref>, as part of the constraint of the capture setup.</p><p>Others constrain the motion of the remote participant, typically as a by-product of the display technology used whether it is a situated stereo display <ref type="bibr" target="#b9">[10]</ref> or a more exotic display technology. Despite this, very compelling telepresence scenarios have emerged using situated autostereo <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b44">45]</ref>, cylindrical <ref type="bibr" target="#b27">[28]</ref>, volumetric <ref type="bibr" target="#b23">[24]</ref>, lightfield <ref type="bibr" target="#b0">[1]</ref> or even true holographic <ref type="bibr" target="#b6">[7]</ref> displays. However, we feel that free-form motion within the scene is an important factor in creating a more faithful reconstruction of actual co-located presence. This importance of mobility has been studied greatly in the context of co-located collaboration <ref type="bibr" target="#b36">[37]</ref>, and has motivated certain tele-robotics systems such as <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b32">33]</ref>, and products such as the Beam-PRO. However, these lack human embodiment, are costly and only support a limited range of motion.</p><p>There have been more recent examples of telepresence systems that allow fluid user motion within the space, such as blue-c <ref type="bibr" target="#b20">[21]</ref> or <ref type="bibr" target="#b3">[4]</ref>. However, these systems use stereoscopic projection which ultimately limits the ability for remote and local users to actually inhabit the exact same space. Instead interaction occurs through a 'window' from one space into the other. To resolve this issue, we utilize full volumetric 3D capture of a space, which is overlaid with the remote environment, and leverage off-the-shelf augmented and virtual reality displays for rendering, which allow spaces to be shared and co-habited. <ref type="bibr" target="#b39">[40]</ref> demonstrate the closest system to ours from the perspective of utilizing multiple depth cameras and optical see-through displays. Whilst extremely compelling, particularly given the lack of high quality, off-the-shelf, optical see-through displays at the time, the reconstructions do lack visual quality due to the use of off-the-shelf depth cameras. End-to-end rendering latency is also high, and the remote user is limited to remaining seated during collaboration.</p><p>Another important factor in co-located collaboration is the use of physical props <ref type="bibr" target="#b36">[37]</ref>. However, many capture systems are limited to or have a strong prior on human bodies <ref type="bibr" target="#b8">[9]</ref>, and cannot be extended easily to reconstruct other objects. Further, even with extremely rich offline shape and pose models <ref type="bibr" target="#b8">[9]</ref>, reconstructions can suffer from the effect of "uncanny valley" <ref type="bibr" target="#b43">[44]</ref>; and clothing or hair can prove problematic <ref type="bibr" target="#b8">[9]</ref>. Therefore another requirement of our work is to support arbitrary objects including people, furniture, props, animals, and to create faithful reconstructions that attempt to avoid the uncanny valley.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SYSTEM OVERVIEW</head><p>In our work, we demonstrate the first end-to-end, real-time, immersive 3D teleconferencing system that allows both local and remote users to move freely within an entire space, and interact with each other and with objects. Our system shows unprecedented quality for real-time capture, allows for low end-to-end communication latency, and further mitigates remote rendering latency to avoid discomfort.</p><p>To build Holoportation we designed a new pipeline as shown . in Fig. <ref type="figure" target="#fig_0">2</ref>. We describe this pipeline in full in the next sections, beginning with the physical setup, our algorithm for high quality depth computation and segmentation, realistic temporal geometry reconstruction, color texturing, image compression and network, and remote rendering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Physical Setup and Capture Pods</head><p>For full 360 • capture of the scene, we employ N = 8 camera pods placed on the periphery of the room, pointing inwards to capture a unique viewpoint of the subject/scene. Each pod (Fig. <ref type="figure" target="#fig_1">3</ref>, middle) consists of 2 Near Infra-Red cameras (NIR) and a color camera mounted on top of an optical bench to ensure its rigidity. Additionally a diffractive optical element (DOE) and laser is used to produce a pseudo-random pattern (for our system we use the same design as the Kinect V1). We also mount NIR filters on top of each NIR camera to filter out the visible light spectrum. Each trinocular pod generates a color-aligned RGB and depth stream using a state-of-the-art stereo matching technique described below. Current baseline used in the active stereo cameras is 15 centimeters, giving an average error of 3 millimeters at 1 meter distance and 6 millimeters at 1.5 meter distance.</p><p>In total, our camera rig uses 24 4MP resolution Grasshopper PointGrey cameras. All the pods are synchronized using an external trigger running at 30fps. Fig. <ref type="figure" target="#fig_1">3</ref>, right, shows an example of images acquired using one of the camera pods.</p><p>The first step of this module is to generate depth streams, which require full intrinsics and extrinsics calibration. In this work we use <ref type="bibr" target="#b62">[63]</ref> for computing the camera parameters. Another standard calibration step ensures homogeneous and consistent color information among the RGB cameras. We perform individual white balancing using a color calibration chart. Once colors have been individually tuned, we use one RGB camera as a reference and warp the other cameras to this reference using a linear mapping. This makes the signal consistent across all the RGB cameras. This process is performed offline and then linear mapping across cameras is applied at runtime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Depth Estimation</head><p>Computing 3D geometry information of the scene from multiple view points is the key building block of our system. In our case, the two key constraints are estimating consistent depth information from multiple viewpoints, and doing so in real-time.</p><p>Depth estimation is a well studied problem where a number of diverse and effective solutions have been proposed. We considered popular depth estimation techniques such as structured light, time-of-flight and depth from stereo for this task.</p><p>Structured light approaches allow for very fast and precise depth estimation <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b14">15]</ref>. However, in our setup of multi-view capture structured light-based depth estimation suffers from interference across devices. Another alternative strategy is time-of-flight based depth estimation <ref type="bibr" target="#b21">[22]</ref>, which has grown to be very popular, however, multi-path issues make this technology unusable in our application. Depth from RGB stereo is another possible solution that has seen several advances in real-time depth estimation <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b51">52]</ref>. Passive stereo uses a pair of rectified images and estimates depth by matching every patch of pixels in one image with the other image. Common matching functions include sum of absolute differences (SAD), sum of squared differences (SSD), and normalized cross-correlation (NCC). This is a well understood approach to estimating depth and has been demonstrated to provide very accurate depth estimates <ref type="bibr" target="#b7">[8]</ref>. However, the main issue with this approach is the inability to estimate depth in case of texture-less surfaces.</p><p>Motivated by these observations, we circumvent all these problems by using active stereo for depth estimation. In an active stereo setup, each camera rig is composed of two NIR cameras plus one or more random IR dot pattern projector (composed of laser plus DOE). Each serves as a texture in the scene to help estimate depth even in case of texture-less surfaces. Since we have multiple IR projectors illuminating the scene we are guaranteed to have textured patches to match and estimate depth.</p><p>Additionally, and as importantly, we do not need to know the pattern that is being projected (unlike standard structured light systems such as Kinect V1). Instead, each projector simply adds more texture to the scene to aid depth estimation. This circumvents the issues of interference which commonly occur when two structured light cameras overlap. Here, overlapping projectors actually result in more texture for our matching algorithm to disambiguate patches across cameras, so the net result is an improvement rather than degradation in depth quality.</p><p>With the type of depth estimation technique narrowed down to active stereo, our next constraint is real-time depth estimation. There are several approaches that have been proposed for depth estimation <ref type="bibr" target="#b52">[53]</ref>. We base our work on PatchMatch stereo <ref type="bibr" target="#b7">[8]</ref>, which has been shown to achieve high quality dense depth maps with a runtime independent of the number of depth values under consideration. PatchMatch stereo alternates between random depth generation and propagation of depth based on the algorithm by <ref type="bibr" target="#b1">[2]</ref>, and has recently been extended to real-time performance by assuming fronto parallel windows and reducing the number of iterations of depth propagation <ref type="bibr" target="#b64">[65]</ref>. In this work, we develop a real-time CUDA implementation of PatchMatch stereo that performs depth estimation at as high as 50fps on a high-end GPU such as an NVIDIA Titan X in our case. Details of the GPU implementation are provided later in the paper. Examples of depthmaps are shown in Fig. <ref type="figure" target="#fig_2">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Foreground Segmentation</head><p>The depth estimation algorithm is followed by a segmentation step, which provides 2D silhouettes of the regions of interest. These silhouettes play a crucial role -not only do they help in achieving temporally consistent 3D reconstructions <ref type="bibr" target="#b13">[14]</ref> but they also allow for compressing the data sent over the network. We do not use green screen setups as in <ref type="bibr" target="#b10">[11]</ref>, to ensure that the system can be used in natural environments that appear in realistic scenarios.</p><p>The input of the segmentation module consists of the current RGB image I t , and depth image D t . The systems also maintains background appearance models for the scene from the perspective of each camera pod in a manner similar to <ref type="bibr" target="#b28">[29]</ref>. These background models are based on depth and RGB appearance of the empty scene (where the objects of interest are absent). They are represented by an RGB and depth image pair for each camera pod {I bg , D bg }, which are estimated by averaging over multiple frames to make the system robust against noise (e.g. depth holes, light conditions etc.).</p><p>We model the foreground/background segmentation labeling problem using a fully-connected Conditional Random Field (CRF) <ref type="bibr" target="#b29">[30]</ref> whose energy function is defined as:</p><formula xml:id="formula_0">E(p) = i ψ u (p i ) + i j =i ψ p (p i , p j )<label>(1)</label></formula><p>where p are all the pixels in the image. The pairwise terms ψ p (p i , p j ) are Gaussian edge potentials defined on image gradients in the RGB space, as used in <ref type="bibr" target="#b29">[30]</ref>. The unary potential is defined as the sum of RGB ψ rgb (p i ) and depth ψ depth (p i ) based terms:</p><formula xml:id="formula_1">ψ u (p i ) = ψ rgb (p i )+ψ depth (p i ).</formula><p>The RGB term is defined as</p><formula xml:id="formula_2">ψ rgb (p i ) = |H bg (p i ) -H t (p i )| + |S bg (p i ) -S t (p i )|,<label>(2)</label></formula><p>where H and S are the HSV components of the color images. Empirically we found that the HSV color space leads to better results which are robust to illumination changes. The depth based unary term is a logistic function of the form of</p><formula xml:id="formula_3">ψ depth (p i ) = 1 -1/(1 + exp(-(D t (p i ) -D bg (p i ))/σ)),</formula><p>where σ controls the scale of the resolution and is fixed to</p><formula xml:id="formula_4">σ = 5cm.</formula><p>Obtaining the Maximum a Posterior (MAP) solution under the model defined in Eq. 1 is computationally expensive.</p><p>To achieve real-time performance, we use the efficient algorithm for performing mean field inference that was proposed in <ref type="bibr" target="#b57">[58]</ref>, which we implemented efficiently on the GPU. In Fig. <ref type="figure" target="#fig_2">4</ref> we depict some results obtained using the proposed segmentation algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Temporally Consistent 3D Reconstruction</head><p>Given the N = 8 depth maps generated as described previously, we want to generate a 3D model of the region of interest. There are mainly 3 different strategies to achieve this. The simplest approach consists of visualizing the 3D model in the form of point clouds, however this will lead to multiple problems such as temporal flickering, holes and flying pixels (see Fig. <ref type="figure" target="#fig_3">5</ref>).</p><p>A better strategy consists of fusing the data from all the cameras <ref type="bibr" target="#b22">[23]</ref>. The fused depth maps generate a mesh per frame, with a reduced noise level and flying pixels compared to a simple point cloud visualization. However the main drawback of this solution is the absence of any temporal consistency: due to noise and holes in the depth maps, meshes generated at each frame could suffer from flickering effects, especially in difficult regions such as hair. This would lead to 3D models with high variation over time, making the overall experience less pleasant.</p><p>Motivated by these findings, we employ a state of the art method for generating temporally consistent 3D models in real-time <ref type="bibr" target="#b13">[14]</ref>. This method tracks the mesh and fuses the data across cameras and frames. We summarize here the key steps, for additional details see <ref type="bibr" target="#b13">[14]</ref>.</p><p>In order to fuse the data temporally, we have to estimate the nonrigid motion field between frames. Following <ref type="bibr" target="#b13">[14]</ref>, we parameterize the nonrigid deformation using the embedded deformation (ED) model of <ref type="bibr" target="#b53">[54]</ref>. We sample a set of K "ED  nodes" uniformly, at locations {g k } K k=1 ⊆ R 3 throughout the mesh V. The local deformation around an ED node g k is defined via an affine transformation A k ∈ R 3×3 and a translation t k ∈ R 3 . In addition, a global rotation R ∈ SO(3) and translation T ∈ R 3 are added. The full parameters to estimate the nonrigid motion field are G = {R, T }∪{A k , t k } K k=1 . The energy function we minimize is:</p><formula xml:id="formula_5">E(G) =λ data E data (G) + λ hull E hull (G) + λcorrEcorr(G) + λrotErot(G) + λregEreg(G),<label>(3)</label></formula><p>which is the weighted sum of multiple terms that take into account the data fidelity, visual hull constraints, sparse correspondences, a global rotation and a smoothness regularizer. All the details regarding these terms, including the implementation for solving this nonlinear problem, are in <ref type="bibr" target="#b13">[14]</ref>.</p><p>Once we retrieve the deformation model between two frames, we fuse the data of the tracked nonrigid parts and we reset the volume to the current observed 3D data for those regions where the tracking fails.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Color Texturing</head><p>After fusing depth data, we extract a polygonal 3D model from its implicit volumetric representation (TSDF) with marching cubes, and then texture this model using the 8 input RGB images (Fig. <ref type="figure" target="#fig_4">6</ref>). A naive texturing approach would compute the color of each pixel by blending the RGB images according to its surface normal n and the camera viewpoint direction vi . In this approach, the color weights are computed as w i = vis•max(0, n• vi ) α , favoring frontal views with factor α and vis is a visibility test. These weights are non-zero if the textured point is visible in a particular view (i.e., not occluded by another portion of the model). The visibility test is needed to avoid back-projection and is done by rasterizing the model from each view, producing rasterized depth maps (to distinguish from input depth maps), and using those for depth comparison. Given imperfect geometry, this may result in so-called "ghosting" artifacts (Fig. <ref type="figure" target="#fig_5">7a</ref>), when missing parts of geometry might cause wrongly projected color to the surfaces behind it. Many existing approaches use global optimization to stitch <ref type="bibr" target="#b17">[18]</ref> or blend <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b45">46]</ref> the incoming color maps to tackle this problem. However, these implementations are not real-time and in some cases use more RGB images.</p><p>Instead we assume that the surface reconstruction error is bounded by some value e. First, we extend the visibility test with an additional depth discontinuity test. At each textured point on a surface, for each input view we search for depth discontinuity in its projected 2D neighborhood in a rasterized depth map with radius determined by e. If such a discontinuity is found, we avoid using this view in a normal-weighted blending (the associated weight is 0). This causes dilation of edges in the rasterized depth map for each view (Fig. <ref type="figure" target="#fig_5">7bc</ref>). This can eliminate most ghosting artifacts, however it can classify more points as unobserved. In this case, we use regular visibility tests and a majority voting scheme for colors: for a given point, to classify each view as trusted, the color candidates for this view must agree with a number of colors (Fig. <ref type="figure" target="#fig_5">7d</ref>) from other views that can see this point, and this number of agreeing views should be maximum. The agreement threshold is multi-level: we start from as small value and increase the threshold if no previous views agreed. This leads to a more accurate classification of trusted views as it helps to avoid false-positives near color discontinuities. Finally, if none of the views agree, we pick the view with the minimal RGB variance for a projected 2D patch in the input RGB image, since larger color discontinuity for a patch is more likely to correspond to a depth discontinuity of a perfectly reconstructed surface, and thus that patch should not be used.</p><p>While this approach works in real time and eliminates most of the artifacts (Fig. <ref type="figure" target="#fig_5">7e</ref>), there are some failure cases when two ghosting color candidates can outvote one true color (Fig. <ref type="figure" target="#fig_5">7f</ref>). This is a limitation of the algorithm, and a trade-off between quality and performance, but we empirically derived that this occurs only in highly occluded regions and does not reduce the fidelity of the color reconstruction in the region of interest, like faces. We also demonstrate that the algorithm can handle complex, multi-user scenarios with many objects (Fig. <ref type="figure" target="#fig_6">8</ref>) using only 8 RGB cameras.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spatial Audio</head><p>Audio is the foundational medium of human communication. Without proper audio, visual communication, however immersive, is usually ineffective. To enhance the sense of immersion, auditory and visual cues must match. If a user sees a person to the left, the user should also hear that person to the left. Audio emanating from a spatial source outside the user's field of view also helps to establish a sense of immersion, and can help to compensate for the limited field of view in some HMDs.</p><p>In Holoportation we synthesize each remote audio source, namely the audio captured from a remote user, as coming from the position and orientation of the remote user in the local user's space. This ensures that the audio and visual cues are matched. Even without visual cues, users can use the spatial audio cues to locate each other. For example, in Holoportation, it is possible for the first time for users to play the game "Marco Polo" while all players are geographically distributed. To the best of our knowledge, Holoportation is the first example of auditory augmented or virtual reality to enable communication with such freedom of movement.</p><p>In our system, each user is captured with a monaural microphone. Audio samples are chunked into 20ms frames, and audio frames are interleaved with the user's current head pose information in the user's local room coordinate system (specifically, x, y, z, yaw, pitch, and roll). The user's audio frame and the head pose information associated with that frame are transmitted to all remote users through multiple unicast connections, although multicast connections would also make sense where available.</p><p>Upon receiving an audio frame and its associated head pose information from a remote user, the local user's system transforms the head pose information from the remote user's room coordinate system to the local user's room coordinate system, and then spatializes the audio source at the proper location and orientation. Spatialization is accomplished by filtering the audio signal using a head related transfer function (HRTF).</p><p>HRTF is a mapping from each ear, each audio frequency, and each spatial location of the source relative to the ear, into a complex number representing the magnitude and phase of the attenuation of the audio frequency as the signal travels from the source location to the ear. Thus, sources far away will be attenuated more than sources that are close. Sources towards the left will be attenuated less (and delayed less) to the left ear than to the right ear, reproducing a sense of directionality in azimuth as well as distance. Sources above the horizontal head plane will attenuate frequencies differently, giving a sense of elevation. Details may be found in <ref type="bibr" target="#b19">[20]</ref>.</p><p>We also modify the amplitude of a source by its orientation relative to the listener, assuming a cardioid radiation pattern. Thus a remote user facing away from the local user will sound relatively muffled compared to a remote user facing toward the local user. To implement audio spatialization, we rely on the HRTF audio processing object in the XAUDIO2 framework in Windows 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compression and Transmission</head><p>The previous steps generate an enormous amount of data per frame. Compression is therefore critical for transmitting that data over the wide area network to a remote location for rendering. Recent work in mesh compression <ref type="bibr" target="#b10">[11]</ref> as well as point cloud compression <ref type="bibr" target="#b11">[12]</ref> suggest that bit rates on the order of low tens of megabits per second per transmitted person, depending on resolution, are possible, although real-time compression at the lowest bit rates is still challenging.</p><p>For our current work, we wanted the rendering to be real time, and also of the highest quality. Hence we perform only a very lightweight real time compression and straightforward wire format over TCP, which is enough to support 5-6 viewing clients between capture locations over a single 10 Gbps link in our point to point teleconferencing scenarios. For this purpose it suffices to transmit a standard triangle mesh with additional color information from the various camera viewpoints.</p><p>To reduce the raw size of our data for real-time transmission, we perform several transformations on per-frame results from the capture and fusion stages, as we now describe.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mesh Geometry</head><p>As described previously, we use a marching cubes polygonalization of the volumetric data generated in the fusion stage as our rendered mesh. The GPU implementation of marching cubes uses 5 millimeters voxels. We perform vertex deduplication and reduce position and normal data to 16 bit floats on the GPU before transfer to host memory for serialization. During conversion to our wire format, we use LZ4 compression on index data to further reduce frame size.</p><p>For our current capture resolution this results in a per-frame transmission requirement of approximately 2MB per frame for mesh geometry (60K vertices, 40K triangles). Note that for proper compositing of remote content in AR scenarios, the reconstruction of local geometry must still be available support occlusion testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Color Imagery</head><p>For color rendering, the projective texturing described above is used for both local and remote rendering, which requires that we transfer relevant color imagery from all eight color cameras. As we only need to transmit those sections of the image that contribute to the reconstructed foreground objects, we leverage the foreground segmentation calculated in earlier stages to set non-foreground regions to a constant background color value prior to transmission. With this in place, the LZ4 compression of the color imagery (8 4MP Bayer images) reduces the average per-frame storage requirements from 32MB to 3MB per frame. Calibration parameters for projective mapping from camera to model space may be transferred per frame, or only as needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Audio Transmission</head><p>For users wearing an AR or VR headset, the associated microphone audio and user pose streams are captured and transmitted to all remote participants to generate spatial audio sources correlated to their rendered location, as described in the Audio subsection above. Each captured audio stream is monaural, sampled at 11 KHz, and represented as 16-bit PCM, resulting in a transmitted stream of 11000*16 = 176 Kbps, plus 9.6 Kbps for the pose information. At the receiving side, the audio is buffered for playback. Network jitter can cause the buffer to shrink when packets are not being delivered on time, and then to grow again when they are delivered in a burst. If the buffer underflows (becomes empty), zero samples are inserted. There is no buffer overflow, but the audio playback rate is set to 11025 samples per second, to provide a slight downward force on the playback buffer size. This keeps the playback buffer in check even if the receiving clock is somewhat slower than the sending clock. The audio+pose data is transmitted independently and bi-directionally between all pairs of remote participants. The audio communication system is peer-to-peer, directly between headsets, and is completely independent of the visual communication system. We do not provide AV sync, and find that the audio and visual reproductions are sufficiently synchronized; any difference in delay between audio and visual systems is not noticeable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bandwidth and Network Topology</head><p>For low-latency scenarios, these approaches reduce the average per-frame transmission size to a 1-2 Gbps transfer rate for an average capture stream at 30fps, while adding only a small overhead ( &lt;10ms) for compression. Compressed frames of mesh and color data are transmitted between capture stations via TCP to each active rendering client. For 'Living Memories' scenarios, described later in the applications section, these packets may be intercepted, stored, and replayed by an intermediary recording server. Large one-to-many broadcast scenarios, such as music or sports performance, requires further compression and multicast infrastructures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Render Offloading and Latency Compensation</head><p>For untethered VR or AR HMDs, like HoloLens, the cost of rendering a detailed, high quality 3D model from the user's perspective natively on the device can be considerable. It also increases motion-to-photon latency, which worsens the user experience. We mitigate this by offloading the rendering costs to a dedicated desktop PC on the receiving side, i.e., perform remote rendering, similar to <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b33">34]</ref>. This allows us to maintain consistent framerate, to reduce perceived latency, and to conserve battery life on the device, while also enabling highend rendering capabilities, which are not always available on mobile GPUs.</p><p>Offloading is done as follows: the rendering PC is connected to an untethered HMD via WiFi, and constantly receives user's 6DoF (six degree of freedom) pose from the HMD. It predicts a headset pose at render time and performs scene rendering with that pose for each eye (Fig. <ref type="figure" target="#fig_7">9a</ref>), encodes them to a video stream (Fig. <ref type="figure" target="#fig_7">9b</ref>), and transmits the stream and poses to the HMD. There, the video stream is decoded and displayed for each eye as a textured quad, positioned based on predicted rendered pose (Fig. <ref type="figure" target="#fig_7">9c</ref>), and then reprojected to the latest user pose (Fig. <ref type="figure" target="#fig_7">9f</ref>) <ref type="bibr" target="#b59">[60]</ref>.</p><p>To compensate for pose mispredictions and PC-to-HMD latency, we perform speculative rendering on the desktop side, based on the likelihood of the user pose. The orientation misprediction can be compensated by rendering into a larger FoV (field of view) (Fig. <ref type="figure" target="#fig_7">9d</ref>), centered around the predicted user direction. The HMD renders the textured quad with actual display FoV, thus allowing some small misprediction in rotation (Fig. <ref type="figure" target="#fig_7">9e</ref>). To handle positional misprediction, we could perform view interpolation techniques as in <ref type="bibr" target="#b33">[34]</ref>. However, it would require streaming the scene depth buffer and its reprojection, which would increase the bandwidth and HMD-side rendering costs. Since the number of objects are small and they are localized in the capture cube, we approximate the scene depth complexity with geometry of the same (textured) quad and dynamically adjust its distance to the user, based on the point of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IMPLEMENTATION</head><p>We use multiple GPUs across multiple machines to achieve real-time performance. At each capture site, 4 PCs compute depth and foreground segmentation (each PC handles two capture pods). Each PC is an Intel Core i7 3.4 Ghz CPU, 16 GB of RAM and it uses two NVIDIA Titan X GPUs. The resulting depth maps and segmented color images are then transmitted to a dedicated dual-GPU fusion machine over point-to-point 10Gbps connections. The dual-GPU unit is an Intel Core i7 3.4GHz CPU, 16GB of RAM, with two NVIDIA Titan X GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Depth Estimation: GPU Implementation</head><p>The depth estimation algorithm we use in this work comprises of three main stages: initialization, propagation, and filtering. Most of these stages are highly parallelizable and its computation is pixel independent (initialization). Therefore, a huge benefit in terms of compute can be obtained by implementing this directly on the GPU. Porting PatchMatch stereo <ref type="bibr" target="#b7">[8]</ref> to the GPU required a modification of the propagation stage in comparison with the original CPU implementation. The original propagation stage is inherently iterative and is performed in row order starting at the top-left pixel to the end of the image and in the reverse order, iterating twice over the image.</p><p>Our GPU implementation modifies this step in order to take advantage of the massive parallelism of current GPUs. The image is subdivided into neighborhoods of regular size and the propagation happens locally on each of these neighborhoods. With this optimization, whole rows and columns can be processed independently on separate threads in the GPU. In this case, the algorithm iterates through four propagation directions in the local neighborhood: from left to right, top to bottom, right to left, and bottom to top.</p><p>Finally, the filtering stage handles the removal of isolated regions that do not represent correct disparity values. For this stage, we implemented a Connected Components labeling algorithm on the GPU, so regions below a certain size are removed. This step is followed by an efficient GPU-based median algorithm to remove noisy disparity values while preserving edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Temporal Reconstruction: Dual-GPU Implementation</head><p>The implementation of <ref type="bibr" target="#b13">[14]</ref> is the most computationally expensive part of the system. Although with <ref type="bibr" target="#b13">[14]</ref> we can reconstruct multiple people in real time, room-sized dynamic scene reconstructions require additional compute power. Therefore, we had to revisit the algorithm proposed in <ref type="bibr" target="#b13">[14]</ref> and implement a novel dual-GPU scheme. We pipeline <ref type="bibr" target="#b13">[14]</ref> into two parts, where each part lives on a dedicated GPU. The first GPU estimates a low resolution frame-to-frame motion field; the second GPU refines the motion field and performs the volumetric data fusion. The coarse frame-to-frame motion field is computed from the raw data (depth and RGB). The second GPU uses the frame-to-frame motion to initialize the modelto-frame motion estimation, which is then used to fuse the data volumetrically. Frame-to-frame motion estimation does not require the feedback from the second GPU, so two GPUs can run in parallel. For this part we use a coarser deformation graph (i.e. we sample an ED node every 8 cm) and run 8 Levenberg-Marquardt (LM) solver iterations, with coarser voxel resolution and only 2 iterations are used for model-toframe motion estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Throughput-oriented Architecture</head><p>We avoid stalls between CPU and GPU in each part of our pipeline by using pinned (non-pageable) memory for CPU-GPU DMA data transfers, organized as ring buffers; overlapping data upload, download, and compute; and using syncfree kernel launches, maintaining relevant subproblem sizes on the GPU, having only its max-bounds estimations on the CPU. We found that overhead of using max-bounds is lower than with CUDA Dynamic Parallelism for nested kernel launches. Ring buffers introduce only processing latency into the system and maintain the throughput we want to preserve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Computational Time</head><p>Image acquisition happens in a separate thread overlapping the computing of the previous frame with the acquisition of the next one and introducing 1 frame latency. The average time for image acquisition is 4ms.</p><p>Each machine on the capture side generates two depth maps and two segmentation masks in parallel. The total time is 21ms and 4ms for the stereo matching and segmentation, respectively. In total each machine uses no more than 25ms to generate the input for the multiview non-rigid 3D reconstruction pipeline. Segmented RGBD frames are generated in parallel to the nonrigid pipeline, but do also introduce 1 frame of latency.</p><p>A master PC, aggregates and synchronizes all the depth maps and segmentation masks. Once the RGBD inputs are available, the average processing time (second GPU) to fuse all the depth maps is 29ms (i.e., 34fps) with 5ms for preprocessing (18% of the whole fusion pipeline), 14ms (47%) for the nonrigid registration (2 LM iterations, with 10 PCG iterations), and 10ms (35%) for the fusion stage. The visualization for a single eye takes 6ms on desktop GPU, and thus enables to display graphics at native refresh rate of 60Hz for HoloLens and 75Hz for Oculus Rift DK2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPLICATIONS</head><p>As shown in the supplementary video and Fig. <ref type="figure" target="#fig_8">10</ref>, we envision many applications for Holoportation. These fall into two broad categories. First, are one-to-one applications: these are scenarios where two remote capture rigs establish a direct connection so that the virtual and physical spaces of each rig are in one-to-one correspondence. Once segmentation is performed any new object in one space will appear in the other and vice versa. As shown in the video, this allows remote participants to be correctly occluded by the objects in the local space. Because our system is agnostic to what is actually captured, objects, props, and even furniture can be captured.</p><p>One to one calls are analogous to a telephone or video chat between two parties. However, with the ability to move around the space, and benefit from many physical cues. The second category consists of one-to-many applications: this is where a single capture rig is broadcasting a live stream of data to many receivers. In this case the physical space within the capture rig corresponds to the many virtual spaces of the remote viewers. This is analogous to the broadcast television model, where a single image is displayed on countless screens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>One-to-one</head><p>One-to-one applications are communication and collaboration scenarios. This could be as simple as a remote conversation. The distinguishing characteristic that Holoportation brings is a sense of physical presence through cues such as movement in space, gestures, and body language.</p><p>Specific one-to-one applications we foresee include:</p><p>• Family gatherings, where a remote participant could visit with loved ones or join a birthday celebration.</p><p>• Personal instruction, where a remote teacher could provide immediate feedback on dance moves, or yoga poses.</p><p>• Doctor patient, where a remote doctor could examine and interact with a patient to provide a diagnosis.</p><p>• Design meetings, where remote parties could share and interact with physical or virtual objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>One-to-many</head><p>A unique characteristic of Holoportation for one-to-many scenarios is live streaming. Other technologies can broadcast pre-processed content, as in Collet et al <ref type="bibr" target="#b10">[11]</ref>. We believe that live streaming provides a much more powerful sense of engagement and shared experience. Imagine a concert viewable from any seat in the house, or even from on stage. Similarly, live sporting events could be watched, or re-watched, from any point. When humans land on Mars, imagine waiting on the surface to greet the astronaut as she steps foot on the planet.</p><p>In both one-to-one or one-to many scenarios, Holoportation content can be recorded for replay from any viewpoint, and any scale. Room-sized events can be scaled to fit on a coffee table for comfortable viewing. Content can be paused, rewound, fast forwarded, to view any event of interest, from any point of view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Beyond Augmented Reality: A Body in VR</head><p>Finally, we point out that Holoportation can also be used in a single or multi player immersive VR experiences, where one's own body is captured by the Holoportation system and inserted into the VR world in real-time. This provides a virtual body that moves and appears like ones own physical body, providing a sense of presence.</p><p>As shown in the supplementary video, these experiences capture many new possibilities for 'social' AR and VR applications in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>USER STUDY</head><p>Having solved many of the technical challenges for real-time high quality immersive telepresence experiences, we wanted to test our prototype in a preliminary user study. We aimed to unveil opportunities and challenges for interaction design, explore technological requirements, and better understand the tradeoffs of AR and VR for this type of communication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Procedure and Tasks</head><p>We recruited a total of 10 participants (3 females; age 22 to 56) from a pool of partners and research lab members not involved with this project. In each study session, two participants were placed in different rooms and asked to perform two tasks (social interaction and physical object manipulation) using each of the target technologies, AR and VR. We counterbalanced the order of technology conditions as well as the order of the tasks using the Latin square design. The whole procedure took approximately 30 minutes.</p><p>For the duration of the study, two researchers observed and recorded participants' behavior, including their interaction patterns, body language, and strategies for collaboration in the shared space. After the study, researchers also conducted semi-structured interviews to obtain additional insight about the most salient elements of the users' experience, challenges, and potential usage scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Social Interaction Task: Tell-a-lie</head><p>To observe how people use Holoportation for verbal communication, we used a tell-a-lie task <ref type="bibr" target="#b65">[66]</ref>. All participants were asked to state three pieces of information about themselves, with one of the statements being false. The goal of the partner was to identify the false fact by asking any five questions. Both deception and deception detection are social behaviors that encourage people to pay attention to and accurately interpret verbal and non-verbal communication. Thus, it presents a very conservative testing scenario for our technology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Physical Interaction Task: Building Blocks</head><p>To explore the use of technology for physical interaction in the shared workspace, we also designed an object manipulation task. Participants were asked to collaborate in AR and VR to arrange six 3D objects (blocks, cylinders, etc.) in a given configuration (Fig. <ref type="figure" target="#fig_9">11</ref>). Each participant had only three physical objects in front of him on a stool, and could see the blocks of the other person virtually. During each task, only one of the participants had a picture of the target layout, and had to instruct the partner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Study Results and Discussion</head><p>Prior to analysis, researchers compared observation notes and partial transcripts of interviews for congruency. Further qualitative analysis revealed insights that fall into 5 categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adapting to Mixed-reality Setups</head><p>Participants experienced feelings of spatial and social copresence with their remote partners, which made their interaction more seamless. P4: "It's way better than phone calls. [...] Because you feel like you're really interacting with the person. It's also better than a video chat, because I feel like we can interact in the same physical space and that we are modifying the same reality."</p><p>The spatial and auditory cues gave an undeniable sense of co-location; so much so that many users even reported a strong sense of interpersonal space awareness. For example, most participants showed non-verbal indicators/body language typical to face-to-face conversations in real life (e.g. adopting the "closed" posture when lying in the social task; automatically using gestures to point or leaning towards each other in the building task). P6: "It made me conscious of [my own image]. It was kind of realistic in that sense, because whenever we are talking to someone around the table we are conscious of not wanting to slouch in front of the other person</p><p>[and] about how we look to the other person."</p><p>Participants also quickly adapted and developed strategies for collaborating in the mixed-reality setup. For example, several participants independently came up with the idea to remove their own blocks to let their partner arrange their shapes first, to avoid confusing the real and virtual objects. While participants often started by verbally instructing and simple pointing, they quickly figured out that it is easier and more natural for them to use gestures or even their own objects or hands to show the intended position and orientation. P2: "When I started I was kind of pointing at the shape, then I was just doing the same with my shape and then just say 'Ok, do that. Because then she could visually see, I could almost leave my shapes and wait until she put her shapes exactly in the same location and then remove mine."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Natural Interaction for Physical and Collaborative tasks</head><p>The shared spatial frame of reference, being more natural than spatial references in a video conversation, was mentioned as a main advantage of Holoportation. For example, P9 found that "[the best thing was] being able to interact remotely and work together in a shared space. In video, in Skype, even just showing something is still a problem: you need to align [the object] with the camera, then 'Can you see it?', then 'Can you turn it?' Here it's easy."</p><p>The perception of being in the same physical space also allows people to interact simply more naturally, even for full body interaction, as nicely described by P4: "I'm a dancer [...] and we had times when we tried to have Skype rehearsals. It's really hard, because you're in separate rooms completely. There's no interaction, they might be flipped, they might not. Instead with this, it's like I could look and say 'ok, this is his right' so I'm going to tell him move it to the right or towards me."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Influence of Viewer Technology</head><p>To probe for the potential uses of Holoportation, we used two viewer technologies with different qualities and drawbacks. Characteristics of the technology (e.g. FoV, latency, image transparency, and others) highly influenced participants' experiences, and, specifically, their feelings of social and spatial co-presence. To some participants, AR felt more realistic/natural because it integrated a new image with their own physical space. In the AR condition, they felt that their partner was "here", coming to their space as described by P7 "In the AR it felt like somebody was transported to the room where I was, there were times I was forgetting they were in a different room." The VR condition gave participants the impression of being in the partner's space, or another space altogether. P6: "[With VR] I felt like I was in a totally different place and it wasn't all connected to my actual surroundings."</p><p>Another interesting finding in the VR condition was the confusion caused by seeing a replica of both local and remote participants and objects. There were many instances where users failed to determine if the block they were about to touch was real or not, passing their hands directly through the virtual object. Obviously, rendering effects could help in this disambiguation, although if these are subtle then they may still cause confusions.</p><p>Related to AR and VR conditions was also the effects of latency. In VR the user's body suffered from latency due to our pipeline. Participants could tolerate the latency in terms of interacting and picking up objects, but a few users suffered from discomfort during the VR condition, which was not perceived in the AR condition. Despite this we were surprised how well participants performed in VR and how they commented on enjoying the greater sense of presence due to seeing their own bodies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Freedom to Choose a Point of View</head><p>One of the big benefits mentioned, was that the technology allows the viewer to assume the view they wanted, and move freely without constraints. This makes it very suitable for exploring objects or environments, or to explain complicated tasks that benefit from spatial instructions. P10 commented: "I do a lot of design reviews or things like that where you actually want to show somebody something and then explain in detail and it's a lot easier if you could both see the same things. So if you have a physical object and you want to show somebody the physical object and point to it, it just makes it a lot easier." However, this freedom of choice might be less suitable in case of a "directors narrative" in which the presenter wants to control the content and the view on it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Requirements for Visual Quality</head><p>While a quantitative analysis of small sample sizes needs careful interpretation, we found that most participants (70%) tended to agree or strongly agree that they perceived the conversation partner to look like a real person. However, some feedback about visual quality was less positive; e.g., occasional flicker that occurred when people were at the edge of the reconstruction volume was perceived to be off-putting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Eye Contact through the Visor</head><p>One key factor that was raised during the study was the presence of the headset in both AR and VR conditions, which clearly impacted the ability to make direct eye contact. Whilst we perceived many aspects of physical co-located interaction, users did mention this as a clear limitation. To overcome this problem, we have begun to investigate a proof-of-concept prototype for augmenting existing seethrough displays with headset removal. We utilize tiny wireless video cameras, one for each eye region that is mounted on the outer rim of the display (see Fig. <ref type="figure" target="#fig_10">12</ref>). We then project the texture associated with the occluded eye-region to a 3D reconstruction of the user's face.</p><p>To perform the projective texturing, we need to not only estimate the intrinsics calibration of the camera, but also the extrinsic parameters, which in this instance corresponds to the 3D position of the camera with respect to the face of the user. Given the 3D position of facial keypoints (e.g. eye corners, wings of the nose) in an image, it is possible to estimate the extrinsics of the camera by leveraging the 2D-3D position of these landmarks using optimization methods like <ref type="bibr" target="#b34">[35]</ref>. We use a discriminately trained cascaded Random Forest to generate series of predictions of the most likely position of facial keypoints in a manner similar to <ref type="bibr" target="#b49">[50]</ref>. To provide predictions that are spatially smooth over time, we perform a mean-shift filtering of the predictions made by Random Forest.</p><p>The method we just described requires the 3D location of keypoints to estimate the camera extrinsics. We extract this information from a high fidelity reconstruction of the user's face obtained by aggregating frames using KinectFusion <ref type="bibr" target="#b22">[23]</ref>. This process requires human intervention, but only takes a few seconds to complete. Note that once we have the 3D model, we render views (from known camera poses) and pass these to our Random Forest to estimate the 2D location of the facial keypoints. Knowing the camera poses used to render, we can ray-cast and estimate where the keypoints lie on the user's 3D face model. We also perform color correction to make sure that the texture coming from both cameras look compatible. At this stage, we have all the components to perform real-time projective texture mapping of eye camera images onto 3D face geometry and to perform geometric mesh blending of the eye region and the live mesh. Our preliminary results are shown in Fig. <ref type="figure" target="#fig_10">12</ref> and more results are presented in the attached video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LIMITATIONS</head><p>While we presented the first high quality 360 o immersive 3D telepresence system, our work does have many limitations. The amount of high-end hardware required to run the system is very high, with pairs of depth cameras requiring a GPU-powered PC, and a seperate GPU-based PC for temporal reconstruction, meshing and transmission. Currently, a 10 Gigabit Ethernet connection is used to communicate between rooms, allowing low latency communication between the users, which limits many Internet scenarios. More efficient compressions schemes need to be developed to address this requirement and compress geometry and color data for lower bandwidth connections. Moreover, during the texturing process we still observed color artifacts caused by extreme occlusions in the scene. More effective algorithms that take further advantage of the non-rigid tracking could further reduce the artifacts and the number of color cameras that are required. Regarding the 3D non-rigid reconstruction, we note that for many fine-grained interaction tasks, the 3D reconstruction of smaller geometry such as fingers produced artifacts, such as missing or merged surfaces. Finally, we note that developing algorithms for making direct eye contact through headset removal is challenging, and as yet we are not fully over the uncanny valley.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CONCLUSION</head><p>We have presented Holoportation: an end-to-end system for high-quality real-time capture, transmission and rendering of people, spaces, and objects in full 3D. We combine a new capture technology with mixed reality displays to allow users to see, hear and interact in 3D with remote colleagues. From an audio-visual perspective, this creates an experience akin to physical presence. We have demonstrated many different interactive scenarios, from one-to-one communication and oneto-many broadcast scenarios, both for live/real-time interaction, to the ability to record and playback 'living memories'. We hope that practitioners and researchers will begin to expand the technology and application space further, leveraging new possibilities enabled by this type of live 3D capture.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Overview of the Holoportation pipeline. Our capture units compute RGBD streams plus a segmentation mask. The data is then fused into a volume and transmitted to the other site. Dedicated rendering PCs perform the projective texturing and stream the live data to AR and VR displays.</figDesc><graphic coords="3,53.86,62.36,510.23,140.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Left: Holoportation rig. Middle: Trinocular Pod: Right: NIR and RGB images acquired from a pod.</figDesc><graphic coords="4,53.86,62.36,510.24,91.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Top: RGB stream. Bottom: depth stream. Images are masked with the segmentation output.</figDesc><graphic coords="5,321.02,205.20,243.08,169.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Temporal consistency pipeline. Top: Point cloud visualization, notice flying pixels and holes. Bottom: temporally reconstructed meshes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Projective Texturing: segmented-out RGB images and reconstructed color.</figDesc><graphic coords="5,53.86,205.20,243.07,187.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. (a) Ghosting artifacts, (b) Dilated depth discontinuities, (c) Dilated depth discontinuities from pod camera view, (d) Number of agreeing color (white = 4), (e) Proposed algorithm, (f) Voting failure case.</figDesc><graphic coords="6,53.86,62.36,243.07,198.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Examples of texturing multiple users and objects.</figDesc><graphic coords="7,53.86,62.36,243.06,128.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Render Offloading: (a) Rendering with predicted pose on PC, (b) Rendered image encoding, (c) Rendering with actual pose on HMD, (d) Predicted view on PC, and its over-rendering with enlarged FOV, (e) Pixels in decoded video stream, used during reprojection, (f) Reprojected actual view.</figDesc><graphic coords="7,321.02,62.36,243.06,181.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Applications. A) One-to-one communication. B) Business meeting. C) Live concert broadcasting. D) Living memory. E) Out-of-body dancing instructions. F) Family gathering. G) Miniature view. H) Social VR/AR.</figDesc><graphic coords="9,53.86,62.36,510.23,143.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. User study setting: Two participants performing the building blocks task in an AR condition (left) and a VR condition (right).</figDesc><graphic coords="10,321.02,62.36,243.05,73.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 12 .</head><label>12</label><figDesc>Figure 12. Visor Removal. The left image in the top row shows the placement of the inward looking cameras on the HMD while those on the center and right show the projections. The bottom row shows the facial landmarks localized by our pipeline to enable the projective texturing.</figDesc><graphic coords="11,321.02,147.18,243.07,145.70" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>http://www.scientificamerican.com/article/ can-videoconferencing-replace-travel/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Real-time 3d light field transmission</title>
		<author>
			<persName><forename type="first">T</forename><surname>Balogh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Kovács</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPIE Photonics Europe, International Society for Optics and Photonics</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="772406" to="772406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">PatchMatch: A randomized correspondence algorithm for structural image editing</title>
		<author>
			<persName><forename type="first">C</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Goldman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGGRAPH and Transaction On Graphics</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Recent progress in coded structured light as a technique to solve the correspondence problem: a survey</title>
		<author>
			<persName><forename type="first">J</forename><surname>Batlle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mouaddib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Salvi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="963" to="982" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Immersive group-to-group telepresence. Visualization and Computer Graphics</title>
		<author>
			<persName><forename type="first">S</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kunert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kulik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Froehlich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="616" to="625" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Miragetable: freehand interaction on a projected augmented reality tabletop</title>
		<author>
			<persName><forename type="first">H</forename><surname>Benko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI conference on human factors in computing systems</title>
		<meeting>the SIGCHI conference on human factors in computing systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="199" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Active, optical range imaging sensors</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Besl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine vision and applications</title>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="127" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Holographic three-dimensional telepresence using large-area photorefractive polymer</title>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Blanche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bablumian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Voorakaranam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Christenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Flores</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-Y</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kathaperumal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">468</biblScope>
			<biblScope unit="page" from="80" to="83" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">PatchMatch Stereo -Stereo Matching with Slanted Support Windows</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bleyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Detailed full-body reconstructions of moving people from monocular rgb-d sequences</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2300" to="2308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Toward a compelling sensation of telepresence: Demonstrating a portal to a distant (static) office</title>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Towles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nyland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Welch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fuchs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on Visualization&apos;00</title>
		<meeting>the conference on Visualization&apos;00</meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="327" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">High-quality streamable free-viewpoint video</title>
		<author>
			<persName><forename type="first">A</forename><surname>Collet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sweeney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gillett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Evseev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Calabrese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kirk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TOG</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">69</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Compression of 3d point clouds using a region-adaptive hierarchical transform</title>
		<author>
			<persName><forename type="first">R</forename><surname>De Queiroz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Chou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Temporally enhanced 3d capture of room-sized dynamic scenes with commodity depth cameras</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fuchs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Virtual Reality (VR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014 iEEE. 2014</date>
			<biblScope unit="page" from="39" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Real-time performance capture of challenging scenes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khamis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Degtyarev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Fanello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kowdle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">O</forename><surname>Escolano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Tankovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2016-07">July 2016</date>
		</imprint>
	</monogr>
	<note>Fusion</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning depth from structured light without matching</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fanello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Tankovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kowdle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Orts Escolano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName><surname>Hyperdepth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06">June 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Immersive 3d telepresence</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Bazin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="46" to="52" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Virtual space teleconferencing using a sea of cameras</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mcmillan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bajcsy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Farid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. First International Conference on Medical Robotics and Computer Assisted Surgery</title>
		<meeting>First International Conference on Medical Robotics and Computer Assisted Surgery</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Seamless montage for texturing models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wexler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ofek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="479" to="486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Teleport-towards immersive copresence</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Gibbs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Arapis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Breiteneder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Systems</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="214" to="221" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Gilkey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Anderson</surname></persName>
		</author>
		<title level="m">Binaural and Spatial Hearing in Real and Virtual Environments</title>
		<imprint>
			<publisher>Psychology Press</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">blue-c: a spatially immersive display and 3d video portal for telepresence</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Würmlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Naef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lamboray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Spagno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kunz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Koller-Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="819" to="827" />
			<date type="published" when="2003">2003</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Time-of-flight cameras: principles, methods and applications</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hansard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Horaud</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Kinectfusion: real-time 3d reconstruction and interaction using a moving depth camera</title>
		<author>
			<persName><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Molyneaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hodges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UIST 2011</title>
		<meeting><address><addrLine>Santa Barbara, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">October 16-19, 2011 (2011</date>
			<biblScope unit="page" from="559" to="568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Achieving eye contact in a one-to-many 3d video teleconferencing system</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fyffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Busch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mcdowall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bolas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Debevec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">64</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Roomalive: Magical experiences enabled by scalable, adaptive projector-camera units</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sodhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Murdock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mehra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Benko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ofek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Macintyre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Raghuvanshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shapira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th annual ACM symposium on User interface software and technology</title>
		<meeting>the 27th annual ACM symposium on User interface software and technology</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="637" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">First steps towards mutually-immersive mobile telepresence</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2002 ACM conference on Computer supported cooperative work</title>
		<meeting>the 2002 ACM conference on Computer supported cooperative work</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="354" to="363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Virtualized reality: Constructing virtual worlds from real scenes</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE multimedia</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="34" to="47" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Telehuman: effects of 3d perspective on gaze and pose estimation with a life-size cylindrical telepresence pod</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Girouard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cooperstock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vertegaal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2531" to="2540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Simultaneous segmentation and pose estimation of humans using dynamic graph cuts</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rihan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="285" to="298" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems: 25th Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Granada, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Immersive 3d environment for remote collaboration and training of physical activities</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kurillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bajcsy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nahrsted</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kreylos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Virtual Reality Conference, 2008. VR&apos;08</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="269" to="270" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Kuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ranieri</surname></persName>
		</author>
		<author>
			<persName><surname>Agustina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zimmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Bazin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<title level="m">Towards next generation 3d teleconferencing systems</title>
		<imprint>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Towards next generation 3d teleconferencing systems</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ranieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zimmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bazin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DTV-Conference: The True Vision-Capture</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
	<note>Transmission and Display of 3D Video (3DTV-CON</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Using speculation to enable low-latency continuous interaction for mobile cloud gaming</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cuervo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Degtyarev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Grizan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wolman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Flinn</forename></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Outatime</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Annual International Conference on Mobile Systems, Applications, and Services</title>
		<meeting>the 13th Annual International Conference on Mobile Systems, Applications, and Services</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="151" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">An accurate o(n) solution to the pnp problem</title>
		<author>
			<persName><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName><surname>Epnp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<date type="published" when="2009-02">Feb. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Real-time high-resolution sparse voxelization with application to image-based modeling</title>
		<author>
			<persName><forename type="first">C</forename><surname>Loop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th High-Performance Graphics Conference</title>
		<meeting>the 5th High-Performance Graphics Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="73" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Mobility in collaboration</title>
		<author>
			<persName><forename type="first">P</forename><surname>Luff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Heath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1998 ACM conference on Computer supported cooperative work</title>
		<meeting>the 1998 ACM conference on Computer supported cooperative work</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="305" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Encumbrance-free telepresence system with real-time 3d capture and display using commodity depth cameras</title>
		<author>
			<persName><forename type="first">A</forename><surname>Maimone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fuchs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mixed and Augmented Reality (ISMAR), 2011 10th IEEE International Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="137" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Real-time volumetric 3d capture of room-sized scenes for telepresence</title>
		<author>
			<persName><forename type="first">A</forename><surname>Maimone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fuchs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DTV-Conference: The True Vision-Capture, Transmission and Display of 3D Video (3DTV-CON)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">General-purpose telepresence with head-worn optical see-through displays and projector-based lighting</title>
		<author>
			<persName><forename type="first">A</forename><surname>Maimone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dierk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>State</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fuchs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Virtual Reality (VR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="23" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Post-rendering 3d warping</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mcmillan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1997 symposium on Interactive 3D graphics</title>
		<meeting>the 1997 symposium on Interactive 3D graphics</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">3d tv: a scalable system for real-time acquisition, transmission, and autostereoscopic display of dynamic scenes</title>
		<author>
			<persName><forename type="first">W</forename><surname>Matusik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="814" to="824" />
			<date type="published" when="2004">2004</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Interactive environment-aware handheld projectors for pervasive computing spaces</title>
		<author>
			<persName><forename type="first">D</forename><surname>Molyneaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hodges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gellersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pervasive Computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="197" to="215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The uncanny valley [from the field]. Robotics &amp; Automation Magazine</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">F</forename><surname>Macdorman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kageki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="98" to="100" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">An autostereoscopic projector array optimized for 3d facial display</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nagano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Busch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bolas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Debevec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2013 Emerging Technologies</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Optimized color models for high-quality 3d scanning</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ International Conference on</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2503" to="2510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Room2room: Enabling life-size telepresence in a projected augmented reality environment</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pejsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kantor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Benko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ofek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CSCW 2016</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Gergle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Morris</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Bjrn</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Konstan</surname></persName>
		</editor>
		<meeting><address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016-03-02">February 27 -March 2, 2016. 2016</date>
			<biblScope unit="page" from="1714" to="1723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Multicamera real-time 3d modeling for telepresence and remote collaboration</title>
		<author>
			<persName><forename type="first">B</forename><surname>Petit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-D</forename><surname>Lesage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Menier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Allard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-S</forename><surname>Franco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Raffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Boyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Faure</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Digital Multimedia Broadcasting</title>
		<imprint>
			<date type="published" when="2009">2010. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Surface measurement by space-encoded projected beam systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Posdamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Altschuler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer graphics and image processing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Face alignment at 3000 fps via regressing local binary features</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1685" to="1692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Fast cost-volume filtering for visual correspondence and beyond</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hosni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bleyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gelautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Accurate real-time disparity estimation with variational methods</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kosov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISVC</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A taxonomy and evaluation of dense two-frame stereo correspondence algorithms</title>
		<author>
			<persName><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="7" to="42" />
			<date type="published" when="2002-04">Apr. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Embedded deformation for shape manipulation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Sumner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pauly</forename></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TOG</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">80</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Real world video avatar: real-time and real-size transmission and presentation of human figure</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tanikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hirota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hirose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2005 international conference on Augmented tele-existence</title>
		<meeting>the 2005 international conference on Augmented tele-existence</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="112" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Near real-time stereo based on effective cost aggregation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mattoccia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Stefano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Addimanda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">3d tele-collaboration over internet2</title>
		<author>
			<persName><forename type="first">H</forename><surname>Towles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-U</forename><surname>Kum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">F N</forename><surname>Kelshikar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mulligan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K J</forename><surname>Mulligan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Workshop on Immersive Telepresence</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Citeseer</publisher>
			<pubPlace>Juan Les Pins</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Filter-based mean-field inference for random fields with higher-order terms and product label-spaces</title>
		<author>
			<persName><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Warrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2012 -12th European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">7576</biblScope>
			<biblScope unit="page" from="31" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Grid coding: A preprocessing technique for robot and machine vision</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Will</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Pennington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="319" to="329" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Late stage reprojection</title>
		<author>
			<persName><forename type="first">O</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Service</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Michail</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pearson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shetter</surname></persName>
		</author>
		<idno>App. 13/951</idno>
		<imprint>
			<date type="published" when="2015">Jan. 29 2015</date>
			<biblScope unit="page">351</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">US Patent</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Real-time global stereo matching using hierarchical belief propagation</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nistr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Viewport: A distributed, immersive teleconferencing system with infrared dot pattern</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Martin-Brualla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="17" to="27" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">A flexible new technique for camera calibration</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1330" to="1334" />
			<date type="published" when="2000-11">Nov. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Color map optimization for 3d reconstruction with consumer depth cameras</title>
		<author>
			<persName><forename type="first">Q.-Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">155</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Real-time non-rigid reconstruction using an rgb-d camera</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Loop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stamminger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Verbal and nonverbal communication of deception</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zuckerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Depaulo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rosenthal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in experimental social psychology</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">59</biblScope>
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
