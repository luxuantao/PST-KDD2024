<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Identifying Meaningful Citations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Marco</forename><surname>Valenzuela</surname></persName>
							<email>marcov@email.arizona.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Arizona Tucson</orgName>
								<address>
									<region>AZ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vu</forename><surname>Ha</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Allen Institute for Artificial Intelligence Seattle</orgName>
								<address>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
							<email>orene@allenai.org</email>
							<affiliation key="aff1">
								<orgName type="institution">Allen Institute for Artificial Intelligence Seattle</orgName>
								<address>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Identifying Meaningful Citations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce the novel task of identifying important citations in scholarly literature, i.e., citations that indicate that the cited work is used or extended in the new effort. We believe this task is a crucial component in algorithms that detect and follow research topics and in methods that measure the quality of publications. We model this task as a supervised classification problem at two levels of detail: a coarse one with classes (important vs. non-important), and a more detailed one with four importance classes. We annotate a dataset of approximately 450 citations with this information, and release it publicly. We propose a supervised classification approach that addresses this task with a battery of features that range from citation counts to where the citation appears in the body of the paper, and show that, our approach achieves a precision of 65% for a recall of 90%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Tracking citations is an important component of analyzing scholarly big data. Citations provide a quantitative way to measure the quality of published works, to detect emerging research topics, and to follow evolving ones.</p><p>In this work we argue that not all citations are equal. While some indeed indicate that the cited work is used or, more importantly, extended in the new publication, some are less important, e.g., they discuss the cited work in the context of related work that does not directly impact the new effort. To illustrate this point, Table <ref type="table">1</ref> lists several citations in increasing order of importance. We further argue that because current citation tracking algorithms do not distinguish between important vs. incidental citations, all of the above applications (e.g., measuring the quality of a publication, or tracking research topics) are negatively affected.</p><p>To our knowledge, this work is among the first to tackle the problem of identifying important citations. The contributions of our work are the following:</p><p>1. We introduce the novel task of identifying important citations, defined as a classification task with either two classes (important vs. non-important citation) or four classes (following the examples in Table <ref type="table">1</ref>).</p><p>Copyright c 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Citation Type</head><p>Citation Text incidental: related work</p><p>Discriminative models have recently been proved to be more effective than generative models in some NLP tasks, e.g., parsing <ref type="bibr">(Collins 2000)</ref>, POS tagging <ref type="bibr">(Collins 2002)</ref> and LM for speech recognition <ref type="bibr">(Roark et al. 2004</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>incidental: comparison</head><p>Online baselines include Top-1 Perceptron <ref type="bibr">(Collins, 2002)</ref>, Top-1 Passive-Aggressive (PA), and k-best PA <ref type="bibr">(Crammer &amp; Singer, 2003;</ref><ref type="bibr">McDonald et al., 2004)</ref>. important: using the work Here, we follow the definition of Collins perceptron <ref type="bibr">(Collins, 2002)</ref>.</p><p>The part-of-speech tagger is our reimplementation of the work in <ref type="bibr">(Collins, 2002)</ref>. important: extending the work</p><p>We describe a new sequence alignment model based on the averaged perceptron <ref type="bibr">(Collins, 2002)</ref>, which shares with the above... Our learning method is an extension of Collins's perceptron-based method for sequence labeling <ref type="bibr">(Collins, 2002)</ref>.</p><p>Table <ref type="table">1</ref>: Citation examples for <ref type="bibr">(Collins, 2002)</ref>, listed in increasing order of importance.</p><p>2. We annotate a dataset of approximately 450 citations with citation importance information. The dataset is publicly available in the hope that it will foster further research on this topic. 3. We propose a supervised classification approach that separates important from incidental citations using a battery of features, ranging from citation counts to where the citation appears in the body of the paper. Our approach models both direct citations, i.e., citations that follow established proceeding formats, and indirect citations, i.e., which use a description of the algorithm instead (e.g., "Stanford parser"). Our method performs well, obtaining a precision of 65% for a recall of 90%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data</head><p>To address this task, we used a collection of 20,527 papers from the ACL anthology<ref type="foot" target="#foot_0">1</ref> , together with their citation graph  and metadata generated by <ref type="bibr">(Elkiss et al. 2008</ref>). There were 106,509 citations among them. We annotated 465 of these citations, represented as tuples of (cited paper, citing paper), with ordinal labels ranging from 0 to 3, in increasing order of importance. The four classes follow the examples in Table 1. To obtain a coarse, binary label set, we also collapsed these fine-grained labels, such that 0 and 1 indicate incidental citations, and 2 and 3 indicate important citations. Table <ref type="table" target="#tab_1">2</ref> summarizes both the fine-grained and the coarse label sets. The citations were annotated by one expert, but we verified inter-annotator agreement between two experts for a subset of the dataset. For the set of four fine-grained labels, the annotators agreed on 83.6% of the citations; for the coarse label set, the inter-annotator agreement was 93.9%. These results indicates that this task is relatively easy for domain experts.</p><p>Crucially, we found that in this dataset only 14.6% of the annotated citations are considered important, i.e., they are labeled 2 (using the cited work) or 3 (extending the work). This further demonstrates that the identification of important citations is an important task, since most citations are actually incidental.</p><p>The dataset of papers behind these citations was preprocessed as follows:</p><p>• To extract the text from the PDF files we used Poppler's pdftotext<ref type="foot" target="#foot_1">2</ref> .</p><p>• The text was normalized by removing diacritics with an in-house script.</p><p>• For sentence splitting, tokenization and POS tagging we used Factorie <ref type="bibr">(McCallum, Schultz, and Singh 2009)</ref>.</p><p>• For shallow parsing, or chunking, we used OpenNLP<ref type="foot" target="#foot_2">3</ref> .</p><p>• To identify the section in which the citation occurs, we used ParsCit to segment the paper into sections (Luong, Nguyen, and Kan 2010). ParsCit provides normalized section names, which we use instead of the section titles. For example, both "State of the Art", and "Previous Work" are normalized to related_work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approach</head><p>Our modeling of citation importance is driven by three key observations:</p><p>1. The more citations a paper receives in the body of the citing work, the more important the citation is likely to be.</p><p>2. It matters where the citation appears. For example, a citation in the Related Work section is likely to indicate an </p><note type="other">Citation Type Citation Text Algorithm name</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm name</head><p>We implement a part-of-speech tagger with averaged perceptron.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm name</head><p>We implemented the MXPOST tagger and integrated it with our algorithm. Author + Algorithm name However, the application of the Yarowsky algorithm to NER involves several domainspecific choices as will become evident below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Author</head><p>The behaviour is slightly different here, with Charniak obtaining better results than Bikel in most cases. incidental citation. On the other hand, a citation in the Methods section indicates that the cited work is used or extended in the citing paper, which signals importance.</p><p>3. Citations appear in many forms. Some are direct, i.e., the citation follows an established proceedings format, or indirect, where the work is cited by mentioning the name of an author, typically the first author, the name of the cited algorithm, of a description of the algorithm. Table <ref type="table" target="#tab_2">3</ref> shows examples of indirect citations. Thus, in order to reliably implement the first two observations, one has to first identify both direct and indirect citations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Identifying Direct and Indirect Citations</head><p>We identified direct citations using rules that follow the citation format of the ACL proceedings, and matched them to unique paper identifiers in our corpus. A regular expression was generated using the paper metadata. This regular expression was designed to match citations that follow the format of the ACL proceedings and some variations that occurred in our corpus. For example, the required syntax for a citation to a paper with three authors is to write the last name of the first author followed by the phrase "et al." but we found that many papers in our corpus mention all the authors last names.</p><p>To identify indirect citations, we implemented two heuristics: one focusing on author names, and one addressing names or descriptions of the cited algorithms. We extracted indirect citations by author name by first finding all citations to any paper in the citing paper's text and then matching the last name of the first author of the cited paper of interest outside any of the direct citations.</p><p>Automatically identifying algorithm name or descriptions is less trivial. For this, we implement a two step algorithm:</p><p>1. For any given cited paper, we first find the papers that cite it in the entire corpus of 20K+ papers, and we extract the corresponding citations. Then we extract: (a) the noun phrase directly before the citation, or (b) the noun phrase following the citation and a verb. For this step, we used the Knowitall Taggers tool, a pattern-matching tool that functions over tokens and incorporates part-of-speech and shallow syntactic information<ref type="foot" target="#foot_3">4</ref> . Table <ref type="table" target="#tab_3">4</ref> shows examples of citations followed by a verb and a noun phrase. Most of these noun phrases are informative, but some do not describe the cited approach, e.g., focusing instead on its "accuracy" and "recall". We address these errors below with a robust heuristic inspired from information retrieval.</p><p>2. In the second step, we collect the unigrams and bigrams found in the noun phrases related to each cited paper and identify the most important ones by selecting the ones with a tf-idf score (Manning, Raghavan, and Schütze 2008) above some threshold (arbitrarily set to 200). Table <ref type="table" target="#tab_5">5</ref> shows examples of the extracted names and descriptions, which illustrates that this filtering manages to remove most of noise introduced in the previous step. A remaining limitation of our approach is that we currently use unigrams and bigrams, which might not to be sufficient to capture longer descriptions. We analyze this issue later in the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Features</head><p>Using both direct and indirect citations, we extract the following features from each citation tuple:</p><p>• (F1) Total number of direct citations: This feature counts the total number of citations to the cited paper. • (F2) Number of direct citations per section: Similar to the above feature, but counts are qualified by the section in which they appear. For this feature we used the normalized section titles produced by ParsCit. For example, if a paper has five citations, with two appearing in the Related Work section and three in Methods, we generate two features: DirectCountsRelatedWork with a value of 2 and DirectCountsMethods with a value of 3.  This feature records the number of citing papers after the transitive closure, e.g., papers that cite the cited work, papers that cite those papers, etc. • (F12) Field of the cited paper: This feature stores the particular computer science subfield to which the cited paper belongs. This is work in progress: we currently developed a classifier that identifies if a paper describes a software system or not. This classifier was developed as part of a scientific literature search engine and is based on bag of words technique matching system names with citation contexts.</p><p>For learning, we used classifiers implemented in the scikit learn toolkit<ref type="foot" target="#foot_4">5</ref> , in particular support vector machines (SVM) and random forests. We normalized all numeric features by centering on the mean and scaling to unit variance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Identifying important citations</head><p>For the main experiments in this section, i.e., identifying important citations, we used a leave-one-out cross-validation setup, i.e., we repeatedly evaluated the performance of our  models on a different citation, by training on all remaining ones. For this experiment, we used only the binary coarse labels, i.e., important vs. incidental, and employed the standard precision, recall, and F1 scores as evaluation measures, considering the important citations as the positive class.</p><p>Figure <ref type="figure" target="#fig_2">1</ref> shows the results of our model trained with two classifiers: SVM with a RBF kernel, and random forests. To obtain the P/R curve, we used various thresholds on the classifier confidence. Both classifiers are compared against a baseline that randomly assigns the "important" label using a probability p, which varies from 0 to 1. The red dot in the figure corresponds to a value of p equal to the the prior distribution of the "important" label in the entire corpus (i.e., 14.6%).</p><p>The results in the figure show that our proposed model considerably outperforms the baseline: for example, for a recall of 0.9 our SVM model has a precision of approximately 0.65, whereas the baseline's precision at the same recall point is under 0.2. This is an important result, which shows that under a high-recall requirement (which is a common scenario for a real-life system -see the Discussion section) our system has a reasonable precision. Overall, both classifiers have an area under the curve of 0.80. We consider this a very encouraging result for our relatively simple model.</p><p>Figure <ref type="figure" target="#fig_4">2</ref> shows the learning curve of the SVM classifier. For this experiment, we used a simpler, three-fold cross val-  idation. For each testing fold, we randomly selected subsets of the training data for each point in the curve. To avoid potential biases in the random subset selection, we repeated the experiments five times and averaged the results. This experiment indicates that the classifier learns relatively quickly, achieving near optimal performance with half of the training data available. This suggests that, even though our corpus is relatively small, its size is not a drastic constraint on performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature analysis</head><p>To understand the contribution of each of the features proposed in the previous section, we performed a post-hoc analysis, where we evaluated variants of our model containing a single feature group at a time. Because we are ultimately interested in a high-recall configuration of the classifier, where no important citations are missed (see the Discussion section), for this analysis we enforced a high recall of 0.9 for all configurations. The results are listed in Table <ref type="table" target="#tab_6">6</ref>.</p><p>The table highlights that all individual features perform better the random baseline. Recall that the baseline had both precision and recall under 0.2, whereas most of our features have a precision of over 0.2 for a recall of 0.9. The fact that all features contribute to the overall performance is highlighted by the performance of the system that uses all features (last row in the table), which is nearly double that of the best performing individual feature.</p><p>However, the individual feature contributions vary widely. The best performing features are the direct citations (both globally and per section) (F2, F1), followed by author overlap between citing and cited papers (F4), and textual hints that the cited work is considered useful by the authors of the citing paper (F5). The least performing features are: our PageRank score (F10) (perhaps due the small size of our paper dataset), the total number of citing papers after transitive closure (F11) (which suggests that influence dissipates beyond the immediate citations), and similarity of abstracts (F9) (suggesting that researchers working on similar topics are not necessarily influencing each other).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluating paper descriptions</head><p>Although the paper descriptions extracted by our algorithm are informative (see Table <ref type="table" target="#tab_5">5</ref>), the analysis in the previous sub-section indicates that indirect citations, which use these descriptions, contribute minimally to the overall performance. To better understand this issue, we performed a direct evaluation of the descriptions that our algorithm extracts.</p><p>In this work we focus solely on the precision of the set of extracted descriptions, which are responsible for the indirect citations. <ref type="foot" target="#foot_5">6</ref> For this purpose, we selected a subset of 50 papers from the larger corpus of +20K papers. 12 of these papers appear as cited papers in our smaller, annotated citation corpus. For these 50 papers, we collected all the descriptions automatically extracted by our approach. The descriptions in this set total 119. A domain expert analyzed these descriptions, and produced two precision scores: a lenient score, which considers a n-gram description as correct if it is part of a correct description, and a strict score, which considers a description as correct only if it forms a complete, nonambiguous description. For example, for the paper "Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling"<ref type="foot" target="#foot_6">7</ref> , the expert considered the description "entity recognizer" correct under the lenient score but incorrect under the strict one, whereas the description "stanford ner" is marked as correct under both scores.</p><p>The results of this evaluation were a lenient precision score of 115/119 = 96.6%, and a strict precision of 46/119 = 38.7%. This analysis indicates that the n-grams extracted are almost always relevant (hence the high lenient score), but seldom complete (hence the low strict score). For example, for the above paper, our algorithm extracts the following descriptions: "stanford named", "named", "recognizer", "named entity", "entity recognizer", "stanford", and "stanford ner". While these are clearly relevant for this paper, the incomplete descriptions, e.g., "named entity", may have two undesired effects: (a) they are likely to match in the context of another paper, yielding incorrect indirect citations, and (b) they may cause spurious citations, when multiple incomplete descriptions that form a complete one (e.g., "stanford named" and "entity recognizer") match in the same sentence. In this work, we mitigate the latter issue by counting sentences rather than individual indirect citations. In future work, we will explore more complex solutions, such as n-gram tiling <ref type="bibr">(Dumais et al. 2002)</ref>, which combine multiple incomplete n-grams to form a complete description. One of the strengths of this work is its immediate applicability to a real-world problem. We have incorporated our citation classifier into a scientific literature search engine, such that users can immediately identify the most important followup work for a given cited paper. Figure <ref type="figure" target="#fig_0">3</ref> shows a screenshot of this search engine using our work. The example highlights that of the ten papers that cite the paper "The infinite HMM for unsupervised PoS tagging" only two are considered important and are shown first. To avoid missing important citations, this system uses the high-recall configuration of our system (R 0.90, P 0.65).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>This work is however far from complete. In future work we will improve the extraction of the paper description and, correspondingly, of the indirect citations, by implementing tiling algorithms that merge incomplete descriptions. We will continue to improve the features used to represent a citation. For example, we will explore different ways of normalizing citation counts, e.g., by dividing by the total number of references and/or citations in the citing paper. Also, we will evaluate new features like the rhetorical function of citations <ref type="bibr">(Teufel, Siddharthan, and Tidhar 2006)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>There has been considerable effort in the past decade on citation indexing systems <ref type="bibr">(Giles, Bollacker, and Lawrence 1998;</ref><ref type="bibr">Lawrence, Giles, and Bollacker 1999;</ref><ref type="bibr" target="#b0">Councill, Lee, and Giles 2006</ref>) and on algorithms that analyze these citation graphs to, e.g., understand the flow of research topics in the literature, model the influence of specific papers in their field, or recommend citations for a given topic; see, inter alia, <ref type="bibr">(Dietz, Bickel, and Scheffer 2007;</ref><ref type="bibr">Gruber, Rosen-Zvi, and Weiss. 2008;</ref><ref type="bibr">Nallapati et al. 2008;</ref><ref type="bibr">Daume III 2009;</ref><ref type="bibr">Sun et al. 2009;</ref><ref type="bibr">Wong et al. 2009;</ref><ref type="bibr">Nallapati, McFarland, and Manning 2011)</ref>. However, by and large, these works assume that all citations are important, which we dispute in our work. We argue that by identifying the citations that are truly important, we will arrive at a better understanding of published research, which will lead to novel or more accurate applications of scholarly big data.</p><p>Our work is closest to <ref type="bibr">(Zhu et al. 2013)</ref>, which focuses on identifying key references for a given paper. Zhu et al. create a dataset of citations, labeled according to their influence by the authors of the citing papers, and train a supervised classifier with four features to predict academic influence. Our work is different in that our dataset of citations is annotated by (unbiased) domain experts and we explore a much larger feature set (twelve vs. four).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>To our knowledge, this paper is among the first to tackle the important task of identifying important citations, which, we believe, will ultimately improve many applications that focus on tracking scholarly citations, such as detecting and following research trends, or quantitatively measuring the quality and impact of publications.</p><p>In addition to introducing and formalizing this task, our contributions include a novel dataset of 465 citation tuples, which is publicly available<ref type="foot" target="#foot_7">8</ref> . We also describe a supervised classification approach for identifying meaningful citations, which uses a battery of features ranging from citation counts to where the citation appears in the body of the paper. Using the previously described dataset, we show that our approach performs well, obtaining a precision of 0.65 for a high recall of 0.9. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Stanford Parser output example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>• (F9) Similarity between abstracts: This feature computes the similarity between the cited and citing paper's abstracts using the cosine similarity of the tf-idf scores. The intuition behind this feature is that the closer the abstracts, the more likely the new work extends the cited paper. • (F10) PageRank: This feature computes the PageRank score (Page et al. 1999) of the cited paper, as a measure of the cited work's importance. • (F11) Number of total citing papers after transitive closure:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Precision-recall curve for our baseline and two classifiers: SVM with a RBF kernel and random forests.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Learning curve for the SVM classifier with RBF kernel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Screenshot of the scientific literature search engine that uses this work. For each cited paper, the top right block lists the important citations out of the total citations found in the indexed corpus.</figDesc><graphic url="image-1.png" coords="5,325.35,345.24,226.80,151.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Daume III, H. 2009. Markov random topic fields. In ACL-IJCNLP. Dietz, L.; Bickel, S.; and Scheffer, T. 2007. Unsupervised prediction of citation influences. In International Conference on Machine Learning. Dumais, S.; Banko, M.; Brill, E.; Lin, J.; and Ng, A. 2002. Web question answering: Is more always better? In SIGIR. Elkiss, A.; Shen, S.; Fader, A.; Erkan, G.; States, D.; and Radev, D. 2008. Blind men and elephants: What do citation summaries tell us about a research article? J. Am. Soc. Inf. Sci. Technol. 59(1):51-62. Giles, C. L.; Bollacker, K. D.; and Lawrence, S. 1998. Citeseer: an automatic citation indexing system. In ACM conference on Digital libraries. Gruber, A.; Rosen-Zvi, M.; and Weiss., Y. 2008. Latent topic models for hypertext. In Uncertainty in Artificial Intelligence. Lawrence, S.; Giles, C. L.; and Bollacker, K. D. 1999. Digital libraries and autonomous citation indexing. Computer 32(6):67-71. Luong, M.-T.; Nguyen, T. D.; and Kan, M.-Y. 2010. Logical structure recovery in scholarly articles with rich document features. International Journal of Digital Library Systems (IJDLS) 1(4):1-23. Manning, C. D.; Raghavan, P.; and Schütze, H. 2008. Introduction to information retrieval, volume 1. Cambridge university press Cambridge. McCallum, A.; Schultz, K.; and Singh, S. 2009. FAC-TORIE: Probabilistic programming via imperatively defined factor graphs. In Neural Information Processing Systems. Nallapati, R. M.; Ahmed, A.; Xing, E. P.; and Cohen, W. W. 2008. Joint latent topic models for text and citations. In KDD. Nallapati, R.; McFarland, D.; and Manning, C. 2011. Unsupervised learning of topic specific influences of hyperlinked documents. In Artificial Intelligence and Statistics. Page, L.; Brin, S.; Motwani, R.; and Winograd, T. 1999. The pagerank citation ranking: Bringing order to the web. Technical Report 1999-66, Stanford InfoLab. Sun, C.; Gao, B.; Cao, Z.; and Li, H. 2009. HTM: a topic model for hypertexts. In Empirical Methods in Natural Language Processing. Teufel, S.; Siddharthan, A.; and Tidhar, D. 2006. Automatic classification of citation function. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing. Wong, C.; Thiesson, B.; Meek, C.; and Blei, D. 2009. Markov topic models. In Artificial Intelligence and Statistics. Zhu, X.; Turney, P.; Lemire, D.; and Vellino, A. 2013. Measuring academic influence: Not all citations are equal. submitted to Journal of the Association for Information Science and Technology (JASIST).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Citation annotation labels.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Indirect citations by name of first author or name/description of the cited algorithm.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Examples of noun phrases following citations. The paper ids are from the ACL ontology; to retrieve the paper content append the id at the end of this URL: http://www.aclweb. org/anthology/.</figDesc><table><row><cell>Cited paper id</cell><cell>Verb</cell><cell>Noun phrase</cell></row><row><cell>P05-1044</cell><cell>proposed</cell><cell>a new objective function</cell></row><row><cell>J96-1002</cell><cell>presented</cell><cell>a Maximum Entropy Approach</cell></row><row><cell>A00-1031</cell><cell>reports</cell><cell>96.7% overall accuracy</cell></row><row><cell>W06-1643</cell><cell>used</cell><cell>skip-chain Conditional Random Fields</cell></row><row><cell>P08-1108</cell><cell>combined</cell><cell>MSTParser and MaltParser</cell></row><row><cell>W02-1001</cell><cell>extended</cell><cell>the perceptron algorithm</cell></row><row><cell>P02-1018</cell><cell>reports</cell><cell>93% precision and 83% recall</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>The intuition behind this feature is that shared authors indicate that the new work is likely to be an extension of the cited paper.• (F5) Is considered helpful (Boolean): This feature is set to true if a sentence in which a citation occurs contains phrases such as "we follow" or "we used", which are hints that the author of the citing work considers the cited paper to be important. • (F6) Citation appears in table or caption (Boolean): Set to true if at least a citation appears in a table or a caption of a figure or table. This is an indicator that the author of the citing work is comparing her results to the cited paper. • (F7) 1 / number of references: This feature computes the inverse of the length of the citing paper's reference list, which hints to the value of receiving one citation, e.g., if it is one citation from a total of two references, this citation is clearly important. • (F8) Number of paper citations / all citations: Similarly, this feature computes the number of direct citations instances for the cited paper over all the direct citation instances in the citing work.</figDesc><table><row><cell>• (F3) Total number of indirect citations and number of</cell></row><row><cell>indirect citations per section: Similar to the previous</cell></row><row><cell>two features but focusing on indirect features. Since the</cell></row><row><cell>description n-grams may be redundant (i.e., we may find</cell></row><row><cell>multiple, slightly different descriptions of the same work)</cell></row><row><cell>we count them differently than direct citations: instead of</cell></row><row><cell>counting occurrences, we count the number of sentences</cell></row><row><cell>in which at least one potential description appears.</cell></row></table><note>• (F4) Author overlap (Boolean): This feature is set to true if the citing and the cited works share at least one common author.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Identified algorithm names/descriptions.</figDesc><table><row><cell>Citation</cell><cell>Paper title</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Performance of the system when using individual feature groups, for a recall of 0.90. The feature groups are listed in descending order of their contribution.</figDesc><table><row><cell>Features</cell><cell>Precision</cell></row><row><cell>Only: direct citations per section (F2)</cell><cell>0.37</cell></row><row><cell>Only: direct citations (F1)</cell><cell>0.30</cell></row><row><cell>Only: author overlap (F4)</cell><cell>0.22</cell></row><row><cell>Only: is useful (F5)</cell><cell>0.22</cell></row><row><cell>Only: direct citations / all citations (F8)</cell><cell>0.22</cell></row><row><cell>Only: research field (F12)</cell><cell>0.22</cell></row><row><cell>Only: in figure/table (F6)</cell><cell>0.20</cell></row><row><cell>Only: indirect citations: author names (F3)</cell><cell>0.19</cell></row><row><cell>Only: indirect citations: descriptions (F3)</cell><cell>0.17</cell></row><row><cell>Only: inverse number of references (F7)</cell><cell>0.17</cell></row><row><cell>Only: PageRank (F10)</cell><cell>0.17</cell></row><row><cell>Only: total citing papers (F11)</cell><cell>0.16</cell></row><row><cell>Only: abstract similarity (F9)</cell><cell>0.14</cell></row><row><cell>All features</cell><cell>0.62</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">http://www.aclweb.org/anthology/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">http://poppler.freedesktop.org</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">http://opennlp.apache.org</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">https://github.com/knowitall/taggers</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">http://scikit-learn.org/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5">Furthermore, a recall-based evaluation in this context is hard: it is not trivial to extract all the possible descriptions of a paper in the literature.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6">https://www.aclweb.org/anthology/P05-1045</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7">http://allenai.org/data.html</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Citeseerx: an architecture and web service design for an academic document search engine</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L I</forename><surname>Councill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on World Wide Web</title>
				<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
