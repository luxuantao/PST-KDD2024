<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bandit Learning with Implicit Feedback</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yi</forename><surname>Qi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">State Key Lab of Intell. Tech. &amp; Sys</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institution for Artificial Intelligence</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Dept. of Comp. Sci. &amp; Tech</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qingyun</forename><surname>Wu</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Virginia</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hongning</forename><surname>Wang</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Virginia</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
							<email>jietang@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">State Key Lab of Intell. Tech. &amp; Sys</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institution for Artificial Intelligence</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Dept. of Comp. Sci. &amp; Tech</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">State Key Lab of Intell. Tech. &amp; Sys</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institution for Artificial Intelligence</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Dept. of Comp. Sci. &amp; Tech</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Bandit Learning with Implicit Feedback</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Implicit feedback, such as user clicks, although abundant in online information service systems, does not provide substantial evidence on users' evaluation of system's output. Without proper modeling, such incomplete supervision inevitably misleads model estimation, especially in a bandit learning setting where the feedback is acquired on the fly. In this work, we perform contextual bandit learning with implicit feedback by modeling the feedback as a composition of user result examination and relevance judgment. Since users' examination behavior is unobserved, we introduce latent variables to model it. We perform Thompson sampling on top of variational Bayesian inference for arm selection and model update. Our upper regret bound analysis of the proposed algorithm proves its feasibility of learning from implicit feedback in a bandit setting; and extensive empirical evaluations on click logs collected from a major MOOC platform further demonstrate its learning effectiveness in practice.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Contextual bandit algorithms <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b18">19]</ref> provide modern information service systems an effective solution to adaptively find good mappings between available items and users. This family of algorithms sequentially select items to serve users using side information about user and item, while adapting their selection strategies based on the immediate user feedback to maximize users' long-term satisfaction. They have been popularly deployed in practical systems for content recommendation <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b25">26]</ref> and display advertising <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>However, the most dominant form of user feedback in such systems is implicit feedback, such as clicks, which is known to be biased and incomplete about users' evaluation of system's output <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b10">11]</ref>. For example, a user skips a recommended item might not be because he/she does not like the item, but he/she just does not examine that display position, i.e., position bias <ref type="bibr" target="#b12">[13]</ref>. Unfortunately, a common practice in contextual bandit applications simply treats no click as a form of negative feedback <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b5">6]</ref>. This introduces inconsistency to model update, since the skipped items might not be truly irrelevant, and it inevitably leads to suboptimal outputs of bandit algorithms over time.</p><p>In this work, we focus on learning contextual bandits with user click feedback, and model such implicit feedback as a composition of user result examination and relevance judgment. Examination hypothesis <ref type="bibr" target="#b7">[8]</ref>, which is a fundamental assumption in click modeling, postulates that a user clicks on a system's returned result if and only if that result has been examined by the user and it is relevant to the user's information need at the moment. Because a user's examination behavior is unobserved, we model it as a latent variable, and realize the examination hypothesis in a probabilistic model. We define the conditional probabilities of result examination and relevance judgment via logistic functions over the corresponding contextual features. To perform model update, we take a variational Bayesian approach to develop a closed form approximation to the posterior distribution of model parameters on the fly. This approximation also paves the way for an efficient Thompson sampling strategy for arm selection in bandit learning. Our finite time analysis proves that, despite the increased complexity in parameter estimation introduced by the latent variables, our Thompson sampling policy based on the true posterior is guaranteed to achieve a sub-linear Bayesian regret with a high probability. We also demonstrate that the regret of Thompson sampling based on the approximated posterior is well-bounded. In addition, we prove that when one fails to model result examination in click feedback, a linearly increasing regret is possible, as the model cannot differentiate examination driven skips from relevance driven skips in the negative feedback.</p><p>We tested the algorithm in XuetangX<ref type="foot" target="#foot_1">1</ref> , a major Massive Open Online Course (MOOC) platform in China, for personalized education. To personalize students' learning experience on this platform, we recommend quiz-like questions in a form of banners on top of the lecture videos when students are watching the videos. The algorithm needs to decide where in a video to display which question to a target student. If the student feels the displayed question is helpful for him/her to understand the lecture content, he/she could click on the banner to read the answer and more related online content about the question. Therefore, our goal is to maximize the click through rate (CTR) on the selected questions. There are several properties of this application that amplifies the bias and incompleteness of click feedback. First, based on the consideration of user experience, to minimize the risk of annoying any student, the displayed time of a banner is limited to a few seconds. Second, as this feature is newly introduced to the platform, many users might not realize that they can click on the question to read more related content about it. As a result, no click on a question does not necessarily indicate its irrelevance. We tested the algorithm in this application in a four-month period, where a total of 69 questions are manually compiled for the algorithm to select over 20 major videos with more than 100 thousands student video watching sessions. Based on the unbiased offline evaluation policy <ref type="bibr" target="#b20">[21]</ref>, our algorithm achieved a 8.9% CTR lift compared to standard contextual bandits <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b8">9]</ref> which do not model users' examination behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>As having been extensively studied in click modeling of user search results <ref type="bibr" target="#b6">[7]</ref>, various factors affect users' click decisions; and among them result examination plays a central role <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b7">8]</ref>. Unfortunately, most applications of bandit algorithms simply treat user clicks as explicit feedback for model update <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b25">26]</ref>, where no click on a selected result is considered as negative feedback. This inevitably leads to inaccurate model update and sub-optimal arm selection. There is a line of related research that develops click model based bandit algorithms for learning to rank problems. For example, by assuming that skipped documents are less attractive than later clicked ones in a ranked list, Kveton et al. <ref type="bibr" target="#b16">[17]</ref> developed a cascading bandit model to learn from both clicks and skips in search results. To enable learning from multiple clicks in the same result ranking list, they adopted the dependent click model <ref type="bibr" target="#b9">[10]</ref> to infer user satisfaction after a sequence of clicks <ref type="bibr" target="#b13">[14]</ref>, and later further extended it to broader types of click models <ref type="bibr" target="#b26">[27]</ref>. However, such algorithms aim at estimating the best ranking of results in a per-query basis, without specifying any specific ranking function. Hence, it is hard for them to generalize to unseen queries. This directly limits their application scenario in practice. The solution developed in Lagrée et al. <ref type="bibr" target="#b17">[18]</ref> is the closest to ours, which exploits bias in reward distribution induced by different examination probabilities at different display positions. Yet they assumed the examination probability only depends on position, while we allow any reasonable feature to be a determinant. Besides, they postulated that the probability of examination at each position is either heuristically set or empirically estimated, and henceforth fixed; while we estimate it on the fly from the observations obtained by interacting with users.</p><p>Another line of related research is bandit learning with latent variables. Maillard and Mannor studied the problem of latent bandit <ref type="bibr" target="#b22">[23]</ref>, which assumes reward distributions are clustered and the clusters are determined by some latent variables. They only studied the problem in a context-free setting, and a very weak performance guarantee is provided when the reward distribution is unknown in those clusters. Kawale et al. developed a Thompson sampling scheme for online matrix-factorization <ref type="bibr" target="#b14">[15]</ref>. Latent features are extracted via an online low-rank matrix completion based on samples selected from Thompson sampling on the fly. Due to the ad-hoc combination of factorization method and bandit method, little theoretical analysis was provided. Wang et al. studied the problem of latent feature learning for contextual bandits <ref type="bibr" target="#b24">[25]</ref>. They extended arms' context vectors with latent features under a linear reward structure, and applied the upper confidence bound principle over coordinate descent to iteratively estimate the hidden features and model parameters. The linear reward structure prohibits it from recognizing the nonlinear dependency between result examination and relevance judgment in click feedback. And their regret analysis depends heavily on the initialization of the algorithm, which could be hard to achieve in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem Setup</head><p>We consider a contextual bandit problem with finite, but possibly large, number of arms. Denote the arm set as A. At each trial t = 1, ..., T , the learner observes a subset of candidate arms A t with A t ⇢ A, where each arm a is associated with a context vector x a summarizing the side information about the arm. Once an arm a t 2 A t is chosen according to some policy ⇡, corresponding implicit binary feedback C at , e.g., user click, will be given to the learner as the reward. The learner's goal is to adjust its arm selection strategy to maximize its cumulative reward over time. What makes this problem unique and challenging is that C at does not truly reflect users' evaluation of the selected arm a t . Based on the examination hypothesis <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b7">8]</ref>, when C at = 1, the chosen a t must be relevant to the user's information need at time t; but when C at = 0, a t might be relevant but the user just does not examine it. Unfortunately, the result examination condition is unobserved to the learner.</p><p>We model a user's result examination via a binary latent variable E at and assume that the context vector x a t of arm a can be decomposed into (x a C,t , x a E,t ), where the dimension of x a C,t and x a E,t are d C and d E respectively. Accordingly, users' result examination and relevance judgment decisions are assumed to be governed by a conjecture of (x a C,t , x a E,t ) and the corresponding bandit parameter</p><formula xml:id="formula_0">✓ ⇤ = (✓ ⇤ C , ✓ ⇤ E ).</formula><p>In the rest of this paper, when no ambiguity is introduced, we drop the index a to simplify the notations. As a result, we make the following generative assumption about an observed click C</p><p>t on arm a t ,</p><formula xml:id="formula_1">P(C t = 1|E t = 0, x C,t ) = 0 P(C t = 1|E t = 1, x C,t ) = ⇢(x T C,t ✓ ⇤ C ) P(E t = 1|x E,t ) = ⇢(x T E,t ✓ ⇤ E ) where ⇢(x) = 1 1+e x . Based on this assumption, we have E[C t |x t ] = ⇢(x T C,t ✓ ⇤ C )⇢(x T E,t ✓ ⇤ E ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As a result, the observed click feedback C</head><p>t is a sample from this generative process. Define</p><formula xml:id="formula_2">f ✓ (x) := E[C|x, ✓] = ⇢(x T C ✓ C )⇢(x T E ✓ E ).</formula><p>The accumulated regret of a policy ⇡ up to time T is formally defined as,</p><formula xml:id="formula_3">Regret(T, ⇡, ✓ ⇤ ) = T X t=1 max a2At f ✓ ⇤ (x a ) f ✓ ⇤ (x at )</formula><p>where x at := (x at C , x at E ) is the context vector of the arm a t 2 A t selected by the policy ⇡ at time t based on the history H</p><formula xml:id="formula_4">t := {(A i , x i , C i )} t 1 i=1 . The Bayesian regret is defined by E ⇥ Regret(T, ⇡, ✓ ⇤ ) ⇤</formula><p>, where the expectation is taken with respect to the prior distribution over ✓ ⇤ , and it can be written as,</p><formula xml:id="formula_5">BayesRegret(T, ⇡) = T X t=1 E ⇥ max a2At f ✓ ⇤ (x a ) f ✓ ⇤ (x at ) ⇤</formula><p>In our online learning setting, the objective is to find the policy ⇡ that minimizes the accumulated regret over T.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Algorithm</head><p>The learner needs to estimate the bandit parameters ✓ ⇤ C and ✓ ⇤ E based on its interactively obtained click feedback {x i , C i } t i=1 over time. Ideally, this estimation can be obtained by maximizing the data likelihood with respect to the bandit model parameters. However, the inclusion of examination as a latent variable in our bandit learning setting poses serious challenges to both parameter estimation and arm selection. Neither conventional least square estimator nor maximum likelihood estimator can be easily obtained, let alone computational efficiency, due to the non-convexity of the corresponding optimization problem. To make things even worse, the two popular bandit learning paradigms, upper confidence bound principle <ref type="bibr" target="#b0">[1]</ref> and Thompson sampling <ref type="bibr" target="#b2">[3]</ref>, both demand an accurate estimation of bandit parameters and their uncertainty. In this section, we present an efficient new solution to tackle these two challenges, which makes use of variational Bayesian inference technique to learn parameters approximately on the fly, as well as to bridge parameter estimation and arm selection policy design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Variational Bayesian for parameter estimation</head><p>To complete the generative process defined in Section 3, we further assume ✓ C and ✓ E follow Gaussian distribution N ( ✓C , ⌃ C ) and N ( ✓E , ⌃ E ) respectively. We are interested in developing a closed form approximation to their posteriors, when a newly obtained observation (x C , x E , C) becomes available. By applying Bayes' rule in the log space, we have,</p><formula xml:id="formula_6">log P(✓ C , ✓ E |x C , x E , C) = log P(C|✓ C , ✓ E , x C , x E ) + log P(✓ C , ✓ E ) + log const =C log ⇢(x T C ✓ C )⇢(x T E ✓ E ) + (1 C) log 1 ⇢(x T C ✓ C )⇢(x T E ✓ E ) 1 2 (✓ C ✓C ) T ⌃ 1 C (✓ C ✓C ) 1 2 (✓ E ✓E ) T ⌃ 1 E (✓ E ✓E ) + log const</formula><p>The key idea is to develop a variational lower bound in the quadratic form of ✓ C and ✓ E for the loglikelihood function. Because of the convexity of log ⇢(x) x 2 with respect to x 2 (See Appendix B.1) and the Jensen's inequality for log x (See Appendix B.2), a lower bound of the required form is achievable. When C = 1, by Eq <ref type="bibr" target="#b15">(16)</ref> in Appendix B.3, we have,</p><formula xml:id="formula_7">l C=1 (x C , x E , ✓) := log ⇢(x T C ✓ C )⇢(x T E ✓ E ) g(x T C ✓, ⇠ C ) + g(x T E ✓, ⇠ E )<label>(1)</label></formula><p>where g(x, ⇠) :</p><formula xml:id="formula_8">= x 2 ⇠ 2 + log ⇢(⇠) (⇠)(x 2 ⇠ 2 ), (⇠) = tanh ⇠ 2 4⇠ , x, ⇠ 2 R.</formula><p>More specifically, g(x, ⇠) is a polynomial of degree 2 with respect to x. When C = 0, by Eq <ref type="bibr" target="#b16">(17)</ref> in Appendix B.3, we have,</p><formula xml:id="formula_9">l C=0 (x C , x E , ✓) := log 1 ⇢(x T C ✓ C )⇢(x T E ✓ E )<label>(2)</label></formula><formula xml:id="formula_10">H(q) + qg( x T C ✓, ⇠ C ) + qg(x T E ✓, ⇠ E,1 ) + (1 q)g( x T E ✓, ⇠ E,2</formula><p>) where H(q) := q log q (1 q) log(1 q). Once the lower bound in the quadratic form is established, we can use a Gaussian distribution to approximate our target posterior, whose mean and covariance matrix are determined by the following equations,</p><formula xml:id="formula_11">⌃ 1 C,post = ⌃ 1 C + 2q 1 C (⇠ C )x C x T C (3) ✓C,post = ⌃ C,post (⌃ 1 C ✓C + 1 2 ( q) 1 C x C )<label>(4)</label></formula><formula xml:id="formula_12">⌃ 1 E,post = ⌃ 1 E + 2 (⇠ E )x E x T E (5) ✓E,post = ⌃ E,post (⌃ 1 E ✓E + 1 2 (2q 1) 1 C x E )<label>(6)</label></formula><p>where the subscript "post" denotes the parameters in the Gaussian distributions that approximate the desired posteriors. Consecutive observations can be incorporated into the approximated posteriors sequentially. There is one thing left to decide, i.e., the choice of variational parameters (⇠ C , ⇠ E , q). A typical criterion is to choose the values such that the likelihood on the observations is maximized. Similar to the choice made by <ref type="bibr" target="#b11">[12]</ref>, we choose the closed form update formulas of those variational parameters as,</p><formula xml:id="formula_13">⇠ C = q E ✓ C [x T C ✓ C ] 2 ⇠ E = q E ✓ E [x T E ✓ E ] 2 q = exp (g(x T C ✓ C , ⇠ C ) + g(x T E ✓ E , ⇠ E ) g( x T E ✓ E , ⇠ E )) 1 + exp (g(x T C ✓ C , ⇠ C ) + g(x T E ✓ E , ⇠ E ) g( x T E ✓ E , ⇠ E )) Algorithm 1 Thompson sampling for E-C Bandit 1: Initiate ⌃ C = I, ⌃ E = I, ✓C = ✓ C,0 , ✓E = ✓ E,0 . 2: for t = 0, 1, 2... do 3:</formula><p>Observe the available arm set A t ⇢ A and its corresponding context set X</p><formula xml:id="formula_14">t := {(x a C , x a E ) : a 2 A t }. 4:</formula><p>Randomly sample ✓C ⇠ N ( ✓C , ⌃ C ), ✓E ⇠ N ( ✓E , ⌃ E ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Select:</p><formula xml:id="formula_15">a t = arg max a2At ⇢((x a C ) T ✓C )⇢((x a E ) T ✓E ) 6:</formula><p>Play the selected arm a t and Observe the reward C t .</p><p>7:</p><p>Update ⌃ C , ✓C , ⌃ E , ✓E according to Eq (3) to Eq (6) respectively. 8: end for where all the expectations are taken under the approximated posteriors. Empirically, we found the iterative update of the approximated posterior and the variational parameters converge quite rapidly, such that it usually only needs a few rounds of iterations to get a satisfactory local maximum in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Thompson sampling with approximated lower bound</head><p>Thompson sampling, also known as probability matching, is widely used in bandit learning to balance exploration and exploitation, and it shows great empirical performance <ref type="bibr" target="#b5">[6]</ref>. Thompson sampling requires a distribution of the model parameters to sample from. In a standard Thompson sampling <ref type="bibr" target="#b2">[3]</ref>, one is required to sample from the true posterior of model parameters. But as logistic regression does not have a conjugate prior, the model defined in our problem does not have an exact posterior. We decide to sample from the approximated posterior as derived in Eq (3) to Eq <ref type="bibr" target="#b5">(6)</ref>. Later we will demonstrate this is a very tight posterior approximation. Once the sampling of ( ✓C , ✓E ) is complete, we can select the corresponding arm a t 2 A t which maximizes ⇢(x T C ✓C )⇢(x T E ✓E ). We name the resulting bandit algorithm as examination-click bandit, or E-C Bandit in short, and summarize it in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Regret Analysis</head><p>Recall our object is to find the policy that minimizes the Beyesian regret,</p><formula xml:id="formula_16">BayesRegret(T, ⇡) = T X t=1 E ⇥ max a2At f ✓ ⇤ (x a ) f ✓ ⇤ (x at ) ⇤ where f ✓ (x) := E[C|x, ✓] = ⇢(x T C ✓ C )⇢(x T E ✓ E ).</formula><p>Our algorithm, which is based on a maximum likelihood estimator, is equivalent to an estimator that minimizes a log-loss with binary random variables. In this section, we will first bound the aggregate empirical discrepancy of the log-loss estimator used in our model in Proposition 1. This prepares for the upper bound of the generic Bayeisan regret under a log-loss estimator with Thompson sampling policy in Theorem 1. Based on this generic Bayesian regret bound, we study the upper bound of Bayesian regret for our proposed E-C Bandit. Due to space limit, we provide all the detailed proofs in the Appendix.</p><p>To further simplify our notations, we use f for f ✓ , which is the reward function based the estimated bandit parameter ✓, and f k for f ✓ (x a k ), i.e., the reward for arm a k . We use f ⇤ for f ✓ ⇤ , which is the reward function based on the ground-truth bandit parameter, and correspondingly</p><formula xml:id="formula_17">f ⇤ k for f ✓ ⇤ (x a k</formula><p>). We assume that f ⇤ lies in a known function space F, where any f 2 F is a function mapping from the arm set A to the range (0, 1). Define the log-loss estimator by f LOGLOSS t 2 arg min f 2F L 2,t (f ) where L 2,t (f ) is the aggregate log-loss written as</p><formula xml:id="formula_18">P t 1 k=1 l k (f ) where l k (f ) = C k log f k + (1 C k ) log(1 f k ) .</formula><p>We have the following proposition, Proposition 1. Denote the aggregate empirical discrepancy</p><formula xml:id="formula_19">P t k=1 (f k f ⇤ k ) 2 by kf f ⇤ k 2 E,t . For all &gt; 0 and ↵ &gt; 0, if F t = n f 2 F : f f LOGLOSS t E,t  p ⇤ t (F, , ↵) o for all t 2 N , P f ⇤ 2 \ 1 t=1 F t &gt; 1 ,<label>(7)</label></formula><p>where ⇤ t (F, , ↵) is an appropriately constructed confidence parameter. In particular, it is defined as</p><formula xml:id="formula_20">⇤ t (F, , ↵) := 2 0 log(N (F, ↵, k•k 1 )/ ) + 2↵⌘ t , where N (F, ↵, k•k 1 ) denotes the alpha-covering number of F, 0 = 3 ( 1 m f + 1 1 M f ) 2 and ⌘ t = 4M f + 1 min{m f ,1 M f } t, in which m f , M f 2 R are upper and lower bounds of f such at 0 &lt; m f  f  M f &lt; 1 for any f 2 F. Remark 1.</formula><p>The proof is provided in Appendix C. Here we discuss two important details related to our later proof about E-C Bandit's regret. First, the precise optimization of f LOGLOSS t 2 arg min f 2F L 2,t (f ) could be hard in some instances of F. For example, when F is a set of non-convex functions. Nevertheless, we can always resort to approximation methods to solve the optimization problem as long as the approximation error can be bounded. Indeed, in our E-C Bandit, we resort to variational inference to estimate f LOGLOSS t on the fly and find it works quite well in practice. Second, when f ⇤ 6 2 F, this corresponds to the problem of model mis-specification. In this situation, the regret bound could be very poor, as the real regret could be linear with respect to time. To show this clearly in our case, in Appendix F we construct a situation in which the regret is inevitably linear if one fails to model the examination condition in click feedback and simply treats no click as negative feedback.</p><p>With Proposition 1, we have the following theorem which bounds the Bayesian regret of the Thompson Sampling strategy under a log-loss estimator. Theorem 1. For all T 2 N , ↵ &gt; 0 and &lt; 1 2T , if ⇡ T S denotes the policy derived from the log-loss estimator and a Thompson sampling strategy along the time steps, then</p><formula xml:id="formula_21">BayesRegret(T, ⇡ T S )  1 + dim A E (F, T 1 ) + 1 C + 4 q dim A E (F, T 1 ) ⇤ T (F, ↵, )T (8) where C = sup f 2F {sup f }, dim A E (F, T 1</formula><p>) is the eluder dimension (see Definition 3 in Russo and Van Roy <ref type="bibr" target="#b23">[24]</ref>) of F with respect to A. Remark 2. We can choose C = 1 in our click feedback case since f 2 (0, 1). C is kept in the theorem to show the same form compared to the Proposition 8 in Russo and Van Roy <ref type="bibr" target="#b23">[24]</ref>. In fact, the proof is almost the same once we have Proposition 1. Hence, we omit the proof in our paper. Now we turn to provide an upper regret bound of our E-C Bandit, based on the above generic Bayeisan regret analysis under a log-loss estimator. We add the following two assumptions which are standard in the literature of contextual bandits. </p><formula xml:id="formula_22">that ⇢(x T C ✓ C ), ⇢(x T E ✓ E ) and f ✓ (x) are bounded. Let M ⇢ = max ✓2Bs,x2Bx ⇢(x T ✓) and m ⇢ = min ✓2Bs,x2Bx ⇢(x T ✓). Hence, 0 &lt; m ⇢  M ⇢ &lt; 1. Similarly, denote the maximum of f ✓ (x) by M f and the minimum by m f , we have 0 &lt; m f  M f &lt; 1.</formula><p>Once the arm set is restricted to a finite cardinality, we have dim A E (F, T 1 )  |A| by Appendix C.1. in Russo and Van Roy <ref type="bibr" target="#b23">[24]</ref>. Choosing the function class as that in our E-C Bandit, i.e., F = {f : B</p><formula xml:id="formula_23">x ! R|f = ⇢(x T C ✓ C )⇢(x T E ✓ E ), ✓ 2 B s }, by Lemma 8 (See Appendix 8 for its proof), we have N (F, ↵, k•k 1 ) = ( /↵) d where = 2M ⇢ k ⇢ x (k ⇢ is the Lipschitz constant of ⇢, see Lemma 4). Hence, choosing ↵ = 1/t 2 and = 1/t leads to ⇤ t (F, 1/t, 1/t 2 ) = 2 0 d log( t 3 ) + 1 t (4M f + 1 m f ).<label>(9)</label></formula><p>Therefore, the upper bound of Bayesian regret of our proposed E-C Bandit takes the following form,</p><formula xml:id="formula_24">BayesRegret(T, ⇡ T S ) = O(|A| + p d|A|T log T ). (<label>10</label></formula><formula xml:id="formula_25">)</formula><p>When T |A| and T d, which is a typical case in practice, it becomes O( p T log T ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiment</head><p>We perform empirical evaluations in simulation and on the online student click logs collected from our MOOC platform to verify the effectiveness of our proposed algorithm. In particular, we compare with those that fail to model the examination condition and directly use click as feedback.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Algorithms for comparison</head><p>We list the models used for empirical comparisons below, and explain how we adjust them in our evaluations.</p><p>Logistic Bandit. This model has been extensively used for online advertisement CTR optimization. In <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b20">21]</ref>, the authors model user clicks by a regularized logistic regression model over observed context features and make decisions by applying Thompson sampling over the learnt model. In particular, no click is treated as negative feedback. Following their setting in <ref type="bibr" target="#b5">[6]</ref>, we used the Laplace approximation and Gaussian prior presented in to update the model parameters on the fly. We also want to highlight that despite mountains of works focusing on generalized linear bandits, most of them are not truly online algorithms, because the estimation of their parameters at each iteration has to involve all historical observations iteratively. This incurs a space complexity at least O(T ) and time complexity at least O(T 2 ) (e.g., Filippi et al. <ref type="bibr" target="#b8">[9]</ref> requires exact optimum of logistic regression on all historical observations at each round).</p><p>hLinUCB. This is an algorithm proposed by Wang et al. <ref type="bibr" target="#b24">[25]</ref> for bandit learning with latent variables. It is related to our model in a sense that both models estimate hidden features. In particular, hLinUCB extends linear contextual bandit by inclusion of hidden features and operates under a UCB-like strategy. However, it still treats click as direct feedback, but aims at learning more expressive features to describe the observed clicks.</p><p>E-C Bandit. This is the algorithm we present in Algorithm 1. We should note that in the experiments on real-world data, the manual separation of examination feature x E and click feature x C in the context vector x offers a principled approach to incorporate one's domain knowledge about what affects user examination and what affects user result relevance. We explain in detail what features are chosen for which component in Appendix G. Thanks to the tight approximation achieved by Bayesian variational inference presented in Section 4, truly online model update is feasible in this algorithm. This provides both computational and storage efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Experiments on simulations</head><p>First we demonstrate the effectiveness of our algorithm by experiment with simulated data. The experiment is performed as follows. at , where a ⇤ t is the optimum arm to be chosen based on the ground-truth bandit parameters (✓ ⇤ C , ✓ ⇤ E ). We repeat the experiment 100 times using the same simulation setting, each containing 10,000 iterations. The average cumulative regret over 100 runs and the corresponding standard deviation (plotted per thousand iterations) are illustrated in Figure <ref type="figure" target="#fig_0">1</ref>. One can clearly notice that the Logistic Bandit suffers from a linear regret with respect to time t, as it mistakenly treats no click as negative feedback. Our E-C Bandit achieves a fast converging sub-linear regret. The result that hLinUCB performs the worst is expected, since it assumes a linear relation between click and context feature vectors. We further investigate how the aggregate empirical discrepancy between E-C bandit and  Logistic Bandit increases with respect to time. Figure <ref type="figure" target="#fig_1">2</ref> illustrates that the aggregate empirical discrepancy of E-C Bandit is well bounded by the upper bound provided by Proposition 1, while the Logistic Bandit's aggregate empirical discrepancy increases linearly. This directly explains their accumulative regret in this experiment comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Experiments on MOOC video watching data</head><p>The MOOC data we used for evaluation is collected from a single course in a 4-month period. The course has 503 lecture videos in total. About 500 high-quality quiz-like questions have been manually crafted and each video is assigned with a subset of them based on human-judged relatedness. We selected 21 videos, whose accumulated watching time are ranked in the first 21 places, for this evaluation. Over the selected videos, on average a video is assigned with 5.5 questions, each of which is associated with 6 possible displaying positions within the video, leading to a total of 33 arms in average (as each question can be placed in all positions). The data set with our manually crafted features and our model implementation have been made publicially available here: https://github.com/qy7171/ec_bandit. We picked one video as an example to analyze students' click behavior. 9 arms are picked and projected by a random Gaussian matrix to a two-dimension plane in Figure <ref type="figure" target="#fig_2">3</ref>. Thus, their relative distance are kept. The number in the parenthesis indicates the empirical CTR of the corresponding arm. It can be clearly seen that while arm c and arm f have the same empirical CTR, the arms between them, such as arm a and d, have lower CTRs. Logistic Bandit is never able to capture this non-monotonicity relation, since its reward prediction increases monotonically with respect to a linear predictor. We construct a more general case in Appendix F to illustrate the scenario that failing to model examination condition would lead to a linear regret. Mapping the illustration back to the MOOC data set, arm a and arm f are two different questions displayed at the same position in the video, while arm a and arm c are the same question displayed at different positions. This phenomenon strongly suggests bias in users' implicit feedback, which again justifies our decomposition of examination and relevance in click feedback.</p><p>We followed <ref type="bibr" target="#b20">[21]</ref> to develop our online data collection policy in our MOOC platform so as to prepare our offline evaluation data set. In particular, any related questions with respect to a video will have an equal probability to be selected and displayed at all positions in this video. We name this policy as Similarity. We create an instance of a bandit model for each video to learn its own optimal question placing policy. See Appendix G for detailed explanations of our examination feature choice. We also added a new baseline here, i.e., PBMUCB <ref type="bibr" target="#b17">[18]</ref>, which assumes a position-based examination probability in any ranking result. To adjust it to our setting, we assumed that the examination probability of any question chosen in a video is determined by and only by its position. Therefore, the key difference between our model and PBMUCB is that ours utilizes the available contextual information to estimate the examination probability, while PBMUCB is context-free. Yet, another important difference is that PBMUCB assumes the probability of examination at different position is known, and is estimated from offline data.  Li et al. <ref type="bibr" target="#b20">[21]</ref> proposed a method to calculate a near-unbiased estimate of CTR of any bandit algorithm based on the collected history data, so that offline evaluation and performance comparison are possible. We take their offline evaluation protocol here and report the estimated CTR in Figure <ref type="figure" target="#fig_3">4</ref>, which is averaged over 100 runs. To avoid disclosing any proprietary information about the platform, all algorithms' CTRs are normalized by that from the Similarity policy. As shown in the figure, independently estimating E-C Bandits across videos achieves an average 40.6% increase in CTR over the Similarity baseline. Meanwhile, E-C Bandit consistently outperforms the other three baseline bandits, i.e., hLinUCB, Logistic Bandit and PBMUCB. The improvement of our model compared to Logistic Bandit clearly suggests the necessity of modeling examination condition in user clicks for improving the online recommendation performance, and the improvement against PBMUCB provides strong evidence of the importance of modelling examination with available contextual information. In addition, the standard of error of E-C Bandit, hLinUCB, Logistic Bandit and PBMUCB's relative CTR performance among 100 trials are 0.032, 0.031, 0.030, 0.041, respectively. Therefore, the variance of our offline evaluation is small and the improvement from our solution to the baselines are statistically significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>Motivated by the examination hypothesis in user click modeling, in this paper we developed E-C Bandit, which differentiates result examination and content relevance in user clicks and actively learns from such implicit feedback. We developed an efficient and effective learning algorithm based on variational inference and demonstrated its effectiveness on both simulated and real-world datasets. We proved that despite the complexity of underlying reward generation assumption and the resulting parameter estimation procedure, the proposed learning algorithm enjoys a sub-linear regret bound. Currently we only studied click feedback on single items; it is important for us to study it in a more general setting, e.g., a list of ranked items, where sequential result examination and relevance judgment introduce richer inter-dependency. In addition, our current regret analysis does not account for the additional discrepancy introduced by the variational inference. Abeille et al. <ref type="bibr" target="#b1">[2]</ref> suggests that an exact posterior is not a necessary condition for a Thompson sampling policy to be optimal. It is important to study a tighter upper regret bound under our approximated posterior in general.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Comparison of cumulative regret over 100 runs of simulation.</figDesc><graphic url="image-1.png" coords="8,108.00,72.00,190.08,142.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Comparison of discrepancy bound provided by Proposition 1.</figDesc><graphic url="image-2.png" coords="8,313.92,72.00,190.08,142.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: 9 arms' feature vectors projected onto a two-dimension plane, such that the relative distances between points are kept. The number in the parenthesis is the arm's empirical CTR.</figDesc><graphic url="image-3.png" coords="9,108.00,72.00,182.16,136.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Performance comparison on MOOC videos' data of different bandit algorithms.</figDesc><graphic url="image-4.png" coords="9,321.84,72.00,182.16,136.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Assumption 1. The optimal ✓ ⇤ lies in B Assumption 2. The norm of context vectors are bounded by x, i.e., (x : kxk 2  x} and x is known as a prior. Based on these two assumptions, it is straightforward to verify</figDesc><table /><note>s := {✓ 2 R d : k✓k 2  s},and s is known as a prior. C , x E ) 2 B x , where B x := {x 2 R d</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>The context vector's dimension dAt each time t, an arm set Ãt is randomly sampled from A such that | Ãt | = 10, i.e., each time we offer 10 randomly selected arms for the algorithm to choose from. An algorithm selects an arm from A</figDesc><table><row><cell cols="3">C and d E = 10. We set |A| = 100, each of which is associated with a unique context E are set to 5, and E ). The ground-truth parameter (✓ ⇤ C + d C , x thus d = d vector (x C , ✓ ⇤ C , x E ) are all E ) and the specific value of (x randomly sampled from the unit ball B = {x 2 R d : kxk 2  1}. Since (✓ ⇤ C , ✓ ⇤ E ) and (x C , x E ) f and M f can be obtained by taking the minimum and maximum of are both sampled from B, m ⇢(x T C ✓ C )⇢(x T E ✓ E ) on B, respectively, i.e., m f = 1 (1+e) 2 and M f = 1 (1+e 1 ) 2 .</cell></row><row><cell cols="3">t and observes the corresponding reward C alg t generated by the Bernoulli distribution C )⇢(x T C,t ✓ ⇤ B(⇢(x T E,t ✓ ⇤ E )). The regret of this algorithm at time t is defined by its received click reward,</cell></row><row><cell>i.e., regret(t) = C</cell><cell>t a ⇤</cell><cell>C</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="32" xml:id="foot_0">32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montréal, Canada.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1">http://www.xuetangx.com/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. We thank the anonymous reviewers for their insightful comments. This paper is based upon work supported by a research fund from XuetangX.com and the National Science Foundation under grant IIS-1553568 and IIS-1618948.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Improved algorithms for linear stochastic bandits</title>
		<author>
			<persName><forename type="first">Yasin</forename><surname>Abbasi-Yadkori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dávid</forename><surname>Pál</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Csaba</forename><surname>Szepesvári</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2312" to="2320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Linear thompson sampling revisited</title>
		<author>
			<persName><forename type="first">Marc</forename><surname>Abeille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Lazaric</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronic Journal of Statistics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="5165" to="5197" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Thompson sampling for contextual bandits with linear payoffs</title>
		<author>
			<persName><forename type="first">Shipra</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navin</forename><surname>Goyal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="127" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Using confidence bounds for exploitation-exploration trade-offs</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Auer</surname></persName>
		</author>
		<ptr target="http://www.jmlr.org/papers/v3/auer02a.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="397" to="422" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A contextual-bandit algorithm for mobile context-aware recommender system</title>
		<author>
			<persName><forename type="first">Djallel</forename><surname>Bouneffouf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amel</forename><surname>Bouzeghoub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alda</forename><forename type="middle">Lopes</forename><surname>Gançarski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="324" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An empirical evaluation of thompson sampling</title>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2249" to="2257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Click models for web search</title>
		<author>
			<persName><forename type="first">Aleksandr</forename><surname>Chuklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Markov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Information Concepts, Retrieval, and Services</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="115" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An experimental comparison of click position-bias models</title>
		<author>
			<persName><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onno</forename><surname>Zoeter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Ramsey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 international conference on web search and data mining</title>
				<meeting>the 2008 international conference on web search and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="87" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Parametric bandits: The generalized linear case</title>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Filippi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Cappe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurélien</forename><surname>Garivier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Csaba</forename><surname>Szepesvári</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="586" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Efficient multiple-click models in web search</title>
		<author>
			<persName><forename type="first">Fan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second ACM International Conference on WSDM</title>
				<meeting>the Second ACM International Conference on WSDM</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="124" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Collaborative filtering for implicit feedback datasets</title>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Mining, 2008. ICDM&apos;08. Eighth IEEE International Conference on</title>
				<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="263" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bayesian parameter estimation via variational methods</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tommi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="37" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Accurately interpreting clickthrough data as implicit feedback</title>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Granka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helene</forename><surname>Hembrooke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geri</forename><surname>Gay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR Forum</title>
				<imprint>
			<publisher>Acm</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="4" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dcm bandits: Learning to rank with multiple clicks</title>
		<author>
			<persName><forename type="first">Sumeet</forename><surname>Katariya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Branislav</forename><surname>Kveton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Csaba</forename><surname>Szepesvari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1215" to="1224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient thompson sampling for online matrix-factorization recommendation</title>
		<author>
			<persName><forename type="first">Jaya</forename><surname>Kawale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung</forename><forename type="middle">H</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Branislav</forename><surname>Kveton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Tran-Thanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Chawla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1297" to="1305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Implicit feedback for inferring user preference: a bibliography</title>
		<author>
			<persName><forename type="first">Diane</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Teevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acm Sigir Forum</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="18" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cascading bandits: Learning to rank in the cascade model</title>
		<author>
			<persName><forename type="first">Csaba</forename><surname>Branislav Kveton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Szepesvari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Azin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><surname>Ashkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="767" to="776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multiple-play bandits in the position-based model</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Lagrée</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Vernade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Cappe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1597" to="1605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The epoch-greedy algorithm for multi-armed bandits with side information</title>
		<author>
			<persName><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="817" to="824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A contextual-bandit approach to personalized news article recommendation</title>
		<author>
			<persName><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th international conference on World wide web</title>
				<meeting>the 19th international conference on World wide web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="661" to="670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unbiased offline evaluation of contextual-bandit-based news article recommendation algorithms</title>
		<author>
			<persName><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanhui</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourth ACM international conference on Web search and data mining</title>
				<meeting>the fourth ACM international conference on Web search and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="297" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Exploitation and exploration in a performance based contextual advertising system</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuerui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruofei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianchang</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 16th SIGKDD</title>
				<meeting>16th SIGKDD</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="27" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Latent bandits</title>
		<author>
			<persName><forename type="first">Odalric-Ambrym</forename><surname>Maillard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shie</forename><surname>Mannor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="136" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning to optimize via posterior sampling</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Russo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Operations Research</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1221" to="1243" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning hidden features for contextual bandits</title>
		<author>
			<persName><forename type="first">Huazheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongning</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM International on Conference on Information and Knowledge Management</title>
				<meeting>the 25th ACM International on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1633" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Linear submodular bandits and their application to diversified retrieval</title>
		<author>
			<persName><forename type="first">Yisong</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2483" to="2491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Online learning to rank in stochastic click models</title>
		<author>
			<persName><forename type="first">Masrour</forename><surname>Zoghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Tunys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Ghavamzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Branislav</forename><surname>Kveton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Csaba</forename><surname>Szepesvari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4199" to="4208" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
