<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Scan Context: Egocentric Spatial Descriptor for Place Recognition within 3D Point Cloud Map</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Giseop</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ayoung</forename><surname>Kim</surname></persName>
						</author>
						<title level="a" type="main">Scan Context: Egocentric Spatial Descriptor for Place Recognition within 3D Point Cloud Map</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DC09F2E467AC3426665F8E93112D8504</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Compared to diverse feature detectors and descriptors used for visual scenes, describing a place using structural information is relatively less reported. Recent advances in simultaneous localization and mapping (SLAM) provides dense 3D maps of the environment and the localization is proposed by diverse sensors. Toward the global localization based on the structural information, we propose Scan Context, a nonhistogram-based global descriptor from 3D Light Detection and Ranging (LiDAR) scans. Unlike previously reported methods, the proposed approach directly records a 3D structure of a visible space from a sensor and does not rely on a histogram or on prior training. In addition, this approach proposes the use of a similarity score to calculate the distance between two scan contexts and also a two-phase search algorithm to efficiently detect a loop. Scan context and its search algorithm make loopdetection invariant to LiDAR viewpoint changes so that loops can be detected in places such as reverse revisit and corner. Scan context performance has been evaluated via various benchmark datasets of 3D LiDAR scans, and the proposed method shows a sufficiently improved performance.</p><p>1 G. Kim and A. Kim are with the Department of Civil and Environmental Engineering, KAIST, Daejeon, S. Korea [paulgkim, ayoungk]@kaist.ac.kr</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>In many robotics applications, place recognition is the important problem. For SLAM, in particular, this recognition provides candidates for loop-closure, which is essential for correcting drift error and building a globally consistent map <ref type="bibr" target="#b0">[1]</ref>. While the loop-closure is critical for robot navigation, wrong registration can be catastrophic and careful registration is required. Visual recognition is popular together with the widespread use of camera sensors, however, it is inherently difficult due to illumination variance and short-term (e.g., moving objects) or long-term (e.g., seasons) changes. Similar environments may occur at different locations often causing perception aliasing. Therefore, recent literature has focused on robust place recognition by examining representation <ref type="bibr" target="#b1">[2]</ref> and resilient back-end <ref type="bibr" target="#b2">[3]</ref>.</p><p>Unlike these visual sensors, LiDARs have recently garnered attention due to their strong invariance to perceptual variance. In the early days, conventional local keypoint descriptors <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>, which were originally designed for the 3D model in computer vision, have been used for place recognition in spite of their vulnerability to noise. LiDARbased methods for place recognition have been widely proposed in robotics literature <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>. These works focus on developing descriptors from structural information (e.g., point clouds) in both local <ref type="bibr" target="#b7">[8]</ref> and global manners <ref type="bibr" target="#b9">[10]</ref>. Using the top view of a point cloud from a 3D scan (a), we partition ground areas into bins, which are split according to both azimuthal (from 0 to 2π within a LiDAR frame) and radial (from center to maximum sensing range) directions. We refer to the yellow area as a ring, the cyan area as a sector, and the black-filled area as a bin. Scan context is a matrix as in (b) that explicitly preserves the absolute geometrical structure of a point cloud. The ring and sector described in (a) are represented by the same-colored column and row, respectively, in (b). The representative value extracted from the points located in each bin is used as the corresponding pixel value of (b). In this paper, we use the maximum height of points in a bin.</p><p>There are two issues that the existing LiDAR-based place recognition methods have been trying to overcome. First, the descriptor is required to achieve rotational invariance regardless of the viewpoint changes. Second, noise handling is the another topic for these spatial descriptors because the resolution of a point cloud varies with distance and normals are noisy. The existing methods mainly use the histogram <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref> to address the two aforementioned issues. However, since the histogram method only provides a stochastic index of the scene, describing the detailed structure of the scene is not straightforward. This limitation makes the descriptor less discernible for place recognition problem, causing potential false positives.</p><p>In this paper we present Scan Context, a novel spatial descriptor with a matching algorithm, specifically targeting outdoor place recognition using a single 3D scan. Our representation encodes a whole point cloud in a 3D scan into a matrix (Fig. <ref type="figure" target="#fig_0">1</ref>). The proposed representation describes egocentric 2.5D information. Contribution points of the proposed method are:</p><p>• Efficient bin encoding function. Unlike existing point cloud descriptors <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10]</ref>, the proposed method needs not count the number of points in a bin, instead it proposes a more efficient bin encoding function for place recognition. This encoding presents invariance to density and normals of a point cloud. • Preservation of internal structure of a point cloud. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, each element value of a matrix is determined by only the point cloud belonging to the bin. Thus, unlike <ref type="bibr" target="#b8">[9]</ref>, which depicts the relative geometry of points as a histogram and loses points' absolute location information, our method preserves the absolute internal structure of a point cloud by intentionally avoiding using a histogram. This improves the discriminative capability and also enables viewpoint alignment of a query scan to a candidate scan (in our experiments, 6 • azimuth resolution) while a distance is calculated. Therefore, detecting a reverse direction loop is also possible by using scan context. • Effective two-phase matching algorithm. To achieve a feasible search time, we provide a rotational invariant subdescriptor for first nearest neighbor search and combine it with pairwise similarity scoring hierarchically, thus avoid searching all databases for loop-detection. • Thorough validation against other state-of-the-art spatial descriptors. In the comparison to other existing global point cloud descriptors, such as M2DP <ref type="bibr" target="#b7">[8]</ref>, Ensemble of Shape Functions (ESF) <ref type="bibr" target="#b10">[11]</ref>, and Z-projection <ref type="bibr" target="#b11">[12]</ref>, the proposed approach presents a substantial improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Place recognition methods for mobile robots can be categorized into vision-based and LiDAR-based methods. Visual methods have been commonly used for place recognition in SLAM literatures <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>. FAB-MAP <ref type="bibr" target="#b12">[13]</ref> increased robustness with the probabilistic approach by learning a generative model for the bag of visual words. However, visual representation has limitations such as vulnerability to light condition change <ref type="bibr" target="#b15">[16]</ref>. Several methods have been proposed to overcome these issues. SeqSLAM <ref type="bibr" target="#b16">[17]</ref> proposed the routebased approach and showed far improved performance than FAB-MAP. SRAL <ref type="bibr" target="#b1">[2]</ref> fused several different representation such as color, GIST <ref type="bibr" target="#b17">[18]</ref>, and HOG <ref type="bibr" target="#b18">[19]</ref> for long-term visual place recognition.</p><p>LiDAR presents strong robustness to these perceptual changes described above. LiDAR-based methods are further catagorized into local and global descriptors. Local descriptors, such as PFH <ref type="bibr" target="#b3">[4]</ref>, SHOT <ref type="bibr" target="#b4">[5]</ref>, shape context <ref type="bibr" target="#b6">[7]</ref>, or spin image <ref type="bibr" target="#b5">[6]</ref>, first find a keypoint, separate nearby points into bins, and encode a pattern of surrounding bins into a histogram. Steder et al. proposed the place recognition method <ref type="bibr" target="#b7">[8]</ref> using point features and the gestalt descriptor <ref type="bibr" target="#b19">[20]</ref> in bag of words manner.</p><p>These keypoint descriptors, however, revealed limitations since they were originally devised for 3D model part matching not for place recognition. For example, the density of a point cloud in a 3D scan (e.g., from VLP-16) varies with respect to the distance from a sensor, unlike the 3D model. Furthermore, normals of points are noisier than the model due to unstructured objects (e.g., trees) in the real world. Hence, local methods usually require normals of keypoints and thus are less suitable for place recognition in outdoor.</p><p>Global descriptors do not include the keypoint detecting phase. GLARE <ref type="bibr" target="#b8">[9]</ref> and its variations <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref> encoded the geometric relationship between points into a histogram in lieu of searching for the keypoint and extracting the descriptor. ESF <ref type="bibr" target="#b10">[11]</ref> used concatenation of histograms made from shape functions. Muhammad and Lacroix proposed Zprojection <ref type="bibr" target="#b11">[12]</ref>, which is a histogram of normal vectors, and a double threshold scheme with two distance functions. He et al. proposed M2DP <ref type="bibr" target="#b9">[10]</ref>, which projects a whole 3D point cloud of a scan to multiple 2D planes and extracts a 192 dimensional compact global representation. M2DP showed higher performance than the existing point cloud descriptors and robustness against noise and resolution changes. As introduced in this paragraph, global descriptors have typically used histograms. Recently, SegMatch <ref type="bibr" target="#b22">[23]</ref> introduced a segment-based matching algorithm. This is a high-level perception but requires a training step, and points are needed to be represented in a global reference frame.</p><p>In this paper, we propose a novel place descriptor called Scan Context that encodes a point cloud of a 3D scan into a matrix. The scan context can be considered as an extension of the Shape Context <ref type="bibr" target="#b6">[7]</ref> for place recognition targeting 3D LiDAR scan data. In detail, scan context has three components: the representation that preserves absolute location information of a point cloud in each bin, efficient bin encoding function, and two-step search algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. SCAN CONTEXT FOR PLACE RECOGNITION</head><p>In this section, we describe scan context creation given a point cloud from a 3D scan and propose a measure that calculates the distance between two scan contexts. Next, the two-step search process is introduced. The overall pipeline of place recognition using scan context is depicted in Fig. <ref type="figure">2</ref>. The Scan Context creation and validation can also be found in scancontext.mp4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Scan Context</head><p>We define a place descriptor called Scan Context for outdoor place recognition. The key idea of a scan context is inspired by Shape Context <ref type="bibr" target="#b6">[7]</ref> proposed by Belongie et al., which encodes the geometrical shape of the point cloud around a local keypoint into an image. While their method simply counts the number of points to summarize the distribution of points, ours differs from theirs in that we use a maximum height of points in each bin. The reason for using the height is to efficiently summarize the vertical shape of surrounding structures without requiring heavy computations to analyze the characteristics of the point cloud. In addition, the maximum height says which part of the surrounding structures is visible from the sensor. This egocentric visibility has been a well-known concept in the urban design literature for analyzing an identity of a place <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>. Similar to shape context <ref type="bibr" target="#b6">[7]</ref>, we first divide a 3D scan into azimuthal and radial bins in the sensor coordinate, but in an equally spaced manner as shown in Fig. <ref type="figure" target="#fig_0">1(a)</ref>. The center of a scan acts as a global keypoint and thus we refer to a scan context as an egocentric place descriptor. N s and N r are the number of sectors and rings, respectively. That is, if we set the maximum sensing range of a LiDAR sensor as L max , the radial gap between rings is Lmax Nr and the central angle of a sector is equal to 2π</p><p>Ns . In this paper, we used N s = 60 and N r = 20.</p><p>Therefore, the first process of making a scan context is to partition whole points of a 3D scan into mutually exclusively separated point clouds as shown in Fig. <ref type="figure" target="#fig_0">1(a)</ref>. P ij is the set of points belonging to the bin where the ith ring and jth sector overlapped. The symbol</p><formula xml:id="formula_0">[N s ] is equal to {1, 2, ..., N s-1 , N s }.</formula><p>Therefore, the partition is mathematically</p><formula xml:id="formula_1">P = i∈[Nr], j∈[Ns] P ij .<label>(1)</label></formula><p>Because the point cloud is divided at regular intervals, a bin far from a sensor has a physically wider area than a near bin. However, both are equally encoded into a single pixel of a scan context. Thus, a scan context compensates for the insufficient amount of information caused by the sparsity of far points and treats nearby dynamic objects as sparse noise.</p><p>After the point cloud partitioning, a single real value is assigned to each bin by using the point cloud in that bin:</p><formula xml:id="formula_2">φ : P ij → R ,<label>(2)</label></formula><p>and we use a maximum height, which is inspired from the urban visibility analysis <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>. Thus, the bin encoding function is</p><formula xml:id="formula_3">φ(P ij ) = max p∈Pij z(p) ,<label>(3)</label></formula><p>where z( • ) is the function that returns a z-coordinate value of a point p. We assign a zero for empty bins. For example, as seen in Fig. <ref type="figure" target="#fig_0">1</ref>(b), a blue pixel in the scan context means that the space corresponding to its bin is either free or not observed due to occlusions. From the foregoing processes, a scan context I is finally represented as a N r × N s matrix as</p><formula xml:id="formula_4">I = (a ij ) ∈ R Nr×Ns , a ij = φ(P ij ) .<label>(4)</label></formula><p>For robust recognition over translation, we leverage scan context augmentation through root shifting. By doing so, acquiring various scan contexts from the raw scan under a slight motion perturbance becomes feasible. A single scan context may be sensitive to the center location of a scan under translational motion during revisit. For example, the row order of a scan context may not be preserved when revisiting the same place in a different lane. To overcome this situation, we translate a raw point cloud into N trans neighbors (N trans = 8 used in the paper) depending on the lane level interval and store scan contexts obtained from these root-shifted point clouds together. We assumed that a similar point cloud is obtained even at the actual moved location, which is valid except for a few cases such as an intersection access point where a new space suddenly appears.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Similarity Score between Scan Contexts</head><p>Given a scan context pair, we then need a distance measure for the similarity of two places. I q and I c are scan contexts acquired from a query point cloud and a candidate point cloud, respectively. They are compared in a columnwise manner. That is, the distance is the sum of distances between columns at a same index. A cosine distance is used to compute a distance between two column vectors at the same index, c q j and c c j . In addition, we divide the summation by the number of columns N s for normalization. Therefore, the distance function is</p><formula xml:id="formula_5">d(I q , I c ) = 1 N s Ns j=1 1 - c q j • c c j c q j c c j .<label>(5)</label></formula><p>The column-wise comparison is particularly effective for dynamic objects by considering the consensus of throughout sectors. However, the column of the candidate scan context may be shifted even in the same place, since a viewpoint of a LiDAR changes for different places (e.g., revisit in an opposite direction or corner). Fig. <ref type="figure" target="#fig_1">3</ref> illustrates such cases. Since a scan context is the representation dependent on the sensor location, the row order is always consistent. However, the column order could be different if the LiDAR sensor coordinate with respect to the global coordinate changed.</p><p>To alleviate this problem, we calculate distances with all possible column-shifted scan contexts and find the minimum distance. I c n is a scan context whose n columns are shifted from the original one, I c . This is the same task as roughly aligning two point clouds for yaw rotation at 2π</p><p>Ns resolution. Then we decide that the number of column shift for the best alignment <ref type="bibr" target="#b6">(7)</ref> and the distance (6) at that time:</p><formula xml:id="formula_6">D(I q , I c ) = min n∈[Ns] d(I q , I c n ) ,<label>(6)</label></formula><formula xml:id="formula_7">n * = argmin n∈[Ns] d(I q , I c n ) .<label>(7)</label></formula><p>Note that this additional shift information may serve as a good initial value for further localization refinement such as Iterated Closest Point (ICP), as shown in Section IV-C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Two-phase Search Algorithm</head><p>Three main streams are typical when searching in the context of place recognition: pairwise similarity scoring, nearest neighbor search, and sparse optimization <ref type="bibr" target="#b25">[26]</ref>. Our search algorithm fuses both pairwise scoring and nearest search hierarchically to achieve a reasonable searching time. Since our distance calculation in ( <ref type="formula" target="#formula_6">6</ref>) is heavier than other global descriptors such as <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b9">10]</ref>, we provide a two-phase hierarchical search algorithm via introducing ring key. Ring key is a rotation-invariant descriptor, which is extracted from a scan context. Each row of a scan context, r, is encoded into a single real value via ring encoding function ψ. The first element of the vector k is from the nearest circle from a sensor, and following elements are from the next rings in order as illustrated in Fig. <ref type="figure" target="#fig_2">4</ref>. Therefore, the ring key becomes a N r -dimensional vector as (8): k = (ψ(r 1 ), ..., ψ(r Nr )), where ψ :</p><formula xml:id="formula_8">r i → R .<label>(8)</label></formula><p>The ring encoding function ψ we use is the occupancy ratio of a ring using L 0 norm:</p><formula xml:id="formula_9">ψ(r i ) = r i 0 N s .<label>(9)</label></formula><p>Since the occupancy ratio is independent of the viewpoint, the ring key achieves rotation invariance.</p><p>Although being less informative than scan context, ring key enables fast search for finding possible candidates for loop. The vector k is used as a key to construct a KD tree. At the same time, the ring key of the query is used to find similar keys and their corresponding scan indexes. The number of top similar keys that will be retrieved is determined by a user. These constant number of candidates' scan contexts are compared against the query scan context by using distance <ref type="bibr" target="#b5">(6)</ref>. The closest candidate to the query satisfying an acceptance threshold is selected as the revisited place:</p><formula xml:id="formula_10">c * = argmin c k ∈C D(I q , I c k ), s.t D &lt; τ , (<label>10</label></formula><formula xml:id="formula_11">)</formula><p>where C is a set of indexes of candidates extracted from KD tree and τ is a given acceptance threshold. c * is the index of the place determined to be a loop. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL EVALUATION</head><p>In this section, our representation and algorithm are evaluated over various datasets and against other state-of-the-art algorithms. Since scan context is the global descriptor, the performance of our representation is compared to three other global representations using a 3D point cloud: M2DP <ref type="bibr" target="#b9">[10]</ref>, Zprojection <ref type="bibr" target="#b11">[12]</ref>, and ESF <ref type="bibr" target="#b10">[11]</ref>. We use ESF in the Point Cloud Library (PCL) implemented in C++, Matlab codes of M2DP on the web 1 from the authors He et al., and implement Zprojection on Matlab ourselves. All experiments are carried out on the same system with an Intel i7-6700 CPU at 3.40GHz and 16GB memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset and Experimental Settings</head><p>We use the KITTI dataset 2 <ref type="bibr" target="#b26">[27]</ref>, the NCLT dataset 3 <ref type="bibr" target="#b27">[28]</ref>, and the Complex Urban LiDAR dataset 4 <ref type="bibr" target="#b28">[29]</ref> for the validation of our method. These three datasets are selected considering diversity, such as the type of the 3D LiDAR sensor (e.g., the number of rays, sensor mount types such as surround and tilted) and the type of loops (e.g., occurred at the same direction or the opposite direction called reverse loop). Characteristics of each dataset are summarized in Table <ref type="table">I</ref>. The term node means a single sampled place.</p><p>1) KITTI dataset: Among the 11 sequences having the ground truth of pose (from 00 to 10), the top four sequences whose the number of loop occurrences is highest are selected: 00, 02, 05, and 08. The sequence 08 has only reverse loops, and others have loop events with the same direction. The scans of the KITTI dataset had been obtained from the 64-ray LiDAR (Velodyne HDL-64E) located in the center of the car. Since the KITTI dataset provides scans with indexes, we use each bin file as a node directly.</p><p>2) NCLT dataset: The NCLT dataset provides long-term measurements of different days along similar routes. Scans of the NCLT dataset were obtained from the 32-ray LiDAR (Velodyne HDL-32E) attached to a segway mobile platform. Four sequences are selected considering the number of loop occurrences and seasonal diversity. In this experiment, the scans are sampled at equidistant (2 m) intervals, and only those sampled scans are used as nodes for convenience.</p><p>3) Complex Urban LiDAR dataset: The Complex Urban LiDAR dataset includes various complex urban environments from residential to metropolitan areas. Four sequences are selected considering the complexity and wide road rate 1 https://github.com/LiHeUA/M2DP 2 http://www.cvlibs.net/datasets/kitti/eval odometry.php 3 http://robots.engin.umich.edu/nclt/ 4 http://irap.kaist.ac.kr/dataset/ provided by <ref type="bibr" target="#b28">[29]</ref>. Among three sub-routes in the sequence 04, 04 0 and 04 1 are used in this experiment. The scans are sampled at 3 m intervals for convenience. The interesting fact is that this dataset uses two tilted LiDARs (Velodyne VLP-16 PUCK) for urban mapping. Thus, a single scan of this dataset is able to measure higher parts of structures but does not have a 360 • surround view. To include more information in all directions, we merge the point clouds from both left and right tilted LiDARs and use them as a single scan to create a scan context.</p><p>If a ground truth pose distance between the query and the matched node is less than 4 m, the detection is considered as true positive. In total 50 previously adjacent nodes are excluded from the search. The experiments for scan context are conducted with 10 candidates and 50 candidates from the KD tree, thus each method is called scan context-10 and scan context-50, respectively. Unlike the scan context, which only compares with a constant number of candidates extracted from the KD tree, other methods (M2DP, ESF, and Z-projection) compare the query description to all in the database. In this paper, we set parameters of scan context as N s = 60, N r = 20, and L max = 80 m. That is, each sector has a 6 • resolution and each ring has a 4 m gap. The number of bins of Z-projection is set as 100. We use the default parameters of the available codes for M2DP and ESF. For the computation efficiency, we downsample point cloud with 0.6 m 3 grid for both scan context and M2DP, since He et al. <ref type="bibr" target="#b9">[10]</ref> reported M2DP is robust to downsampling, whereas Z-projection and ESF use an original point cloud without downsampling because they are vulnerable to low density. We change only an acceptance threshold in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Precision Recall Evaluation</head><p>The performance of Scan Context is analyzed using the precision-recall curve as in Fig. <ref type="figure" target="#fig_3">5</ref>. The histogram-based approaches, ESF and Z-projection, reported poor performances on all datasets. These methods rely on the histogram and distinguish places only when the structure of the visible space is substantially different. Unlike these histogram based methods, ours presented the meaningful performance for the entire data sequences. Overall, scan context-50 always reveals better performance than scan context-10. The performance of scan context depends on the number of candidates from the KD tree. Since ring key is less informative than scan context, inspecting a small number (e.g., 10 of more than 3000 nodes) of candidates is vulnerable if there are many similar structures.</p><p>The proposed method outperformed other approaches when applied to the outdoor urban dataset. This is due to the fact the motivation for using the vertical height is from urban analysis. However, the performance is limited when applied to an indoor environment where variation in vertical height is less significant. When applied to the NCLT dataset, the scan context presented low performance both for recall and precision (left part of each graph) because the trajectory of the NCLT dataset contains narrow indoor environments where an only small area is available.</p><p>Evaluating with the Complex Urban LiDAR dataset, all methods show poorer performance than at the KITTI dataset. In particular, Urban 02 provides the most challenging case for all methods since this sequence has narrow roads and repeated structures with similar height and rectangle shapes 5 compared to KITTI. The example of scan context from this challenging Urban 02 is given in Fig. <ref type="figure" target="#fig_4">6</ref>. Despite some level of performance drop is reported in this 5 http://irap.kaist.ac.kr/dataset/webgl/urban02/urban02 sick.html challenging dataset, the proposed method still outperformed other existing methods.</p><p>The proposed descriptor presented a strong rotationinvariance even for a reversed revisit by using view alignment based matching. For example, M2DP failed to detect a reverse loop. Among the datasets, KITTI 08 has only reverse loops and the proposed method substantially outperformed others. This phenomenon is also observed in NCLT sequences having partial reverse loops. Therefore, at NCLT sequences, M2DP reports high precision at the very low recalls because the forward loops are detected correctly. However, since reverse loops are missed, the slope of the curve rapidly decreases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Localization Accuracy</head><p>The proposed method can also be used when providing robust initial estimate for other localization approaches such as ICP. We conducted the experiment using KITTI 08 having reverse loops. ICP is performed point-to-point without downsampling. The example of ICP results with and without initialization are depicted in Fig. <ref type="figure">7</ref>. For this sequence, we further validate the improvement in terms of both computation time and root mean square error (RMSE). Fig. <ref type="figure">8</ref> shows the improved performance with the initial yaw rotation estimates using <ref type="bibr" target="#b6">(7)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Computational Complexity</head><p>The average computation times evaluated on KITTI 00 are given in Table <ref type="table" target="#tab_2">II</ref>. Point cloud downsampling with a 0.6 m 3 grid is used for all methods. In these experiments, the scan context creation takes longer because we employ scan context augmentation, which is non-mandatory. Thus, the time required to create a single scan context (0.0143 s, except for scan context augmentation) is shorter than it is with the other methods. The search time of the scan context includes both creation of the KD tree and computation of the distance. Scan context may require a longer search time than other global descriptors, but in a reasonable bound (2-5 Hz on Matlab).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we presented a spatial descriptor, Scan Context, summarizing a place as a matrix that explicitly describes the 2.5D structural information of an egocentric environment. Compared to existing global descriptors using a point cloud, scan context showed higher loop-detection performance across various datasets.</p><p>In future work, we plan to extend scan context by introducing additional layers. That is, other bin encoding functions (e.g., a bin's semantic information) can be used to improve performance, even for datasets with highly repetitive structures such as the Complex Urban LiDAR dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. Two-step scan context creation. Using the top view of a point cloud from a 3D scan (a), we partition ground areas into bins, which are split according to both azimuthal (from 0 to 2π within a LiDAR frame) and radial (from center to maximum sensing range) directions. We refer to the yellow area as a ring, the cyan area as a sector, and the black-filled area as a bin. Scan context is a matrix as in (b) that explicitly preserves the absolute geometrical structure of a point cloud. The ring and sector described in (a) are represented by the same-colored column and row, respectively, in (b). The representative value extracted from the points located in each bin is used as the corresponding pixel value of (b). In this paper, we use the maximum height of points in a bin.</figDesc><graphic coords="1,340.40,417.94,185.71,59.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Example of scan contexts from the same place with time interval. The change of the sensor viewpoint at the revisit causes column shifts of the scan context as in (a). However, the two matrices contain similar shapes and show the same row order.</figDesc><graphic coords="4,91.79,151.11,176.09,56.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The ring key generation for the fast search.</figDesc><graphic coords="4,346.86,56.57,188.55,182.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Precision-recall curves for the evaluation dataset. The route direction during the revisit is shown in parentheses.</figDesc><graphic coords="6,352.44,505.26,176.23,56.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. A challenging example captured from Complex Urban LiDAR dataset sequence 02. The road is so narrow in all directions that the amount of available information is too small.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .Fig. 8 .</head><label>78</label><figDesc>Fig.7. An example of point-to-point ICP results from KITTI 08. The query and the detected point clouds are from the 1785 th and 109 th scans, respectively. The LiDAR sensor frame represents the coordinates of the point clouds. Scoring the similarity between two scan contexts provides a coarse yaw rotation, which serves as an initial estimate to guide finer localization (i.e., ICP). In the case of this reverse loop, registration easily fails without such an initial estimate. By contrast, even this kind of unstructured environment can be registered with the use of an initial estimate obtained from the scan context.</figDesc><graphic coords="7,89.25,58.01,90.93,87.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>. Pairwise Similarity Score Loop detection 2. Search Fig</head><label></label><figDesc>. 2. Algorithm overview. First, a point cloud in a single 3D scan is encoded into scan context. Then, Nr (the number of rings) dimensional vector is encoded from the scan context and is used for retrieving the nearest candidates as well as the construction of the KD tree. Finally, the retrieved candidates are compared to the query scan context. The candidate that satisfies the acceptance threshold and is closest to the query is considered the loop.</figDesc><table><row><cell></cell><cell cols="2">2-1. Nearest Neighbor Search</cell></row><row><cell></cell><cell>Ring key</cell><cell>KD tree</cell><cell>Top similar places</cell></row><row><cell>1. Description</cell><cell>2-2</cell><cell></cell></row><row><cell>3D Point cloud (of a single scan)</cell><cell>Scan Context</cell><cell></cell><cell>Candidate Scan Contexts in the database</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II AVERAGE</head><label>II</label><figDesc>TIME COSTS ON KITTI00.</figDesc><table><row><cell></cell><cell cols="2">Calculating Descriptor (s) Searching Loop (s)</cell></row><row><cell>Scan context-10</cell><cell>0.1291</cell><cell>0.0807</cell></row><row><cell>Scan context-50</cell><cell>0.1291</cell><cell>0.3331</cell></row><row><cell>M2DP</cell><cell>0.0218</cell><cell>0.0032</cell></row><row><cell>Z-projection</cell><cell>0.0472</cell><cell>0.0035</cell></row><row><cell>ESF</cell><cell>0.0635</cell><cell>0.0043</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2018" xml:id="foot_0"><p>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) Madrid, Spain, October 1-5, 2018 978-1-5386-8094-0/18/$31.00 ©2018 IEEE</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work is supported through a grant from the Korea MOTIE (No. 10051867) and [High-Definition Map Based Precise Vehicle Localization Using Cameras and LIDARs] project funded by Naver Labs Corporation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Past, present, and future of simultaneous localization and mapping: Toward the robust-perception age</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cadena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Carlone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Carrillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Latif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Neira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Leonard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Robot</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1309" to="1332" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">SRAL: Shared representative appearance learning for long-term visual place recognition</title>
		<author>
			<persName><forename type="first">F</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rentschler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robot. and Automat. Lett</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1172" to="1179" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Robust loop closing over time for pose graph SLAM</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Latif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cadena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Neira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intl. J. of Robot. Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="1611" to="1626" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Aligning point cloud views using persistent feature histograms</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Blodow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Marton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Beetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/RSJ Intl. Conf. on Intell. Robots and Sys</title>
		<meeting>IEEE/RSJ Intl. Conf. on Intell. Robots and Sys</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="3384" to="3391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">SHOT: Unique signatures of histograms for surface and texture description</title>
		<author>
			<persName><forename type="first">S</forename><surname>Salti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Di</forename><surname>Stefano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="page" from="251" to="264" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Using spin images for efficient object recognition in cluttered 3D scenes</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intell</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="433" to="449" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Shape matching and object recognition using shape contexts</title>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Puzicha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="509" to="522" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Place recognition in 3D scans using a combination of bag of words and point feature based relative pose estimation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Steder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ruhnke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Grzonka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/RSJ Intl. Conf. on Intell. Robots and Sys</title>
		<meeting>IEEE/RSJ Intl. Conf. on Intell. Robots and Sys</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1249" to="1255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Large scale place recognition in 2D LIDAR scans using geometrical landmark relations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Himstedt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Frost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hellbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Böhme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Maehle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/RSJ Intl. Conf. on Intell. Robots and Sys</title>
		<meeting>IEEE/RSJ Intl. Conf. on Intell. Robots and Sys</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="5030" to="5035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">M2DP: a novel 3D point cloud descriptor and its application in loop closure detection</title>
		<author>
			<persName><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/RSJ Intl. Conf. on Intell. Robots and Sys</title>
		<meeting>IEEE/RSJ Intl. Conf. on Intell. Robots and Sys</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="231" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Ensemble of shape functions for 3D object classification</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wohlkinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vincze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2987" to="2992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Loop closure detection using small-sized signatures from 3D LIDAR data</title>
		<author>
			<persName><forename type="first">N</forename><surname>Muhammad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lacroix</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Safety, Security, and Rescue Robot</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="333" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">FAB-MAP: Probabilistic localization and mapping in the space of appearance</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intl. J. of Robot. Research</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="647" to="665" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast and incremental method for loop-closure detection using bags of visual words</title>
		<author>
			<persName><forename type="first">A</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Filliat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Doncieux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-A</forename><surname>Meyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Robot</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1027" to="1037" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bags of binary words for fast place recognition in image sequences</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gálvez-López</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Tardós</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Robot</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1188" to="1197" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">SIFT, SURF and Seasons: Long-term outdoor localization using local features</title>
		<author>
			<persName><forename type="first">C</forename><surname>Valgren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. on Mobile Robot</title>
		<meeting>European Conf. on Mobile Robot</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="253" to="258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">SeqSLAM: Visual route-based navigation for sunny summer days and stormy winter nights</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Milford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Wyeth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Intl. Conf. on Robot. and Automat</title>
		<meeting>IEEE Intl. Conf. on Robot. and Automat</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1643" to="1649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">BRIEF-Gist-Closing the loop by simple means</title>
		<author>
			<persName><forename type="first">N</forename><surname>Sünderhauf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Protzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/RSJ Intl. Conf. on Intell. Robots and Sys</title>
		<meeting>IEEE/RSJ Intl. Conf. on Intell. Robots and Sys</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1234" to="1241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Robust visual robot localization across seasons using network flows</title>
		<author>
			<persName><forename type="first">T</forename><surname>Naseer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Spinello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stachniss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI National Conf. on Art. Intell</title>
		<meeting>AAAI National Conf. on Art. Intell</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2564" to="2570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Keypoint design and evaluation for place recognition in 2D lidar maps</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zlot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robot. and Autonomous Sys</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1211" to="1224" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Efficient loop closure based on FALKO lidar features for online robot localization and mapping</title>
		<author>
			<persName><forename type="first">F</forename><surname>Kallasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Rizzini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/RSJ Intl. Conf. on Intell. Robots and Sys</title>
		<meeting>IEEE/RSJ Intl. Conf. on Intell. Robots and Sys</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1206" to="1213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Place recognition of 3D landmarks based on geometric relations</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Rizzini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/RSJ Intl. Conf. on Intell. Robots and Sys</title>
		<meeting>IEEE/RSJ Intl. Conf. on Intell. Robots and Sys</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">SegMatch: Segment based place recognition in 3D point clouds</title>
		<author>
			<persName><forename type="first">R</forename><surname>Dubé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dugas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Stumm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nieto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cadena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Intl. Conf. on Robot. and Automat</title>
		<meeting>IEEE Intl. Conf. on Robot. and Automat</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5266" to="5272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">To take hold of space: isovists and isovist fields</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Benedikt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Env. and Plan. B: Plan. and Design</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="65" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A digital image of the city: 3D isovists in lynch&apos;s urban analysis</title>
		<author>
			<persName><forename type="first">E</forename><surname>Morello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ratti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Env. and Plan. B: Plan. and Design</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="837" to="853" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Robust multimodal sequence-based loop closure detection via structured sparsity</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Robot.: Science &amp; Sys</title>
		<meeting>Robot.: Science &amp; Sys</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the KITTI vision benchmark suite</title>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Comput. Vision and Pattern Recog</title>
		<meeting>IEEE Conf. on Comput. Vision and Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">University of Michigan North Campus long-term vision and lidar dataset</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carlevaris-Bianco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Ushani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Eustice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intl. J. of Robot. Research</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1023" to="1035" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Complex Urban LiDAR Data Set</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-S</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Roh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Intl. Conf. on Robot. and Automat</title>
		<meeting>IEEE Intl. Conf. on Robot. and Automat</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>in print</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
