<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Topology Attack and Defense for Graph Neural Networks: An Optimization Perspective</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kaidi</forename><surname>Xu</surname></persName>
							<email>xu.kaid@husky.neu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Electrical &amp; Computer Engineering</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hongge</forename><surname>Chen</surname></persName>
							<email>chenhg@mit.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Electrical Engineering &amp; Computer Science</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sijia</forename><surname>Liu</surname></persName>
							<email>sijia.liu@ibm.com</email>
							<affiliation key="aff2">
								<orgName type="department">MIT-IBM Watson AI Lab</orgName>
								<address>
									<country>IBM Research</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
							<email>pin-yu.chen@ibm.com</email>
							<affiliation key="aff2">
								<orgName type="department">MIT-IBM Watson AI Lab</orgName>
								<address>
									<country>IBM Research</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tsui-Wei</forename><surname>Weng</surname></persName>
							<email>twweng@mit.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Electrical Engineering &amp; Computer Science</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mingyi</forename><surname>Hong</surname></persName>
							<email>mhong@umn.edu</email>
							<affiliation key="aff3">
								<orgName type="department">Electrical &amp; Computer Engineering</orgName>
								<orgName type="institution">University of Minnesota</orgName>
								<address>
									<settlement>Minneapolis</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xue</forename><surname>Lin</surname></persName>
							<email>xue.lin@northeastern.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Electrical &amp; Computer Engineering</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Topology Attack and Defense for Graph Neural Networks: An Optimization Perspective</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph neural networks (GNNs) which apply the deep neural networks to graph data have achieved significant performance for the task of semisupervised node classification. However, only few work has addressed the adversarial robustness of GNNs. In this paper, we first present a novel gradient-based attack method that facilitates the difficulty of tackling discrete graph data. When comparing to current adversarial attacks on GNNs, the results show that by only perturbing a small number of edge perturbations, including addition and deletion, our optimization-based attack can lead to a noticeable decrease in classification performance. Moreover, leveraging our gradientbased attack, we propose the first optimizationbased adversarial training for GNNs. Our method yields higher robustness against both different gradient based and greedy attack methods without sacrificing classification accuracy on original graph. Code is available at https://github.com/ KaidiXu/GCN_ADV_Train.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graph structured data plays a crucial role in many AI applications. It is an important and versatile representation to model a wide variety of datasets from many domains, such as molecules, social networks, or interlinked documents with citations. Graph neural networks (GNNs) on graph structured data have shown outstanding results in various applications <ref type="bibr">[Kipf and Welling, 2016;</ref><ref type="bibr" target="#b7">Veličković et al., 2017;</ref><ref type="bibr" target="#b8">Xu et al., 2019a]</ref>. However, despite the great success on inferring from graph data, the inherent challenge of lacking adversarial robustness in deep learning models still carries over to security-related domains such as blockchain or communication networks.</p><p>In this paper, we aim to evaluate the robustness of GNNs from a perspective of first-order optimization adversarial attacks. It is worth mentioning that first-order methods have achieved great success for generating adversarial attacks on audios or images <ref type="bibr" target="#b1">[Carlini and Wagner, 2018;</ref><ref type="bibr" target="#b8">Xu et al., 2019b;</ref><ref type="bibr" target="#b3">Chen et al., 2018b;</ref><ref type="bibr" target="#b8">Xu et al., 2019c;</ref><ref type="bibr" target="#b3">Chen et al., 2018a]</ref>. However, some recent works <ref type="bibr" target="#b4">[Dai et al., 2018;</ref><ref type="bibr" target="#b0">Bojcheski and Günnemann, 2018]</ref> suggested that conventional (first-order) continuous optimization methods do not directly apply to attacks using edge manipulations (we call topology attack) due to the discrete nature of graphs. We close this gap by studying the problem of generating topology attacks via convex relaxation so that gradient-based adversarial attacks become plausible for GNNs. Benchmarking on node classification tasks using GNNs, our gradient-based topology attacks outperform current state-of-the-art attacks subject to the same topology perturbation budget. This demonstrates the effectiveness of our attack generation method through the lens of convex relaxation and first-order optimization. Moreover, by leveraging our proposed gradient-based attack, we propose the first optimization-based adversarial training technique for GNNs, yielding significantly improved robustness against gradientbased and greedy topology attacks.</p><p>Our new attack generation and adversarial training methods for GNNs are built upon the theoretical foundation of spectral graph theory, first-order optimization, and robust (mini-max) optimization. We summarize our main contributions as follows:</p><p>• We propose a general first-order attack generation framework under two attacking scenarios: a) attacking a pre-defined GNN and b) attacking a re-trainable GNN. This yields two new topology attacks: projected gradient descent (PGD) topology attack and min-max topology attack. Experimental results show that the proposed attacks outperform current state-of-the-art attacks.</p><p>• With the aid of our first-order attack generation methods, we propose an adversarial training method for GNNs to improve their robustness. The effectiveness of our method is shown by the considerable improvement of robustness on GNNs against both optimization-based and greedy-search-based topology attacks. <ref type="bibr">:1906.04214v3 [cs.</ref>LG] 14 Oct 2019 Some recent attentions have been paid to the robustness of graph neural network. <ref type="bibr">Both [Zügner et al., 2018]</ref> and <ref type="bibr" target="#b4">[Dai et al., 2018]</ref> studied adversarial attacks on neural networks for graph data. <ref type="bibr" target="#b4">[Dai et al., 2018]</ref>   <ref type="bibr" target="#b0">[Bojcheski and Günnemann, 2018]</ref>. This attack is based on perturbation theory to maximize the loss obtained from DeepWalk <ref type="bibr">[Perozzi et al., 2014]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv</head><p>In <ref type="bibr" target="#b8">[Zügner and Günnemann, 2019]</ref>, training-time attacks on GNNs were also investigated for node classification by perturbing the graph structure. The authors solved a min-max problem in training-time attacks using meta-gradients and treated the graph topology as a hyper-parameter to optimize.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem Statement</head><p>We begin by providing preliminaries on GNNs. We then formalize the attack threat model of GNNs in terms of edge perturbations, which we refer as 'topology attack'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries on GNNs</head><p>It has been recently shown in <ref type="bibr">[Kipf and Welling, 2016;</ref><ref type="bibr" target="#b7">Veličković et al., 2017;</ref><ref type="bibr" target="#b8">Xu et al., 2019a]</ref>  </p><formula xml:id="formula_0">A ij = 0 if (i, j) / ∈ E.</formula><p>In a GNN, we assume that each node i is associated with a feature vector x i ∈ R M0 and a scalar label y i . The goal of GNN is to predict the class of an unlabeled node under the graph topology A and the training data {(x i , y i )} Ntrain i=1 . Here GNN uses input features of all nodes but only N train &lt; N nodes with labeled classes in the training phase.</p><p>Formally, the kth layer of a GNN model obeys the propagation rule of the generic form</p><formula xml:id="formula_1">h (k) i = g (k) {W (k−1) h (k−1) j Ãij, ∀j ∈ N (i)} , ∀i ∈ [N ] (1)</formula><p>where h (k) i ∈ R M k denotes the feature vector of node i at layer k, h (0) i = x i ∈ R M0 is the input feature vector of node i, g (k) is a possible composite mapping (activation) function, W (k−1) ∈ R M k ×M k−1 is the trainable weight matrix at layer (k − 1), Ãij is the (i, j)th entry of Ã that denotes a linear mapping of A but with the same sparsity pattern, and N (i) denotes node i's neighbors together with itself, i.e., N (i) = {j|(i, j) ∈ E, or j = i}.</p><p>A special form of GNN is graph convolutional networks (GCN) <ref type="bibr">[Kipf and Welling, 2016]</ref>. This is a recent approach of learning on graph structures using convolution operations which is promising as an embedding methodology. In GCNs, the propagation rule (1) becomes <ref type="bibr">[Kipf and Welling, 2016]</ref> </p><formula xml:id="formula_2">h (k) i = σ   j∈N i W (k−1) h (k−1) j Ãij   ,<label>(2)</label></formula><p>where σ(•) is the ReLU function. Let Ãi,: denote the ith row of Ã and</p><formula xml:id="formula_3">H (k) = (h (k) 1 ) ; . . . ; (h (k)</formula><p>N ) , we then have the standard form of GCN,</p><formula xml:id="formula_4">H (k) = σ ÃH (k−1) (W (k−1) ) .<label>(3)</label></formula><p>Here Ã is given by a normalized adjacency matrix Ã = D−1/2 Â D−1/2 , where Â = A + I, and Dij = 0 if i = j</p><p>and Dii = 1 Â:,i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Topology Attack in Terms of Edge Perturbation</head><p>We introduce a Boolean symmetric matrix S ∈ {0, 1} N ×N to encode whether or not an edge in G is modified. That is, the edge connecting nodes i and j is modified (added or removed) if and only if S ij = S ji = 1. Otherwise, S ij = 0 if i = j or the edge (i, j) is not perturbed. Given the adjacency matrix A, its supplement is given by Ā = 11 T − I − A, where I is an identity matrix, and (11 T − I) corresponds to the fullyconnected graph. With the aid of edge perturbation matrix S and Ā, a perturbed graph topology A against A is given by</p><formula xml:id="formula_5">A = A + C • S, C = Ā − A,<label>(4)</label></formula><p>where • denotes the element-wise product. In (4), the positive entry of C denotes the edge that can be added to the graph A, and the negative entry of C denotes the edge that can be removed from A. We then formalize the concept of topology attack to GNNs: Finding minimum edge perturbations encoded by S in (4) to mislead GNNs. A more detailed attack formulation will be studied in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Topology Attack Generation: A First-Order Optimization Perspective</head><p>In this section, we first define attack loss (beyond the conventional cross-entropy loss) under different attacking scenarios.</p><p>We then develop two efficient attack generation methods by leveraging first-order optimization. We call the resulting attacks projected gradient descent (PGD) topology attack and min-max topology attack, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Attack Loss &amp; Attack Generation</head><p>Let Z(S, W; A, {x i }) denote the prediction probability of a GNN specified by A in (4) and W under input features {x i }.</p><p>Then Z i,c denotes the probability of assigning node i to class c. It has been shown in existing works <ref type="bibr" target="#b4">[Goodfellow et al., 2015;</ref><ref type="bibr">Kurakin et al., 2017]</ref> that the negative cross-entropy (CE) loss between the true labels (y i ) and the predicted labels ({Z i,c }) can be used as an attack loss at node i, denoted by f i (S, W; A, {x i }, y i ). We can also propose a CW-type loss similar to Carlili-Wagner (CW) attacks for attacking image classifiers <ref type="bibr" target="#b0">[Carlini and Wagner, 2017]</ref>,</p><formula xml:id="formula_6">fi(S, W; A, {xi}, yi) = max Zi,y i − max c =y i Zi,c, −κ , (5)</formula><p>where κ ≥ 0 is a confidence level of making wrong decisions.</p><p>To design topology attack, we seek S in (4) to minimize the per-node attack loss (CE-type or CW-type) given a finite budge of edge perturbations. We consider two threat models: a) attacking a pre-defined GNN with known W; b) attacking an interactive GNN with re-trainable W. In the case a) of fixed W, the attack generation problem can be cast as</p><formula xml:id="formula_7">minimize s i∈V fi(s; W, A, {xi}, yi) subject to 1 s ≤ , s ∈ {0, 1} n ,<label>(6)</label></formula><p>where we replace the symmetric matrix variable S with its vector form that consists of n := N (N − 1)/2 unique perturbation variables in S. We recall that f i could be either a CE-type or a CW-type per-node attack loss. In the case b) of re-trainable W, the attack generation problem has the following min-max form minimize</p><formula xml:id="formula_8">1 s≤ ,s∈{0,1} n maximize W i∈V fi(s, W; A, {xi}, yi),<label>(7)</label></formula><p>where the inner maximization aims to constrain the attack loss by retraining W so that attacking GNN is more difficult. Motivated by targeted adversarial attacks against image classifiers <ref type="bibr" target="#b0">[Carlini and Wagner, 2017]</ref>, we can define targeted topology attacks that are restricted to perturb edges of targeted nodes. In this case, we require to linearly constrain S in (4) as S i,• = 0 if i is not a target node. As a result, both attack formulations ( <ref type="formula" target="#formula_7">6</ref>) and ( <ref type="formula" target="#formula_8">7</ref>) have extra linear constraints with respect to s, which can be readily handled by the optimization solver introduced later. Without loss of generality, we focus on untargeted topology attacks in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">PGD Topology Attack</head><p>Problem ( <ref type="formula" target="#formula_7">6</ref>) is a combinatorial optimization problem due to the presence of Boolean variables. For ease of optimization, we relax s ∈ {0, 1} n to its convex hull s ∈ [0, 1] n and solve the resulting continuous optimization problem,</p><formula xml:id="formula_9">minimize s f (s) := i∈V fi(s; W, A, {xi}, yi) subject to s ∈ S,<label>(8)</label></formula><p>where</p><formula xml:id="formula_10">S = {s | 1 T s ≤ , s ∈ [0, 1] n }.</formula><p>Suppose that the solution of problem ( <ref type="formula" target="#formula_9">8</ref>) is achievable, the remaining question is how to recover a binary solution from it. Since the variable s in (8) can be interpreted as a probabilistic vector, a randomization sampling <ref type="bibr" target="#b5">[Liu et al., 2016]</ref> is suited for generating a near-optimal binary topology perturbation; see details in Algorithm 1.</p><p>Algorithm 1 Random sampling from probabilistic to binary topology perturbation 1: Input: probabilistic vector s, K is # of random trials 2: for k = 1, 2, . . . , K do 3:</p><p>draw binary vector u (k) following</p><formula xml:id="formula_11">u (k) i = 1 with probability si 0 with probability 1 − si , ∀i<label>(9)</label></formula><p>4: end for 5: choose a vector s * from {u (k) } which yields the smallest attack loss f (u (k) ) under 1 T s ≤ .</p><p>We solve the continuous optimization problem (8) by projected gradient descent (PGD),</p><formula xml:id="formula_12">s (t) = ΠS s (t−1) − ηt ĝt , (<label>10</label></formula><formula xml:id="formula_13">)</formula><p>where t denotes the iteration index of PGD, η t &gt; 0 is the learning rate at iteration t, ĝt = ∇f (s (t−1) ) denotes the gradient of the attack loss f evaluated at s (t−1) , and Π S (a) := arg min s∈S s − a 2 2 is the projection operator at a over the constraint set S. In Proposition 1, we show that the projection operation yields the closed-form solution. Proposition 1 Given S = {s | 1 T s ≤ , s ∈ [0, 1] n }, the projection operation at the point a with respect to S is</p><formula xml:id="formula_14">ΠS (a) =      P [0,1] [a − µ1]</formula><p>If µ &gt; 0 and</p><formula xml:id="formula_15">1 T P [0,1] [a − µ1] = , P [0,1] [a] If 1 T P [0,1] [a] ≤ ,<label>(11)</label></formula><p>where</p><formula xml:id="formula_16">P [0,1] (x) = x if x ∈ [0, 1], 0 if x &lt; 0, and 1 if x &gt; 1.</formula><p>Proof: We express the projection problem as minimize</p><formula xml:id="formula_17">s 1 2 s − a 2 2 + I [0,1] (s) subject to 1 s ≤ ,<label>(12)</label></formula><p>where</p><formula xml:id="formula_18">I [0,1] (s) = 0 if s ∈ [0, 1] n , and ∞ otherwise.</formula><p>The Lagrangian function of problem ( <ref type="formula" target="#formula_17">12</ref>) is given by</p><formula xml:id="formula_19">1 2 s − a 2 2 + I [0,1] (s) + µ(1 s − ) = i 1 2 (si − ai) 2 + I [0,1] (si) + µsi − µ ,<label>(13)</label></formula><p>where µ ≥ 0 is the dual variable. The minimizer to the above Lagrangian function (with respect to the variable s) is</p><formula xml:id="formula_20">s = P [0,1] (a − µ1),<label>(14)</label></formula><p>where P [0,1] is taken elementwise. Besides the stationary condition ( <ref type="formula" target="#formula_20">14</ref>), other KKT conditions for solving problem (12) are</p><formula xml:id="formula_21">µ(1 s − ) = 0, (15) µ ≥ 0, (<label>16</label></formula><formula xml:id="formula_22">)</formula><formula xml:id="formula_23">1 s ≤ . (<label>17</label></formula><formula xml:id="formula_24">)</formula><p>If µ &gt; 0, then the solution to problem ( <ref type="formula" target="#formula_17">12</ref>) is given by ( <ref type="formula" target="#formula_20">14</ref>), where the dual variable µ is determined by ( <ref type="formula" target="#formula_20">14</ref>) and ( <ref type="formula">15</ref>)</p><formula xml:id="formula_25">1 T P [0,1] [a − µ1] = , and µ &gt; 0.<label>(18)</label></formula><p>If µ = 0, then the solution to problem ( <ref type="formula" target="#formula_17">12</ref>) is given by ( <ref type="formula" target="#formula_20">14</ref>) and ( <ref type="formula" target="#formula_23">17</ref>),</p><formula xml:id="formula_26">s = P [0,1] (a), and 1 s ≤ , (<label>19</label></formula><formula xml:id="formula_27">)</formula><p>The proof is complete.</p><p>In the projection operation (11), one might need to solve the scalar equation 1 T P [0,1] [a − µ1] = with respect to the dual variable µ. This can be accomplished by applying the bisection method <ref type="bibr" target="#b0">[Boyd and Vandenberghe, 2004;</ref><ref type="bibr" target="#b5">Liu et al., 2015]</ref> </p><formula xml:id="formula_28">over µ ∈ [min(a − 1), max(a)]. That is because 1 T P [0,1] [a − max(a)1] ≤ and 1 T P [0,1] [a − min(a − 1)1] ≥</formula><p>, where max and min return the largest and smallest entry of a vector. We remark that the bisection method converges in the logarithmic rate given by log 2 [(max(a) − min(a − 1))/ξ] for the solution of ξ-error tolerance. We summarize the PGD topology attack in Algorithm 2.</p><p>Algorithm 2 PGD topology attack on GNN 1: Input: s (0) , &gt; 0, learning rate η t , and iterations T 2: for t = 1, 2, . . . , T do 3: gradient descent:</p><formula xml:id="formula_29">a (t) = s (t−1) − η t ∇f (s (t−1) ) 4:</formula><p>call projection operation in (11) 5: end for 6: call Algorithm 1 to return s * , and the resulting A in (4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Min-max Topology Attack</head><p>We next solve the problem of min-max attack generation in <ref type="bibr">(7)</ref>. By convex relaxation on the Boolean variables, we obtain the following continuous optimization problem where S has been defined in (8). We solve problem (20) by first-order alternating optimization <ref type="bibr" target="#b6">[Lu et al., 2019a;</ref><ref type="bibr" target="#b6">Lu et al., 2019b]</ref>, where the inner maximization is solved by gradient ascent, and the outer minimization is handled by PGD same as (10). We summarize the min-max topology attack in Algorithm 3. We remark that one can perform multiple maximization steps within each iteration of alternating optimization. This strikes a balance between the computation efficiency and the convergence accuracy <ref type="bibr" target="#b2">[Chen et al., 2017;</ref><ref type="bibr" target="#b7">Qian et al., 2018]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Robust Training for GNNs</head><p>With the aid of first-order attack generation methods, we now introduce our adversarial training for GNNs via robust optimization. Similar formulation is also used in <ref type="bibr" target="#b6">[Madry et al., 2017]</ref>. In adversarial training, we solve a min-max problem for robust optimization:</p><formula xml:id="formula_30">minimize W maximize s∈S −f (s, W),<label>(21)</label></formula><p>Algorithm 3 Min-max topology attack to solve (20)</p><p>1: Input: given W (0) , s (0) , learning rates β t and η t , and iteration numbers T 2: for t = 1, 2, . . . , T do 3: inner maximization over W: given s (t−1) , obtain</p><formula xml:id="formula_31">W t = W t−1 + βt∇ W f (s t−1 , W t−1 ) 4:</formula><p>outer minimization over s: given W (t) , running PGD (10), where ĝt = ∇ s f (s t−1 , W t ) 5: end for 6: call Algorithm 1 to return s * , and the resulting A in (4).</p><p>where f (x, W) denotes the attack loss specified in (20). Following the idea of adversarial training for image classifiers in <ref type="bibr" target="#b6">[Madry et al., 2017]</ref>, we restrict the loss function f as the CE-type loss. This formulation tries to minimize the training loss at the presence of topology perturbations.</p><p>We note that problems ( <ref type="formula" target="#formula_30">21</ref>) and ( <ref type="formula" target="#formula_8">7</ref>) share a very similar min-max form, however, they are not equivalent since the loss f is neither convex with respect to s nor concave with respect to W, namely, lacking saddle point property [Boyd and <ref type="bibr" target="#b0">Vandenberghe, 2004]</ref>. However, there exists connection between ( <ref type="formula" target="#formula_8">7</ref>) and ( <ref type="formula" target="#formula_30">21</ref> By changing variable q := −p, problem ( <ref type="formula">23</ref>) is equivalent to maximize W,q q subject to f (s, W) ≥ q, ∀s ∈ S.</p><p>(24)</p><p>By eliminating the epigraph variable q, problem (24) becomes ( <ref type="formula">22</ref>). By max-min inequality <ref type="bibr">[Boyd and Vandenberghe, 2004, Sec. 5</ref>.4], we finally obtain that maximize</p><formula xml:id="formula_32">W minimize s∈S f (s, W) ≤ minimize s∈S maximize W f (s, W).</formula><p>The proof is now complete. We summarize the robust training algorithm in Algorithm 4 for solving problem ( <ref type="formula">22</ref>). Similar to Algorithm 3, one usually performs multiple inner minimization steps (with respect to s) within each iteration t to have a solution towards minimizer during alternating optimization. This improves the stability of convergence in practice <ref type="bibr" target="#b7">[Qian et al., 2018;</ref><ref type="bibr" target="#b6">Madry et al., 2017]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>In this section, we present our experimental results for both topology attack and defense methods on a graph convolutional networks (GCN) <ref type="bibr">[Kipf and Welling, 2016]</ref>. We demonstrate the misclassification rate and the convergence of the Algorithm 4 Robust training for solving problem <ref type="bibr">(22)</ref> 1: Input: given W (0) , s (0) , learning rates β t and η t , and iteration numbers T 2: for t = 1, 2, . . . , T do 3: inner minimization over s: given W (t−1) , running PGD (10), where ĝt = ∇ s f (s t−1 , W t−1 ) 4: outer maximization over W: given s (t) , obtain</p><formula xml:id="formula_33">W t = W t−1 + βt∇ W f (s t , W t−1 ) 5: end for 6: return W T .</formula><p>proposed 4 attack methods: negative cross-entropy loss via PGD attack (CE-PGD), CW loss via PGD attack (CW-PGD), negative cross-entropy loss via min-max attack (CE-minmax), CW loss via min-max attack (CW-min-max). We then show the improved robustness of GCN by leveraging our proposed robust training against topology attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experimental Setup</head><p>We evaluate our methods on two well-known datasets: Cora and Citeseer <ref type="bibr" target="#b7">[Sen et al., 2008]</ref>. Both datasets contain unweighted edges which can be generated as symmetric adjacency matrix A and sparse bag-of-words feature vectors which can be treated the input of GCN. To train the model, all node feature vectors are fed into GCN but with only 140 and 120 labeled nodes for Cora and Citeseer, respectively. The number of test labeled nodes is 1000 for both datasets. At each experiment, we repeat 5 times based on different splits of training/testing nodes and report mean ± standard deviation of misclassification rate (namely, 1 − prediction accuracy) on testing nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Attack Performance</head><p>We compare our four attack methods (CE-PGD, CW-PGD, CE-min-max, CW-min-max) with DICE ('delete edges internally, connect externally') <ref type="bibr" target="#b7">[Waniek et al., 2018]</ref>, Meta-Self attack <ref type="bibr" target="#b8">[Zügner and Günnemann, 2019</ref>] and greedy attack, a variant of Meta-Self attack without weight re-training for GCN. The greedy attack is considered as a fair comparison with our CE-PGD and CW-PGD attacks, which are generated on a fixed GCN without weight re-training. In minmax attacks (CE-min-max and CW-min-max), we show misclassification rates against both natural and retrained models from Algorithm 3, and compare them with the state-of-theart Meta-Self attack. For a fair comparison, we use the same performance evaluation criterion in Meta-Self, testing nodes' predicted labels (not their ground-truth label) by an independent pre-trained model that can be used during the attack. In the attack problems ( <ref type="formula" target="#formula_7">6</ref>) and ( <ref type="formula" target="#formula_8">7</ref>), unless specified otherwise the maximum number of perturbed edges is set to be 5% of the total number of existing edges in the original graph. In Algorithm 1, we set the iteration number of random sampling as K = 20 and choose the perturbed topology with the highest misclassification rate which also satisfies the edge perturbation constraint.</p><p>In Table <ref type="table" target="#tab_2">1</ref>, we present the misclassification rate of different attack methods against both natural and retrained model from ( <ref type="formula">20</ref>). Here we recall that the retrained model arises due to the scenario of attacking an interactive GCN with re-trainable weights (Algorithm 3). For comparison, we also show the misclassification rate of a natural model with the true topology (denoted by 'clean'). As we can see, to attack the natural model, our proposed attacks achieve better misclassification rate than the existing methods. We also observe that compared to min-max attacks (CE-min-max and CW-min-max), CE-PGD and CW-PGD yield better attacking performance since it is easier to attack a pre-defined GCN. To attack the model that allows retraining, we set 20 steps of inner maximization per iteration of Algorithm 3. The results show that our proposed min-max attack achieves very competitive performance compared to Meta-Self attack. Note that evaluating the attack performance on the retrained model obtained from ( <ref type="formula">20</ref>) is not quite fair since the retrained weights could be suboptimal and induce degradation in classification. In Fig. <ref type="figure" target="#fig_2">1</ref>, we present the CE-loss and the CW-loss of the proposed topology attacks against the number of iterations in Algorithm 2. Here we choose T = 200 and η t = 200/ √ t. As we can see, the method of PGD converges gracefully against iterations. This verifies the effectiveness of the first-order optimization based attack generation method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Defense Performance</head><p>In what follows, we invoke Algorithm 4 to generate robust GCN via adversarial training. We set T = 1000, β t = 0.01 and η t = 200/ √ t. We run 20 steps for inner minimization. Inspired by <ref type="bibr" target="#b6">[Madry et al., 2017]</ref>, we increase the hidden units from 16 to 32 in order to create more capacity for this more complicated classifier. Initially, we set the maximum number of edges we can modify as 5% of total existing edges.</p><p>In Figure <ref type="figure">2</ref>, we present convergence of our robust training. As we can see, the loss drops reasonably and the 1, 000 iterations are necessary for robust training rather than normal training process which only need 200 iterations. We also observe that our robust training algorithm does not harm the test accuracy when = 5%, but successfully improves the robustness as the attack success rate drops from 28.0% to 22.0% in Cora dataset as shown in Table <ref type="table" target="#tab_3">2,</ref> After showing the effectiveness of our algorithm, we explore deeper in adversarial training on GCN. We aim to show how large we can use in robust training. So we set from 5% to 20% and apply CE-PGD attack following the same setting. The results are presented in Table <ref type="table" target="#tab_4">3</ref>. Note that when = 0, the first row shows misclassification rates of test nodes on natural graph as the baseline for lowest misclassification rate we can obtain; the first column shows the CE-PGD attack misclassification rates of natural model as the baseline for highest misclassification rate we can obtain. We can conclude that when a robust model trained under an constraint, the model will gain robustness under this distinctly. Considering its importance to keep the original graph test performance, we suggest generating robust model under = 0.1. Moreover, please refer to Figure <ref type="figure">3</ref> that a) our robust trained model can provide universal defense to CE-PGD, CW-PGD and Greedy attacks; b) when increasing , the difference between both test accuracy and CE-PGD attack accuracy increases substantially, which also implies the robust model under larger is harder to obtain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cora Citeseer</head><p>A/natural model 18.2 ± 0.1 28.9 ± 0.1 A/robust model 18.1 ± 0.3 28.7 ± 0.4 A /natural model 28.0 ± 0.1 36.0 ± 0.2 A /robust model 22.0 ± 0.2 32.2 ± 0.4  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we first introduce an edge perturbation based topology attack framework that overcomes the difficulty of attacking discrete graph structure data from a first-order optimization perspective. Our extensive experiments show that with only a fraction of edges changed, we are able to compromise state-of-the-art graph neural networks model noticeably. Additionally, we propose an adversarial training framework to improve the robustness of GNN models based on our attack methods. Experiments on different datasets show that our method is able to improve the GNN model's robustness against both gradient based and greedy search based attack methods without classification performance drop on original graph. We believe that this paper provides potential means for theoretical study and improvement of the robustness of deep learning models on graph data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>, W; A, {xi}, yi),(20)    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>); see Proposition 2. Proposition 2 Given a general attack loss function f , problem (21) is equivalent to maximize (22) ≤ (20). Proof: By introducing epigraph variable p [Boyd and Vandenberghe, 2004], problem (21) can be rewritten as minimize W,p p subject to −f (s, W) ≤ p, ∀s ∈ S.(23)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: CE-PGD and CW-PGD attack losses on Cora and Citeseer datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: Robust training loss on Cora and Citeseer datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>studied test-time non-targeted adversarial attacks on both graph classification and node classification. Their work restricted the attacks to perform modifications on discrete structures, that is, an attacker is only allowed to add or delete edges from a graph to construct a new graph. White-box, practical black-box and restricted blackbox graph adversarial attack scenarios were studied. Authors in<ref type="bibr" target="#b8">[Zügner et al., 2018]</ref> considered both test-time (evasion) and training-time (data poisoning) attacks on node classification task. In contrast to<ref type="bibr" target="#b4">[Dai et al., 2018]</ref>, besides adding or removing edges in the graph, attackers in<ref type="bibr" target="#b8">[Zügner et al., 2018]</ref> may modify node attributes. They designed adversarial attacks based on a static surrogate model and evaluated their impact by training a classifier on the data modified by the attack. The resulting attack algorithm is for targeted attacks on single nodes. It was shown that small perturbations on the graph structure and node features are able to achieve misclassification of a target node. A data poisoning attack on unsupervised node representation learning, or node embeddings, has been proposed in</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Misclassification rates (%) under 5% perturbed edges</figDesc><table><row><cell></cell><cell></cell><cell>Cora</cell><cell>Citeseer</cell></row><row><cell></cell><cell>clean</cell><cell>18.2 ± 0.1</cell><cell>28.9 ± 0.3</cell></row><row><cell></cell><cell>DICE</cell><cell>18.9 ± 0.2</cell><cell>29.8 ± 0.4</cell></row><row><cell>fixed</cell><cell>Greedy</cell><cell>25.2 ± 0.2</cell><cell>34.6 ± 0.3</cell></row><row><cell>natural</cell><cell>Meta-Self</cell><cell>22.7 ± 0.3</cell><cell>31.2 ± 0.5</cell></row><row><cell>model</cell><cell>CE-PGD</cell><cell cols="2">28.0 ± 0.1 36.0 ± 0.2</cell></row><row><cell></cell><cell>CW-PGD</cell><cell cols="2">27.8 ± 0.4 37.1 ± 0.5</cell></row><row><cell></cell><cell>CE-min-max</cell><cell>26.4 ± 0.1</cell><cell>34.1 ± 0.3</cell></row><row><cell></cell><cell cols="2">CW-min-max 26.0 ± 0.3</cell><cell>34.7 ± 0.6</cell></row><row><cell>retrained</cell><cell>Meta-Self</cell><cell cols="2">29.6 ± 0.4 39.7 ± 0.3</cell></row><row><cell>model</cell><cell cols="3">CE-min-max 30.8 ± 0.2 37.5 ± 0.3</cell></row><row><cell>from (20)</cell><cell cols="2">CW-min-max 30.5 ± 0.5</cell><cell>39.6 ± 0.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Misclassification rates (%) of robust training (smaller is better for defense task) with at most 5% of edge perturbations. A means the natural graph, A means the generated adversarial graph under = 5%. X/M means the misclassification rate of using model M on graph X.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">in robust training (in %)</cell></row><row><cell></cell><cell>0</cell><cell>5</cell><cell>10</cell><cell>15</cell><cell>20</cell></row><row><cell>in attack (in %)</cell><cell cols="5">0 18.1 18.2 19.0 20.2 21.3 5 27.9 22.0 23.9 24.8 26.5 10 32.7 32.1 26.4 27.7 31.0 15 36.7 36.2 33.4 29.7 32.9</cell></row><row><cell></cell><cell cols="5">20 40.2 40.1 36.3 36.3 33.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Misclassification rates (%) of CE-PGD attack against robust training model versus (smaller is better) different (%) on Cora dataset. Here = 0 in training means natural model and = 0 in attack means unperturbed topology.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is supported by Air Force Research Laboratory FA8750-18-2-0058 and the MIT-IBM Watson AI Lab.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName><forename type="first">Günnemann</forename><surname>Bojcheski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojcheski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><forename type="middle">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vandenberghe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.01093</idno>
	</analytic>
	<monogr>
		<title level="m">Security and Privacy (SP), 2017 IEEE Symposium on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004">2018. 2018. 2004. 2004. 2017</date>
			<biblScope unit="page" from="39" to="57" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Adversarial attacks on node embeddings</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Audio adversarial examples: Targeted attacks on speech-to-text</title>
		<author>
			<persName><forename type="first">Wagner</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Security and Privacy Workshops (SPW)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Robust optimization for non-convex objectives</title>
		<author>
			<persName><forename type="first">Chen</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="4705" to="4714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Attacking visual language grounding with adversarial examples: A case study on neural image captioning</title>
		<author>
			<persName><forename type="first">Chen</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>AAAI</publisher>
			<date type="published" when="2018">2018a. 2018. 2018b. 2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2587" to="2597" />
		</imprint>
	</monogr>
	<note>EAD: elastic-net attacks to deep neural networks via adversarial examples</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semisupervised classification with graph convolutional networks</title>
		<author>
			<persName><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.02371</idno>
		<idno>arXiv:1609.02907</idno>
	</analytic>
	<monogr>
		<title level="m">Adversarial attack on graph structured data</title>
				<imprint>
			<publisher>Alexey Kurakin, Ian Goodfellow, and Samy Bengio</publisher>
			<date type="published" when="2015">2018. 2018. 2015. 2015. 2016. 2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sparsity-aware sensor collaboration for linear coherent estimation</title>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3509" to="3522" />
			<date type="published" when="2015">2015. 2015. 2016. 2016</date>
		</imprint>
	</monogr>
	<note>IEEE Transactions on Signal Processing</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Block alternating optimization for non-convex min-max problems: Algorithms and applications in signal processing and communications</title>
		<author>
			<persName><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06083</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<editor>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</editor>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2019a. 2019. 2019b. 2019. 2017. 2017. 2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>ICASSP</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hiding individuals and communities in a social network</title>
		<author>
			<persName><surname>Qian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
	</analytic>
	<monogr>
		<title level="m">Graph attention networks</title>
				<imprint>
			<date type="published" when="2008">2018. 2018. 2008. 2008. 2017. 2017. 2018. 2018</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">139</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>AI magazine</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Structured adversarial attack: Towards general implementation and better interpretability</title>
		<author>
			<persName><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.02057</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<publisher>Daniel Zügner and Stephan Günnemann</publisher>
			<date type="published" when="2018">2019a. 2019. 2019b. 2019. 2019c. 2019. 2018. 2018. 2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>ICLR</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
