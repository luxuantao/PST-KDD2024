<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Communication-Aware DNN Accelerator on ImageNet Using in-Memory Entry-Counting Based Algorithm-Circuit-Architecture Co-Design in 65nm CMOS</title>
				<funder ref="#_VdVkmeP #_qHXfmRE">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_u8PKnGP">
					<orgName type="full">Science and Technology Commission of Shanghai Municipality</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Student Member, IEEE</roleName><forename type="first">Haozhe</forename><surname>Zhu</surname></persName>
						</author>
						<author role="corresp">
							<persName><roleName>Member, IEEE</roleName><forename type="first">Chixiao</forename><surname>Chen</surname></persName>
							<email>cxchen@fudan.edu.cn</email>
						</author>
						<author>
							<persName><roleName>Student Member, IEEE</roleName><forename type="first">Shiwei</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Qiaosha</forename><surname>Zou</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mingyu</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Lihua</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Xiaoyang</forename><surname>Zeng</surname></persName>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">C.-J</forename><forename type="middle">Richard</forename><surname>Shi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">C</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">China</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory of ASIC and System</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<region>PR</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Shanghai Engineering Research Center for AI and Robotics</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country>PR China. Q. Zou</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">institute of Brain-Inspired Circuits and Systems</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Communication-Aware DNN Accelerator on ImageNet Using in-Memory Entry-Counting Based Algorithm-Circuit-Architecture Co-Design in 65nm CMOS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/JETCAS.2020.3014920</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/JETCAS.2020.3014920, IEEE Journal on Emerging and Selected Topics in Circuits and Systems Manuscript received May X, 2020; revised July X, 2020.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>neural network accelerator</term>
					<term>processing-inmemory</term>
					<term>dataflow</term>
					<term>CAM</term>
					<term>algorithm-circuit codesign</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a communication-aware processing-in-memory deep neural network accelerator, which implements an in-memory entry-counting scheme for low bitwidth quantized multiplication-and-accumulations (MACs). To maintain good accuracy on ImageNet, the proposed design adopts a full-stack co-design methodology, from algorithms, circuits to architectures. In the algorithm level, an entry-counting based MAC is proposed to fit the learned step-sized quantization scheme, and exploit the sparsity of both activations and weights intrinsically. In the circuit level, content addressable memory cells and multiplexed arrays are developed in the processing-inmemory macro. In the architecture level, the proposed design is compatible with different stationary dataflow mappings, further reducing the memory access. An in-memory entry-counting silicon prototype and its entire peripheral circuits are fabricated in 65nm LP CMOS technology with an active area of 0.76?0.66 mm 2 . The 7.36-Kb processing-in-memory macro with 128 search entries can reduce the multiplication number by 12.8?. The peak throughput is 3.58 GOPS, achieved at a clock rate of 143MHz and a power supply of 1.23V. The peak energy efficiency of the processing-in-memory macro is 11.6 TOPS/W, achieved at a clock rate of 40MHz and a power supply of 1.01V. Note that the physical design of the entry-counting memory is completed in a standard digital placement and routing flow by augmenting the library with two dedicated memory cells. A 3-bit quantized ResNet-18 on the ImageNet dataset is performed, where the top-1 accuracy is 64.4%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>T HE success of artificial intelligence drives application- specific circuit design for energy-efficient acceleration of deep neural network (DNN) computation. The massive data movement in DNNs makes the memory wall challenge increasingly prominent. To narrow the communication gap, high-efficiency computing architectures are transformed from classical Von-Neumann architectures <ref type="bibr" target="#b0">[1]</ref> into new solutions. Integrating computing logic inside memory dies, including inmemory and near-memory architectures, is considered to have the potential to break the memory wall, as depicted in Fig. <ref type="figure" target="#fig_0">1</ref>. The idea of tightly coupling memory and processing units first spawned processing-near-memory systems that fabricated processors in DRAM chips, dating back to 1990s <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. Furthermore, processing-in-memory (PIM) architectures <ref type="bibr" target="#b3">[4]</ref> and silicon prototypes <ref type="bibr" target="#b4">[5]</ref> stand out recently due to their ultra good energy efficiency.</p><p>Utilizing analog/mixed-signal computing, PIM chips have already achieved 10-100x energy efficiency improvement compared with CPU architectures or reconfigurable DNN accelerators. Capacitor array-based charge sharing techniques were reported for always-on image classification tasks in <ref type="bibr" target="#b5">[6]</ref>. CMOS-inverter based resistive ladders were adopted in <ref type="bibr" target="#b6">[7]</ref> to perform signed accumulation. In addition to the voltage domain, time domain accumulation also can be realized by delay line based pulse width modulation <ref type="bibr" target="#b7">[8]</ref>. The idea of inmemory actually means replacing traditional digital computing circuits with regular placed analog ones, closely coupling to memory cells and likely occupying the same layout area but different devices <ref type="bibr" target="#b8">[9]</ref>.</p><p>Though featuring high efficiency, state-of-the-art PIM chips suffer from two issues. The first is the accuracy issue for complicated vision tasks, such as classification on ImageNet <ref type="bibr" target="#b9">[10]</ref>. In analog-computing circuits, the dynamic range is usually limited by the thermal noise. In order to overcome the dynamic range barrier, a higher power supply voltage and/or larger capacitors are required, incurring more power consumption. Therefore, their advantages of energy efficiency only maintain when the signal precision is less than 6 bits <ref type="bibr" target="#b10">[11]</ref>. However, advanced algorithms on ImageNet, such as ResNet <ref type="bibr" target="#b11">[12]</ref>, require 8-16 bit precision, especially for intermediate partial sums.</p><p>It is not easy to overcome such a precision gap. Moreover, non-ideal devices due to process/voltage/temperature (PVT) variation deviate circuit functions from the behavior models, further degrading the reliability of analog computing. As an alternative, hybrid analog-digital computing circuits <ref type="bibr" target="#b12">[13]</ref> was proposed to break the limit.</p><p>The second issue is with respect to communication awareness. DNN is one of the most data-intensive applications, whose performance is highly sensitive to data communication bandwidth and memory access latency. Reconfigurable <ref type="bibr" target="#b13">[14]</ref> and sparsity-aware <ref type="bibr" target="#b14">[15]</ref> architectures were addressed in digital DNN accelerators. But it is not promising to transfer these techniques into PIM architectures. LeCun, a prominent scholar in DNN algorithms and a pioneer of DNN hardware, attributed this phenomenon to two factors <ref type="bibr" target="#b15">[16]</ref>. One is lossy conversions from and to digital due to analog computing, and the other is lack of hardware multiplexing, which is the key in dataflowenhanced architectures. The nature of these factors is the failure to precisely copy an analog signal in an efficient way.</p><p>Therefore, it is desired to investigate new communicationaware schemes for PIM architectures and chips. A systemlevel in-memory computing prototype, capable of speech recognition, was recently reported in <ref type="bibr" target="#b16">[17]</ref>. It utilized multiple PIM macros. The precision issue was avoided through binary quantization on the network, while the communication issue was circumvented by an output stationary dataflow. However, both techniques are not feasible when porting to larger-scale networks.</p><p>In this paper, the authors target a communication-aware and reliable solution using PIM circuits for ImageNet-level tasks. It turns out that such a solution requires a full-stack co-design, from algorithms, circuits to architectures. We proposed a complete design methodology to cover each step in the full stack. In addition, a silicon prototype is also designed and fabricated in 65nm CMOS technology to verify its effectiveness. The proposed design methodology features:</p><p>1) Algorithm-level co-design: investigate low-bitwidth quantization schemes for DNNs, where memory accesses can be reduced from full precision to quantized bitwidth. We propose a learned step-size quantized inmemory entry-counting scheme, which has great potentials to reduce the multiplication number by 10 1 ?10 3 x. 2) Circuit-level co-design: develop an in-memory entrycounting macro for memory arrays, where an all-digital MAC implementation is embedded. Its results are almost lossless compared with full-precision results. 3) Architecture-level co-design: build a complete PIMbased DNN accelerator for ResNet, where efficient and flexible data movement is available. We propose a multiplexed hardware system and its tensorization strategy to perform communication-aware dataflow mappings.</p><p>This paper is organized as follows. Section II briefly reviews quantization schemes from the DNN algorithm community and demonstrates the proposed learned step-size quantized in-memory counting scheme. PIM generator circuit design is introduced in Section III. Sections IV and V present the overall architecture and dataflow mapping details of the proposed PIM DNN deployment on the resource-constrained hardware of mobile devices is attracting much attention. Originally released DNN algorithms normally employ a floating-point format for both computing and memory access. Unfortunately, this format brings high bandwidth for data communication and great computational power for data processing. For example, a ResNet-50 model <ref type="bibr" target="#b11">[12]</ref> with 98M byte weights requires 3.8G floating-point MACs to complete one image. Such a scale limits DNNs' operating on edge scenarios.</p><p>Quantization is one of the most commonly used methods to compress DNNs. The key point of quantization is to represent weights and/or activations of the model with binary codes of a limited bitwidth, not only reducing the model size but also simplifying arithmetical hardware. The main challenge is to minimize the degradation after quantization.</p><p>Early works on neural network quantization, such as <ref type="bibr" target="#b17">[18]</ref>, focused more on convergence and performance of fixed-point training process when using quantization. However, explorations of new hardware architectures, such as in-memory computing, have prompted more attention to extremely lowbitwidth quantization. Binary/ternary quantization and binary weight quantization appeared first <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>. However, the accuracy loss on ImageNet is significant. Recently, algorithm researchers have turned more attention to 2?4-bit quantization models <ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref>, which have the potentials to bring its inference accuracy close to the original floating-point baseline.</p><p>These quantization schemes have many advantages. First, they significantly reduce model sizes, alleviating the communication bottleneck, also known as the memory wall. In addition, there are more efficient implementation choices for the MAC hardware due to the low precision, such as analog computing.</p><p>We classify these quantization schemes into two categories: 1) Rounding Quantization: : It converts the floating-point parameters into fixed-point ones by rounding them to the nearest integers. The low-bitwidth numbers after quantization maintain a uniform distribution, thus accommodating standard integer arithmetic hardware. 2) Codebook Quantization: : It performs a nonuniform number mapping for the floating-point parameters, normally using an indexed dictionary, also known as codebook. Though achieving higher accuracy, codebook-based quantization requires specific arithmetic implementation such as look-uptable, compared to the rounding quantization. Moreover, the communication cost of codebook should also be concerned.</p><p>Table <ref type="table">I</ref> summarizes state-of-the-art DNN quantization frameworks, demonstrating the effectiveness of different quantized ResNet-18 models. It can be found that the learned stepsize based 3-bit quantization (LSQ) <ref type="bibr" target="#b20">[21]</ref> on both activations and weights achieves the same accuracy as the 32-bit floatingpoint baseline. It belongs to the rounding quantization category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Learned Step-sized Quantization Scheme</head><p>In this paper, we adopt the LSQ-based scheme, depicted in Fig. <ref type="figure" target="#fig_1">2</ref> and detailed as follows. Since the original paper does not provide the LSQ-Net code, we open source an unofficial one via https://github.com/zhutmost/lsq-net.</p><p>In an N -bit quantized network, the activations and weights are N -bit signed integer numbers. Their numerical range are limited by [-2 N -1 , 2 N -1 -1]. For low-bitwidth quantization, i.e., N ? {2, 3, 4}, both number's presenting range and resolution are remarkably reduced. In order to map the quantized codes to the original presenting numbers, learned step-size parameters are introduced in LSQ as scaling factors, s w and s a for weights and activations respectively. Note that these factors differ layer by layer in a well-trained DNN. The input weight and activation vectors with a length of 8 are listed in the table on the left. There are only three nonzero computing patterns among them, and the number of occurrences of each pattern is counted in the table on the right. Each effective pattern corresponds to a multiplication term in the resulting expression. In this example, the dot product result 3 is obtained by only three multiplications.</p><p>After the range scaling, the quantization satisfies the following mapping rule,</p><formula xml:id="formula_0">x q = Round[ Clip( x o s , R min , R max ) ]<label>(1)</label></formula><p>where x o is the original weight/activation to be quantized, x q is the N -bit quantized code, and s stands for the step-size learned in the training process. There are two functions in (1). One is the clip function, Clip(z, R min , R max ). It clamps a real number z in the range [R min , R max ]. If z exceeds the range, the function result will be saturated as either R min or R max .</p><p>For N -bit weights, R min and R max are chosen as -2 N -1 + 1 and 2 N -1 -1 for symmetry, respectively. For N -bit activations, R min and R max are 0 and 2 N -1 -1, respectively, provided that ReLU is the activation function and always produces nonnegative numbers. The other function, Round(z), performs common rounding operations. At this point, the inputs of tensor computing blocks, convolution layers and fully connected layers, are all lowbitwidth numbers. Related hardware implementation is greatly simplified by the quantization scheme.</p><p>When the tensor computing is completed, the results of convolution and full connection need to be rescaled by the step-size product s a ? s w . As many DNNs require, batch normalization and ReLU activation are involved afterwards. Then, the quantization of the next layer begins. For hardware implementation, a linear transfer function, k ? x + b, can be performed to include all these operations with two parameters only. Note that the weights are quantized offline and loaded from the off-chip memory, while only the activations need to be quantized online in real time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Entry-Counting based MAC</head><p>An entry-counting operator is proposed in this paper to further reduce the hardware cost. Typical convolutional DNNs for ImageNet, such as AlexNet/VGG/ResNet, often require millions of MACs in one layer. Therefore, an array of hundreds of multipliers is equipped to explore such parallelism. In addition to hungry power consumption, data communication between multipliers and memories often result in utilization bottlenecks for real deployment.</p><p>Entry-counting based MACs can perform the equivalent arithmetic results as standard MACs, but only use about ten multiplications. Its principle is exemplified in Fig. <ref type="figure" target="#fig_2">3</ref>. In a typical 3?3 convolution layer with 16?512 input channels, each output activation requires 10 2 ?10 4 multiplications. However, in a 3-bit quantized MAC, there are merely nine unsigned possible multiplication patterns in total. In other words, there are only nine possible multiplication results in the MAC of each output activation, even though the total multiplication number is up to 10 2 ?10 4 . Hence, instead of performing thousands of multiplications, the entry-counting scheme searches the weight/activation memory space, counts the occurring numbers of each multiplication pattern, and substitutes the products of patterns into the final MAC result, i.e.,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MAC Result =</head><formula xml:id="formula_1">Pattern P pat ? (N pos -N neg ),<label>(2)</label></formula><p>where P pat is the product for each pattern, N pos and N neg are the positive and negative occurring numbers. Since the multiplication patterns are determined by the binary encoding, they can be calculated offline, and stored in a reconfigurable lookup table before the convolution/full connection starts. Note that the entry-counting method can also support codebook based quantization schemes. The entry-counting scheme is intrinsically sparsity-aware in both activations and weights, and thus no power is dissipated by calculations with zero results. Figure <ref type="figure" target="#fig_3">4</ref> details the both weight and activation sparsity in a quantized network. It shows that 29.0% of the weights and 52.5% of the activations are zeros, and about 66% of multiplications have no contributions at all to the MAC results. The weight sparsity is caused by low-bit-width binary representation, i.e. all near-zero values are represented by zeros, while the activation one is due to both low bitwdith and ReLU functions. To exploit the sparsity of DNNs to improve energy efficiency, many architectures, such as <ref type="bibr" target="#b24">[25]</ref>, are proposed to detect and skip the multiplications associated with zeros. To locate the irregular appearance of zero elements in the matrices, an additional set of vectors are needed to store the indices or guard bits of nonzero elements <ref type="bibr" target="#b14">[15]</ref>. However, for the ultra-low-bitwidth PIM architectures with a bitwidth of only 2?4 bits, neither the index vectors nor the guard bits are feasible, because the communication and power cost of storing, decoding, and transmitting them may be higher than the data matrices themselves. Moreover, the zero-skipping logic might be more power-hungry than the energy efficient PIM circuits. In the proposed entry-counting MAC scheme, the intrinsic sparsity awareness does not demand additional zero-skipping logic.</p><p>The advantages of the entry-counting scheme are as follows. First, it saves most multiplication power consumption by reducing the multiplication number of an output activation, including nine pattern multiplications and one merged linear operation. Second, it improves the performance density by searching the entire memory in one shot. Third, it is aware of both weight/activation sparsity intrinsically and skips unnecessary multiplication, because the effective patterns only count nonzero multiplicand cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. CIRCUITS: IN-MEMORY ENTRY COUNTING</head><p>This section presents a dedicated in-memory circuit design to both store weights/activations and perform the entrycounting based MACs of the 3-bit quantized LSQ network.</p><p>As mentioned above, the entry-counting method has two steps. The first step is to search the entire memory space and output entries that both the activation and the weight match the pattern, and the second one is to count the positive and negative occurring number of the matched entries, i.e., N pos and N neg . To address these both steps, the proposed design adopts a content addressable memory (CAM, <ref type="bibr" target="#b25">[26]</ref>) based digital implementation instead of analog MAC circuits.</p><p>A CAM based in-memory entry-counting scheme is exemplified in Fig. <ref type="figure" target="#fig_4">5</ref>. The memory consists of three entries, each with five bits arranged horizontally, corresponding to a 2-bit activation and a 3-bit weight. Among the five bits, four of them are inside the search space, labeled as C-box. During operation, a search data state machine generates and broadcasts the nine possible patterns onto differential search line pairs (SL i ), while match lines (ML i ) monitor whether a hit or miss condition occurs for each pattern. The remaining bit, labeled as S-box, stands for the sign of the weight. Assuming that activations are always non-negative due to ReLU and do not need cells to store. If one entry is hit by the input pattern, a demultiplexer in the entry connects the hit signal to either a positive or negative counter according to the saved sign bit. Finally, the two counters, synthesized from RTL codes, output N pos and N neg respectively. Note that the bit lines and word lines to write memory bits are omitted for simplicity.</p><p>The two custom memory cells are demonstrated in Fig. <ref type="figure" target="#fig_5">6</ref>. The sign cell is the same as a typical 8T SRAM cell, whose cross-coupled latch stores a weight sign. The hit cell, used for CAM, is custom designed. There are two operation modes for hit cells, the write mode and the search mode. For easy routing, the search line and write bit line are shared in the memory array, labeled as WHBL. Thus, the hit cell consists of two parts: the upper 6T make up the write circuitry in the write mode, and the lower 4T facilitate an XOR logic in the search mode. The XOR function can be explained through the following example. Considering that the latch Q is set to 1 during the write mode, the right pass transistor is on while the left is off. Then the WHBL is switched to the search mode and broadcasts a signal of "0", and thus its complementary line is "1". At this point, the output node of the hit cell, HBL, passes a "1" through the right switch. The cell will generate a "0" if the input line is "1", and vice versa. Therefore, the XOR function is embedded in the 10T hit cell. In conventional NAND-type CAM cells <ref type="bibr" target="#b25">[26]</ref>, the pass transistors are NMOS only, making the high voltage level of the output node V DD -V th . A latch-based sense amplifier is normally required to recover the rail-torail voltage level. Compared to the conventional topology, two PMOS transistors are equipped parallel with the two NMOS transistors in our case, converting NMOS-only switches into CMOS transmission gates. The modification alleviates the sense amplifier design under the low voltage supply.</p><p>During the search mode, the patterns for entry counting The layout stick diagram of the two cells is also shown in Fig. <ref type="figure" target="#fig_5">6</ref>. Rather than purchasing ultra-high density of common SRAM patterns, the two layouts follow the digital standard cell patterns, where the top and bottom OD rails connect to voltage supplies and grounds, respectively. In addition, the N-well (for PMOS) region is placed above the P-sub (for NMOS) region. In this way, the two cells are compatible with the standard cell library. Hence, the entire in-memory entry-counting circuitry, together with other standard gates, can be automatically and agilely placed and routed through the standard digital tool flow, eliminating tedious and error-prone custom design efforts as required by other custom-circuit based PIM designs.</p><p>In addition, DNN accelerators should exploit data reuse to satisfy communication awareness. Figure <ref type="figure" target="#fig_6">7</ref> illustrates common data reuses in convolutional DNNs, including weight reuse, activation reuse, and convolutional reuse <ref type="bibr" target="#b26">[27]</ref>. Weight reuse is defined as that one weight kernel is convoluted on different activations of the same size belonging to one input feature map. Activation reuse means one feature map is convoluted by multiple weight kernels. The number of weight kernels determines the output channel number in each layer. The convolutional reuse occurs when the stride parameter in CNNs is 1 or 2. In a case of stride = 1, the activation window only updates 1/3 data for each horizontal sliding. The rest 2/3 data can be reused. The reuse implementation requires point-wise operations that each 1 ? 1 ? C tensor, called point-wise tensor (PWT), of the activation map is multiplied by all nine PWTs of a 3 ? 3 ? C weight and by all weight kernels.</p><p>A multiplexed structure can accommodate these operations, shown in Fig.  whereas the rest columns are set as don't care. On the basis of CAM, the unselecting operation can be regarded as setting X's in ternary CAM (TCAM), where the "X" represents a don'tcare bit in the search line. There are two examples, case 1 and 2, demonstrating different selectivity in Fig. <ref type="figure" target="#fig_8">8</ref>. Although the original TCAM cells consume 2x area than the CAM cells, the regular X's patterns allow us to utilize switches on the search lines, rather than taking additional area cost. For easy denotation, memory cells connecting to the same match line are referred to as a row in the memory macro.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. OVERALL ARCHITECTURE</head><p>A complete PIM-based DNN accelerator architecture is proposed for the ResNet benchmark. A top-level block diagram of the accelerator architecture is illustrated in Fig. <ref type="figure">9</ref>. It consists of an entry-counting PIM macro (ECM), a convolution unit (CU), a quantization unit (QU), and a controller. The controller is responsible for the entire process flow, which receives the instructions from the input interface, decodes them, and then generates configuration parameters. It also includes a pattern generator, producing cyclic unsigned patterns for the PIM macro and their corresponding product table for the CU.</p><p>The ECM contains 9 weight columns and 16 activation columns. Each column has 128 rows, corresponding to 128 input channels. It can be freely switched between the two modes, write and search, in one cycle. During the write mode, the macro updates its content with the received weights and activations from the input interface. During the search mode, one weight and one feature column are selected by the controller to perform the entry-counting MAC. In convolution layers, the exploitation of data reuse results in that each weight is likely to be multiplied by all activations in the same input channel. In other words, the weight and activation columns in ECM are required to be multiplexed so that reconfigurable dataflow can be applied.</p><p>The CU receives the ECM's outputs, and calculates the final convolution results. In ResNet, the input channels of a few convolution operations are less than 128, so the entry counting logic cannot be fully utilized when processing these operations. Thus, to improve the utilization of the ECM, there are two sets of adder trees and pattern MACs inside the CU. Each adder tree adds up the ECM hit outputs from 64 rows, and the pattern MAC multiplies the hit number with the pre-stored pattern product to update partial sums. For the convolution operations with more than 128 input channels, these two data paths can be merged as one, which adds up the 128 input channels at a time, and the redundant logic will be clock-gated to save power. A row aggregation block the partial sums with the intermediate results in the partial-sum scratchpad, and delivers the final 12-bit integer results to the QU.</p><p>The QU processes the merged linear operation and the ReLU activation function. It fuses all the linear operations between the convolution/full-connection and the quantization, including the batch normalization and the scaling/re-scaling operations, as mentioned in Section II.B. After the linear operation, the final QU results are rounded from the 12-bit sums into 2 bits and a positive sign.</p><p>Although the architecture can reduce the multiplication numbers of each output activation ideally to only ten to maximize the energy efficiency, the entry number in an entrycounting MAC, i.e. the row number of the ECM, is limited by the practical implementation. In the proposed accelerator, the ECM has 128 rows, so in an entry-counting iteration the accelerator can calculate a 128?128 MAC with only ten multiplications. In spite of the finite row number of the macro, it still reduces the number of multiplications in convolution layers by 12.8x, eliminating 92.2% multiplications in a network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. TENSORIZATION: POINT-WISE DATAFLOW MAPPING</head><p>Utilizing the data movement patterns is highly desired in communication-aware DNN accelerators. Memory access bandwidth can be remarkably reduced if data reuse in tensor primitives is partitioned appropriately. The method solving such an issue is known as tensorization or dataflows, whose details are discussed in <ref type="bibr" target="#b27">[28]</ref>. It is discovered that local storage and movement of partial sums are critical during tensorization, in addition to the input activations and weights. The partial sums of different input channels can be saved temporarily and then called at the appropriate time, while the input activations and weights are fixed. As a result, unnecessary and repeated loads of the same data can be eliminated, thereby <ref type="bibr" target="#b10">11</ref>.</p><p>Reconfigurable dataflow mapping method on ECM for a CONV3 2 1 layer in ResNet-18. The 3?3 weight kernel is convoluted on the 1?16 feature tile from the row N in the input feature map. The pattern search and entry counting are performed along the 128 input channels (iChan) in parallel, and three partial sums are generated, corresponding to the three rows of the weight kernel. These three partial sums are aggregated with the previous intermediate results in the scratchpad in a row-stationary way. making efficient use of the limited computation bandwidth. In practice, it is difficult to fully exhaust all the data reuse due to the limited on-chip storage and computing resources. Therefore, dataflow optimization methods targeting PE-array architectures have been developed to decrease data communication and improve energy efficiency <ref type="bibr" target="#b26">[27]</ref>. Moreover, the final optimal depends on both network hyper-parameters, including size, stride, and input/output channel numbers, as well as the hardware specifications, such as the number of PEs and the local scratchpad size.</p><p>Transferring these tensorization techniques from digital accelerators into PIM accelerators is not easy. Previous analogcomputing based PIM architectures are not effective in performing complicated dataflow optimization to reduce unnecessary data movement. Only output-stationary designs were reported <ref type="bibr" target="#b16">[17]</ref>. Because analog signals' replication, storage, and accumulation all require power and area hungry amplifiers.</p><p>To take advantage of these tensorization techniques, the codesign methodology proposes point-wise dataflow mapping among different ECM columns. Thanks to the in-memory entry-counting, the vector products between two point-wise tensors can be easily obtained, buffered, and indexed with a SRAM scratchpad. Therefore, the proposed architecture can Reconfigurable dataflow mapping method on ECM for a CONV2 2 1 layer in ResNet-18. For those convolutions with less than 128 channels, the upper and lower halves of the feature banks store the two adjacent feature rows with 64 channels. These two feature rows are convoluted with the same weight kernels, generating two sets of three partial sums. These two sets of sums are aggregated with the previous partial sums to produce two output feature rows. accommodate complicated dataflow optimization to exploit data reuse.</p><p>The execution flow of the accelerator is depicted in Fig. <ref type="figure" target="#fig_10">10</ref>. Point-wise row-stationary dataflow is supported inside a tile, while point-wise input and weight stationary can be implemented among different tiles. Note that the tile is defined as a set of activations and weights fitting the entire PIM macro. The three dataflows corresponds to the convolutional reuse, activation, and weight reuse, respectively. For further illustration, two typical dataflow mapping paradigms in ResNet-18 on the proposed architecture are exemplified as follows.</p><p>Figure <ref type="figure" target="#fig_0">11</ref> shows the dataflow mapping scheme for the CONV3 2 1 layer in ResNet-18, where the weight kernel size is 3?3?128 and the input feature size is 28?28?128. All the weight coefficients can be evenly filled into the 9?128 ECM weight bank, and each row of the weight bank stores an unfolded 3?3 kernel. The input features must be partitioned to fit the 16?128 ECM feature bank. The entire input feature map is split into multiple tiles whose sizes are 1?16?128 and each tile is filled into the ECM feature bank. One 128?128 MAC can be performed between one weight and one feature column in consecutive nine cycles. Each tile, part of one row of the entire input feature, interacts with all the three rows of the weights respectively. The intermediate partial sum results are accumulated and then stored into different addresses of the scratchpad. Due to the low-stride characteristic of convolutions, most point-wise activations in the feature bank are reused by 9 times and multiplied by each column of the weight bank. Afterward, for input/weight-stationary dataflow, the controller holds the content of the feature/weight bank, and fetches the next weight/feature tile from the off-chip memory. The choice of dataflow can be made according to the network characteristics. The data that is kept stationary in the ECM will be reused across as many tiles as possible until there is no room for any more intermediate results in the scratchpad.</p><p>The other mapping paradigm is the CONV2 2 1 layer in ResNet-18, which has 3?3?64 weight kernels, depicted in Fig. <ref type="figure" target="#fig_11">12</ref>. The key difference between such a convolution layer and the previous case is that it has only 64 input channels. To maintain the high efficiency, the upper/lower half of one column in the ECM weight bank stores two identical 1?1?64 point-wise data, while the feature bank is filled with two consecutive rows of a tile. In addition, the adder trees are split into two sets and the pattern MACs computes partial sums in two neighboring rows, respectively.</p><p>The convolution layers with more than 128 input channels, such as CONV4 1 2 and following layers in ResNet-18, are sliced along the input channel. In each iteration, the partial sums across 128 input channels are calculated by the pattern MAC, and then added to the partial sums of previous slices. All partial sums produced by the row aggregation unit are stored in the scratchpad until all the input channels are calculated.</p><p>To obtain the same normalized throughput as the proposed PIM architecture, conventional PE-array based accelerators need at least 128 MACs, while there are only two pattern MACs and one scaling MAC in our PIM accelerator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. MEASUREMENT RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Low-bitwidth Quantization Algorithm Results</head><p>The quantization algorithm based on LSQ is implemented in PyTorch <ref type="bibr" target="#b28">[29]</ref>. We evaluate the quantization framework on ResNet for the ImageNet classification dataset. The framework uses pre-trained ResNet models from the Model Zoo as a basis of the training procedure. The network architecture follows the model in the torchvision <ref type="bibr" target="#b29">[30]</ref>. We refactor the convolution functions in it. The LSQ quantizers of weights and activations are inserted before the built-in convolution function call. Note that the PyTorch does not support training models represented in fixed-point integers, so the low-bitwidth activations and weights are re-scaled to floating-point tensors before convolution operations. Because the convolution and re-scaling operations are both linear, this is mathematically equivalent to the quantization scheme described in Section II.B, and does not affect the training results. The first and last layers are not quantized to low-bitwidth, because the model accuracy is sensitive to the precision of these layers <ref type="bibr" target="#b22">[23]</ref>.</p><p>The design of the LSQ quantizer in our implementation follows Esser's practice <ref type="bibr" target="#b20">[21]</ref>, but there are some differences in Most of the previous low-bitwidth quantization schemes, including <ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref>, do not quantize these parameters into fixedpoint integers. Our experiments show that after these data are directly truncated to 12-bit fixed-point numbers, the accuracy of the model will drop by about 3?5%. The reason for such a severe decline is that the model's robustness to error bits after ultra-low-bitwidth quantization is not good enough. A single-bit error in a middle layer will cause dramatic changes in the final prediction result. For ResNet-18, it causes that top-1 accuracy falls to 64.4%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Silicon Prototype Results</head><p>To demonstrate the proposed methodology, a silicon prototype is fabricated in a 65nm CMOS LP technology. Figure <ref type="figure" target="#fig_12">13</ref> shows the die photo of the prototype, which covers an active area of 0.76?0.66 mm 2 . The PIM size is 7.36Kb, including 128 rows and 25 columns. The macro is generated by standard digital placement and routing tools, where custom-designed cells are placed regularly inside the macro as arrays, and automatically routed by the backend tools.</p><p>The prototype chip is measured with the following results. It can operate at a maximum clock rate of 143MHz, under a  power supply of 1.23V. The best power efficiency is achieved at 40MHz, when the power supply is 1.01V, consuming 417?W. The corresponding overall energy efficiency is 2.43 TOPS/W. The power consumption of the PIM macro in the write mode and the search mode is 96.0?W and 85.9?W, respectively. The power breakdown is shown in 14, which is obtained by simulation with a power supply of 1.1V and a clock frequency of 100MHz. Note that 79% of power is dissipated by synchronized digital circuits including synthesized clock trees and dataflow controllers. Therefore, the energy efficiency of the PIM macro is 11.6 TOPS/W. Table <ref type="table">II</ref> summaries the overall specifications and compares the prototype with other published multi-bit PIM accelerators and reconfigurable digital DNN accelerators. The proposed accelerator is the first PIM-based architectures to support a complicated neural network capable of running the ImageNet dataset, to the authors' best knowledge. Most previous PIMbased architectures are designed for a specific neural network model. Under the guidance of algorithm-circuit-architecture co-design, the proposed accelerator is optimized for general convolution/full-connection operations. Even after being manufactured, the accelerator still maintains flexible dataflow reconfigurability. The overall accelerator energy efficiency is 2.4 TOPS/W, including the PIM macro, the clock tree, the peripheral controller circuits and the I/O interface. The most</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Near-memory and in-memory accelerator architectures narrow the communication gap between the computing logic and the memory array.</figDesc><graphic url="image-1.png" coords="1,314.13,199.67,246.76,126.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The Learned Step-size Quantization Scheme [21].</figDesc><graphic url="image-2.png" coords="3,51.12,56.81,246.76,274.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig.3. An example of a 3-bit MAC operation with the entry-counting scheme. The input weight and activation vectors with a length of 8 are listed in the table on the left. There are only three nonzero computing patterns among them, and the number of occurrences of each pattern is counted in the table on the right. Each effective pattern corresponds to a multiplication term in the resulting expression. In this example, the dot product result 3 is obtained by only three multiplications.</figDesc><graphic url="image-3.png" coords="3,314.13,56.40,246.76,156.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The weight/activation sparsity of a 3-bit ResNet-18 model quantized with the LSQ-based method. The activation sparsity is a average of measurements on the 50,000 images from the ImageNet validation set.</figDesc><graphic url="image-4.png" coords="4,51.44,56.07,246.22,208.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. CAM based in-memory entry counting.</figDesc><graphic url="image-5.png" coords="4,311.74,56.79,254.33,115.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. The schematic and layout of an 8T sign cell and a 10T hit cell.</figDesc><graphic url="image-6.png" coords="5,61.79,56.11,225.91,152.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Data reuse in convolutional DNNs.</figDesc><graphic url="image-7.png" coords="5,309.98,53.74,252.30,112.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>8. Multiple activation and weight columns are devised into one multiplexed in-memory entry counting circuits. Each column corresponds to one point-wise tensor. During the write mode, all weights and activations are first stored into the memory. During the search mode, only one activation and weight column are activated simultaneously,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Multiplexed in-memory entry counting. During the search mode, one feature column and one weight column are selected to perform an entry-counting based MAC.</figDesc><graphic url="image-8.png" coords="6,59.24,55.21,493.53,185.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig.</head><label></label><figDesc>Fig. The system architecture of the proposed in-memory DNN accelerator.</figDesc><graphic url="image-9.png" coords="6,311.60,288.43,247.70,186.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. The execution flow efficiently exploits data reuse with the support of multiple point-wise dataflows.</figDesc><graphic url="image-10.png" coords="7,47.42,54.35,253.11,328.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12.Reconfigurable dataflow mapping method on ECM for a CONV2 2 1 layer in ResNet-18. For those convolutions with less than 128 channels, the upper and lower halves of the feature banks store the two adjacent feature rows with 64 channels. These two feature rows are convoluted with the same weight kernels, generating two sets of three partial sums. These two sets of sums are aggregated with the previous partial sums to produce two output feature rows.</figDesc><graphic url="image-12.png" coords="8,57.71,56.07,233.57,349.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Die photo of the prototype.</figDesc><graphic url="image-13.png" coords="9,93.22,56.82,162.75,178.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>1 Fig. 14 .</head><label>114</label><figDesc>Fig. 14. The power breakdown of the prototype (1.1V/100MHz).</figDesc><graphic url="image-14.png" coords="9,297.28,242.43,262.12,164.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. Verification system setup for demonstration.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Authorized licensed use limited to: Carleton University. Downloaded on August 09,2020 at 15:05:14 UTC from IEEE Xplore. Restrictions apply.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGEMENT</head><p>The authors would like to thank <rs type="person">Yunlong Wei</rs> of <rs type="affiliation">Fuzhou University</rs> for his help in designing the testing PCB. The authors are also grateful to <rs type="person">Di Hu</rs> of <rs type="affiliation">OPPO Inc.</rs> and other <rs type="person">GitHub users</rs> for their contributions to the open-source repository.</p></div>
			</div>
			<div type="funding">
<div><p>This work was supported by the <rs type="funder">National Natural Science Foundation of China</rs> under Grants <rs type="grantNumber">61974033</rs> and <rs type="grantNumber">61702459</rs>, and the <rs type="funder">Science and Technology Commission of Shanghai Municipality</rs> under Grant <rs type="grantNumber">2018SHZDZX01</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_VdVkmeP">
					<idno type="grant-number">61974033</idno>
				</org>
				<org type="funding" xml:id="_qHXfmRE">
					<idno type="grant-number">61702459</idno>
				</org>
				<org type="funding" xml:id="_u8PKnGP">
					<idno type="grant-number">2018SHZDZX01</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>2156-3357 (c) 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p><p>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: <ref type="bibr">DOI</ref>   TNAM <ref type="bibr" target="#b30">[31]</ref> Twin-8T <ref type="bibr" target="#b31">[32]</ref> Sandwich-RAM <ref type="bibr" target="#b7">[8]</ref> Wang et al. <ref type="bibr" target="#b32">[33]</ref> Thinker <ref type="bibr" target="#b13">[14]</ref> iFPNA <ref type="bibr" target="#b27">[28]</ref> This Work  power consuming part is the clock tree. The clock tree synthesis strategy is for high performance computing, even though the PIM computing circuits power is reduced remarkably. This strategy is not suitable for energy efficient PIM accelerators and needs to be optimized in the future. If only the PIM macro responsible for low-bitwidth matrix multiplication is considered, the core energy efficiency of 11.6 TOPS/W is competitive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. System Setup and Benchmark Results</head><p>A verification system to demonstrate ResNet-18 on the PIM prototype is illustrated in Fig. <ref type="figure">15</ref>. The system contains a Xilinx ZC706 Zynq development board and a prototype coreboard, which are connected through an FPGA Mezzanine Card (FMC) socket. A PYNQ framework is deployed on the Zynq SoC so that the embedded ARM micro-controllers can be interactively programmed in Python in an internet browser on the host PC. The ResNet-18 model is quantized offline and its parameters as well as the test images are stored in the SD card. In order to improve the transmission efficiency of the system bus, the activations and weights fetched from the SD-card are buffered with a block RAM based cache. All convolution layers, excluding the first one, are computed by the prototype chip, while the first convolution layer and the final full-connection layer are implemented on the FPGA. The latency breakdown is measured on the fly and shown in Table <ref type="table">III</ref>. The average utilization rate of the PIM computing logic is 81.1%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>A communication-aware PIM accelerator is proposed in this paper. It employs a full-stack co-design methodology to achieve communication awareness, from algorithms, circuits to architectures. The design features in-memory entrycounting based MAC, and achieves almost lossless accuracy on ImageNet by utilizing 3-bit quantization schemes. The communication awareness is enabled by flexible dataflow mappings. A silicon prototype with a 7.36Kb ECM macro is designed, fabricated, and measured to validate the architecture, where the final ResNet-18 accuracy is close to the original floating-point result. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><surname>Neumann</surname></persName>
		</author>
		<title level="m">IEEE Annals of the History of Computing</title>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="27" to="75" />
		</imprint>
	</monogr>
	<note>First Draft of a Report on the EDVAC</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A case for intelligent RAM</title>
		<author>
			<persName><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="34" to="44" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Active pages: A computation model for intelligent memory</title>
		<author>
			<persName><forename type="first">M</forename><surname>Oskin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 25th Annual International Symposium on Computer Architecture (Cat. No. 98CB36235)</title>
		<meeting>25th Annual International Symposium on Computer Architecture (Cat. No. 98CB36235)</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="192" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">ISAAC: A Convolutional Neural Network Accelerator with In-Situ Analog Arithmetic in Crossbars</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shafiee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM/IEEE 43rd Annual International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="14" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Conv-RAM: An energy-efficient SRAM with embedded convolution computation for low-power CNN-based machine learning applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Chandrakasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Solid State Circuits Conference (ISSCC)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="488" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An Always-On 3.8 ? J/86% CIFAR-10 mixed-signal binary CNN processor with all memory on chip in 28-nm CMOS</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bankman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Solid-State Circuits</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="158" to="172" />
			<date type="published" when="2019-01">Jan. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">XNOR-SRAM: In-Memory Computing SRAM Macro for Binary/Ternary Deep Neural Networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Solid-State Circuits</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sandwich-RAM: an energy-efficient inmemory BWN architecture with pulse-width modulation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Solid-State Circuits Conference (ISSCC)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="394" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A 64-Tile 2.4-Mb In-Memory-Computing CNN Accelerator Employing Charge-Domain Compute</title>
		<author>
			<persName><forename type="first">H</forename><surname>Valavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Solid-State Circuits</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1789" to="1799" />
			<date type="published" when="2019-06">June 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1409.0575" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Low-Power Analog Processing for Sensing Applications: Low-Frequency Harmonic Signal Classification</title>
		<author>
			<persName><forename type="first">W</forename><surname>Daniel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="9604" to="9623" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1512.03385" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A 28nm 64Kb 6T SRAM computingin-memory macro with 8b MAC operation for AI edge chips</title>
		<author>
			<persName><forename type="first">X</forename><surname>Si</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Solid-State Circuits Conference (ISSCC)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="246" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A high energy efficient reconfigurable hybrid neural network processor for deep learning applications</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Solid-State Circuits</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="968" to="982" />
			<date type="published" when="2018-04">April 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">STICKER: An energy-efficient multi-Sparsity compatible accelerator for convolutional neural networks in 65-nm CMOS</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Solid-State Circuits</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="465" to="477" />
			<date type="published" when="2020-02">Feb. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep learning past, present, and future</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Solid-State Circuits Conference (ISSCC)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="12" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A 5.1pJ/Neuron 127.3us/Infererence RNN-based Speech Recognition Processor using 16 Computing-in-Memory SRAM Macros in 65nm CMOS</title>
		<author>
			<persName><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium VLSI Circuits</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="120" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep learning with limited numerical precision</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1737" to="1746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="525" to="542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Ternary weight networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1605.04711" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Online] Available</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learned Step Size Quantization</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Esser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations 2020 (ICLR)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">LQ-Nets: Learned Quantization for Highly Accurate and Compact Deep Neural Networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="365" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">PACT: Parameterized clipping activation for quantized neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1805.06085" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding</title>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1510.00149" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">SCNN: An Accelerator for Compressed-sparse Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Parashar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="27" to="40" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Contentaddressable memory (CAM) circuits and architectures: a tutorial and survey</title>
		<author>
			<persName><forename type="first">K</forename><surname>Pagiamtzis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sheikholeslami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Solid-State Circuits</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="712" to="727" />
			<date type="published" when="2006-03">March 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficient Processing of Deep Neural Networks: A Tutorial and Survey</title>
		<author>
			<persName><forename type="first">V</forename><surname>Sze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2017-12">December 2017</date>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page" from="2295" to="2329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">iFPNA: A Flexible and Efficient Deep Learning Processor in 28-nm CMOS Using a Domain-Specific Instruction Set and Reconfigurable Fabric</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal on Emerging and Selected Topics in Circuits and Systems</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="346" to="357" />
			<date type="published" when="2019-06">June 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">PyTorch: An Imperative Style, High-Performance Deep Learning Library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Torchvision the Machine-Vision Package of torch</title>
		<author>
			<persName><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rodriguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM International Conference on Multimedia</title>
		<meeting>the 18th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2010-10">October 2010</date>
			<biblScope unit="page" from="1485" to="1488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A Ternary Based Bit Scalable, 8.80 TOPS/W CNN accelerator with Many-core Processing-inmemory Architecture with 896K synapses/mm 2</title>
		<author>
			<persName><forename type="first">S</forename><surname>Okumura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symp. VLSI Circuits</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="248" to="C249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A Twin-8T SRAM Computation-In-Memory Macro for Multiple-Bit CNN-Based Machine Learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Si</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Solid State Circuits Conf. (ISSCC 2019)</title>
		<meeting><address><addrLine>San Francisco</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="396" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A Compute SRAM with Bit-Serial Integer/Floating-Point Operations for Programmable In-Memory Vector Acceleration</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Solid State Circuits Conf. (ISSCC 2019)</title>
		<meeting><address><addrLine>San Francisco</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="224" to="226" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
