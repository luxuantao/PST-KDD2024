<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Under review as a conference paper at ICLR 2021 PRE-TRAINING TEXT-TO-TEXT TRANSFORMERS FOR CONCEPT-CENTRIC COMMON SENSE</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<title level="a" type="main">Under review as a conference paper at ICLR 2021 PRE-TRAINING TEXT-TO-TEXT TRANSFORMERS FOR CONCEPT-CENTRIC COMMON SENSE</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Rahul fights him</term>
					<term>stops his performance</term>
					<term>and drives to a local bar. generative common sense Text-to-Text Transformer Text-to-Text Transformer</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Pre-trained language models (PTLM) have achieved impressive results in a range of natural language understanding (NLU) and generation (NLG) tasks. However, current pre-training objectives such as masked token prediction (for BERT-style PTLMs) and masked span infilling (for T5-style PTLMs) do not explicitly model the relational commonsense knowledge about everyday concepts, which is crucial to many downstream tasks that need common sense to understand or generate. To augment PTLMs with concept-centric commonsense knowledge, in this paper, we propose both generative and contrastive objectives for learning common sense from the text, and use them as intermediate self-supervised learning tasks for incrementally pre-training PTLMs (before task-specific fine-tuning on downstream datasets). Furthermore, we develop a joint pre-training framework to unify generative and contrastive objectives so that they can mutually reinforce each other. Extensive experimental results show that our method, concept-aware language model (CALM) 1 , can pack more commonsense knowledge into the parameters of a pre-trained text-to-text transformer without relying on external knowledge graphs, yielding better performance on both NLU and NLG tasks. We show that while only incrementally pre-trained on a relatively small corpus for a few steps, CALM outperforms baseline methods by a consistent margin and even comparable with some larger PTLMs, which suggests that CALM can serve as a general, "plug-and-play" method for improving the commonsense reasoning ability of a PTLM.</p><p>Under review as a conference paper at ICLR 2021</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concept-to-Sentence</head><p>Input: &lt;c2s&gt; Generate a sentence with the concepts: forward, Simpson, ignore, information, prosecutor Output: The information was forwarded to Simpson 's prosecutors, but it was ignored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concept Order Recovering</head><p>Input: &lt;cor&gt; Correct the order of the given sentence: Rahul stops him, fights his bar, and drives to a local performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Pre-trained language models (PLTMs) such as BERT <ref type="bibr" target="#b7">(Devlin et al., 2018)</ref> and T5 <ref type="bibr" target="#b31">(Raffel et al., 2019)</ref> have revolutionized the field of NLP, yielding impressive performance on various conventional natural language understanding (NLU) and generation (NLG) tasks. BERT and its novel variants such as RoBERTa <ref type="bibr" target="#b19">(Liu et al., 2019)</ref> and ALBERT <ref type="bibr" target="#b13">(Lan et al., 2019)</ref> capture syntactical and semantic knowledge mainly from the pre-training task of masked language modeling, while T5-style models such as BART <ref type="bibr" target="#b14">(Lewis et al., 2019)</ref> instead focus on masked span infilling tasks. Though yielding better performance on many downstream tasks, these pre-training objectives, however, do not explicitly guide the models to reason with concept-centric commonsense knowledge from language, including the relation and composition of daily concepts in our lives. This leaves room for equipping current PTLMs with richer commonsense reasoning ability.</p><p>For example, consider a multi-choice question "What do you fill with ink to write notes on a piece of copy paper? (A) fountain pen (B) pencil case (C) printer (D) notepad". The current state-of-the-art question answering model, UnifiedQA <ref type="bibr" target="#b12">(Khashabi et al., 2020)</ref>, which was fine-tuned on T5-large with multiple datasets, still predicts '(C) printer' as its answer. The model may be overly sensitive to the co-occurrence between phrases in question sentence like 'ink' and 'copy paper' and the answer choice 'printer', but fails to reason with the concept-centric knowledge that 'fountain pen' is a writing instrument that needs to be filled with 'ink'. Such mistake in commonsense reasoning becomes a bottleneck for current PTLMs <ref type="bibr">(Davis &amp; Marcus, 2015)</ref>. Towards augmenting PTLMs with more knowledge, prior works mainly focus on training larger models <ref type="bibr" target="#b4">(Brown et al., 2020)</ref>, adding specific architectures to exploit external knowledge <ref type="bibr" target="#b26">(Peters et al., 2019)</ref>, or incorporating knowledge bases for pre-training <ref type="bibr" target="#b42">(Xiong et al., 2020)</ref>. In this paper, we instead look to explicitly teach pre-trained models to write and reason with common concepts through novel pre-training strategies.</p><p>We present two kinds of self-supervised pre-training tasks: concept-to-sentence generation (C2S) and concept order recovering (COR). C2S trains the pre-trained model to compose ("write") sentences given a set of concepts, and expects the generated sentences to be fluent and plausible in terms of commonsense. COR aims to teach models to detect and revise a corrupted sentence with incorrect ordering of concepts. As illustrated in Figure <ref type="figure">1</ref>, both tasks require a pre-trained model to recall relevant commonsense facts about the concepts and to understand the underlying commonsense relations between them. Both of the proposed objectives can explicitly encourage the model to capture the relational concept-centric commonsense knowledge and perform compositional reasoning.</p><p>Specifically, we need a generative pre-training objective to encourage models to capture this generative commonsense reasoning ability, so that models can learn to generate sentences with commonsense knowledge for both C2S and COR. Also, to teach modes to distinguish truth sentences from less plausible ones, we need to teach models with discriminative commonsense through contrastive self-training. To unify both generative and contrastive objectives within a joint learning framework so that the model can learn both generative and discriminative commonsense knowledge at the same time, we propose to use the sentences generated by the model itself as the distractors and train the model to distinguish the generated sentences from real sentences. In this way, the model is forced to acquire new commonsense knowledge in order to distinguish the distractors generated by itself, which probably exploit the knowledge the model already possesses. Therefore, the model is trained to iteratively improve upon itself in a self-play fashion. We share all the parameters between the generator (trained with the generative objective) and the discriminator (trained with the contrastive objective), then train multiple objectives with different prefixes. Compared to previous works <ref type="bibr" target="#b26">(Peters et al., 2019;</ref><ref type="bibr" target="#b8">Li et al., 2019;</ref><ref type="bibr" target="#b42">Xiong et al., 2020)</ref> that utilize external knowledge bases like Wikidata or ConceptNet, our approach can directly improve the generative and discriminative commonsense reasoning ability of PTLMs at the same time without relying on external knowledge bases.</p><p>To evaluate the effectiveness of our proposed method, we apply our method in an intermediate-task transfer learning setting <ref type="bibr" target="#b29">(Pruksachatkun et al., 2020)</ref> based on the pre-trained T5-base model to train a Concept-Aware Language Model (CALM). While only continually pre-trained on a small dataset for a relatively fewer number of updates (compared to conventional pre-training), CALM consistently outperforms T5-base on four commonsense-related NLU datasets (i.e., COMMONSENSEQA, OPEN-BOOKQA, PIQA, and ANLI) and COMMONGEN, a commonsense-related NLG dataset. Our results and careful ablation studies demonstrate the potential of our method to serve as a "plug-and-play" method for any pre-trained text-to-text transformer before fine-tuning on commonsense-related tasks. To the best of our knowledge, our work is the first to investigate concept-centric self-supervised objectives that improve both generative and discriminative commonsense reasoning ability of a pre-trained language model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">SELF-SUPERVISED OBJECTIVES FOR CONCEPT-CENTRIC LEARNING</head><p>In this section, we first describe the proposed generative and contrastive objectives used for improving the commonsense reasoning ability of pre-trained text-to-text transformers. Then, we introduce the joint learning framework which unifies the proposed self-supervised objectives and learn a unified text-to-text transformer based on pre-trained models such as T5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">GENERATIVE OBJECTIVES</head><p>Similar to many other pre-training tasks such as masked language modeling, we aim to teach models to recover original sentences from corrupted inputs, which is often regarded as a denoising process. We propose two generative self-supervised pre-training objectives: concept-to-sentence generation (C2S) and concept order recovering (COR).</p><p>Concept Extraction. Given an input x = [x 1 , x 2 , . . . , x n ], we first conduct part-of-speech tagging with Spacy for the sentence and extract Verb, Noun, and Proper Nouns from the sentence to use as concepts 2 . Next, we form concept-sets C = [v 1 , v 2 , . . . , v p , n 1 , n 2 . . . , n q ] where v i and n i denotes Figure <ref type="figure">1</ref>: Two self-supervised pre-training objectives that teach text-to-text transformers with generative common sense: (1) Concept-to-Sentence Generation (C2S) pre-trains the model to recover the original sentence with a shuffled concept set, e.g., {forward, Simpson, ignore, information, prosecutor} → "The information was forwarded to Simpson's prosecutors, but it was ignored."</p><p>(2) Concept Order Recovering (COR), similarly, teaches the model to correct the mispositioned concepts in the original sentence. For example, the concepts (stops, fights, bar, drives, performance), are randomly reordered in the input, while the model should recover the original sentence. the i-th verb or noun/proper noun concept (token) in x. We denote C v and C n as the set of verb and noun/proper noun concepts respectively in C. (i.e.</p><formula xml:id="formula_0">C v = [v 1 , v 2 , . . . , v p ] and C n = [n 1 , n 2 , . . . , n q ].)</formula><p>Concept-to-Sentence Generation (C2S). The concept-to-sentence generation (C2S) objective requires the text-to-text transformer to recover the original sentence given only a few unordered keywords of the sentence. Specifically, given a sentence, we shuffle the extracted concept-set C to create the perturbed source sequence and train the model to generate the original sentence with a prefix (denoted as &lt;c2s&gt;) as described in Fig. <ref type="figure">1</ref>. Formally, the C2S objective can be formulated as:</p><formula xml:id="formula_1">L c2s = E n i=1 − log p(x i |&lt;c2s&gt;; PERMUTE(C); x 1:i−1 )<label>(1)</label></formula><p>where the PERMUTE() function randomly shuffle the concepts in the concept-set. This objective requires the model to construct an acceptable commonsense sentence by adhering to and reasoning over the commonsense relations between the given concepts. Therefore, relational commonsense knowledge is implicitly injected into the parameters of the model. The C2S objective is motivated by the task proposed in <ref type="bibr" target="#b17">Lin et al. (2020)</ref>. Compared to their work, the concept-set used in C2S covers more concepts such as named entities, while the original task only includes the concepts appearing in ConceptNet. We apply the task in a general domain and as a pre-training objective, instead of merely serving as an evaluation task.</p><p>Concept Order Recovering (COR). As for the concept order recovering (COR) objective, we shuffle the order of concept in a sentence and train the model to recover the original sentence. As illustrated in Figure <ref type="figure">1</ref>, given an input sentence "tree grows on the apple,", the models would shuffle the concepts including "tree", "grow", and "apple" to recover the original sentence "apple grows on the tree." The noise introduced by concept shuffling is different from that by traditional self-supervised objectives like mask language modeling and mask span prediction because the corrupted source sentences are in general complete (i.e., no tokens or spans are masked) and grammatically correct, while not acceptable in terms of commonsense because the order and relation between concepts are shuffled. By training the model to detect and correct the disorder of concepts in a sentence, the model is expected to acquire some relational commonsense knowledge like "apple generally grows on a tree" instead of "tree grows on an apple."</p><p>Formally, the COR objective can be formulated as:</p><formula xml:id="formula_2">L cor = E n i=1 − log p(x i |&lt;cor&gt;; CONCEPT-PERMUTE(x, C); x 1:i−1 ) , (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>where &lt;cor&gt; is the prefix for the COR objective illustrated in Figure <ref type="figure">1</ref>. The function CONCEPT-PERMUTE() permutes the order between concepts in the same category (i.e. noun or verb) in the sentence, which can be formally defined as:</p><formula xml:id="formula_4">CONCEPT-PERMUTE(x, C) = [x 1 , x 2 , . . . , x n ] where x i =    x i x i / ∈ C PERMUTE(C v )[j] x i = v j PERMUTE(C n )[j] x i = n j (3)</formula><p>Our proposed objectives require the model to capture the relational commonsense knowledge between concepts and perform relational (COR) and compositional (C2S) commonsense reasoning in order to successfully reconstruct the original sentence. Therefore, the model is encouraged to acquire conceptcentric commonsense knowledge more effectively. In contrast, conventional pre-training objectives like masked language modeling and masked span infilling mainly focus on general token-level co-occurrence patterns and thus are less effective for learning commonsense knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">CONTRASTIVE OBJECTIVE</head><p>Generative QA Input: &lt;cont&gt; Which sentence is correct?: options: 1. The increased number of male visitors inspired by the article raised security concerns 2. The increased article of male visitors raised by the number inspired security concerns</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output:</head><p>The increased number of male visitors inspired by the article raised security concerns  The contrastive objective encourages the pretrained model to distinguish the real sentence from a distractor sentence: a sentence that is similar to the real sentence, generally grammatically correct, but may not follow common sense. We expect it to improve the pre-trained model's discriminative commonsense reasoning ability so that the model's performance on commonsensereasoning-discriminative tasks, like Com-monsenseQA, can be improved. We formulate the contrastive objective as a Generative QA task: we take the concatenation of a prefix &lt;cont&gt; (question / context), the real sentence x (answer), and the distractor x (distractor) as the input and train the model to output the real sentence x. Formally, we have the loss function of the contrastive objective defined as:</p><formula xml:id="formula_5">L cont = E − log p(x|&lt;cont&gt;; PERMUTE(x; x )) ,<label>(4)</label></formula><p>where the prefix &lt;cont&gt; is described in Figure <ref type="figure" target="#fig_1">2</ref>. The distractor x is either constructed by concept shuffling as described previously (i.e. x = CONCEPT-PERMUTE(x, C)) when used independently, or generated by a generator trained with the aforementioned generative objectives when used in the joint training framework, which will be described in the next section. The aforementioned generative and contrastive selfsupervised objectives can be applied independently or simply combined in a multi-task learning fashion. We argue that these two objectives can mutually reinforce each other: the generated sentences from the generative objective can help the contrastive module learn to distinguish commonsense sentences from less plausible ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">JOINT TRAINING WITH GENERATIVE AND CONTRASTIVE OBJECTIVES</head><p>Therefore, we propose a joint training framework to unify generative objectives and contrastive objectives by using the generator to produce distractors for learning towards contrastive objective. Specifically, we have a generator G θ (trained with the generative objectives) and a discriminator D φ (trained with the contrastive objective). Given an input sentence x, we first use the method for either C2S or COR to produce the corrupted source sequence x . Then, we use the generator G θ trained  Given an input sentence x ("She was the first woman to hold the position."), we extract concept-set C (woman, hold, position). Given x and C, we produce corrupted source sequence x either for C2S and COR. The generator trained with the corresponding objective recovers sentences as distractors x to the discriminator. The discriminator is trained to distinguish truth sentences from randomly selected distractor among two objectives. Parameters between the generator and discriminator are shared.</p><p>with the corresponding objective to generate the recovered sentence x = G θ (x ). We then take x as the distractor to train the discriminator D φ with the contrastive objective. The loss function of the proposed joint training framework consists of two parts: the first part is the loss of generative objectives, which is identical to the loss described in Eq.( <ref type="formula" target="#formula_1">1</ref>) and Eq.( <ref type="formula" target="#formula_2">2</ref>) and is used to update the generator G θ . The second part is the loss of the contrastive objective as described in Eq.( <ref type="formula" target="#formula_5">4</ref>), which can be formulated as:</p><formula xml:id="formula_6">L cont_joint_c2s = E − log D φ (y|&lt;cont&gt;; x; G θ (&lt;c2s&gt;; PERMUTE(C))</formula><p>(5)</p><formula xml:id="formula_7">L cont_joint_cor = E − log D φ (y|&lt;cont&gt;; x; G θ (&lt;cor&gt;; CONCEPT-PERMUTE(x, C))<label>(6)</label></formula><p>where L cont_joint_c2s and L cont_joint_cor is the contrastive loss with the distractor generated with either the C2S or the COR objective and y is the original sentence. We then have the overall objective for the joint training framework defined as :</p><formula xml:id="formula_8">L joint = (L c2s + L cor ) + β(L cont_joint_c2s + L cont_joint_cor ).</formula><p>(7) L c2s and L cor are defined in Eq.(1) and Eq.( <ref type="formula" target="#formula_2">2</ref>) respectively and β is a hyperparameter controlling the relative weight between the generative and contrastive objectives. Note that since we would like to inject both generative and discriminative commonsense reasoning ability into the parameters of a single text-to-text transformer, we share the parameters between the generator G θ and the discriminator D φ .</p><p>Finally, we describe the overall procedure to apply the proposed self-supervised objectives and the joint training framework on a pre-trained text-to-text transformer. We apply a two-stage training strategy. During the first stage, we apply our proposed generative and contrastive objectives individually on the model in a multi-task learning fashion with different prefixes. This provides a good starting point for the second stage where the joint training framework is applied. We summarize the workflow of our method in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, motivated by the observation of <ref type="bibr" target="#b29">Pruksachatkun et al. (2020)</ref> that tasks requiring commonsense reasoning ability generally serve as good intermediate task, we test our method in the intermediate task transfer setting. Specifically, we initialize our model with T5-base, a pre-trained text-to-text transformer model, and training the model with our proposed method as intermediate task before fine-tuning and target downstream tasks. Another reason for adopting this setting is because we expect our method to serve as a "plug-and-play" method that can be applied to any pre-trained text-to-text transformer by simply continually training for a few steps.</p><p>Details for Pre-training and Fine-tuning CALM is continually pre-trained with our proposed self-supervised objectives as intermediate tasks based on the pre-trained T5-base model following the setting in <ref type="bibr" target="#b29">Pruksachatkun et al. (2020)</ref>. We randomly sample 500K sentences from the English Wikipedia corpus 3 , which is used for pre-training BERT and its variants, as the source dataset for our proposed self-supervised objectives which serve as intermediate tasks. We then fine-tune the CALM on each downstream task individually and report the average performance of three runs with different random seeds for fine-tuning on each dataset since the performance is sensitive to different random seeds. Training details and hyperparameter settings are presented in Appendix A.1 and A.2.</p><p>Datasets We consider five commonsense benchmark datasets as target tasks. We categorize these datasets into discriminative and generative tasks. Discriminative tasks are classification tasks while generative tasks are text generation tasks. We consider four datasets for discriminative task: Com-monsenseQA <ref type="bibr" target="#b35">(Talmor et al., 2018)</ref>, OpenbookQA <ref type="bibr" target="#b21">(Mihaylov et al., 2018)</ref>, PIQA <ref type="bibr" target="#b3">(Bisk et al., 2020)</ref>, aNLI <ref type="bibr" target="#b2">(Bhagavatula et al., 2019)</ref>   <ref type="formula">3</ref>) T5-base + SSM is continual pre-trained with a variant of the salient span masking objective <ref type="bibr" target="#b10">(Guu et al., 2020;</ref><ref type="bibr" target="#b32">Roberts et al., 2020)</ref> objective that masks text spans of concepts extracted with POS tagging instead of named entities extracted by a pre-trained NER model, which makes it more focused on concepts. ( <ref type="formula" target="#formula_5">4</ref>) CALM(Generative-Only) is continually pre-trained with the proposed generative objectives including concept-to-sentence generation(C2S) and concept order recovering(COR) as intermediate tasks. ( <ref type="formula">5</ref>) CALM(Contrastive-Only) is continually pre-trained with the proposed contrastive objective as described in section 2.2 using the distractor generated by concept shuffling. ( <ref type="formula" target="#formula_7">6</ref>) CALM(Mix-only) is continually pre-trained with both the generative objectives and the contrastive objective, combined with a multi-task learning fashion with identical weights for each objective as the intermediate task. ( <ref type="formula">7</ref>) CALM (w/o Mix warmup) is continually pre-trained with the joint training objective described in Eq (7) directly from the pre-trained T5-base model. ( <ref type="formula">8</ref>) CALM is our main model trained as described in Algorithm 1. The difference between CALM and CALM (Joint) is that the former is initialized by the CALM(Mix). We also include the performance of the BERT-base model and two knowledge enhanced PTLMs that have similar architecture to BERT-base.</p><p>Evaluation Metrics For discriminative tasks, we choose accuracy as our metric following other conventional question answering tasks. For generative tasks, we report automated metrics including BLEU <ref type="bibr" target="#b23">(Papineni et al., 2002)</ref>, METEOR <ref type="bibr" target="#b1">(Banerjee &amp; Lavie, 2005)</ref>, CIDEr <ref type="bibr" target="#b39">(Vedantam et al., 2015)</ref>, and SPICE <ref type="bibr" target="#b0">(Anderson et al., 2016)</ref> following the leaderboard of <ref type="bibr">COMMONGEN (Lin et al., 2020)</ref>. Results for COMMONGEN are on the test set and others are on the official development set. We tune the hyperparameters based on the models' performance on a in-house split dev set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">EXPERIMENTAL RESULTS</head><p>The result is presented in Table <ref type="table" target="#tab_1">1</ref>. First, we can see that our CALM model consistently and significantly (with p-value &lt; 0.01) outperforms the backbone T5-base model on all five datasets by a margin range from 1.5 to 2.9 accuracy on discriminative tasks and 1.5/0.6 BLEU/SPICE score on CommonGEN. This is an impressive result since we are only performing intermediate training on a relatively small dataset for only around 20k updates. It demonstrates the potential of our method for serving as a "plug-and-play" method for packing more commonsense knowledge into a pre-trained text-to-text transformer. Table 3 also shows that CALM performs comparably with several large-size PTLMs like BART, T5-large, and GPT-2 on the COMMONGEN dataset. The performance is worse than KG-BART <ref type="bibr" target="#b18">(Liu et al., 2020)</ref>, the current state-of-the-art on COMMONGEN, which is a contemporary work that exploits external knowledge bases as additional information, and is based on a larger backbone(i.e., BART <ref type="bibr" target="#b14">(Lewis et al., 2019)</ref>).</p><p>In addition, we can observe that both the proposed generative and contrastive objective outperforms the backbone T5-base model, as well as its variants that continually pre-trained with the original masked span prediction objective and the concept-specific salient span masking scheme, when applied independently. Note that we find the variant of salient span masking that focuses on concept is not very effective. We suspect this is because the resulting training data would be somewhat similar to the original text infilling objective because concepts are very common in the corpus and we only train for a few steps. The combination of the generative and contrastive objectives (i.e., CALM(Mix-only))  Above baselines are reported number in the leaderboard. T5-base(our implementation) uses different hyperparmeter setting than that reported in the leaderboard.</p><p>To further confirm the effectiveness of our approach, we also apply our method to continually pre-train T5-large with the same data and number of training steps.</p><p>We then compare the performance of the resulting model with that of the original T5-large model in Table <ref type="table" target="#tab_11">10</ref>. We find that both the proposed training objectives and the joint training framework consistently and significantly (with p-value &lt; 0.01) improve upon the original T5-large, showing our approach is effective for models with different sizes. Our model also outperforms BERT-large by a large margin. However, our model performs slightly worse compared to RoBERTa-large. We suspect this is because RoBERTa-large is optimized for more steps than T5-large and our CALM-large. This is also observed in many other tasks and datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">PERFORMANCE ANALYSIS Analysis on Generative objective</head><p>To investigate the contribution of each generative objective, we conduct an ablation study by continually pre-training three models from the same T5-base model with C2S, COR, and text infilling, which is the original objective for pre-training T5, as the objective for the intermediate task. We continually pre-train these models for the same number of steps and then evaluate their performance by fine-tuning on different target tasks. The result is shown in Table <ref type="table" target="#tab_4">4</ref>. We can see that both C2S and COR works better than the original masked span infilling objective on itself. This confirms the effectiveness of our proposed generative objectives on improving the commonsense reasoning ability of pre-trained text-to-text transformers.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance with fewer training examples</head><p>To investigate the effectiveness of our objective in the lowresource setting, we explore the performance of our model and baselines fine-tuning with different fractions of the training data. From Figure <ref type="figure" target="#fig_5">4</ref>, we can see that the performance improvement yielded by our models upon the T5-base model is more significant in the low-resource regime. This shows that CALM may already pack some commonsense knowledge in its parameters so that it does not require much data for fine-tuning before obtaining a good performance. In contrast, the original T5 model requires much data for fine-tuning, which suggests it may fail to encode much commonsense knowledge and must fit the correlation patterns in the downstream datasets to get a good performance.</p><p>Comparison of Generated Data Table <ref type="table">5</ref> shows the comparison of generated examples for the COMMONGEN test set between T5-base and CALM. We can see that the sentences generated by CALM are generally more acceptable in terms of commonsense plausibility while T5-base sometimes generates sentences that do not make sense.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concept-set T5-base CALM</head><p>Grass, Dog, Ball, Chase a dog is chased by a ball on the grass. dog chasing a ball in the grass. Net, Cast, Boat, Water fishing boat casts a net in the water. fisherman casts a net into the water from a fishing boat. Hole, Tree, Plant, Dig a man digs a hole in a tree to plant a new tree . he digs the man digging a hole to plant a tree. Ingredient, Add, Pan, Fry a pan filled with ingredients adds a touch of spice to the fry . add the ingredients to a pan and fry. Water, Hold, Hand, Walk A man holding a hand and walking in the water. A man is holding water. man holding a bottle of water in his hand as he walks down the street. Place, Use, Metal tool A man uses a metal tool to make a piece of metal. woman uses a metal tool to make a piece of jewelry. Hair, Wax, Apply, Remove remove the wax from your hair and apply it to your hair .</p><p>woman applies wax to her hair and then removes it with a comb. Sidewalk, Dog, Walk, Leash A dog walking on a leash on the sidewalk.</p><p>dog walking on a sidewalk with a leash.</p><p>Table <ref type="table">5</ref>: Comparison of generated sentences with same concept-set. For same concept-set which is from CommonGEN test set, we compare generated sentences between T5-base and CALM.</p><p>Knowledge Probing To investigate how much concept-centric knowledge our model pack, we conducted two probing methods with our model : Language Model Analysis (LAMA) probe <ref type="bibr" target="#b27">(Petroni et al., 2019)</ref>, Knowledge Intensive Language Task (KILT) <ref type="bibr" target="#b28">(Petroni et al., 2020)</ref>. We summarize the results on Table <ref type="table" target="#tab_5">6</ref> and Appendix A.4. We could find that our model outperforms the baseline. tasks compared with static word embeddings <ref type="bibr" target="#b22">Mikolov et al. (2013)</ref>; <ref type="bibr" target="#b24">Pennington et al. (2014)</ref>. More recently, large scale language models based on transformer architecture <ref type="bibr" target="#b38">(Vaswani et al., 2017)</ref> pretrained with either mask language modeling objective <ref type="bibr" target="#b7">(Devlin et al., 2018;</ref><ref type="bibr" target="#b19">Liu et al., 2019;</ref><ref type="bibr" target="#b13">Lan et al., 2019)</ref> or mask span infilling objective <ref type="bibr" target="#b14">(Lewis et al., 2019;</ref><ref type="bibr" target="#b31">Raffel et al., 2019)</ref> have been explored further advanced the state-of-the-art on multiple NLU and NLG tasks. Our method is based on these techniques and we focus on improving the commonsense reasoning ability of pre-trained text-to-text transformers. More recently, <ref type="bibr" target="#b5">Clark et al. (2020)</ref> propose a new self-supervised pre-training objective called Replaced Token Detection (RTD). RTD uses a mask language model like BERT to fill in the mask and train a discriminator to predict whether a token is generated or real. This pre-training paradigm is related to our proposed joint training framework. Some major differences include that (1) Our method employs sentence-level distractors that are in general grammatically correct but not in line with commonsense, thus require the model to perform relational commonsense reasoning while RTD is a token-level discrimination task and can often be solved with syntactic and shallow semantic knowledge <ref type="bibr" target="#b33">(Rosset et al., 2020)</ref>; (2) Our method unifies generative and contrastive objectives with one model, which can be applied to both NLU and NLG downstream tasks; and (3) The discriminator in our framework is "contrastive", takes both the real sentence and the distractor as input simultaneously. (2020) utilizes an external knowledge base to incorporate entity knowledge with PTLMs; however, these approaches require specialized resources like knowledge bases, which limits the domain they can be applied to. <ref type="bibr" target="#b42">Xiong et al. (2020)</ref> proposes WikiLM that encodes world knowledge into the parameters of a BERT <ref type="bibr" target="#b7">(Devlin et al., 2018)</ref>-like pre-trained model with a novel entity replacement detection objective that incorporates Wikipedia to form distractors. Their approach differs from ours because it requires an external knowledge base (i.e., Wikipedia) which limits the domain it can be applied, is limited to discriminative pre-training objectives and downstream tasks, and focuses on world knowledge instead of relational commonsense knowledge. More recently, <ref type="bibr" target="#b33">(Rosset et al., 2020)</ref> propose KALM, an entity-aware language model with more world knowledge packed into its parameters. Their method is restricted to the training of language models instead of masked language models or text-to-text transformers which can be used for more downstream tasks. Also, all the aforementioned work mainly focuses on world knowledge of named entities. In contrast, our work mainly focuses on commonsense knowledge about quotidian concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We propose novel self-supervised strategies that encourage the model to focus on concept-centric information that is related to commonsense understanding and reasoning instead of simple word coocurrence patterns so that the commonsense learning capability of pre-trained text-to-text transformers can be improved. Despite merely continually pre-trained on a small dataset with only around 20k steps, our CALM model consistently outperforms the T5-base model on all commonsense-related datasets, and even yields better performance compared with some larger size PTLMs on the COMMONGEN dataset. The performance gain is larger when we use fewer examples for fine-tuning on different downstream tasks, indicating that CALM effectively encodes more commonsense knowledge and rely less on fitting superficial patterns of datasets compared to traditional pre-trained language models. Our work suggests that text-to-text models can be pre-trained with better parameter and sample efficiency by carefully designed self-supervised objectives that focus more on the ability (e.g., commonsense reasoning ability) required by target tasks. As for future work, we plan to explore training with our method with larger text corpora for more steps, as well as the combination of our method and other methods that focus on injecting world knowledge into pre-trained language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX A.1 PRE-TRAINING DETAILS</head><p>The following details apply to both base architecture and joint-training architecture. We implement our pre-train models using Pytorch-lightning (Falcon, 2019) and Hugginface's Pytorch Transformers <ref type="bibr" target="#b41">(Wolf et al., 2019)</ref>. For pre-training phase, we use the Adam optimizer with maximum sequence length 256, train batch size 8, gradient accumulation 8, warmup steps 10000, weight decay 0.01 and adam epsilon 1e-6. We train the models with 8 V100 GPUs and FP32 precision for 17 hours. The model is pre-trained for at most 3 epochs to prevent overfitting. We searched for the best learning rate for our model out of [1e-4, 2e-5, 2e-6, 5e  • PIQA <ref type="bibr" target="#b3">(Bisk et al., 2020)</ref> is multiple-choice question answering task, which chooses the most appropriate solution for physical commonsense questions.</p><p>• aNLI <ref type="bibr" target="#b2">(Bhagavatula et al., 2019</ref>) is a binary-classification task, which picks the most plausible explanatory hypothesis given two observations from narrative contexts.</p><p>• CommonGEN (Lin et al., 2020) is a constrained text generation task, which generates a coherent sentence describing an everyday scenario using common concepts.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 KNOWLEDGE PROBING</head><p>LAMA probe is consisting of a set of knowledge sources, each comprised of a set of fact. It defines that a pre-trained language model knows a fact (subject, relation, object) such as (Bird, CapableOf, Fly) if it can predict masked objects in cloze statement such as "Birds can [MASK]". For evaluation, we first filtered out examples that mask label is not in vocabulary list of T5. Then, we evaluate the model based on how highly it ranks the ground truth token against every other word in a fixed vocabulary list of T5, and get mean precision at k to check whether the object is ranked among the top k results. We summarize the results of ConceptNet <ref type="bibr" target="#b34">(Speer &amp; Havasi, 2012)</ref> in Table <ref type="table" target="#tab_5">6</ref>. Unlike other language models which are optimised to masked word anywhere in a given sequence, T5 is trained with different denoising method. It might cause low performance on such slot filling task, but compared to T5, our model shows better performance compared to base model.</p><p>KILT task is a benchmark for assessing models that need to access specific knowledge in a defined snapshot of Wikipedia to solve tasks spanning five domains. The goal is to analyze the model whether it has task-agnostic representations of knowledge. We test our model on domain of fact checking, entity linking. Fact checking verifies textual claims against textual sources. For this task, we use FEVER <ref type="bibr" target="#b36">(Thorne et al., 2018)</ref> which is a large dataset for claim veracity that requires evidence from multiple Wikipedia pages to determine whether the claim is supported or refuted. Entity Linking assigns Wikipedia page to entities mentioned in text. We use AIDA CoNLL-YAGO (AY2) <ref type="bibr" target="#b11">(Hoffart et al., 2011)</ref> which supplements the CoNLL 2003 <ref type="bibr" target="#b37">(Tjong Kim Sang &amp; De Meulder, 2003)</ref> with Wikipedia URL annotations for all entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 EXPERIMENTS WITH BART AS BACKBONE</head><p>To show that our approach is versatile to different pre-trained models, we conduct experiments with BART as the backbone model. We can see that our approach consistently and significantly (with p-value &lt; 0.01) improves BART-base on all datasets. This result shows that our method is versatile to different pre-trained models. We also conducted an ablation study about the choice of using either nouns or verbs as concepts. We can see that using either nouns-only or verbs-only as concepts for our approach leads to substantial performance drop. This supports our choice about using both nouns and verbs as concepts. We conducted a human evaluation of CommonGEN predictions between T5 and CALM. We asked three annotators to choose the most reasonable sentence between T5-base and CALM-base predictions.</p><p>The evaluation was conducted on 50 test sentences in binary selection by majority voting. Cohen's Kappa score, which is a measurement of inter-annotator agreement, was 0.73. Annotators say that for 60% of test sentences, CALM-base generated better.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of Contrastive self-supervised pre-training objectives. Generative QA style contrastive objective requires the model to distinguish truth sentences from less plausible ones.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>position She was the first woman to hold the position She was the first woman to hold the position</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Proposed Joint Training Framework. Given an input sentence x ("She was the first woman to hold the position."), we extract concept-set C (woman, hold, position). Given x and C, we produce corrupted source sequence x either for C2S and COR. The generator trained with the corresponding objective recovers sentences as distractors x to the discriminator. The discriminator is trained to distinguish truth sentences from randomly selected distractor among two objectives. Parameters between the generator and discriminator are shared.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>and one dataset for generative task: CommonGEN (Lin et al., 2020). Details on datasets are discussed in Appendix A.3. Compared Methods We compare our model with following models continually trained with different intermediate tasks based on the pre-trained T5-base model: (1) T5-base is the pre-trained T5-base model without continually training on any intermediate task. (2) T5-base w/ additional epochs is continually pre-trained using the original pre-training objective of T5 with additional training steps. The total number of additional training steps is equal to that of our final model. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Performance of compared models fine-tuned with different fraction of the datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Knowledge-augmented PTLMs. As standard pre-trained language models usually do not explicitly model knowledge, a number of works have examined the problem of incorporating world knowledge with the PTLMs. Recent work Zhang et al. (2019); Peters et al. (2019); Wang et al. (2020); Liu et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Experimental results on commonsense reasoning datasets. The first group of models are baselines. The models in the middle group and last group except the CALM model are trained with the proposed objectives independently and the final CALM model is trained by joint training. Best models are bold and second best ones are underlined within each metric.</figDesc><table><row><cell>Methods</cell><cell>CSQA</cell><cell>OBQA</cell><cell>PIQA</cell><cell>aNLI</cell><cell></cell><cell>CommonGEN</cell></row><row><cell></cell><cell></cell><cell cols="2">Accuracy (official dev)</cell><cell></cell><cell cols="2">BLEU-4 METEOR CIDEr SPICE</cell></row><row><cell>BERT-base</cell><cell cols="4">53.08(±0.16) 57.60(±0.8) 64.86(±0.52) 61.88(±0.56)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ERNIE</cell><cell cols="4">54.06(±0.12) 58.90(±0.9) 66.47(±0.58) 63.04(±0.46)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>KnowBERT</cell><cell cols="4">53.88(±0.15) 58.50(±0.8) 66.61(±0.63) 63.18(±0.52)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>T5-base</cell><cell cols="4">61.88(±0.08) 58.20(±1.0) 68.14(±0.73) 61.10(±0.38)</cell><cell>24.90</cell><cell>31.20</cell><cell>12.99</cell><cell>32.40</cell></row><row><cell cols="5">T5-base + cont. pretraining 61.92(±0.45) 58.10(±0.9) 68.19(±0.77) 61.15(±0.52)</cell><cell>25.10</cell><cell>31.00</cell><cell>13.12</cell><cell>32.40</cell></row><row><cell>T5-base + SSM</cell><cell cols="4">62.08(±0.41) 58.30(±0.8) 68.27(±0.71) 61.25(±0.51)</cell><cell>25.20</cell><cell>31.20</cell><cell>13.28</cell><cell>32.40</cell></row><row><cell>CALM (Generative-Only)</cell><cell cols="4">62.28(±0.36) 58.90(±0.4) 68.91(±0.88) 60.95(±0.46)</cell><cell>25.80</cell><cell>31.20</cell><cell>13.81</cell><cell>32.60</cell></row><row><cell cols="5">CALM (Contrastive-Only) 62.73(±0.41) 59.30(±0.3) 70.67(±0.98) 61.35(±0.06)</cell><cell>25.50</cell><cell>31.20</cell><cell>13.58</cell><cell>32.60</cell></row><row><cell cols="5">CALM (w/o Mix warmup) 62.18(±0.48) 59.00(±0.5) 69.21(±0.57) 61.25(±0.55)</cell><cell>25.80</cell><cell>31.20</cell><cell>13.77</cell><cell>32.60</cell></row><row><cell>CALM (Mix-only)</cell><cell cols="4">63.02(±0.47) 60.40(±0.4) 70.07(±0.98) 62.79(±0.55)</cell><cell>26.00</cell><cell>31.20</cell><cell>13.82</cell><cell>32.80</cell></row><row><cell>CALM</cell><cell cols="4">63.32(±0.35) 60.90(±0.4) 71.01(±0.61) 63.20(±0.52)</cell><cell>26.40</cell><cell>31.40</cell><cell>13.88</cell><cell>33.00</cell></row><row><cell>Methods</cell><cell>CSQA</cell><cell>OBQA</cell><cell>PIQA</cell><cell>aNLI</cell><cell></cell><cell>CommonGEN</cell></row><row><cell></cell><cell></cell><cell cols="2">Accuracy (official dev)</cell><cell></cell><cell cols="2">BLEU-4 METEOR CIDEr SPICE</cell></row><row><cell>BERT-large</cell><cell cols="4">57.06(±0.12) 60.40(±0.6) 67.08(±0.61) 66.75(±0.61)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>T5-large</cell><cell cols="4">69.81(±1.02) 61.40(±1.0) 72.19(±1.09) 75.54(±1.22)</cell><cell>28.60</cell><cell>30.10</cell><cell>14.96</cell><cell>31.60</cell></row><row><cell cols="5">CALM-large (Mix-only) 70.26(±0.23) 62.50(±1.0) 73.70(±1.09) 75.99(±1.26)</cell><cell>29.20</cell><cell>31.30</cell><cell>15.24</cell><cell>33.10</cell></row><row><cell>CALM-large</cell><cell cols="4">71.31(±0.04) 66.00(±1.0) 75.11(±1.65) 77.12(±0.34)</cell><cell>29.50</cell><cell>31.90</cell><cell>15.61</cell><cell>33.20</cell></row><row><cell>RoBERTa-large 4</cell><cell cols="4">71.81(±0.25) 63.90(±0.8) 76.90(±0.62) 82.35(±0.54)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Experimental results on large model. Comparison between large models of other PTLMs and CALM. Best models are bold and second best ones are underlined within each metric.</figDesc><table><row><cell>Methods</cell><cell>Params</cell><cell></cell><cell cols="2">CommonGEN</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="4">BLEU-4 METEOR CIDEr SPICE</cell></row><row><cell>GPT-2 (Radford et al., 2019)</cell><cell>774M</cell><cell>21.10</cell><cell>26.20</cell><cell>12.15</cell><cell>25.90</cell></row><row><cell>UniLM (Dong et al., 2019)</cell><cell>340M</cell><cell>27.70</cell><cell>29.70</cell><cell>14.85</cell><cell>30.20</cell></row><row><cell>BART (Lewis et al., 2020)</cell><cell>406M</cell><cell>26.30</cell><cell>30.90</cell><cell>13.92</cell><cell>30.60</cell></row><row><cell>T5-base (Raffel et al., 2019)</cell><cell>220M</cell><cell>16.40</cell><cell>23.00</cell><cell>9.16</cell><cell>22.00</cell></row><row><cell>T5-large 5 (Raffel et al., 2019)</cell><cell>770M</cell><cell>28.60</cell><cell>30.10</cell><cell>14.96</cell><cell>31.60</cell></row><row><cell>KG-BART 6 (Liu et al., 2020)</cell><cell>406M</cell><cell>30.90</cell><cell>32.40</cell><cell>16.83</cell><cell>32.70</cell></row><row><cell>T5-base (our implementation)</cell><cell>220M</cell><cell>24.90</cell><cell>31.20</cell><cell>12.99</cell><cell>32.40</cell></row><row><cell>CALM-base</cell><cell>220M</cell><cell>26.40</cell><cell>31.40</cell><cell>13.88</cell><cell>33.00</cell></row><row><cell>CALM-large</cell><cell>774M</cell><cell>29.50</cell><cell>31.90</cell><cell>15.61</cell><cell>33.20</cell></row></table><note>yields further improvement upon the model trained independently with either generative or contrastive objectives. Also, we find that the CALM model consistently outperforms CALM(Mix), demonstrating the effectiveness of the proposed joint training framework. Applying joint training directly on top of a pre-trained model (i.e., CALM(w/o Mix warmup)) does not work very well, demonstrating the necessity of applying mixed training to initialize the model before starting joint training.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison between PTLMs on CommonGEN.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Analysis on Contrastive and Generative objectives. Left table shows the performance on downstream tasks by pre-training with different generative objective (COR, C2S, and original objective for pre-training T5). Right table shows the performance on downstream tasks by pre-training with different task formats of contrastive objective.Task Formulation of the Contrastive objectives For contrastive objectives, we test three different task formats: Multi-choice QA, Generative QA, and True/False. Multi-choice QA and Generative QA takes the concatenation of the real sentence and the distractor. Then, Multi-choice QA output the index of the real sentence following other conventional Multi-choice QA tasks, and Generative QA output the real sentence respectively. True/False takes either the real sentence or the distractor and train the model to perform a binary classification problem of whether the input sentence makes sense. The result is shown in Table4. We could find that the format of Generative QA performs the best. We suspect this is because the Generative QA format is closer to the format used during the original pre-training stage of the T5 model and the format used for fine-tuning.</figDesc><table><row><cell>Methods</cell><cell cols="2">CSQA PIQA</cell><cell></cell><cell cols="2">CommonGEN</cell><cell></cell><cell>Methods</cell><cell cols="2">CSQA PIQA</cell><cell></cell><cell cols="2">CommonGEN</cell><cell></cell></row><row><cell></cell><cell cols="2">Accuracy</cell><cell cols="4">BLEU-4 METEOR CIDEr SPICE</cell><cell></cell><cell cols="2">Accuracy</cell><cell cols="4">BLEU-4 METEOR CIDEr SPICE</cell></row><row><cell>T5 -Text Infilling</cell><cell>61.92</cell><cell>68.19</cell><cell>25.10</cell><cell>31.00</cell><cell>13.13</cell><cell>32.40</cell><cell>Multi-choice QA</cell><cell>62.21</cell><cell>68.82</cell><cell>25.00</cell><cell>31.20</cell><cell>13.28</cell><cell>32.60</cell></row><row><cell>CALM -COR</cell><cell>62.36</cell><cell>68.77</cell><cell>25.70</cell><cell>31.20</cell><cell>13.65</cell><cell>32.60</cell><cell>True/False</cell><cell>62.24</cell><cell>67.81</cell><cell>25.10</cell><cell>31.20</cell><cell>13.41</cell><cell>32.60</cell></row><row><cell>CALM -C2S</cell><cell>62.24</cell><cell>68.75</cell><cell>25.90</cell><cell>31.40</cell><cell>13.94</cell><cell>32.80</cell><cell>Generative QA</cell><cell>62.73</cell><cell>70.67</cell><cell>25.50</cell><cell>31.20</cell><cell>13.58</cell><cell>32.60</cell></row><row><cell></cell><cell cols="4">(a) Generative objectives</cell><cell></cell><cell></cell><cell></cell><cell cols="4">(b) Contrastive objectives</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Experimental results on Knowledge Probing. Left table shows the mean precision on LAMA probing task of ConceptNET. Right table shows the performance on Fact checking and Entity linking, which are from KILT task.</figDesc><table><row><cell>Methods</cell><cell cols="4">MRR Precision@50 Precision@10 Precision@1</cell><cell>Methods</cell><cell>FEVER</cell><cell>AY2</cell></row><row><cell>T5-Base</cell><cell>11.53</cell><cell>38.52</cell><cell>21.60</cell><cell>5.93</cell><cell>T5-base</cell><cell>76.65</cell><cell>74.97</cell></row><row><cell cols="2">CALM (Mix-only) 11.77</cell><cell>38.93</cell><cell>21.92</cell><cell>6.10</cell><cell>CALM (Mix-only)</cell><cell>77.05</cell><cell>76.27</cell></row><row><cell>CALM</cell><cell>12.09</cell><cell>39.69</cell><cell>22.53</cell><cell>6.46</cell><cell>CALM</cell><cell>77.44</cell><cell>77.24</cell></row><row><cell></cell><cell></cell><cell>(a) LAMA probe</cell><cell></cell><cell></cell><cell cols="2">(b) KILT task</cell><cell></cell></row></table><note>Self-Supervised Language Representation Pre-Training. Motivated by the fact that words can have different meanings in different contexts, contextual language representation methods<ref type="bibr" target="#b20">(McCann et al., 2017;</ref><ref type="bibr" target="#b25">Peters et al., 2018)</ref> have been developed and shown superior performance on downstream</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>V100 GPUs and use FP32. For all discriminative tasks, we use the Adam optimizer with maximum sequence length 256, batch size 4 and gradient accumulation 16. For generative task, we use the Adam optimizer with maximum source length 32, maximum target length 32, batch size 8, gradient accumulation 16. For all tasks, we use warmup fraction 0.01. Learning rates and train epochs are listed in Table7.</figDesc><table><row><cell></cell><cell></cell><cell>-7].</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">A.2 FINE-TUNING DETAILS</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">For fine-tuning, we use 4 Hyperparameter CommonsenseQA OpenbookQA</cell><cell>PIQA</cell><cell>aNLI</cell><cell>CommonGEN</cell></row><row><cell>Learning rate</cell><cell>[1e-4, 2e-4, 3e-4]</cell><cell cols="4">[5e-5, 1e-4, 2e-4, 3e-4] [1e-4, 2e-4, 3e-4] [2e-5, 3e-5] [2e-5]</cell></row><row><cell>Train Epochs</cell><cell>20</cell><cell>20</cell><cell>20</cell><cell>10</cell><cell>20</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Fine-tuning hyperparameters.</figDesc><table><row><cell>A.3 DATASET PROPERTIES</cell></row><row><cell>• CommonsenseQA (Talmor et al., 2018) is a multiple-choice question answering task, which</cell></row><row><cell>picks the most appropriate answer on general commonsense questions.</cell></row></table><note>• OpenbookQA<ref type="bibr" target="#b21">(Mihaylov et al., 2018)</ref> is a multiple-choice question answering task, which is modeled after open book exams on elementary-level core science questions. The task requires open book fact and additional commonsense which is not contained in the book. To test the commonsense reasoning ability, we do not use open book fact.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Properties of Commonsense benchmark datasets.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Experimental results with BART as backbone model. Best models are bold. A.6 EXPERIMENTS WITH NOUN/VERB AS CONCEPTS</figDesc><table><row><cell>Methods</cell><cell>CSQA</cell><cell>OBQA</cell><cell>PIQA</cell><cell>aNLI</cell><cell></cell><cell>CommonGEN</cell></row><row><cell></cell><cell></cell><cell cols="2">Accuracy (official dev)</cell><cell></cell><cell cols="2">BLEU-4 METEOR CIDEr SPICE</cell></row><row><cell cols="5">BART-base (Mix-only) 56.31(±0.28) 58.30(±1.1) 67.53(±1.01) 59.85(±1.14)</cell><cell>25.10</cell><cell>29.50</cell><cell>13.16</cell><cell>30.20</cell></row><row><cell>CALM (BART-base)</cell><cell cols="4">58.22(±0.21) 59.10(±1.0) 69.40(±1.23) 61.28(±0.30)</cell><cell>26.40</cell><cell>29.90</cell><cell>13.71</cell><cell>31.10</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>Experimental results with Noun/Verb as Concepts. Best models are bold. A.7 HUMAN EVALUATION ON COMMONGEN GENERATIONS</figDesc><table><row><cell>Methods</cell><cell>CSQA</cell><cell>OBQA</cell><cell>PIQA</cell><cell>aNLI</cell><cell></cell><cell>CommonGEN</cell></row><row><cell></cell><cell></cell><cell cols="2">Accuracy (official dev)</cell><cell></cell><cell cols="2">BLEU-4 METEOR CIDEr SPICE</cell></row><row><cell>CALM</cell><cell cols="4">63.32(±0.35) 60.90(±0.4) 71.01(±0.61) 63.20(±0.52)</cell><cell>26.40</cell><cell>31.40</cell><cell>13.88</cell><cell>33.00</cell></row><row><cell cols="5">CALM-nouns 62.45(±0.42) 59.40(±0.5) 69.05(±0.70) 61.55(±0.58)</cell><cell>25.70</cell><cell>31.20</cell><cell>13.17</cell><cell>32.60</cell></row><row><cell cols="5">CALM-verbs 62.51(±0.47) 59.10(±0.7) 69.24(±0.65) 61.40(±0.51)</cell><cell>25.60</cell><cell>31.20</cell><cell>13.24</cell><cell>32.60</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Code and data have been uploaded and will be published: https://anonymous.4open.science/repository/ 6fdeed55-ec2c-4ffa-aee8-0cc3b7f5ade5</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">We split the concepts with multiple tokens (under Spacy tokenization) into single token to ensure the concepts discussed afterwards all contain a single token.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">https://dumps.wikimedia.org/enwiki/latest/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Spice: Semantic propositional image caption evaluation</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">METEOR: An automatic metric for MT evaluation with improved correlation with human judgments</title>
		<author>
			<persName><forename type="first">Satanjeev</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/W05-0909" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization</title>
				<meeting>the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization<address><addrLine>Ann Arbor, Michigan</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005-06">June 2005</date>
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaitanya</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keisuke</forename><surname>Malaviya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.05739</idno>
		<title level="m">Abductive commonsense reasoning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Piqa: Reasoning about physical commonsense in natural language</title>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fourth AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><surname>Electra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10555</idno>
		<title level="m">Pre-training text encoders as discriminators rather than generators</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Commonsense reasoning and commonsense knowledge in artificial intelligence</title>
		<author>
			<persName><forename type="first">Ernest</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gary</forename><surname>Marcus</surname></persName>
		</author>
		<idno type="DOI">10.1145/2701413</idno>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page">2015</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unified language model pre-training for natural language understanding and generation</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiao-Wuen</forename><surname>Hon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Alché-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="13063" to="13075" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Pytorch lightning. GitHub</title>
		<author>
			<persName><surname>Wa Falcon</surname></persName>
		</author>
		<ptr target="https://github.com/PyTorchLightning/pytorch-lightning" />
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">3</biblScope>
			<pubPlace>Note</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zora</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08909</idno>
		<title level="m">Realm: Retrievalaugmented language model pre-training</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Robust disambiguation of named entities in text</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Hoffart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><forename type="middle">Amir</forename><surname>Yosef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilaria</forename><surname>Bordino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hagen</forename><surname>Fürstenau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manfred</forename><surname>Pinkal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Spaniol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bilyana</forename><surname>Taneva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Thater</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D11-1072" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Edinburgh, Scotland, UK.</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011-07">July 2011</date>
			<biblScope unit="page" from="782" to="792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sabhwaral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
		<title level="m">Unifiedqa: Crossing format boundaries with a single qa system. EMNLP -findings</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11942</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ves</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.13461</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<pubPlace>Bart</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07">July 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Teaching pretrained models with commonsense reasoning: A preliminary kb-based approach</title>
		<author>
			<persName><forename type="first">Shiyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianshu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dian</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.09743</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Commongen: A constrained text generation challenge for generative commonsense reasoning. Findings of EMNLP</title>
		<author>
			<persName><forename type="first">Wangchunshu</forename><surname>Bill Yuchen Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><surname>Ren</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Kg-bart: Knowledge graph-augmented bart for generative commonsense reasoning</title>
		<author>
			<persName><forename type="first">Ye</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learned in translation: Contextualized word vectors</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6294" to="6305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Can a suit of armor conduct electricity? a new dataset for open book question answering</title>
		<author>
			<persName><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.02789</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073083.1073135</idno>
		<ptr target="https://www.aclweb.org/anthology/P02-1040" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002-07">July 2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing</title>
				<meeting>the 2014 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Mark</forename><surname>Matthew E Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05365</idno>
		<title level="m">Deep contextualized word representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Mark</forename><surname>Matthew E Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vidur</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.04164</idno>
		<title level="m">Knowledge enhanced contextual word representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Language models as knowledge bases?</title>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1250</idno>
		<ptr target="https://www.aclweb.org/anthology/D19-1250" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11">November 2019</date>
			<biblScope unit="page" from="2463" to="2473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Majid</forename><surname>Yazdani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicola</forename><forename type="middle">De</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.02252</idno>
		<title level="m">Vassilis Plachouras, Tim Rocktäschel, et al. Kilt: a benchmark for knowledge intensive language tasks</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Intermediate-task transfer learning with pretrained models for natural language understanding: When and why does it work</title>
		<author>
			<persName><forename type="first">Yada</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haokun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mon</forename><surname>Phu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyi</forename><surname>Htut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">Yuanzhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katharina</forename><surname>Vania</surname></persName>
		</author>
		<author>
			<persName><surname>Kann</surname></persName>
		</author>
		<author>
			<persName><surname>Samuel R Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00628</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">How much knowledge can you pack into the parameters of a language model</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08910</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Corby</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh</forename><surname>Phan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.00655</idno>
		<title level="m">Xia Song, Paul Bennett, and Saurabh Tiwary. Knowledgeaware language model pretraining</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Representing general relational knowledge in ConceptNet 5</title>
		<author>
			<persName><forename type="first">Robyn</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Havasi</surname></persName>
		</author>
		<ptr target="http://www.lrec-conf.org/proceedings/lrec2012/pdf/1072_Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC&apos;12)</title>
				<meeting>the Eighth International Conference on Language Resources and Evaluation (LREC&apos;12)<address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-05">May 2012</date>
			<biblScope unit="page" from="3679" to="3686" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA)</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Commonsenseqa: A question answering challenge targeting commonsense knowledge</title>
		<author>
			<persName><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Lourie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00937</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">FEVER: a large-scale dataset for fact extraction and VERification</title>
		<author>
			<persName><forename type="first">James</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Christodoulopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arpit</forename><surname>Mittal</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1074</idno>
		<ptr target="https://www.aclweb.org/anthology/N18-1074" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-06">June 2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="809" to="819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fien</forename><surname>De Meulder</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/W03-0419" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</title>
				<meeting>the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Cider: Consensus-based image description evaluation</title>
		<author>
			<persName><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4566" to="4575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">K-adapter: Infusing knowledge into pre-trained models with adapters</title>
		<author>
			<persName><forename type="first">Ruize</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cuihong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.01808</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><surname>Gugger</surname></persName>
		</author>
		<idno>ArXiv, abs/1910.03771</idno>
		<editor>Mariama Drame, Quentin Lhoest, and Alexander M. Rush</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Pretrained encyclopedia: Weakly supervised knowledge-pretrained language model</title>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BJlzm64tDH" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.07129</idno>
		<title level="m">Ernie: Enhanced language representation with informative entities</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
