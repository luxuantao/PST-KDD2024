<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Re-establishing Fetch-Directed Instruction Prefetching: An Industry Perspective</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yasuo</forename><surname>Ishii</surname></persName>
							<email>yasuo.ishii@arm.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Austin</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jaekyu</forename><surname>Lee</surname></persName>
							<email>jaekyu.lee@arm.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Austin</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Krishnendra</forename><surname>Nathella</surname></persName>
							<email>krishnendra.nathella@arm.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Austin</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dam</forename><surname>Sunwoo</surname></persName>
							<email>dam.sunwoo@arm.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Austin</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Re-establishing Fetch-Directed Instruction Prefetching: An Industry Perspective</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/ISPASS51385.2021.00034</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Instruction prefetching</term>
					<term>branch prediction</term>
					<term>172</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Instruction prefetching can play a pivotal role in improving the performance of workloads with large instruction footprints and frequent, costly frontend stalls. In particular, Fetch Directed Prefetching (FDP) is an effective technique to mitigate frontend stalls since it leverages existing branch prediction resources in a processor and incurs very little hardware overhead. Modern processors have been trending towards provisioning more frontend resources, which bodes well for FDP as it requires these resources to be effective. However, recent academic research has been using outdated and less than optimal frontend baselines that employ smaller structures, resulting in equivocal outcomes. This paper presents a detailed FDP microarchitecture and evaluates two improvements, better branch history management and post-fetch correction. Our mechanism provides a 41.0% speedup over the baseline (no prefetching, no FDP) with only 195 bytes of hardware overhead and outperforms the 1st Instruction Prefetching Championship (IPC-1) winners that had a 128KB storage budget. We believe that our FDP-based frontend design can serve as a new reference baseline for instruction prefetching research to bridge the gap between academia and industry.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The instruction footprints of both data-center and client applications have been growing larger, causing more instruction cache (I-cache) misses and frequent pipeline stalls in modern processors. Instruction prefetching <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b21">[18]</ref> is a well-known technique to reduce I-cache misses. Next line prefetching <ref type="bibr" target="#b22">[19]</ref>, <ref type="bibr" target="#b23">[20]</ref> can effectively cover sequential I-cache accesses, but its effectiveness is drastically reduced for discontinuous control flows due to various branches. Temporal prefetching mechanisms <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b7">[7]</ref>, <ref type="bibr" target="#b21">[18]</ref> try to tackle discontinuity by tracking temporally correlated streams of I-cache misses. If an instruction address that triggers a stream is seen again, subsequent addresses will be prefetched. However, temporal prefetching generally requires huge metadata storage to be effective.</p><p>Fetch-Directed Prefetching (FDP) <ref type="bibr" target="#b8">[8]</ref>- <ref type="bibr" target="#b14">[12]</ref>, instead, leverages existing branch prediction components, such as direction predictors <ref type="bibr" target="#b24">[21]</ref>- <ref type="bibr" target="#b26">[23]</ref> and the Branch Target Buffer (BTB), that are highly accurate and well provisioned in modern processors, making FDP effective with marginal hardware overhead. FDP is built on a decoupled frontend design <ref type="bibr" target="#b27">[24]</ref>. The instruction addresses predicted by the branch predictor are stored in a dedicated structure called a Fetch Target Queue This paper is an extension of the following paper: Y. Ishii, J. Lee, K. Nathella and D. Sunwoo, "Rebasing Instruction Prefetching: An Industry Perspective" in IEEE Computer Architecture Letters, vol. 19, no. 2, pp. 147-150, 1 July-Dec. 2020.</p><p>(FTQ). I-cache lines that FTQ entries point at are fetched into the I-cache before the demand request. The decoupled frontend enables the processor to run ahead and predict future instruction streams independent of instruction fetch. Recently disclosed commercial CPU designs <ref type="bibr" target="#b28">[25]</ref>- <ref type="bibr" target="#b31">[28]</ref> indicate that they employed a variant of FDP.</p><p>FDP relies on accurate branch predictors and sufficiently large BTBs to cover the control flow. In particular, the BTB is solely used to detect branch information during instruction fetching. FDP can run into some issues when the BTB does not well cover the predicted control flow. One issue stems from branch history management. The branch history is used to access branch predictors, so FDP needs to update the branch history from all branches regardless of their outcomes (taken or not taken). If the BTB does not cover some not-taken branches, the predictor cannot make a direction prediction, and the branch history is not updated. Another issue occurs when a taken branch misses in the BTB. A branch that misses in the BTB goes undetected even if the branch direction predictor could have predicted the branch outcome correctly, leading to the wrong, sequential control path. Thus, to cover more branches and improve the effectiveness of FDP, commercial processors have been trending towards adding more resources in the branch predictor, as seen in recent CPUs <ref type="bibr" target="#b28">[25]</ref>- <ref type="bibr" target="#b32">[29]</ref>.</p><p>Academic research, however, shows the opposite trend. First, a less-than-optimal frontend baseline is often used. In the recently held 1st Instruction Prefetching Championship (IPC-1) <ref type="bibr" target="#b16">[14]</ref>, although the baseline has a decoupled frontend and basic FDP capability <ref type="bibr" target="#b8">[8]</ref>, its shallow 12-instruction FTQ limited the runahead capability of FDP. To understand the impact of the limited FDP, we compared various instruction prefetching mechanisms, including a next-line, the top-3 prefetchers, D-JOLT <ref type="bibr" target="#b33">[30]</ref>, FNL+MMA <ref type="bibr" target="#b34">[31]</ref>, and EIP <ref type="bibr" target="#b21">[18]</ref>, from IPC-1, and perfect <ref type="bibr" target="#b35">[32]</ref> with and without FDP. In this experiment, all mechanisms use the same framework with perfect target prediction used in IPC-1. Fig. <ref type="figure" target="#fig_0">1</ref> shows the speedup over the baseline (no prefetching, no FDP). As presented in IPC-1, the top-3 prefetchers offer over 28% speedup, achieving close to perfect prefetching (30.6%). The simplistic FDP only with a larger 192-instruction FTQ shows a 30.2% speedup. The top-3 IPC-1 prefetchers with FDP provide little benefit over FDP alone. These results show that 1) the baseline used in academia lags behind that of industry and 2) employing additional prefetching mechanisms on top of FDP cannot be justified without reasonably high performance benefits.  Second, as shown in Table <ref type="table" target="#tab_0">I</ref>, many academic papers used a smaller BTB for their evaluations, while commercial processors, based on recently disclosed information, implemented a larger BTB. Instead, BTB prefetching mechanisms <ref type="bibr" target="#b11">[10]</ref>- <ref type="bibr" target="#b15">[13]</ref> are proposed to compensate for the limited BTB capacities. Upon an I-cache miss and subsequent fill, BTB prefetching mechanisms pre-decode instructions in the same I-cache line and add identified branch information (target) in the BTB. These mechanisms often require more complex basic-blockbased BTB structures. Moreover, they can cause BTB pollution since branch target information will be blindly inserted even for strongly biased not-taken branches.</p><p>To reduce the gap between frontend baselines in industry and academia, we revisit an FDP design and provide comprehensive microarchitecture details, which is more representative of a modern CPU. We also evaluate two features to further improve FDP. First, using taken-only branch target history, as opposed to branch direction history from all branches used in most academic papers, can make FDP more tolerant of BTBmiss not-taken branches. Branch direction history can be easily disturbed by BTB-miss not-taken branches. Using taken-only target history naturally resolves this issue and allows BTBs to keep taken branches only, improving BTB utilization.</p><p>Second, post-fetch correction (PFC) can quickly recover the misprediction penalty from BTB-miss taken branches by eagerly re-steering the prefetched instruction stream. Even if a branch misses in the BTB, its direction can be accurately predicted by the direction predictor. Also, the branch target can be obtained shortly after the BTB miss for a PC-relative branch (offset embedded in an instruction) or a function return (target obtained from Return Address Stack (RAS)). Without PFC, upon BTB-miss taken branches, a core will continue to execute wrong-path instructions until the pipeline flush that is initiated after the branch mispredictions are identified. PFC was previously used only for simple unconditional and loop branches <ref type="bibr" target="#b28">[25]</ref>, and we extend PFC for all PC-relative This paper claims the following key contributions:</p><p>• We present a comprehensive microarchitecture for FDP design and evaluate two features that improve FDP. • Our FDP design performs 41.0% better than the baseline (no prefetching, no FDP) with only 195 bytes of hardware overhead and outperforms the IPC-1 winners that had a 128KB storage budget. • We find that adding a dedicated prefetcher on top of FDP provides only marginal performance improvement for our evaluated workloads. • Our improved FDP design will bridge the gap in frontend designs between industry and academia, and it can be used as the baseline for future instruction prefetching research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Branch Prediction</head><p>Branch prediction mechanisms are crucial for sustaining core performance by continuously supplying instructions to the pipeline, which has spurred a wide range of studies on branch prediction. Modern processors include various branch predictors that help with branch directions, targets, loops, and return addresses. Direction predictors <ref type="bibr" target="#b24">[21]</ref>- <ref type="bibr" target="#b26">[23]</ref>, <ref type="bibr" target="#b36">[33]</ref> predict the direction (taken or not taken) of branches. Each predictor entry has a saturating counter, which is trained using branch outcomes, and its most significant bit (MSB) indicates the predicted direction. The BTB stores branch targets and is used for target prediction. Dedicated indirect branch predictors <ref type="bibr" target="#b37">[34]</ref> can be used to predict indirect branches that may have multiple targets. Function return addresses are pushed into the RAS upon function calls, and the address at the top of the RAS is used for return instructions. Loop predictors also exist to identify loops with their loop iteration counts. Fig. <ref type="figure" target="#fig_1">2</ref> shows an overview of branch prediction with these components. target hash = (instruction address 2) ⊕ (target 3) (2)</p><formula xml:id="formula_0">target history = (target history 2) ⊕ target hash (3)</formula><p>Maintaining precise recent branch histories is essential for accurate predictions since the history is used to index branch predictor entries. Incorrect history leads to accessing a wrong entry for the prediction, potentially leading to a branch misprediction. Branch outcomes (taken or not taken) are collected after instructions are fetched <ref type="bibr" target="#b38">[35]</ref> and maintained in the global history register (GHR). The GHR can be updated using either directions from all branches (e.g., Eq. 1) or targets from taken branches (e.g., Eq. 3).</p><p>Branch predictors can be accessed at the fetch stage to improve prediction latency. Although the instruction type is not identified yet, the BTB may store the fetch address from the previous instance of a branch instruction. Upon a BTB hit with a predicted taken direction, the instruction fetch will be re-directed to the predicted address. As described in Section I, commercial processors are trending towards increasing the size of branch predictors. Predictors are often implemented using multiple ports or multiple banks with costeffective single-ported RAM <ref type="bibr" target="#b38">[35]</ref> to sustain the latency and bandwidth for such large structures. Also, similar to the multilevel cache hierarchy, the multi-level BTB hierarchy can be implemented <ref type="bibr" target="#b28">[25]</ref>- <ref type="bibr" target="#b31">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Fetch Directed Prefetch</head><p>FDP <ref type="bibr" target="#b8">[8]</ref> was proposed to exploit the existing branch prediction mechanism to initiate instruction prefetches. The branch predictor is decoupled from the instruction fetch logic that consists of the instruction translation lookaside buffer (I-TLB) and the I-cache. The branch predictor continues to predict future fetch addresses and pushes them into the FTQ. The BTB is accessed for addresses stored in the FTQ to produce the next fetch addresses without accessing the I-cache. During control flow prediction, if a series of branches are covered by the BTB, i.e., BTB hits, with the directions supplied from the direction predictors, FDP can effectively prefetch instruction streams. The GHR is updated based on branch predictor outcomes.</p><p>FDP enables a cost-efficient, high-performance frontend thanks to the reuse of existing branch prediction resources. However, its performance can be limited by the BTB capacity. When taken branches miss in the BTB, these branches are not detected and cause the control flow to continue on the sequential path, leading to wrong-path instruction fetching. Moreover, the branch predictor will fail to update the GHR for all BTB-miss not-taken branches, leading to imprecise histories and further mispredictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. IMPROVING FETCH DIRECTED PREFETCHING</head><p>In this section, we elaborate on how FDP suffers from issues previously discussed and how to overcome these issues with two enhancements, taken-only branch target history and postfetch correction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Taken-only Branch Target History</head><p>The direction history is widely used in academic research, but the run-ahead capability enabled by FDP can easily disturb precise direction history management. FDP relies on the BTB to collect branch history information. With this approach, BTB-miss not-taken branches go undetected, and the branch predictor fails to push not-taken branch outcome into the GHR, incurring imprecise branch history. On the other hand, BTBmiss taken branches do not cause the same issue because the pipeline eventually gets an opportunity to fix branch history upon pipeline flush caused by branch misprediction.</p><p>During the pipeline flush, the branch history information is unrolled, and the GHR can be updated based on taken branch information that caused the pipeline flush. BTB-miss not-taken branches can be handled in two ways. First, we can skip the GHR update for these branches, but this may lead to more branch mispredictions because of imprecise history. Second, the GHR can be corrected once the branch has been identified after prediction. This scheme incurs more frontend flushes and backend pipeline stalls because the GHR correction may change the branch predictor output. Basicblock-based BTB constructions <ref type="bibr" target="#b13">[11]</ref>, <ref type="bibr" target="#b14">[12]</ref> were proposed to eliminate these undetected (BTB-miss) not-taken branches. By keeping all branches in the BTB (exactly one branch per BTB entry by the definition of a basic-block), they can recognize BTB-miss not-taken branches upon prediction. However, this strategy can cause BTB pollution. For example, these schemes require dedicated BTB entries even for basic blocks with (almost) always not-taken branches (e.g., conditional branch for exception condition detection), which are less useful. Also, because of the variable size of basic-blocks, the BTB structure becomes more complex, including BTB indexing and additional fields to specify the basic-block start address, its size, and the branch offset.</p><p>Taken-only branch target history naturally resolves the imprecise branch history issue since not-taken branches are ignored for the GHR update. This strategy also allows the BTB to store only taken branches. These benefits led many commercial CPU designs to employ the target branch history <ref type="bibr" target="#b28">[25]</ref>, <ref type="bibr" target="#b39">[36]</ref>, <ref type="bibr" target="#b40">[37]</ref>, and we adopt it for our FDP design as well. Table <ref type="table" target="#tab_1">II</ref> summarizes how BTB-miss not-taken branches can be handled and also shows the benefit of using target history. In Section VI-C, we compare the different history management policies to see how they affect performance and branch prediction accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Post-Fetch Correction</head><p>In the original FDP implementation, on a BTB miss, a branch goes undetected, and no prediction is made. This would lead to fetching wrong I-cache lines for taken branches until the branch misprediction is detected. Assuming that an undetected branch is not-taken is a valid approach, and some commercial processors behave this way <ref type="bibr" target="#b32">[29]</ref>. However, this works well only if the instruction footprint fits in the BTB. Otherwise, FDP will frequently go down the sequential fetch flow, causing more branch mispredictions. However, we have observed that modern branch predictors <ref type="bibr" target="#b26">[23]</ref> can correctly predict the direction of these BTB-miss branches. Based on this observation, we propose to add a postfetch correction (PFC) capability to FDP. PFC was restricted to simple unconditional and loop branches <ref type="bibr" target="#b28">[25]</ref>, and we extend PFC capability for all PC-relative conditional branches. The idea is to unconditionally make direction predictions on all instructions (including non-branch instructions), as in the Alpha EV8 processor <ref type="bibr" target="#b38">[35]</ref>, and store the prediction information (direction for BTB miss branches) in the FTQ. When instructions are (pre-)decoded after the fetch stage, we can accurately identify all branch instructions. Branch targets can also be identified for PC-relative branches (offset embedded in an instruction) and function returns (target obtained from RAS). The FTQ is then checked whether 1) an unconditional branch instruction was predicted not taken or missed in the BTB, or 2) a conditional branch was predicted taken and missed in the BTB. If either condition is met, the FTQ is flushed, the GHR is fixed, and the prefetch stream resumes from the new branch target immediately. Otherwise, I-cache lines will be fetched from the sequential path.</p><p>Fig. <ref type="figure" target="#fig_3">3</ref> illustrates the performance benefit by PFC. Without PFC, the core continues to execute wrong-path instructions until the pipeline flush occurs after the execution stage. Instead, by eagerly detecting BTB-miss taken branches and fixing the control flow, PFC can reduce the branch misprediction penalty and prevent I-cache pollution, improving performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. COMPREHENSIVE FRONTEND DESIGN FOR FDP</head><p>We describe a comprehensive frontend design for FDP in this section. Fig. <ref type="figure" target="#fig_4">4</ref> shows an overview of our frontend design. Similar to existing FDP designs, the frontend contains separate branch prediction and instruction fetch pipelines, and the FTQ connects the two pipelines. We assume fixed-length 32-bit instructions in this paper, but FDP can handle variable instruction lengths as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. FTQ Structure and Hardware Cost</head><p>As described in Section II-B, the FTQ is the only additional structure required by the FDP design. Each FTQ entry covers a 32-byte aligned instruction block so that all instructions in an FTQ entry fall into the same I-cache line. Each FTQ entry has the following fields:    This is only additional field to support PFC from the original FDP implementation. Table <ref type="table" target="#tab_3">III</ref> summarizes all fields of an FTQ entry and the total hardware overhead. We use a 24-entry FTQ based on an empirical study ( §VI-G), so the total storage overhead is 195 bytes. Compared to a conventional FDP implementation, our proposed FDP incurs only 24 bytes (branch direction hint) hardware overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Branch Prediction Pipeline</head><p>Branch predictors are accessed to predict the next instruction address. We model the high-bandwidth branch predictors similar to Alpha EV8 <ref type="bibr" target="#b38">[35]</ref> as follows. First, we use 16bank TAGE predictor tables, where each bank is a singleported RAM. Since we can use the same branch history for all predictions in a given cycle thanks to taken-only branch target history ( §III-A), <ref type="foot" target="#foot_1">1</ref> TAGE can produce up to 16 direction predictions. Second, we use a 16B-indexed BTB, i.e., all branch instructions in the same 16B block will reside in the same BTB set. Our 4-bank, 4-way L2 BTB can read out up to 16 instructions simultaneously. Lastly, we assume that the branch predictor can produce up to 1 taken prediction per cycle to simplify the branch prediction design.</p><p>To utilize this bandwidth, we use a sequential instruction block to access branch predictors without bank conflicts. For example, with the instruction address A under our baseline 12-instruction per cycle prediction bandwidth ( §V), 12 instructions (of which addresses range from A to A+44) will access the predictors concurrently. If a block has any predicted-taken branch with a BTB hit, the corresponding branch target from the BTB or indirect predictor will be fed into the prediction pipeline in the next cycle. As discussed in Section III-A, GHR will be updated by this predicted-taken branch. Otherwise, the prediction pipe continues on the sequential path (i.e., A+48)  without updating the GHR. After the prediction, the 32-byte aligned sequential instruction block (up to 8-instruction) with its prediction information (direction hint and taken branch offset, if any) will be inserted into the FTQ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Instruction Fetch Pipeline</head><p>The instruction fetch pipeline serves two primary purposes: I-cache fills and instruction fetches. First, we access the Icache tag array to see whether the I-cache line associated with an FTQ entry exists in the I-cache. In particular, the two oldest entries that are ready (FTQ State: 1, §IV-A), will be selected to access the I-TLB, and the I-cache tag array will be subsequently accessed after the translation. If the I-cache tag access results in a cache miss, the state is updated (FTQ State: 2), and an I-cache fill request is initiated to the lowerlevel cache without waiting for the entry to be the head of FTQ. Upon a cache fill, the filled cache way information is written into I-cache way of the corresponding FTQ entry, and the state is updated accordingly (FTQ State: 3). Similarly, in case of a hit, the hitting way information will be saved in the FTQ entry, and the state is updated (FTQ State: 3).</p><p>Second, for instructions at the head of the FTQ, the corresponding I-cache line in the specified I-cache way will be fetched from the I-cache data array. <ref type="foot" target="#foot_2">2</ref> Then, instructions will be added into the decode queue. By decoupling I-cache fills and instruction fetches, we can initiate I-cache fills early regardless of any back-pressure from the decode queue and backend. An FTQ entry is released after all instructions from the FTQ entry are sent to the decode queue.</p><p>By pre-decoding instructions that are brought in from the Icache data array, the instruction fetch pipeline can detect PFC candidate branches. Fig. <ref type="figure" target="#fig_6">5</ref> shows an example with an FTQ entry with the start address at offset 2 and the end of the block at offset 6 by a predicted taken branch. PFC will be initiated for two different cases. The first case (Case 1) is when the fetch unit finds an unconditional branch whose offset within the block is less than the end offset (3&lt;6 in the example). If the prediction hint for this instruction is 0, this was due to a wrong direction prediction. Otherwise, it was due to a BTB miss. The second case (Case 2) is by a PC-relative conditional branch whose offset is again less than the end offset with its direction hint set. This case is always caused by a BTB miss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EVALUATION METHODOLOGY</head><p>We evaluate our proposed FDP using Champsim <ref type="bibr" target="#b41">[38]</ref>, an open-source trace-based simulator used in IPC-1. We run 50M instructions with 50M warmup using all workloads, including server, client, and SPEC <ref type="bibr" target="#b42">[39]</ref>, published by IPC-1 <ref type="bibr" target="#b16">[14]</ref>. Each workload shows over 5% performance uplift with a perfect I-cache over a 32KB I-cache. We implemented the proposed comprehensive frontend microarchitecture with various branch predictors, including the TAGE <ref type="bibr" target="#b26">[23]</ref>, BTB, indirect <ref type="bibr" target="#b37">[34]</ref>, and RAS predictors.</p><p>We use core parameters similar to Intel Sunny Cove <ref type="bibr" target="#b43">[40]</ref>. The branch predictors are sized based on recently announced commercial processors <ref type="bibr" target="#b29">[26]</ref>, <ref type="bibr" target="#b30">[27]</ref>, <ref type="bibr" target="#b32">[29]</ref> and tuned for evaluated workloads. We use a 260-bit branch history length for both TAGE and ITTAGE. The branch history is updated only by taken-only branch targets, as shown in Eq. 3. The branch predictor bandwidth is double the fetch bandwidth to support the run-ahead capability <ref type="bibr" target="#b29">[26]</ref>. We evaluated 2-entry (16instruction) FTQ to disable FDP because this configuration limits the run-ahead capability. We evaluated and compared the following mechanisms using common parameters shown in Table <ref type="table" target="#tab_6">IV:</ref> • Baseline: no FDP, no prefetching.</p><p>• Next line (NL1): prefetch the next line upon a cache miss.</p><p>• The top-3 prefetchers from IPC-1: FNL+MMA <ref type="bibr" target="#b34">[31]</ref>,</p><p>D-JOLT <ref type="bibr" target="#b33">[30]</ref>, EIP-128KB (with the original 128KB 34way Entangled Tables) <ref type="bibr" target="#b21">[18]</ref>, and EIP-27KB (with a realistic 27KB 8-way Entangled Table <ref type="table">)</ref>. • Perfect prefetching (Perfect): proposed in <ref type="bibr" target="#b35">[32]</ref>, where we assume that a prefetch brings the data into the cache instantaneously but still sends out the request to the memory subsystem to simulate the traffic overhead. • FDP: our improved FDP with 24-entry (192-instruction) FTQ. PFC is enabled. FDP can be employed independent of other configurations.    We report the geometric mean for instructions per cycle (IPC) and the arithmetic mean for mispredictions per kilo instructions (MPKI) using all evaluated workloads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Performance Improvement by FDP</head><p>We evaluate prefetching mechanisms with and without FDP. Fig. <ref type="figure" target="#fig_9">6a</ref> shows speedup over the baseline (no prefetching, no FDP). Without FDP, NL1 and EIP-27KB offer 10.6% and 32.4% speedup, respectively. Three IPC-1 prefetchers, FDP shows significantly better performance over the baseline, and additional performance can be obtained in two ways: better branch target prediction and instruction prefetching. As explained in Section III-A, BTB misses limit the effectiveness of FDP. We identify that a perfect BTB improves the performance of FDP by 3.4%, which also shows the upper bound of BTB prefetching <ref type="bibr" target="#b11">[10]</ref>- <ref type="bibr" target="#b15">[13]</ref>. Better instruction prefetching shows slightly higher performance uplifts. FDP with EIP-128KB and Perfect give 4.3% and 5.4% additional performance improvements, respectively. When both perfect BTB and prefetching are used, it provides 46.9% performance improvement.</p><p>Fig. <ref type="figure" target="#fig_9">6b</ref> shows performance improvements of EIP-128KB for each workload with FDP on and off. In this chart, each workload has two data points (performance improvements with/without FDP) that share the same x-coordinate as the branch MPKI remains unchanged. When FDP is disabled, EIP-128KB improves performance by up to 2.01x, and most workloads with greater than 20 MPKI show higher than 50% performance improvement. However, with FDP enabled, the max speedup falls to 14.8%, and two workloads show a 0.5% performance degradation. This result shows that FDP can cover most I-cache misses, while EIP-128KB helps capture some of the remaining hard-to-predict patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Performance Impact of Post-Fetch Correction</head><p>As explained in Section III-B, PFC can improve the effectiveness of FDP, particularly with smaller BTBs. To confirm this, we evaluate PFC with BTB sizes varying from 1K to 32K entries by increasing the number of sets. Fig. <ref type="figure">7</ref> shows that PFC results in 2.4% and 9.3% better performance than when PFC is not enabled for an 8K-entry and an 1K-entry BTB, respectively. The performance benefit of PFC comes from reduced mispredictions (25.2% and 75.0% for an 8Kentry and an 1K-entry BTB, respectively).</p><p>However, PFC is less useful for larger BTBs, which is expected since PFC is proposed to handle BTB capacity misses.  For strongly biased branches, PFC can be rather harmful.</p><p>For example, any never-taken branch will not be allocated in the BTB while the large BTB has all other previously taken branches. When a direction predictor makes a wrong prediction (taken) for this BTB-miss branch, PFC incorrectly fixes the FTQ, re-streeing the instruction stream to the wrong taken path and eventually leading to a pipeline flush. As a result, PFC increases mispredictions by 1.5% with only 0.1% performance uplift for a 32K-entry BTB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Performance Impact of Branch History Management</head><p>Many modern processors use taken-only branch target history <ref type="bibr" target="#b40">[37]</ref>, but we notice that academic papers generally use other configurations ( §III-A). To see the performance impact of history management schemes, we evaluate configurations with different history types, history corrections, and BTB allocation policies, as shown in Table <ref type="table" target="#tab_6">V</ref>. Among GHR configurations evaluated, GHR3 with PFC off is commonly used with basic-block-based BTBs. Note that we selected 280bit branch direction history from an empirical study using the idealized GHR configuration to achieve the best branch prediction accuracy.</p><p>Fig. <ref type="figure" target="#fig_10">8</ref> shows the performance and branch mispredictions per kilo instruction (MPKI) results. First of all, enabling PFC shows better performance across all configurations by reducing branch misprediction. Since the overall trend is similar between PFC on and PFC off, we will describe results with PFC enabled now on. THR shows similar performance to Ideal, showing the benefit of using branch target history. Since we tuned the branch predictors to improve accuracy for each configuration, THR in some cases achieves a marginally higher accuracy than Ideal. Compared to Ideal, GHR2 shows similar branch prediction accuracy, but frontend flushes to fix up the history incur more pipeline stalls, resulting in 23.7% lower performance. Not fixing the history as in GHR0 increases branch mispredictions by 19.5% with 1.5% lower performance than Ideal. The taken-only BTB allocation policies show better prediction accuracy (GHR0 vs. GHR1 and GHR2 vs. GHR3), but the performance impact varies depending on whether we correct not-taken branch histories. Both GHR2 and GHR3 require pipeline flushes for BTB-miss not-taken branches to fix history. GHR3 incurs more mispredictions as it allocates entries for not-taken branches as well, but this helps decrease the pipeline flushes. This experiment shows that the FDP configuration with taken-only branch target history (THR) outperforms all other configurations, and, thus, the target history should be used in the frontend design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. ISO-Budget Comparison</head><p>Dedicated instruction prefetchers offer some performance improvement on top of FDP ( §VI-A), but these prefetching mechanisms require additional storage and complex logic. Based on Samsung Exynos M3 data <ref type="bibr" target="#b30">[27]</ref>, <ref type="bibr" target="#b31">[28]</ref>, we estimated that a BTB entry needs 7 bytes per branch. Since EIP-27KB ( §V) has similar storage cost as a 4K-entry BTB, we compare 1) 8K-entry BTB, 2) 4K-entry BTB with EIP-27KB, and 3) 4K-entry BTB as a reference, on top of FDP.</p><p>Fig. <ref type="figure" target="#fig_11">9</ref> shows speedup, branch MPKI, starvation cycles per kilo instructions, and the number of I-cache tag accesses per kilo instructions. Here, starvation is due to fetch-stalls, and starvation cycles is defined as the total number of cycles when the decode queue contains less than a decode-width number of instructions. Both configurations perform similarly (41.0% vs. 40.6% speedup), although the two mechanisms take different approaches. We find that thanks to a larger BTB, FDP with an 8K-BTB has 12% fewer branch mispredictions compared to with EIP-27KB. However, useful prefetches by EIP-27KB result in 13.5% lower starvation cycles compared to FDP with an 8K-BTB. The biggest difference between the two schemes is the number of I-cache tag accesses. Prefetching mechanisms inevitably have redundant prefetches, which are less harmful than inaccurate prefetches. However, without an effective filtering mechanism, redundant prefetches will probe the I-cache tag array. 3 As a result, as shown in Fig. <ref type="figure" target="#fig_11">9</ref>, EIP-27KB has 3.5 times more I-cache tag accesses, which will increase dynamic cache power consumption. This study suggests that the complexity and power consumption by having an additional prefetching mechanism cannot be justified over the straightforward design of FDP with a larger BTB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. BTB Prefetching with PFC</head><p>As explained in Section II-B, BTB is critical to FDP effectiveness. BTB prefetching [10]- <ref type="bibr" target="#b15">[13]</ref> has been proposed to compensate for limited BTB capacities. When a BTB miss occurs, a branch will go undetected and be mispredicted if the branch is taken. Unlike instruction prefetching mechanisms, BTB prefetching requires an additional pre-decoder to identify branch instructions from the I-cache lines. Note that our FDP design also requires a pre-decoder for PFC.</p><p>In this section, we evaluate Divide-and-conquer <ref type="bibr" target="#b15">[13]</ref>, which is based on FDP and comprises three predictors: selectivenext-four-line (SN4L), discontinuity <ref type="bibr" target="#b3">[4]</ref> (Dis), and BTB prefetching. SN4L improves next-four-line prefetcher using usefulness filter. Only previously useful cache lines among the next four will be prefetched. Dis records jumps between two I-cache miss lines in DisTable. On I-cache accesses, Dis looks up DisTable and generates prefetches if the missing line is found. Otherwise, the BTB will be consulted. Finally, BTB prefetching is employed to reduce BTB misses. BTB prefetching can be activated when an I-cache fill is processed. The predecoder identifies the PC-relative branches and installs those detected branches unconditionally into the BTB array. Because of the nature of the prefetching scheme, the register-relative indirect branches cannot be prefetched by this mechanism.</p><p>We evaluate Divide-and-conquer with BTB prefetching (SN4L+Dis+BTB) and without BTB prefetching (SN4L+Dis) on different BTB sizes (2K-, 8K-entry, and perfect BTBs), different branch history managements (THR: target history in our FDP design and GHR: GHR3 in Table V used in academic papers), and with/without PFC. Fig. <ref type="figure" target="#fig_12">10</ref> shows the result, and we make the following observations. PFC is more effective than BTB prefetching. In the case of a BTB miss, BTB prefetching causes pipeline stalls until the BTB miss is resolved. However, PFC can speculatively progress without 3 FNL+MMA <ref type="bibr" target="#b34">[31]</ref> implemented such a filter, but it still requires 1.5x more I-cache tag accesses over no prefetch. waiting for BTB miss resolution, as long as the BTB miss branch satisfies a PFC condition. As we demonstrated in Section VI-C, we can see that THR is always more beneficial than GHR. Also, we can observe that BTB prefetching is less effective with larger BTBs. When GHR is used, BTB prefetching improves performance by 8.8% and 3.2% for 2Kand 8K-entry BTBs, respectively. For 8K-entry BTB with THR, BTB prefetching hurts performance. The performance degradation is due to BTB pollution caused by prefetching strongly biased, e.g., (almost) never taken, branches into the BTB. These branches victimize useful BTB entries, including nonprefetchable indirect branches. This result shows the limitation of BTB prefetching for commercial processors that employ larger BTBs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Branch Prediction Sensitivity 1) BTB capacity:</head><p>As explained in Section I, FDP requires a sufficiently large BTB capacity to be effective. To see how FDP works with different BTB sizes, we vary the size from 1K to 32K entries. Fig. <ref type="figure" target="#fig_13">11</ref> shows the result. With FDP, PFC can compensate for more BTB misses in smaller BTBs, resulting in much higher performance improvements in smaller BTBs (between 1K-to 8K-entry). However, without FDP, we can see only moderate performance improvement by increasing BTB size and the largest jump at 16K-entry BTB, i.e., branch footprint fits in 16K-entry BTB. After 16K entries, both configurations show diminishing performance benefits. As explained in Section IV-C, although the number of branch mispredictions is similar, FDP performs significantly better in all BTB capacities because BTB and I-cache access latencies can be well hidden by FDP.</p><p>2) Branch direction predictor sensitivity: FDP also relies on accurate branch direction prediction. In this section, we evaluate Gshare <ref type="bibr" target="#b24">[21]</ref> (8KB) with a 15-bit idealized branch direction history, various TAGE <ref type="bibr" target="#b26">[23]</ref> sizes, including 9KB (a half), 18KB (baseline), 36KB (2x), and perfect prediction. For the TAGE predictor, we change the number of entries in all prediction tables without optimizing the history length.</p><p>Fig. <ref type="figure" target="#fig_2">12</ref> shows the result. As expected, FDP with the Gshare predictor performs worse than a similarly-sized TAGE (31.4% vs. 37.1%). Also, PFC degrades performance with Gshare, unlike with TAGE predictors, resulting in 6.0% performance loss. With less accurate direction predictors, BTB-miss not-taken branches with an incorrectly predicted direction (predicted taken) are correctly handled without PFC. However, PFC incorrectly fixes the branch prediction, causing a misprediction and eventually a pipeline flush at branch resolution time. This result demonstrates that conventional wisdom that says "do not make taken prediction upon a BTB miss" has changed thanks to the improvement in branch direction prediction algorithms. Using perfect direction prediction makes PFC more effective (4.6% improvement), and using both perfect direction and target predictions (Perfect All) leads to 49.4% performance improvement.</p><p>3) Prediction bandwidth/latency sensitivity: We also vary prediction bandwidth and BTB latency parameters. Fig. <ref type="figure" target="#fig_3">13</ref> shows the results. As explained in Section V, we use 2x higher prediction bandwidth (12-instruction per cycle) than fetch bandwidth (6-instruction per cycle) to improve run-ahead capability. Also, we stop the prediction after a predicted-taken branch is found. In this section, we evaluate 6-instruction per cycle (a half bandwidth, B6), 12 (the baseline, B12), 18 (1.5x, B18), and 18 with multiple taken branches (B18m) configurations. B18 does not improve performance over B12 since 2x fetch bandwidth can sufficiently feed instructions to the decode stage. Having half the bandwidth degrades performance by 0.6%. When we predict more than one taken branch, it (B18m) gives a marginal 0.2% performance improvement.</p><p>We also vary the BTB latency from one to four cycles. In a steady state, the BTB latency is less critical since BTB accesses are pipelined, but the latency affects performance when the FTQ does not contain enough instructions after pipeline flushes. Compared to the baseline 2-cycle latency, 4cycle latency hurts performance by 1.8%.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. FTQ Size Sensitivity</head><p>We evaluate the performance sensitivity of FDP to the FTQ size, sweeping it from 2 to 32 entries. Fig. <ref type="figure" target="#fig_4">14</ref> shows the performance normalized to a 2-entry FTQ (which is equivalent to no FDP). The speedup increases from 23.7% for a 4-entry FTQ to 39.5% for a 12-entry FTQ, and the increase is marginal thereafter.</p><p>To further understand performance gains from larger FTQ sizes, we classify cache misses into 1) fully exposed, 2) partially exposed, or 3) covered. A cache miss is covered if the corresponding cache transaction receives the cache line before observing a starvation cycle. A cache miss is fully exposed only if a cache miss request is initiated after the corresponding FTQ entry becomes the head of FTQ, otherwise partially exposed. As shown in Fig. <ref type="figure" target="#fig_4">14</ref>, 76% of cache misses are fully or partially exposed with a 2-entry FTQ, and FDP with a 24-entry FTQ removes 90.6% of such exposed cache misses. The exposed misses are continuously reduced up to 32-entry, mostly from partially exposed misses. Consequently, reducing fully exposed cache misses could be one focus area for a dedicated prefetching algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. RELATED WORK</head><p>Instruction and BTB prefetching are mutually complementary. BTB prefetching requires to see instruction streams to decode branch information from the instruction block and fill the information into the BTB. For instruction prefetching to be effective, it requires accurate prediction of branch target information from the BTB. In this section, we will discuss these two prefetching mechanisms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Instruction Prefetching</head><p>Unlike spatial prefetchers, such as next line prefetching <ref type="bibr" target="#b22">[19]</ref>, <ref type="bibr" target="#b23">[20]</ref>, that prefetch adjacent I-cache lines, temporal prefetching <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b6">[6]</ref>, <ref type="bibr" target="#b21">[18]</ref> can track temporally correlated streams of I-cache misses and prefetch subsequent addresses upon seeing triggering lines again. Spatial compaction is often applied to temporal streams <ref type="bibr" target="#b6">[6]</ref>. SHIFT <ref type="bibr" target="#b7">[7]</ref> reduced the storage overhead of temporal history by sharing it across cores. Call graph prefetching (CGP) <ref type="bibr" target="#b45">[42]</ref> analyzed call graphs to prefetch instructions of the function that may soon be called. RASdirected instruction prefetching (RDIP) <ref type="bibr" target="#b9">[9]</ref> correlated I-cache misses with the program context captured from the RAS. When a program encounters the same context, recorded Icache missing lines will be prefetched. The main drawback of temporal prefetching mechanisms is that they require large metadata storage to record the instruction streams to be effective.</p><p>Various hardware instruction prefetching mechanisms were published in the IPC-1 <ref type="bibr" target="#b16">[14]</ref>. EIP <ref type="bibr" target="#b21">[18]</ref> introduced the concept of entangling the cache line to be prefetched (destination) with a source cache line such that the destination would be prefetched when the source cache line is encountered. FNL+MMA <ref type="bibr" target="#b34">[31]</ref> combines efficient, aggressive next line prefetcher (Footprint Next Line) and temporal correlation prefetcher (Multiple Miss Ahead). D-JOLT <ref type="bibr" target="#b33">[30]</ref> improves upon RDIP by using a new signature generation mechanism that generates the prefetches from a FIFO structure that stores the function return addresses instead of the stack structure used by RDIP. Most of these recent prefetching mechanisms require large structures and complex logic, while we show that FDP can achieve similar benefits by leveraging existing branch prediction structures.</p><p>Software prefetching mechanisms also exist <ref type="bibr" target="#b17">[15]</ref>- <ref type="bibr" target="#b20">[17]</ref>, <ref type="bibr" target="#b46">[43]</ref>. pTask <ref type="bibr" target="#b19">[16]</ref> identified that operating system (OS)-intensive applications keep switching tasks between applications, system calls, and interrupt handlers, and the I-cache hit rate drops significantly right after task switches. A specific OS event generally separated these tasks. pTask annotates the OS and applications to profile frequent functions, associates them with OS events, and records the compressed prefetch list in memory, which is later used to issue prefetches. However, pTask was evaluated without a realistic BTB implementation and did not faithfully model a decoupled frontend. AsmDB <ref type="bibr" target="#b17">[15]</ref> and I-SPY <ref type="bibr" target="#b20">[17]</ref> are recent profile-guided compiler-based software instruction prefetching proposals. While these approaches may be effective, they were evaluated using a simple frontend design that lacked FDP and realistic branch prediction structures, such as BTB and RAS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. BTB Prefetching</head><p>Confluence <ref type="bibr" target="#b11">[10]</ref> proposed unified metadata for instruction and BTB prefetching. The metadata includes a branch type and a target displacement field for an I-cache line. To incorporate per-block metadata, they also proposed a new block-based BTB construction, AirBTB, which has bitmaps to identify branches in a block and stores other metadata related to the branch (type and target information). Boomerang <ref type="bibr" target="#b13">[11]</ref> was proposed to remove metadata needed by Confluence by using a basic-block-based BTB implementation. Upon BTB misses, the corresponding I-cache line will be fetched and pre-decoded to identify branches in it. The identified branches will be filled in the BTB. However, more complex basic-block-based BTBs require additional storage to store additional information, such as basic-block size, and track (almost) always nottaken branches. They also need to be accessed to identify all branches, including not-taken branches, to decide the start address of the next basic-block, limiting the BTB bandwidth while being more power-hungry. Shotgun <ref type="bibr" target="#b14">[12]</ref> optimizes the BTB construction with three BTB structures: unconditional branch BTB (U-BTB), return instruction buffer (RIB), and conditional branch BTB (C-BTB). U-BTB incorporates the spatial footprint of calls and long jumps. For these remote branches, U-BTB stores spatial information at both caller and callee sites. Upon function calls and returns, prefetches will be generated based on their spatial coding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSIONS</head><p>Addressing frontend performance bottlenecks is critical for applications with large code footprints. However, academic and industrial communities have diverged and are using different approaches to resolve the same issue. We revisited FDP that many commercial CPUs have employed, presented a comprehensive design, and evaluated two features, takenonly branch target history and post-fetch correction, to address challenges in designing an effective FDP. Our FDP design offers 41.0% speedup over no prefetching with marginal 195 bytes of hardware overhead. It also outperforms IPC-1 winners and performs 5.4% lower than perfect prefetching. This design can also be coupled with other prefetchers to further improve performance for more frontend intensive workloads. We believe that our work can guide future instruction prefetching research to be more impactful to commercial CPUs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Prefetching limit study using the IPC-1 framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The overview of branch prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>direction history = (direction history 1 )</head><label>1</label><figDesc>| branch taken<ref type="bibr" target="#b0">(1)</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The performance benefit of PFC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Overview of frontend microarchitecture design.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(3-bit)  to fetch I-cache line without extra I-cache tag access ( §IV-C).• State (2-bit) to track I-TLB/I-cache tag lookup completion: 0: Invalid, 1: Branch prediction completed and ready for address translation, 2: Address translation completed and is waiting for the I-cache fill, and 3: Ready to send instructions to the decode queue. • Branch direction hint (8-bit): 1-bit per each instruction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. PFC candidate branches from an FTQ entry.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(a) IPC improvement by instruction prefetching.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Per-trace performance improvement of EIP-128KB. Each workload has two data points (with and without FDP) that share the same x-coordinate (same branch MPKI).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Instruction prefetching performace results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Branch history management results (line: branch MPKI).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 9 .</head><label>9</label><figDesc>Fig.9. ISO-budget analysis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. BTB prefetching results (line: branch MPKI).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. BTB capacity sensitivity (line: branch MPKI).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 12 .Fig. 13 .</head><label>1213</label><figDesc>Fig. 12. Branch predictor sensitivity (line: branch MPKI).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I</head><label>I</label><figDesc></figDesc><table><row><cell cols="4">BTB CAPACITY GAP BETWEEN ACADEMIA AND INDUSTRY (#</cell></row><row><cell></cell><cell cols="2">ENTRIES).</cell><cell></cell></row><row><cell>Academia</cell><cell>BTB</cell><cell>Industry</cell><cell>BTB</cell></row><row><cell>Shotgun [12]</cell><cell>2.1K</cell><cell>AMD Zen2 [29]</cell><cell>7 K</cell></row><row><cell>Confluence [10]</cell><cell>1.5K</cell><cell>Samsung Exynos M3 [27]</cell><cell>16K</cell></row><row><cell cols="2">Divide&amp;Conquer [13] 2K</cell><cell>Arm Neoverse N1 [26]</cell><cell>6 K</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II HANDLING</head><label>II</label><figDesc>BTB-MISS NOT-TAKEN BRANCHES.</figDesc><table><row><cell>History type</cell><cell>GHR</cell><cell>Branch mis-</cell><cell>Frontend</cell><cell>BTB</cell></row><row><cell></cell><cell>fixup</cell><cell>predictions</cell><cell>stalls</cell><cell>allocation</cell></row><row><cell>Target</cell><cell cols="2">No need Fewer</cell><cell>None</cell><cell>Taken</cell></row><row><cell>Direction (no fix)</cell><cell>No</cell><cell>Most</cell><cell>None</cell><cell>All</cell></row><row><cell>Direction (fix)</cell><cell>Yes</cell><cell>More</cell><cell>More</cell><cell>All</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III HARDWARE</head><label>III</label><figDesc>OVERHEAD.   </figDesc><table><row><cell>Field</cell><cell>Size</cell></row><row><cell>Start address</cell><cell>48-bit</cell></row><row><cell>Block predicted taken</cell><cell>1-bit</cell></row><row><cell>Block termination offset</cell><cell>3-bit</cell></row><row><cell>I-cache way</cell><cell>3-bit</cell></row><row><cell>State</cell><cell>2-bit</cell></row><row><cell>Direction hint</cell><cell>8-bit</cell></row><row><cell>Total (24-entry)</cell><cell>195 bytes</cell></row></table><note>• Instruction start address (48-bit): The FTQ needs not store physical addresses since the I-cache tag lookup follows immediately after I-TLB lookup. • Predicted taken (1-bit) to indicate whether the block is terminated by a predicted taken branch. • Block termination offset (3-bit) by the taken branch within a 32-byte block (8 instructions). • I-cache way</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V BRANCH</head><label>V</label><figDesc>HISTORY MANAGEMENT POLICIES.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">Authorized licensed use limited to: Tsinghua University. Downloaded on December 31,2022 at 08:23:28 UTC from IEEE Xplore. Restrictions apply.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1">Until a taken branch, all instructions can use the same target history.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2">The cache line will be protected, i.e., not evicted, until the instruction fetch. Although this can be handled differently (e.g., I-cache fills check and flush the FTQ if a matching entry exists), we empirically confirm that this case rarely occurs (none from our simulations) and will not be discussed further.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank the anonymous reviewers for their valuable feedback.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Authorized licensed use limited to: Tsinghua University. Downloaded on December 31,2022 at 08:23:28 UTC from IEEE Xplore. Restrictions apply.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Instruction prefetching using branch prediction information</title>
		<author>
			<persName><forename type="first">K</forename><surname>I-Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chih-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Mudge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Design: VLSI in Computers and Processors (ICCD)</title>
				<meeting>the International Conference on Computer Design: VLSI in Computers and Processors (ICCD)</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="593" to="601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Instruction cache prefetching using multilevel branch prediction</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Veidenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on High Performance Computing (ISHPC)</title>
				<meeting>the International Symposium on High Performance Computing (ISHPC)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="51" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Branch history guided instruction prefetching</title>
		<author>
			<persName><forename type="first">V</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Tyson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Charney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Puzak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th IEEE International Symposium on High-Performance Computer Architecture (HPCA)</title>
				<meeting>the 7th IEEE International Symposium on High-Performance Computer Architecture (HPCA)</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="291" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Effective instruction prefetching in chip multiprocessors for modern commercial applications</title>
		<author>
			<persName><forename type="first">L</forename><surname>Spracklen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Abraham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th IEEE International Symposium on High-Performance Computer Architecture (HPCA)</title>
				<meeting>the 11th IEEE International Symposium on High-Performance Computer Architecture (HPCA)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="225" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Temporal instruction fetch streaming</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moshovos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
				<meeting>the 41st Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/MICRO.2008.4771774</idno>
		<ptr target="https://doi.org/10.1109/MICRO.2008.4771774" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Proactive instruction fetch</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kaynak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<idno type="DOI">10.1145/2155620.2155638</idno>
		<ptr target="https://doi.org/10.1145/2155620.2155638" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
				<meeting>the 44th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="152" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">SHIFT: Shared history instruction fetch for lean-core server processors</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kaynak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Grot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
				<meeting>the 46th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="272" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fetch directed instruction prefetching</title>
		<author>
			<persName><forename type="first">G</forename><surname>Reinman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Austin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
				<meeting>the 32nd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="16" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">RDIP: Return-address-stack directed instruction prefetching</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kolli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
				<meeting>the 46th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<idno type="DOI">10.1145/2540708.2540731</idno>
		<ptr target="https://doi.org/10.1145/2540708.2540731" />
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="page" from="260" to="271" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Confluence: Unified instruction supply for scale-out servers</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kaynak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Grot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
				<meeting>the 48th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<idno type="DOI">10.1145/2830772.2830785</idno>
		<ptr target="https://doi.org/10.1145/2830772.2830785" />
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="page" from="166" to="177" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Boomerang: A metadata-free architecture for control flow delivery</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Grot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nagarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd IEEE International Symposium on High-Performance Computer Architecture (HPCA</title>
				<meeting>the 23rd IEEE International Symposium on High-Performance Computer Architecture (HPCA</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="493" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Blasting through the front-end bottleneck with shotgun</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Grot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nagarajan</surname></persName>
		</author>
		<idno type="DOI">10.1145/3173162.3173178</idno>
		<ptr target="https://doi.org/10.1145/3173162.3173178" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
				<meeting>the 23rd ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="30" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Divide and conquer frontend bottleneck</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ansari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lotfi-Kamran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sarbazi-Azad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 47th Annual International Symposium on Computer Architecture (ISCA)</title>
				<meeting>the 47th Annual International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="65" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The 1st instruction prefetching championship</title>
		<author>
			<persName><surname>Ipc-1</surname></persName>
		</author>
		<ptr target="https://research.ece.ncsu.edu/ipc/" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">AsmDB: Understanding and mitigating front-end stalls in warehouse-scale computers</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ayers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Nagendra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">I</forename><surname>August</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kanev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Litz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moseley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th Annual International Symposium on Computer Architecture (ISCA)</title>
				<meeting>the 46th Annual International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<idno type="DOI">10.1145/3307650.3322234</idno>
		<ptr target="https://doi.org/10.1145/3307650.3322234" />
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="page" from="462" to="473" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">pTask: A smart prefetching scheme for os intensive applications</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kallurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Sarangi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual IEEE/ACM International Symposium on Microarchitecture</title>
				<meeting>the 49th Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">I-SPY: Context-driven conditional instruction prefetching with coalescing</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sriraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Devietti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pokam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Litz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kasikci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual IEEE/ACM International Symposium on Microarchitecture</title>
				<meeting>the 53rd Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="146" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The entangling instruction prefetcher</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jimborean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Architecture Letters (CAL)</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="84" to="87" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Prefetching in supercomputer instruction caches</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1992 ACM/IEEE Conference on Supercomputing (SC)</title>
				<meeting>the 1992 ACM/IEEE Conference on Supercomputing (SC)<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="588" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An improved lookahead instruction prefetching</title>
		<author>
			<persName><forename type="first">G.-H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O.-Y</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-B</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings High Performance Computing on the Information Superhighway (HPC Asia</title>
				<meeting>High Performance Computing on the Information Superhighway (HPC Asia</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="712" to="715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Combining branch predictors</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mcfarling</surname></persName>
		</author>
		<idno>TN- 36</idno>
		<imprint>
			<date type="published" when="1993-06">June 1993</date>
		</imprint>
		<respStmt>
			<orgName>Digital Western Research Laboratory</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dynamic branch prediction with perceptrons</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th IEEE International Symposium on High-Performance Computer Architecture (HPCA)</title>
				<meeting>the 7th IEEE International Symposium on High-Performance Computer Architecture (HPCA)<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2001-01">Jan 2001</date>
			<biblScope unit="page" from="197" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">TAGE-SC-L branch predictors again</title>
		<author>
			<persName><forename type="first">A</forename><surname>Seznec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 5th JILP Workshop on Computer Architecture Competitions (JWAC-5): Championship Branch Prediction (CBP-5)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A scalable front-end architecture for fast instruction delivery</title>
		<author>
			<persName><forename type="first">G</forename><surname>Reinman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
		<idno type="DOI">10.1145/300979.300999</idno>
		<ptr target="https://doi.org/10.1145/300979.300999" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Symposium on Computer Architecture (ISCA). USA: IEEE Computer Society</title>
				<meeting>the 26th Annual International Symposium on Computer Architecture (ISCA). USA: IEEE Computer Society</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="234" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The IBM z15 High Frequency Mainframe Branch Predictor</title>
		<author>
			<persName><forename type="first">N</forename><surname>Adiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bonanno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Collura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Heizmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Prasky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saporito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 47th Annual International Symposium on Computer Architecture (ISCA)</title>
				<meeting>the 47th Annual International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="27" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The Arm Neoverse N1 platform: Building blocks for the next-gen cloud-to-edge infrastructure SoC</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pellegrini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Stephens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pusdesris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Raja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Abernathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Koppanalil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ringe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tummala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Werkheiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="53" to="62" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Samsung Exynos M3 processor</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rupley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Hot Chips 30 Symposium (HCS)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Evolution of the Samsung Exynos CPU Microarchitecture</title>
		<author>
			<persName><forename type="first">B</forename><surname>Grayson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rupley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">Z</forename><surname>Zuraski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Quinnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jiménez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nakra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kitchin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hensley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brekelbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghiya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 47th Annual International Symposium on Computer Architecture (ISCA)</title>
				<meeting>the 47th Annual International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="40" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The AMD &quot;Zen 2&quot; processor</title>
		<author>
			<persName><forename type="first">D</forename><surname>Suggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Subramony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bouvier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="45" to="52" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">D-JOLT: Distant jolt prefetcher again</title>
		<author>
			<persName><forename type="first">T</forename><surname>Nakamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Koizumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Degawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sakai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shioya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 1st Instruction Prefetching Championship (IPC-1)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The FNL+MML instruction cache prefetcher</title>
		<author>
			<persName><forename type="first">A</forename><surname>Seznec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 1st Instruction Prefetching Championship (IPC-1)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">PIPS: Prefetching instructions with probabilistic scouts</title>
		<author>
			<persName><forename type="first">P</forename><surname>Michaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 1st Instruction Prefetching Championship (IPC-1)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Two-level adaptive training branch prediction</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
		<idno type="DOI">10.1145/123465.123475</idno>
		<ptr target="http://doi.acm.org/10.1145/123465.123475" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
				<meeting>the 24th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="51" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A 64-kbytes ITTAGE indirect branch predictor</title>
		<author>
			<persName><forename type="first">A</forename><surname>Seznec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2nd JILP Workshop on Computer Architecture Competitions</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>Championship Branch Prediction (CBP-3)</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Design tradeoffs for the Alpha EV8 conditional branch predictor</title>
		<author>
			<persName><forename type="first">A</forename><surname>Seznec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sazeides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Annual International Symposium on Computer Architecture (ISCA)</title>
				<meeting>the 29th Annual International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Experiment flows and microbenchmarks for reverse engineering of branch predictor structures</title>
		<author>
			<persName><forename type="first">V</forename><surname>Uzelac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Milenkovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS</title>
				<meeting>the 2009 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="207" to="217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Software optimization guide for amd family 17h processors</title>
		<ptr target="https://developer.amd.com/wordpress/media/2013/12/55723SOGFam17hProcessors3.00.pdf" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
		<respStmt>
			<orgName>AMD</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">ChampSim</title>
		<ptr target="https://github.com/ChampSim/ChampSim" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Standard Performance Evaluation Corporation</title>
		<ptr target="https://www.spec.org/cpu" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>SPEC CPU benchmark suites</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Examining Intel&apos;s Ice Lake processors: Taking a bite of the Sunny Cove microarchitecture</title>
		<author>
			<persName><forename type="first">I</forename><surname>Cutress</surname></persName>
		</author>
		<ptr target="https://www.anandtech.com/show/14514/examining-intels-ice-lake-microarchitecture-and-sunny-cove/3" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Path confidence based lookahead prefetching</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Pugsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Gratz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L N</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chishti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual IEEE/ACM International Symposium on Microarchitecture</title>
				<meeting>the 49th Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Call graph prefetching for database applications</title>
		<author>
			<persName><forename type="first">M</forename><surname>Annavaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Davidson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th IEEE International Symposium on High-Performance Computer Architecture (HPCA)</title>
				<meeting>the 7th IEEE International Symposium on High-Performance Computer Architecture (HPCA)</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="281" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Cooperative prefetching: compiler and hardware support for effective instruction prefetching in modern processors</title>
		<author>
			<persName><forename type="first">Chi-Keung</forename><surname>Luk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Mowry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st Annual IEEE/ACM International Symposium on Microarchitecture</title>
				<meeting>the 31st Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="182" to="193" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
