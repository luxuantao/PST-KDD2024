<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Role Allocation and Reallocation in Multiagent Teams: Towards A Practical Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ranjit</forename><surname>Nair</surname></persName>
							<email>nair@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Dept Univ. of Southern California</orgName>
								<address>
									<postCode>90089</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Milind</forename><surname>Tambe</surname></persName>
							<email>tambe@usc.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Computer Science Dept Univ. of Southern California</orgName>
								<address>
									<postCode>90089</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Stacy</forename><surname>Marsella</surname></persName>
							<email>marsella@isi.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Information Sciences Institute Univ. of Southern California Marina del Rey</orgName>
								<address>
									<postCode>90292</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Role Allocation and Reallocation in Multiagent Teams: Towards A Practical Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E68B7FFD7E637672892C7FEC580CB610</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T04:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>I.2.11 [Distributed Artificial Intelligence]: Multiagent systems Measurement</term>
					<term>Performance</term>
					<term>Algorithms Role allocation</term>
					<term>Reallocation</term>
					<term>Analysis of Teams</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite the success of the BDI approach to agent teamwork, initial role allocation (i.e. deciding which agents to allocate to key roles in the team) and role reallocation upon failure remain open challenges. What remain missing are analysis techniques to aid human developers in quantitatively comparing different initial role allocations and competing role reallocation algorithms. To remedy this problem, this paper makes three key contributions. First, the paper introduces RMTDP (Role-based Multiagent Team Decision Problem), an extension to MTDP [9], for quantitative evaluations of role allocation and reallocation approaches. Second, the paper illustrates an RMTDP-based methodology for not only comparing two competing algorithms for role reallocation, but also for identifying the types of domains where each algorithm is suboptimal, how much each algorithm can be improved and at what computational cost (complexity). Such algorithmic improvements are identified via a new automated procedure that generates a family of locally optimal policies for comparative evaluations. Third, since there are combinatorially many initial role allocations, evaluating each in RMTDP to identify the best is extremely difficult. Therefore, we introduce methods to exploit task decompositions among subteams to significantly prune the search space of initial role allocations. We present experimental results from two distinct domains.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The belief-desire-intention (BDI) approach to agent teamwork has led to many practical multiagent applications <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b13">13]</ref>. Ini-tial role allocation, i.e. which agents to allocate to the various roles in the team, and role reallocation upon failures or new tasks, are two continuing challenges for building teams <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b7">7]</ref>. For instance, in mission rehearsal simulations <ref type="bibr" target="#b12">[12]</ref>, we need to select the numbers and types of helicopter agents to allocate to different roles in the team, and to decide how role substitution should occur upon failures. Similarly, in disaster rescue <ref type="bibr">[8]</ref>, initial role allocations (e.g. which brigades for each fire) can greatly impact team performance.</p><p>Critically needed now are analysis techniques to aid human developers in quantitatively comparing and evaluating different initial role allocations and competing role reallocation algorithms. Such evaluations are currently difficult because there are significant uncertainties and costs associated with agents' execution of roles and roles may need to be reallocated upon execution failures. In particular, given domain uncertainty and costs, experimentally evaluating all possible role allocation and reallocation combinations can be expensive or infeasible. Fortunately, the recent emergence of distributed partially observable Markov decision problems (POMDPs) provides quantitative multiagent analysis techniques <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b2">2,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b16">16]</ref> that allow such evaluations. The key idea is to encode different multiagent coordination protocols as policies in distributed POMDPs, and compare them against specific baseline policies to investigate any potential for improvements in the protocols.</p><p>Unfortunately, while the previous distributed-POMDP analysis techniques are powerful, from our perspective they suffer from two key limitations. First, analysis in previous work focused on communication <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b16">16]</ref>, rather than role allocation or any other coordination decisions. To address this limitation, we demonstrate an overall methodology to analyze different aspects of teamwork coordination. In particular, we derive RMTDP (Role-based Multiagent Team Decision Problem), a distributed POMDP framework for analyzing role allocation and reallocation.</p><p>A second limitation is that even after defining a model such as RMTDP, techniques tailored for analysis of role allocation and reallocation are unavailable. Partly, the problem is the difficulty in deriving baseline policies for comparison: as we prove using RMTDP, the derivation of a globally optimal role-allocation policy is NEXPcomplete. Partly, the logical separation of initial role allocation from reallocation in BDI teams needs to be reflected in the analysis. Indeed, in BDI teams, initial role allocation is considered part of the domain-specific organization structure; this interacts with but is separate from role reallocation, which is often part of the domainindependent coordination infrastructure to coordinate the organization <ref type="bibr" target="#b13">[13]</ref>. For example, such separation is clearly seen in the teamoriented program (TOP) approach to building BDI teams <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b13">13]</ref>. Thus, in many instances, analyses must focus on selecting just the right role allocation for a given domain, since the coordination infrastructure is deployed and fixed. In other instances, since external constraints fix the organization structure, analyses should aim to improve an existing, domain-independent role reallocation algorithm within the coordination infrastructure -this improvement must be effective across different domains.</p><p>To address this second limitation, we treat role allocation and reallocation as two separate, interacting design tasks, and tailor analysis techniques for each. Thus, we first analyze role reallocation assuming a fixed initial role allocation. We apply RMTDP to quantitatively compare two published role reallocation algorithms over differing domain conditions -comparisons that were previously very difficult to obtain. We also illustrate a quantitative comparison of these strategies with "locally optimal" baseline policies to identify the types of domains where individual strategies may be suboptimal, how much a strategy can be improved and the computational cost of such improvements. While previous work proposed one restricted kind of "locally optimal" policy as a baseline <ref type="bibr" target="#b9">[9]</ref>, this paper presents: (i) an automated procedure to generate an entire family of locally optimal policies and (ii) a methodology for role reallocation analysis using such policies.</p><p>Next we use RMTDPs to improve initial role allocations, given fixed role reallocation strategies. While we could enumerate each possible role allocation for a given domain and evaluate each as a separate RMTDP policy, combinatorially many role allocations would be evaluated. Instead, we exploit task decomposition among subteams of a team to significantly prune the search space.</p><p>This work is motivated by the needs of role allocation and reallocation in two concrete domains: RoboCupRescue <ref type="bibr">[8]</ref> and mission rehearsal simulation <ref type="bibr" target="#b12">[12]</ref>. We have constructed large-scale teams using BDI approaches for both domains <ref type="bibr" target="#b12">[12]</ref>; in particular, we have used a team-oriented programming approach (TOP) <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b13">13]</ref>, described in Section 2. We assume these BDI systems as a given and apply RMTDP only to analyze them -presenting experimental results from both domains. While we use TOP as an example BDI approach, our methodology applies to other related teamwork approaches that need role (re)allocation as well <ref type="bibr" target="#b11">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">DOMAINS AND MOTIVATION</head><p>For expository purposes, we have intentionally simplified the mission rehearsal domain and describe it first (the more complex RoboCupRescue domain is described later): A helicopter team is executing a mission of transporting valuable cargo from X to Y through enemy terrain (see Figure <ref type="figure">1</ref>). There are three paths from X to Y of different lengths and different risk due to enemy fire. One or more scouting subteams must be sent out, and the larger the size of a scouting subteam the safer it is. When scouts clear up any one path from X to Y, the transports can then move more safely along that path. However, the scouts may fail along a path, and may need to be replaced by a transport at the cost of not transporting cargo. Owing to partial observability, the transports may not receive an observation that a scout has failed or that a route has been cleared. We wish to transport the most amount of cargo in the quickest possible manner within the mission deadline.</p><p>The TOPs for domains such as these consist of three key aspects of a team: (i) a team organization hierarchy consisting of roles; (ii) a team (reactive) plan hierarchy; and (iii) an assignment of roles to execute plans. Thus, the developer need not specify low-level coordination details. Instead the TOP interpreter (the underlying coordination infrastructure) automatically enables agents to decide when and with whom to communicate and how to reallocate roles upon failure. In the TOP for this example, we first specify the team organization hierarchy (see Figure <ref type="figure" target="#fig_0">2(a)</ref>). Task Force is the highest level team in this organization and consists of two roles Scouting Reactive team plans explicitly express joint activities of the relevant team and consist of: (i) initiation conditions under which the plan is to be proposed; (ii) termination conditions under which the plan is to be ended; and (iii) team-level actions to be executed as part of the plan. In Figure <ref type="figure" target="#fig_0">2</ref>(b), the highest level plan Execute Mission has three subplans: DoScouting to make one path from X to Y safe for the transports, DoTransport to move the transports along a scouted path, and RemainingScouts for the scouts which haven't reached the destination yet to get there.</p><p>Figure <ref type="figure" target="#fig_0">2</ref>(b) also shows coordination relationships: An AND relationship is indicated with a solid arc, while an OR relationship is indicated with a dotted arc. Thus, DoScouting, DoTransport and RemainingScouts must all three be done while at least one of UseRoute1, UseRoute2 or UseRoute3 need be performed. There is also a temporal dependence relationship among the subplans, which implies that subteams assigned to perform DoTransport or RemainingScouts cannot do so until the DoScouting plan has completed . However, DoTransport and RemainingScouts execute in parallel. Finally, we assign roles to plans -Figure <ref type="figure" target="#fig_0">2</ref>(b) shows the assignment in brackets adjacent to the plans. For instance, Task Force team is assigned to jointly perform Execute Mission.</p><p>This example scenario helps explain the key challenges faced in role allocation and reallocation. First, a human developer must allocate available agents to the organization hierarchy (Figure <ref type="figure" target="#fig_0">2(a)</ref>). However, there are combinatorially many allocations to choose from <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b12">12]</ref>. For instance, starting with even 6 homogeneous helicopters results in 84 different ways of deciding how many agents to assign to each scouting and transport subteam. This problem is exacerbated by the fact that the best allocations varies significantly based on domain variations, e.g. Figure <ref type="figure" target="#fig_2">3</ref> shows three different assignments of agents to the team organization hierarchy, each found in our analysis to be the best for a given setting of probability of helicopter failures (details in Section 6). For example, interchanging the probability of failures for routes 2 and route 3 resulted in the number of transports in the best allocation changing from 3 (see Figure <ref type="figure" target="#fig_2">3(b)</ref>) to 4 (see Figure <ref type="figure" target="#fig_2">3(c)</ref>). If the probability of failure on route 3 was reduced further, the number of transports increased to 5 (see Figure <ref type="figure" target="#fig_2">3(a)</ref>). Furthermore, what reallocation algorithm to use to reallocate transports to scouting role, is a critical challenge for the TOP interpreter (coordination infrastructure). For instance, should the algorithm require all scouts to fail before role reallocation, or would a more pre-emptive reallocation approach work  better for this given range of domains? Our analysis takes a step towards answering the above questions.</p><p>The second example scenario, set up in the RoboCupRescue disaster simulation environment <ref type="bibr">[8]</ref>, consists of 7 fire brigades at three different fire stations (2 each at stations 1 &amp; 2 and the rest at station 3) and 5 ambulances stationed at the ambulance center. Two fires start that need to be extinguished by the fire brigades. After a fire is extinguished, ambulance agents need to save the surviving civilians. As time passes, the health of civilians deteriorates and fires increase in intensity and so the goal is to rescue as many civilians as possible with minimal damage to the buildings. Here, partial observability (each agent can only view objects within its visual range), and large action uncertainty add significantly to the difficulty.</p><p>The plan hierarchy for this scenario, consists of two Execute-Rescue plans executed in parallel, one for each of the fires. Each such plan consists of a ExtinguishFire plan and a RescueCivilians plan, which further decompose into individual plans. The organizational hierarchy consists of Task Force comprising of two Rescue subteams, one for each fire. Each such subteam is comprised of a Brigade Team and an Ambulance Team, where the brigade team is assigned to extinguishing the fire while the ambulance team is assigned to rescuing civilians. The problem is which brigades and ambulances to assign to each Brigade Team and Ambulance Team. Note that brigades have differing capabilities due to differing distances from fires.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MULTIAGENT TEAM DECISION PROB-LEM</head><p>For quantitative analysis of role allocation and reallocation, we extend the Multiagent Team Decision Problem (MTDP) <ref type="bibr" target="#b9">[9]</ref>. While our extension focuses on role (re)allocation, it also illustrates a general methodology for analysis of other aspects of team coordination. Note that, while we use MTDP, other possible distributed POMDP models could potentially also serve as a basis <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b16">16]</ref>.</p><p>Given a team of agents α, an MTDP <ref type="bibr" target="#b9">[9]</ref> is defined as a tuple: S, A, P, Ω, O, R . It consists of a finite set of states S = Ξ1 × • • • × Ξn. Each agent i can perform an action from its set of actions Ai. P (s, &lt; a1, . . . , a |α| &gt;, s ) gives the probability of transitioning from state s to state s given that the agents perform the actions &lt; a1, . . . , a |α| &gt; jointly. Each agent i receives an observation ωi ∈ Ωi based on the function O(s, &lt; a1, . . . , a |α| &gt; , ω1, . . . , ω |α| ), which gives the probability that the agents receive the observations, ω1, . . . , ω |α| given that the world state is s and they perform &lt; a1, . . . , a |α| &gt; jointly. The agents receive a single joint reward R(s, a1, . . . , a |α| ).</p><p>The state of the world, s need not be observable to the agent. Thus, each agent i chooses its actions based on its local policy, πi, which is a mapping of its observation history to actions. Thus, at time t, agent i will perform action πi(ω 0 i , . . . , ω t i ). π =&lt; π1, . . . , π |α| &gt; refers to the joint policy of the team of agents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Extension for explicit coordination:RL</head><p>Beginning with MTDP, the next step in our methodology is to make an explicit separation between domain-level actions and the coordination actions of interest. Earlier work introduced the COM-MTDP model <ref type="bibr" target="#b9">[9]</ref> where the coordination action was fixed to be the communication action. However, other coordination actions could also be separated from domain-level actions in order to investigate their impact. Thus, to investigate role allocation and reallocations, actions for allocating agents to roles and to reallocate such roles are separated out. To that end, we define RMTDP (Role-based Multiagent Team Decision Problem) as a tuple, S, A, P, Ω, O, R, RL with a new component, RL. In particular, RL = {r1, . . . , rs} is a set of all roles that the agents can undertake . Each instance of role rj may be assigned some agent i to fulfill it. Agents' actions are now distinguishable into two types:</p><p>Role-Taking actions: Υ = i∈α Υi is a set of combined roletaking actions, where Υi = {υir j } contains the role-taking actions for agent i. υir j ∈ Υi means that agent i takes on the role rj ∈ RL.</p><p>Role-Execution Actions: Φ = i∈α Φi is a set of combined execution actions, where Φi = ¡ ∀r j ∈RL Φir j contains the execution actions for agent i. Φir j is the set of agent i's actions for executing role rj ∈ RL Thus, in RMTDP, successive epochs alternate between role-taking (Υ) and role-execution actions(Φ). If the time index is divisible by 2, agents are in the role-taking epoch, executing role-taking actions, and otherwise they are in the role-execution epoch. Although this sequencing of role-taking and role-execution epochs restricts different agents from running role-taking and role-execution actions in the same epoch, it is conceptually simple and synchronization is automatically enforced. More importantly, the distinction between role-taking and -execution actions is critical to enable a separation in their costs, so as to more easily analyze the costs of role-taking actions. To this end, in RMTDP, reward is role-taking reward, RΥ(s, a1, . . . , a |α| ), for even time indices and role-execution reward, RΦ(s, a1, . . . , a |α| ), otherwise. We view the role-taking reward as the cost (negative reward) for taking up different roles in different teams. For instance, in our example domain, when transports change roles to be scouts, there is cost for dumping its cargo and loading scout equipment. However, such change of roles may potentially provide significant future rewards.</p><p>Within this model, we can represent the specialized behaviors associated with each role, e.g. a transport vs. a scout role. While filling a particular role, rj, agent i can only perform role-execution actions, φ ∈ Φir j , which may be different from the role-execution actions Φir l for role r l . These different roles can produce varied effects on the world state (modeled via transition probabilities, P ) and the team's utility. Thus, the policies must ensure that agents for each role have the capabilities that benefit the team the most.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Complexity results with RMTDP</head><p>While previous sections qualitatively emphasized the difficulty of role (re)allocation, RMTDP helps in understanding the complexity more precisely. In particular, we can define a role-taking policy, πiΥ for each agent's role-taking action, a role-execution policy, πiΦ for each agent's role-execution action. The goal in RMTDP is then to come up with joint policies πΥ and πΦ that will maximize the total reward over a finite horizon T . Such an optimal role taking policy not only provides for role allocation, but it also takes into account optimal future role reallocations. The following theorem illustrates the complexity of finding such optimal joint policies. THEOREM 1. The decision problem of determining if there exist policies, πΥ and πΦ, for an RMTDP, that yield a reward at least K over some finite horizon T is NEXP-complete.</p><p>PROOF. Proof follows from the reduction of MTDP <ref type="bibr" target="#b9">[9]</ref> to/from RMTDP. To reduce MTDP to RMTDP, we set RMTDP's role taking actions, Υ to null. To reduce RMTDP to MTDP, we generate a new MTDP whose state space contains an additional feature to indicate if the current state corresponds to a role-taking or -executing stage of the RMTDP. The transition function, P , augments the original function P : P ( ξ 1b , . . . , ξ nb , taking , υ1, . . . , υ |α| , ξ1e, . . . , ξne, executing ) =P ( ξ 1b ,. . . , ξ nb ,υ1,. . . , υ |α| , ξ1e, . . . , ξne ) where υ1, . . . , υ |α| is a role-taking action in the RMTDP(similarly from executing to taking). Finding the required policy in MTDP is NEXP-complete <ref type="bibr" target="#b9">[9]</ref>.</p><p>While the previous theorem focused on the complexity of combined role-taking and role execution actions, we can focus on the complexity of just determining the role taking actions, given fixed role-execution actions. THEOREM 2. The decision problem of determining if there exists a role-taking policy, πΥ, for an RMTDP, that yields a reward at least K together with a fixed role-execution policy πΦ, over some finite horizon T is NEXP-complete.</p><p>PROOF. We reduce an MTDP to an RMTDP with a different role-taking action for every action in the MTDP. The role-execution policy is to perform the action corresponding to the current role.</p><p>Note that Theorem 2 refers to a completely general globally optimal role-taking policy, where any number of agents can change roles at any point in time. Given the above result, in general the globally optimal role-taking policy will be of doubly exponential complexity, and so we may be left no choice but to run a bruteforce policy search, i.e. to enumerate all the role-taking policies and then evaluate them, which together determines the run-time of finding the globally optimal policy. The number of policies is</p><formula xml:id="formula_0">|Υ| |Ω| T -1 |Ω|-1 ¡</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>|α|</head><p>, i.e. doubly exponential in the finite horizon and the number of agents. This clearly illustrates the point made in Section 1, that the search for a globally optimal policy is intractable.</p><p>Note that, in the worst case, cost of evaluating a single policy can be given by O ¢ (|S| • |Ω|) T £ <ref type="bibr" target="#b9">[9]</ref>. We will in general assume a fixed procedure for policy evaluation and primarily focus on the number of policies being evaluated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Constructing an RMTDP</head><p>Constructing an RMTDP for evaluating a TOP is a key step in our approach. To that end, we must define each of the elements of the RMTDP tuple, specifically, S, A, P, Ω, O, R, RL , by a process that relies on both the TOP plans as well as the underlying domain. While this step has not been automated, we briefly describe mapping techniques based on the work on our two domains.</p><p>First, we need to define the set of states S. To this end, it is critical to model the variables tested in the preconditions and termination conditions of the TOP plans. For complex domains, it is useful to consider abstract descriptions of the state modeling only the significant variables. Agents' role-taking and -execution actions in RMTDP are defined as follows. For each role in the TOP organization hierarchy, we define a role-taking action in each state s. The role-execution actions are those allowed for that role in the TOP plan hierarchy given the variable values in state s.</p><p>To illustrate these steps, consider the plans in Figure <ref type="figure" target="#fig_0">2</ref>(b). The preconditions of plans such as UseRoute1 and others test the start location of the helicopters to be at start location X, while the termination conditions test that scouts are at end location Y. Thus, the locations of all the helicopters are critical variables modeled in our set of states S. For role-taking, each helicopter can perform one of four actions, i.e. being a member of one of the three scouting teams or of the transport team. Role-execution actions are the TOP actions for the plan that the agent's role is assigned in the TOP. In our case, the role execution policy for the scout role is to always go forward until it reaches Y, while for the transport role the policy is to wait at X until it obtains observation of a signal that one scouting subteam has reached Y.</p><p>Further, the types of observations for each agent must be defined. We define the set of observations to be the variables tested in the preconditions and termination conditions of the TOP plans and individual agent plans. For instance, the transport helos may observe the status of scout helos (normal or destroyed), as well as a signal that a path is safe. Finally, we must define the transition, observation and reward functions. Determining these functions requires some combination of human domain expertise and empirical data on the domain behavior. However, as shown later in Section 6, even an approximate dynamic and observational model, is sufficient to deliver significant benefits. Defining the reward and transition function may sometimes require additional state variables to be modeled. In our helicopter domain, the time at which each the scouting and transport mission was completed determined the amount of reward and hence time was included as a state variable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">ANALYSIS OF ROLE REALLOCATION</head><p>This section and the next one will now illustrate how RMTDP could be applied for iterative improvements in role allocation (in the TOP) and role reallocation algorithms (in the TOP interpreter). In this section, we focus on selecting the right reallocation strategy for a range of domains of interest -an important issue for developers of BDI coordination infrastructures. We show the application of RMTDP in quantitatively contrasting alternative reallocation strategies used in BDI systems and present an automated approach to suggest improvements to those strategies.</p><p>Our approach starts from the perspective of how BDI coordination actions, reallocation in particular, actually work. For illustration, we consider the reallocation strategy in the STEAM "interpreter", given that STEAM has been applied in several real domains <ref type="bibr" target="#b12">[12]</ref>. Inspired by the SharedPlans theory <ref type="bibr" target="#b5">[5]</ref>, this reallocation strategy focuses on role replacement, where a failed agent must be replaced by another. Such events, as the failure of an agent, often trigger a BDI system's "local" decision on whether to reallocate roles. Thus, in STEAM, given a trigger of a failure of an agent F , an agent R will decide to replace a failed agent F only if the following inequality holds:</p><formula xml:id="formula_1">Criticality (Role F ) -Criticality (Role R ) &gt; 0 (1) Criticality (x) = 1 if x is critical; = 0 otherwise</formula><p>Replacement occurs if F 's role is considered critical and R's role is not critical. Thus STEAM's classification of reallocation triggers is role-failure-critical or role-failure-not-critical.</p><p>Is this STEAM approach guaranteed to be better than other related approaches in other BDI systems? For instance, the role exchange strategy of the FC Portugal RoboCup soccer team <ref type="bibr" target="#b10">[10]</ref> allows two agents to exchange roles, thus performing a pairwise reallocation of roles. Their approach employs a utility calculation based on the tradeoffs between the assumed rewards/costs of each agent taking on the other's role versus not doing this exchange. Exchanges are triggered when the two agents agree that the payoff is positive. Assuming the two agents are A and B:</p><formula xml:id="formula_2">U tility (Role B , A) + U tility (Role A , B) -U tility (Role A , A) -U tility (Role B , B) &gt; 0<label>(2)</label></formula><p>Such an approach can be straightforwardly adapted to role reallocation upon failures. Agent A would replace a failed agent B if the  following inequality holds (where utility of a failed agent is 0):</p><formula xml:id="formula_3">U tility (Role B , A) -U tility (Role A , A) &gt; 0<label>(3)</label></formula><p>RMTDP analysis can be used to quantitatively contrast such alternative role reallocation strategies and determine under what condition one is preferable. Furthermore, we can compare these strategies with a "locally optimal" policy to determine the types of domains where improvements are possible in each strategy and by how much. A locally optimal policy is one that considers reallocations at exactly the same junctures (reallocation triggers) as the strategy it is being compared with (as opposed to a globally optimal policy that may consider reallocations at other times). The derivation of alternative local optimal policy provides a means to check if, for a trigger like role failure, whether the BDI system's behavior could be improved or not. In defining such trigger events for RMTDP analysis, the classification can be more fine-grained than the BDI system being analyzed, e.g. first-role-failure vs. secondrole-failure vs. third-failure, etc. The finer grain allows the analysis to uncover cases where the BDI's decision-making is doing well versus not so well.</p><p>To further facilitate the analysis, we have automated the process of deriving these alternative locally optimal policies. Earlier methodology, as specified in <ref type="bibr" target="#b9">[9]</ref> dictated that we first derive an algorithm for a "locally optimal" policy by hand. However, there are two problems in this derivation: (i) Deriving such a complex expression by hand is cumbersome and hinders analysis; (ii) The focus there remained on a single trigger, and no guidance is provided on multiple triggers. It is possible to automatically generate various locally optimal policies by replacing or perturbing the response of a particular reallocation trigger. For example, our approach can perturb STEAM's response to the first and second failure that occurs by replacing it with an optimal RMTDP policy that is derived under the assumption that the response to all other triggers remains the same as STEAM's. Furthermore, the response may also vary with time, even for a single trigger. Indeed, Figure <ref type="figure" target="#fig_3">4</ref> presents such an algorithm that automatically generates a locally optimal replacement policy. In this algorithm, we require a list of trigger events and a specific agent that would respond to each trigger. For each decision epoch and trigger, the responding agent chooses between performing "respond" (i.e. replace) or "dontRespond". We can obtain a family of locally optimal policies by varying the list of triggers.</p><p>Deriving a local optimal policy achieves considerable computational savings over the global optimal. Recall derivation of the The number of policies that need to be considered for a locally optimal policy depends on the how many triggers it considers. For example, 2 T policies have to be evaluated for a locally optimal policy that varies its response depending on what time the trigger (only one trigger) occurred, while for deriving a locally optimal policy that varies its response depending on both time of the trigger and which trigger (1st, 2nd, etc.), it has to evaluate 2 |triggers| * T policies, as shown in Figure <ref type="figure" target="#fig_3">4</ref>.</p><p>To demonstrate the empirical utility of automated RMTDP reallocation analysis, consider the helicopter domain from Section 2. These agents must decide whether to do a role replacement when a failure occurs. We compare the performance of various policies, across a space of distinct domains obtained by varying the replacement cost and probability of failure. For this experiment, we start with 6 helicopters and various starting role allocations. Here, we will discuss the results for an allocation of 3 scouts (all assigned to path 2) and 3 transports. The results for other allocations are similar. When a scout is replaced by a transport, a Role replacement cost is incurred. (We assume that there is an ordering that determines which transport will perform a role replacement.) To ensure a focus on role replacement, we assume that for all approaches, the policies for role-execution and communication are the same, as given in Section 3.3. The reward is higher if more transports reach the destination and if they reach early rather than late.</p><p>We compared the performance of 4 policies. In the STEAM policy, the transports use the inequality1 to determine whether to replace a failed scout. In STEAM, failure of the last remaining scout would be seen as critical and all other roles as non-critical. In the policy we call FCP helo (based on <ref type="bibr" target="#b10">[10]</ref>), inequality 3 is used to determine whether to replace a failed scout. The FCP helo policy would replace a failed scout by a transport if the utility for the successful completion of the scouting mission and transport mission exceeds the replacement cost and the loss of reward because there is one less transport (under the assumption of no subsequent failures). PertSTEAM2 and PertSTEAM3 are locally optimal perturbations of the STEAM policy, generated automatically by running the algorithm in Figure <ref type="figure" target="#fig_3">4</ref>, considering the trigger to be the 2nd and 3rd failure respectively. The above 4 policies were then compared to a benchmark policy, generated from Figure <ref type="figure" target="#fig_3">4</ref> with list of all failures as the list of triggers. As discussed earlier, such a policy is more expensive to compute than PertSTEAM2 and PertSTEAM3, given that it varies its response depending on which trigger it is (1st, 2nd, etc). Note that this is still cheaper than the globally optimal policy.</p><p>In Figure <ref type="figure" target="#fig_4">5</ref> we compare the sub-optimality of various replacement polices. In Figure <ref type="figure" target="#fig_4">5</ref>(a), we varied the replacement cost, keeping the probability of failure fixed at 0.1. In Figure <ref type="figure" target="#fig_4">5</ref>(b) we varied the probability of failure keeping the replacement cost fixed at 12. In both figures we plot the sub-optimality with respect to the bench-mark policy, Vopt -V , on the Y-axis (lower values are better). The two graphs identify that:</p><p>• In domains with low replacement cost or low probability of failure, STEAM is close to benchmark, suggesting that a more expensive reallocation strategy is not required. • If we must choose between STEAM and FCP helo , then for domains with low probability of failure and low cost, use STEAM else we use FCP helo . • STEAM's response to second failure is optimal since STEAM and PertSTEAM2 exhibit identical performance in both graphs. • However, in both graphs, PertSTEAM3 does better than the other 3 policies, clearly identifying room for improvement.</p><p>In particular, on third failures, STEAM will always replace and thus, in both high replacement cost and failure rate domains, must be over-eager to replace, even when helicopters will not be able to meet the mission deadline. • STEAM would be improved by factoring in replacement cost and failure rate.</p><p>In summary, RMTDP analysis has identified where the two BDI strategies do well, where they do more poorly as well as which decisions and factors need to be taken into account in order to improve them. It does this analysis both in terms of contrasting the strategies in relation to each other and in relation to local optimums. Finally, it provides a worst case estimate of the cost of making the improvements, via the cost of deriving the local optimum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">ANALYSIS OF ROLE ALLOCATION</head><p>While the previous section focused on analysis of role reallocation in the TOP interpreter, this section focuses on role allocation done in the TOP itself. As mentioned earlier, role allocation focuses on deciding how many and what types of agents to allocate to different roles in the organization hierarchy. Figure <ref type="figure" target="#fig_6">6</ref> shows a partially expanded role allocation space defined by the TOP organization hierarchy in Figure <ref type="figure" target="#fig_0">2</ref> for 6 helicopters. Each node of the role allocation space completely specifies the allocation of agents to roles at the corresponding level of the organization hierarchy (Ignore for now, the number to the right of each node). For instance, the root node of the role allocation space specifies that 6 helicopters are assigned to the Task Force (level 0) of the organization hierarchy while the leftmost leaf node (at level 2) in Figure <ref type="figure" target="#fig_6">6</ref> specifies that 1 helicopter is assigned to SctTeamA, 0 to SctTeamB, 0 to Sct-TeamC and 5 helicopters to Transport Team. Thus as we can see each leaf node in the role allocation space is a complete, valid role allocation of agents to roles in the organization hierarchy.</p><p>In order to determine if one leaf node (role allocation) is superior to another we compare by constructing an RMTDP policy for each leaf. The role allocation specified by the leaf node corresponds to the role-taking actions that each agent will execute at time=0. For example, in the case of the left most leaf in Figure <ref type="figure" target="#fig_6">6</ref>, at time 0, one agent (recall from Section 2 that this is a homogeneous team and hence which specific agent does not matter) will become a member of SctTeamA while all other agents will become members of Transport Team. The rest of the role-taking policy will be the role replacement policy determined in Section 4, i.e. for the range of domains of interest it is the STEAM policy. Each agent's roleexecution policies are determined by the plan associated to their role. Thus, we have been able to construct a policy for the RMTDP that corresponds to the role allocation.</p><p>We could do a brute force search through all role allocations, evaluating each in order to determine the best role allocation. However, the number of possible role allocations is exponential in the  leaf roles in the organization hierarchy. Thus, we must prune the search space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Pruning the role allocation space</head><p>We prune the space of valid role allocations using heuristic overestimates (max estimates) for the parents of the leaves of the role allocation space (Section 5.2). In particular, once we obtain max estimates for all the parent nodes (shown in brackets to the right of each parent node in Figure <ref type="figure" target="#fig_6">6</ref>), we use branch-and-bound style pruning. First, we sort the parent nodes by their estimates and then start evaluating children of the parent with the highest max estimate. In the case of the role allocation space in Figure <ref type="figure" target="#fig_6">6</ref>, we would start with evaluating the leaves of the parent node that has 1 helicopter in Scouting Team and 5 in Transport Team. The value of evaluating each leaf node is shown to the right of the leaf node. Once we have obtained the value of the best leaf node, in this case 1500.12, we compare this with the max estimates of the other parents of the role allocation space. As we can see from Figure <ref type="figure" target="#fig_6">6</ref> this would result in pruning of 3 parent nodes (left most parent and right two parents). Next, we would then proceed to evaluate all the leaf nodes under the parent with 2 helos in Scouting Team and 4 in Transport Team. This would result in pruning of all the remaining unexpanded parent nodes and we will return the leaf with the highest value, which in this case is the node corresponding to 2 helos allocated to Sct-TeamA and 4 to Transport Team. Although demonstrated for a 3level hierarchy, extending to deeper hierarchies is straightforward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Calculating over-estimates for parents</head><p>We will now discuss how the max estimates can be calculated for each parent. The max estimate of a parent must necessarily be an overestimate of the maximum expected reward of all the leaf nodes under it or else we might end up pruning potentially useful role allocations. In order to calculate the max estimate of each parent we could evaluate each of the leaf nodes below it using the RMTDP, but this would nullify the benefit of any subsequent pruning. We, therefore turn to the plan hierarchy (see Figure <ref type="figure" target="#fig_0">2(b)</ref>) to see how this evaluation of the parent node can be broken up into components, which can be evaluated separately thus decomposing the problem. Our approach exploits the structure of the BDI program to construct small-scale RMTDPs, unlike other decomposition techniques <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b6">6]</ref>. For each parent in the role allocation space, we use these RMTDPs to evaluate the values for each component. The values of the components of each parent can be added to obtain its max estimate (an upper bound on its children's values).</p><p>As shown in Figure <ref type="figure" target="#fig_7">8</ref>, the first step in our approach involves deciding how to decompose the plan hierarchy into components and then create smaller RMTDPs, one for each component. We explain this methodology using the plan hierarchy in Figure <ref type="figure" target="#fig_0">2(b)</ref>. Using the temporal constraints in the plan hierarchy, we choose a level of the plan hierarchy at which to do a decomposition. For example, since temporal constraints exist between DoScouting, DoTransport and RemainingScouts, we choose these as the components for our max estimation. The process of constructing an RMTDPs for a compo-nent is similar to the method described in Section 3.3. In particular, we determine the set of state variables relevant to the component that it corresponds to, e.g. those variables that are present in the preconditions and termination conditions of this component are clearly relevant. Fortunately, all state variables that are irrelevant to the component can be eliminated. For example, in the DoTransport component, only the variables that refer to number of members of Transport Team, the locations of each of these transports and the route that they should follow (the scouted route) are relevant. Information about the other scouts is not important to this component and can be eliminated from the state.</p><p>After constructing RMTDPs for each component, we evaluate the max estimate for each parent node in the role allocation space (See Figure <ref type="figure" target="#fig_7">8</ref>). First, we identify the start states for each component from which to evaluate the RMTDPs. We explain this step using a parent node from Figure <ref type="figure" target="#fig_6">6</ref> -Scouting Team = 2 helos, Transport Team = 4 helos (see Figure <ref type="figure">7</ref>). For the very first component, the start states corresponds to all the role allocations under the parent node. As shown in Figure <ref type="figure">7</ref>, the start states of the DoScouting component for this parent, correspond to all possible role allocations of 2 helos to Scouting Team and 4 helos to Transport Team, e.g. 1 helo to SctTeamB, 1 helo to SctTeamC and 4 helos to Transport Team. The role allocation corresponding to a start state tells the agents what role to take in that start state. The remainder of the role-taking policy is specified by the role replacement policy. For each of the next components -where the next component is one linked by a sequential dependence -the start states are the end states of the preceding component. However, as explained later in this section, we can significantly reduce this list of start states from which each component can be evaluated. Note, if the next component is not sequentially dependent on the prior one, then its start states are determined from its own children.</p><p>Similarly, the starting observation histories for a component are the observation histories on completing the preceding component (no observation history for the very first component). BDI plans do not normally refer to entire observation histories but rely only on key observations which are typically referred to in the preconditions of the component. Each starting observation history can be shortened to include only these relevant observations, thus obtaining a reduced list of starting observation sequences.</p><p>In order to obtain the max estimate for a parent node of the role allocation space, we simply sum up the maximum of the evaluation for each component over all its start states and starting observations. E.g. the maximum values of each component (see right of each component in Figure <ref type="figure">7</ref>) were summed to obtain the max estimate (84 + 3330 + 36 = 3420). The calculation of the max estimate for a parent nodes should be much faster than evaluating the leaf nodes below it in most cases for three reasons. Firstly, parent nodes are evaluated component-wise. Thus, if multiple start states result in the same end state, we can remove duplicates to get the start states of the next component. This prevents a lot of duplication of the evaluation effort, something that cannot be avoided for leaf nodes, where each state is evaluated independently from start to finish. Secondly, since each component only contains the state variables relevant to it, we can further reduce the set of start states drastically. As seen in Figure <ref type="figure">7</ref>, the start states of the DoTransport component only considers the scouted route and number of transports (some transports may have replaced failed scouts), thus greatly reducing the set of end states of DoScouting component. Finally, the number of starting observation sequences will be much less than the number of ending observation histories of the preceding components.</p><p>We refer to this methodology of obtaining the max estimates of  each parent as MAXEXP. A variation of this, the maximum expected reward with no failures (NOFAIL), is obtained in a similar fashion except that we assume that the probability of any agent failing is 0. This will result in less branching and hence evaluation of each component will proceed much quicker. The NOFAIL heuristic only works if the evaluation of any policy without failures occurring is higher than the evaluation of the same policy with failures possible. This should normally be the case in most domains. The evaluation of the NOFAIL heuristics for the role allocation space for 6 helicopters is shown in square brackets in Figure <ref type="figure" target="#fig_6">6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">EXPERIMENTAL RESULTS</head><p>For the two domains introduced in Section 2, helo and RoboCupRescue <ref type="bibr">[8]</ref>, we focus on determining the best initial assignment of agents to roles; but assume a fixed TOP and role replacement strategy (we use the STEAM policy from Section 4). Thus, the initial role assignment considers future role replacements, as discussed in Section 1. For the helicopter domain, the TOP is the one discussed in Section 2. As can be seen in Figure <ref type="figure" target="#fig_0">2</ref>(b), the organization hierarchy requires determining the number of agents to be allocated to the three scouting subteams and the remaining helos must be allocated to the transport subteam. Different numbers of initial helicopters were attempted, varying from 3 to 10.</p><p>Figure <ref type="figure" target="#fig_8">9</ref> shows the results of comparing the different methods for searching the role allocation space; in particular, we show results from MAXEXP, NOFAIL and NOPRUNE (brute force). In Figure <ref type="figure" target="#fig_8">9</ref>(a), the Y-axis is the number of nodes in the role allocation space evaluated, while in Figure <ref type="figure" target="#fig_8">9</ref>(b) the Y-axis represents the runtime in seconds on a logarithmic scale. In both figures, we vary the number of agents on the X-axis. Figure <ref type="figure" target="#fig_8">9</ref>(a) clearly shows significant reductions over NOPRUNE in the numbers of nodes evaluated due to the pruning by MAXEXP and NOFAIL. This reduction grows quadratically to more than 20-fold at 10 agents. Note that the NOFAIL heuristic results in less pruning than MAXEXP for 9 and 10 agents because, although it is cheaper to evaluate, its estimate is higher than the MAXEXP estimate. Figure <ref type="figure" target="#fig_8">9</ref>(b) shows how the MAX-EXP heuristic results in a 14-fold speedup over the NOPRUNE in the 10 agent case. The NOFAIL heuristic, which is very quick to compute the max estimates, far out performs the MAXEXP heuristic (180-fold speedup over MAXEXP for 10 agents). Speedups of MAXEXP and NOFAIL continually increase with increasing number of agents. The speedup of the NOFAIL method over MAXEXP is so marked because, in this domain, ignoring failures results in  much less branching. There were different best role allocations for different settings of the probability of failure, as shown in Figure <ref type="figure" target="#fig_2">3</ref>, for 6 helos, indicating the difficulty of finding the best allocation. Our next set of experiments shows the practical utility of our role allocation analysis in complex domains. We are able to show significant performance improvements in the actual RoboCupRescue domain using the role allocations generated by our analysis. First, we construct an RMTDP for the rescue scenario, described in Section 2, by taking guidance from the TOP and the underlying domain (as described in Section 3.3). We then use the NOFAIL heuristic to determine the best role allocation -NOPRUNE could not be run because of its slowness. The best allocation recommended assigning three ambulances and three fire brigades(2 from station1 and 1 from station2) for the first fire and the remaining agents to the second fire. We tested this best allocation in the actual RoboCup Rescue simulation. For comparison, we considered two alternate allocations. Alternate 1 was the next best allocation with a significant value difference (RMTDP ranked four allocations to be extremely close in value, so we consider the fifth). It allocated 3 ambulances and 2 fire brigades (1 from station1 and 1 from station3) for first fire, remaining agents to second fire. While, alternate 2 was an allocation that RMTDP predicted would perform poorly -2 ambulances and 2 fire brigades (both from station1) for first fire, remaining agents to second fire. In Table <ref type="table" target="#tab_2">1</ref>, we show how the best role allocation compared to the alternate allocations. The best allocation resulted in 6 survivors out of 7 trapped civilians, while the alternate allocations results in 4 and 2 survivors, respectively. Also, the best allocation resulted in less percentage damage to the buildings due to fire and received a better (lower) RoboCupRescue score, which considers civilian lives lost, damage to buildings and injuries (most weight for lives lost).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSION AND RELATED WORK</head><p>While the BDI approach to agent teamwork remains the most successful within multiagents, initial role allocation and reallocation remain open challenges. Critically needed now are analysis techniques to help human developers in quantitatively comparing different initial role allocations and competing role reallocation algorithms. To this end, this paper provided three key contributions. First, it introduced RMTDP, a distributed POMDP based framework, for analysis of role (re)allocation, generalizing prior work on analysis of communication. RMTDP analysis enables a virtuous cycle of improvements to role allocation and role reallocation to ensue. Second, for role reallocation, the paper illustrated a methodology not only in comparing two competing algorithms, but also identified the types of domains where each is suboptimal, how much each algorithm can be improved and at what computational cost (complexity). Such algorithmic improvements are identified via a new automated procedure that generates a family of locally optimal policies for comparative evaluations. Third, given the combinatorially many initial role allocations, we introduced methods to exploit task decompositions among subteams to significantly prune the search space of initial role allocations. We presented results from two distinct domains to illustrate our methodology.</p><p>While the work used team-oriented programming <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b14">14</ref>] as an example BDI approach, it is relevant to other similar techniques of modeling and tasking collectives of agents, such as Lesser et al's <ref type="bibr" target="#b4">[4]</ref> TAEMS approach. In other related work, role allocation based on matching of capabilities <ref type="bibr" target="#b15">[15]</ref> and combinatorial auctions <ref type="bibr" target="#b7">[7]</ref> has been proposed earlier. The key difference with prior work is our use of stochastic models (RMTDPs) to evaluate allocations: this enables us to compute the benefits of role allocation, taking into account costs of reallocation upon failure. Our key contributions focused on improving the efficiency of this stochastic analysis. Finally, in terms of MDP research, MTDP is itself identical to the DEC-POMDP <ref type="bibr" target="#b1">[1]</ref> model; and could be seen to generalize Boutilier's MMDP <ref type="bibr" target="#b2">[2]</ref> model. RMTDP extends MTDP to analyze role allocation and reallocation in BDI teams, which has required the development of novel, practical techniques for analysis.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 1: Helicopter Domain Task Force Scouting Team Transport Team SctTeamA SctTeamB SctTeamC</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Best role allocations for different settings</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Locally optimal policy generation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Sub-optimality of replacement policies, a: Varying replacement cost and b: Varying probability of failure "globally optimal" policy must evaluate |Υ| |Ω| T -1 |Ω|-1 ¡ |α| policies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Partially expanded role allocation space (6 helos)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Calculating over-estimates for parents.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Performance of role allocation space search, a: Number of nodes evaluated, and b: Run-time in seconds</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>01. epochs ← {0, . . . , T }; policySpace ← null 02. for each agent decision epoch t &lt;= T 03.for each joint obs. history ω 0 . . . ω t-1</figDesc><table><row><cell>04.</cell><cell>for each policy π in policySpace</cell></row><row><cell>05.</cell><cell>π ← π; π ← π</cell></row><row><cell>06.</cell><cell>for each joint observation ω t</cell></row><row><cell>07.</cell><cell>for each trigger in trigger list</cell></row><row><cell>08.</cell><cell>if trigger.triggered(ω 0 . . . ω t )=true</cell></row><row><cell>09.</cell><cell>i ← trigger.respondingAgent</cell></row><row><cell>10. 11.</cell><cell>π i [ω 0 i . . . ω t i ] ← respond π i [ω 0 i . . . ω t i ] ← dontRespond</cell></row><row><cell>12.</cell><cell>policySpace ← policySpace.add(π )</cell></row><row><cell>13.</cell><cell>policySpace ← policySpace.add(π )</cell></row><row><cell>14.</cell><cell>else</cell></row><row><cell>15.</cell><cell>π i [ω 0 i . . . ω t i ] ← π original [ω 0 i . . . ω t i ]</cell></row><row><cell>16.</cell><cell>policySpace ← policySpace.add(π )</cell></row><row><cell>17.</cell><cell>policySpace ← policySpace.remove(π)</cell></row><row><cell cols="2">18. return best(policySpace)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 : Performance of role allocations in RoboCupRescue</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell cols="3">Civilians saved Bldg. damage(%) RoboCupRescue</cell></row><row><cell>Best Alloc.</cell><cell>6</cell><cell>0.10</cell><cell>1.17</cell></row><row><cell>Alt. Alloc. 1</cell><cell>4</cell><cell>1.42</cell><cell>3.29</cell></row><row><cell>Alt. Alloc. 2</cell><cell>2</cell><cell>8.18</cell><cell>5.42</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement: Thanks to J. Blythe, A. Cassandra, H. Jung, S. Kapetanakis, S. Koenig, M. Littman, D. Pynadath and P. Scerri for valuable discussions. This research was supported by NSF grant #0208580.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The complexity of decentralized control of MDPs</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zilberstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Immerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Planning, learning &amp; coordination in multiagent decision processes</title>
		<author>
			<persName><forename type="first">C</forename><surname>Boutilier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<pubPlace>TARK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Decomposition techniques for planning in stochastic domains</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Quantitative modeling of complex computational task environments</title>
		<author>
			<persName><forename type="first">K</forename><surname>Decker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lesser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Collaborative plans for complex group action</title>
		<author>
			<persName><forename type="first">B</forename><surname>Grosz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kraus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="269" to="357" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Context specific multiagent coordination and planning with factored MDPs</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venkataraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A combinatorial auction for collaborative planning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hunsberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Grosz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMAS</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Robocup-rescue: Search and rescue for large scale disasters as a domain for multiagent research</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kitano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tadokoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Noda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference SMC</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multiagent teamwork: Analyzing the optimality complexity of key theories and models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pynadath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tambe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAMAS</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Situation based strategic positioning for coordinating a team of homogeneous agents</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Reis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">LNCS</title>
		<imprint>
			<biblScope unit="volume">2103</biblScope>
			<biblScope unit="page">175</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Task decomposition, dynamic role assignment, and low-bandwidth communication for real-time strategic teamwork</title>
		<author>
			<persName><forename type="first">P</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Veloso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intel</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="241" to="273" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Building dynamic agent organizations in cyberspace</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tambe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pynadath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chauvat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet Computing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Vowels co-ordination model</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tavares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Demazeau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAMAS</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Team-oriented programming: Social structures</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tidhar</surname></persName>
		</author>
		<idno>47</idno>
		<imprint>
			<date type="published" when="1993">1993</date>
			<pubPlace>Australian A.I. Institute</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Guided team selection</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tidhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sonenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMAS</title>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Communication decisions in multiagent cooperation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lesser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zilberstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Agents</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
