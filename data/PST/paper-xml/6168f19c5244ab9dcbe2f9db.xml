<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">WHY PROPAGATE ALONE? PARALLEL USE OF LABELS AND FEATURES ON GRAPHS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-10-14">14 Oct 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yangkun</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiarui</forename><surname>Jin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yongyi</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiuhai</forename><surname>Chen</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<addrLine>4 Amazon</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Quan</forename><surname>Gan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yong</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zengfeng</forename><surname>Huang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">David</forename><surname>Wipf</surname></persName>
							<email>daviwipf@amazon.com</email>
						</author>
						<title level="a" type="main">WHY PROPAGATE ALONE? PARALLEL USE OF LABELS AND FEATURES ON GRAPHS</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-10-14">14 Oct 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2110.07190v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph neural networks (GNNs) and label propagation represent two interrelated modeling strategies designed to exploit graph structure in tasks such as node property prediction. The former is typically based on stacked message-passing layers that share neighborhood information to transform node features into predictive embeddings. In contrast, the latter involves spreading label information to unlabeled nodes via a parameter-free diffusion process, but operates independently of the node features. Given then that the material difference is merely whether features or labels are smoothed across the graph, it is natural to consider combinations of the two for improving performance. In this regard, it has recently been proposed to use a randomly-selected portion of the training labels as GNN inputs, concatenated with the original node features for making predictions on the remaining labels. This so-called label trick accommodates the parallel use of features and labels, and is foundational to many of the top-ranking submissions on the Open Graph Benchmark (OGB) leaderboard. And yet despite its wide-spread adoption, thus far there has been little attempt to carefully unpack exactly what statistical properties the label trick introduces into the training pipeline, intended or otherwise. To this end, we prove that under certain simplifying assumptions, the stochastic label trick can be reduced to an interpretable, deterministic training objective composed of two factors. The first is a data-fitting term that naturally resolves potential label leakage issues, while the second serves as a regularization factor conditioned on graph structure that adapts to graph size and connectivity. Later, we leverage this perspective to motivate a broader range of label trick use cases, and provide experiments to verify the efficacy of these extensions. combinations of the two for improving performance. Examples motivated by this intuition, at least to varying degrees, include APPNP (</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Node property prediction is a ubiquitous task involving graph data with node features and/or labels, with a wide range of instantiations across real-world scenarios such as node classification <ref type="bibr" target="#b21">(Veličković et al., 2017)</ref> and link prediction <ref type="bibr" target="#b29">(Zhang &amp; Chen, 2018)</ref>, while also empowering graph classification <ref type="bibr" target="#b2">(Gilmer et al., 2017)</ref>, etc. Different from conventional machine learning problems where there is typically no explicit non-iid structure among samples, nodes are connected by edges, and a natural assumption is that labels and features vary smoothly over the graph. This smoothing assumption has inspired two interrelated lines of research: First, graph neural networks (GNNs) <ref type="bibr" target="#b10">(Kipf &amp; Welling, 2016;</ref><ref type="bibr" target="#b4">Hamilton et al., 2017;</ref><ref type="bibr" target="#b14">Li et al., 2018;</ref><ref type="bibr" target="#b27">Xu et al., 2018b;</ref><ref type="bibr" target="#b15">Liao et al., 2019;</ref><ref type="bibr" target="#b26">Xu et al., 2018a</ref>) leverage a parameterized message passing strategy to convert the original node features into predictive embeddings that reflect the features of neighboring nodes. However, this approach does not directly utilize existing label information beyond their influence on model parameters through training. And secondly, label propagation algorithms <ref type="bibr">(Zhu, 2005;</ref><ref type="bibr">Zhou et al., 2004;</ref><ref type="bibr">Zhang et al., 2006;</ref><ref type="bibr" target="#b22">Wang &amp; Zhang, 2007;</ref><ref type="bibr" target="#b9">Karasuyama &amp; Mamitsuka, 2013;</ref><ref type="bibr" target="#b3">Gong et al., 2016;</ref><ref type="bibr" target="#b16">Liu et al., 2018)</ref> spread label information via graph diffusion to make predictions, but cannot exploit node features.</p><p>As GNNs follow a similar propagation mechanism as the label propagation algorithm, the principal difference being whether features or labels are smoothed across the graph, it is natural to consider † Work done during internship at Amazon Web Services Shanghai AI Lab.</p><p>And yet despite its far-reaching adoption, thus far the label trick has been motivated primarily as a training heuristic without a strong theoretical foundation. Moreover, many aspects of its underlying operational behavior have not been explored, with non-trivial open questions remaining. For example, while originally motivated from a stochastic perspective, is the label trick reducible to a more transparent deterministic form that is amenable to interpretation and analysis? Similarly, are there any indirect regularization effects with desirable (or possibly undesirable) downstream consequences? And how do the implicit predictions applied by the model to test nodes during the stochastic training process compare with the actual deterministic predictions used during inference? If there is a discrepancy, then the generalization ability of the model could be compromised. And finally, are there natural use cases for the label trick that have so far flown under the radar and been missed?</p><p>We take a step towards answering these questions via the following two primary contributions:</p><p>• We prove that in certain restricted settings, the original stochastic label trick can be reduced to an interpretable, deterministic training objective composed of two terms: (1) a data-fitting term that naturally resolves potential label leakage issues and maintains consistent predictions during training and inference, and (2) a regularization factor conditioned on graph structure that adapts to graph size and connectivity. Furthermore, complementary experiments applying the label trick to a broader class of graph neural network models corroborate that similar effects exists in more practical real-world settings, consistent with our theoretical findings.</p><p>• Although in prior work the label trick has already been integrated within a wide variety of GNN models, we introduce novel use-cases motivated by our analysis. This includes exploiting the label trick to: (i) train simple end-to-end variants of label propagation and C&amp;S, and (ii) replace stochastic use cases of the label trick with more stable, deterministic analogues that are applicable to GNN models with linear propagation layers such as SGC <ref type="bibr" target="#b25">(Wu et al., 2019)</ref>, TWIRLS <ref type="bibr" target="#b28">(Yang et al., 2021)</ref> and SIGN <ref type="bibr" target="#b1">(Frasca et al., 2020)</ref>. Empirical results on node classification benchmarks verify the efficacy of these simple enhancements.</p><p>Collectively, these efforts establish a more sturdy foundation for the label trick, and in doing so, help to ensure that it is not underutilized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>Consider a graph G = (V, E) with n = |V | nodes, the node feature matrix is denoted by X ∈ R n×d and the label matrix of the nodes is denoted by Y ∈ R n×c , with d and c being the number of channels of features and labels, respectively. Let A be the adjacency matrix, D the degree matrix and S = D − 1 2 AD − 1 2 the symmetric normalized adjacency matrix. The symmetric normalized Laplacian L can then be formulated as L = I n − S. We also define a training mask matrix as M tr = I m 0 0 0 n×n , where w.l.o.g. we are assuming that the first m nodes, denoted D tr , form the training dataset. We use P to denote a propagation matrix, where the specific P will be described in each context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">LABEL PROPAGATION ALGORITHM</head><p>Label propagation is a semi-supervised algorithm that predicts unlabeled nodes by propagating the observed labels across the edges of the graph, with the underlying smoothness assumption that two nodes connected by an edge are likely to share the same label. Following <ref type="bibr">(Zhou et al., 2004;</ref><ref type="bibr" target="#b28">Yang et al., 2021)</ref>, the implicit energy function of label propagation is given by</p><formula xml:id="formula_0">E(F ) = (1 − λ) F − Y tr 2 2 + λ tr[F ⊤ LF ],</formula><p>(1) where Y tr = M tr Y is the label matrix of training nodes, and λ ∈ (0, 1) is a regularization coefficient that determines the trade-off between the two terms. The first term is a fitting constraint, with the intuition that the predictions of a good classifier should remain close to the initial label assignments, while the second term introduces a smoothness constraint, which favors similar predictions between neighboring nodes in the graph.</p><p>It is not hard to derive that the closed-formed optimal solution of this energy function is given by F * = P Y , where P = (1 − λ)(I n − λS) −1 . However, since the stated inverse is impractical to compute for large graphs, P Y is often approximated in practice via P ≈ (1 − λ) k i=0 λ i S i Y . From this expression, it follows that F can be estimated by the more efficient iterations 0) , where F (0) = Y tr and for each k, S smooths the training labels across the edges of the graph.</p><formula xml:id="formula_1">F (k+1) = λSF (k) + (1 − λ)F (</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">GRAPH NEURAL NETWORKS FOR PROPAGATING NODE FEATURES</head><p>In contrast to the propagation of labels across the graph, GNN models transform and propagate node features using a series of feed-forward neural network layers. Popular examples include <ref type="bibr">GCN (Kipf &amp; Welling, 2016</ref><ref type="bibr">), GraphSAGE (Hamilton et al., 2017)</ref>, GAT <ref type="bibr" target="#b21">(Veličković et al., 2017)</ref>, and GIN <ref type="bibr" target="#b26">(Xu et al., 2018a)</ref>. For instance, the layer-wise propagation rule of GCN can be formulated as X (k+1) = σ(SX (k) W (k) ) where σ(•) is an activation function such as ReLU, X (k) is the k-th layer node representations with X (0) = X, and W (k) is a trainable weight matrix of the k-th layer. Compared with the label propagation algorithm, GNNs can sometimes exhibit a more powerful generalization capability via the interaction between discriminative node features and trainable weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">COMBINING LABEL AND FEATURE PROPAGATION</head><p>While performing satisfactorily in many circumstances, GNNs only indirectly incorporate groundtruth training labels via their influence on the learned model weights. But these labels are not actually used during inference, which can potentially degrade performance relative to label propagation, especially when the node features are noisy or unreliable. Therefore, it is natural to consider the combination of label and feature propagation to synergistically exploit the benefits of both as has been proposed in <ref type="bibr" target="#b11">(Klicpera et al., 2018;</ref><ref type="bibr" target="#b16">Liu et al., 2018;</ref><ref type="bibr" target="#b23">Wang &amp; Leskovec, 2020;</ref><ref type="bibr" target="#b24">Wang et al., 2021;</ref><ref type="bibr" target="#b18">Shi et al., 2020;</ref><ref type="bibr" target="#b7">Huang et al., 2020)</ref>.</p><p>One of the most successful among these hybrid methods is the so-called label trick, which can be conveniently retrofitted within most standard GNN architectures while facilitating the parallel propagation of labels and features in an end-to-end trainable fashion. As mentioned previously, a number of top-performing GNN pipelines have already adopted this trick, which serves to establish its widespread relevance <ref type="bibr" target="#b20">(Sun &amp; Wu, 2020;</ref><ref type="bibr" target="#b24">Wang et al., 2021;</ref><ref type="bibr" target="#b12">Kong et al., 2020;</ref><ref type="bibr" target="#b13">Li et al., 2021;</ref><ref type="bibr" target="#b18">Shi et al., 2020)</ref> and motivates our investigation of its properties herein. To this end, we formally define the label trick as follows:</p><p>Definition 1 (label trick) The label trick is based on creating random partitions of the training data as in D tr = D in ∪D out and D in ∩D out = ∅, where node labels from D in are concatenated with the original features X and provided as GNN inputs (for nodes not in D in zero-padding is used), while the labels from D out serve in the traditional role as supervision. The resulting training objective then becomes E splits i∈Dout</p><formula xml:id="formula_2">ℓ y i , f (X, Y in ; W) i (2)</formula><p>where Y in ∈ R n×c , is defined row-wise as y in,i = </p><formula xml:id="formula_3">y i if i ∈ D</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RELIABLE RANDOMNESS THOUGH THE LABEL TRICK</head><p>Despite its widespread adoption, the label trick has thus far been motivated as merely a training heuristic without formal justification. To address this issue, we will now attempt to quantify the induced regularization effect that naturally emerges when using the label trick. However, since the formal analysis of deep networks is challenging, we herein adopt the simplifying assumption that the function f from ( <ref type="formula">2</ref>) is linear, analogous to the popular SGC model from <ref type="bibr" target="#b25">(Wu et al., 2019)</ref>.</p><p>For simplicity of exposition, in Sections 3.1 and 3.2 we will consider the case where no node features are present to isolate label-trick-specific phenomena. Later in Sections 3.3 and 3.4 we will reintroduce node features to present our general results, as well as considering nonlinear extensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">LABEL TRICK WITHOUT NODE FEATURES</head><p>Assuming no node features X, we begin by considering the deterministic node label loss</p><formula xml:id="formula_4">L(W ) = i∈Dtr ℓ y i , [P Y tr W ] i ,<label>(3)</label></formula><p>where [ • ] i indicates the i-th row of a matrix and P Y tr W is a linear predictor akin to SGC, but with only the zero-padded training label matrix Y tr as an input. However, directly employing (3) for training suffers from potential label leakage issues given that a simple identity mapping suffices to achieve the minimal loss at the expense of accurate generalization to test nodes. Furthermore, there exists an inherent asymmetry between the predictions computed for training nodes, where the corresponding labels are also used as inputs to the model, and the predictions for testing nodes where no labels are available.</p><p>At a conceptual level, these issues can be resolved by the label trick, in which case we introduce random splits of D tr and modify (3) to</p><formula xml:id="formula_5">L(W ) = E splits i∈Dout ℓ y i , [P Y in W ] i .<label>(4)</label></formula><p>For each random split, the resulting predictor only includes the label information of D in (through Y in ), and thus there is no unresolved label leakage issue when predicting the labels of D out .</p><p>In practice, we typically sample the splits by assigning a given node to D in with some probability α ∈ (0, 1); otherwise the node is set to</p><formula xml:id="formula_6">D out . It then follows that E[|D in |] = α|D tr | and E[Y in ] = αY tr .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">SELF-EXCLUDED SIMPLIFICATION OF THE LABEL TRICK</head><p>Because there exists an exponential number of different possible random splits, for analysis purposes (with later practical benefits as well) we first consider a simplified one-versus-all case whereby we enforce that |D out | = 1 across all random splits, with each node landing with equal probability in D out . In this situation, the objective function from (4) can be re-expressed more transparently without any expectation as</p><formula xml:id="formula_7">L(W ) = E splits i∈Dout ℓ y i , [P Y in W ] i = E splits i∈Dout ℓ y i , [P (Y tr − Y i )W ] i = i∈Dtr ℓ y i , [(P − C)Y tr W ] i ,<label>(5)</label></formula><p>where Y i represents a matrix that shares the i-th row of Y and pads the rest with zeros, and C = diag(P ). This then motivates the revised predictor given by</p><formula xml:id="formula_8">f(Y tr ; W ) = (P − C)Y tr W ,<label>(6)</label></formula><p>where P here can in principle be any reasonable propagation matrix, not necessarily the one associated with the original label propagation algorithm.</p><p>Remark 1 From this expression we observe the intuitive role that C plays in blocking the direct pathway between each training node input label to the output predicted label for that same node. In this way the predictor propagates the labels of each training node excluding itself, and for both training and testing nodes alike, the predicted label of a node is only a function of other node labels. This resolves the asymmetry mentioned previously with respect to the predictions from (3).</p><p>Remark 2 It is generally desirable that a candidate model produces the same predictions on test nodes during training and inference to better insure proper generalization. Fortunately, this is in fact the case when applying ( <ref type="formula" target="#formula_8">6</ref>), which on test nodes makes the same predictions as label propagation.</p><p>To see this, note that M te (P − C)Y tr = M te P Y tr , where M te = I n − M tr is the diagonal mask matrix of test nodes and P Y tr is the original label propagation predictor.</p><p>Although ultimately (6) will serve as a useful analysis tool below, it is also possible to adopt this predictor in certain practical settings. In this regard, C can be easily computed with the same computational complexity as is needed to approximate P as discussed in Section 2.1 (and for alternative propagation operators that are available explicitly, e.g., the normalized adjacency matrix, C is directly available).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">FULL EXECUTION OF THE LABEL TRICK AS A REGULARIZER</head><p>We are now positioned to extend the self-excluded simplification of the label trick to full execution with arbitrary random sampling, as well as later, the reintroduction of node features. For this purpose, we first define Y out = Y tr − Y in , and also rescale by a factor of 1/α to produce Y in = Y in /α. The latter allows us to maintain a consistent mean and variance of the predictor across different sampling probabilities.</p><p>Assuming a mean square error (MSE) loss as computed for each node via ℓ(y, y) = ||y − y|| 2 2 (later we consider categorical cross-entropy), our overall objective is to minimize</p><formula xml:id="formula_9">L(W ) = E splits i∈Dout ℓ(y i , [P Y in W ] i ) = E splits Y out − M out P Ỹ in W 2 F ,<label>(7)</label></formula><p>where M out is a diagonal mask matrix defined such that Y out = M out Y = Y tr − Y in and the random splits follow a node-wise Bernoulli distribution with parameter α as discussed previously.</p><p>We then have the following:</p><p>Theorem 1 The label trick objective from ( <ref type="formula" target="#formula_9">7</ref>) satisfies</p><formula xml:id="formula_10">1 1 − α E splits Y out − M out P Ỹ in W 2 F = Y tr − M tr (P − C)Y tr W 2 F + 1 − α α ΓW 2 F , (8) where Γ = diag(P T P ) − C T C 1 2 Y tr .</formula><p>Note that (diag(P T P ) − C T C) is a positive semi-definite diagonal matrix, and hence its real square root will always exist. Furthermore, we can extend this analysis to include node features by incorporating the SGC-like linear predictor P XW x such that Theorem 1 can then naturally be generalized as follows:</p><p>Corollary 1 Under the same conditions as Theorem 1, if we add the node feature term P XW x to the label-based predictor from (7), we have that</p><formula xml:id="formula_11">1 1 − α E splits Y out − M out P XW x − M out P Ỹ in W y 2 F = Y tr − M tr P XW x − M tr (P − C)Y tr W y 2 F + 1 − α α ΓW y 2 F .<label>(9)</label></formula><p>The details of the proofs of Theorem 1 and Corollary 1 are provided in Appendix A.1. This then effectively leads to the more general, feature and label aware predictor</p><formula xml:id="formula_12">f(X, Y tr ; W) = P XW x + (P − C)Y tr W y ,<label>(10)</label></formula><p>where W = {W x , W y }. These theoretical results reveal a number of interesting properties regarding how the label trick behaves, which we summarize as follows:</p><p>Remark 3 Although the original loss involves an expectation over random data splits that is somewhat difficult to interpret, based on ( <ref type="formula" target="#formula_11">9</ref>), the label trick can be interpreted as inducing a deterministic objective composed of two terms:</p><p>1. The error accrued when combining the original node features with the self-excluded label propagation predictor from ( <ref type="formula" target="#formula_8">6</ref>) to mitigate label leakage, and</p><p>2. An additional graph-dependent regularization factor on the model weights associated with the labels that depends on α (more on this below).</p><p>Moreover, we can easily verify from ( <ref type="formula" target="#formula_12">10</ref>) that the model effectively applies the same prediction to test nodes during both training and inference, consistent with Remark 2.</p><p>Remark 4 Regarding the Γ-dependent penalty term, if the graph has no edges, then there is no chance for overfitting to labels and diag(P T P ) − C T C 1 2 = 0 shuts off the regularization. In contrast, for a fully connected graph, the value of Γ can be significantly larger, which can potentially provide a beneficial regularization effect. Additionally, given that Γ also grows larger with graph size (assuming edges grow as well), ΓW y 2 F scales proportionately with the data fitting term, which is generally expected to increase linearly with the number of nodes. Hence ( <ref type="formula" target="#formula_11">9</ref>) is naturally balanced to problem size.</p><p>Remark 5 The splitting probability α in ( <ref type="formula" target="#formula_11">9</ref>) controls the regularization strength. Specifically, when α tends to zero, fewer labels are used as input to predict a large number of output labels, which may be less reliable, and corresponds with adding a larger regularization effect. Additionally, it means placing more emphasis on the original node features and downplaying the importance of the labels as input in ( <ref type="formula" target="#formula_11">9</ref>), which explains the addition of a penalty on W y . Conversely, when α tends to one, more labels are used as input to predict the output and the model approaches the deterministic self-excluded label trick. Specifically, for random splits where |D out | = 1, the loss mimics one random term from the self-excluded label trick summation, while for the splits when |D out | = 0, the contribution to the expectation is zero and therefore does not influence the loss. Splits with |D out | &gt; 1 will have very low probability. So this situation naturally corresponds with canceling out the regularization term. Later in Section 3.4 we will extend these observations to general nonlinear models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remark 6</head><p>The regularization term is also loosely analogous to the one associated with dropout <ref type="bibr" target="#b19">(Srivastava et al., 2014)</ref> for the case of linear regression, where the splitting probability α is similar to the keep probability of dropout. A smaller α means that fewer labels are used as input, implying stronger regularization. While this is an interesting association, there remain critical differences, such as the natural emergence of C which complicates the problem considerably; see the proof for further details.</p><p>We now turn to the categorical cross-entropy loss, which is more commonly applied to node classification problems. While we can no longer compute closed-form simplifications as we could with MSE, it is nonetheless possible to show that the resulting objective when using the original label trick is an upper bound on the analogous objective from the self-excluded label trick. More specifically, we have the following (see Appendix A.3 for the proof):</p><p>Theorem 2 Under the same conditions as in Theorem 1 and Corollary 1, if we replace the MSE loss with categorical cross-entropy we obtain the bound</p><formula xml:id="formula_13">1 1 − α E splits CrossEntropy Dout (Y out , P XW x + P Ỹ in W y ) ≥ CrossEntropy Dtr (Y tr , P XW x + (P − C)Y tr W y ),<label>(11)</label></formula><p>where CrossEntropy S (•, •) denotes the sum of row-wise cross-entropy of S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">NONLINEAR EXTENSIONS</head><p>When we move towards more complex GNN models with arbitrary nonlinear interactions, it is no longer feasible to establish explicit, deterministic functional equivalents of the label trick for general α. However, we can still at least elucidate the situation at the two extremes where α → 0 or α → 1 alluded to in Remark 5. Regarding the former, clearly with probability approaching one, Y in will always equal zero and hence the model will default to a regular GNN, effectively involving no label information as an input. In contrast, for the latter we provide the following:</p><p>Theorem 3 Let f GN N (X, Y ; W) denote an arbitrary GNN model with concatenated inputs X and Y , and ℓ(y, ŷ) a training loss such that i∈Dout ℓ y i , f GN N [X, Y in ; W] i is bounded for all D out . It then follows that</p><formula xml:id="formula_14">lim α→1 1 1 − α E splits i∈Dout ℓ y i , f GN N [X, Y in ; W] i = m i=1 ℓ y i , f GN N [X, Y tr −Y i ; W] i . (<label>12</label></formula><formula xml:id="formula_15">)</formula><p>The proof is given in Appendix A.4. This result can be viewed as a natural generalization of ( <ref type="formula" target="#formula_7">5</ref>), with one minor caveat: we can no longer guarantee that the predictor implicitly applied to test nodes during training will exactly match the explicit function f GN N [X, Y tr ; W] applied at inference time. Indeed, each f GN N [X, Y tr − Y i ; W] i will generally produce slightly different predictions for all test nodes depending on i unless f GN N is linear. But in practice this is unlikely to be consequential.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">BROADER USE CASES OF THE LABEL TRICK</head><p>Although the label trick has already been integrated within a wide variety of GNN pipelines, in this section we introduce three novel use-cases motivated by our analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">TRAINABLE LABEL PROPAGATION</head><p>In Sections 3.1 and 3.2 we excluded the use of node features to simplify the exposition of the label trick; however, analytical points aside, the presented methodology can also be useful in and of itself for facilitating a simple, trainable label propagation baseline.</p><p>The original label propagation algorithm from <ref type="bibr">(Zhou et al., 2004)</ref> is motivated as a parameter-free, deterministic mapping from a training label set to predictions across the entire graph. However, clearly the randomized label trick from Section 3.1, or its deterministic simplification from Section 3.2 can be adopted to learn a label propagation weight matrix W . The latter represents a reasonable enhancement that can potentially help to compensate for interrelated class labels that may arise in multi-label settings. In contrast, the original label propagation algorithm implicitly assumes that different classes are independent. Beyond this, other entry points for adding trainable weights are also feasible such as node-dependent weights, nonlinear weighted mappings, step-wise weights for heterophily graphs, or weights for different node types for heterogeneous graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">DETERMINISTIC APPLICATION TO GNNS WITH LINEAR PROPAGATION LAYERS</head><p>Many prevailing GNN models follow the architecture of message passing neural networks (MPNNs). Among these are efficient variants that share node embeddings only through linear propagation layers. Representative examples include SGC <ref type="bibr" target="#b25">(Wu et al., 2019)</ref>, SIGN <ref type="bibr" target="#b1">(Frasca et al., 2020)</ref> and TWIRLS <ref type="bibr" target="#b28">(Yang et al., 2021)</ref>. We now show how to apply the deterministic label trick algorithm as introduced in Section 3.2 with the aforementioned GNN methods.</p><p>We begin with a linear SGC model. In this case, we can compute (P − C)Y tr beforehand as the self-excluded label information and then train the resulting features individually without graph information, while avoiding label leakage problems. And if desired, we can also concatenate with the original node features. In this way, we have an algorithm that minimizes an energy function involving both labels and input features.</p><p>Additionally, for more complex situations where the propagation layers are not at the beginning of the model, the predictor can be more complicated such as f (X, Y ; W) =</p><formula xml:id="formula_16">h 1 i Ph 0 ([X, Y − Y i ]) i = h 1 (Ph 0 ([X, Y ]) − Ch 0 ([X, Y ]) + Ch 0 ([X, 0])),<label>(13)</label></formula><p>where [•, •] denotes the concatenation operation, P = [P 0 , P 1 , . . . , P k−1 ] T is the integrated propagation matrix, C = [diag(P 0 ), diag(P 1 ), . . . , diag(P k−1 )] T , h 0 and h 1 can be arbitrary nodeindependent functions, typically multi-layer perceptrons (MLPs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">TRAINABLE CORRECT AND SMOOTH</head><p>Correct and Smooth (C&amp;S) <ref type="bibr" target="#b7">(Huang et al., 2020</ref>) is a simple yet powerful method which consists of multiple stages. A prediction matrix Ỹ is first obtained whose rows correspond with a prediction from a shallow node-wise model. Ỹ is subsequently modified via two post-processing steps, correct and smooth, using two propagation matrices {P c , P s }, where typically</p><formula xml:id="formula_17">P i = (1 − λ i )(I n − λ i S) −1 , i ∈ {c, s}.</formula><p>For the former, we compute the difference between the ground truth and predictions on the training set as E = Y tr − Ỹ tr and then form Ẽ = γ(P c E) as the correction matrix, where γ(•) is some row-independent scaling function. The final smoothed prediction is formed as f C&amp;S ( Ỹ ) = P s (Y tr + M te ( Ỹ + Ẽ)). This formulation is not directly amenable to end-to-end training because of label leakage issues introduced through Y tr .</p><p>In contrast, with the label trick, we can equip C&amp;S with trainable weights to further boost performance. To this end, we first split the training dataset into D in and D out as before. Then we multiply Ẽin = γ(P c (Y in − Ỹ in )), the correction matrix with respect to Y in , with a weight matrix W c . We also multiply the result after smoothing with another weight matrix W s . Thus the predictor under this split is</p><formula xml:id="formula_18">f T C&amp;S (Y in , Ỹ ; W) = P s (Y in + (M te + M out )( Ỹ + Ẽin W c ))W s = P s ((Y in + (M te + M out ) Ỹ )W s + P s (M te + M out ) Ẽin W c W s = P s ((Y in + (M te + M out ) Ỹ )W s + P s (M te + M out ) Ẽin Ŵ c ,<label>(14)</label></formula><p>where W = { Ŵ c , W s } and Ŵ c is the reparameterization of W c W s . The objective function for optimizing W is</p><formula xml:id="formula_19">L(W ) = E splits i∈Dout ℓ(y i , f T C&amp;S [Y in , Ỹ ; W] i ) . (<label>15</label></formula><formula xml:id="formula_20">)</formula><p>Since this objective resolves the label leakage issue, it allows end-to-end training of C&amp;S with gradients passing through both the neural network layers for computing Ỹ and the C&amp;S steps. At times, however, this approach may have disadvantages, including potential overfitting problems or inefficiencies due to computationally expensive backpropagation. Consequently, an alternative option is to preserve the two-stage training. In this situation, the base prediction in the first stage is the same as the original algorithm; however, we can nonetheless still train the C&amp;S module as a post-processing step, with parameters as in ( <ref type="formula" target="#formula_18">14</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>As mentioned previously, the effectiveness of the label trick in improving GNN performance has already been demonstrated in prior work, and therefore, our goal here is not to repeat these efforts. Instead, in this section we will focus on conducting experiments that complement our analysis from Section 3 and showcase the broader application scenarios from Section 4.</p><p>The label trick can actually be implemented in two ways. The first is the one that randomly splits the training nodes, and the second is the simpler version introduced herein with the deterministic one-versus-all splitting strategy, which does not require any random sampling. To differentiate the two versions, we denote the stochastic label trick with random splits by label trick (S), and the deterministic one by label trick (D). The latter is an efficient way to approximate the former with a higher splitting probability α, which is sometimes advantageous in cases where the training process is slowed down by high α. Accordingly, we conduct experiments with both, thus supporting our analysis with comparisons involving the two versions.</p><p>We use four relatively large datasets for evaluation, namely Cora-full, Pubmed <ref type="bibr" target="#b17">(Sen et al., 2008)</ref>, ogbn-arxiv and ogbn-products <ref type="bibr" target="#b5">(Hu et al., 2020)</ref>. For Cora-full and Pubmed, we randomly split the nodes into training, validation, and test datasets with the ratio of 6:2:2 using different random seeds.</p><p>For ogbn-arxiv and ogbn-products, we adopt the standard split from OGB <ref type="bibr" target="#b5">(Hu et al., 2020)</ref>. We report the average classification accuracy and standard deviation after 10 runs with different random seeds, and these are the results on the test dataset when not otherwise specified. See Appendix B for further implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">TRAINABLE LABEL PROPAGATION</head><p>We first investigate the performance of applying the label trick to label propagation as introduced in (2) in the absence of features, and compare it with the original label propagation algorithm. Table <ref type="table" target="#tab_1">1</ref> shows that the trainable weights applied to the label propagation algorithm can boost the performance consistently. Given the notable simplicity of label propagation, this represents a useful enhancement. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">DETERMINISTIC LABEL TRICK APPLIED TO GNNS WITH LINEAR LAYERS</head><p>We also test the deterministic label trick by applying it to different GNN architectures involving linear propagation layers along with (13). Due to the considerable computational effort required to produce P and C with large propagation steps for larger graphs, we only conduct tests on the Cora-full, Pubmed and ogbn-arxiv datasets, where the results are presented in Table <ref type="table" target="#tab_3">3</ref>. From these results we observe that on Pubmed and ogbn-arxiv, the deterministic label trick boosts the performance consistently on different models, while on Cora-full, it performs comparably. This is reasonable given that the training accuracy on Cora-full (not shown) is close to 100%, in which case the model does not benefit significantly from the ground-truth training labels, as the label information is already adequately embedded in the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">EFFECT OF SPLITTING PROBABILITY</head><p>In terms of the effect of the splitting probability α, we compare the accuracy results of a linear model and a three-layer GNN on ogbn-arxiv as shown in Figure <ref type="figure" target="#fig_0">1</ref>. For the linear model, α serves as a regularization coefficient. More specifically, when α tends to zero, the model converges to one without the label trick, while when α tends to one, it converges to the a case with the self-excluded label trick. For the the nonlinear GNN, α has a similar effect as predicted by theory for the linear models. As α decreases, the model converges to that without the label trick. Moreover, a larger α is preferable for linear models in contrast to GNNs, probably because a simple model does not require strong regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">TRAINABLE CORRECT AND SMOOTH</head><p>We also verify the effectiveness of our approach when applied to Correct and Smooth (C&amp;S), as described in Section 4.3. Due to the significant impact of end-to-end training on the model, it is not suitable for direct comparison with vanilla C&amp;S for a natural ablation. Therefore, in this experiment, we train the C&amp;S as post-processing steps. In Table <ref type="table" target="#tab_4">4</ref>, we report the test and validation accuracy, showing that our method outperforms the vanilla C&amp;S on Cora-full and Pubmed. And for ogbnarxiv and ogbn-products, the trainable C&amp;S performs better in terms of validation accuracy while is comparable to vanilla C&amp;S in terms of test accuracy. In principle, C&amp;S can be applied to any base predictor model. To accommodate tabular node features, we choose to use gradient boosted decision trees (GBDT), which can be trained end-to-end using methods such as <ref type="bibr">(Authors, 2021;</ref><ref type="bibr" target="#b8">Ivanov &amp; Prokhorenkova, 2021)</ref> combined with the label trick to avoid data leakage issues as we have discussed. For evaluation, we adopt the four tabular node regression data sets from <ref type="bibr" target="#b8">(Ivanov &amp; Prokhorenkova, 2021)</ref> and train using the approach from (Authors, 2021). Results are shown in Table <ref type="table" target="#tab_2">2</ref>, where it is clear that the label trick can significantly improve performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this work we closely examine from a theoretical prospective the recently emerged label trick, which enables the parallel propagation of labels and features and benefits various SOTA GNN architectures, and yet has thus far not be subject to rigorous analysis. In filling up this gap, we first introduce a deterministic self-excluded simplification of the label trick, and then prove that the full stochastic version can be regarded as introducing a regularization effect on the self-excluded label weights. Beyond this, we also discuss broader applications of the label trick with respect to:</p><p>1. Facilitating the introduction of trainable weights within graph-based methods that were previously either parameter-free (e.g., label propagation) or not end-to-end (e.g., C&amp;S), and 2. Eliminating the effects of randomness by incorporating self-excluded propagation within GNNs composed of linear propagation layers.</p><p>We verify these applications and evaluate the performance gains against existing approaches over multiple benchmark datasets. We first define the mask matrix for input labels:</p><formula xml:id="formula_21">M in = diag(r i ) 0 0 0 , r i ∼ Bernoulli(α),<label>(16)</label></formula><p>where α ∈ (0, 1) is the label rate, and the mask matrix for output is defined as</p><formula xml:id="formula_22">M out = M tr −M in .</formula><p>Then the objective is</p><formula xml:id="formula_23">L(W ) = E splits [ Y out − M out P Ỹ in W 2 F ] = E splits Y out − 1 α M out P Y in W 2 F = E splits [ Y out 2 F ] − 2 α E splits [tr[Y T out M out P Y in W ]] + 1 α 2 E splits [ M out P Y in W 2 F ] = E splits [ M out Y tr 2 F ] − 2 α E splits [tr[Y T tr M out P M in Y tr W ]] + 1 α 2 E splits [ M out P M in Y tr W 2 F ].<label>(17)</label></formula><p>Next we compute each term separately. For notational convenience, we denote the i-th row and j-th column entry of M in by M ij .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">For</head><formula xml:id="formula_24">E splits [ M out Y tr 2 F ], we have E splits [ M out Y tr 2 F ] = E splits [tr[Y T tr M T out M out Y tr ]] = E splits [tr[Y T tr M out Y tr ]] = E splits [tr[M out Y tr Y T tr ]] = tr[ E splits [M out ]Y tr Y T tr ] = (1 − α) tr[M tr Y tr Y T tr ] = (1 − α) tr[Y tr Y T tr ] = (1 − α) Y tr 2 F . (18) 2. For E splits [tr[Y T tr M out P M in Y tr W ]], since M out = M tr − M in , we then have E splits [tr[Y T tr M out P M in Y tr W ]] = E splits [tr[M out P M in Y tr W Y T tr ]] = tr[ E splits [M out P M in ]Y tr W Y T tr ] = tr[ E splits [(M tr − M in )P M in ]Y tr W Y T tr ] = tr[(M tr P E splits [M in ] − E splits [M in P M in ])Y tr W Y T tr ] = tr[(αM tr P M tr − E splits [M in P M in ])Y tr W Y T tr ] = α tr[M tr P M tr Y tr W Y T tr ] − tr E splits [M in P M in ]Y tr W Y T tr = α tr[Y T tr P Y tr W ] − tr E splits [M in P M in ]Y tr W Y T tr (19) Let us calculate E splits [M in P M in ]. Since M in is a diagonal matrix, we have E splits [M in P M in ] = ( E splits [M ii M jj P ij ]) = α 2 P tr + (α − α 2 )C tr ,<label>(20)</label></formula><p>where P tr = M tr P M tr and C tr = diag(P tr ). Then we have</p><formula xml:id="formula_25">tr[ E splits [M in P M in ]Y tr W Y T tr ] = α 2 tr[Y T tr P tr Y tr W ] + (α − α 2 ) tr[Y T tr C tr Y tr W ].<label>(21)</label></formula><p>Therefore, 1 2 , and then</p><formula xml:id="formula_26">E splits [tr[Y T tr M out P M in Y tr W ]] = (α − α 2 ) tr[Y T tr P tr Y tr W ] − (α − α 2 ) tr[Y T tr C tr Y tr W ] = (α − α 2 ) tr[Y T tr (P tr − C tr )Y tr W ]. (22) 3. For E splits [ M out P M in Y tr W 2 F ], we have E splits [ M out P M in Y tr W 2 F ] = E splits [tr[W T Y T tr M T in P T M T out M out P M in Y tr W ]] = E splits [tr[W T Y T tr M T in P T M out P M in Y tr W ]] = tr[ E splits [W T Y T tr M T in P T (M tr − M in )P M in ]Y tr W ] = tr[ E splits [W T Y T tr M T in P T M tr P M in ]Y tr W ] − tr[ E splits [W T Y T tr M T in P T M in P M in ]Y tr W ] = tr[W T Y T tr E splits [M T in P T M tr P M in ]Y tr W ] − tr[W T Y T tr E splits [M T in P T M in P M in ]Y tr W ].<label>(</label></formula><formula xml:id="formula_27">E splits M T in P T M tr P M in ij = E splits m k=1 M ii M jj P ki P kj = E splits [M ii M jj ] m k=1 P ki P kj = α 2 P T tr P tr + (α − α 2 ) diag(P 2 tr ) ij = α 2 P T tr P tr + (α − α 2 )(Q 2 tr ) ij .<label>(24)</label></formula><p>Therefore,</p><formula xml:id="formula_28">tr[W T Y T tr E splits [M T in P T M tr P M in ]Y tr W ] = α 2 P tr Y tr W 2 F + (α − α 2 ) Q tr Y tr W 2 F . (25) (b) To compute tr[W T Y T tr E splits [M T in P T M in P M in ]Y tr W ], we have that E splits M in P T M in P M in ij = E splits [M ii M jj n k=1 M kk P ki P kj ] = n k=1 E splits [M ii M jj M kk ]P ki P kj .<label>(26)</label></formula><p>• For the diagonal entries, where i = j, we have</p><formula xml:id="formula_29">n k=1 E splits [M ii M jj M kk ]P ki P kj = n k=1 α 2 P 2 tr,ki + (α − α 2 )P 2 tr,ii ,<label>(27)</label></formula><p>and therefore,</p><formula xml:id="formula_30">diag E splits [M in P T M in P M in ] = α 2 diag(P 2 tr )+(α−α 2 )C 2 tr = α 2 Q 2 tr +(α−α 2 )C 2 tr . (28)</formula><p>• For the off-diagonal entries, where i = j, we have</p><formula xml:id="formula_31">k E splits [M ii M jj M kk ]P ki P kj = α 3 k =i∧k =j</formula><p>P tr,ki P tr,kj +α 2 (P tr,ii P tr,ij +P tr,ji P tr,jj ).</p><p>(29) So E splits [M in P T M in P M in ] (ignore the diagonal entries) is</p><formula xml:id="formula_32">α 3 P 2 tr + (α 2 − α 3 )(C tr P tr + P tr C tr ).<label>(30)</label></formula><p>From ( <ref type="formula">28</ref>) and ( <ref type="formula" target="#formula_32">30</ref>), we know that</p><formula xml:id="formula_33">E splits [M in P T M in P M in ] =α 3 P 2 tr + (α 2 − α 3 )Q 2 tr + (α − 3α 2 + 2α 3 )C 2 tr + (α 2 − α 3 )(C tr P tr + P tr C tr ).<label>(31)</label></formula><p>Unfortunately because C tr P tr + P tr C tr is not necessarily positive semi-definite, we cannot simplify the proof using the Cholesky decomposition. So instead we proceed as follows:</p><formula xml:id="formula_34">tr[W T Y T tr E[M T in P tr M in P tr M in ]Y tr W ] = tr[W T Y T tr (α 3 P 2 tr + (α 2 − α 3 ) diag(P 2 tr ) + (α 2 − α 3 )(C tr P tr + P tr C tr ) + (α 2 − α 3 ) diag((C tr P tr + P tr C tr )) + (α − α 2 )C 2 tr )Y tr W ] = tr[α 3 W T Y T tr P 2 tr Y tr W + (α 2 − α 3 )W T Y T tr diag(P 2 tr )Y tr W + (α 2 − α 3 )W T Y T tr (C tr P tr + P tr C tr )Y tr W + (α − 3α 2 + 2α 3 )W T Y T tr C 2 tr Y tr W ] = α 3 P tr Y tr W 2 F + (α 2 − α 3 ) Q tr Y tr W 2 F + (α − 3α 2 + 2α 3 ) C tr Y tr W 2 F + (α 2 − α 3 ) tr[W T Y T tr (C tr P tr + P tr C tr )Y tr W ].<label>(32)</label></formula><p>Combining the three terms, we get</p><formula xml:id="formula_35">E splits [ M out P M in Y tr W 2 F ] = (α 2 − α 3 ) P tr Y tr W 2 F + (α − 2α 2 + α 3 ) Q tr Y tr W 2 F − (α 2 − α 3 ) tr[W T Y tr (P tr C tr + C tr P tr )Y tr W ] − (α − 3α 2 + 2α 3 ) C tr Y tr W 2 F .<label>(33)</label></formula><p>The overall result</p><formula xml:id="formula_36">L(W ) is E splits [ Y out − M out P Ỹ in W 2 F ] = E splits [ M out Y tr 2 F ] − 2 α E splits [tr[Y T tr M out P M in Y tr W ]] + 1 α 2 E splits [ M out P M in Y tr W 2 F ] = (1 − α) Y tr 2 F − 2(1 − α) tr[Y T tr (P tr − C tr )Y tr W ] + (1 − α) P tr Y tr W 2 F + ( 1 α − 2 + α) Q tr Y tr W 2 F − (1 − α) tr[W T Y tr (P tr C tr + C tr P tr )Y tr W ] − ( 1 α − 3 + 2α) C tr Y tr W 2 F ,<label>(34)</label></formula><p>where C tr = diag(P tr ) and</p><formula xml:id="formula_37">Q tr = diag(P 2 tr ) 1 2 . Next we compute Y tr − M tr (P − C)Y tr W 2 F . Since M tr (P − C)Y tr W 2 F = (P tr − C tr )Y tr W 2 F = tr[W T Y T tr (P tr − C tr ) T (P tr − C tr )Y tr W ] = P tr Y tr W 2 F + C tr Y tr W 2 F − tr[W T Y tr (P tr C tr + C tr P tr )Y tr W ],<label>(35)</label></formula><p>we have that</p><formula xml:id="formula_38">Y tr − M tr (P − C)Y tr W 2 F = Y tr 2 F + P tr Y tr W 2 F + C tr Y tr W 2 F − tr[W T Y tr (P tr C tr + C tr P tr )Y tr W ] − 2 tr[Y T tr (P tr − C tr )Y tr W ].<label>(36)</label></formula><p>Therefore, , where m is the number of trials and 1 − α is the success probability. And then we choose the elements to be assigned to D out uniformly over the possible combinations m |Dout| . Given that when |D out | = 0 there is no contribution to the loss, we may therefore reexpress the lefthand side of (12), excluding the limit, as</p><formula xml:id="formula_39">1 1 − α E splits [ Y out − M out P Ỹ in W 2 F ] = Y tr</formula><formula xml:id="formula_40">1 1 − α E splits i∈Dout ℓ y i , f GN N [X, Y in ; W] i = P [|D out | = 1] 1 − α E P Dout||Dout|=1 i∈Dout ℓ y i , f GN N [X, Y in ; W] i + P [|D out | &gt; 1] 1 − α E P Dout||Dout|&gt;1 i∈Dout ℓ y i , f GN N [X, Y in ; W] i ,<label>(44)</label></formula><p>where it naturally follows that</p><formula xml:id="formula_41">E P Dout||Dout|=1 i∈Dout ℓ y i , f GN N [X, Y in ; W] i = 1 m m i=1 ℓ y i , f GN N [X, Y tr −Y i ; W] i .</formula><p>(45) We also have that</p><formula xml:id="formula_42">P [|D out | &gt; 1] 1 − α = m i=2 m i (1 − α) i α m−i 1 − α = m i=2 m i (1 − α) i−1 α m−i<label>(46)</label></formula><p>and</p><formula xml:id="formula_43">P [|D out | = 1] 1 − α = m(1 − α)α m−1 1 − α = mα m−1 . (<label>47</label></formula><formula xml:id="formula_44">)</formula><p>From these expressions, we then have that And given the assumption that i∈Dout ℓ y i , f GN N [X, Y in ; W] i &lt; B &lt; ∞ for some B, we also have</p><formula xml:id="formula_45">E P Dout||Dout|&gt;1 i∈Dout ℓ y i , f GN N [X, Y in ; W] i &lt; E P Dout||Dout|&gt;1 B = B,<label>(49)</label></formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Accuracy results on validation dataset of linear propagation of features and labels (left) and GNNs (right) varying α.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>23) Let us consider each term separately. (a) To compute tr[W T Y T tr E splits [M T in P T M tr P M in ]Y tr W ], we first define Q tr = diag(P 2 tr )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>2FF=F=F=YY 2 FF=[</head><label>2</label><figDesc>− 2 tr[Y T tr (P tr − C tr )Y tr W ] + P tr Y tr W 2 F + 1 − α α Q tr Y tr W 2 F − tr[W T Y tr (P tr C tr + C tr P tr )Y tr W ] + 2α − 1 α C tr Y tr W 2 Y tr − M tr (P − C)Y tr W 2 (diag(P T P ) − C T C) 1 2, and Γ = U Y tr . Then we have1 1 − α E splits [ Y out − M out P Ỹ in W 2 F ] = Y tr − M tr (P − C)Y tr W 2 F + 1 − α α M tr U Y tr W 2 Y tr − M tr (P − C)Y tr W 2 F + 1 − α α U Y tr W 2 Y tr − M tr (P − C)Y tr W 2 out − M out P XW x − M out P Ỹ in W y out − M out P Ỹ in W y 2 F + M out P XW x − 2 tr[Y T out M out P XW x ] + 2 α tr[W T y Y T in P T M out P XW x ] = Y tr − (P tr − C tr )Y tr W y − 2 tr[Y T tr P XW x ] + 2 tr[W T y Y T tr (P tr − C tr )P XW x ] = Y tr − M tr P XW x − (P tr − C tr )Y tr W y Y tr − M tr P XW x − M tr (P − C)Y tr W y P Y in W y ] = 1 α M tr P E splits [M in ]Y tr W y − 1 α E splits [M in P M in ]Y tr W y = M tr P Y tr W y − (αP tr − αC tr + C tr )Y tr W y = (1 − α)M tr P Y tr W y − (1 − α)C tr Y tr W y = (1 − α)M tr (P − C)Y tr W y .(40)And then by Jensen's inequality we haveE splits [CrossEntropy Dout (Y out , M out P XW x + M out P Ỹ in W y )] ≥ CrossEntropy Dout (Y out , E splits [M out P XW x + M out P Ỹ in W y ]) = CrossEntropy Dtr (Y tr , (1 − α)(P XW x + (P − C)Y tr W y ))).(41)Notice based on the nature of cross-entropy, if 0 &lt; α &lt; 1, we have CrossEntropy(Z, (1 − α) Z) ≥ (1 − α)CrossEntropy(Z, Z). CrossEntropy Dout (Y out , P XW x + P Ỹ in W y )] ≥ CrossEntropy Dtr (Y tr , XW x + (P − C)Y tr W y ). (43) A.4 PROOF OF THEOREM 3 Although the label trick splits are drawn randomly using an iid Bernoulli distribution to sample the elements to be added to either D in or D out , we can equivalently decompose this process into two parts. First, we draw |D out | from a binomial distribution P [|D out |] = Binom[m, 1 − α]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Y in ; W) represents a message-passing neural network with parameters W and the concatenation of X and Y in as inputs, and ℓ(•, •) denotes a point-wise loss function over one sample/node. At inference time, we then use the deterministic predictor f (X, Y tr ; W) i for all test nodes i / ∈ D tr .</figDesc><table><row><cell>in 0 otherwise</cell><cell>for all i, the function</cell></row><row><cell>f (X,</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Accuracy results (%) of label propagation and trainable label propagation.</figDesc><table><row><cell cols="3">Method Label Propagation Trainable Label Propagation</cell></row><row><cell>Cora-full</cell><cell>66.44 ± 0.93</cell><cell>67.23 ± 0.66</cell></row><row><cell>Pubmed</cell><cell>83.45 ± 0.63</cell><cell>83.52 ± 0.59</cell></row><row><cell>Arxiv</cell><cell>67.11 ± 0.00</cell><cell>68.42 ± 0.01</cell></row><row><cell>Products</cell><cell>74.24 ± 0.00</cell><cell>75.61 ± 0.21</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>MSE on node regression tasks with GBDT base model and C&amp;S.</figDesc><table><row><cell>Method</cell><cell>C&amp;S</cell><cell>Trainable C&amp;S</cell></row><row><cell>House</cell><cell>0.51 ± 0.01</cell><cell>0.45 ± 0.01</cell></row><row><cell>County</cell><cell>1.42 ± 0.14</cell><cell>1.13 ± 0.09</cell></row><row><cell>VK</cell><cell>7.02 ± 0.20</cell><cell>6.95 ± 0.22</cell></row><row><cell>Avazu</cell><cell>0.106 ± 0.014</cell><cell>0.106 ± 0.014</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Accuracy results (%) with/without label trick (D).</figDesc><table><row><cell>Method</cell><cell>SGC</cell><cell></cell><cell>SIGN</cell><cell></cell><cell>TWIRLS</cell><cell></cell></row><row><cell>label trick (D)</cell><cell>✗</cell><cell>✓</cell><cell>✗</cell><cell>✓</cell><cell>✗</cell><cell>✓</cell></row><row><cell>Cora-full</cell><cell cols="6">65.87 ± 0.61 65.81 ± 0.69 68.54 ± 0.76 68.44 ± 0.88 70.36 ± 0.51 70.40 ± 0.71</cell></row><row><cell>Pubmed</cell><cell cols="6">85.02 ± 0.43 85.23 ± 0.57 87.94 ± 0.52 88.09 ± 0.59 89.81 ± 0.56 90.08 ± 0.52</cell></row><row><cell>Arxiv</cell><cell cols="6">69.07 ± 0.01 70.22 ± 0.03 69.97 ± 0.16 70.98 ± 0.21 72.93 ± 0.19 73.22 ± 0.10</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Test and validation accuracy (%) of C&amp;S and trainable C&amp;S with MLP as base predictor. Validation accuracy reported in parentheses. Products 67.60 ± 0.15 (87.07 ± 0.05) 83.16 ± 0.13 (91.70 ± 0.06) 83.10 ± 0.15 (91.99 ± 0.07)</figDesc><table><row><cell>Method</cell><cell>MLP</cell><cell>MLP+C&amp;S</cell><cell>MLP+Trainable C&amp;S</cell></row><row><cell cols="4">Cora-full 60.12 ± 0.29 (61.09 ± 0.39) 66.95 ± 1.46 (68.26 ± 1.24) 67.89 ± 1.37 (69.09 ± 1.25)</cell></row><row><cell cols="4">Pubmed 88.72 ± 0.34 (89.25 ± 0.26) 89.12 ± 0.27 (89.45 ± 0.17) 89.76 ± 0.17 (89.62 ± 0.18)</cell></row><row><cell>Arxiv</cell><cell cols="3">71.48 ± 0.15 (72.95 ± 0.05) 73.05 ± 0.35 (74.01 ± 0.17) 73.03 ± 0.18 (74.44 ± 0.08)</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>where the bound is independent of α. Consequently,</p><p>such that when combined with the other expressions above, the conclusion </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B EXPERIMENTAL DETAILS B.1 TRAINABLE LABEL PROPAGATION</head><p>In Table <ref type="table">1</ref>, we employ trainable label propagation with the same settings as the original label propagation algorithm, including the trade-off term λ in (1) and the number of propagation steps. In the implementation, we set λ as 0.6 and use 50 propagation steps. We search the best splitting probability α in the range of {0.05, 0.1, . . . , 0.95}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 DETERMINISTIC LABEL TRICK APPLIED TO GNNS WITH LINEAR LAYERS</head><p>We use three models with linear propagation layers and simply choose one specific set of hyperparameters and run the model on each dataset with or without label trick in Table <ref type="table">3</ref>. The hyperparameters for each model is described as follows. For TWIRLS, we use 2 propagation steps for the linear propagation layers and λ is set to 1 <ref type="bibr" target="#b28">(Yang et al., 2021)</ref>. For SIGN, we sum up all immediate results instead of concatenating them to simplify the implementation and speed up training. Our SIGN model also use 5 propagation steps, and we tune the number of MLP layers from 1 or 2 on each dataset. And for SGC, the number of propagation steps is set to 3, and there is one extra linear transformation after the propagation steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 EFFECT OF SPLITTING PROBABILITY</head><p>To further present the effect of splitting probability α, in Figure <ref type="figure">1</ref>, we use α in the range of {0.00625, 0.0125, 0.025, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.975, 0.9875, 0.99375} to compare a linear model (similar to a trainable label propagation algorithm with features and labels as input) with a 3-layer <ref type="bibr">GCN (Kipf &amp; Welling, 2016)</ref>. Each model uses the same set of hyperparameters, except for the number of epochs, since when α is close to zero or one, the model requires more epochs to converge. For the linear model, λ is set to 0.9 and the number of propagation steps is 9. The GCN has 256 hidden channels and an activation function of ReLU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 TRAINABLE CORRECT AND SMOOTH</head><p>We begin by restating the predictor formulation of trainable Correct and Smooth in ( <ref type="formula">14</ref>):</p><p>Note that the computation of Ỹ s and Ỹ c does not involve any trainable parameters. Therefore, we can compute them beforehand for each training split.</p><p>We now describe our experimental setup used to produce Table <ref type="table">4</ref>. We first trained an MLP with exactly the same hyperparameters as in <ref type="bibr" target="#b7">(Huang et al., 2020)</ref>. For each splitting probability α ∈ {0.1, 0.2, • • • , 0.9}, we generate 10 splits and precompute Ỹ s and Ỹ c . Then for each training epoch, we cycle over the set of Ỹ s and Ỹ c .</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Anomymous Authors. under review</title>
		<imprint>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Fabrizio</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emanuele</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Eynard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.11198</idno>
		<title level="m">Sign: Scalable inception graph neural networks</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Samuel S Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Label propagation via teaching-to-learn and learning-to-teach</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1452" to="1465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Ogb-lsc: A large-scale challenge for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maho</forename><surname>Nakata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.09430</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Combining label propagation and simple models out-performs graph neural networks</title>
		<author>
			<persName><forename type="first">Qian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhay</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ser-Nam</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Austin</forename><forename type="middle">R</forename><surname>Benson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.13993</idno>
		<imprint>
			<date type="published" when="2003">2020. 2, 3</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Boost then convolve: Gradient boosting meets graph neural networks</title>
		<author>
			<persName><forename type="first">Sergei</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liudmila</forename><surname>Prokhorenkova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.08543</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Manifold-based similarity adaptation for label propagation</title>
		<author>
			<persName><forename type="first">Masayuki</forename><surname>Karasuyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroshi</forename><surname>Mamitsuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.05997</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Flag: Adversarial data augmentation for graph neural networks</title>
		<author>
			<persName><forename type="first">Kezhi</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mucong</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gavin</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.09891</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.07476</idno>
		<title level="m">Training graph neural networks with 1000 layers</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI conference on artificial intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Lanczosnet: Multi-scale deep graph convolutional networks</title>
		<author>
			<persName><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhizhen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.01484</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning to propagate labels: Transductive propagation network for few-shot learning</title>
		<author>
			<persName><forename type="first">Yanbin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minseop</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saehoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunho</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10002</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AI magazine</title>
				<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Masked label prediction: Unified message passing model for semi-supervised classification</title>
		<author>
			<persName><forename type="first">Yunsheng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengjie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shikun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.03509</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Adaptive graph diffusion networks with hop-wise attention</title>
		<author>
			<persName><forename type="first">Chuxiong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoshi</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15024</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Label propagation through linear neighborhoods</title>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changshui</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="67" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Unifying graph convolutional neural networks and label propagation</title>
		<author>
			<persName><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.06755</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Bag of tricks for node classification with graph neural networks</title>
		<author>
			<persName><forename type="first">Yangkun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiarui</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wipf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.13355</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amauri</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2007">2019. 2, 4, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<imprint>
			<date type="published" when="2018">2018a</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018b</date>
			<biblScope unit="page" from="5453" to="5462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Graph neural networks inspired by classical iterative algorithms</title>
		<author>
			<persName><forename type="first">Yongyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangkun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinjing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhewei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zengfeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wipf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2003">2021. 2, 3</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Link prediction based on graph neural networks</title>
		<author>
			<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="5165" to="5175" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
