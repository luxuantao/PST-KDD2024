<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Automated Self-Supervised Learning for Graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-06-10">10 Jun 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wei</forename><surname>Jin</surname></persName>
							<email>jinwei2@msu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Xiaorui</forename><surname>Liu</surname></persName>
							<email>xiaorui@msu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Xiaoyu</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
							<email>mayao4@msu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Neil</forename><surname>Shah</surname></persName>
							<email>nshah@snap.com</email>
						</author>
						<author>
							<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Clu</forename><surname>Dgi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Pairdis</forename><surname>Pairsim Par</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Michigan State University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Michigan State University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Michigan State University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Michigan State University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Michigan State University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Automated Self-Supervised Learning for Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-06-10">10 Jun 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2106.05470v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph self-supervised learning has gained increasing attention due to its capacity to learn expressive node representations. Many pretext tasks, or loss functions have been designed from distinct perspectives. However, we observe that different pretext tasks affect downstream tasks differently across datasets, which suggests that searching over pretext tasks is crucial for graph self-supervised learning. Different from existing works focusing on designing single pretext tasks, this work aims to investigate how to automatically leverage multiple pretext tasks effectively. Nevertheless, evaluating representations derived from multiple pretext tasks without direct access to ground truth labels makes this problem challenging. To address this obstacle, we make use of a key principle of many real-world graphs, i.e., homophily, or the principle that "like attracts like," as the guidance to effectively search various self-supervised pretext tasks. We provide theoretical understanding and empirical evidence to justify the flexibility of homophily in this search task. Then we propose the AUTOSSL framework which can automatically search over combinations of various self-supervised tasks. By evaluating the framework on 7 real-world datasets, our experimental results show that AUTOSSL can significantly boost the performance on downstream tasks including node clustering and node classification compared with training under individual tasks. Code will be released at https://github.com/ChandlerBang/AutoSSL.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graphs are pivotal data structures describing the relationships between entities in various domains such as social media, biology, transportation and financial systems <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. Due to their prevalence and rich descriptive capacity, pattern mining and discovery on graph data is a prominent research area with powerful implications. As the generalization of deep neural networks on graph data, graph neural networks (GNNs) have proved to be powerful in learning representations for graphs and associated entities (nodes, edges, subgraphs), and they have been employed in various applications such as node classification <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, node clustering <ref type="bibr" target="#b4">[5]</ref>, recommender systems <ref type="bibr" target="#b5">[6]</ref> and drug discovery <ref type="bibr" target="#b6">[7]</ref>.</p><p>In recent years, the explosive interest in self-supervised learning (SSL) has suggested its great potential in empowering stronger neural networks in an unsupervised manner <ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref>. Many selfsupervised methods have also been developed to facilitate graph representation learning <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> such as DGI <ref type="bibr" target="#b12">[13]</ref>, PAR/CLU <ref type="bibr" target="#b13">[14]</ref> and MVGRL <ref type="bibr" target="#b14">[15]</ref>. Given graph and node attribute data, they construct pretext tasks, which are called SSL tasks, based on structural and attribute information to provide self-supervision for training graph neural networks without accessing any labeled data. For example, the pretext task of PAR is to predict the graph partitions of nodes. We examine how a variety of SSL tasks including DGI, PAR, CLU, PAIRDIS <ref type="bibr" target="#b15">[16]</ref> and PAIRSIM <ref type="bibr" target="#b16">[17]</ref> perform over 3 datasets. Their node clustering and node classification performance ranks are illustrated in Figure <ref type="figure" target="#fig_0">1a</ref> and 1b, respectively. From these figures, we observe that different SSL tasks have distinct downstream performance cross datasets. This observation suggests that the success of SSL tasks strongly depends on the datasets and downstream tasks. Learning representations with a single task naturally leads to ignoring useful information from other tasks. As a result, searching SSL tasks is crucial, which motivates us to study on how to automatically compose a variety of graph self-supervised tasks to learn better node representations.</p><p>However, combining multiple different SSL tasks for unlabeled representation learning is immensely challenging. Although promising results have been achieved in multi-task self-supervised learning for computer vision, most of them assign equal weights to SSL tasks <ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref>. Such combination might not always yield better performance than a single task, as different tasks have distinct importance according to specific dataset and downstream tasks. To illustrate this intuition, we combine two SSL tasks, PAIRDIS and PAIRSIM, with varied weights and illustrate the corresponding node clustering performance in Figure <ref type="figure" target="#fig_0">1c</ref>. It clearly indicates that different choices of weights yield different performance. To circumvent this problem, we could plausibly search different weights for SSL tasks to optimize downstream tasks. However, to achieve such goal, we have two obstacles. First, the search space is huge, and thus search can be highly expensive. Hence, it is desirable to automatically learn these weights. Second, searching for optimal task weights typically requires guidance from downstream performance, which is naturally missing under the unsupervised setting. Thus, how to design an unsupervised surrogate evaluation measure that can guide the search process is necessary.</p><p>It is evident that many real-world graphs such as friendship networks, citation networks, co-authorship networks and co-purchase networks <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref> satisfy the homophily assumption, i.e., "like attracts like", or that connected nodes tend to share the same label. This is useful prior knowledge, as it directly relates the label information of downstream tasks to the graph structure. In this work, we explicitly take advantage of this prior knowledge and assume that the predicted labels from good node embeddings should also adhere to homophily. Given the lack of ground-truth labels during SSL, we propose a pseudo-homophily measure to evaluate the quality of the node embeddings trained from specific combinations of SSL task. With pseudo-homophily, we are able to design an automated framework for SSL task search, namely AUTOSSL. Our work makes three significant contributions:</p><p>(1) To bridge the gap between unsupervised representation and downstream labels, we propose pseudo-homophily to measure the quality of the representation. Moreover, given graphs with high homophily, we theoretically show that pseudo-homophily maximization can help maximize the upper bound of mutual information between pseudo-labels and downstream labels. (2) Based on pseudo-homophily, we propose two strategies to efficiently search SSL tasks, one employing evolution algorithm and the other performing differentiable search via meta-gradient descent. AUTOSSL is able to adjust the task weights during search as shown in Figure <ref type="figure" target="#fig_0">1d</ref>. <ref type="bibr" target="#b2">(3)</ref> We evaluate the proposed AUTOSSL by composing various individual tasks on 7 real-world datasets. Extensive experiments have demonstrated that AUTOSSL can significantly improve the performance of individual tasks on node clustering and node classification (e.g., up to 10.0% relative improvement on node clustering).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Related Work</head><p>Graph Neural Networks. Graph neural networks (GNNs) are powerful tools for extracting useful information from graph data <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b4">5]</ref>. They aim to learn a mapping function f θ parameterized by θ to map the input graph into a low-dimensional space. Most graph neural networks follow a message passing scheme <ref type="bibr" target="#b25">[26]</ref> where the node representation is obtained by aggregating the representation of its neighbors and its own representation.</p><p>Self-Supervised Learning in GNNs. Graph neural networks have achieved superior performance in various applications; but they also require costly task-dependent labels to learn rich representations. To alleviate the need for the huge amount of labeled data, recent studies have employed self-supervised learning in graph neural networks to provide additional supervision <ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref>. Specifically, those SSL methods construct a pre-defined pretext task to assign pseudo-labels for unlabeled nodes/graphs and then train the model on the designed pretext task to learn representations. A recent work JOAO <ref type="bibr" target="#b29">[30]</ref> on graph contrastive learning is proposed to automatically select data augmentation. We note that JOAO is designed for contrastive learning framework on graph classification task, while our proposed AUTOSSL focuses on node-level applications and is more flexible in that AUTOSSL can be applied to various SSL tasks.</p><p>Multi-Task Self-Supervised Learning. Our work is also related to multi-task self-supervised learning <ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref>. However, most of them assume the tasks with equal weights and perform training under the supervised setting. Our work differs from them by (1) learning different weights for different tasks and ( <ref type="formula" target="#formula_1">2</ref>) not requiring access to labeled data. Automated Loss Function Search. Tremendous efforts have been paid to automate every aspect of machine learning applications <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>, such as feature engineering, model architecture search and loss function search. Among them, our work is highly related to loss function search <ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref>. However, these methods are developed under the supervised setting and not applicable in self-supervised learning. Another related work is ELo <ref type="bibr" target="#b36">[37]</ref>, which evolves multiple self-supervised losses based on Zipf distribution matching for action recognition. However, it is designed exclusively for image data and not applicable to non-grid graph-structured data. The problem of self-supervised loss search for graphs remains rarely explored. To bridge the gap, we propose an automated framework for searching SSL losses towards graph data in an unsupervised manner.</p><p>3 Automated Self-Supervised Task Search with AUTOSSL</p><p>In this section, we present the proposed framework of automated self-supervised task search, namely AUTOSSL. Given a graph G, a GNN encoder f θ (•) and a set of n self-supervised losses (tasks) { 1 , 2 , . . . , n }, we aim at learning a set of loss weights {λ 1 , λ 2 , . . . , λ n } such that f θ (•) trained with the weighted loss combination n i=1 λ i i can extract meaningful features from the given graph data. The key challenge is how to mathematically define "meaningful features". If we have the access to the labels of the downstream task, we can define "meaningful features" as the features (node embeddings) that can have high performance on the given downstream task. Then we can simply adopt the downstream performance as the optimization goal and formulate the problem of automated self-supervised task search as follows:</p><formula xml:id="formula_0">min λ 1 ,••• ,λn H(f θ * (G)), s.t. θ * = arg min θ L(f θ , {λi}, { i}) = arg min θ n i=1 λi i(fθ (G)),<label>(1)</label></formula><p>where H denotes the quality measure for the obtained node embeddings, and it can be any metric that evaluates the downstream performance such as cross-entropy loss for the node classification task. However, under the self-supervised setting, we do not have the access to labeled data and thus cannot employ the downstream performance to measure the embedding quality. Instead, we need an unsupervised quality measure H to evaluate the quality of obtained embeddings. In a nutshell, one challenge of automated self-supervised learning is: how to construct the goal of automated task search without the access to label information of the downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Pseudo-Homophily</head><p>Most common graphs adhere to the principle of homophily, i.e., "birds of a feather flock together" <ref type="bibr" target="#b20">[21]</ref>, which suggests that connected nodes often belong to the same class; e.g. connected publications in a citation network often have the same topic, and friends in social networks often share interests <ref type="bibr" target="#b37">[38]</ref>.</p><p>Homophily is often calculated as the fraction of intra-class edges in a graph <ref type="bibr" target="#b38">[39]</ref>. Formally, it can be defined as follows, Definition 1 (Homophily). The homophily of a graph G with node label vector y is given by</p><formula xml:id="formula_1">h(G, y) = 1 |E| (v1,v2)∈E 1(y v1 = y v2 ),<label>(2)</label></formula><p>where y vi indicates node v i 's label and 1(•) is the indicator function.</p><p>We calculate the homophily for seven widely used datasets as shown in Appendix A and we find that they all have high homophily, e.g., 0.93 in the Physics dataset. Considering the high homophily in those datasets, intuitively the predicted labels from the extracted node features should also have high homophily. Hence, the prior information of graph homophily in ground truth labels can serve as strong guidance for searching combinations of self-supervised tasks. As mentioned before, in self-supervised tasks, the ground truth labels are not available. Motivated by DeepCluster <ref type="bibr" target="#b39">[40]</ref> which uses the cluster assignments of learned features as pseudo-labels to train the neural network, we propose to calculate the homophily based on the cluster assignments, which we term as pseudohomophily. Specifically, we first perform k-means clustering on the obtained node embeddings to get k clusters. Then the cluster results are used as the pseudo labels to calculate homophily based on Eq. ( <ref type="formula" target="#formula_1">2</ref>). Note that though many graphs in the real world have high homophily, there also exist heterophily graphs <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b40">41]</ref> which have low homophily. We leave such type of graphs for future work.</p><p>Theoretical analysis. In this work, we propose to achieve self-supervised task search via maximizing pseudo-homophily. To understand its rationality, we develop the following theorem to show that pseudo-homophily maximization is related to the upper bound of mutual information between pseudo-labels and ground truth labels.</p><p>Theorem 1. Suppose that we are given with a graph G = {V, E}, a pseudo label vector A ∈ {0, 1} N and a ground truth label vector B ∈ {0, 1} N defined on the node set. We denote the homophily of A and B over G as h A and h B , respectively. If the classes in A and B are balanced and h A &lt; h B , the following results hold: (1) the mutual information between A and B, i.e., MI(A,B), has an upper bound U A,B , where </p><formula xml:id="formula_2">U A,B = 1 N 2∆ log( 4 N ∆) + 2( N 2 − ∆) log 4 N ( N 2 − ∆) with ∆ = (h B −h A )|E|</formula><formula xml:id="formula_3">(2) if h A &lt; h A &lt; h B , we have U A,B &lt; U A ,B .</formula><p>Proof. The detailed proof of this theorem can be found in Appendix B.</p><p>The above theorem suggests that a larger difference between pseudo-homophily and real homophily results in a lower upper bound of mutual information between the pseudo-labels and ground truth labels. Therefore, maximizing pseudo-homophily is to maximize the upper bound of mutual information between pseudo-labels and ground truth labels, since we assume that the given graph has high homophily.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Search Algorithms</head><p>In the last subsection, we have demonstrated the importance of maximizing pseudo-homophily. Thus in the optimization problem of Eq. (1), we can simply set H to be negative pseudo-homophily. However, the evaluation of a specific task combination involves fitting a model and evaluating its pseudo-homophily, which can be highly expensive. Therefore, another challenge for automated self-supervised task search is how to design an efficient algorithm. In the following, we introduce the details of the search strategies designed in this work, i.e. AUTOSSL-ES and AUTOSSL-DS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">AUTOSSL-ES: Evolutionary Strategy</head><p>Evolution algorithms are often used in automated machine learning such as hyperparameter tuning due to their parallelism nature by design <ref type="bibr" target="#b41">[42]</ref>. In this work, we employ the covariance matrix adaptation evolution strategy (CMA-ES) <ref type="bibr" target="#b42">[43]</ref>, a state-of-the-art optimizer for continuous black-box functions, to evolve the combined self-supervised loss. We name this self-supervised task search approach as AUTOSSL-ES. In each iteration of CMA-ES, it samples a set of candidate solutions (i.e., task weights {λ i }) from a multivariate normal distribution and trains the GNN encoder under the combined loss function. The embeddings from the trained encoder are then evaluated by H. Based on H, CMA-ES adjusts the normal distribution to give higher probabilities to good samples that can potentially produce a lower value of H. Note that we constrain {λ i } in [0, 1] and sample 8 candidate combinations for each iteration, which is trivially parallelizable as every candidate combination can be evaluated independently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">AUTOSSL-DS: Differentiable Search via Meta-Gradient Descent</head><p>While the aforementioned AUTOSSL-ES is parallelizable, the search cost is still expensive because it requires to evaluate a large population of candidate combinations where every evaluation involves fitting the model in large training epochs. Thus, it is desired to develop gradient-based search methods to accelerate the search process. In this subsection, we introduce the other variant of our proposed framework, AUTOSSL-DS, which performs differentiable search via meta-gradient descent. However, pseudo-homophily is not differentiable as it is based on hard cluster assignments from k-means clustering. Next, we will first present how to make the clustering process differentiable and then introduce how to perform differentiable search.</p><p>Soft Clustering. Although k-means clustering assigns hard assignments of data samples to clusters, it can be viewed as a special case of Gaussian mixture model which makes soft assignments based on the posterior probabilities <ref type="bibr" target="#b43">[44]</ref>. Given a Gaussian mixture model with centroids {c 1 , c 2 , . . . , c k } and fixed variances σ 2 , we can calculate the posterior probability as follows:</p><formula xml:id="formula_4">p (x | c i ) = 1 √ 2πσ 2 exp − x − c i 2 2σ 2 , (<label>3</label></formula><formula xml:id="formula_5">)</formula><p>where x is the feature vector of data samples. By Bayes rule and considering an equal prior, i.e., p(c 1 ) = p(c 2 ) = . . . = p(c k ), we can compute the probability of a feature vector x belonging to a cluster c i as:</p><formula xml:id="formula_6">p (c i | x) = p (c i ) p (x | c i ) k j p (c j ) p (x | c j ) = exp − (x−ci) 2 2σ 2 k j=1 exp − (x−cj ) 2 2σ 2 . (<label>4</label></formula><formula xml:id="formula_7">)</formula><p>If σ → 0, we can obtain the hard assignments as the k-means algorithm. As we can see, the probability of each feature vector belonging to a cluster reduces to computing the distance between them. Then we can construct our homophily loss function as follows:</p><formula xml:id="formula_8">H(f θ * (G)) = 1 k|E| k i=1 (v1,v2)∈E (p(x v1 | c i ), p(x v2 | c i )) ,<label>(5)</label></formula><p>where is a loss function measuring the difference between the inputs. With soft assignments, the gradient of H w.r.t. θ becomes tractable.</p><p>Search via Meta Gradient Descent. We now detail the differentiable search process for AUTOSSL-DS. A naive method to tackle bilevel problems is to alternatively optimize the inner and outer problems through gradient descent. However, we cannot perform gradient descent for the outer problem in Eq. ( <ref type="formula" target="#formula_0">1</ref>) where H is not directly related to {λ i }. To address this issue, we can utilize meta-gradients, i.e., gradients w.r.t. hyperparameters, which have been widely used in solving bi-level problems in meta learning <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46]</ref>. To obtain meta-gradients, we need to backpropagate through the learning phase of the neural network. Concretely, the meta-gradient of H with respect to {λ i } is expressed as</p><formula xml:id="formula_9">∇ meta {λi} := ∇ {λi} H(f θ * (G)) s.t. θ * = opt θ (L(f θ , {λ i , i })),<label>(6)</label></formula><p>where opt θ stands for the inner optimization that obtains θ * and it is typically multiple steps of gradient descent. As an illustration, we consider opt θ as T + 1 steps of vanilla gradient descent with learning rate ,</p><formula xml:id="formula_10">θ t+1 = θ t − ∇ θt L(f θt , {λ i , i }).<label>(7)</label></formula><p>By unrolling the training procedure, we can express meta-gradient as</p><formula xml:id="formula_11">∇ meta {λi} := ∇ {λi} H(f θ T (G)) = ∇ f θ T H(f θ T (G)) • [∇ {λi} f θ T (G) + ∇ θ T f θ T (G)∇ {λi} θ T ], (8) with ∇ {λi} θ T = ∇ {λi} θ T −1 − ∇ {λi} ∇ θ T −1 L(f θ T −1 , {λ i , i }). Since ∇ {λi} f θ T (G) = 0, we have ∇ meta {λi} := ∇ {λi} H(f θ T (G)) = ∇ f θ T H(f θ T (G)) • ∇ θ T f θ T (G)∇ {λi} θ T .<label>(9)</label></formula><p>Note that θ T −1 also depends on the task weights {λ i } (see Eq. ( <ref type="formula" target="#formula_10">7</ref>)). Thus, its derivative w.r.t. the task weights chains back until θ 0 . By unrolling all the inner optimization steps, we can obtain the meta-gradient ∇ meta {λi} and use it to perform gradient descent on {λ i } to reduce H:</p><formula xml:id="formula_12">{λ i } ← − {λ i } − η∇ meta {λi} ,<label>(10)</label></formula><p>where η is the learning rate for meta-gradient descent (outer optimization).</p><p>However, if we use the whole training trajectory θ 0 , θ 1 , . . . , θ T to calculate the precise meta-gradient, it would have an extremely high memory footprint since we need to store θ 0 , θ 1 , . . . , θ T in memory. Thus, inspired by DARTS <ref type="bibr" target="#b31">[32]</ref>, we use an online updating rule where we only perform one step gradient descent on θ and then update {λ i } in each iteration. Note that under this training paradigm our aim is not to automatically find the best task weights{λ i }, as {λ i } are changing at each iteration. Instead we target at generating appropriate gradients to update GNN parameters that can minimize H. During the process, we constrain {λ i } in [0, 1] and dynamically adjust the task weights in a differentiable manner. The detailed algorithm for AUTOSSL-DS is summarized in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Evaluation</head><p>In this section, we empirically evaluate the effectiveness of the proposed AUTOSSL on self-supervised task search on real-world datasets. We aim to answer four questions as follows. Q1: Can AUTOSSL achieve better performance compared to training on individual SSL tasks? Q2: How does AUTOSSL compare to other unsupervised and supervised node representation learning baselines? Q3: Can we observe relations between AUTOSSL's pseudo-homophily objective and downstream classification performance? and Q4: How do the SSL task weights, pseudo-homophily objective, and downstream performance evolve during AUTOSSL's training?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setting</head><p>Since our goal is to enable automated combination search and discovery of SSL tasks, we use 5 such tasks including 1 contrastive learning method and 4 predictive methods -(1) DGI <ref type="bibr" target="#b12">[13]</ref>: it is a contrastive learning method maximizing the mutual information between graph representation and node representation; (2) CLU <ref type="bibr" target="#b13">[14]</ref>, it predicts partition labels from Metis graph partition <ref type="bibr" target="#b46">[47]</ref>; (3) PAR <ref type="bibr" target="#b13">[14]</ref>, it predicts clustered labels from k-means clustering on node features; (4) PAIRSIM <ref type="bibr" target="#b16">[17]</ref>, it predicts pairwise feature similarity between node pairs and ( <ref type="formula" target="#formula_8">5</ref>) PAIRDIS <ref type="bibr" target="#b15">[16]</ref>, it predicts shortest path length between node pairs. The proposed AUTOSSL framework automatically learns to jointly leverage the 5 above tasks and carefully mediate their influence. We also note that (1) the recent contrastive learning method, MVGRL <ref type="bibr" target="#b14">[15]</ref>, needs to deal with a dense diffusion matrix and is prohibitively memory/time-consuming for larger graphs; thus, we only include it as a baseline to compare as shown in Table <ref type="table" target="#tab_2">2</ref>; and (2) the proposed framework is general and it is straightforward to combine other SSL tasks.</p><p>We perform experiments on 7 real-world datasets widely used in the literature <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b48">49]</ref>, i.e., Physics, CS, Photo, Computers, WikiCS, Citeseer and CoraFull. To demonstrate the effectiveness of the proposed framework, we follow <ref type="bibr" target="#b14">[15]</ref> and evaluate all methods on two different downstream tasks: node clustering and node classification. For the task of node clustering, we perform k-means clustering on the obtained embeddings. We set the number of clusters to the number of ground truth classes and report the normalized mutual information (NMI) between the cluster results and ground truth labels. Regarding the node classification task, we train a logistic regression model on the obtained node embeddings and report the classification accuracy on test nodes. Note that labels are never used for self-supervised task search. All experiments are performed under 5 different random seeds and results are averaged. Following DGI and MVGRL, we use a simple one-layer GCN <ref type="bibr" target="#b2">[3]</ref> as our encoder and set the size of hidden dimensions to 512. We set 2σ 2 = 0.001 and use L1 loss in the homophily loss function throughout the experiments. Further details of dataset statistics, data splits, and hyperparameter settings can be found in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Performance Comparison with Individual Tasks</head><p>To answer Q1, Table <ref type="table" target="#tab_0">1</ref> summarizes the results for individual self-supervised tasks and AUTOSSL under the two downstream tasks, i.e., node clustering and node classification. From the table, we make several observations. Obs. 1: individual self-supervised tasks have different node clustering and node classification performance for different datasets. For example, in Photo, DGI achieves the highest classification accuracy while PAR achieves the highest clustering performance; CLU performs better than PAIRDIS in both NMI and ACC on Physics while it cannot outperform PAIRDIS in WikiCS, Citeseer, Computers and CoraFull. This observation suggests the importance of searching suitable SSL tasks to benefit downstream tasks on different datasets. Obs. 2: Most of the time, combinations of SSL tasks searched by AUTOSSL can consistently improve the node clustering and classification performance over the best individual task on the all datasets. For example, the relative improvement over the best individual task on NMI from AUTOSSL-ES is 7.3% for WikiCS and 10.0% for Photo, and its relative improvement on ACC is 1.3% for WikiCS. These results indicate that composing multiple SSL tasks can help the model encode different types of information and avoid overfitting to one single task. Obs. 3: We further note that individual tasks resulted in different pseudo-homophily as shown in the P-H rows of Table <ref type="table" target="#tab_0">1</ref>. Among them, CLU tends to result in a low pseudo-homophily and often performs much worse than other tasks in node clustering task, which supports our theoretical analysis in Section 3.1. It also demonstrates the necessity to increase pseudo-homophily as the two variants of AUTOSSL effectively search tasks that lead to higher pseudo-homophily. Obs. 4: The performance of AUTOSSL-ES and AUTOSSL-DS is close when their searched tasks lead to similar pseudo-homophily: the differences in pseudo-homophily, NMI and ACC are relative smaller in datasets other than Photo and Computers. It is worth noting that sometimes AUTOSSL-DS can even achieve higher pseudo-homophily than AUTOSSL-ES. This indicates that the online updating rule for {λ i } in AUTOSSL-DS not only can greatly reduce the searching time but also can generate good task combinations. In addition to efficiency, we highlight another major difference between them: AUTOSSL-ES directly finds the best task weights while AUTOSSL-DS adjusts the task weights to generate appropriate gradients to update model parameters.</p><p>Hence, if we hope to find the best task weights and retrain the model, we should turn to AUTOSSL-ES. More details on their differences can be found in Appendix D.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Performance Comparison with Supervised and Unsupervised Baselines</head><p>To answer Q2, we compare AUTOSSL with representative unsupervised and supervised node representation learning baselines. Specifically, for node classification we include 4 unsupervised baselines, i.e., GAE <ref type="bibr" target="#b24">[25]</ref>, VGAE <ref type="bibr" target="#b24">[25]</ref>, ARVGA <ref type="bibr" target="#b4">[5]</ref> and MVGRL, and 2 supervised baselines, i.e. GCN and GAT <ref type="bibr" target="#b3">[4]</ref>. We also provide the logistic regression performance on raw features and embeddings generated by a randomly initialized encoder, named as Raw-Feat and Random-Init, respectively. Note that the two supervised baselines, GCN and GAT, use label information for node representation learning in an end-to-end manner, while other baselines and AUTOSSL do not leverage label information to learn representations. The average performance and variances are reported in Table <ref type="table" target="#tab_2">2</ref>. From the table, we find that AUTOSSL outperforms unsupervised baselines in all datasets except Citeseer while the performance on Citeseer is still comparable to the state-of-the-art contrastive learning method MVGRL. When compared to supervised baselines, AUTOSSL-DS outperforms GCN and GAT in 4 out of 7 datasets, e.g., a 1.7% relative improvement over GAT on Computers. AUTOSSL-ES also outperforms GCN and GAT in 3/4 out of 7 datasets. In other words, our unsupervised representation learning AUTOSSL can achieve comparable performance with supervised representation learning baselines. In addition, we use the same unsupervised baselines for node clustering and report the results in  relative improvement on these two datasets. These results further validate that composing SSL tasks appropriately can produce expressive and generalizable representations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Relation between Downstream Performance and Pseudo-Homophily</head><p>In this subsection, we investigate the relation between downstream performance and pseudohomophily and correspondingly answer Q3. Specifically, we use the candidate task weights sampled in the AUTOSSL-ES searching trajectory, and illustrate their node clustering (NMI) and node classification performance (ACC) with respect to their pseudo-homophily. The results on Computers and WikiCS are shown in Figure <ref type="figure" target="#fig_2">2</ref> and results for other datasets are shown in Appendix D.1. We observe that the downstream performance tends to be better if the learned embeddings tend to have higher pseudo-homophily. We also can observe that clustering performance has a clear relation with pseudo-homophily for all datasets. Hence, the results empirically support our theoretical analysis in Section 3.1 that lower pseudo-homophily leads to a lower upper bound of mutual information with ground truth labels. While classification accuracy has a less evident pattern, we can still observe that higher accuracy tends to concentrate on the high pseudo-homophily regions for 5 out of 7 datasets.   Computers; DGI is crucial for citation/co-authorship networks, i.e. Physics, CS, Citeseer, and CoraFull. We conjecture that local structure information (the information that Par captures) is essential for co-purchase networks while both local and global information (the information that DGI captures) are necessary in citation/co-authorship networks. Obs. 3: AUTOSSL-ES always gives very low weights to CLU. It could be the reason that the pseudo-labels clustered from raw features are not good supervision on the selected datasets.</p><p>We also provide the evolution of task weights in AUTOSSL-DS for CoraFull dataset in Figure <ref type="figure">3b</ref>. The weights of the 5 tasks eventually become stable: CLU and PAIRDIS are assigned with small values while PAIRSIM, DGI and CLU are assigned with large values. Thus, both AUTOSSL-ES and AUTOSSL-DS agree that PAIRDIS and PAR are less important for CoraFull.</p><p>Pseudo-Homophily Over Iterations. We further investigate how pseudo-homophily changes over iterations. For AUTOSSL-ES, we illustrate the mean value of resulted pseudo-homophily in each iteration (round) in Figure <ref type="figure">4</ref>. We only show the results on CoraFull and Citeseer while similar patterns are exhibited in other datasets. It is clear that AUTOSSL-ES can effectively increase the pseudo-homophily and thus search for better self-supervised task weights. For AUTOSSL-DS, we plot the changes of pseudo-homophily, NMI, ACC and homophily loss (Eq. ( <ref type="formula" target="#formula_8">5</ref>)) in Figure <ref type="figure">5</ref>. From Figure <ref type="figure">5a</ref> and 5b, we can observe that pseudo-homophily first increases and then becomes stable through iterations. The situation is a bit different for clustering and classification performance: NMI and ACC first increase with the increase of pseudo-homophily and then drop when pseudo-homophily is relatively stable. This indicates that overtraining can hurt downstream performance as the model will have the risk of overfitting on the combined SSL tasks. However, as shown in the figure, if we stop at the iteration when pseudo-homophily reaches the maximum value we can still get a high NMI and ACC. On a separate note, Figure <ref type="figure">5c</ref> shows how the homophily loss used in AUTOSSL-DS changes over iterations. We note that in the first iterations the homophily loss is low but the pseudo-homophily is also low. This is because the embeddings in the first few epochs are less separable and would lead to very close soft-assignment of clusters. As shown in the figure, however, the problem is resolved as the embeddings become more distinguishable through iterations. Thus, we argue that the homophily loss in Eq. ( <ref type="formula" target="#formula_8">5</ref>) is still a good proxy in optimizing pseudo-homophily.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Graph self-supervised learning has achieved great success in learning expressive node/graph representations. In this work, however, we find that SSL tasks designed for graphs perform differently on different datasets and downstream tasks. Thus, it is worth composing multiple SSL tasks to jointly encode multiple sources of information and produce more generalizable representations.</p><p>However, without access to labeled data, it poses a great challenge in measuring the quality of the combinations of SSL tasks. To address this issue, we take advantage of graph homophily and propose pseudo-homophily to measure the quality of combinations of SSL tasks. We then theoretically show that maximizing pseudo-homophily can help maximize the upper bound of mutual information between the pseudo-labels and ground truth labels. Based on the pseudo-homophily measure, we develop two automated frameworks AUTOSSL-ES and AUTOSSL-DS to search the task weights efficiently. Extensive experiments have demonstrated that AUTOSSL is able to produce more generalize representations by combining various SSL tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Experimental Setup</head><p>Dataset Statistics. We evaluate the proposed framework on seven real-world datasets. The dataset statistics are shown in 4. All datasets can be loaded from PyTorch Geometric <ref type="bibr" target="#b49">[50]</ref>. When we evaluate the node classification performance, we need to use the training and test data. For WikiCS <ref type="bibr" target="#b48">[49]</ref> and Citeseer <ref type="bibr" target="#b47">[48]</ref>, we use the public data splits provided by the authors. For other datasets, we split the nodes into 10%/10%/80% for training/validation/test. Hyper-parameter Settings. When calculating pseudo-homophily, we set the number of clusters to 5 for all datasets. Following DGI <ref type="bibr" target="#b12">[13]</ref> and MVGRL <ref type="bibr" target="#b14">[15]</ref>, we we use a simple one-layer GCN <ref type="bibr" target="#b2">[3]</ref> as our encoder. We set the size of hidden dimensions to 512, weight decay to 0, dropout rate to 0. For individual SSL methods and AUTOSSL-ES, we set learning rate to 0.001, use Adam optimizer <ref type="bibr" target="#b50">[51]</ref>, train the models with 1000 epochs and adopt early stopping strategy with a patience of 20 epochs. For AUTOSSL-DS, we train the models with 1000 epochs and choose the model checkpoint that achieves the highest pseudo-homophily. We use Adam optimizer for both inner and outer optimization. The learning rate for outer optimization is set to 0.05. For AUTOSSL-ES, we use a population size of 8 for each round. Due to limited computational resources, we perform 80 rounds for Citeseer, 40 rounds for CS, Computers, CoraFull, Photo, Physics, Computers and WikiCS. We repeat the experiments on 5 different random seeds and report the mean values and variances for downstream performance. To fit DGI into GPU memory on larger datasets and accelerate its training, instead of using all the nodes we sample 2000 positive samples and 2000 negative samples for DGI on all datasets except Citeseer .</p><p>Hardware and Software Configurations. We perform experiments on one NVIDIA Tesla K80 GPU and one NVIDIA Tesla V100 GPU. Additionally, we use eight CPUs, with the model name as Intel(R) Xeon(R) Platinum 8260 CPU @ 2.40GHz. The operating system we use is CentOS Linux 7 (Core).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Proof</head><p>Theorem 1. Suppose that we are given with a graph G = {V, E}, a pseudo label vector A ∈ {0, 1} N and a ground truth label vector B ∈ {0, 1} N defined on the node set. We denote the homophily of A and B over G as h A and h B , respectively. If the classes in A and B are balanced and h A &lt; h B , the following results hold: (1) the mutual information between A and B, i.e., MI(A,B), has an upper bound U A,B , where </p><formula xml:id="formula_13">U A,B = 1 N 2∆ log( 4 N ∆) + 2( N 2 − ∆) log 4 N ( N 2 − ∆) with ∆ = (h B −h A )|E|</formula><formula xml:id="formula_14">(2) if h A &lt; h A &lt; h B , we have U A,B &lt; U A ,B .</formula><p>Proof. (1) We start with the proof of the first result. The mutual information between two random variables X and Y is expressed as</p><formula xml:id="formula_15">M I(X, Y ) = y∈Y x∈X p (A,B) (x, y) log p (X,Y ) (x, y) p X (x)p Y (y) .<label>(11)</label></formula><p>Let A i and B i denote the set of nodes in the i-th class in A and B, respectively. Following the definition in Eq. <ref type="bibr" target="#b10">(11)</ref>, the mutual information between A and B can be formulated as,</p><formula xml:id="formula_16">M I(A, B) = n A −1 i=0 n B −1 j=0 |A i ∩ B j | N log N |A i ∩ B j | |A i | |B j | ,<label>(12)</label></formula><p>where n A , n B denote the number of classes in A and B. Since here we only consider 2 classes in A and B, we have</p><formula xml:id="formula_17">M I(A, B) = |A 0 ∩ B 0 | N log N |A 0 ∩ B 0 | |A 0 | |B 0 | + |A 0 ∩ B 1 | N log N |A 0 ∩ B 1 | |A 0 | |B 1 | + |A 1 ∩ B 0 | N log N |A 1 ∩ B 0 | |A 1 | |B 0 | + |A 1 ∩ B 1 | N log N |A 1 ∩ B 1 | |A 1 | |B 1 | . (<label>13</label></formula><formula xml:id="formula_18">) Let |A 0 ∩ B 0 | = x, |A 0 | = a and |B 0 | = b. We then have          |A 0 | + |A 1 | = N ⇒ |A 1 | = N − a, |B 0 | + |B 1 | = N ⇒ |B 1 | = N − b, |A 0 ∩ B 0 | + |A 0 ∩ B 1 | = |A 0 | ⇒ |A 0 ∩ B 1 | = a − x, |A 0 ∩ B 0 | + |A 1 ∩ B 0 | = |B 0 | ⇒ |A 1 ∩ B 0 | = b − x, |A 0 ∩ B 1 | + |A 1 ∩ B 1 | = |B 1 | ⇒ |A 1 ∩ B 1 | = N − b − a + x.<label>(14)</label></formula><p>With the equations above, we rewrite M I(A, B) as follows,</p><formula xml:id="formula_19">M I(A, B) = 1 N [x log N x ab + (a − x) log N (a − x) a(N − b) + (b − x) log N (b − x) (N − a)b + (N − b − a + x) log N (N − b − a + x) (N − a)(N − b) ].<label>(15)</label></formula><p>Then we rewrite result (1) in the theorem as an optimization problem,  </p><formula xml:id="formula_20">max f (x) = M I(A, B) (16) with constraints,          0 ≤ |A 0 ∩ B 0 | ≤ |A 0 | ⇒ 0 ≤ x ≤ a, 0 ≤ |A 0 ∩ B 0 | ≤ |B 0 | ⇒ 0 ≤ x ≤ b, |A 1 ∩ B 1 | ≥ 0 ⇒ x ≥ a + b − N , |A 0 ∩ B 0 | + |A 1 ∩ B 1 | ≤ N ⇒ x ≤ a+b 2 , |A 0 ∩ B 1 | + |A 1 ∩ B 0 | ≤ N ⇒ x ≥ a+b−N 2 ,<label>(17)</label></formula><formula xml:id="formula_21">∩ B 0 | + |A 1 ∩ B 1 | = N . Let ∆ = |E A |−|E B | 2dmax = (h B −h A )|E| 2dmax</formula><p>, and we have</p><formula xml:id="formula_22">|A 0 ∩ B 0 | + |A 1 ∩ B 1 | ≤ N − 2∆ and |A 0 ∩ B 1 | + |A 1 ∩ B 0 | ≤ N − 2∆.</formula><p>tasks, from a multivariate normal distribution. Then we train K graph neural networks independently on each set of task weights. Afterwards, we calculate the pseudo-homohily for each network and adjust the mean and variance of the multivariate normal distribution through CMA-ES based on their pseudo-homohily.</p><p>The detailed algorithm for AUTOSSL-DS is summarized in Algorithm 2. Specifically, we first update the GNN parameter θ through one step gradient descent; then we perform k-means clustering to obtain centroids, which are used to calculate the homophily loss H. Afterwards, we calculate the meta-gradient ∇ meta {λi} , update {λ i } through gradient descent and clip {λ i } to [0, 1]. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Additional Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Relationship between Downstream Performance and Pseudo-Homophily</head><p>We provide more results on the relation between downstream performance and pseudo-homophily in Figure <ref type="figure" target="#fig_10">7</ref>. Observations are already made in Section 4.4.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Comparison with Different Strategy</head><p>In this subsection, we examine how other strategies of assigning task weights affect the quality of learned representations. The results are summarized in Table <ref type="table" target="#tab_8">6</ref>. In this table, "Best SSL" indicates the best performance achieved by the individual tasks; "Random Weight" indicates the performance achieved by randomly assigning task weights; "Equal Weight" indicates the performance achieved by assigning the same task weights (i.e., all 1). The values that outperform "Best SSL" are underlined.</p><p>From the table, we make two observations. Obs 1. Unlike AUTOSSL, "Random Weight" and "Equal Weight" would hurt both NMI and ACC on some datasets, e.g., Citeseer. This suggests that SSL tasks might conflict with each other and thus harm the downstream performance. Obs 2. In some cases like Physics, "Equal Weight" can also improve both ACC and NMI, which aligns well with our initial motivation that combinations of SSL can help capture multiple sources of information and benefit the downstream performance. The two observations suggest that it is important to design a clever strategy that can automatically compose graph SSL tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Comparison with Random Search</head><p>In this subsection, we choose Citeseer to study the difference between random search and evolutionary algorithm (AUTOSSL-ES), and report the result in Table <ref type="table" target="#tab_9">7</ref>. Specifically, the random search method randomly generates 800 sets of tasks weights and we evaluate the pseudo-homophily from the models trained with those task weights. Note that in AUTOSSL-ES we also evaluated 800 sets of tasks weights in total. From the table, we can see that random search is not as efficient as AUTOSSL-ES: with the same search cost, the resulted pseudo-homophily of random search is not as high as AUTOSSL-ES and the downstream performance is also inferior. This result suggests that search with evolutionary algorithm can find the optimum faster than random search. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Broader Impact</head><p>Graph neural networks (GNNs) are commonly used for node and graph representation learning tasks due to their representational power. Such models have also been recently proposed for use in large-scale social platforms for tasks including forecasting <ref type="bibr" target="#b51">[52]</ref>, friend ranking <ref type="bibr" target="#b52">[53]</ref> and item recommendation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b53">54]</ref>, which impact many end users. Like other machine learning models, GNNs can suffer from typical unfairness issues which may arise due to sensitive attributes, label parity issues, and more <ref type="bibr" target="#b54">[55]</ref>. Moreover, GNNs can also suffer from degree-related biases <ref type="bibr" target="#b55">[56]</ref>. Self-supervised learning (SSL) is often used to learn high-quality representations without supervision from labeled data sources, and is especially useful in low-resource settings or in pre-training/fine-tuning scenarios.</p><p>Several works have illustrated the potential for representations learned in a self-supervised way to encode bias unintentionally, for example in language modeling <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b57">58]</ref>, image representation learning <ref type="bibr" target="#b58">[59]</ref> and outlier detection <ref type="bibr" target="#b59">[60]</ref>.</p><p>Our work on automated self-supervised learning with graph neural networks shares the caveats of these two domains in terms of producing inadvertent or biased outcomes. We propose an approach to learn self-supervised representations by utilizing multiple types of pretext tasks in conjunction with one another. While this produces improved performance on standard tasks used for benchmarking representation quality, it does not guarantee that these representations are fair and should be used</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (a)(b): Performance of 5 SSL tasks ranked best (1) to worst (5) by color on node clustering and classification, showing disparate performance across datasets and tasks. (c): Clustering performance heatmap on Citeseer when combining 2 SSL tasks, PAIRSIM and PAIRDIS, with different weights. (d) AUTOSSL's search trajectory for task weights, achieving near-ideal performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>2dmax and d max denoting the largest node degree in the graph;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Relationship between downstream performance and pseudo-homophily.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>4. 5 Figure 3 :</head><label>53</label><figDesc>Photo CS Wiki-CS Citeseer CoraFull Computers</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: P-H change of AUTOSSL-ES</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>2dmax and d max denoting the largest node degree in the graph;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Note that the equality of |A 0 ∩ B 0 | + |A 1 ∩ B 1 | ≤ N holds when A and B are the same. However, A and B have different homophily, which indicates |A 0 ∩ B 0 | + |A 1 ∩ B 1 | cannot reach N (the same for |A 0 ∩ B 1 | + |A 1 ∩ B 0 |). Let E A , E B denote the inter-class edges for A and B, respectively. Thus, h A = 1 − |E A | |E| and h B = 1 − |E B | |E| . Since h A &lt; h B , we have |E A | &gt; |E B |. This indicates that there are at least |E A | − |E B | edges in A connecting nodes that belong to the same ground truth class, as shown in Figure 6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Illustration for |A 0 ∩ B 0 | + |A 1 ∩ B 1 |. The two dashed rectangles divide the nodes into A 0 and A 1 ; red and blue nodes denote nodes in B 0 and B 1 , respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>(j) Photo: ACC</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Relationship between downstream performance and pseudo-homophily.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Performance comparison of self-supervised tasks (losses) on node clustering and node classification. The NMI rows indicate node clustering performance; ACC rows indicate node classification accuracy (%); P-H stands for pseudo-homophily. AUTOSSL regularly outperforms individual pretext tasks. (Bold: best in all methods; Underline: best in individual tasks).</figDesc><table><row><cell>Dataset</cell><cell>Metric</cell><cell></cell><cell></cell><cell>Self-Supervised Task</cell><cell></cell><cell></cell><cell cols="2">AUTOSSL</cell></row><row><cell></cell><cell></cell><cell>CLU</cell><cell>PAR</cell><cell>PAIRSIM</cell><cell>PAIRDIS</cell><cell>DGI</cell><cell>ES</cell><cell>DS</cell></row><row><cell></cell><cell>NMI</cell><cell>0.177±0.02</cell><cell>0.262±0.02</cell><cell>0.341 ±0.01</cell><cell>0.169±0.04</cell><cell>0.310±0.02</cell><cell>0.366±0.01</cell><cell>0.344±0.02</cell></row><row><cell>WikiCS</cell><cell>ACC</cell><cell>74.19±0.21</cell><cell>75.81 ±0.17</cell><cell>75.80±0.17</cell><cell>75.28±0.08</cell><cell>75.49±0.17</cell><cell>76.80±0.13</cell><cell>76.58±0.28</cell></row><row><cell></cell><cell>P-H</cell><cell>0.549</cell><cell>0.567</cell><cell>0.693</cell><cell>0.463</cell><cell>0.690</cell><cell>0.751</cell><cell>0.749</cell></row><row><cell></cell><cell>NMI</cell><cell>0.318±0.00</cell><cell>0.416±0.00</cell><cell>0.428±0.01</cell><cell>0.404±0.01</cell><cell>0.439 ±0.00</cell><cell>0.449±0.01</cell><cell>0.449±0.01</cell></row><row><cell>Citeseer</cell><cell>ACC</cell><cell>63.17±0.19</cell><cell>69.25±0.51</cell><cell>71.36±0.42</cell><cell>70.72±0.53</cell><cell>71.64 ±0.44</cell><cell>72.14±0.41</cell><cell>72.00±0.32</cell></row><row><cell></cell><cell>P-H</cell><cell>0.787</cell><cell>0.916</cell><cell>0.885</cell><cell>0.901</cell><cell>0.934</cell><cell>0.943</cell><cell>0.934</cell></row><row><cell></cell><cell>NMI</cell><cell>0.171±0.00</cell><cell>0.433 ±0.00</cell><cell>0.387±0.01</cell><cell>0.300±0.01</cell><cell>0.318±0.02</cell><cell>0.447±0.01</cell><cell>0.459±0.01</cell></row><row><cell>Computers</cell><cell>ACC</cell><cell>75.20±0.20</cell><cell>87.26 ±0.15</cell><cell>82.64±1.15</cell><cell>85.20±0.41</cell><cell>83.45±0.54</cell><cell>87.26±0.64</cell><cell>88.18±0.43</cell></row><row><cell></cell><cell>P-H</cell><cell>0.313</cell><cell>0.562</cell><cell>0.374</cell><cell>0.365</cell><cell>0.405</cell><cell>0.578</cell><cell>0.770</cell></row><row><cell></cell><cell>NMI</cell><cell>0.128±0.00</cell><cell>0.498 ±0.00</cell><cell>0.409±0.02</cell><cell>0.406±0.01</cell><cell>0.462±0.01</cell><cell>0.506±0.01</cell><cell>0.500±0.00</cell></row><row><cell>CoraFull</cell><cell>ACC</cell><cell>44.93±0.53</cell><cell>57.54±0.32</cell><cell>56.23±0.59</cell><cell>58.48±0.80</cell><cell>60.42 ±0.39</cell><cell>61.01±0.50</cell><cell>61.10±0.68</cell></row><row><cell></cell><cell>P-H</cell><cell>0.494</cell><cell>0.887</cell><cell>0.649</cell><cell>0.728</cell><cell>0.888</cell><cell>0.903</cell><cell>0.895</cell></row><row><cell></cell><cell>NMI</cell><cell>0.658±0.01</cell><cell>0.767 ±0.01</cell><cell>0.749±0.01</cell><cell>0.635±0.03</cell><cell>0.747±0.01</cell><cell>0.772±0.01</cell><cell>0.771±0.01</cell></row><row><cell>CS</cell><cell>ACC</cell><cell>88.58±0.27</cell><cell>92.75 ±0.12</cell><cell>92.68±0.09</cell><cell>89.56±1.01</cell><cell>90.91±0.51</cell><cell>93.26±0.16</cell><cell>93.35±0.09</cell></row><row><cell></cell><cell>P-H</cell><cell>0.845</cell><cell>0.882</cell><cell>0.886</cell><cell>0.786</cell><cell>0.883</cell><cell>0.895</cell><cell>0.890</cell></row><row><cell></cell><cell>NMI</cell><cell>0.692±0.00</cell><cell>0.704 ±0.00</cell><cell>0.674±0.00</cell><cell>0.420±0.05</cell><cell>0.670±0.00</cell><cell>0.725±0.00</cell><cell>0.726±0.00</cell></row><row><cell>Physics</cell><cell>ACC</cell><cell>93.60±0.07</cell><cell>95.07 ±0.06</cell><cell>95.05±0.10</cell><cell>91.69±1.02</cell><cell>94.03±0.15</cell><cell>95.57±0.02</cell><cell>95.13±0.36</cell></row><row><cell></cell><cell>P-H</cell><cell>0.911</cell><cell>0.913</cell><cell>0.905</cell><cell>0.821</cell><cell>0.906</cell><cell>0.921</cell><cell>0.923</cell></row><row><cell></cell><cell>NMI</cell><cell>0.327±0.00</cell><cell>0.509 ±0.01</cell><cell>0.439±0.04</cell><cell>0.293±0.08</cell><cell>0.376±0.03</cell><cell>0.560±0.04</cell><cell>0.515±0.03</cell></row><row><cell>Photo</cell><cell>ACC</cell><cell>90.33±0.22</cell><cell>91.75±0.25</cell><cell>91.13±0.35</cell><cell>91.97±0.32</cell><cell>92.08 ±0.37</cell><cell>92.04±0.89</cell><cell>92.71±0.32</cell></row><row><cell></cell><cell>P-H</cell><cell>0.434</cell><cell>0.602</cell><cell>0.428</cell><cell>0.327</cell><cell>0.401</cell><cell>0.791</cell><cell>0.616</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>Node clustering NMI</cell><cell>0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55 Pseudo-homophily 0.1 0.2 0.3 0.4</cell><cell>Node clustering NMI</cell><cell>0.64 0.66 0.68 0.70 0.72 0.74 Pseudo-homophily 0.26 0.28 0.30 0.32 0.34 0.36</cell><cell>Node classification accuracy</cell><cell>0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55 Pseudo-homophily 80 82 84 86 88</cell><cell>Node classification accuracy</cell><cell>Pseudo-homophily 0.64 0.66 0.68 0.70 0.72 0.74 75.50 75.75 76.00 76.25 76.50 76.75 77.00 77.25</cell></row><row><cell></cell><cell>(a) Computers: NMI</cell><cell></cell><cell>(b) WikiCS: NMI</cell><cell></cell><cell>(c) Computers: ACC</cell><cell></cell><cell>(d) WikiCS: ACC</cell></row></table><note>. Both AUTOSSL-ES and AUTOSSL-DS show highly competitive clustering performance. For instance, AUTOSSL-ES achieves 22.2% and 27.5% relative improvement over the second best baseline on Physics and WikiCS; AUTOSSL-DS also achieves 22.2% and 19.8%</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Node classification accuracy (%). The last two rows are supervised baselines. AUTOSSL consistently outperforms alternative self-supervised approaches, and frequently outperforms supervised ones. (Bold: best; Underline: runner-up).</figDesc><table><row><cell>Model</cell><cell>WikiCS</cell><cell>Citeseer</cell><cell>Computers</cell><cell>CoraFull</cell><cell>CS</cell><cell>Physics</cell><cell>Photo</cell></row><row><cell>Random-Init</cell><cell>75.07±0.15</cell><cell>64.06±2.28</cell><cell>74.42±0.29</cell><cell>45.07±0.38</cell><cell>28.57±0.90</cell><cell>53.33±0.52</cell><cell>87.01±0.39</cell></row><row><cell>Raw-Feat</cell><cell>72.06±0.03</cell><cell>61.50±0.00</cell><cell>74.15±0.48</cell><cell>37.17±0.30</cell><cell>87.12±0.42</cell><cell>92.81±0.24</cell><cell>79.03±0.37</cell></row><row><cell>GAE</cell><cell>74.85±0.24</cell><cell>64.76±1.35</cell><cell>80.25±0.42</cell><cell>57.85±0.29</cell><cell>92.35±0.09</cell><cell>94.66±0.10</cell><cell>91.51±0.39</cell></row><row><cell>VGAE</cell><cell>74.16±0.16</cell><cell>67.50±0.42</cell><cell>81.05±0.41</cell><cell>53.72±0.30</cell><cell>92.15±0.16</cell><cell>94.58±0.17</cell><cell>88.98±1.05</cell></row><row><cell>ARVGA</cell><cell>71.64±1.03</cell><cell>46.88±2.15</cell><cell>67.61±0.92</cell><cell>45.20±1.33</cell><cell>87.26±1.07</cell><cell>93.84±0.13</cell><cell>77.74±1.16</cell></row><row><cell>MVGRL</cell><cell>75.89±0.12</cell><cell>72.36±0.49</cell><cell>84.66±0.62</cell><cell>60.56±0.33</cell><cell>90.18±0.19</cell><cell>94.30±0.20</cell><cell>92.49 ±0.40</cell></row><row><cell>AUTOSSL-ES</cell><cell>76.80±0.13</cell><cell>72.14 ±0.41</cell><cell>87.26 ±0.64</cell><cell>61.01 ±0.50</cell><cell>93.26 ±0.16</cell><cell>95.57±0.02</cell><cell>92.04±0.89</cell></row><row><cell>AUTOSSL-DS</cell><cell>76.58 ±0.28</cell><cell>72.00±0.32</cell><cell>88.18±0.43</cell><cell>61.10±0.68</cell><cell>93.35±0.09</cell><cell>95.13 ±0.36</cell><cell>92.71±0.32</cell></row><row><cell>GCN</cell><cell>76.42±0.02</cell><cell>71.26±0.15</cell><cell>87.53±0.21</cell><cell>63.77±0.37</cell><cell>93.04±0.09</cell><cell>95.66±0.15</cell><cell>93.09±0.11</cell></row><row><cell>GAT</cell><cell>77.30±0.01</cell><cell>71.00±0.62</cell><cell>86.74±0.69</cell><cell>63.73±0.43</cell><cell>92.53±0.19</cell><cell>95.54±0.08</cell><cell>92.30±0.28</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell>Model</cell><cell>WikiCS</cell><cell>Citeseer</cell><cell>Computers</cell><cell>CoraFull</cell><cell>CS</cell><cell>Physics</cell><cell>Photo</cell></row><row><cell>Random-Init</cell><cell>0.107±0.02</cell><cell>0.354±0.03</cell><cell>0.155±0.01</cell><cell>0.318±0.01</cell><cell>0.716±0.02</cell><cell>0.551±0.01</cell><cell>0.246±0.04</cell></row><row><cell>Raw-Feat</cell><cell>0.182±0.00</cell><cell>0.316±0.00</cell><cell>0.166±0.00</cell><cell>0.215±0.00</cell><cell>0.642±0.00</cell><cell>0.489±0.00</cell><cell>0.282±0.00</cell></row><row><cell>GAE</cell><cell>0.243±0.02</cell><cell>0.313±0.02</cell><cell>0.441±0.00</cell><cell>0.485±0.00</cell><cell>0.731±0.01</cell><cell>0.545±0.06</cell><cell>0.616±0.01</cell></row><row><cell>VGAE</cell><cell>0.261±0.01</cell><cell>0.364±0.01</cell><cell>0.423±0.00</cell><cell>0.453±0.01</cell><cell>0.733±0.00</cell><cell>0.563±0.02</cell><cell>0.530±0.04</cell></row><row><cell>ARVGA</cell><cell>0.287±0.02</cell><cell>0.191±0.02</cell><cell>0.237±0.01</cell><cell>0.301±0.01</cell><cell>0.616±0.03</cell><cell>0.526±0.05</cell><cell>0.301±0.01</cell></row><row><cell>MVGRL</cell><cell>0.263±0.01</cell><cell>0.452±0.01</cell><cell>0.244±0.00</cell><cell>0.400±0.01</cell><cell>0.740±0.01</cell><cell>0.594±0.00</cell><cell>0.344±0.04</cell></row><row><cell>AUTOSSL-ES</cell><cell>0.366±0.01</cell><cell>0.449±0.01</cell><cell>0.447 ±0.01</cell><cell>0.506±0.01</cell><cell>0.772±0.01</cell><cell>0.725 ±0.00</cell><cell>0.560 ±0.04</cell></row><row><cell>AUTOSSL-DS</cell><cell>0.344 ±0.02</cell><cell>0.449 ±0.01</cell><cell>0.459±0.01</cell><cell>0.500 ±0.00</cell><cell>0.771 ±0.01</cell><cell>0.726±0.00</cell><cell>0.515±0.03</cell></row></table><note>Clustering performance (NMI). AUTOSSL embeddings routinely exhibit superior NMI to alternatives. (Bold: best; Underline: runner-up).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Dataset statistics.</figDesc><table><row><cell>Dataset</cell><cell>Network Type</cell><cell>#Nodes</cell><cell>#Edges</cell><cell>#Classes</cell><cell>#Features</cell><cell>Homophily</cell></row><row><cell>WikiCS</cell><cell>Reference network</cell><cell>11,701</cell><cell>216,123</cell><cell>10</cell><cell>300</cell><cell>0.70</cell></row><row><cell>CS</cell><cell>Co-authorship network</cell><cell>18,333</cell><cell>81,894</cell><cell>15</cell><cell>6,805</cell><cell>0.81</cell></row><row><cell>Physics</cell><cell>Co-authorship network</cell><cell>34,493</cell><cell>247,962</cell><cell>5</cell><cell>8,415</cell><cell>0.93</cell></row><row><cell>Computers</cell><cell>Co-purchase network</cell><cell>13,381</cell><cell>245,778</cell><cell>10</cell><cell>767</cell><cell>0.78</cell></row><row><cell>Photo</cell><cell>Co-purchase network</cell><cell>7,487</cell><cell>119,043</cell><cell>8</cell><cell>745</cell><cell>0.83</cell></row><row><cell>CoraFull</cell><cell>Citation network</cell><cell>19,793</cell><cell>65,311</cell><cell>70</cell><cell>8,710</cell><cell>0.57</cell></row><row><cell>Citeseer</cell><cell>Citation network</cell><cell>3,327</cell><cell>4,732</cell><cell>6</cell><cell>3,703</cell><cell>0.74</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Algorithm 1: AUTOSSL-ES: AutoSSL with Evolutionary Strategy for r in {0, . . . , R} do 1. Sample K sets of tasks weights from a multivariate normal distribution 2. Train K networks w.r.t. each set of task weights from scratch 3. Calculate pseudo-homophily P-H of node embeddings from each network 4. Adjust the multivariate normal distribution through CMA-ES based on P-H end Algorithm 2: AUTOSSL-DS: AutoSSL with Differential Search Initialize self-supervised task weights {λ i } and GNN parameters θ; for t in {0, . . . , T } do 1. θ t+1 = θ t − ∇ θt L(f θt , {λ i , i }) 2. Perform k-means clustering on f θt (G) and obtain centroids {c 1 , c 2 , . . . , c</figDesc><table /><note>k } 3. Calculate p (c i | x) according to Eq. (4) 4. Calculate homophily loss H according to Eq. (5) 5. {λ i } ← − {λ i } − η∇ meta {λi} 6. Clip {λ i } to [0,1] end</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>We analyze the time complexity of the proposed AUTOSSL. Here we call one set of task weights {λ i } as one candidate solution. We denote the time of training one epoch on a given set of SSL tasks as t o and the evaluation time is t e . Suppose we need to train T epochs for the network. Then the time for running one single candidate solution is T t o + t e ; the time of running R rounds of AUTOSSL-ES should be RT t o + Rt e . For AUTOSSL-DS, the running time is T t o + T t e . As an illustration, in an L-layer GCN with d as the number of hidden dimensions, t o can be expressed as O(LN d 2 ) and t e has time complexity of O(KIN d) with K being the number of clusters and I being the number of iterations for k-means. Hence, we also express the time complexity of AUTOSSL-ES as O(RT LN d 2 + RKIN d) and that of AUTOSSL-DS as O(T LN d 2 + T KIN d). Both of them linearly increase with number of nodes N .For empirical comparison, we take Citeseer, Photo, CoraFull as examples to illustrate. As shown in Table5, we compare the running time of different methods for training 1000 epochs on one NVIDIA-K80 GPU. The column of 5-Tasks indicates the running time of training a combination of 5 SSL tasks, i.e. CLU, PAR, PAIRSIM, PAIRDIS and DGI, for 1000 epochs. Note that we report the running time of AUTOSSL-ES as the multiplication between 500 and the time of 5-Tasks, i.e., running 500 candidate solutions. From the table, we can see that the running time of AUTOSSL-ES depends on the number of candiate solutions and usually takes a long time to run. However, AUTOSSL-DS significantly reduces the running time of AUTOSSL-ES. It is worth noting the state-of-the-art SSL task, MVGRL, takes a long time to run and suffers from the OOM issue when the dataset gets larger. Comparison of running time for training 1000 epochs on one NVIDIA-K80 GPU (12 GB memory). OOM indicates out-of-memory on this GPU.</figDesc><table><row><cell cols="2">D.2 Efficiency Analysis</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">D.2.1 Time Complexity Analysis</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">D.2.2 Empirical Comparison</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="5">DGI MVGRL 5-Tasks AUTOSSL-ES AUTOSSL-DS</cell></row><row><cell cols="2">Citeseer 222s</cell><cell>1220s</cell><cell>322s</cell><cell>322s×500</cell><cell>1222s</cell></row><row><cell>Photo</cell><cell>177s</cell><cell>1074s</cell><cell>507s</cell><cell>507s×500</cell><cell>1766s</cell></row><row><cell cols="2">CoraFull 553s</cell><cell>OOM</cell><cell>858s</cell><cell>858s×500</cell><cell>3584s</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Performance comparison of different strategies of assigning task weights. The NMI rows indicate node clustering performance; ACC rows indicate node classification accuracy (%); P-H stands for pseudo-homophily. (Underline: better than "Best SSL").</figDesc><table><row><cell>Dataset</cell><cell>Metric</cell><cell>Best SSL</cell><cell>Random Weight</cell><cell>Equal Weight</cell><cell>AUTOSSL-ES</cell><cell>AUTOSSL-DS</cell></row><row><cell></cell><cell>NMI</cell><cell>0.439±0.00</cell><cell>0.398±0.01</cell><cell>0.408±0.00</cell><cell>0.449 ±0.01</cell><cell>0.449 ±0.01</cell></row><row><cell>Citeseer</cell><cell>ACC</cell><cell>71.64±0.44</cell><cell>70.64±0.07</cell><cell>70.80±0.31</cell><cell>72.14 ±0.41</cell><cell>72.00 ±0.32</cell></row><row><cell></cell><cell>P-H</cell><cell>−</cell><cell>0.897</cell><cell>0.904</cell><cell>0.943</cell><cell>0.934</cell></row><row><cell></cell><cell>NMI</cell><cell>0.433±0.00</cell><cell>0.341±0.03</cell><cell>0.290±0.00</cell><cell>0.447 ±0.01</cell><cell>0.459 ±0.01</cell></row><row><cell>Computers</cell><cell>ACC</cell><cell>87.26±0.15</cell><cell>86.86±0.25</cell><cell>87.24±0.38</cell><cell>87.26 ±0.64</cell><cell>88.18 ±0.43</cell></row><row><cell></cell><cell>P-H</cell><cell>−</cell><cell>0.450</cell><cell>0.414</cell><cell>0.578</cell><cell>0.770</cell></row><row><cell></cell><cell>NMI</cell><cell>0.498±0.00</cell><cell>0.458±0.02</cell><cell>0.493±0.00</cell><cell>0.506 ±0.01</cell><cell>0.500 ±0.00</cell></row><row><cell>CoraFull</cell><cell>ACC</cell><cell>60.42±0.39</cell><cell>58.88±0.32</cell><cell>59.01±0.29</cell><cell>61.01 ±0.50</cell><cell>61.10 ±0.68</cell></row><row><cell></cell><cell>P-H</cell><cell>−</cell><cell>0.811</cell><cell>0.868</cell><cell>0.903</cell><cell>0.895</cell></row><row><cell></cell><cell>NMI</cell><cell>0.767±0.01</cell><cell>0.761±0.01</cell><cell>0.770 ±0.01</cell><cell>0.772 ±0.01</cell><cell>0.771 ±0.01</cell></row><row><cell>CS</cell><cell>ACC</cell><cell>92.75±0.12</cell><cell>92.88 ±0.20</cell><cell>93.22 ±0.12</cell><cell>93.26 ±0.16</cell><cell>93.35 ±0.09</cell></row><row><cell></cell><cell>P-H</cell><cell>−</cell><cell>0.879</cell><cell>0.881</cell><cell>0.895</cell><cell>0.890</cell></row><row><cell></cell><cell>NMI</cell><cell>0.509±0.01</cell><cell>0.341±0.02</cell><cell>0.366±0.02</cell><cell>0.560 ±0.04</cell><cell>0.511 ±0.03</cell></row><row><cell>Photo</cell><cell>ACC</cell><cell>92.08±0.37</cell><cell>92.04±0.28</cell><cell>92.54 ±0.29</cell><cell>92.04±0.89</cell><cell>92.71 ±0.32</cell></row><row><cell></cell><cell>P-H</cell><cell>−</cell><cell>0.412</cell><cell>0.472</cell><cell>0.791</cell><cell>0.626</cell></row><row><cell></cell><cell>NMI</cell><cell>0.704±0.00</cell><cell>0.692±0.00</cell><cell>0.709 ±0.01</cell><cell>0.725 ±0.00</cell><cell>0.726 ±0.00</cell></row><row><cell>Physics</cell><cell>ACC</cell><cell>95.07±0.06</cell><cell>95.09 ±0.08</cell><cell>95.39 ±0.10</cell><cell>95.57 ±0.02</cell><cell>95.13 ±0.36</cell></row><row><cell></cell><cell>P-H</cell><cell>−</cell><cell>0.914</cell><cell>0.916</cell><cell>0.921</cell><cell>0.923</cell></row><row><cell></cell><cell>NMI</cell><cell>0.341±0.01</cell><cell>0.305±0.01</cell><cell>0.323±0.01</cell><cell>0.366 ±0.01</cell><cell>0.344 ±0.02</cell></row><row><cell>WikiCS</cell><cell>ACC</cell><cell>75.81±0.17</cell><cell>76.29 ±0.17</cell><cell>76.49 ±0.21</cell><cell>76.80 ±0.13</cell><cell>76.58 ±0.28</cell></row><row><cell></cell><cell>P-H</cell><cell>−</cell><cell>0.675</cell><cell>0.690</cell><cell>0.751</cell><cell>0.749</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Comparison with random search. The NMI indicates node clustering performance; ACC indicates node classification accuracy (%); P-H stands for pseudo-homophily.</figDesc><table><row><cell>Dataset</cell><cell cols="3">Metric Random Search AUTOSSL-ES</cell></row><row><cell></cell><cell>NMI</cell><cell>0.443±0.00</cell><cell>0.449±0.01</cell></row><row><cell>Citeseer</cell><cell>ACC</cell><cell>71.68±0.55</cell><cell>72.14±0.41</cell></row><row><cell></cell><cell>P-H</cell><cell>0.934</cell><cell>0.943</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the new constraints, we rewrite the optimization problem as max f (x) = M I(A, B) s.t.</p><p>Further, the derivative of f (x) is expressed as follows,</p><p>Note that in the theorem we assume the pseudo-labels and ground truth classes are balanced, i.e.,</p><p>Hence, f (x) is monotonically decreasing at [∆, N  4 ] and monotonically increasing</p><p>, and we can get the maximum value of f (x) as follows,</p><p>. In other words, M I(A, B) reaches its upper bound when</p><p>Based on the discussion in (1), we know that f (∆) is monotonically decreasing at [0, N  4 ], which means an increase of ∆ leads to a decrease in f (∆), i.e., a smaller value of</p><p>, a decrease in h A will lead to a increase in ∆. Then we have</p><p>Remark on a more generalized case. We now discuss the case where we do not have assumptions on a and b. As we demonstrated in the above discussion, f (x) is monotonically decreasing at [max(0, a+ b−N, a+b−(N −2∆) 2 ), ab N ] and monotonically increasing [ ab N , min(a, b, a+b−2∆</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head><p>)]. Thus, the maximum value of f (x) should be one of the values of f (0</p><p>). As our goal is to show that U A,B would be small with low h A , to simplify the analysis, we consider a large value of ∆ (or a small value of h A ) which satisfies ∆ ≥ ]. Then the maximum value of f (x), i.e., U A,B , is expressed as</p><p>When a+b−(N −2∆)</p><p>, it is easy to see that larger ∆ (or smaller h A ) will lead to smaller U A,B because both a+b−(N −2∆)   ≤ ab N , U A,B decreases with the increase of ∆ (or the decrease of h A ). To sum up, for small h A , the upper bound of M I(A, B), i.e., U A,B , decreases with the decrease of h A .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Algorithm</head><p>The detailed algorithm for AUTOSSL-ES is shown in Algorithm 1. Concretely, for each round (iteration) of AUTOSSL-ES, we sample K sets of task weights, i.e., K different combinations of SSL without typical fairness checks in industrial contexts. However, such concerns are not inherently posed by our proposed ideas, but by the foundations it builds on in GNNs and SSL. We anticipate our ideas will drive further research in more sophisticated and powerful self-supervised graph learning, and do not anticipate direct negative outcomes from this work.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00596</idno>
		<title level="m">A comprehensive survey on graph neural networks</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Relational inductive biases, deep learning, and graph networks</title>
		<author>
			<persName><forename type="first">Jessica</forename><forename type="middle">B</forename><surname>Peter W Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinicius</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adversarially regularized graph autoencoder for graph embedding</title>
		<author>
			<persName><forename type="first">Ruiqi</forename><surname>Shirui Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lina</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Joint Conference on Artificial Intelligence, IJCAI&apos;18</title>
				<meeting>the 27th International Joint Conference on Artificial Intelligence, IJCAI&apos;18</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2609" to="2615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName><forename type="first">Dougal</forename><surname>David K Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Aguilera-Iparraguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Gómez-Bombarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alán</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Revisiting self-supervised visual representation learning</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="1920">1920-1929, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Self-supervised learning of graph neural networks: A unified review</title>
		<author>
			<persName><forename type="first">Yaochen</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingtun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.10757</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Graph structure learning for robust graph neural networks</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaorui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianfeng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="66" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep Graph Infomax</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">When does self-supervision help graph convolutional networks? ICML</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Contrastive multi-view representation learning on graphs</title>
		<author>
			<persName><forename type="first">Kaveh</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Hosein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khasahmadi</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Self-supervised graph representation learning via global context prediction</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixiang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minnan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghua</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.01604</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Node similarity preserving graph convolutional networks</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tyler</forename><surname>Derr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zitao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM International Conference on Web Search and Data Mining</title>
				<meeting>the 14th ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-task self-supervised visual learning</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2051" to="2060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cross-domain self-supervised multi-task feature learning using synthetic imagery</title>
		<author>
			<persName><forename type="first">Zhongzheng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Jae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="762" to="771" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Taskonomy: Disentangling task transfer learning</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Amir R Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3712" to="3722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Birds of a feather: Homophily in social networks</title>
		<author>
			<persName><forename type="first">Lynn</forename><surname>Miller Mcpherson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">M</forename><surname>Smith-Lovin</surname></persName>
		</author>
		<author>
			<persName><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual review of sociology</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="415" to="444" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Mumme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.05868</idno>
		<title level="m">Pitfalls of graph neural network evaluation</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Elastic graph neural networks</title>
		<author>
			<persName><forename type="first">Xiaorui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Variational graph auto-encoders</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Samuel S Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Strategies for pre-training graph neural networks</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Gcc: Graph contrastive coding for graph neural network pre-training</title>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qibin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1150" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Graph contrastive learning with adaptive augmentation</title>
		<author>
			<persName><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.14945</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Graph contrastive learning automated</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning</title>
				<meeting>International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Taking human out of learning applications: A survey on automated machine learning</title>
		<author>
			<persName><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengshuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyuan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Feng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Wei</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.13306</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.09055</idno>
		<title level="m">Darts: Differentiable architecture search</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Autoloss: Automated loss function search in recommendations</title>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haochen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 27th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Autoloss: Learning discrete schedules for alternate optimization</title>
		<author>
			<persName><forename type="first">Haowen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.02442</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Loss function search for face recognition</title>
		<author>
			<persName><forename type="first">Xiaobo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10029" to="10038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Amlfs: Automl for loss function search</title>
		<author>
			<persName><forename type="first">Chuming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minghao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
				<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8410" to="8419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Evolving losses for unsupervised video representation learning</title>
		<author>
			<persName><forename type="first">Anelia</forename><surname>Aj Piergiovanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="133" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Mark</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><surname>Networks</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Oxford university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Beyond homophily in graph neural networks: Current limitations and effective designs</title>
		<author>
			<persName><forename type="first">Jiong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingxiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Heimann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
				<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="132" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Geom-gcn: Geometric graph convolutional networks</title>
		<author>
			<persName><forename type="first">Hongbin</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingzhe</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Chen-Chuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05287</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.07269</idno>
		<title level="m">Cma-es for hyperparameter optimization of deep neural networks</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Reducing the time complexity of the derandomized evolution strategy with covariance matrix adaptation (cma-es)</title>
		<author>
			<persName><forename type="first">Nikolaus</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sibylle</forename><forename type="middle">D</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petros</forename><surname>Koumoutsakos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Evolutionary computation</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Pattern recognition and machine learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Adversarial attacks on graph neural networks via meta learning</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Zügner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.08412</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A fast and high quality multilevel scheme for partitioning irregular graphs</title>
		<author>
			<persName><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vipin</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on scientific Computing</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Wiki-cs: A wikipedia-based benchmark for graph neural networks</title>
		<author>
			<persName><forename type="first">Péter</forename><surname>Mernyei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cȃtȃlina</forename><surname>Cangea</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.02901</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Fast graph representation learning with pytorch geometric</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Eric Lenssen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.02428</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Knowing your fate: Friendship, action and temporal explanations for user engagement prediction on social apps</title>
		<author>
			<persName><forename type="first">Xianfeng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yozen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prasenjit</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2269" to="2279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Graph neural networks for friend ranking in large-scale social platforms</title>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Sankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yozen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
				<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2535" to="2546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Session-based recommendation with graph neural networks</title>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuyuan</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="346" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Say no to the discrimination: Learning fair graph neural networks with limited sensitive attribute information</title>
		<author>
			<persName><forename type="first">Enyan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM International Conference on Web Search and Data Mining</title>
				<meeting>the 14th ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="680" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Investigating and mitigating degree-related biases in graph convoltuional networks</title>
		<author>
			<persName><forename type="first">Xianfeng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaxiu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiwei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charu</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prasenjit</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management</title>
				<meeting>the 29th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1435" to="1444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">On the dangers of stochastic parrots: Can language models be too big?</title>
		<author>
			<persName><forename type="first">Emily</forename><forename type="middle">M</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timnit</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angelina</forename><surname>Mcmillan-Major</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shmargaret</forename><surname>Shmitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency</title>
				<meeting>the 2021 ACM Conference on Fairness, Accountability, and Transparency</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="610" to="623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">Jieyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03310</idno>
		<title level="m">Gender bias in contextualized word embeddings</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Quantifying the extent to which popular pre-trained convolutional neural networks implicitly learn high-level protected attributes</title>
		<author>
			<persName><forename type="first">Claudia</forename><surname>Veronica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberts</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
		<respStmt>
			<orgName>Princeton University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Fairod: Fairness-aware outlier detection</title>
		<author>
			<persName><forename type="first">Shubhranshu</forename><surname>Shekhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.03063</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
