<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pre-training of Graph Augmented Transformers for Medication Recommendation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Junyuan</forename><surname>Shang</surname></persName>
							<email>junyuan.shang@iqvia.com</email>
							<affiliation key="aff0">
								<orgName type="department">Analytics Center of Excellence</orgName>
								<orgName type="institution">IQVIA</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<settlement>Atlanta</settlement>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
							<email>tengfei.ma1@ibm.com</email>
							<affiliation key="aff1">
								<orgName type="institution">IBM Research AI</orgName>
								<address>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Cao</forename><surname>Xiao</surname></persName>
							<email>cao.xiao@iqvia.com</email>
							<affiliation key="aff0">
								<orgName type="department">Analytics Center of Excellence</orgName>
								<orgName type="institution">IQVIA</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jimeng</forename><surname>Sun</surname></persName>
							<email>jsun@cc.gatech.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<settlement>Atlanta</settlement>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Pre-training of Graph Augmented Transformers for Medication Recommendation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Medication recommendation is an important healthcare application. It is commonly formulated as a temporal prediction task. Hence, most existing works only utilize longitudinal electronic health records (EHRs) from a small number of patients with multiple visits ignoring a large number of patients with a single visit (selection bias). Moreover, important hierarchical knowledge such as diagnosis hierarchy is not leveraged in the representation learning process. To address these challenges, we propose G-BERT, a new model to combine the power of Graph Neural Networks (GNNs) and BERT (Bidirectional Encoder Representations from Transformers) for medical code representation and medication recommendation. We use GNNs to represent the internal hierarchical structures of medical codes. Then we integrate the GNN representation into a transformer-based visit encoder and pre-train it on EHR data from patients only with a single visit. The pre-trained visit encoder and representation are then fine-tuned for downstream predictive tasks on longitudinal EHRs from patients with multiple visits. G-BERT is the first to bring the language model pre-training schema into the healthcare domain and it achieved state-of-the-art performance on the medication recommendation task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The availability of massive electronic health records (EHR) data and the advances of deep learning technologies have provided unprecedented resource and opportunity for predictive healthcare, including the computational medication recommendation task.</p><p>A number of deep learning models were proposed to assist doctors in making medication recommendation <ref type="bibr" target="#b5">[Xiao et al., 2018a;</ref><ref type="bibr" target="#b3">Shang et al., 2019;</ref><ref type="bibr" target="#b0">Baytas et al., 2017;</ref><ref type="bibr">Choi et al., 2018;</ref><ref type="bibr">Ma et al., 2018]</ref>. They often learn representations for medical entities (e.g., patients, diagnosis, medications) from patient EHR data, and then use the learned representations to predict medications that are suited to the patient's health condition. To provide effective medication recommendation, it is important to learn accurate representation of medical codes. Despite that various considerations were handled in previous works for improving medical code representations <ref type="bibr">[Ma et al., 2018;</ref><ref type="bibr" target="#b0">Baytas et al., 2017;</ref><ref type="bibr">Choi et al., 2018]</ref>, there are two limitations with the existing work:</p><p>1. Selection bias: Data that do not meet training data criteria are discarded before model training. For example, a large number of patients who only have one hospital visit were discarded from training in <ref type="bibr" target="#b3">[Shang et al., 2019]</ref>.</p><p>2. Lack of hierarchical knowledge: For medical knowledge such as diagnosis code ontology (Figure <ref type="figure">.</ref> 1), their internal hierarchical structures were rarely embedded in their original graph form when incorporated into representation learning.</p><p>To mitigate the aforementioned limitations, we propose G-BERT that combines the pre-training techniques and graph neural networks for better medical code representation and medication recommendation. G-BERT is enabled and demonstrated by the following technical contributions:</p><p>1. Pre-training to leverage more data: Pre-training techniques, such as ELMo <ref type="bibr" target="#b2">[Peters et al., 2018]</ref>, OpenAI <ref type="bibr">GPT [Radford et al., 2018]</ref> and <ref type="bibr">BERT [Devlin et al., 2018]</ref>, have demonstrated a notably good performance in various natural language processing tasks. These techniques generally train language models from unlabeled data, and then adapt the derived representations to different tasks by either feature-based (e.g. ELMo) or fine-tuning (e.g. OpenAI GPT, BERT) methods. We developed a new pre-training method based on BERT for Instance-based methods focus on current health conditions. Among them, Leap <ref type="bibr" target="#b7">[Zhang et al., 2017]</ref> formulates a multi-instance multi-label learning framework and proposes a variant of sequence-to-sequence model based on content-attention mechanism to predict combination of medicines given patient's diagnoses. Longitudinal-based methods leverage the temporal dependencies among clinical events, see <ref type="bibr" target="#b1">[Choi et al., 2016;</ref><ref type="bibr" target="#b6">Xiao et al., 2018b;</ref><ref type="bibr" target="#b2">Lipton et al., 2015]</ref>. Among them, <ref type="bibr">RETAIN [Choi et al., 2016]</ref> uses a two-level neural attention model to detect influential past visits and significant clinical variables within those visits for improved medication recommendation.</p><p>Pre-training Techniques. The goal of pre-training techniques is to provide model training with good initializations. Pre-training has been shown extremely effective in various areas such as image classification <ref type="bibr" target="#b2">[Hinton et al., 2006]</ref> and machine translation <ref type="bibr" target="#b3">[Ramachandran et al., 2016]</ref>. The unsupervised pre-training can be considered as a regularizer that supports better generalization from the training dataset <ref type="bibr">[Erhan et al., 2010]</ref>. Recently, language model pre-training techniques such as <ref type="bibr" target="#b2">[Peters et al., 2018;</ref><ref type="bibr">Radford et al., 2018;</ref><ref type="bibr" target="#b1">Devlin et al., 2018]</ref> have shown to largely improve the performance on multiple NLP tasks. As the most widely used one, <ref type="bibr">BERT [Devlin et al., 2018]</ref> builds on the Transformer <ref type="bibr" target="#b4">[Vaswani et al., 2017]</ref> architecture and improves the pre-training using a masked language model for bidirectional representation. In this paper, we adapt the framework of BERT and pre-train our model on each visit of the EHR data to leverage the single-visit data that were not fit for training in other medication recommendation models.</p><p>Graph Neural Networks (GNN). GNNs are neural networks that learn node or graph representations from graphstructured data. Various graph neural networks have been proposed to encode the graph-structure information, including graph convolutional neural networks (GCN) <ref type="bibr" target="#b2">[Kipf and Welling, 2017]</ref>, message passing networks (MPNN) <ref type="bibr" target="#b1">[Gilmer et al., 2017]</ref>, graph attention networks (GAT) <ref type="bibr">[Velickovic et al., 2017]</ref>. GNNs have already been demonstrated useful on EHR modeling <ref type="bibr" target="#b1">[Choi et al., 2017;</ref><ref type="bibr" target="#b3">Shang et al., 2019]</ref>. <ref type="bibr">GRAM [Choi et al., 2017]</ref> represented a medical concept as a combination of its ancestors in the medical ontology using an attention mechanism. It's different from G-BERT from two aspects as described in Section 4.2. Another work worth mentioning is <ref type="bibr">GAMENet [Shang et al., 2019]</ref>, which also used graph neural network to assist the medication recommendation task. However, GAMENet has a different motivation which results in using graph neural networks on drugdrug-interaction graphs instead of medical ontology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem Formalization</head><p>Definition 1 (Longitudinal Patient Records). In longitudinal EHR data, each patient can be represented as a sequence of multivariate observations: </p><formula xml:id="formula_0">X (n) = {X (n) 1 , X (n) 2 , • • • , X (n) T (n) } where n ∈ {1, 2, . . . , N }, N</formula><formula xml:id="formula_1">1:t = {X 1 , X 2 , • • • , X t−1 }, we want to recommend multiple medications by generating multi-label output ŷt ∈ {0, 1} |Cm| .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Method</head><p>The overall framework of G-BERT is described in Figure <ref type="figure" target="#fig_1">2</ref>. G-BERT first derives the initial embedding of medical codes from medical ontology using graph neural networks. Then, in order to fully utilize the rich EHR data, G-BERT constructs an adaptive BERT model on the discarded single-visit data for visit representation. Finally we add a prediction layer and fine-tune the model in the medication recommendation task. In the following we will describe G-BERT in detail. But firstly, we give a brief background of BERT especially for the two pre-training objectives which will be later adapted to EHR data in Section 4.3.  <ref type="table" target="#tab_4">3, 4</ref>). Then we input set of diagnosis and medication ontology embedding separately to shared weight BERT which is pretrained using Eq. 6, 7, 8. Finally, we concatenate the mean of all previous visit embeddings and the last visit embedding as input and fine-tune the prediction layers using Eq. 10 for medication recommendation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Notation</head><p>Description X (n)  longitudinal observations for n-th patient</p><formula xml:id="formula_2">C d , C m diagnoses and medications codes set O d , O m</formula><p>diagnoses and medications codes ontology</p><formula xml:id="formula_3">C * ⊂ O * non-leaf medical codes of type * c * ∈ O * single medical code of type * pa(c * ) function retrieve c * ' ancestors' code ch(c * ) function retrieve c * ' direct children's code W e ∈ R |O * |×d initial medical embedding matrix in O * H e ∈ R |O * |×d enhanced medical embeddings in stage 1 o c * ∈ R d ontology embedding in stage 2 α k i,j k-th attention between nodes i, j W k ∈ R d×d k-th weight matrix applied to each node g(•, •, •) graph aggregator function v t * t-th visit embedding of type * ŷt ∈ {0, 1} |C * | multi-label prediction Table 1: Notations used in G-BERT.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Background of BERT</head><p>Based on a multi-layer Transformer encoder <ref type="bibr" target="#b4">[Vaswani et al., 2017]</ref> (The transformer architecture has been ubiquitously used in many sequence modeling tasks recently, so we will not introduce the details here), BERT is pre-trained using two unsupervised tasks:</p><p>• Masked Language Model. Instead of predicting words based on previous words, BERT randomly selects words to mask out and then predicts the original vocabulary IDs of the masked words from their (bidirectional) context.</p><p>• Next Sentence Prediction. Many of BERT's downstream tasks are predicting the relationships of two sentences, thus in the pre-training phase, BERT has am a binary sentence prediction task to predict whether one sentence is the next sentence of the other.</p><p>A typical input to BERT is as follows <ref type="bibr" target="#b1">( [Devlin et al., 2018]</ref>):</p><formula xml:id="formula_4">Input = [CLS] the man went to [MASK] store [SEP] he bought a gallon [MASK] milk [SEP] Label = IsNext</formula><p>where <ref type="bibr">[CLS]</ref> is the first token of each sentence pair to represent the special classification embedding, i.e. the final state of this token is used as the aggregated sequence representation for classification tasks; [SEP] is used to separate two sentences; [MASK] is used to mask out the predicted words in the masked language model. Using this form, these inputs facilitate the two tasks described above, and they will also be used in our method description in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Input Representation</head><p>The G-BERT model takes medical codes' ontology embeddings as input, and obtains intermediate representations from a Transformer encoder as the visit embeddings. It is then pre-trained on EHR from patients who only have one hospital visit. The derived encoder and visit embedding will be fed into a classifier and fine-tuned to make predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ontology Embedding</head><p>We constructed ontology embedding from diagnosis ontology O d and medication ontology O m . Since the medical codes in raw EHR data can be considered as leaf nodes in these ontology trees, we can enhance the medical code embedding using graph neural networks (GNNs) to integrate the ancestors' information of these codes. Here we perform a two-stage procedure with a specially designed GNN for ontology embedding.</p><p>To start, we assign an initial embedding vector to every medical code c * ∈ O * with a learnable embedding matrix W e ∈ R |O * |×d where d is the embedding dimension. In particular, we implement the aggregation function g(•, •, •) as follows (taking stage 2 for an example): </p><formula xml:id="formula_5">g(c * , pa(c * ), H e ) = K k=1 σ   j∈Nc * α k c * ,j W k h j  <label>(3)</label></formula><formula xml:id="formula_6">α k c * ,j = exp LeakyReLU(a [W k h c * ||W k h j ]) k∈Nc * exp (LeakyReLU(a [W k h c * ||W k h k ]))<label>(4</label></formula><p>) where a ∈ R 2m is a weight vector and LeakyReLU is a nonlinear function. (we assume m = d/K).</p><p>As shown in Figure <ref type="figure" target="#fig_1">2</ref>, we construct ICD-9 tree for diagnosis and ATC tree for medication using the same structure.</p><p>Here the direction of arrow shows the information flow where ancestor nodes can get information from their direct children (in stage 1) and similarly leaf nodes can get information from their connected ancestors (in stage 2).</p><p>It is worth mentioning that our graph embedding method on medical ontology is different from GRAM <ref type="bibr" target="#b1">[Choi et al., 2017]</ref> from the following two aspects: 1. Initialization: we initialize all the node embeddings from a learnable embedding matrix, while GRAM learns them using Glove from the co-occurrence information. 2. Updating: we develop a two-step updating function for both leaf nodes and ancestor nodes; while in GRAM, only the leaf nodes are updated (as a combination of their ancestor nodes and themselves).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visit Embedding</head><p>Similar to BERT, we use a multi-layer Transformer architecture <ref type="bibr" target="#b4">[Vaswani et al., 2017]</ref> as our visit encoder. The model takes the ontology embedding as input and derive visit embedding v t * ∈ R d for a patient at t-th visit:</p><formula xml:id="formula_7">v t * = Transformer({[CLS]} ∪ {o t c * |c * ∈ C t * })[0]</formula><p>(5) where [CLS] is a special token as in BERT. It is put in the first position of each visit of type * and its final state can be used as the representation of the visit. Intuitively, it is more reasonable to use Transformers as encoders (multi-head attention based architecture) than RNN or mean/sum to aggregate multiple medical embedding for visit embedding since the set of medical codes within one visit is not ordered. Note that symbol [SEP] is also ignored considering there is no clear separate among codes within one visit.</p><p>It is worth noting that our Transformer encoder is different from the original one in the position embedding part. Position embedding, as an important component in Transformers and BERT, is used to encode the position and order information of each token in a sequence. However, one big difference between language sentences and EHR sequences is that the medical codes within the same visit do not generally have an order, so we remove the position embedding in our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Pre-training</head><p>We adapted the original BERT model to be more suitable for our data and task. In particular, we pre-train the model on each single EHR visit (within both single-visit EHR sequences and multi-visit EHR sequences). We modified the input and pre-training objectives of the BERT model: (1) For the input, we built the Transformer encoder on the GNN outputs, i.e. ontology embeddings, for visit embedding. For the original EHR sequence, it means essentially we combine the GNN model with a Transformer to become a new integrated encoder. In addition, we removed the position embedding as we explained before. (2) As for the pre-training procedures, we modified the original pre-training tasks i.e., Masked LM (language model) task and Next Sentence prediction task to self-prediction task and dual-prediction task. The idea to conduct these tasks is to make the visit embedding absorb enough information about what it is made of and what it is able to predict. (note that we omit superscript t as only single visit is used for pre-training procedure).</p><p>Thus, for the self-prediction task, we want the visit embedding v * to recover what it is made of, i.e., the input medical codes C * limited by the same type for each visit as follows:</p><formula xml:id="formula_8">L se (v * , C (n) * ) = − log p(C (n) * |v * ) = − c * ∈C (n) * log p(c * |v * ) + c * ∈{C * \C (n) * } log p(c * |v * ) (6)</formula><p>where C</p><p>(n) * is the medical codes set of n-th patient, * ∈ {d, m} and we minimize the binary cross entropy loss L se . For instance, assume that the n-th patient takes 10 different medications out of total 100 medications which means |C Likewise, for the dual-prediction task, since the visit embedding v * carries the information of medical codes of type * , we can further expect it has the ability to do more taskspecific prediction as follows:</p><formula xml:id="formula_9">L du = − log p(C d |v m ) − log p(C m |v d ) (7)</formula><p>where we use the same transformation function Sigmoid(f 1 (v m )), Sigmoid(f 2 (v d )) 1 with different weight matrix to transform the visit embedding and optimize the binary cross entropy loss L du expanded same as L se in Eq. 6. This is a direct adaptation of the next sentence prediction 1 f1, f2 are the multiple layer perceptron (MLP) with one hidden layer  task. In BERT, the next sentence prediction task facilitates the prediction of sentence relations, which is a common task in NLP. However, in healthcare, most predictive tasks do not have a sequence pair to classify. Instead, we are often interested in predicting unknown disease or medication codes of the sequence. For example, in medication recommendation, we want to predict multiple medications given only the diagnosis codes. Inversely, we can also predict unknown diagnosis given the medication codes.</p><p>Thus, our final pre-training optimization objective can simply be the combination of the aforementioned losses, as shown in Eq. 8.</p><formula xml:id="formula_10">L pr = L se (v d , C d ) + L se (v m , C m ) + L du<label>(8)</label></formula><p>In practise, we could integrate using mini-batch technique to train on EHR data from all patients with a single visit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Fine-tuning</head><p>After obtaining pre-trained visit representation for each visit, for a prediction task on a multi-visit sequence data, we aggregate all the visit embedding and add a prediction layer for the medication recommendation task as shown in Figure <ref type="figure">.</ref> 4. To be specific, from pre-training on all visits, we have a pretrained Transformer encoder, which can then be used to get the visit embedding v τ * at time τ . The known diagnosis codes C t d at the prediction time t is also represented using the same model as v t * . Concatenating the mean of previous diagnoses visit embeddings and medication visit embeddings, also the last diagnoses visit embedding, we built an MLP based prediction layer to predict the recommended medication codes as in Equation <ref type="formula">9</ref>.</p><formula xml:id="formula_11">y t = Sigmoid(W 1 [( 1 t − 1 τ &lt;t v τ d )||( 1 t − 1 τ &lt;t v τ m )||v t d ] + b) (9) where W 1 ∈ R |Cm|×3d is a learnable transformation matrix.</formula><p>Given the true labels ŷt at each time stamp t, the loss func- tion for the whole EHR sequence (i.e. a patient) is</p><formula xml:id="formula_12">L = − 1 T − 1 T t=2 (y t log( ŷt ) + (1 − y t ) log(1 − ŷt )) (10)</formula><p>5 Experiment</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setting Data</head><p>We used EHR data from MIMIC-III <ref type="bibr" target="#b2">[Johnson et al., 2016]</ref> and conducted all our experiments on a cohort where patients have more than one visit. We utilize data from patients with both single visit and multiple visits in the training dataset as pre-training data source (multi-visit data are split into visit slices and duplicate codes within a single visit are removed in order to avoid leakage of information). In this work, we transform the drug coding from NDC to ATC Third Level for using the ontology information. The statistics of the datasets are summarized in Table <ref type="table" target="#tab_2">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baselines</head><p>We compared G-BERT 2 with the following baselines. All methods are implemented in <ref type="bibr">PyTorch [Paszke et al., 2017]</ref> and trained on an Ubuntu 16.04 with 8GB memory and Nvidia 1080 GPU. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metrics</head><p>To measure the prediction accuracy, we used Jaccard Similarity Score (Jaccard), Average F1 (F1) and Precision Recall AUC (PR-AUC). Jaccard is defined as the size of the intersection divided by the size of the union of ground truth set Y</p><formula xml:id="formula_13">(k) t and predicted set Ŷ (k) t . Jaccard = 1 N k T k t 1 N k T k t |Y (k) t ∩ Ŷ (k) t | |Y (k) t ∪ Ŷ (k) t |</formula><p>where N is the number of patients in test set and T k is the number of visits of the k th patient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>We randomly divide the dataset into training, validation and testing set in a 0.6 : 0.2 : 0.2 ratio. For G-BERT, the hyperparameters are adjusted on evaluation set: (1) GAT part: input embedding dimension as 75, number of attention heads as 4;</p><p>(2) BERT part: hidden dimension as 300, dimension of position-wise feed-forward networks as 300, 2 hidden layers with 4 attention heads for each layer. Specially, we alternated the pre-training with 5 epochs and fine-tuning procedure with 5 epochs for 15 times to stabilize the training procedure.</p><p>For LR, we use the grid search over typical range of hyperparameter to search the best hyperparameter values which result in L1 norm penalty with weight as 1.1. For deep learning models, we implemented RNN using a gated recurrent unit (GRU) <ref type="bibr" target="#b1">[Cho et al., 2014]</ref> and utilize dropout with a probability of 0.4 on the output of embedding. We test several embedding choice for baseline methods and determine the dimension for medical embedding as 300 and thershold for final prediction as 0.3 for better performance. Training is done through Adam <ref type="bibr" target="#b2">[Kingma and Ba, 2014]</ref> at learning rate 5e-4. We fix the best model on evaluation set within 100 epochs and report the performance in test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>Table <ref type="table">.</ref> 3 compares the performance on the medication recommendation task. For variants of G-BERT, G-BERT G − ,P − performs worse compared with G-BERT G − and G-BERT P − which demonstrate the effectiveness of using ontology information to get enhanced medical embedding as input and employ an unsupervised pre-training procedure on larger abundant data. Incorporating both hierarchical ontology information and pre-training procedure, the end-to-end model G-BERT has more capacity and achieve comparable results with others. As for baseline models, LR and Leap are worse than our most basic model (G-BERT G − ,P − ) in terms of most metrics. Comparing G-BERT P − and GRAM, which both used medical ontology information without pre-training, the scores of our G-BERT P − is slightly higher in all metrics. This can demonstrate the validness of using Transformer encoders and the specific prediction layer for medication recommendation. Our final model G-BERT is also better than the attention based model, RETAIN, and the recently published state-of-the-art model, GAMENet. Specifically, even adding the extra information of DDI knowledge and procedure codes, GAMENet still performs worse than G-BERT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper we proposed a pre-training model named G-BERT for medical code representation and medication recommendation. To our best knowledge, G-BERT is the first that utilizes language model pre-training techniques in healthcare domain. It adapted BERT to the EHR data and integrated medical ontology information using graph neural networks. By additional pre-training on the EHR from patients who only have one hospital visit which are generally discarded before model training, G-BERT outperforms all baselines in prediction accuracy on medication recommendation task. One direction for the future work is to add more auxiliary and struc-3 https://projector.tensorflow.org/ tural tasks to improve the ability of code representaion. Another direction may be to adapt our model to be suitable for even larger datasets with more heterogeneous modalities.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Graphical illustration of ICD-9 Ontology.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: The framework of G-BERT. It consists of three main parts: ontology embedding, BERT and fine-tuned classifier. Firstly, we derive ontology embedding for medical code laid in leaf nodes by cooperating ancestors information by Eq. 1 and 2 based on graph attention networks (Eq. 3, 4). Then we input set of diagnosis and medication ontology embedding separately to shared weight BERT which is pretrained using Eq. 6, 7, 8. Finally, we concatenate the mean of all previous visit embeddings and the last visit embedding as input and fine-tune the prediction layers using Eq. 10 for medication recommendation tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Stage 1 .</head><label>1</label><figDesc>For each non-leaf node c * ∈ C * , we obtain its enhanced medical embedding h c * ∈ R d as follows:h c * = g(c * , ch(c * ), W e ) (1)where g(•, •, •) is an aggregation function which accepts the target medical code c * , its direct child codes ch(c * ) and initial embedding matrix. Intuitively, the aggregation function can pass and fuse information in target node from its direct children which result in the more related embedding of ancestor' code to child codes' embedding. Stage 2. After obtaining enhanced embeddings, we pass the enhance embedding matrix H e ∈ R |O * |×d back to get ontology embedding for leaf codes c * ∈ C * as follows: o c * = g(c * , pa(c * ), H e ) (2) where g(•, •, •) accepts ancestor codes of target medical code c * . Here, we use pa(c * ) instead of ch(c * ), since utilizing the ancestors' embedding can indirectly associate all medical codes instead of taking each leaf code as independent input. The option for the aggregation function g(•, •, •) is flexible, including sum, mean. Here we choose the one from graph attention networks (GAT) [Velickovic et al., 2017], which has shown efficient embedding learning ability on graphstructured tasks, e.g., node classification and link prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>where N * is the neighboring nodes of c * including c * itself which depends on what stage we are on. Since we are on the stage 2 now, N * = {{c * } ∪ pa(c * )} (likewise for stage 1, N * = {{c * } ∪ ch(c * )}), represents concatenation which enables the multi-head attention mechanism, σ is a nonlinear activation function, W k ∈ R m×d is the weight matrix for input transformation, and α k c * ,j are the corresponding kth normalized attention coefficients computed as follows:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Graphical illustration of pre-training procedure. We firstly randomly mask the input medical codes using a [MASK] symbol. Orange arrow: self-prediction task takes vm or v d as input to restore the original medical codes with the same type. Green arrow: dual-prediction task takes one type of visit embedding such as vm or v d and tries to predict the other type of medical codes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>10 and |C m | = 100. In such case, we instantiate and minimize L se (v m , C (n) m ) to produce high probabilities among 10 taken medications captured by − c * ∈C (n) m log p(c * |v m ) and lower the probabilities among 90 non-taken ones captured by c * ∈{Cm\C (n) m } log p(c * |v m ). In practise, Sigmoid(f (v * )) should be transformed by applying a fully connected neural network f (•) with one hidden layer. With an analogy to the Masked LM task in BERT, we also used specific symbol [MASK] to randomly replace the original medical code c * ∈ C * . So there are 15% codes in C * which will be replaced randomly and the model should have the ability to predict the masked code based on others.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Graphical illustration of fine-tuning procedure. We separately aggregate the visit embeddings produced by diagnoses and medications before t-visit. Then, these two aggregated visit embeddings are concatenated with diagnosis visit embedding at t-visit as input for fine-tuning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>-training on each visit of EHR so that the data with only one hospital visit can also be utilized. We revised BERT to fit EHR data in both input and pre-training objectives. To our best knowledge, G-BERT is the first model that leverages Transformers and language model pre-training techniques in healthcare domain. Compared with other supervised models, G-BERT can utilize discarded/unlabeled data more efficiently.</figDesc><table><row><cell cols="3">2. Medical ontology embedding with graph neural net-</cell></row><row><cell cols="3">works: We enhance the representation of medical codes</cell></row><row><cell cols="3">via learning medical ontology embedding for each med-</cell></row><row><cell cols="3">ical codes with graph neural networks. We then in-</cell></row><row><cell cols="3">put the ontology embedding into a multi-layer Trans-</cell></row><row><cell cols="3">former [Vaswani et al., 2017] for BERT-style pre-</cell></row><row><cell cols="2">training and fine-tuning.</cell><cell></cell></row><row><cell cols="2">2 Related Work</cell><cell></cell></row><row><cell>Medication</cell><cell>Recommendation. Medication</cell><cell>Recom-</cell></row><row><cell cols="3">mendation can be categorized into instance-based and</cell></row><row><cell cols="3">longitudinal recommendation methods [Shang et al., 2019].</cell></row></table><note>arXiv:1906.00346v2 [cs.AI] 27 Nov 2019 pre</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>denotes the medical code set and |C * | the size of the code set. c * ∈ C * is the medical code.</figDesc><table><row><cell>is the</cell></row><row><cell>total number of patients; T (n) is the number of visits of the</cell></row><row><cell>n th patient. Here we choose two main medical code to rep-</cell></row><row><cell>resent each visit X t = C t d ∪ C t m of a patient which is a union set of corresponding diagnoses codes C t d ⊂ C d and medica-tions codes C t m ⊂ C m . For simplicity, we use C t  *  to indicate</cell></row><row><cell>the unified definition for different type of medical codes and</cell></row><row><cell>drop the superscript (n) for a single patient whenever it is un-</cell></row><row><cell>ambiguous. C  Definition 2 (Medical Ontology). Medical codes are usu-</cell></row><row><cell>ally categorized according to a tree-structured classification</cell></row><row><cell>system such as ICD-9 ontoloy for diagnosis and ATC ontol-</cell></row><row><cell>ogy for medication. We use O d , O m to denote the ontology</cell></row><row><cell>for diagnosis and medication. Similarly, we use O  *  to indi-</cell></row><row><cell>cate the unified definition for different type of medical codes.</cell></row><row><cell>In detial, O  *  = C  *  ∪ C  *  where C  *  denotes the codes excluding</cell></row><row><cell>leaf codes. For simplicity, we define two function pa(•), ch(•)</cell></row><row><cell>which accept target medical code and return ancestors' code</cell></row><row><cell>set and direct child code set.</cell></row><row><cell>Problem Definition (Medication Recommendation).</cell></row><row><cell>Given diagnosis codes C t</cell></row></table><note>* d of the visit at time t, patient history X</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Statistics of the Data (dx for diagnosis, rx for medication).</figDesc><table><row><cell>Stats</cell><cell cols="2">Single-Visit Multi-Visit</cell></row><row><cell># of patients</cell><cell>30,745</cell><cell>6,350</cell></row><row><cell cols="2">avg # of visits 1.00</cell><cell>2.36</cell></row><row><cell>avg # of dx</cell><cell>39</cell><cell>10.51</cell></row><row><cell>avg # of rx</cell><cell>52</cell><cell>8.80</cell></row><row><cell cols="2"># of unique dx 1,997</cell><cell>1,958</cell></row><row><cell cols="2"># of unique rx 323</cell><cell>145</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Performance on Medication Recommendation Task.</figDesc><table><row><cell>Methods</cell><cell cols="2">Jaccard PR-AUC</cell><cell>F1</cell><cell># of parameters</cell></row><row><cell>LR</cell><cell>0.4075</cell><cell>0.6716</cell><cell>0.5658</cell><cell>-</cell></row><row><cell>GRAM</cell><cell>0.4176</cell><cell>0.6638</cell><cell>0.5788</cell><cell>3,763,668</cell></row><row><cell>LEAP</cell><cell>0.3921</cell><cell>0.5855</cell><cell>0.5508</cell><cell>1,488,148</cell></row><row><cell>RETAIN</cell><cell>0.4456</cell><cell>0.6838</cell><cell>0.6064</cell><cell>2,054,869</cell></row><row><cell>GAMENet −</cell><cell>0.4401</cell><cell>0.6672</cell><cell>0.5996</cell><cell>5,518,646</cell></row><row><cell>GAMENet</cell><cell>0.4555</cell><cell>0.6854</cell><cell>0.6126</cell><cell>5,518,646</cell></row><row><cell cols="2">G-BERT G − ,P − 0.4186</cell><cell>0.6649</cell><cell>0.5796</cell><cell>2,634,145</cell></row><row><cell>G-BERT G −</cell><cell>0.4299</cell><cell>0.6771</cell><cell>0.5903</cell><cell>2,634,145</cell></row><row><cell>G-BERT P −</cell><cell>0.4236</cell><cell>0.6704</cell><cell>0.5844</cell><cell>3,034,045</cell></row><row><cell>G-BERT</cell><cell>0.4565</cell><cell>0.6960</cell><cell>0.6152</cell><cell>3,034,045</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the National Science Foundation award IIS-1418511, CCF-1533768 and IIS-1838042, the National Institute of Health award 1R01MD011682-01 and R56HL138415.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Patient subtyping via time-aware lstm networks</title>
		<author>
			<persName><surname>Baytas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Retain: An interpretable predictive model for healthcare using reverse time attention mechanism</title>
		<author>
			<persName><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1259</idno>
		<idno>arXiv:1810.04805</idno>
	</analytic>
	<monogr>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
				<editor>
			<persName><forename type="first">Aaron</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Pierre-Antoine</forename><surname>Courville</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Pascal</forename><surname>Manzagol</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Samy</forename><surname>Vincent</surname></persName>
		</editor>
		<editor>
			<persName><surname>Bengio</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2010">2014. 2014. 2016. 2016. 2017. 2017. 2018. 2018. 2018. 2018. Feb. 2010. 2017. 2017</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="625" to="660" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Health-atm: A deep architecture for multifaceted patient health record representation and risk prediction</title>
		<author>
			<persName><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.03677</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 SIAM International Conference on Data Mining</title>
				<meeting>the 2018 SIAM International Conference on Data Mining<address><addrLine>Matt Gardner, Christopher Clark, Kenton Lee,</addrLine></address></meeting>
		<imprint>
			<publisher>Alec Radford</publisher>
			<date type="published" when="2006">2006. 2006. 2016. 2016. 2014. 2014. 2017. 2015. 2015. 2012. 2012. 2018. 2018. 2017. 2018. 2018. 2018. 2018</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>NAACL. Radford et al.. Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Gamenet: Graph augmented memory networks for recommending medication combination</title>
		<author>
			<persName><surname>Ramachandran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02683</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016. 2019. 2019</date>
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Unsupervised pretraining for sequence to sequence learning</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><surname>Vaswani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
	</analytic>
	<monogr>
		<title level="m">Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks</title>
				<imprint>
			<publisher>Petar Velickovic</publisher>
			<date type="published" when="2017">2017. 2017. 2017. 2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Advances in Neural Information Processing Systems</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Opportunities and challenges in developing deep learning models using electronic health records data: a systematic review</title>
		<author>
			<persName><forename type="first">Xiao</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Informatics Association</title>
		<imprint>
			<date type="published" when="2018">2018a. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Readmission prediction via deep contextual embedding of clinical concepts</title>
		<author>
			<persName><forename type="first">Xiao</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2018">2018b. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Leap: Learning to prescribe effective and safe treatment combinations for multimorbidity</title>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="1315" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
