<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">D</forename><surname>Goldberg</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of General Engineering</orgName>
								<orgName type="laboratory">Illinois Genetic Algorithms Laboratory</orgName>
								<orgName type="institution" key="instit1">University of Illinois</orgName>
								<orgName type="institution" key="instit2">Urbana-Champaign</orgName>
								<address>
									<postCode>61801</postCode>
									<settlement>Urbana</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">He is with Google, Inc</orgName>
								<address>
									<postCode>94041</postCode>
									<settlement>Mountain View</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of General Engineering</orgName>
								<orgName type="laboratory">Illinois Genetic Algorithms Laboratory</orgName>
								<orgName type="institution" key="instit1">University of Illinois</orgName>
								<orgName type="institution" key="instit2">Urbana-Champaign</orgName>
								<address>
									<settlement>Urbana</settlement>
									<region>IL</region>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Department of Environmental Engineering</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Faculdade de CiÃªncias e Tecnologia</orgName>
								<orgName type="institution">Universidade Nova de Lisboa</orgName>
								<address>
									<settlement>Lisbon</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Department of General Engineering</orgName>
								<orgName type="institution" key="instit1">University of Illinois</orgName>
								<orgName type="institution" key="instit2">Urbana-Champaign</orgName>
								<address>
									<postCode>61801</postCode>
									<settlement>Urbana</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DCEFCAA746DF8BD09C10F564E9B24460</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T05:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Brief Papers</head><p>The Compact Genetic Algorithm Georges R. Harik, Fernando G. Lobo, and David E. Goldberg Abstract-This paper introduces the compact genetic algorithm (cGA) which represents the population as a probability distribution over the set of solutions and is operationally equivalent to the order-one behavior of the simple GA with uniform crossover. It processes each gene independently and requires less memory than the simple GA. The development of the compact GA is guided by a proper understanding of the role of the GA's parameters and operators. The paper clearly illustrates the mapping of the simple GA's parameters into those of an equivalent compact GA. Computer simulations compare both algorithms in terms of solution quality and speed.</p><p>Finally, this work raises important questions about the use of information in a genetic algorithm, and its ramifications show us a direction that can lead to the design of more efficient GA's.</p><p>Index Terms-Bit wise simulated crossover, genetic algorithms, population based incremental learning, probabilistic modeling, univariate marginal distribution algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>T HERE is a tendency in the community of evolutionary computation to treat the population with almost mystical reverence, and certainly the population deserves our respect as it is the source of all that goes right (or wrong) in a genetic algorithm (GA) with respect to function evaluation, schema evaluation, and partition identification <ref type="bibr" target="#b13">[14]</ref>. But if one lesson is clear from the history of GA analysis and design, it is that genetic algorithms are complex objects and multiple perspectives are useful in understanding what they can and cannot do.</p><p>In this paper, we take a minimalist view of the population and create a GA that mimics the order-one behavior of a simple GA using a finite memory bit by bit. Although the resulting compact genetic algorithm (cGA) is not intended to replace population-oriented GA's, it does teach us important lessons regarding GA memory and efficiency. As a matter of design, the cGA shows us an interesting way of getting more information out of a finite set of evaluations.</p><p>We start by discussing the inspiration of this work from a random walk model that has been proposed recently. We then present the cGA and describe the mapping of the sGA's parameters into those of an equivalent cGA. Along the way, computer simulations compare the two algorithms, both in terms of solution quality and speed. At the end of the paper, important ramifications are outlined concerning the design of more efficient GA's.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. MOTIVATION AND RELATED WORK</head><p>This work is primarily inspired by the random walk model introduced by Harik et al. <ref type="bibr" target="#b9">[10]</ref>. In that work, the authors gave accurate estimates of the GA's convergence on a special class of problems: problems consisting of tightly coded, nonoverlapping building blocks. A building block is a set of genes that as a whole give a high contribution to the fitness of an individual. Because there are no interactions among building blocks, the authors made the assumption that they could be solved independently. Therefore, their model focused on one building block at a time. The next paragraph describes the basic idea of the model.</p><p>In the initial population, there will be some instances of the building block. Then, during the action of a GA run, the number of instances of the building block can increase or decrease. Eventually, the building block will spread throughout all the population members or it will become extinct.</p><p>This type of process is easily modeled using a random walk as a mathematical tool. Using such a model, Harik et al. were able to accurately predict the GA's convergence. There, the random walk variable represents the number of building blocks in the population at a given time. Two absorbing barriers (one at zero and one at the population size) represent the success or failure in the overall decision of the GA. The transition probability of the random walk is given by the probability that the GA commits a decision error on two competing schemata. This error in decision making occurs because a schema is always evaluated within the context of a larger individual. The GA can make an incorrect decision in a partition because of the noise coming from the remaining partitions. In the model, the population plays the role of a memory to account for a finite number of such decision-errors.</p><p>The dynamics of the random walk model suggests that it is possible to directly simulate its behavior for order-one problems. <ref type="foot" target="#foot_0">1</ref> The idea is to simulate independent random walks bit by bit. The next section, which introduces the cGA, shows how this is possible. The cGA represents the population as a probability distribution over the set of solutions. By discretizing its probability representation, the proposed algorithm reduces the GA's memory requirements. In addition, the manner in which the cGA manipulates this distribution allows it to mimic the order-one behavior of the simple genetic algorithm (sGA). But before introducing the cGA, let us review other related works.</p><p>Ackley <ref type="bibr" target="#b0">[1]</ref> introduced a learning algorithm that manipulates a gene vector via positive and negative feedback coming from the population members. He used a political metaphor to describe the algorithm where the voters (population) express their satisfaction or dissatisfaction toward an -member government (a point in the search space).</p><p>Syswerda <ref type="bibr" target="#b17">[18]</ref> introduced an operator called bit-based simulated crossover (BSC) that uses the statistics in the GA's population to generate offspring. BSC does a weighted average of the alleles of the individuals along each bit position (a bit column). By using the fitness of the individuals in this computation, BSC integrates the selection and crossover operators into a single step. A variation of BSC was also discussed by Eshelman and Schaffer <ref type="bibr" target="#b7">[8]</ref> in the context of investigating how GA's differ from population-based hillclimbers.</p><p>Population-based incremental learning (PBIL) was introduced by Baluja <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>. As opposed to storing the whole population as in BSC, PBIL uses a probability vector over the chromosome to represent its population. Specifically, it records the proportion of ones (and consequently zeroes) at each gene position. These probabilities are initially set to 0.5 and move toward zero or one as the search progresses. The probability vector is used to generate new solutions and thus represents the combined experiences of the PBIL algorithm at any one time. Using the probability vector, PBIL generates a certain number of solutions and updates the vector based on the fitnesses of these solutions. The aim of this update is to move the probability vector toward the fittest of the generated solutions. The update rule is similar to that used in learning vector quantization <ref type="bibr" target="#b11">[12]</ref>. Fig. <ref type="figure" target="#fig_0">1</ref> shows the pseudocode of PBIL.</p><p>The number of individuals generated, the number of individuals to update from, the stopping criterion, and the rate of the probability vector's change are all parameters of the algorithm. Attempts were made to relate PBIL's parameters to the simple GA. For instance, the number of individuals generated was equated with the GA's population size. These attempts were not successful because the GA manipulates its distributions in a different way. In Section III we show how this is possible in a related algorithm.</p><p>Another related algorithm is the univariate marginal distribution algorithm (UMDA), proposed by MÃ¼hlenbein and PaaÃ <ref type="bibr" target="#b14">[15]</ref>. UMDA is similar to PBIL as it also treats each gene independently from each other. UMDA maintains a population of individuals. Then it applies a selection method to create a new population. Based on the new population, the frequencies of each gene are computed and are used to generate a new population of individuals. This generation step is a kind of population-wise crossover operator and replaces the traditional pairwise crossover operator of the traditional genetic algorithm.</p><p>The following section introduces the compact GA, an algorithm similar to PBIL and UMDA. The main difference is the connection that is made between the compact GA and the simple GA. Specifically, it is shown that for order-one problems, the two algorithms are approximately equivalent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THE COMPACT GENETIC ALGORITHM</head><p>Harik et al. <ref type="bibr" target="#b9">[10]</ref> analyzed the growth and decay of a particular gene in the population as a one-dimensional random walk. As the GA progresses, genes fight with their competitors and their number in the population can go up or down depending on whether the GA makes good or bad decisions. These decisions are made implicitly by the GA when selection takes place. The next section explores the effects of this decision making.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Selection</head><p>Selection gives more copies to better individuals. But it does not always do so for better genes. This is because genes are always evaluated within the context of a larger individual. For example, consider the onemax problem (that of counting ones).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Suppose individual competes with individual individual chromosome fitness</head><p>When these two individuals compete, individual will win. At the level of the gene, however, a decision error is made on the second position. That is, selection incorrectly prefers the Imagine the following selection scheme: pick two individuals randomly from the population, and keep two copies of the better one. This scheme is equivalent to a steady-state binary tournament selection. In a population of size , the proportion of the winning alleles will increase by . For instance, in the previous example the proportion of 1's will increase by at gene positions 1 and 3, and the proportion of 0's will also increase by at gene position 2. At gene position 4, the proportion will remain the same. This thought experiment suggests that an update rule increasing a gene's proportion by simulates a small step in the action of a GA with a population of size</p><p>The next section explores how the generation of individuals from a probability distribution mimics the effects of crossover.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Crossover</head><p>The role of crossover in the GA is to combine bits and pieces from fit solutions. A repeated application of most commonly used crossover operators eventually leads to a decorrelation of the population's genes. In this decorrelated state, the population is more compactly represented as a probability vector. Thus the generation of individuals from this vector can be seen as a shortcut to the eventual aim of crossover. Fig. <ref type="figure" target="#fig_1">2</ref> gives pseudocode of the compact GA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Two Main Differences from PBIL</head><p>The proposed algorithm differs from PBIL in two ways: 1) it can simulate a GA with a given population size, and 2) it reduces the memory requirements of the GA. The update step of the compact GA has a constant size of . While the simple GA needs to store bits for each gene position, the compact GA only needs to keep the proportion of ones (and zeros), a finite set of numbers that can be stored with bits. With PBIL's update rule (see 1), an element in the probability vector can have any arbitrary precision, and the number of values that can be stored in an element of the vector is not finite. Therefore, PBIL cannot achieve the same level of memory compression as the cGA. While in many problems computer memory is not a concern, we can easily imagine large problems that need huge population sizes. In such cases, cutting down the memory requirement from to results in significant savings.</p><p>PBIL typically generates a large number of individuals from the probability vector. According to Baluja and Caruana <ref type="bibr" target="#b2">[3]</ref>, that number was something analogous to the population size. In the compact GA, the size of the update step is the "thing" that is analogous to the population size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS</head><p>This section presents simulation results and compares the compact GA with the simple GA, both in terms of solution quality and in the number of function evaluations taken. All experiments are averaged over 50 runs. The simple GA uses binary tournament selection without replacement and uniform crossover with exchange probability 0.5. Mutation is not used and crossover is applied with probability one. All runs end when the population fully converges, that is, when for each gene position all the population members have the same allele value (zero or one). Figs. <ref type="figure" target="#fig_2">3</ref> and<ref type="figure" target="#fig_3">4</ref> show the results of the experiments on a 100-bit onemax problem (the counting ones problem). Fig. <ref type="figure" target="#fig_2">3</ref> plots the solution quality (number of correct bits at the end of the run) for different population sizes. Fig. <ref type="figure" target="#fig_3">4</ref> plots the number of function evaluations taken until convergence for the various population sizes. On both graphs, the solid line is for the simple GA and the dashed line is for the compact GA. Additional simulations were performed with the binary integer function and with De Jong's test functions <ref type="bibr" target="#b4">[5]</ref>. The results obtained were similar to these, and are collected in Appendix A. The match between the two algorithms seems accurate, and gives evidence that the two are doing roughly the same thing and that they are somehow equivalent. Note however that while the sGA has a memory requirement of bits, the cGA requires only bits. The generation of individuals in the compact GA is equivalent to performing an infinite number of crossover rounds per generation in a simple GA. Thus the compact GA completely decorrelates the genes, while the simple GA still carries a little bit of correlation among the population's genes. Another difference is that the compact GA is incremental based while the simple GA is generational based. One could get a better approximation of the two algorithms by doing a batch-update of the probability vector once every competitions are performed. This would more closely mimic the generational behavior of the simple GA. We did not do that here because the difference is not significant. We are simply interested in showing that the two algorithms are approximately equivalent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. SIMULATING HIGHER SELECTION PRESSURES</head><p>This section introduces a modification to the compact GA that allows it to simulate higher selection pressures. We would like to simulate a tournament of size . The following mechanism produces such an effect. 1) Generate individuals from the probability vector and find out the best one. 2) Let the best individual compete with the other individuals, updating the probability vector along the way. Clearly, the best individual wins all the competitions, thus the above procedure simulates something like a tournament of size Steps 2-4 of the cGA's pseudocode (Fig. <ref type="figure" target="#fig_1">2</ref>) would have to be replaced by the ones shown in Fig. <ref type="figure" target="#fig_4">5</ref>.</p><p>Experiments on the onemax problem with and are shown in Fig. <ref type="figure">6</ref> confirming our expectations. Once more, the graphs show the solution quality and also the number of function evaluations needed to reach convergence. The runs were done for different population sizes. The top graphs are for the middle ones for and the bottom ones are for In all of them, the solid line is for the simple GA and the dashed line is for the compact GA.</p><p>Being able to simulate higher selection rates should allow the compact GA to solve problems with higher order building blocks in approximately the same way that a simple GA with uniform crossover does. It is known that to solve such problems, high selection rates are needed to compensate for the highly disruptive effects of crossover. Moreover, the population size required to solve such problems grows exponentially with the problem size <ref type="bibr" target="#b18">[19]</ref>. To test the compact GA on problems with higher-order building blocks, ten copies of a 3-bit deceptive subfunction are concatenated to form a 30-bit problem. Each subfunction is a 3-bit trap function with deceptive-to-optimal ratio of 0.7 <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b5">[6]</ref>. The results are presented in Fig. <ref type="figure">7</ref>.</p><p>In this case there is a discrepancy between the two algorithms. This can be explained on schema theorem grounds. Using uniform crossover, an order-building block has a survival probability of According to the schema theorem, the simple GA should be able to propagate these building blocks as long as the selection rate is high enough to compensate for the crossover disruption. For an order-3 schema, the survival probability is 1/4, so the sGA should start to work well when the selection rate is greater than 4. In the case of the cGA, we can think of a global schema theorem. The survival probability of a schema under the cGA would then be given by survival of In the cGA, all the start with 1/2. This means that initially the survival probability of an order-3 building block is 1/8. Therefore, the building block should grow when the selection rate is greater than eight. This argumentation explains the results obtained in Fig. <ref type="figure">7</ref> (see the cases and . Observe that a selection rate of is not enough to combat the disruptive effects of crossover. No matter what population size is used, the compact GA (and also the simple GA) with will fail to solve this problem. This is an indication that the problem has higher order building blocks, and that it can only be solved with these kind of algorithms by raising the selection pressure. Fig. <ref type="figure">6</ref>. The plots illustrate the mapping of the selection rate of the compact GA into that of the simple GA using the onemax function. Selection rates are two, four, and eight. The algorithms were ran from population size 4-100 with increments of two (4, 6, 8, . . . ; 100). For selection rate eight, the initial population size is eight. On the left side, the graphs plot the number of correct bits at the end of the run for the various population sizes. On the right side, the graphs plot the number of function evaluations taken to converge. Selection rates are s = 2 (top), s = 4 (middle), and s = 8 (bottom). The solid lines are for the simple GA, and the dashed lines are for the compact GA.</p><p>As mentioned previously, the compact GA completely decorrelates the population's genes, while the simple GA still carries a little bit of allele correlation. This effect may also help to explain the difference that is observed in the case of the deceptive problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. GETTING MORE WITH LESS</head><p>This section introduces a concept that is unusual in terms of standard GA practice. To motivate the discussion, let us start with an analogy between the selection operator of a GA and a tennis (or soccer) competition.</p><p>In tennis there are two kinds of tournaments: elimination and round-robin. In both cases, the players are matched in pairs. In the elimination case, the losers are eliminated from the tournament and the winners proceed to next round. In the round-robin variation, everybody plays with everybody. It is also possible to have competitions that are something in between these two. An example is the soccer World Cup. There, the teams are divided in groups and within each group the teams play round-robin. Then, the top-within each group proceed to the next phase.</p><p>After this brief detour, let us shift back to our discussion on genetic algorithms. Typically, a GA using binary tournament selection is very much like an elimination tennis competition. The only difference is that in the GA, each individual participates in two tournaments. This is because we do not want the population to be chopped by a half after each generation. Round-robin competitions are not usually done in GA's, because this would make the population size grow after each generation. Fig. <ref type="figure">7</ref>. These plots compare the compact GA and the simple GA on the ten copies of a 3-bit trap function, using selection rates of two, four, and eight. The algorithms were ran for population sizes 8, 500, 1000, 1500, 2000, 2500, and 3000. On the left side, the graphs plot the number of correct building blocks (sub-functions) at the end of the run for the various population sizes. On the right side, the graphs plot the number function evaluations taken to converge. Selection rates are s = 2 (top), s = 4 (middle), and s = 8 (bottom). The solid lines are for the simple GA, and the dashed lines are for the compact GA.</p><p>The remainder of this section shows how it is possible to have round-robin-like competitions within the compact GA while maintaining a fixed population size. To implement it, we do the following: instead of generating two individuals, generate individuals and make a round-robin tournament among them, updating the probability vector along the way. Steps 2-4 of the cGA's pseudocode (Fig. <ref type="figure" target="#fig_1">2</ref>) would have to be replaced by the ones shown in Fig. <ref type="figure" target="#fig_5">8</ref>.</p><p>This results in a faster search because binary tournaments are made using only function evaluations. On the other hand, this scheme takes bigger steps in the probability vector and therefore more decision-making mistakes are made. When the tournaments are played using elimination. When the tournament is played in a round-robin fashion among all the population members. When is between 2 and , we get something that is neither a pure elimination scheme nor a pure round-robin scheme. Experiments of the cGA with a selection rate of are performed again, but this time using different values of  Plots for the onemax problem are shown in Figs. 9-11. Fig. <ref type="figure" target="#fig_6">9</ref> shows the solution quality (number of correct bits at the end of the run) of the compact GA with for different population sizes. Fig. <ref type="figure" target="#fig_7">10</ref> shows the number of function evaluations taken to converge by the compact GA with for the different population sizes. Fig. <ref type="figure" target="#fig_8">11</ref> is a combination of Figs. 9 and 10. It shows that a given solution quality can be obtained faster by using or instead of . In other words, although using higher values of reduces the solution quality, the corresponding increase in speed makes it worth its while. Observe that after a certain point, it is risky to increase due to solution degradation. In this example, using is worse than using or This shows that there must be an optimal and raises important questions concerning GA efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. EXTENSIONS</head><p>Two extensions are proposed for this work: 1) investigate extensions of the cGA for order-problems and 2) investigate how to maximize the information contained in a finite set of evaluations in order to design more efficient GA's. The compact GA is basically a 1-bit optimizer and ignores the interactions among the genes. The set of problems that can be solved efficiently with such schemes are problems that are somehow easy. The representation of the population in the compact GA explicitly stores all the order-one schemata contained in the population. It is possible to have a similar scheme that is also capable of storing higher order schemata in a compact way. Recently, a number of algorithms have been suggested that are capable of dealing with pairwise gene interactions <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b15">[16]</ref> and even with order-interactions <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b16">[17]</ref>.</p><p>Another direction is to investigate more deeply the results discussed in Section VI and discover their implications for the design of more efficient GA's. Our preliminary work has shown that it is possible to extract more information from a set of function evaluations, than the usual information extracted by the simple GA. But how to use this additional information in the context of a simple GA is still an open question and deserves further research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSIONS</head><p>This paper presented the compact GA, an algorithm that mimics the order-one behavior of a simple GA with a given population size and selection rate, but that reduces its memory requirements. The design of the compact GA was explained, and computational experiments illustrated the approximate equivalence of the compact GA with a simple GA using uniform crossover.</p><p>Although the compact GA approximately mimics the orderone behavior of the simple GA with uniform crossover, it is not a replacement for the simple GA. Simple GA's can perform quite well when the user has some knowledge about the nonlinearities in the problem. In that case, the building blocks can be tightly coded and they can be propagated throughout the population through the repeated action of selection and recombination. Note that in general, this linkage information is not known. In most applications, however, the GA user has some knowledge about the problem's domain and tends to code together in the chromosome features that are somehow spatially related in the original problem. In a way, the GA user has partial knowledge about the linkage. This is probably one of the main reasons why simple GA's have had so much success in real-world applications. Of course, sometimes the user think he has a good coding, when in fact he does not. In such cases, simple GA's are likely to perform poorly.</p><p>Finally, and most important, this study has introduced new ideas that have important ramifications for GA design. By looking at the simple GA from a different perspective, we learned more about complex dynamics and opened new doors toward the goal of having more efficient GA's.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A</head><p>This appendix presents simulation results comparing the compact GA and the simple GA on the binary integer function, and on De Jong's test functions <ref type="bibr" target="#b4">[5]</ref>. All experiments are averaged over 50 runs. The simple GA uses binary tournament selection without replacement and uniform crossover with exchange probability 0.5. Mutation is not used, and crossover is applied all the time. All runs end when the population fully converges-that is-when all the individuals have the same alleles at each gene position.</p><p>The binary integer function is defined as The GA solves this problem in a sequential way (domino-like). First it pays attention to the most significant bits and then, once those bits have converged, it moves on to next most significant bits. For this function, the solution quality is measured by the number of consecutive bits solved correctly.</p><p>For De Jong's test functions, the solution quality is measured by the objective function value obtained at the end of the run. Each parameter is coded with the same precision as described in his dissertation <ref type="bibr" target="#b4">[5]</ref> In functions F3 and F4, the number of function evaluations needed to reach convergence by the two algorithms do not match very closely. F3 has many optima, and F4 is a noisy fitness function. Due to the effects of genetic drift, it takes a long time for both algorithms to fully converge. Genetic drift occurs when there is no selection pressure to distinguish between two or more individuals, and conse- quently, the convergence process starts to look like a randomwalk. The drift time is slightly longer in the case of the simple GA, however, possibly because the simple GA uses tournament selection without replacement, while the compact GA does something more similar to tournament with replacement. The important thing to retain is that the behavior of compact GA is approximately equivalent to that of the simple GA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX B PHYSICAL INTERPRETATION</head><p>An analogy with a potential field can be made to explain the search process of the compact GA and is easily visualized for 2-bit problems. Similar results were obtained in <ref type="bibr" target="#b12">[13]</ref> in the context of studying the convergence behavior of the PBIL algorithm. For completeness, they are presented again.</p><p>Consider Fig. <ref type="figure" target="#fig_15">18</ref>. The corner points are the four points of the search space (00, 01, 10, 11). Each corner point applies a force to attract the particle (population) represented by the black dot. The position of the particle is given by , which represents the proportion of 1's in the first and second gene positions respectively. The particle (population) is submitted to a potential field on the search space, seeking its minimum. As the search progresses, the particle (population) moves up or down, left or right (the proportions of 1's in each gene increase or decrease by</p><p>) and eventually, one of the corners will capture the particle (the population converges). Let us illustrate this with a 2-bit onemax problem and with the minimal deceptive problem (MDP) <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Onemax</head><p>Let and be the proportion of 1's at the first and second genes respectively. The search space, the potential field, and a graphical interpretation is shown as point fitness</p><p>The potential at position is:  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MDP</head><p>Likewise, for the minimal deceptive problem, the search space is point fitness  and the potential at position is Fig. <ref type="figure" target="#fig_16">19</ref> shows that the onemax is an easy function. Fig. <ref type="figure" target="#fig_17">20</ref> gives a visual representation of Goldberg's observation <ref type="bibr" target="#b8">[9]</ref> that on the MDP, the GA could converge to the deceptive attractor given certain initial conditions (high proportion of 00 in the initial population).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Pseudocode of PBIL.</figDesc><graphic coords="2,314.64,59.58,234.00,286.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Pseudocode of the compact GA.</figDesc><graphic coords="3,57.30,59.58,222.72,276.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Comparison of the solution quality (number of correct bits at the end of the run) achieved by the compact GA and the simple GA on a 100-bit onemax problem. The algorithms were ran from population size 4-100 with increments of 2 (4, 6, 8, . . ., 100). The solid line is for the simple GA. The dashed line is for the compact GA.</figDesc><graphic coords="3,340.86,59.58,181.44,150.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Comparison of the compact GA and the simple GA in the number of function evaluations needed to achieve convergence on a 100-bit onemax problem. The algorithms were ran from population size 4-100 with increments of 2 (4, 6, 8, . . ., 100). The solid line is for the simple GA. The dashed line is for the compact GA.</figDesc><graphic coords="4,75.30,59.58,186.48,150.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Modification of the compact GA that implements tournament selection of size s. This would replace steps 2-4 of the cGA code.</figDesc><graphic coords="4,307.02,59.58,249.12,138.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Modification of the compact GA that implements a round-robin tournament. This would replace steps 2-4 of the cGA code.</figDesc><graphic coords="6,305.16,551.34,252.96,130.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. This graph shows the solution quality on a 100-bit onemax problem for various population sizes (4, 6, 8, . . . ; 100), using different values of m:Observe that the solution quality decreases as m increases.</figDesc><graphic coords="7,77.46,59.58,182.40,150.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. This graph shows the number of function evaluations needed to reach convergence on a 100-bit onemax problem, using various population sizes (4, 6, 8, . . . ; 100), and different values of m: Observe that the speed increases as m increases.</figDesc><graphic coords="7,74.88,262.44,187.44,150.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. This is a combination of the previous two graphs. It shows that to achieve a given solution quality, it is better to use m = 4 or m = 8 instead of m = 2 or m = 40: In other words, the best strategy is neither to use a pure elimination tournament, nor a pure round-robin tournament.</figDesc><graphic coords="7,338.94,59.58,185.28,150.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Comparison of the simple GA and the compact GA on a 30-bit binary integer function. (a) shows the solution quality obtained at the end of the runs. (b) shows the number of function evaluations taken to converge. The algorithms were ran from population size 4-140 with increments of two (4, 6, 8, . . . ; 140). The solid line is for the simple GA and the dashed line is for the compact GA.</figDesc><graphic coords="8,75.30,233.70,186.48,150.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Comparison of the simple GA and the compact GA on function F1. (a) shows the solution quality obtained at the end of the runs. (b) shows the number of function evaluations taken to converge. The algorithms were ran from population size 4-100 with increments of two (4, 6, 8, . . . ; 100). The solid line is for the simple GA and the dashed line is for the compact GA.</figDesc><graphic coords="8,337.74,234.42,187.68,149.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. Comparison of the simple GA and the compact GA on function F2. (a) shows the solution quality obtained at the end of the runs. (b) shows the number of function evaluations taken to converge. The algorithms were ran from population size 4-100 with increments of two (4, 6, 8, . . . ; 100). The solid line is for the simple GA and the dashed line for the compact GA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. Comparison of the simple GA and the compact GA on function F3. (a) shows the solution quality obtained at the end of the runs. (b) shows the number of function evaluations taken to converge. The algorithms were ran from population size 4-100 with increments of two (4, 6, 8, . . . ; 100). The solid line is for the simple GA and the dashed line is for the compact GA.</figDesc><graphic coords="9,335.46,232.98,192.24,150.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 16 .</head><label>16</label><figDesc>Fig. 16. Comparison of the simple GA and the compact GA on function F4. (a) shows the solution quality obtained at the end of the runs. (b) shows the number of function evaluations taken to converge. The algorithms were ran from population size 4-100 with increments of two (4, 6, 8, . . . ; 100). The solid line is for the simple GA and the dashed line is for the compact GA.</figDesc><graphic coords="10,72.72,236.22,191.76,149.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 17 .</head><label>17</label><figDesc>Fig. 17. Comparison of the simple GA and the compact GA on function F5. (a) shows the solution quality obtained at the end of the runs. (b) shows the number of function evaluations taken to converge. The algorithms were ran from population size 4-100 with increments of two (4, 6, 8, . . . ; 100). The solid line is for the simple GA and the dashed line is for the compact GA.</figDesc><graphic coords="10,361.50,472.98,140.16,130.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 18 .</head><label>18</label><figDesc>Fig. 18. The black circle represents the population. Its coordinates are p and q, the proportion of 1's in the first and second gene positions. The four corners are the points in the search space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 19 .</head><label>19</label><figDesc>Fig. 19. Potential field for the 2-bit onemax problem.</figDesc><graphic coords="11,45.66,59.58,245.76,196.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 20 .</head><label>20</label><figDesc>Fig. 20. Potential field for the MDP.</figDesc><graphic coords="11,45.90,296.58,245.28,201.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="5,103.20,54.96,393.84,435.96" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>By an order-one problem, we mean a problem that can be solved to optimality by combining only order-one schemata.</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was sponsored by the U.S. Air Force Office of Scientific Research Grant F49620-97-1-0050 and the U.S. Army Research Laboratory Grant DAAL01-96-2-0003. G. Harik was with the</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A Connectionist Machine for Genetic Hill Climbing</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Ackley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
			<publisher>Kluwer</publisher>
			<pubPlace>Boston, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Population-based incremental learning: A method for integrating genetic search based function optimization and competitive learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Baluja</surname></persName>
		</author>
		<idno>CMU- CS-94-163</idno>
		<imprint>
			<date type="published" when="1994">1994</date>
			<pubPlace>Pittsburgh, PA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon Univ.</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Removing the genetics from the standard genetic algorithm</title>
		<author>
			<persName><forename type="first">S</forename><surname>Baluja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<idno>CMU-CS-95-141</idno>
		<imprint>
			<date type="published" when="1995">1995</date>
			<pubPlace>Pittsburgh, PA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon Univ.</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Using optimal dependency-trees for combinatorial optimization: Learning the structure of the search space</title>
		<author>
			<persName><forename type="first">S</forename><surname>Baluja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Davies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 14th Int. Conf. Machine Learning</title>
		<meeting>14th Int. Conf. Machine Learning<address><addrLine>San Mateo, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="30" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">An analysis of the behavior of a class of genetic adaptive systems</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1975">1975</date>
			<pubPlace>Ann Arbor</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Univ. Michigan</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Analyzing deception in trap functions</title>
		<author>
			<persName><forename type="first">K</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Whitley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Foundations of Genetic Algorithms 2</title>
		<meeting><address><addrLine>San Mateo, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="93" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">MIMIC: Finding optima by estimating probability densities</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>De Bonet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Isbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Systems</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Mozer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Jordan</surname></persName>
		</editor>
		<editor>
			<persName><surname>Petsche</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">424</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Crossover&apos;s niche</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Eshelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Schaffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th Int. Conf. Genetic Algorithms, S. Forrest</title>
		<meeting>5th Int. Conf. Genetic Algorithms, S. Forrest<address><addrLine>San Mateo, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="9" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Simple genetic algorithms and the minimal, deceptive problem</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Genetic Algorithms and Simulated Annealing</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Davis</surname></persName>
		</editor>
		<meeting><address><addrLine>San Mateo, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="74" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The gambler&apos;s ruin problem, genetic algorithms, and the sizing of populations</title>
		<author>
			<persName><forename type="first">G</forename><surname>Harik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>CantÃº-Paz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th Int. Conf. Evolutionary Computation, T. BÃ¤ck</title>
		<meeting>4th Int. Conf. Evolutionary Computation, T. BÃ¤ck<address><addrLine>Ed. Piscataway, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="7" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Linkage learning via probabilistic modeling in the ECGA</title>
		<author>
			<persName><forename type="first">G</forename><surname>Harik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IlliGAL Rep. 99010</title>
		<meeting><address><addrLine>Urbana-Champaign</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Introduction to the Theory of Neural Computation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hertz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krogh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Palmer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Addison-Wesley</publisher>
			<pubPlace>Reading, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Toward a theory of population-based incremental learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>HÃ¶hfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rudolph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th Int. Conf. Evolutionary Computation</title>
		<editor>
			<persName><forename type="first">T</forename><surname>BÃ¤ck</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ed</forename><surname>Piscataway</surname></persName>
		</editor>
		<meeting>4th Int. Conf. Evolutionary Computation<address><addrLine>NJ</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">SEARCH, blackbox optimization, and sample complexity</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kargupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Foundations of Genetic Algorithms</title>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Belew</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Vose</surname></persName>
		</editor>
		<meeting><address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="291" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">From recombination of genes to the estimation of distributions I. binary parameters</title>
		<author>
			<persName><forename type="first">H</forename><surname>MÃ¼hlenbein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>PaaÃ</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Parallel Problem Solving from Nature</title>
		<editor>
			<persName><forename type="first">H.-M</forename><surname>Ppsn Iv</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Voigt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Ebeling</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H.-P</forename><surname>Rechenberg</surname></persName>
		</editor>
		<editor>
			<persName><surname>Schwefel</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="178" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The bivariate marginal distribution algorithm</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pelikan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>MÃ¼hlenbein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Soft Computing-Engineering Design and Manufacturing</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Roy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Furuhashi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Chawdhry</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="521" to="535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">BOA: The Bayesian optimization algorithm</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pelikan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>CantÃº-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Univ. Illinois</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<pubPlace>Urbana-Champaign</pubPlace>
		</imprint>
	</monogr>
	<note>IlliGAL Rep. 99003</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Simulated crossover in genetic algorithms</title>
		<author>
			<persName><forename type="first">G</forename><surname>Syswerda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Foundations of Genetic Algorithms</title>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Whitley</surname></persName>
		</editor>
		<meeting><address><addrLine>San Mateo, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="239" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mixing in genetic algorithms</title>
		<author>
			<persName><forename type="first">D</forename><surname>Thierens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th Int. Conf. Genetic Algorithms, S. Forrest</title>
		<meeting>5th Int. Conf. Genetic Algorithms, S. Forrest<address><addrLine>San Mateo, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
