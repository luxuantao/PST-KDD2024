<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EGR: Equivariant Graph Refinement and Assessment of 3D Protein Complex Structures</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-05-20">20 May 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Alex</forename><surname>Morehead</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering &amp; Computer Science</orgName>
								<orgName type="institution">University of Missouri Columbia</orgName>
								<address>
									<postCode>65211</postCode>
									<region>MO</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiao</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering &amp; Computer Science</orgName>
								<orgName type="institution">University of Missouri Columbia</orgName>
								<address>
									<postCode>65211</postCode>
									<region>MO</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tianqi</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering &amp; Computer Science</orgName>
								<orgName type="institution">University of Missouri Columbia</orgName>
								<address>
									<postCode>65211</postCode>
									<region>MO</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jian</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering &amp; Computer Science</orgName>
								<orgName type="institution">University of Missouri Columbia</orgName>
								<address>
									<postCode>65211</postCode>
									<region>MO</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Jianlin</forename><surname>Cheng</surname></persName>
							<email>chengji@umsystem.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering &amp; Computer Science</orgName>
								<orgName type="institution">University of Missouri Columbia</orgName>
								<address>
									<postCode>65211</postCode>
									<region>MO</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">EGR: Equivariant Graph Refinement and Assessment of 3D Protein Complex Structures</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-05-20">20 May 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2205.10390v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Protein complexes are macromolecules essential to the functioning and well-being of all living organisms. As the structure of a protein complex, in particular its region of interaction between multiple protein subunits (i.e., chains), has a notable influence on the biological function of the complex, computational methods that can quickly and effectively be used to refine and assess the quality of a protein complex's 3D structure can directly be used within a drug discovery pipeline to accelerate the development of new therapeutics and improve the efficacy of future vaccines. In this work, we introduce the Equivariant Graph Refiner (EGR), a novel E(3)-equivariant graph neural network (GNN) for multi-task structure refinement and assessment of protein complexes. Our experiments on new, diverse protein complex datasets, all of which we make publicly available in this work, demonstrate the state-of-the-art effectiveness of EGR for atomistic refinement and assessment of protein complexes and outline directions for future work in the field. In doing so, we establish a baseline for future studies in macromolecular refinement and structure analysis. 1  Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Protein complexes, and their structures in particular, underpin many important biological functions in living organisms <ref type="bibr" target="#b0">[1]</ref>. With an enhanced understanding of how protein chains interact to form complexes, fundamental research in fields such as drug discovery and materials science is likely to accelerate considerably <ref type="bibr" target="#b1">[2]</ref>. For example, to obtain the structure of a complex thought to interact with a candidate compound, in a typical drug discovery pipeline one must first either analytically derive its structure using powerful yet time and resource-intensive techniques such as X-ray crystallography and nuclear magnetic resonance spectroscopy <ref type="bibr" target="#b2">[3]</ref> or instead directly predict the structure using computational methods <ref type="bibr" target="#b3">[4]</ref>.</p><p>The benefits offered by using purely computational approaches for structure determination are numerous, such as significantly accelerating the quantity and speed at which one can obtain 3D structures for downstream studies. Such tools, which for single protein chains have advanced considerably in the last several years <ref type="bibr" target="#b4">[5]</ref>, have been used to accelerate the analysis of protein function at genomic scales <ref type="bibr" target="#b5">[6]</ref> and have inspired the development of new methods designed to predict interactions between protein chains <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> and to assess the quality of 3D protein structures <ref type="bibr" target="#b8">[9]</ref>. However, these methods are currently less reliable for modeling protein complexes <ref type="bibr" target="#b9">[10]</ref>. 1 Inference code as well as pre-trained models are available at https://github.com/BioinfoMachineLearning/DeepRefine In such cases, researchers then turn to structure refinement methods that aim to improve the quality of initial protein structures via either statistical techniques or learning-based methods <ref type="bibr" target="#b10">[11]</ref>. Computational tools designed to refine and assess the structure of protein complexes, especially low-quality structures, can simultaneously increase the available quality of such structures while offering researchers enhanced insight into the variety of molecular conformations and functions into which a complex can resolve. Nonetheless, refinement of 3D protein structures is a difficult task, with many challenges arising from the size of the search space in which a higher-quality structure can be found and the physical constraints that must be respected by an atomic system to form a realistic protein structure. Consequently, existing methods for complex refinement are driven by molecular dynamics and relaxation protocols, which rely on expert knowledge embedded in the system through the design of such protocols. The limits of such methods have previously been explored <ref type="bibr" target="#b10">[11]</ref> and, as such, there is an increasing level of interest to see if learning-based methods (e.g., deep learning (DL) models) can be used to refine complex structures.</p><p>Here, we introduce EGR, a novel E(3)-equivariant graph deep learning model for multi-task structure refinement and quality assessment of protein complexes -Figure <ref type="figure" target="#fig_0">1</ref>. Notably, we exploit equivariant graph neural networks (EGNNs) <ref type="bibr" target="#b11">[12]</ref> and careful input regularization to make a direct prediction of the refined structure for an initial protein complex as well as the per-residue quality of the refined structure. Given that our method requires only a single forward pass to finalize its predictions, we achieve significant speed-ups in inference time compared to traditional refinement software solutions. Moreover, our model refines the positions of all atoms in an input protein, making it the first of its kind in DL-based complex structure refinement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>We now proceed to describe prior works relevant to DL-based structure refinement and assessment.</p><p>Biomolecular structure prediction. Until very recently, the conventional means of determining a molecule's 3D structure would involve costly and time-consuming physical trials performed by experimental scientists <ref type="bibr" target="#b12">[13]</ref>. However, using fast computational inference, new DL methods have now made it possible to determine the structures of proteins and other biomolecules in a matter of minutes rather than weeks or months <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>. Such methods have promoted the widespread adoption of structure prediction software <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref> and have inspired several new works in DL-driven structure prediction <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>.</p><p>Protein representation learning (for DL-based molecular modeling). Proteins can be represented in many different forms. For example, representing proteins by their amino acid sequences has been shown to provide powerful structural information by comparing sequences to each other <ref type="bibr" target="#b13">[14]</ref> and extracting rich unsupervised learning representations using self-attention Transformers <ref type="bibr" target="#b19">[20]</ref>. From a geometric viewpoint, several works have aimed to encode protein structural priors directly within neural network architectures to model proteins hierarchically <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>, as computationally-efficient point clouds <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>, or as k-nearest neighbors (k-NN) geometric graphs <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref> for tasks such as protein function prediction <ref type="bibr" target="#b26">[27]</ref>, protein model quality assessment <ref type="bibr" target="#b27">[28]</ref>, and protein interaction region prediction <ref type="bibr" target="#b28">[29]</ref>.</p><p>Applications of structure prediction methods. Markedly, structure prediction models have been applied to predict the structures of all proteins in the human proteome and have aided in efforts to better understand the mechanisms underlying disordered proteins <ref type="bibr" target="#b29">[30]</ref>. Moreover, DL-based structure prediction methods have accelerated the discovery of promising drug candidates for therapeutics research <ref type="bibr" target="#b30">[31]</ref> and have enabled fast virtual drug screening <ref type="bibr" target="#b31">[32]</ref> and materials discovery <ref type="bibr" target="#b32">[33]</ref> at scale.</p><p>Deep learning for protein-protein docking. Recent advancements in geometric deep learning have supported the development of new DL-based models for rigid body protein-protein docking <ref type="bibr" target="#b33">[34]</ref>, as well as docking in a single-shot setting <ref type="bibr" target="#b34">[35]</ref>. In addition, previous DL models for single-chain protein structure prediction have been repurposed to predict protein complex structures <ref type="bibr" target="#b35">[36]</ref> and refine the geometry of side-chain atoms <ref type="bibr" target="#b36">[37]</ref>.</p><p>Deep learning for protein structure refinement. DL models have also been used to guide the refinement of residue positions within tertiary protein structures <ref type="bibr" target="#b37">[38]</ref> or predict refined residue positions using indirect target values such as inter-residue distances <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40]</ref>. Unfortunately, such methods, following refinement, require all-atom restoration procedures as a post-processing step to recover the positions of backbone and side-chain atoms. Promisingly, <ref type="bibr" target="#b40">[41]</ref> have begun exploring the use of DL models for all-atom refinement of protein tertiary structures, however, this method has demonstrated success solely for tertiary structures due to its high computational memory complexity.</p><p>Deep learning for protein structure quality assessment. As DL-driven structure prediction methods have matured, new methods for quality assessment (QA) of protein structures have also been developed to facilitate automatic ranking of protein structures. In particular, 2D convolutional neural networks (CNNs) <ref type="bibr" target="#b41">[42]</ref>, 3D CNNs <ref type="bibr" target="#b42">[43]</ref>, and GNNs <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b8">9]</ref> have recently been adopted for structure ranking.</p><p>Incorporating symmetries in GNNs. Embuing neural networks with inductive priors has a longstanding history in the field <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48]</ref>. Priors of particular importance for 3D structure modeling include rotation <ref type="bibr" target="#b48">[49]</ref> and translation <ref type="bibr" target="#b49">[50]</ref> equivariance. Such priors form the basis of 3D Euclidean transformations that can now directly be found within neural network layers <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b53">54]</ref> to increase networks' data efficiency and generalization capabilities <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b55">56]</ref>. Our work follows that of <ref type="bibr" target="#b11">[12]</ref> to incorporate E(3)-equivariance in our message passing neural network for 3D structure refinement and quality assessment. However, we go beyond this method by adding skip connections shown to improve gradient flow after position updates, regularizing node input features according to geometric priors for 3D molecules, and employing loss functions for refinement and quality assessment in a semi-supervised manner, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions.</head><p>Our work builds upon prior works by making the following contributions:</p><p>? We provide the first example of applying deep learning to the task of all-atom refinement of protein complex structures.</p><p>? We provide the first example of applying equivariant graph message passing to the multi-task setting of refining and assessing protein complex structures concurrently.</p><p>? We introduce the new semi-supervised EGR model, showcasing its effective use in improving the structure of interface regions between protein chains and estimating its confidence in such improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EGR Model</head><p>We now turn to describe the EGR model, which is shown in Figure <ref type="figure" target="#fig_0">1</ref> and detailed in Figure <ref type="figure" target="#fig_1">2</ref>. The EGR model receives as its input a single 3D protein complex graph representing an initial decoy structure for a given protein target. As such, all chains in the complex are modeled within the same homogeneous input graph structure. As described in Appendix B, we differentiate pairs of atoms belonging to different chains using a one-hot encoded edge feature to initialize the EGR model with knowledge of the complex's separate chains. In doing so, we allow for structural flexibility of each protein chain in our work, without assuming any rigid conformations within a complex. Notably, our model does this while being trained using only geometric information obtained from each input protein, as it does not make use of any coevolutionary or hand-crafted features for the task at hand. E(3)-equivariant transformations. In this work, we argue that E(3)-equivariance offers a suitable inductive prior for modeling 3D protein structures. We make this point based primarily on the computational efficiency of E(3)-equivariant networks today. As such, for data efficiency and generalization capabilities, we then turn to designing a neural network capable of capturing within its hidden layers E(3)-transformations of any input protein, especially since for this task we are left to train on a relatively small number of input examples. Towards this end, we propose the Equivariant Graph Refiner (EGR) model, which combines insights from Equivariant Graph Neural Networks <ref type="bibr" target="#b11">[12]</ref>, EquiDock <ref type="bibr" target="#b34">[35]</ref>, and EquiBind <ref type="bibr" target="#b18">[19]</ref>. The EGR model learns to transform node features and node positions in R 3 to perform graph message passing across each input complex graph. Implicitly, this means our model is exchanging information between protein chains in a complex. More formally, EGR(X,</p><formula xml:id="formula_0">F) = Z ? R 3?n , H ? R d?n</formula><p>, where H represents node embeddings and Z represents transformed node coordinates. We note that, in practical terms, EGR layers can be stacked sequentially in an E(3)-equivariant manner, such that 3D translations or rotations of an input graph will be reflected in the output of any EGR layer. Namely, for an arbitrary translation vector b ? R 3 and orthogonal matrix U ? SO(3), EGR(UX + b, F) = UZ + b, H. Subsequently, our definition of a single EGR layer is:</p><formula xml:id="formula_1">m j?i = ? e (h (l) i , h (l) j , f j?i , x (l) i -x (l) j 2 ), ?(i, j) ? E<label>(1)</label></formula><p>x</p><formula xml:id="formula_2">(l+1) i = ?x (0) i + (1 -?)x (l) i + 1 |N (i)| j?N (i) (x (l) i -x (l) j )? x (m j?i )<label>(2)</label></formula><formula xml:id="formula_3">m i = 1 |N (i)| j?N (i) m j?i , ?i ? V (3) h (l+1) i = ? ? ? h (h (l) i , m i , a i , f i ) + (1 -?) ? h (l) i , ?i ? V,<label>(4)</label></formula><p>where ? and ?, respectively, represent scalar coordinates-wise and node representations-wise skip connection strengths; N (i) denotes the graph neighbors of node i; a i are optional SE(3)-invariant attention coefficients <ref type="bibr" target="#b34">[35]</ref> derived from H; and the remaining ? functions are represented as shallow neural networks, with ? x producing a single scalar and all others a d-dimensional vector. We note that in our studies here, we compute a i from H using a variant of the Linear Attention Transformer <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b58">59</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Refining initial coordinates</head><p>As mentioned previously, each EGR layer predicts transformed coordinates Z. The output of our model's last EGR layer, Z L , then serves as our estimate of the input complex's refined coordinates, X (i.e., X = Z L ).</p><p>Refinement loss. To guide the EGR model towards a refined molecular conformation, we use the Huber loss <ref type="bibr" target="#b59">[60]</ref> between the EGR model's refined coordinates X and the ground truth coordinates X * . Formally, the EGR model's loss for refinement (i.e., PSR loss) is:</p><formula xml:id="formula_4">L i P SR = 1 2 (x i -x * i ) 2 , if |x i -x * i | &lt; ?, ? * (|x i -x * i | -1 2 * ?) otherwise L P SR = 1 |P| i?P L i P SR ,<label>(5)</label></formula><p>where x i are the model's predicted coordinates for node i, x * i are the ground truth coordinates for node i, and P is the set of indices of atoms for which ground truth coordinates exist in the corresponding native structure. Additionally, we let ? = 1.0 in the context of this study. In the context of refinement, outlier coordinate predictions may occur at any stage during model training, so we must take care to ensure our network's gradients do not explode or saturate inadvertently. As such, we chose this refinement loss function to make it less sensitive to outliers than a mean squared error (MSE) criterion and for it to be smoother near zero compared to a mean absolute error criterion.</p><p>Positional reconstruction. Immediately before training, we inject random Gaussian noise into the initial state of our node positions. This corresponds to a small corruption of the location of each atom in an input protein. This methodology follows closely after the Noisy Nodes methodology from <ref type="bibr" target="#b60">[61]</ref> and, as such, may facilitate better learning over the manifold of ground truth structures. We study the behavior of the EGR model with and without positional corruption in Section A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Assessing structure quality</head><p>Following refinement of initial node coordinates, the EGR model also predicts new embeddings for each node, H. Intuitively, we consider H to be the model's knowledge of its predicted molecular structure as defined by X . As such, using H, we train an additional head of the network to predict an estimate of the structural accuracy of X .</p><p>Quality assessment loss. We formulate assessment of a predicted structure's quality as a node regression task. Under this framework, we employ an MSE loss to train the second head of the network to predict the carbon-alpha (C?) atom local Distance Difference Test (LDDT-C?) value <ref type="bibr" target="#b61">[62]</ref> of each residue in the complex graph. Formally, the EGR model's loss for quality assessment (i.e., QA loss) is:</p><formula xml:id="formula_5">L QA = 1 |C| i?C q i -q * i 2 ,<label>(6)</label></formula><p>where q i is the model's predicted LDDT-C? for node i, q * i is the ground truth LDDT-C? for node i, and C is the set of C? atom indices in the input graph. As demonstrated by <ref type="bibr" target="#b13">[14]</ref>, models trained to predict such a quantity can produce reasonable accuracy estimates for predicted structures, taking one step towards making the EGR model more interpretable for its users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Learning paradigm</head><p>As mentioned previously, not all atoms in an input decoy structure have a corresponding position in the ground truth structure. Moreover, since during training we can only calculate the ground truth LDDT labels for the input graph's C? atoms, the EGR model jointly learns to predict refined structures and LDDT-C? scores in a semi-supervised end-to-end manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data</head><p>For the task of complex structure refinement and assessment, we introduce new Protein Structure Refinement (PSR) datasets comprised of 46,174 homomeric and heteromeric protein complex structural decoys, corresponding to 3,498 unique protein targets. The proteins comprising these datasets (i.e., PSR-Dockground, PSR-DeepHomo, and PSR-EVCoupling) are originally derived from the Dockground <ref type="bibr" target="#b62">[63]</ref>, DeepHomo <ref type="bibr" target="#b63">[64]</ref>, and EVCoupling <ref type="bibr" target="#b64">[65]</ref> datasets, respectively. The Dockground dataset provides decoy structures of various structural error ranges for each protein target, so we directly include its 1-6 ? decoys and their ground truth structures in our PSR dataset. As the DeepHomo and EVCoupling datasets only provide ground truth structures for each of their protein targets, to generate each target's decoy structures for refinement, we subsequently use AlphaFold-Multimer <ref type="bibr" target="#b35">[36]</ref> to predict and select the best decoy structure for each of these datasets' protein targets. Using TM-score <ref type="bibr" target="#b65">[66]</ref>, we then uniquely superimpose all ground truth target structures corresponding to each decoy structure to ensure the coordinate systems of both decoy and target structures are aligned before model training. To support the reproducibility of our work, we make our PSR datasets publicly available along with standardized training, validation, and testing splits of the input proteins.</p><p>Motivation for introducing new datasets. To the best of our knowledge, our work is the first to introduce cross-validation datasets curated for structural refinement of protein complexes, datasets that we have intentionally designed to be amenable to machine learning methods. In constructing these datasets, we sought to make use of existing protein data sources to maximize their size and structural scope. Notably, since the average structural quality of AlphaFold-Multimer for DeepHomo and EVCoupling proteins is reasonably high, we included proteins from the Dockground dataset to diversify the overall quality of structures in our PSR dataset.</p><p>Overlap Reduction. Our constituent PSR datasets are initially combined using random populationproportionate sampling according to a protein's type and dataset of origin (i.e., Dockground-Heteromer, DeepHomo-Homomer, and EVCoupling-Heteromer). Using MMseqs2 <ref type="bibr" target="#b66">[67]</ref>, these proportionate splits are then filtered such that no split contains a protein chain with more than 30% sequence similarity to any chain in another split, across all datasets employed in this study. This filtering technique, among others, was chosen to satisfy multiple practical criteria for tasks in this domain: assurance that models (1) cross-validated on such splits do not overfit w.r.t. homology <ref type="bibr" target="#b67">[68]</ref> or structural similarity <ref type="bibr" target="#b68">[69]</ref> between proteins and (2) will be trained on a sufficient number of input proteins to enable generalization to unseen data.</p><p>Test Datasets. The first test dataset we use to evaluate models performing best on our PSR dataset's validation split is comprised of complexes originally held out from our PSR dataset. As such, this PSR test dataset is comprised of Dockground, DeepHomo, and EVCoupling proteins that have previously been filtered according to 30% sequence similarity. Following the work of <ref type="bibr" target="#b35">[36]</ref>, we then adopt the heteromeric Benchmark 2 dataset for blind evaluation of our model's generalization for the task of refinement. Additionally, to score our model's ability to estimate the quality of a protein complex, we curated a new M4S dataset for complex quality assessment. This dataset consists of randomly-sampled heteromeric protein complexes available in the Protein Data Bank (PDB), where each of the 11 selected complexes was first subjected to 30% sequence identity filtering w.r.t. all of our other cross-validation dataset splits, yielding 1,160 decoys across 11 targets for QA benchmarking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Setup</head><p>Baselines. Modeller <ref type="bibr" target="#b69">[70]</ref> is a classical refinement program for 3D protein structures, with support for modeling protein complexes in particular. We also include a version of EGR trained for only C? atom refinement and assessment (i.e., EGR-C?) to evaluate Modeller's ability to reconstruct all-atom structures from the output C? atom coordinates of EGR-C?. GalaxyRefineComplex <ref type="bibr" target="#b70">[71]</ref> is another popular refinement protocol, one specifically designed for refining structural interfaces between chains in a protein complex. Similarly, we also include GNNRefine <ref type="bibr" target="#b39">[40]</ref>, which uses GNN-based distance predictions to drive tertiary structural refinement with PyRosetta <ref type="bibr" target="#b71">[72]</ref>. We note that we are only able to include GalaxyRefineComplex and GNNRefine's results on our smaller Benchmark 2 test dataset, as their extraordinarily high refinement runtimes (e.g., 1,200 seconds per decoy with 16 CPU threads) prevent us from evaluating their performance in a reasonable amount of time on our full PSR test dataset consisting of over 5,000 complexes. For our remaining all-atom baselines, we selected two recent geometric deep learning methods with which to compare the EGR model's performance for all-atom refinement and QA. These methods include SE(3)-Transformers (SETs) <ref type="bibr" target="#b51">[52]</ref> and Steerable Equivariant Graph Neural Networks (SEGNNs) <ref type="bibr" target="#b52">[53]</ref>, both of which have previously been evaluated for their ability to model 3D molecular systems. We evaluate these models using default hyperparameters specified by the authors of each method. Regarding QA, besides including all geometric deep learning methods trained to predict per-residue LDDT-C? scores as baselines, we also compare EGR-AllAtom and EGR-C? to GNN_DOVE <ref type="bibr" target="#b72">[73]</ref>, a state-of-the-art (SOTA) graph deep learning method for all-atom protein complex structure QA.</p><p>EGR Models. In addition to our original EGR model (i.e., EGR-AllAtom), we include three separate versions of the EGR model where we individually remove node or edge-specific features from our input graphs during training and test time. These versions are the EGR model without positional corruption (i.e., EGR-AllAtom-NPC), the model without atom-wise protein surface proximities (i.e., EGR-AllAtom-NSP), and the model without edge-wise relative geometric features (i.e., EGR-AllAtom-NRGF). Such versions are listed in Appendix A to serve as ablation studies on the EGR model, to understand the relative importance of individual input features in our datasets.</p><p>Evaluation Metrics. For the task of refinement, for all relevant metrics we report the change in the metric's value (i.e., the ? metric) when comparing a model's refined structure and the original decoy structure. To evaluate the structural quality of a protein complex, we use the DockQ score <ref type="bibr" target="#b73">[74]</ref>, interface root mean squared deviation (iRMSD), ligand RMSD (LRMSD), fraction of decoy DockQ scores improved (FI-DockQ), and average percentage of decoy DockQ score improvement (API-DockQ). Here, iRMSD is the mean squared error (MSE) between the atoms of the predicted and ground truth inter-chain interaction regions, and LRMSD is the MSE between the atoms of the predicted and ground truth ligand chains, respectively. Additionally, F nat is the fraction of native contacts found in a decoy structure. Formally, DockQ score can be calculated using Equations 7 and 8, letting d 1 = 8.5 and d 2 = 1.5:</p><formula xml:id="formula_6">RM SD scaled (RM SD, d i ) = 1 1 + RM SD di 2 (7) DockQ = 1 3 (F nat + RM SD scaled (LRM SD, d 1 ) + RM SD scaled (iRM SD, d 2 )) .<label>(8)</label></formula><p>To evaluate a model's ranking ability, we use two common metrics, its Top-N hit rate and ranking loss <ref type="bibr" target="#b74">[75,</ref><ref type="bibr" target="#b4">5]</ref>. A model's hit rate is defined as the fraction of protein target complexes where the model ranked at least one acceptable or higher quality decoy within its predicted list of Top-N decoys. A hit rate is represented by three numbers separated by the character /. The three numbers, in order, represent how many decoys with acceptable or higher quality, medium or higher quality, and high quality were among the Top-N ranked decoys. In this work, we employ the Top-10 hit rate measure. Similarly, a per-target ranking loss is defined as the difference between the DockQ score of a target's native structure and the decoy for which the model predicted the highest structural quality. As such, a low ranking loss reflects a model's ability to successfully rank decoys for downstream tasks.</p><p>Implementation Details. We train all our models using the AdamW optimizer <ref type="bibr" target="#b75">[76]</ref> and perform early stopping with a patience of 50 epochs based on the average full-complex RMSD of a model's predictions on our PSR dataset's validation split. All hyperparameters and protein graph features employed are described in Appendix B. Source code to perform fast structural refinement and quality assessment using our provided model weights can be found at https://github.com/BioinfoMachineLearning/DeepRefine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>Blind Refinement. In Table <ref type="table" target="#tab_1">1</ref>, we see that the all-atom EGR model surpasses the refinement performance of all other baseline methods except on the low initial quality structures from Dockground. We observe that methods outperforming EGR on the Dockground targets also appear to underperform EGR on targets from datasets of higher initial structural quality, suggesting these methods are overfitting to low-quality structures in the PSR training dataset. EGR-AllAtom, however, appears to generalize better to all test datasets compared to any other method (accounting for the fact that GNNRefine may have seen overly-similar proteins during training), with a computational complexity (e.g., average of 5 seconds) orders of magnitude lower than that of methods such as GalaxyRefineComplex and GNNRefine (e.g., average of 1,200 and 600 seconds, respectively).</p><p>Blind Quality Assessment.    the state-of-the-art QA predictor for protein complexes, on a large collection of decoy structures. Interestingly, we also find that modeling protein complexes at the granularity of C? atoms leads to EGR-C?-Modeller achieving new SOTA results. This suggests interesting avenues for future research into molecular modeling for structural QA. Nonetheless, within a single model, EGR-AllAtom can outperform all other methods for QA while simultaneously achieving SOTA performance for refinement, demonstrating the complementarity of the two tasks in the context of all-atom modeling.</p><p>Visualizations. We display in Figure <ref type="figure" target="#fig_2">3</ref> a successful example of a PSR test protein other refinement methods (e.g., Modeller) cannot refine but that EGR nonetheless can refine atomistically.</p><p>Limitations. Although EGR has driven significant progress in protein complex structure refinement and assessment, the model is currently only able to improve the structure of inter-chain interfaces marginally on average due partly to its relatively small training dataset size and partly to limited computing resources available for generating sufficiently large datasets. As such, we conjecture that new large datasets comprised of medium and high-quality complex decoys are required for refinement methods to better generalize their predictions to higher-quality structures. We leave an exploration of such ideas for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We presented EGR which introduces an E(3)-equivariant method for refining and assessing the quality of 3D protein complex structures. Our experiments validate the effectiveness of EGR for structural refinement and assessment using a diverse, open-source collection of protein complexes and establish a baseline for future studies in geometric deep learning for protein structure refinement and analysis. Used with care and with feedback from domain experts, we believe EGR could encourage the use of DL models in responsible drug discovery and development. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Additional Results</head><p>In Table <ref type="table" target="#tab_4">4</ref>, we investigate the effect of removing individual components or input features from the EGR model. We then compare these results to the baseline EGR model to assess these components' relative importance on generalization and model accuracy. Similarly, in addition to iRMSD, LRMSD, FI-DockQ, and API-DockQ, we can also inspect the fraction of native and non-native inter-chain contacts present in a structural decoy for a protein target. Such measures can yield insights into how the interface between two or more chains in a complex was formed and whether or not the interface is near-native. We show the results of these measures in Table <ref type="table">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Implementation Details</head><p>Featurization. In each all-atom protein complex graph, we include as a node feature an atom's type using a 38-dimensional one-hot encoding vector for each atom. Then, to add a new node feature representing the normalized proximity of an atom to the surface of the protein chain to which it belongs, using MSMS <ref type="bibr" target="#b76">[77]</ref> we take the complement of each atom's chain-wise buriedness to derive atomic surface proximities for surface contact modeling <ref type="bibr" target="#b34">[35]</ref>. Notably, for the EGR-C?-Modeller model, its input graphs are comprised of C? atoms as nodes rather than all available atoms. In such C? atom graphs, we also include node features describing each residue's dihedral angles <ref type="bibr" target="#b25">[26]</ref>. Lastly, we note that in our experiments with the SEGNN, for this model we included an additional type-1 (i.e., vector-valued) node feature describing the coordinates-wise displacement between a given node's position and the mean atomic coordinates of the protein complex.</p><p>For our graphs' edge features, we start by adding a binary feature indicating whether or not a pair of connected atoms belong to the same chain. We then add an edge-wise sinusoidal positional encoding of the difference between the source and destination node's indices in the input graph. As another edge feature, we include a binary value indicating whether or not a pair of atoms are connected by a covalent bond. Our final type-0 edge feature describes the relative geometric features such as distance, direction, and orientation between the local coordinate systems representing the residues</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An overview of the structure refinement and assessment problem addressed by EGR.</figDesc><graphic url="image-1.png" coords="2,108.00,72.00,396.00,124.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: EGR model architecture.</figDesc><graphic url="image-2.png" coords="4,108.00,72.00,396.01,93.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: A cherry-picked example of a test protein complex (PDB ID: 6GS2) successfully refined by EGR. Note in all the above subfigures the ground truth structure is highlighted in the color SLATE.</figDesc><graphic url="image-3.png" coords="9,119.37,286.63,122.76,81.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2</head><label>2</label><figDesc></figDesc><table /><note><p>summarizes a few key findings regarding models' QA performance. The first is that both EGR-AllAtom and EGR-C?-Modeller outperform GNN_DOVE,</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Performance of different refinement methods on each test dataset.</figDesc><table><row><cell>?Metric</cell><cell>DockQ ?</cell><cell>iRMSD ?</cell><cell>LRMSD ?</cell><cell>FI-DockQ ?</cell><cell>API-DockQ ?</cell></row><row><cell></cell><cell></cell><cell></cell><cell>PSR-Dockground (4,799)</cell><cell></cell><cell></cell></row><row><cell>Modeller</cell><cell>+0.0002</cell><cell>-0.6331</cell><cell>-1.0027</cell><cell>63.03%</cell><cell>0.32%</cell></row><row><cell>EGR-C?-Modeller</cell><cell>+0.0053 ? 0.0011</cell><cell>-1.2285 ? 0.0330</cell><cell>-3.5226 ? 0.3125</cell><cell>79.30% ? 0.93%</cell><cell>0.89% ? 0.15%</cell></row><row><cell>SET-AllAtom</cell><cell>+0.0132 ? 0.0040</cell><cell>-0.8808 ? 0.1158</cell><cell>-1.6478 ? 0.1047</cell><cell>84.90% ? 1.13%</cell><cell>1.69% ? 0.35%</cell></row><row><cell>SEGNN-AllAtom</cell><cell>+0.0144 ? 0.0024</cell><cell>-2.4562 ? 0.0499</cell><cell>-6.6603 ? 0.6702</cell><cell>94.46% ? 0.60%</cell><cell>1.89% ? 0.29%</cell></row><row><cell>EGR-AllAtom</cell><cell>+0.0097 ? 0.0002</cell><cell>-0.6274 ? 0.0669</cell><cell>-2.5561 ? 0.1584</cell><cell>83.66% ? 0.49%</cell><cell>1.59% ? 0.11%</cell></row><row><cell></cell><cell></cell><cell></cell><cell>PSR-DeepHomo (376)</cell><cell></cell><cell></cell></row><row><cell>Modeller</cell><cell>-0.2465</cell><cell>+1.5912</cell><cell>+5.3457</cell><cell>8.24%</cell><cell>0.53%</cell></row><row><cell>EGR-C?-Modeller</cell><cell>-0.2796 ? 0.0055</cell><cell>+2.2075 ? 0.0839</cell><cell>+6.1711 ? 0.1842</cell><cell>8.16% ? 0.76%</cell><cell>1.17% ? 0.18%</cell></row><row><cell>SET-AllAtom</cell><cell>-0.0034 ? 0.0003</cell><cell>+0.0275 ? 0.0050</cell><cell>+0.0273 ? 0.0104</cell><cell>27.39% ? 4.36%</cell><cell>0.20% ? 0.08%</cell></row><row><cell>SEGNN-AllAtom</cell><cell>-0.0468 ? 0.0091</cell><cell>+0.2950 ? 0.0741</cell><cell>+0.3593 ? 0.1722</cell><cell>16.31% ? 3.54%</cell><cell>0.87% ? 0.20%</cell></row><row><cell>EGR-AllAtom</cell><cell>-0.0006 ? 0.0018</cell><cell>+0.0121 ? 0.0054</cell><cell>+0.0013 ? 0.0028</cell><cell>45.12% ? 6.99%</cell><cell>0.41% ? 0.03%</cell></row><row><cell></cell><cell></cell><cell></cell><cell>PSR-EVCoupling (195)</cell><cell></cell><cell></cell></row><row><cell>Modeller</cell><cell>-0.1738</cell><cell>+1.1467</cell><cell>+4.9877</cell><cell>7.18%</cell><cell>0.74%</cell></row><row><cell>EGR-C?-Modeller</cell><cell>-0.2150 ? 0.0073</cell><cell>+1.9651 ? 0.0647</cell><cell>+5.8477 ? 0.7759</cell><cell>9.91% ? 1.74%</cell><cell>1.49% ? 0.37%</cell></row><row><cell>SET-AllAtom</cell><cell>-0.0016 ? 0.0002</cell><cell>+0.0149 ? 0.0007</cell><cell>+0.0108 ? 0.0040</cell><cell>27.86% ? 5.24%</cell><cell>0.31% ? 0.11%</cell></row><row><cell>SEGNN-AllAtom</cell><cell>-0.0250 ? 0.0069</cell><cell>+0.1646 ? 0.0633</cell><cell>+0.2400 ? 0.1044</cell><cell>18.29% ? 3.41%</cell><cell>0.89% ? 0.18%</cell></row><row><cell>EGR-AllAtom</cell><cell>+0.0010 ? 0.0010</cell><cell>+0.0026 ? 0.0031</cell><cell>-0.0059 ? 0.0017</cell><cell>43.93% ? 5.00%</cell><cell>0.48% ? 0.03%</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Benchmark 2 (17)</cell><cell></cell><cell></cell></row><row><cell>Modeller</cell><cell>-0.1855</cell><cell>+0.7939</cell><cell>+3.0277</cell><cell>5.88%</cell><cell>0.60%</cell></row><row><cell>GalaxyRefineComplex</cell><cell>-0.0074</cell><cell>+0.0778</cell><cell>-0.0246</cell><cell>22.22%</cell><cell>2.12%</cell></row><row><cell>GNNRefine</cell><cell>+0.0025</cell><cell>+0.0226</cell><cell>+0.0602</cell><cell>47.06%</cell><cell>1.26%</cell></row><row><cell>EGR-C?-Modeller</cell><cell>-0.2644 ? 0.0437</cell><cell>+2.118 ? 0.7832</cell><cell>+5.9196 ? 1.8589</cell><cell>15.69% ? 2.77%</cell><cell>1.28% ? 0.84%</cell></row><row><cell>SET-AllAtom</cell><cell>-0.0078 ? 0.0015</cell><cell>+0.0729 ? 0.0186</cell><cell>+0.0469 ? 0.0114</cell><cell>29.63% ? 2.62%</cell><cell>0.33% ? 0.14%</cell></row><row><cell>SEGNN-AllAtom</cell><cell>-0.0328 ? 0.0062</cell><cell>+0.0807 ? 0.0790</cell><cell>+0.0781 ? 0.1371</cell><cell>31.37% ? 5.54%</cell><cell>1.24% ? 0.59%</cell></row><row><cell>EGR-AllAtom</cell><cell>-0.0010 ? 0.0028</cell><cell>-0.0002 ? 0.003</cell><cell>-0.0121 ? 0.0021</cell><cell>43.14% ? 10.00%</cell><cell>0.59% ? 0.08%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Hit rate performance of different QA methods on the M4S test dataset.</figDesc><table><row><cell>ID</cell><cell>EGR-C?-Modeller</cell><cell>SET-AllAtom</cell><cell>SEGNN-AllAtom</cell><cell>EGR-AllAtom</cell><cell>GNN_DOVE</cell><cell>Top-10 Best</cell></row><row><cell>7AOH</cell><cell>10/10/6</cell><cell>9/8/6</cell><cell>9/9/9</cell><cell>9/9/9</cell><cell>9/9/0</cell><cell>10/10/10</cell></row><row><cell>7D7F</cell><cell>0/0/0</cell><cell>2/0/0</cell><cell>0/0/0</cell><cell>0/0/0</cell><cell>0/0/0</cell><cell>5/0/0</cell></row><row><cell>7AMV</cell><cell>10/10/8</cell><cell>10/10/5</cell><cell>10/10/9</cell><cell>10/10/5</cell><cell>10/10/6</cell><cell>10/10/10</cell></row><row><cell>7OEL</cell><cell>10/10/0</cell><cell>10/10/0</cell><cell>10/9/0</cell><cell>10/9/0</cell><cell>10/10/0</cell><cell>10/10/0</cell></row><row><cell>7O28</cell><cell>10/10/0</cell><cell>10/10/0</cell><cell>10/10/0</cell><cell>10/10/0</cell><cell>10/10/0</cell><cell>10/10/0</cell></row><row><cell>7MRW</cell><cell>6/5/0</cell><cell>0/0/0</cell><cell>0/0/0</cell><cell>0/0/0</cell><cell>0/0/0</cell><cell>10/10/0</cell></row><row><cell>7D3Y</cell><cell>0/0/0</cell><cell>0/0/0</cell><cell>0/0/0</cell><cell>1/0/0</cell><cell>0/0/0</cell><cell>10/0/0</cell></row><row><cell>7NKZ</cell><cell>10/10/9</cell><cell>10/9/9</cell><cell>10/10/3</cell><cell>10/9/9</cell><cell>10/9/9</cell><cell>10/10/10</cell></row><row><cell>7LXT</cell><cell>10/10/0</cell><cell>4/3/0</cell><cell>6/5/0</cell><cell>8/7/0</cell><cell>1/0/0</cell><cell>10/10/0</cell></row><row><cell>7KBR</cell><cell>10/10/10</cell><cell>10/10/10</cell><cell>10/10/10</cell><cell>10/10/9</cell><cell>10/10/9</cell><cell>10/10/10</cell></row><row><cell>7O27</cell><cell>10/5/0</cell><cell>10/7/0</cell><cell>10/6/0</cell><cell>10/4/0</cell><cell>10/4/0</cell><cell>10/10/0</cell></row><row><cell>Summary</cell><cell>9/9/4</cell><cell>9/8/4</cell><cell>8/8/4</cell><cell>9/8/4</cell><cell>8/7/3</cell><cell>11/9/4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Ranking loss of different QA methods on the M4S test dataset.</figDesc><table><row><cell>ID</cell><cell>EGR-C?-Modeller</cell><cell>SET-AllAtom</cell><cell>SEGNN-AllAtom</cell><cell>EGR-AllAtom</cell><cell>GNN_DOVE</cell></row><row><cell>7AOH</cell><cell>0.0610</cell><cell>0.9280</cell><cell>0.9280</cell><cell>0.0350</cell><cell>0.9280</cell></row><row><cell>7D7F</cell><cell>0.4700</cell><cell>0.4700</cell><cell>0.4710</cell><cell>0.4590</cell><cell>0.0030</cell></row><row><cell>7AMV</cell><cell>0.1730</cell><cell>0.3420</cell><cell>0.0130</cell><cell>0.3420</cell><cell>0.3420</cell></row><row><cell>7OEL</cell><cell>0.2100</cell><cell>0.2100</cell><cell>0.3790</cell><cell>0.2100</cell><cell>0.2100</cell></row><row><cell>7O28</cell><cell>0.2330</cell><cell>0.0240</cell><cell>0.2740</cell><cell>0.2440</cell><cell>0.2440</cell></row><row><cell>7MRW</cell><cell>0.6000</cell><cell>0.5550</cell><cell>0.6030</cell><cell>0.5550</cell><cell>0.5980</cell></row><row><cell>7D3Y</cell><cell>0.3240</cell><cell>0.2950</cell><cell>0.1740</cell><cell>0.2950</cell><cell>0.2950</cell></row><row><cell>7NKZ</cell><cell>0.0220</cell><cell>0.1100</cell><cell>0.1830</cell><cell>0.4590</cell><cell>0.4590</cell></row><row><cell>7LXT</cell><cell>0.0500</cell><cell>0.2950</cell><cell>0.2950</cell><cell>0.3890</cell><cell>0.2950</cell></row><row><cell>7KBR</cell><cell>0.1700</cell><cell>0.1520</cell><cell>0.0520</cell><cell>0.1520</cell><cell>0.0680</cell></row><row><cell>7O27</cell><cell>0.3340</cell><cell>0.3340</cell><cell>0.3650</cell><cell>0.3180</cell><cell>0.3340</cell></row><row><cell>Summary</cell><cell>0.2406 ? 0.1801</cell><cell>0.3377 ? 0.2486</cell><cell>0.3397 ? 0.2613</cell><cell>0.3144 ? 0.1506</cell><cell>0.3432 ? 0.2538</cell></row><row><cell cols="2">Original Decoy</cell><cell cols="2">Modeller-Refined Decoy</cell><cell cols="2">EGR-Refined Decoy</cell></row><row><cell cols="2">RMSD = 8.715</cell><cell cols="2">RMSD = 10.131</cell><cell cols="2">RMSD = 8.636</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Ablation studies on the EGR model using each test dataset.</figDesc><table><row><cell>?Metric</cell><cell>DockQ ?</cell><cell>iRMSD ?</cell><cell>LRMSD ?</cell><cell>FI-DockQ ?</cell><cell>API-DockQ ?</cell></row><row><cell></cell><cell></cell><cell></cell><cell>PSR-Dockground (4,799)</cell><cell></cell><cell></cell></row><row><cell>EGR-AllAtom-NPC</cell><cell>+0.0083 ? 0.0006</cell><cell>-0.6549 ? 0.1589</cell><cell>-2.5129 ? 0.1584</cell><cell>82.95% ? 1.98%</cell><cell>1.46% ? 0.09%</cell></row><row><cell>EGR-AllAtom-NSP</cell><cell>+0.0048 ? 0.0039</cell><cell>-0.4881 ? 0.1622</cell><cell>-1.5799 ? 0.5891</cell><cell>74.15% ? 1.77%</cell><cell>1.46% ? 0.09%</cell></row><row><cell>EGR-AllAtom-NRGF</cell><cell>+0.0035 ? 0.0018</cell><cell>-0.4817 ? 0.0470</cell><cell>-0.8711 ? 0.2320</cell><cell>73.61% ? 2.39%</cell><cell>0.71% ? 0.19%</cell></row><row><cell>EGR-AllAtom</cell><cell>+0.0097 ? 0.0002</cell><cell>-0.6274 ? 0.0669</cell><cell>-2.5561 ? 0.2893</cell><cell>83.66% ? 0.49%</cell><cell>1.59% ? 0.11%</cell></row><row><cell></cell><cell></cell><cell></cell><cell>PSR-DeepHomo (376)</cell><cell></cell><cell></cell></row><row><cell>EGR-AllAtom-NPC</cell><cell>-0.0011 ? 0.0019</cell><cell>+0.0044 ? 0.0021</cell><cell>-0.0003 ? 0.0010</cell><cell>36.08% ? 9.57%</cell><cell>0.30% ? 0.08%</cell></row><row><cell>EGR-AllAtom-NSP</cell><cell>0.0000 ? 0.0015</cell><cell>+0.0088 ? 0.0041</cell><cell>+0.0008 ? 0.0015</cell><cell>46.10% ? 11.47%</cell><cell>0.39% ? 0.10%</cell></row><row><cell>EGR-AllAtom-NRGF</cell><cell>-0.1296 ? 0.0276</cell><cell>+0.7784 ? 0.1660</cell><cell>+0.5925 ? 0.1625</cell><cell>10.28% ? 3.83%</cell><cell>1.11% ? 0.16%</cell></row><row><cell>EGR-AllAtom</cell><cell>-0.0006 ? 0.0018</cell><cell>+0.0121 ? 0.0054</cell><cell>+0.0013 ? 0.0028</cell><cell>45.12% ? 6.99%</cell><cell>0.41% ? 0.03%</cell></row><row><cell></cell><cell></cell><cell></cell><cell>PSR-EVCoupling (195)</cell><cell></cell><cell></cell></row><row><cell>EGR-AllAtom-NPC</cell><cell>-0.0003 ? 0.0014</cell><cell>+0.0007 ? 0.0009</cell><cell>-0.0041 ? 0.0012</cell><cell>33.16% ? 10.60%</cell><cell>0.31% ? 0.08%</cell></row><row><cell>EGR-AllAtom-NSP</cell><cell>+0.0007 ? 0.0013</cell><cell>+0.0021 ? 0.0016</cell><cell>-0.0057 ? 0.0020</cell><cell>39.66% ? 9.02%</cell><cell>0.39% ? 0.13%</cell></row><row><cell>EGR-AllAtom-NRGF</cell><cell>-0.0743 ? 0.0183</cell><cell>+0.4843 ? 0.1084</cell><cell>+0.3218 ? 0.1003</cell><cell>11.11% ? 4.48%</cell><cell>1.07% ? 0.18%</cell></row><row><cell>EGR-AllAtom</cell><cell>+0.0010 ? 0.0010</cell><cell>+0.0026 ? 0.0031</cell><cell>-0.0059 ? 0.0017</cell><cell>43.93% ? 5.00%</cell><cell>0.48% ? 0.03%</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Benchmark 2 (17)</cell><cell></cell><cell></cell></row><row><cell>EGR-AllAtom-NPC</cell><cell>-0.0006 ? 0.0014</cell><cell>-0.0038 ? 0.0008</cell><cell>-0.0127 ? 0.0067</cell><cell>41.18% ? 12.71%</cell><cell>0.26% ? 0.20%</cell></row><row><cell>EGR-AllAtom-NSP</cell><cell>-0.0007 ? 0.0030</cell><cell>-0.0049 ? 0.0016</cell><cell>-0.0185 ? 0.0020</cell><cell>41.18% ? 12.71%</cell><cell>0.51% ? 0.18%</cell></row><row><cell>EGR-AllAtom-NRGF</cell><cell>-0.0070 ? 0.0054</cell><cell>+0.0010 ? 0.0027</cell><cell>-0.0126 ? 0.0004</cell><cell>27.45% ? 7.34%</cell><cell>0.22% ? 0.14%</cell></row><row><cell>EGR-AllAtom</cell><cell>-0.0010 ? 0.0028</cell><cell>-0.0002 ? 0.003</cell><cell>-0.0121 ? 0.0021</cell><cell>43.14% ? 10.00%</cell><cell>0.59% ? 0.08%</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgments</head><p>The project is partially supported by two NSF grants (DBI 1759934 and IIS 1763246), one NIH grant (GM093123), three DOE grants (DE-SC0020400, DE-AR0001213, and DE-SC0021303), and the computing allocation on the Summit compute cluster provided by Oak Ridge Leadership Computing Facility (Contract No. DE-AC05-00OR22725).</p><p>corresponding to an atom pair, following <ref type="bibr" target="#b77">[78,</ref><ref type="bibr" target="#b13">14]</ref>. Lastly, in our SEGNN experiments, we also included an additional type-1 edge feature describing the relative coordinates-wise displacement between a connected pair of atoms.</p><p>Hardware Used. The Oak Ridge Leadership Facility (OLCF) at the Oak Ridge National Laboratory (ORNL) is an open science computing facility that supports HPC research. The OLCF houses the Summit compute cluster. Summit, launched in 2018, delivers 8 times the computational performance of Titan's 18,688 nodes, using only 4,608 nodes. Like Titan, Summit has a hybrid architecture, and each node contains multiple IBM POWER9 CPUs and NVIDIA Volta GPUs all connected with NVIDIA's high-speed NVLink. Each node has over half a terabyte of coherent memory (high bandwidth memory + DDR4) addressable by all CPUs and GPUs plus 800GB of non-volatile RAM that can be used as a burst buffer or as extended memory. To provide a high rate of I/O throughput, the nodes are connected in a non-blocking fat-tree using a dual-rail Mellanox EDR InfiniBand interconnect. We used the Summit compute cluster to train all our models. Software Used. We used Python 3.8 <ref type="bibr" target="#b78">[79]</ref>, PyTorch 1.10.0 <ref type="bibr" target="#b79">[80]</ref>, PyTorch Lightning 1.5.10 <ref type="bibr" target="#b80">[81]</ref>, PyTorch Geometric 2.0.4 <ref type="bibr" target="#b81">[82]</ref>, and DGL 0.8 <ref type="bibr" target="#b82">[83]</ref> to run our deep learning experiments. PyTorch Lightning was used to facilitate model checkpointing, metrics reporting, and distributed data parallelism across 144 Tesla V100 GPUs. A more in-depth description of the software environment used to run inference with our models can be found at https://github.com/BioinfoMachineLearning/DeepRefine. Further hyperparameters. We use a learning rate of 10 -4 for all EGR models. The learning rate is kept constant throughout each model's training. Models with the lowest RMSD on our validation data split are then tested on all our sequence-filtered test splits. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Protein complexes and functional modules in molecular networks</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Spirin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonid</forename><forename type="middle">A</forename><surname>Mirny</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.2032324100</idno>
		<ptr target="https://www.pnas.org/doi/abs/10.1073/pnas.2032324100" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="12123" to="12128" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The interface of protein-protein complexes: analysis of contacts and prediction of interactions</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Bahadur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zacharias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cellular and Molecular Life Sciences</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1059" to="1072" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mass spectrometry supported determination of protein complex structure</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Walzthoeni</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.sbi.2013.02.008</idno>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S0959440X13000377" />
	</analytic>
	<monogr>
		<title level="j">Current Opinion in Structural Biology</title>
		<idno type="ISSN">0959-440</idno>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="252" to="260" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>Theory and simulation / Macromolecular assemblies</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Computational methods in drug discovery</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sumudu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Leelananda</surname></persName>
		</author>
		<author>
			<persName><surname>Lindert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Beilstein journal of organic chemistry</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2694" to="2718" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Prediction of protein assemblies, the next frontier: The CASP14-CAPRI experiment</title>
		<author>
			<persName><surname>Marc F Lensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins: Structure, Function, and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="1800" to="1823" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">High-Performance Deep Learning Toolbox for Genome-Scale Prediction of Protein Structure and Function</title>
		<author>
			<persName><forename type="first">Mu</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/ACM Workshop on Machine Learning in High Performance Computing Environments (MLHPC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="46" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">DIPS-Plus: The Enhanced Database of Interacting Protein Structures for Interface Prediction</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Morehead</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04362</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Geometric Transformers for Protein Interface Contact Prediction</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Morehead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianlin</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.02423</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">3D-equivariant graph neural networks for protein model quality assessment</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The prospects and opportunities of protein structure prediction with AI</title>
		<author>
			<persName><forename type="first">Kathryn</forename><surname>Tunyasuvunakool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Molecular Cell Biology</title>
		<imprint>
			<biblScope unit="page" from="1" to="2" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Benchmarking of structure refinement methods for protein complex models</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Verburgt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daisuke</forename><surname>Kihara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins: Structure, Function, and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="83" to="95" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">E (n) equivariant graph neural networks</title>
		<author>
			<persName><forename type="first">Garcia</forename><surname>V?ctor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emiel</forename><surname>Satorras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Hoogeboom</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno>PMLR. 2021</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="9323" to="9332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Overcoming barriers to membrane protein structure determination</title>
		<author>
			<persName><forename type="first">Roslyn</forename><forename type="middle">M</forename><surname>Bill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature biotechnology</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="335" to="340" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Highly accurate protein structure prediction with AlphaFold</title>
		<author>
			<persName><forename type="first">John</forename><surname>Jumper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">596</biblScope>
			<biblScope unit="page" from="583" to="589" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Accurate prediction of protein structures and interactions using a three-track neural network</title>
		<author>
			<persName><forename type="first">Minkyung</forename><surname>Baek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">373</biblScope>
			<biblScope unit="page" from="871" to="876" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">AlphaFold Protein Structure Database: Massively expanding the structural coverage of protein-sequence space with high-accuracy models</title>
		<author>
			<persName><forename type="first">Mihaly</forename><surname>Varadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="439" to="D444" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">AlphaFold2 and the future of structural biology</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Cramer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Structural &amp; Molecular Biology</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="704" to="705" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Geometric deep learning of RNA structure</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Raphael</surname></persName>
		</author>
		<author>
			<persName><surname>Townshend</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.abe5650</idno>
		<ptr target="https://www.science.org/doi/abs/10.1126/science.abe5650" />
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">373</biblScope>
			<biblScope unit="page" from="1047" to="1051" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Equibind: Geometric deep learning for drug binding structure prediction</title>
		<author>
			<persName><forename type="first">Hannes</forename><surname>St?rk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.05146</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Transformer protein language models are unsupervised structure learners</title>
		<author>
			<persName><forename type="first">Roshan</forename><surname>Rao</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=fylclEqgvgd" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations. 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-Scale Representation Learning on Proteins</title>
		<author>
			<persName><forename type="first">Ram</forename><surname>Vignesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charlotte</forename><surname>Somnath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Bunne</surname></persName>
		</author>
		<author>
			<persName><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Intrinsic-Extrinsic Convolution and Pooling for Learning on 3D Protein Structures</title>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Hermosilla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.06252</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deciphering interaction fingerprints from protein molecular surfaces using geometric deep learning</title>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Gainza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="184" to="192" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fast end-to-end learning on protein surfaces</title>
		<author>
			<persName><forename type="first">Freyr</forename><surname>Sverrisson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="15272" to="15281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning from protein structure with geometric vector perceptrons</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Bowen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.01411</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Iterative refinement graph neural network for antibody sequence-structure co-design</title>
		<author>
			<persName><forename type="first">Jin</forename><surname>Wengong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.04624</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Structure-based protein function prediction using graph convolutional networks</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Gligorijevi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Protein model quality assessment using rotation-equivariant, hierarchical neural networks</title>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Eismann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.13557</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Protein interaction interface region prediction by geometric deep learning</title>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Bailey-Kellogg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="2580" to="2588" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">AlphaFold and implications for intrinsically disordered proteins</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kiersten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohit V</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName><surname>Pappu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Molecular Biology</title>
		<imprint>
			<biblScope unit="volume">433</biblScope>
			<biblScope unit="page">167208</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">AlphaFold Accelerates Artificial Intelligence Powered Drug Discovery: Efficient Discovery of a Novel Cyclin-dependent Kinase 20 (CDK20) Small Molecule Inhibitor</title>
		<author>
			<persName><forename type="first">Feng</forename><surname>Ren</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.09647</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning physics confers pose-sensitivity in structure-based virtual screening</title>
		<author>
			<persName><forename type="first">Pawel</forename><surname>Gniewek</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.15459</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Structure prediction drives materials discovery</title>
		<author>
			<persName><surname>Artem R Oganov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Materials</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="331" to="348" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Physics-informed deep neural network for rigid-body protein docking</title>
		<author>
			<persName><forename type="first">Freyr</forename><surname>Sverrisson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR2022 Machine Learning for Drug Discovery</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Independent SE (3)-Equivariant Models for End-to-End Rigid Protein Docking</title>
		<author>
			<persName><forename type="first">Octavian-Eugen</forename><surname>Ganea</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.07786</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Protein complex prediction with AlphaFold-Multimer</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BioRxiv</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Side-chain Packing Using SE (3)-Transformer</title>
		<author>
			<persName><forename type="first">Akhil</forename><surname>Jindal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PACIFIC SYMPOSIUM ON BIOCOMPUTING 2022</title>
		<imprint>
			<publisher>World Scientific</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="46" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Improved protein structure refinement guided by deep learning based accuracy estimation</title>
		<author>
			<persName><forename type="first">Naozumi</forename><surname>Hiranuma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">DeepRefiner: highaccuracy protein structure refinement by deep network calibration</title>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Md Hossain Shuvo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debswapna</forename><surname>Gulfam</surname></persName>
		</author>
		<author>
			<persName><surname>Bhattacharya</surname></persName>
		</author>
		<idno type="DOI">10.1093/nar/gkab361</idno>
		<ptr target="https://doi.org/10.1093/nar/gkab361" />
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Research</title>
		<idno type="ISSN">0305-1048</idno>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="147" to="W152" />
			<date type="published" when="2021-05">May 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Fast and effective protein model refinement using deep graph neural networks</title>
		<author>
			<persName><forename type="first">Xiaoyang</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinbo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Computational Science</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="462" to="469" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Atomic protein structure refinement using all-atom graph representations and SE(3)-equivariant graph neural networks</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianlin</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="DOI">10.1101/2022.05.06.490934</idno>
		<ptr target="https://www.biorxiv.org/content/early/2022/05/06/2022.05.06.490934" />
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">DISTEMA: distance map-based estimation of single protein model accuracy with attentive 2D convolutional neural network</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianlin</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Protein model quality assessment using 3D oriented convolutional neural networks</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Pag?s</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Charmettant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergei</forename><surname>Grudinin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="3313" to="3319" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">ProteinGCN: Protein model quality assessment using graph convolutional networks</title>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Sanyal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BioRxiv</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">GraphQA: protein model quality assessment using graph convolutional networks</title>
		<author>
			<persName><forename type="first">Federico</forename><surname>Baldassarre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="360" to="366" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Relational inductive biases, deep learning, and graph networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><surname>Battaglia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Learning inductive biases with simple neural networks</title>
		<author>
			<persName><forename type="first">Reuben</forename><surname>Feinman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brenden</surname></persName>
		</author>
		<author>
			<persName><surname>Lake</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.02745</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">In search of the real inductive bias: On the role of implicit regularization in deep learning</title>
		<author>
			<persName><forename type="first">Ryota</forename><surname>Behnam Neyshabur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Tomioka</surname></persName>
		</author>
		<author>
			<persName><surname>Srebro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6614</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Rotation equivariant vector field networks</title>
		<author>
			<persName><forename type="first">Diego</forename><surname>Marcos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5048" to="5057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds</title>
		<author>
			<persName><forename type="first">Nathaniel</forename><surname>Thomas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08219</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Group equivariant convolutional networks</title>
		<author>
			<persName><forename type="first">Taco</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<idno>PMLR. 2016</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<biblScope unit="page" from="2990" to="2999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Se (3)-transformers: 3d roto-translation equivariant attention networks</title>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Fuchs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1970" to="1981" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Geometric and Physical Quantities improve E(3) Equivariant Message Passing</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Brandstetter</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=_xwr8gOBeV1" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">E (3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials</title>
		<author>
			<persName><forename type="first">Simon</forename><surname>Batzner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Efficient Equivariant Network</title>
		<author>
			<persName><forename type="first">Lingshen</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Generalization capabilities of translationally equivariant neural networks</title>
		<author>
			<persName><forename type="first">Srinath</forename><surname>Bulusu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review D</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">74504</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Factorized Attention: Self-Attention with Linear Complexities</title>
		<author>
			<persName><forename type="first">Zhuoran</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.01243</idno>
		<ptr target="http://arxiv.org/abs/1812.01243" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Interpretable learning for self-driving cars by visualizing causal attention</title>
		<author>
			<persName><forename type="first">Jinkyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2942" to="2950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<author>
			<persName><forename type="first">Phil</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://github.com/lucidrains/linear-attention-transformer.2021" />
		<title level="m">Linear Attention Transformer</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">An alternative probabilistic interpretation of the huber loss</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gregory</surname></persName>
		</author>
		<author>
			<persName><surname>Meyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ieee/cvf conference on computer vision and pattern recognition</title>
		<meeting>the ieee/cvf conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5261" to="5269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Very deep graph neural networks via noise regularisation</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Godwin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.07971</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">lDDT: a local superposition-free score for comparing protein structures and models using distance difference tests</title>
		<author>
			<persName><forename type="first">Valerio</forename><surname>Mariani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2722" to="2728" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Dockground: a comprehensive data resource for modeling of protein complexes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Petras</surname></persName>
		</author>
		<author>
			<persName><surname>Kundrotas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Protein Science</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="172" to="181" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Accurate prediction of inter-protein residue-residue contacts for homo-oligomeric protein complexes</title>
		<author>
			<persName><forename type="first">Yumeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng-You</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Briefings in bioinformatics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">38</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">The EVcouplings Python framework for coevolutionary sequence analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><surname>Hopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1582" to="1584" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Scoring function for automated assessment of protein structure template quality</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Skolnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins: Structure, Function, and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="702" to="710" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">MMseqs2 enables sensitive protein sequence searching for the analysis of massive data sets</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Steinegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>S?ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature biotechnology</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1026" to="1028" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">On the relationship between sequence and structure similarities in proteomics</title>
		<author>
			<persName><forename type="first">Evgeny</forename><surname>Krissinel</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btm006</idno>
		<ptr target="https://doi.org/10.1093/bioinformatics/btm006" />
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="717" to="723" />
			<date type="published" when="2007-01">Jan. 2007</date>
		</imprint>
	</monogr>
	<note type="report_type">eprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Sequence variations within protein families are linearly related to structural variations</title>
		<author>
			<persName><forename type="first">Patrice</forename><surname>Koehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Levitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of molecular biology</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="page" from="551" to="562" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Comparative protein structure modeling using MOD-ELLER</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Webb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Sali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current protocols in bioinformatics</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="5" to="6" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">GalaxyRefineComplex: Refinement of protein-protein complex model structures driven by interface repacking</title>
		<author>
			<persName><forename type="first">Lim</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hasup</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaok</forename><surname>Seok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">PyRosetta: a script-based interface for implementing molecular modeling algorithms using Rosetta</title>
		<author>
			<persName><forename type="first">Sidhartha</forename><surname>Chaudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Lyskov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">J</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="689" to="691" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Protein Docking Model Evaluation by Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><forename type="middle">T</forename><surname>Flannery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daisuke</forename><surname>Kihara</surname></persName>
		</author>
		<idno type="DOI">10.3389/fmolb.2021.647915</idno>
		<ptr target="https://www.frontiersin.org/article/10.3389/fmolb.2021.647915" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Molecular Biosciences</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">DockQ: a quality measure for protein-protein docking models</title>
		<author>
			<persName><forename type="first">Sankar</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bj?rn</forename><surname>Wallner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">161879</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Evaluation of model refinement in CASP14</title>
		<author>
			<persName><forename type="first">J</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><surname>Simpkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins: Structure, Function, and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="1852" to="1869" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Reduced surface: an efficient way to compute molecular surfaces</title>
		<author>
			<persName><forename type="first">Arthur</forename><forename type="middle">J</forename><surname>Michel F Sanner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Claude</forename><surname>Olson</surname></persName>
		</author>
		<author>
			<persName><surname>Spehner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biopolymers</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="305" to="320" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Generative models for graph-based protein design</title>
		<author>
			<persName><forename type="first">John</forename><surname>Ingraham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Python 3 Reference Manual</title>
		<author>
			<persName><forename type="first">Guido</forename><surname>Van Rossum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fred</forename><forename type="middle">L</forename><surname>Drake</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>CreateSpace</publisher>
			<pubPlace>Scotts Valley, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">PyTorch: An Imperative Style, High-Performance Deep Learning Library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<ptr target="http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">PyTorch Lightning</title>
		<ptr target="https://github.com/PyTorchLightning/pytorch-lightning3" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Fast Graph Representation Learning with PyTorch Geometric</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Eric Lenssen</surname></persName>
		</author>
		<ptr target="https://github.com/pyg-team/pytorch_geometric" />
		<imprint>
			<date type="published" when="2019-05">May 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Deep Graph Library: A Graph-Centric, Highly-Performant Package for Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01315</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Table 5: Performance of different refinement methods, including ablation studies, on all test datasets</title>
	</analytic>
	<monogr>
		<title level="m">?Metric Fraction of Native Contacts ? Fraction of Non-Native Contacts ? PSR-Dockground</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">799</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title/>
		<author>
			<persName><surname>Psr-Deephomo</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">376</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title/>
		<author>
			<persName><surname>Psr-Evcoupling</surname></persName>
		</author>
		<imprint>
			<date>195</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
