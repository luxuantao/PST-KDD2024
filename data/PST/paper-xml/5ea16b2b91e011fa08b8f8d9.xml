<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">torchgpipe: On-the-fly Pipeline Parallelism for Training Giant Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-04-21">21 Apr 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chiheon</forename><surname>Kim</surname></persName>
							<email>chiheon.kim@kakaobrain.com</email>
							<affiliation key="aff0">
								<orgName type="institution">UNIST</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Heungsub</forename><surname>Lee</surname></persName>
							<email>heungsub.lee@kakaobrain.com</email>
							<affiliation key="aff0">
								<orgName type="institution">UNIST</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Myungryong</forename><surname>Jeong</surname></persName>
							<email>myungryong.jeong@kakaobrain.com</email>
							<affiliation key="aff0">
								<orgName type="institution">UNIST</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Woonhyuk</forename><surname>Baek</surname></persName>
							<email>wbaek@kakaobrain.com</email>
							<affiliation key="aff0">
								<orgName type="institution">UNIST</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Boogeon</forename><surname>Yoon</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UNIST</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ildoo</forename><surname>Kim</surname></persName>
							<email>ildoo.kim@kakaobrain.com</email>
							<affiliation key="aff0">
								<orgName type="institution">UNIST</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sungbin</forename><surname>Lim</surname></persName>
							<email>sungbin@unist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">UNIST</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sungwoong</forename><surname>Kim</surname></persName>
							<email>swkim@kakaobrain.com</email>
							<affiliation key="aff0">
								<orgName type="institution">UNIST</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kakao</forename><surname>Brain</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UNIST</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">torchgpipe: On-the-fly Pipeline Parallelism for Training Giant Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-04-21">21 Apr 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2004.09910v1[cs.DC]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We design and implement a ready-to-use library in PyTorch for performing micro-batch pipeline parallelism with checkpointing proposed by GPipe <ref type="bibr" target="#b10">[11]</ref>. In particular, we develop a set of design components to enable pipeline-parallel gradient computation in PyTorch's defineby-run and eager execution environment. We show that each component is necessary to fully benefit from pipeline parallelism in such environment, and demonstrate the efficiency of the library by applying it to various network architectures including AmoebaNet-D [23] and U-Net [24]. Our library is available at https://github.com/ kakaobrain/torchgpipe.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years, deep learning has seen significant growth, driven by several methodologies which enable the training of deep neural networks (DNNs) in a scalable way and by development of more powerful hardwares. It is observed that increased capacity of DNN effectively has improved the performance. For example, AmoebaNet-B <ref type="bibr" target="#b22">[23]</ref> scaled with GPipe <ref type="bibr" target="#b10">[11]</ref> has 557 million parameters and has achieved top-1 accuracy 84.4% which was state-of-the-arts result at the time, and GPT-2 <ref type="bibr" target="#b21">[22]</ref> is a Transformer-based <ref type="bibr" target="#b27">[28]</ref> language model which has 1.5 billion parameters (see Figure <ref type="figure">1</ref> of <ref type="bibr" target="#b10">[11]</ref> for the effect of model scaling). However, training such a massive model is very resource intensive. One can mitigate this issue by reducing the size of the model without losing the performance by pruning the model <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b0">1]</ref>, designing more efficient architectures <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b26">27]</ref>, architecture search under resource constraints <ref type="bibr" target="#b2">[3]</ref>, and many more.</p><p>We may wonder a rather direct approach is possible: can we train a massive model fast enough, given a large pool of devices? One obstacle is that common optimization tech-niques to train a neural network are sequential in nature. Those algorithms repeatedly compute the gradient of the loss with respect to the given mini-batch at a time and update the model parameters using the gradient. With abundant computational resource, data parallelism <ref type="bibr" target="#b16">[17]</ref> is commonly used to speed up the overall optimization procedure by dividing the mini-batch into micro-batches and delegating per micro-batch computation to available devices. With careful hyperparameter tuning, this effectively reduce the training time up to a certain size of mini-batch which may depend on model, optimization algorithm, and data <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b24">25]</ref>. One drawback of data-parallel training is that devices hold their own version of network for executing the subdivided task, and network parameters must be synchronized after each parameter update. This may induce heavy communication load when there are lots of parameters to synchronize.</p><p>Note that data parallelism is not applicable when the model is so big that it is impossible to compute gradient even when a single data point is fed into the network. Model parallelism <ref type="bibr" target="#b4">[5]</ref> is a method for training such a massive model, which partitions the model into several pieces and places them on different devices. Each device only computes a small part of the model, and updates only the parameters in that part. However, model parallelism suffers from its underutilization behavior. Since most neural networks consist of sequence of layers, the device holding the later part of the model must wait until computation in devices holding earlier parts of the model.</p><p>Another possible solution is to use gradient checkpointing <ref type="bibr" target="#b3">[4]</ref> which saves memory by only storing the subset of activation maps and re-computing the discarded activation maps when necessary. Obviously, this requires certain part of the model be computed twice and overall training time would be increased.</p><p>It is benefitting to combine different types of parallelization strategies <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b6">7]</ref>, and recent lines of research questions how to find an optimal strategy <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b28">29]</ref>. Among them, pipeline parallelism a way to accelerate neural network training by combining model parallelism with data pipelining, either in synchronous way as in GPipe <ref type="bibr" target="#b10">[11]</ref> or in asynchronous way as in <ref type="bibr" target="#b11">[12]</ref>, PipeDream <ref type="bibr" target="#b8">[9]</ref>, and XPipe <ref type="bibr" target="#b6">[7]</ref>. We remark that gradient checkpointing (also called re-materialization) is further combined in GPipe to allow training even bigger models.</p><p>In this paper, we design and implement torchgpipe, a ready-to-use library for GPipe in PyTorch <ref type="bibr" target="#b20">[21]</ref>. In particular, we develop a set of design components for optimized pipeline-parallel computations in PyTorch's defineby-run and eager execution environment. We show that each component is necessary to fully benefit from pipeline parallelism in such environment, and demonstrate the efficiency of torchgpipe by conducting the speed and memory benchmarks on AmoebaNet-D <ref type="bibr" target="#b22">[23]</ref> and U-Net <ref type="bibr" target="#b23">[24]</ref> when trained with the library.</p><p>The rest of the paper is organized as follows. In Section 2, we discuss how the forward and backward passes can be decomposed into subtasks (under certain assumptions), describe the device placement strategy of microbatch pipeline parallelism, and demonstrate what the desired order of execution per device is. In Section 3, we discuss complications for achieving the optimal timeline of pipeline parallelism in PyTorch and explain how torchgpipe resolves them. Additionally, we relax the assumption that the model is sequentially composed, and provide a way for expressing models with long skip connections so that pipeline parallelism still applies without giving up the efficiency. Then, we demonstrate that the optimization components suggested in the paper are essential for the performance, and evaluate the performance of the proposed library in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Pipeline Parallelism</head><p>Suppose that we have a neural network which is represented as a composition of sequence of subnetworks. Let us denote the subnetworks by</p><formula xml:id="formula_0">f 1 , • • • , f n with parameters θ 1 , • • • , θ n and let the full network be f = f n • f n−1 • • • • • f 1 , parameterized by θ = (θ 1 , • • • , θ n ).</formula><p>For clarity, we call f j the jth partition of f and assume that the parameters of partitions are mutually disjoint.</p><p>When training the network, gradient-based methods such as stochastic gradient descent requires computing the outcome f (x) of the network given a mini-batch x of training data and the corresponding loss, and the gradient g of the loss with respect to the network parameter θ. Those two stages are called forward and backward pass, respectively.</p><p>Since f is sequentially composed, in forward pass f (x) can be computed by letting x 0 = x and sequentially applying the partitions as x j = f j (x j−<ref type="foot" target="#foot_0">1</ref> ) for j = 1, • • • , L.</p><p>Furthermore, if x consists of m smaller batches x 1 , • • • , x m called micro-batches, computing f (x) dissolves into tasks F i,j where x 0 i = x i and</p><formula xml:id="formula_1">x j i ← f j (x j−1 i ) (F i,j ) for i = 1, • • • , m and j = 1, • • • , n</formula><p>, assuming that f does not involve any intra-batch computation. One prominent exception for this is batch normalization <ref type="bibr" target="#b12">[13]</ref> 1 . The loss is obtained by aggregating x n i = f (x i ) and evaluating the loss function on them.</p><p>In a similar fashion, backward pass is decomposed into tasks B i,j where dx n i is the gradient of the loss with respect to x n i and</p><formula xml:id="formula_2">dx j−1 i ← ∂ x f j (dx j i ) g j i ← ∂ θ j f j (dx j i ) (B i,j ) for i = 1, • • • , m and j = 1, • • • , n.</formula><p>Here</p><formula xml:id="formula_3">∂ x f j : v → v T • df j dx x=x j−1 i</formula><p>is a function which does backward propagation (also known as vector-Jacobian product) through the partition f j , and ∂ θ j f j is defined likewise. As a result, we get the gradient of the loss with respect to θ j by summing g j i over i's. Note that there are data dependencies between tasks. For example, F i,j requires x j−1 i which is only available after F i,j−1 , hence F i,j−1 must be completed before starting F i,j and the same applies for B i,j and B i,j+1 . Figure <ref type="figure">1</ref> shows the full dependency graph in the case of m = 4 and n = 3.</p><p>Given the set of tasks {F i,j } and {B i,j } and a pool of devices which can work in parallel, different parallelization strategies have their own rule to assign tasks to devices. Each device computes one or more assigned tasks as soon as the dependencies are resolved. In the setting above, all dependencies are among the tasks with the same micro-batch index i. Hence, one can effectively parallelize the tasks by assigning tasks with different micro-batch indices to different devices -which is data parallelism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Dependency Graph of GPipe</head><p>Pipeline parallelism's strategy is to assign tasks with respect to the partition index j so that jth partition entirely lies in the jth device. In addition to this, it is enforced that F i,j must be completed before executing F i+1,j and B i,j must be completed before executing B i−1,j .</p><p>In addition to the micro-batch pipelining, GPipe <ref type="bibr" target="#b10">[11]</ref> further reduces the memory requirement by utilizing gradient checkpointing for each B i,j . Since jth device executes B i,j</p><formula xml:id="formula_4">F 1,1 F 1,2 F 1,3 B 1,3 B 1,2 B 1,1 F 2,1 F 3,1 F 4,1 F 2,2 F 2,3 B 2,3 B 2,2 B 2,1 F 3,2 F 3,3 B 3,3 B 3,2 B 3,1 F 4,2 F 4,3 B 4,3 B 4,2 B 4,1</formula><p>Figure <ref type="figure">1</ref>: Minimal dependency graph for forward and backward pass. </p><formula xml:id="formula_5">F 1,1 F 2,1 F 3,1 F 4,1 F 1,2 F 2,2 F 3,2 F 4,2 F 1,3 F 2,3 F 3,3 F 4,3 F 1,3 F 2,3 F 3,3 B 4,3 B 1,3 B 2,3 B 3,3 F 1,2 F 2,2 F 3,2 B 4,2 B 1,2 B 2,2 B 3,2 F 1,1 F 2,1 F 3,1 B 4,1 B 1,1 B 2,1 B 3,1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Forward</head><p>Send</p><formula xml:id="formula_6">dx j−1 i to D j−1 F′ i,j B i,j F′ i−1,j Receive dx j i from D j+1 Receive dx j i−1 from D j+1 Send dx j−1 i+1 to D j−1 Send x j i to D j+1 Receive x j−1 i+1 from D j−1 F i,j F i+1,j</formula><p>Backward one at a time, only the activation maps obtained from F i,j are needed to complete B i,j . By recomputing the forward pass F i,j right before executing B i,j , memory consumption is reduced by a factor of m. Moreover, the re-computation can take place while the device is waiting for B i,j+1 being done. This is summarized in Figure <ref type="figure" target="#fig_0">2</ref>, where dashed arrows denotes the execution order between independent tasks induced by the micro-batch order, and F i,j denotes the recomputation of F i,j . We remark that re-computations for the last micro-batch, i.e., F m,j for j = 1, • • • , n are unnecessary. This is because that on jth device the last task in the forward pass is F m,j , so discarding intermediate activations of it in forward pass and re-computing them in the beginning of backward pass has no effect of reducing memory, only slowing down the pipeline. For this reason, F m,j is omitted from the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Device-wise Execution Order</head><p>To summarize, in pipeline parallelism (with checkpointing) each device is assigned with a set of tasks with the prescribed order. Each device will execute the given tasks one-by-one as soon as cross-device dependencies are met. However, there is a missing component in this picture -data tranfer between the devices. For illustration, the full execution order that device j must follow is shown in Figure <ref type="figure" target="#fig_1">3</ref>. Here data transfer operations are explicitly denoted as 'receive' and 'send' for emphasis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">torchgpipe: A PyTorch Library for GPipe</head><p>torchgpipe is a PyTorch library for micro-batch pipeline parallelism with checkpointing, as known as GPipe. The library provides a simple way to apply GPipe to a generic sequential module written in PyTorch. The usage of torchgpipe resembles that of the data parallel module of PyTorch -just wrap your model with the wrapper.</p><p>Users must specify the number of micro-batches m and how consecutive layers form n partitions. Here we remark that even though we simplified our assumption to that the model is a sequence of partitions, it is strictly required in torchgpipe that the model is a sequence of layers to give flexibility for users how to split the model. torchgpipe will assume that each layer is a non-divisible, black-box, and referentially transparent<ref type="foot" target="#foot_1">2</ref> algorithm.</p><p>For convenience, the library provides the submodule torchgpipe.balance which computes a partition whose pairwise resource discrepancy is small, where resource consumption is computed by profiling. Specifically, we used the algorithm from <ref type="bibr" target="#b1">[2]</ref>.</p><p>As torchgpipe is built on PyTorch equipped with CUDA backend, we will often assume that devices are NVIDIA GPU throughout this section. Nevertheless, the underlying principle of the library applies in general for implementing pipeline parallelism any eager execution environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Complications in PyTorch</head><p>Our primary concern is efficiency. As we discussed in Section 2.2, in order for pipeline parallelism to work as desired, the tasks must be assigned to each device in the correct order. There are several complications to achieve this in PyTorch.</p><p>First of all, kernels are issued to each device on-the-fly due to PyTorch's define-by-run style and its eager execution behavior (as opposed to in construct-and-run type frameworks). Hence, one must design the host code carefully so not only that device-bound tasks are issued in the correct order within each device, but also that execution of the tasks on devices (asynchronous to CPU) are not delayed due to the Python interpreter failing to request it ahead of the time. This kind of delay may happen when some of the tasks are CPU-intensive or involve a lot of cheap kernel calls. As a solution, torchgpipe introduces deterministic clock-cycle which gives the total ordering of the tasks.</p><p>Secondly, the computation graph for backward pass is constructed dynamically during the forward pass in PyTorch. In other words, "it avoids ever materializing a "forward graph", recording only what is necessary to differentiate the computation." <ref type="bibr" target="#b20">[21]</ref> Since PyTorch does not record the forward computation graph nor maintain a gradient tape, the automatic differentiation (autograd) engine of PyTorch does back-propagation solely with respect to the graph. It implies that autograd engine may not run exactly in the reverse order of execution as in the forward pass, unless enforced by the structure of the graph. To deal with this, we develop a pair of primitive functions called 'fork' and 'join' to create explicit dependencies on the fly in the backward computation graph.</p><p>Thirdly, communication between several devices can cause two-way synchronization, if not carefully managed. This may cause under-utilization since sender may wait to synchronize with the receiver even when there is no explicit dependency between the copy and next task in queue, or vice versa. torchgpipe avoids this issue by using nondefault CUDA streams so that copies would never block computations unless the computation must wait for the data.</p><p>Lastly, torchgpipe attempts to relax the restriction of micro-batch pipeline parallelism that model must be sequential. Although any neural network can be written in a sequential form in principle, this requires knowing the entire computation graph ahead of the time which is not the case in PyTorch. In particular, if there is a tensor which skips from a layer in device j to another layer in device j &gt; j + 1, the tensor will be copied to all devices in between since torchgpipe cannot know it ahead. To circumvent this issue, we design an interface to signify which intermediate tensors are skipped and which layers use them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Optimization Components</head><p>In the remainder of this section, it is explained how the components of torchgpipe are designed and why each of them is essential for performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Forward Dependency: Deterministic Clock-cycle</head><p>As we discussed in Section 3.1, the total ordering of tasks is determined by the host code in the forward pass. Each device implicitly understands the dependency between tasks by the order they are assigned by CPU. Ideally, if tasks could be assigned to devices with no cost, CPU may assign tasks to devices in any order as long as the ordering within device is correct. However, this assumption is not realistic enough, as launching kernels on a GPU is not free for CPU, memory transfer between GPUs may require synchronization, or a task is CPU-intensive. For this reason, we minimize the delay coming from CPU by sorting all tasks by the distance to F 1,1 .</p><p>Algorithm 1: Deterministic clock-cycle</p><formula xml:id="formula_7">for k from 1 to m + n − 1 do for i, j such that i + j − 1 = k do if j &gt; 1 then Copy x j−1 i to device j.</formula><p>for i, j such that i</p><formula xml:id="formula_8">+ j − 1 = k do Execute F i,j .</formula><p>We call this deterministic clock-cycle (Algorithm 1). In the algorithm, CPU executes the clock cycles starting from the counter k = 1 to k = m + n − 1. In kth clock cycle, all copy kernels for data needed to execute tasks F i,j where i + j − 1 = k are first issued, and then the computation kernels for executing the tasks are registered to corresponding devices (which can be safely multithreaded since tasks in the same clock cycle are independent).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Backward Dependency: Fork and Join</head><p>Suppose now that we run a forward pass according to the deterministic clock-cycle. The resulting computation graph Here the virtual depedency of F i,j on B i+1,j is created via Fork and Join, which is illustrated by dashed arrows.</p><formula xml:id="formula_9">B i+1,j−1 F′ i+1,j−1 Join Join B i+1,j B i,j F′ i,j Fork Fork B i,j+1 Micro-batch i Micro-batch i + 1</formula><p>for backward will look rather like 1 than 2, even when the forward tasks F 1,j , • • • , F m,j on device j were executed in order. From such a graph, autograd engine of PyTorch would never know that B i+1,j must be executed before B i,j , and this messes up the timeline of the backward pass. For this reason, virtual dependencies (dashed arrows in Figure <ref type="figure" target="#fig_0">2</ref>) must be explicitly drawn during the forward pass. We design a pair of primitive functions called Fork and Join to express such dependency. Basically, Fork is the autograd function mapping a tensor x to the pair (x, ∅) where ∅ is an empty tensor 3 , and Join is the autograd function mapping a pair (x, ∅) to the tensor x. Now, dependency of F i+1,j upon F i,j (which translates to the dependency of B i,j upon B i+1,j in the backward computation graph) can be expressed as</p><formula xml:id="formula_10">(x j i , ∅) ← Fork(x j i ) x j−1 i+1 ← Join(x j−1 i+1 , ∅).</formula><p>See Figure <ref type="figure" target="#fig_2">4</ref> for illustration. 3 In principle, the tensor which indicates the virtual dependency can be arbitrary. We chose to use the empty tensor for this, however, to remove any unnecessary computation caused by the tensor such as gradient accumulation in PyTorch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Concurrent Copy and Computation: Streams</head><p>PyTorch issues every device-bound kernels to the default stream, unless it is specified otherwise. Stream is a devicebound sequence of kernels that is executed in order. Kernels in the same stream are guaranteed to be executed in the prescribed order, but kernels in different streams can be interleaved, and even can overlap when possible. In particular, nearly all CUDA devices with compute capability 1.1 and higher support concurrent copy and execution: data transfer between devices can always overlap with kernel execution (see section 4.5.1.5 of <ref type="bibr" target="#b19">[20]</ref>).</p><p>torchgpipe registers every copy kernel to non-default streams while keeping computation kernels on the default stream. This allows the device j processing F i,j in concurrent with sending x j i−1 to the device j + 1 and/or receiving x j−1 i from the device j − 1. Moreover, each device uses different streams for each micro-batch. Since there is no true dependency between different micro-batches, this use of streams is safe and this allows copies to occur as fast as possible. See Figure <ref type="figure">5</ref> for illustration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Autograd Functions with Shared Memory</head><p>So far in this section, we did not discuss how to schedule re-computation tasks F i,j when gradient checkpointing is in use. It must be scheduled in prior to the back-propagation task B i,j upon completion of B i+1,j . This must be encoded in the computation graph as well for autograd engine. Indeed, PyTorch supports such functionality via an in-house autograd function for checkpointing.</p><p>Checkpoint in PyTorch is implemented by defining an autograd function which computes as usual function in the forward pass without storing intermediate activation maps but the inputs. In the backward pass, this function constructs a local computation graph for backward by recomputing the function using the stored inputs, and computes gradients by back-propagating through the local graph. However, this tightly binds F i,j and B i,j together. Ultimately, we would like to insert the instruction for waiting the result dx j i of B i,j+1 to be copied from device j + 1 to device j in between F i,j and B i,j , to allow that F i,j and the copy happens concurrently.</p><p>For such a fine-grained order control, torchgpipe implements checkpointing with two separate autograd functions Checkpoint and Recompute. At the execution time of the task F i,j , a pair of Checkpoint and Recompute which have a shared memory is generated. This shared memory is used in the backward pass for transferring the local computation graph made by executing Recompute to Checkpoint for back-propagation. By arranging the functions so that F i,j , synchronization for receiving dx j i , and B i,j are executed in the order during the backward pass, it is ensured that re-computation and copy can happen concurrently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Dealing with Non-sequential Models</head><p>In Section 2, we assumed that the model f is composed of partitions f 1 , • • • , f n in sequence. In principle, any neural network can be represented in this form by sorting all nodes in the forward computation graph of f in topological ordering. Hence, pipeline parallelism is applicable to any model.</p><p>However, consider a symptomatic case that all the partitions except the first and the last one are parallel, i.e.,</p><formula xml:id="formula_11">f (y) = g n (x 2 , • • • , x n−1 )</formula><p>where x 1 = g 1 (x) and x j = g j (x 1 ) for j = 2, • • • , n − 1. In a sequential form, this is equivalent to</p><formula xml:id="formula_12">f = f n • • • • • f 1 such that f n (x 1 , x 2 , • • • , x n−1 ) := g n (x 2 , • • • , x n−1 ), f (x 1 , • • • , x j−1 ) := (x 1 , • • • , x j−1 , f j (x 1 )) for j = 2, • • • , n − 1, and f 1 = g 1 .</formula><p>In this case, it is quite inefficient to use pipeline parallelism in its native form since at the boundary of device j − 1 and j, the tuple</p><formula xml:id="formula_13">(x 1 i , • • • , x j−1 i</formula><p>) must be copied instead of a single tensor x 1 i which is the only required data to compute jth partition.</p><p>torchgpipe provides a submodule which allows users to indicate skipping tensors from which layer to which layer: torchgpipe.skip. With the decorator @skippable, user-defined layer can stash a tensor for later or pop a stashed one via yield operator in Python without returning it. This in particular does not change the input and output signature of a layer. Hence, minimal effort is needed for adding skip connection to a preexisting sequential model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Hiding Skip Tensors in the Graph: Portals</head><p>Adding skip connections into the dependency graph (Figure <ref type="figure" target="#fig_0">2</ref>) is fairly straightforward. Indeed, no additional dependency would be introduced no matter how many skip connections are added, hence only the copy kernels for skip connections need extra care. In torchgpipe, this is taken care by portals consisting of three autograd functions PortalBlue, PortalOrange, and PortalCopy sharing memory, like Checkpoint and Recompute in Section 3.2.4. Each does the job of saving the skip tensor, loading the tensor, and moving the saved tensor to the skipped device, respectively (and vice versa in the backward pass). This mechanism is illustrated in Figure <ref type="figure" target="#fig_4">6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Every experiment was conducted with NVIDIA Tesla P40 GPUs with CUDA 10.1.243, each having 22 GiB of memory. For reproducibility, codes for all benchmarks provided in this section is made available in the repository 4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Effects of Optimization Components</head><p>We conducted an experiment to show that every component of torchgpipe is necessary to achieve the maximal efficiency. Starting from the baseline which only has deterministic clock-cycle but no others, each component (backward dependency via Fork and Join, non-default streams for copy kernels, and portals for skip connections) is added 4 Further details available at this link.    incrementally. We report the throughput, GPU utilization, and memory usage under each setting to measure how each component contributed to the performance of torchgpipe.</p><formula xml:id="formula_14">Copy i,1→2 F i,2 F i,3 Copy i,2→3 Copy i,1→3 F i,1 (b) With portals F i,1 Copy i,1→2 F i,2 Copy i,2→3 F i,3<label>(a)</label></formula><p>We find that addition of each component gives a speed-up, and with all components torchgpipe runs nearly twice as fast as the baseline. Results can be found in Table <ref type="table" target="#tab_0">1</ref>.</p><p>We used U-Net for the experiment. Details of the architecture can be found in Section 4.2.2 and we set (B, C) to be (5, 64) as in the speed benchmark. In settings without portals, the model is implemented as a fully sequential version where skip connections are encoded as inputs and outputs of layers that they pass through, as described in the symptomatic example of Section 3.3. For the setting with all components, it is implemented with torchgpipe.skip while the architecture is identical.</p><p>We also visualized per GPU timelines to help understanding each component's role, illustrated in Figure <ref type="figure" target="#fig_6">7</ref>. Explanation for each picture is summarized as follows.</p><p>(a) By deterministic clock-cycle, all kernels are issued in the correct order during forward pass. It is illustrated by the left part of the timeline. However, without explicit dependency encoded in the computation graph, the autograd engine processes the micro-batches in an uncontrollable order so the timeline is messed up.</p><p>(b) With backward dependency, kernels are now issued in the correct, deterministic order in backward pass.</p><p>(c) By using non-default copy streams, copies and computations are now concurrent as illustrated by overlapping blue and red bars.</p><p>(d) Portals remove unnecessary copies caused by transferring the skipping tensor to all devices in between. This is illustrated by that the length of red bars are reduced compared to (c).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Performance Benchmarks</head><p>To demonstrate the efficiency of torchgpipe, we report performance benchmarks similar to that conducted by GPipe <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">AmoebaNet-D Speed Benchmark</head><p>We measured the throughput of AmoebaNet-D with various number of devices. For this, we measured the throughput of the model when torchgpipe is applied, with n partitions and m micro-batches. Here throughput means the number of samples processed per second.</p><p>The experiment is conducted for each pair (m, n) where m ∈ {1, 4, 32} and n ∈ {2, 4, 8}. When m = 1, we used checkpointing to all micro-batches<ref type="foot" target="#foot_2">5</ref> to make a fair comparison of loss due to checkpointing with <ref type="bibr" target="#b10">[11]</ref>. The model we used is our implementation of a sequential version of AmoebaNet-D in PyTorch <ref type="foot" target="#foot_3">6</ref> .</p><p>The model is trained by plain SGD for 10 epochs and reported the average throughput over the epochs except the first one. To exclude the overhead caused by data loading, we used a synthesized dataset which consists of 10,000 images whose dimension is 3 × 224 × 224. For each setting, the batch size and the number of micro-batches are chosen to maximize the throughput. Relative speed-up is calculated against the baseline case (m, n) = (1, 2) and reported in Table 2. We included the speed-up of GPipe for comparison.</p><p>The relative speed-up of torchgpipe shows similar trend to that of GPipe. We remark that differences in performance reported in Table <ref type="table">2</ref> might be due to many unknown factors such as balance of the partitions, discrepancy between the implementation, difference in devices, and so on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">U-Net Memory Benchmark</head><p>To evaluate the effectiveness of torchgpipe for models with long skip connections, we used U-Net <ref type="bibr" target="#b23">[24]</ref> for 2dimensional segmentation. The version of U-Net we used has five down-sampling layers and five up-sampling layers, and two hyper-parameters B and C determining the size of the model. Here B stands for the number of convolution blocks in between down-sampling layers, and C stands for the number of output channels of the first convolution. Channels are doubled after each down-sampling layers (or halved after each up-sampling layers, respectively). Our implementation of U-Net is rather symmetric than the original model proposed in <ref type="bibr" target="#b23">[24]</ref> for effective balancing.</p><p>We conducted an experiment to measure the ability of torchgpipe for training a bigger model. For 1, 2, 4 and 8 GPUs, we found maximum (B, C) to occupy each number of devices. In all settings, the input size is set to 3 × 192 × 192, the output size to 1 × 192 × 192, and the batch size to 32. The total memory usage for training each model is reported in Table <ref type="table" target="#tab_2">3</ref>. Here parameters consumes 8 bytes each for itself and its gradients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">U-Net Speed Benchmark</head><p>We also measured the throughput of U-Net with various number of devices. Naive-1 denotes the baseline without pipeline parallelism nor checkpointing, and Pipeline-1, - Table <ref type="table">2</ref>: Speed benchmark on AmoebaNet-D <ref type="bibr" target="#b17">(18,</ref><ref type="bibr">256)</ref>.</p><p>In <ref type="bibr" target="#b10">[11]</ref>, Cloud TPUv3s were used while we used NVIDIA Tesla P40 GPUs in our experiments. with the corresponding number of partitions. The hyperparameters determining the size of U-Net is set to (B, C) = (5, 64) in this experiment. The batch size, the number of micro-batches (m), and the balance to partitions are chosen to maximize the throughput. For each setting, throughput is measured as in Section 4.2.1 except that the image size was 3 × 192 × 192 in this experiment. Result is summarized in Table <ref type="table" target="#tab_3">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we introduced torchgpipe, a ready-touse library in PyTorch for micro-batch pipeline parallelism with checkpointing proposed by GPipe <ref type="bibr" target="#b10">[11]</ref>. This library is designed and implemented in PyTorch's define-by-run and eager execution environment. Ablation study and performance benchmarks presented in Section 4 demonstrate that all components of torchgpipe are essential to endeavor the desired advantanges of pipeline parallelism with checkpointing in eager execution environment. We believe that general principles we established in the paper apply to any other frameworks with eager execution environment.</p><p>We tried to avoid going too deep into technical details involved in torchgpipe.</p><p>Our code is available at https://github.com/kakaobrain/ torchgpipe for those who are interested in further details, and those who want to apply pipeline parallelism to their model in PyTorch.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Dependency graph for pipeline parallelism with checkpointing. Colors denote the devices that tasks are computed in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The execution order that jth device must follow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The backward computation graph with Fork and Join. Different colors correspond to different devices. Arrows are drawn according to the direction in backward computation graph and these relations are constructed during the forward pass.Here the virtual depedency of F i,j on B i+1,j is created via Fork and Join, which is illustrated by dashed arrows.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>1 Figure 5 :</head><label>15</label><figDesc>Figure 5: Timeline of device j with or without non-default streams for copy. (a): If only default streams are used, copy kernels may block computation kernels (and vice versa) until the copy is completely finished. (b): With copy streams, computation can happen in concurrent with sending or receiving data from other devices.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The flow of skip connection with or without portals. (a): Without portals, skipped tensor from device 1 is copied to device 2 and subsequently to device 3. (b): With portals, the tensor is directly copied to device 3. The gradient flows in the exact reverse direction in the backward pass.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>torchgpipe when optimization components are incrementally added. The U-Net model with (B, C) = (5, 64) is used for the experiment. The batch size and the number of micro-batches are fixed as 128 and 8, respectively. The model is partitioned and placed on four devices via torchgpipe. Here the partition was found manually with the aid of torchgpipe.balance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Detailed view of CUDA timeline for each setting in Table 1, profiled with NVIDIA Nsight Systems 2019.5.1.58. Starting from the top, adjacent lanes with blue bars and red bars visualize the timeline per device. Blue bars represent computation kernels while red bars represent device-to-device copy (length proportional to time).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Performance of</figDesc><table><row><cell cols="3">Optimization components</cell><cell cols="4">Throughput Speed up Utilization Memory usage</cell></row><row><cell>×</cell><cell>×</cell><cell>×</cell><cell>30.662/s</cell><cell>1</cell><cell>44%</cell><cell>52.2 GiB</cell></row><row><cell>Dependency</cell><cell>×</cell><cell>×</cell><cell>41.306/s</cell><cell>1.347</cell><cell>59%</cell><cell>19.1 GiB</cell></row><row><cell cols="2">Dependency Streams</cell><cell>×</cell><cell>55.191/s</cell><cell>1.800</cell><cell>71%</cell><cell>30.0 GiB</cell></row><row><cell cols="3">Dependency Streams Portals</cell><cell>58.477/s</cell><cell>1.907</cell><cell>75%</cell><cell>23.5 GiB</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>2, -4, -8 denotes that the model is trained with torchgpipe</figDesc><table><row><cell>AmoebaNet-D</cell><cell></cell><cell>GPipe [11]</cell><cell></cell><cell></cell><cell>Ours</cell><cell></cell></row><row><cell>n =</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>2</cell><cell>4</cell><cell>8</cell></row><row><cell>m = 1</cell><cell>1</cell><cell cols="2">1.13 1.38</cell><cell>1</cell><cell cols="2">1.00 0.93</cell></row><row><cell>m = 4</cell><cell cols="6">1.07 1.26 1.72 1.54 1.67 2.62</cell></row><row><cell>m = 32</cell><cell cols="6">1.21 1.84 3.48 1.77 2.71 4.95</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Memory benchmark on U-Net.</figDesc><table><row><cell>U-Net</cell><cell>(B, C)</cell><cell cols="3">Parameters Memory usage</cell></row><row><cell>Naive-1</cell><cell>(6, 72)</cell><cell>362.2M</cell><cell>20.3 GiB</cell><cell></cell></row><row><cell cols="2">Pipeline-1 (11, 128)</cell><cell>2.21B</cell><cell>20.5 GiB</cell><cell></cell></row><row><cell cols="2">Pipeline-2 (24, 128)</cell><cell>4.99B</cell><cell>43.4 GiB</cell><cell></cell></row><row><cell cols="2">Pipeline-4 (24, 160)</cell><cell>7.80B</cell><cell>79.1 GiB</cell><cell></cell></row><row><cell cols="2">Pipeline-8 (48, 160)</cell><cell>15.82B</cell><cell>154.1 GiB</cell><cell></cell></row><row><cell>U-Net</cell><cell cols="4">Throughput Speed up Batch size m</cell></row><row><cell>Naive-1</cell><cell>28.500/s</cell><cell>1</cell><cell>40</cell><cell>×</cell></row><row><cell>Pipeline-1</cell><cell>24.456/s</cell><cell>0.858</cell><cell>80</cell><cell>2</cell></row><row><cell>Pipeline-2</cell><cell>35.502/s</cell><cell>1.246</cell><cell>512</cell><cell>32</cell></row><row><cell>Pipeline-4</cell><cell>67.042/s</cell><cell>2.352</cell><cell>512</cell><cell>16</cell></row><row><cell>Pipeline-8</cell><cell>88.497/s</cell><cell>3.105</cell><cell>640</cell><cell>40</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Speed benchmark on U-Net with (B, C) = (5, 64).</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Applying pipeline parallelism to a network with batch normalization is feasible while the computation is not identical anymore. Indeed, this discrepancy also exists in data-parallel training scheme and it may results in degradation of the result.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">This is required especially for checkpointing: referential transparency ensures that recomputation is identical to the computation done in the forward pass.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2">torchgpipe does not use checkpointing on the last micro-batch by default, as explained in Section 2. This means that no checkpointing is applied when m = 1.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3">We tried to make it as close as possible to the model in the official repository of TensorFlow (link).</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning the number of neurons in deep networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathieu</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2270" to="2278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Block partitions of sequences</title>
		<author>
			<persName><forename type="first">Imre</forename><surname>Bárány</surname></persName>
		</author>
		<author>
			<persName><surname>Victor</surname></persName>
		</author>
		<author>
			<persName><surname>Grinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Israel Journal of Mathematics</title>
		<imprint>
			<biblScope unit="volume">206</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="155" to="164" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">ProxylessNAS: Direct neural architecture search on target task and hardware</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Training deep nets with sublinear memory cost</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.06174</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Large scale distributed deep networks</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc'aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1223" to="1231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Xpipe: Efficient pipeline model parallelism for multi-gpu dnn training</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wotao</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xicheng</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.04610</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning both weights and connections for efficient neural network</title>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1135" to="1143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Harlap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amar</forename><surname>Phanishayee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Seshadri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Devanur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Ganger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Gibbons</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03377</idno>
		<title level="m">Pipedream: Fast and efficient pipeline parallel dnn training</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Menglong</forename><surname>Andrew G Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Gpipe: Efficient training of giant neural networks using pipeline parallelism</title>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youlong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2008">2019. 1, 2, 7, 8</date>
			<biblScope unit="page" from="103" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Decoupled parallel backpropagation with convergence guarantee</title>
		<author>
			<persName><forename type="first">Zhouyuan</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.10574</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>PMLR. 2</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
				<editor>
			<persName><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">David</forename><surname>Blei</surname></persName>
		</editor>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07">Jul 2015</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="7" to="09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Exploring hidden dimensions in accelerating convolutional neural networks</title>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sina</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Aiken</surname></persName>
		</author>
		<idno>PMLR. 1</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
				<editor>
			<persName><forename type="first">Jennifer</forename><surname>Dy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</editor>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsmssan, Stockholm Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07-15">10-15 Jul 2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="2274" to="2283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Beyond data and model parallelism for deep neural networks</title>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Aiken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Conference on Systems and Machine Learning (SysML)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">One weird trick for parallelizing convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.5997</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A hierarchical model for device placement</title>
		<author>
			<persName><forename type="first">Azalia</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Goldie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Device placement optimization with reinforcement learning</title>
		<author>
			<persName><forename type="first">Azalia</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rasmus</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuefeng</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2430" to="2439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">CUDA programming guide 1.1. (link)</title>
		<author>
			<persName><surname>Nvidia</surname></persName>
		</author>
		<author>
			<persName><surname>Nvidia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Autodiff Workshop</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4780" to="4789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">LNCS</title>
		<imprint>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="234" to="241" />
			<date type="published" when="2015">2015</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Jaehoon</forename><surname>Christopher J Shallue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Antognini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.03600</idno>
		<title level="m">Measuring the effects of data parallelism on neural network training</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Mesh-tensorflow: Deep learning for supercomputers</title>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youlong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Penporn</forename><surname>Koanantakool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingsheng</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cliff</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="10414" to="10423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudip</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amirali</forename><surname>Abdolrashidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiumin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Goldie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Azalia</forename><surname>Mirhoseini</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01578</idno>
		<title level="m">Generalized device placement for dataflow graphs</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
