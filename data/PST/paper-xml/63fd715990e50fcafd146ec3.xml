<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IGB: Addressing The Gaps In Labeling, Features, Heterogeneity, and Size of Public Graph Datasets for Deep Learning Research</title>
				<funder>
					<orgName type="full">Amazon Research Awards</orgName>
				</funder>
				<funder>
					<orgName type="full">IMPACT</orgName>
				</funder>
				<funder>
					<orgName type="full">NVIDIA</orgName>
				</funder>
				<funder>
					<orgName type="full">IBM-ILLINOIS Discovery Accelerator Institute</orgName>
					<orgName type="abbreviated">IIDA</orgName>
				</funder>
				<funder>
					<orgName type="full">NVIDIA Research</orgName>
				</funder>
				<funder ref="#_5adMCG5">
					<orgName type="full">IBM-ILLINOIS Center for Cognitive Computing Systems Research</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-02-27">27 Feb 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Vikram</roleName><forename type="first">Arpandeep</forename><surname>Khatua</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Vikram</forename><surname>Sharma Mailthody</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Bhagyashree</forename><surname>Taleka</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Wen-Mei</forename><surname>Hwu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sharma</forename><surname>Mailthody</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xiang</forename><surname>Song</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">UIUC</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">NVIDIA</orgName>
								<address>
									<country>UIUC USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">USC</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">IBM Research</orgName>
								<address>
									<country>USA Xiang Song Amazon AWS USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">NVIDIA</orgName>
								<address>
									<country>UIUC USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">KDD</orgName>
								<address>
									<addrLine>August 06-10</addrLine>
									<postCode>2023</postCode>
									<settlement>Long Beach</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">IGB: Addressing The Gaps In Labeling, Features, Heterogeneity, and Size of Public Graph Datasets for Deep Learning Research</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-02-27">27 Feb 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2302.13522v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Datasets</term>
					<term>Graph neural networks (GNNs)</term>
					<term>Graphs</term>
					<term>Deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph neural networks (GNNs) have shown high potential for a variety of real-world, challenging applications, but one of the major obstacles in GNN research is the lack of large-scale flexible datasets. Most existing public datasets for GNNs are relatively small, which limits the ability of GNNs to generalize to unseen data. The few existing large-scale graph datasets provide very limited labeled data. This makes it difficult to determine if the GNN model's low accuracy for unseen data is inherently due to insufficient training data or if the model failed to generalize. Additionally, datasets used to train GNNs need to offer flexibility to enable a thorough study of the impact of various factors while training GNN models.</p><p>In this work, we introduce the Illinois Graph Benchmark (IGB), a research dataset tool that the developers can use to train, scrutinize and systematically evaluate GNN models with high fidelity. IGB includes both homogeneous and heterogeneous graphs of enormous sizes, with more than 40% of their nodes labeled. Compared to the largest graph datasets publicly available, the IGB provides over 162? more labeled data for deep learning practitioners and developers to create and evaluate models with higher accuracy. The IGB dataset is designed to be flexible, enabling the study of various GNN architectures, embedding generation techniques, and analyzing system performance issues. IGB is open-sourced, supports DGL and PyG frameworks, and comes with releases of the raw text that we believe foster emerging language models and GNN research projects. An early public version of IGB is available at https://github.com/IllinoisGraphBenchmark/IGB-Datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>? Computing methodologies ? Neural networks; Knowledge representation and reasoning; Natural language processing.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph neural networks (GNNs) are a class of neural networks that operate on graph-structured data. GNNs have shown to be effective in addressing a variety of real-world applications such as fraud detection <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b71">73]</ref>, recommendation systems <ref type="bibr" target="#b59">[61,</ref><ref type="bibr" target="#b68">70,</ref><ref type="bibr" target="#b69">71]</ref>, predicting molecular and protein structure <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b55">57]</ref>, knowledge representation <ref type="bibr" target="#b14">[15]</ref>, and more recently helping in fine-tuning large language models <ref type="bibr" target="#b70">[72]</ref>. Their popularity has led to the development of various optimized frameworks and libraries <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b61">63,</ref><ref type="bibr" target="#b72">74]</ref> that enable the fast application of GNNs on new domains, making it easier for researchers and practitioners to leverage the power of GNNs in their work.</p><p>However, high-quality frameworks and libraries are necessary but not sufficient for enabling fast research progress in GNN. One of the major challenges in GNN research is the lack of large-scale datasets. This is because large graph datasets are typically propriety and most publicly available ones are rather small. The small size of these datasets makes it difficult to train GNNs that can handle large-graph structures and prevents the use of powerful pre-training techniques <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b57">59,</ref><ref type="bibr" target="#b63">65]</ref>. These challenges make it difficult to fully leverage GNN potential and its applications.</p><p>To address these challenges, recent work such as OGBN and MAG <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref> have proposed open large graph benchmark suites providing up to 244 million nodes and 1.7 billion edge graphs. These datasets contain a diverse set of graphs and have been widely used in the research community to benchmark GNNs' performance. However, most existing datasets, including OGBN, provide very limited labeled data. As GNN downstream tasks are often trained as supervised learning tasks, having large labeled data matters, especially for multi-label classification problems. However, both OGBN and MAG use Arxiv <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref> class labels which provide 1.4 million labeled nodes, meaning only about 1% of the overall dataset is labeled! With such small labeled data usage during training, it becomes challenging to determine if the GNN model's low accuracy for unseen data is inherently due to insufficient training data or if the model itself fails to generalize <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b60">62,</ref><ref type="bibr" target="#b74">76]</ref>.</p><p>Furthermore, the lack of flexible datasets hinders the researcher's ability to scrutinize and systematically evaluate the scalability of the GNN models, frameworks, and systems. Ideally, a dataset should provide (a) capability to study the impact of embedding generation techniques and their properties on the GNN model's accuracy, (b) provide sub-datasets of varying graph sizes and embeddings maintaining consistent homophily, and (c) provide a range of multi-class classification tasks with varying degrees of complexity. Without such flexible datasets, it is difficult to train models on small graph datasets and then evaluate their accuracy and execution efficiency on larger data corpus, a common scenario in industrial settings. Moreover, the framework and system scalability problems encountered with small datasets are different from those with large datasets, making it challenging to study the system requirements of GNNs.</p><p>To this end, this work proposes Illinois Graph Benchmark (IGB), a research dataset tool that the researchers can use to scrutinize and systematically evaluate the GNN models and their execution efficiency. IGB provides access to enormous graph datasets for deep learning research consisting of both homogeneous and heterogeneous graphs, providing a diverse range of graph structures for training and evaluating GNN models. IGB homogeneous graph dataset (IGB) has up to 269 million nodes and about four billion edges. IGB heterogeneous (IGBH) graph dataset has up to 547 million nodes and about six billion edges. Both of these datasets come with more than 220 million labeled nodes created with a novel approach and with two complex node classification tasks (19 and 2983 unique classes). Compared to the largest graph dataset publicly available <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>, IGB provides over 162? more labeled data, providing ample opportunities for GNN and deep learning practitioners to generalize the models to unseen data.</p><p>Furthermore, as a research tool, IGB offers flexible design choices for GNN model developer to investigate and systematically assess their models' performance and execution efficiency. IGB provides variable-sized embeddings, can generate embeddings from different language models and provides a range of variable-size graphs with consistent homophily. To demonstrate the usefulness of such flexibility, we conduct an extensive ablation study to show the impact of node embedding generation on the GNN model accuracy. We find that using the Roberta NLP embeddings provides better accuracy on GNN models and that a larger embedding dimension further assists in the GNN model accuracy. However, for users who are memory constrained, we show that applying PCA dimensionality reduction can reduce the embedding vectors from 1024-dimensions to 384dimensions, resulting in a 2.67? reduction in memory footprint with a maximum loss of 3.55% in the GNN model accuracy.</p><p>Lastly, this work also discusses the system-level challenges faced when training and inferring GNN models on large graph datasets like IGB. As the dataset size increases, we observe a reduction in the effective GPU utilization due to the increased time spent waiting for the embedding sampling and gathering operation to complete. This is particularly pronounced with the full-scale IGB datasets which require over 1TB of memory space in a system and necessitates memory mapping from the storage. Our profiling shows the existing systems fail to adequately support efficient training and inference of GNN models when the datasets exceed host CPU memory.</p><p>Overall this work makes the following key contributions:</p><p>(1) We propose IGB, a research dataset tool that innovatively fills the critical gap in labeling, features, heterogeneity, and size of public graph datasets. (2) We show IGB offers flexible design choices enabling the exploration of different GNN designs, embedding generation schemes, the effect of labeled data, and how system performance evolves with increasing dataset size.</p><p>IGB is compatible with popular GNN frameworks like DGL <ref type="bibr" target="#b61">[63]</ref> and PyG <ref type="bibr" target="#b18">[19]</ref>, and comes with several predefined popular models. IGB is available for public usage including raw text used to generate embedding and with a public leaderboard soon <ref type="bibr" target="#b29">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND AND MOTIVATION</head><p>In this section, we provide a brief overview of GNNs and their applications. We then cover existing GNN datasets and discuss the importance of high-quality, large-scale datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">GNN overview</head><p>GNNs are inspired by the idea of extending neural networks that are usually defined for vector inputs to graph-structured inputs.</p><p>The key idea of GNNs is to pass messages between the nodes in a graph. They use a neural network to propagate information along the edges of the graph while updating the representations of the nodes as the model learns. This enables the GNN to take into account the relationships between the nodes while learning their representations and properties. GNNs can be used to solve a wide variety of tasks such as node classification, link prediction, and graph classification, commonly found in applications such as fraud detection <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b71">73]</ref>, protein structure discovery <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b55">57]</ref>, and recommendation system <ref type="bibr" target="#b59">[61,</ref><ref type="bibr" target="#b68">70,</ref><ref type="bibr" target="#b69">71]</ref>.</p><p>GNNs can be applied to both homogeneous graphs and heterogeneous graphs. Homogeneous graphs consist of a single node type and a single relation, while heterogeneous graph consists of multiple types of nodes and multiple relations. Different types of GNN models have been developed to operate on these graphs. For homogeneous graphs, the most popular GNN models are graph convolutional network (GCN) <ref type="bibr" target="#b31">[32]</ref>, GraphSAGE <ref type="bibr" target="#b23">[24]</ref>, and graph attention network <ref type="bibr" target="#b57">[59]</ref>. These models primarily differ in how they pass messages between the nodes to learn the representation. For heterogeneous graphs, the most popular ones are relational-GCN <ref type="bibr" target="#b49">[51]</ref> and relational-GAT(RGAT) <ref type="bibr" target="#b10">[11]</ref>. While there are several GNN models, we can formalize all their update functions into a single equation:</p><formula xml:id="formula_0">? ? ? ? UPDATE(? ? ? , SAMPLE(? ?-1 ? ))<label>(1)</label></formula><p>Depending on the number of types of relations and neighbor sampling method we can derive respective updated functions of the popular graph neural net models as shown in </p><formula xml:id="formula_1">? ?? W ? UPDATE ? ?-1 ? R-GCN [51] ? ? ? ? ? SAMPLE ?? ? ?R ?? ? ?N ? (?) (1/? ?? )W r ? UPDATE ? ?-1 ? R-GAT [11] ? ? ? ? ? SAMPLE ?? ? ?R ?? ? ?N ? (?) ? ?? W r ? UPDATE ? ?-1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>Note that these equations ignore the self-node consideration and the bias term which remains the same for all models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">GNN Dataset Generation Techniques</head><p>With the increasing popularity of GNNs, researchers have opted to generate GNN datasets based on their needs to simulate and test their model performance. Some of the popular GNN dataset generation techniques include: (a) Real-world graph datasets: This technique takes an existing real-world database such as a transaction database or webpages and extracts graph structure from it <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b30">31]</ref>. (b) Graph sampling: This technique selects a subset of nodes and edges from existing large graphs to create a sample dataset <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b71">73]</ref>. (c) Synthetic graph generation: This method creates synthetic graphs by randomly connecting nodes according to certain probability distributions such as power law or by using Kronecker graph generators <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b42">43]</ref>. This technique can also be used as data augmentation to increase the dataset diversity. IGB datasets are created using real-world datasets and different configurations are created by performing graph sampling operations. We will describe the IGB dataset generation methodology in ?3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Existing Datasets And Their Problems</head><p>The existing dataset collection for GNNs are relatively small in size limiting the ability of GNN to generalize to unseen data <ref type="bibr">[3, 5-9, 14, 17, 22, 25, 33, 34, 39, 42, 50, 52-55, 58, 60, 66, 68, 69, 73]</ref>. More recently, work such as OGBN <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b35">36]</ref> have proposed datasets providing up to 244 million nodes and 1.7 billion edge graphs. These datasets contain a diverse set of graphs and have been widely used by the research community to benchmark GNN's performance. Table <ref type="table" target="#tab_2">2</ref> summarizes the various types of publicly disclosed graph datasets. We define flexibility in datasets as the ability to provide various configurations while maintaining homophily. This includes subgraphs with different node-degree distributions, variable-size embeddings, and different language models used to generate embeddings. Homophily refers to the tendency for individual nodes to be connected to other nodes with similar properties. It is fundamental for GNNs because it allows the model to learn a more accurate representation of the nodes in the graph. With datasets that have similar homophily, researchers can better understand and develop GNN models by training with smaller graphs and evaluating accuracy on larger graphs.</p><p>Table <ref type="table" target="#tab_2">2</ref> covers both synthetic and real large graph datasets that are used to study the GNN model performance in both closed and open-source environments. PinSAGE <ref type="bibr" target="#b68">[70]</ref> is one of the "few" publicly disclosed proprietary industrial datasets that have more than three billion nodes and eighteen billion edges with an unknown number of labels. Among the real-world open source datasets, MAG240M from OGB-LSC <ref type="bibr" target="#b25">[26]</ref> provides the maximum number of nodes to evaluate multi-class node classification tasks. Yet, as noted from Table <ref type="table" target="#tab_2">2</ref>, most existing datasets including OGBN datasets provide a tiny set of labeled data, provide no flexibility and the embeddings are generated in closed source.</p><p>As the GNN downstream tasks are often trained as supervised learning tasks, having large labeled data helps in increasing the accuracy of multi-label classification problems. As several prior works have shown <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b60">62]</ref>, with such a small number of usable labeled nodes during training, it becomes challenging to determine if the GNN's low accuracy for unseen data is primarily due to insufficient training data or inability to generalize.</p><p>Besides, it is crucial to have access to flexible datasets when training GNN models in order to fully understand the impact of various factors on their accuracy. One key aspect is the generation of embeddings associated with the nodes or edges in the graph. As NLP models are used to generate the embeddings for GNN models, the quality of the NLP model can have a direct effect on the GNN model's accuracy and their downstream tasks. Thus, the datasets should provide mechanisms to study different embedding generation techniques along with methods to vary their proprieties such as node embedding dimensions, and pruning.</p><p>Furthermore, as GNNs become increasingly popular, it is crucial to comprehend how their accuracy and efficiency (wall clock time) scale with the size and complexity of the graph. In this regard, researchers are developing optimized hardware and software frameworks tailored to the needs of the GNN applications <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b73">75]</ref>. However, the lack of flexible datasets hinders their ability to evaluate the scalability of their systems as the datasets grow larger. Ideally, a dataset should provide sub-datasets of varying sizes that maintain the same graph structural properties to enable such comprehensive research on GNN systems. This is because the challenges faced with small datasets differ from those encountered with large datasets.</p><p>For instance, training a small IGB graph can be completed within a reasonable time frame as the graph datasets and features can fit within a single system's memory. On the other hand, training a full version of the IGB heterogeneous dataset with a single system with state-of-the-art software stack is currently challenging due to large memory requirements and complex software management techniques. To fully understand the potential and scalability of GNNs, it is essential to study their performance on a range of graph sizes and complexities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">IGB DESIGN</head><p>Illinois Graph Benchmark (IGB) datasets are designed to address the limitations discussed in ? 2.3. IGB datasets are created using real-world data extracted from Microsoft Academic Graph [49] and SemanticScholar datasets <ref type="bibr" target="#b30">[31]</ref> as discussed in ? 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">IGB Overview</head><p>The IGB brings unique opportunities to assist in understanding the impact of dataset creation on the GNN model's accuracy. To this end, IGB addresses following set of challenges:</p><p>(1) Open data licensing and text accesses: The use of datasets containing graphs, text, and embeddings to train emerging GNN models <ref type="bibr" target="#b49">[51,</ref><ref type="bibr" target="#b65">67,</ref><ref type="bibr" target="#b70">72]</ref> is on the rise. However, data sources used to create the datasets must have open licensing for ease of adaption and derivative product creation. IGB meets open-data-license requirements (ODC-By-1.0) and provides a collection of datasets containing graphs, text, and embeddings for GNN and language model training. (2) Large ground-truth: IGB offers substantial ground-truth labels extracted from human-annotated data, eliminating the need for manually labeling millions of nodes. The majority of the IGB datasets are fully labeled, and the largest datasets have at least 40% of their nodes labeled, which is achieved by fusing multiple databases to form a large labeled dataset while preserving the accuracy of the information. (3) Flexibility for ablation study: The lack of flexible datasets limits the ability to evaluate the impact of the various components of the datasets on the performance of GNN models and understand their execution efficiency with different hardware and software systems. IGB overcomes this by offering a flexible research tool providing variable-sized embeddings, the ability to generate embeddings from different language models, and a range of variable-sized graphs with consistent homophily. (4) Task complexity: The IGB includes a range of multi-class classification tasks with varying degrees of complexity, which is crucial for evaluating the capabilities of GNN models. This is because while a GNN model can perform well on a binary classification task, it might not be effective for more finegrain classification tasks. This feature in IGB also enables the investigation of efficient transfer learning techniques for fine-tuning downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">IGB Dataset Generation Methodology</head><p>Generating IGB datasets involves curating data from various sources. Each of the IGB dataset has a graph, ground truth labels, and node embeddings. We will first describe how we generate the graph by extracting information from real-world data. Input Data: While there are a number of publicly available datasets that can be used as input source <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b51">53,</ref><ref type="bibr" target="#b58">60]</ref>, many of them are either small or lack the necessary information for building large-scale graph datasets for GNN application. Microsoft Academic Graph (MAG) [49] and SemanticScholar Corpus <ref type="bibr" target="#b30">[31]</ref> are two publicly available datasets that meet the criteria for building large graph datasets. The MAG database is particularly useful as it contains a wide range of relationships and information about different types of data points, including papers and authors, as well as information about paper citations, authorship, affiliations, and field of study. SemanticScholar corpus <ref type="bibr" target="#b30">[31]</ref>, while slightly smaller than MAG, also provides a similar set of functionalities. However, what particularly sets both of these datasets apart is the explicit permission we received to release the raw text data under the ODC-By-1.0 licensing scheme <ref type="foot" target="#foot_0">1</ref> .</p><p>The MAG comprises over 260 million entries for papers and about four billion relations representing a citation between two papers. The schema used by MAG has many tables among which paper, author, affiliation, URLs, conference/journals, and field of study tables are of interest. The paper table contains information such as title, published date, authors, citations, and names of journals or conferences where the article is published. The paper citations are stored in a file using coordinate (COO) graph storage format where the first paper ID cites the paired paper ID. The author table includes data on the author's name, affiliations, and papers they wrote. SemanticScholar corpus <ref type="bibr" target="#b30">[31]</ref> has a similar schema providing up to 205 million entries for papers and about three billion relations representing citation.</p><p>IGB Dataset Graphs: IGB provides two types of graphs: homogeneous and heterogeneous. The IGB homogeneous graph (IGB) is created by extracting only the paper nodes and the paper-citespaper relation between these nodes. The IGB heterogeneous graph (IGBH) is created by extracting four different types of nodes: papers, authors, institutions, and fields of study. These different types of nodes are connected by various types of edges as shown in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>IGB Ground-Truth Labels: The challenge of obtaining a large collection of real-world ground truth labels is a major issue pertaining to dataset generation especially when manual annotation at the million scales is unattainable. Currently, the largest graph datasets such as MAG240M <ref type="bibr" target="#b25">[26]</ref> and OGBN-paper100M utilize ArXiv <ref type="bibr" target="#b0">[1]</ref> labels and have a maximum of 1.4 million nodes categorized into 172 paper topics and eight subject areas. However, such a tiny set of labeled nodes is insufficient for real-world applications as the model cannot accurately predict unseen data.</p><p>To address this, IGB extracts human annotated label information from the MAG [49] and SemanticScholar <ref type="bibr" target="#b30">[31]</ref> databases. Extraction of information and aligning two databases pose a challenging problem. First, the two databases are distinct, and the nodes from both sides must be correctly aligned. We address this by leveraging the reverse mapping of MAG paper IDs from the SemanticScholar database and merging the databases to create the IGB graph.</p><p>Second, the two databases have different labeling methodologies. To handle this issue, we carefully create a union of distinct labels from the MAG and SemanticScholar databases. Third, the two databases can have different distinct labels for each paper and can create a merge conflict. We address this merge conflict by manually checking the very small number of nodes that have this issue and assigning the right label that appropriately defines the paper from the union of distinct labels from MAG and SemanticScholar databases. Using this merged database, IGB datasets can cover a large portion of the data with labeled nodes. As shown in Table <ref type="table" target="#tab_2">2</ref>, IGB provides more than 81% and 40% of nodes labeled for homogeneous and heterogeneous graphs.</p><p>IGB Downstream Tasks: The IGB dataset collections are designed for multi-class classification problems with two different numbers of classes (19 and 2983) depending on the degree of complexity. The 19 class task is curated by combining classes from MAG and SemanticScholar and mapping them into a common structure. Examples of 19-class labels are history, geology, economics, and many more. The 2983 class task is created by bucketing all papers with the same set of paper topics from the set of labels provided by SemanticScholar corpus. Examples of class labels are pediatrics, criminology, and computer engineering. The 19-class task is intended for model development and testing while the 2983-class task can be used to stress test the models and develop robust GNN models for noisy real-world data. Although the IGB dataset comprises node classification problems to solve, it is easy to extend the downstream task to train for edge prediction tasks such as citation recommendation or reviewer recommendation. For instance, an edge prediction task can be constructed by masking our existing edges in the graph, training an edge prediction model and use the masked edges as the ground truth output.</p><p>IGB Embedding Generation: GNN models operate on graph structure and their embeddings. Embeddings can be associated with a node or an edge and capture the nature of the nodes as well as the nature of relationship between the nodes and help the GNN modes to extract deeper semantic information from the graph. In the past, one-hot vectors and word dictionaries were used to generate these embeddings. However, with the introduction of word2vec embeddings and more recently deep learning-based word and text embeddings, GNNs are being initialized with embeddings generated using foundational language models.</p><p>For IGB datasets, we generate embeddings for each node in the graph <ref type="foot" target="#foot_1">2</ref> . Node embeddings are generated by passing the paper titles and abstracts through a Sentence-BERT model <ref type="bibr" target="#b47">[48]</ref> and obtaining a 1024-embedding vector for each paper node. Sentence-BERT, based on Siamese network, can generate semantically meaningful sentence embeddings by modifying a pre-trained BERT model <ref type="bibr" target="#b15">[16]</ref> such as RoBERTa <ref type="bibr" target="#b37">[38]</ref> or GPT <ref type="bibr" target="#b9">[10]</ref> while maintaining high accuracy with least run-time overheads.</p><p>As IGB graphs are extracted from scientific data <ref type="bibr" target="#b30">[31,</ref><ref type="bibr">49]</ref>, it is possible to create domain-specific embeddings instead of general language model-based embeddings. For scientific text, SPECTER <ref type="bibr" target="#b11">[12]</ref>, an embedding model created using SciBERT <ref type="bibr" target="#b3">[4]</ref>, a variant of BERT, can be used. SPECTER <ref type="bibr" target="#b11">[12]</ref> uses positive and negative samples of the papers from the SemanticScholar corpus <ref type="bibr" target="#b30">[31]</ref> to optimize the embeddings for the scientific text, leading to a 1.5% gain in the F1 score in the 19-class task compared to the Sentence-BERT model when tested on papers from the MAG dataset.</p><p>We chose to use sentence-BERT embeddings trained on web data as the default IGB embeddings, as we want to benchmark the GNN model's ability to extract structural information from the embeddings that do not contain inherent fine-tuning towards a specific domain. This also reflects the practical considerations of real-world industrial settings where fine-tuning language models for each domain-specific task is time-consuming and expensive.</p><p>The IGB author node embeddings are generated by taking the average of the node embeddings of all the papers written by the author, a methodology followed by Hu, et al. <ref type="bibr" target="#b25">[26]</ref>. Institute node embeddings are generated similarly taking the average of the node embeddings of all the authors affiliated with the particular institute. Field of study node embeddings is generated using the topic name provided in the field of study in the database.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">IGB Datasets</head><p>The IGB dataset suite offers five datasets for both homogeneous and heterogeneous graphs with varying sizes for deep learning practitioners as shown in Table <ref type="table" target="#tab_3">3</ref> and<ref type="table" target="#tab_4">Table 4</ref>. Each dataset is larger than the previous one (by about an order of magnitude) and is designed for a specific purpose. Each of the smaller datasets are created by carefully sampling the graph such that we maintain similar homophily across different dataset variations. Homophily of the IGB dataset varies from 47.75% to 59.93% consistent with the homophily of prior released citation graphs <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b25">26]</ref>. The tiny dataset is meant for testing and development of GNN models and can be run on a laptop or mobile or an edge device. The small and medium datasets are ideal for training and testing new GNN models during development and can be trained with lower to mid-end GPUs or a powerful CPU. The small and medium datasets are also representative of large-scale labeled datasets currently available to the public <ref type="bibr" target="#b25">[26]</ref>. The large dataset requires significant computing resources and can be trained on high-end accelerators and is suitable for building robust GNN models and testing by system designers. The full dataset is a massive dataset for GNN developers to construct practical models and stress test the distributed training platforms. Each dataset is initialized with 1024-dimensional node embeddings and has two different output classes that increase the difficulty to stress testing the GNN models and optimizing model development. The datasets are randomly split with 60% for training, and 20% each for validation and testing. The dataset also includes the year of publication metadata for every paper node in case the user wants to set a specific splitting rule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">IGB DATASET CASE STUDIES</head><p>The IGB dataset flexibility enables extensive ablation study to understand the impact of dataset generation technique on the GNN model performance. Through these case studies, we make the following key observations:</p><p>? GNN models for node classification tasks observe up to 12.96% boost in their accuracy as the fraction of labeled nodes in the dataset increased from 1/150 to fully labeled. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>Models: We present performance benchmarks for three commonly used GNN models (GCN <ref type="bibr" target="#b31">[32]</ref>, GraphSAGE <ref type="bibr" target="#b23">[24]</ref>, and GAT <ref type="bibr" target="#b57">[59]</ref>) on the IGB homogeneous dataset and for three popular GNN models (RGCN <ref type="bibr" target="#b49">[51]</ref>, RSAGE <ref type="foot" target="#foot_2">3</ref> and RGAT <ref type="bibr" target="#b10">[11]</ref>) on the IGB heterogeneous dataset. All models are trained with 0.01 learning rate with two layers. Most of the models are trained to three epochs and numbers are reported. Unless explicitly stated, we used a batch size of 10K to maximize GPU utilization and used IGB-medium as the default dataset. Lastly, most evaluations are reported with homogeneous IGB datasets but are equally applicable to heterogeneous datasets. System: We conducted our evaluations on a high-performance server system with dual AMD EPYC 7R32 processors, 256GB of DRAM, and an NVIDIA A100-40GB PCIe GPU with NVMe SSDs. The experiments were carried out using the DGLv0.9 framework <ref type="bibr" target="#b61">[63]</ref> built on top of PyTorch <ref type="bibr" target="#b43">[44]</ref> as obtained from the NVIDIA NGC repository. To reflect real-world cost considerations in industry, all experiments were run on a single EC2 instance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Impact Of Labeled Nodes</head><p>In Section 2.3, we emphasized the importance of labeled data. In this section, the impact of labeled data is evaluated for the IGB medium dataset. To achieve this, sub-datasets with varying fractions (i.e., 1/150, 1/100, and so on.) of labeled data were created and then  trained and evaluated using the GCN, GraphSAGE and GAT models on the 19-class node classification task. 1/150 labeled sub-dataset is representative of the largest publicly available labeled dataset, MAG240M <ref type="bibr" target="#b25">[26]</ref> and is created by picking one labelled node every 150 nodes. As shown in Figure <ref type="figure">2</ref>, the model accuracy improved by up to 10% as more labeled data was added. Increasing the classification task complexity from 19-class to 2983-class shows a similar trend in the performance. This is expected behavior as these models are trained as supervised learning tasks and more labeled data should help in boosting the model's performance.</p><p>The experiment was also performed on three sub-variants of the homogeneous dataset and on medium size IGB heterogeneous dataset. As shown in Table <ref type="table" target="#tab_6">5</ref>, adding more labeled data helps improve the model performance significantly even with the IGB dataset variants including heterogeneous datasets. This is promising mainly because, with datasets that are 100% labeled, the GNN developer can develop more accurate models and not be constrained by a lack of labeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Embedding Generation Ablation Study</head><p>Embeddings are key to the GNN model's performance but none of the prior work has shown how they impact GNN's performance. In this section, we will discuss the importance of embedding, and its generation process in-depth.</p><p>Random vs. NLP embeddings The GNN performance depends on the node information provided to the model, which is initialized as node embeddings. In this evaluation, we compared the difference in performance between randomly initialized vectors and RoBERTa <ref type="bibr" target="#b37">[38]</ref> NLP-based embeddings on three IGB homogeneous datasets as shown in Figure <ref type="figure" target="#fig_1">3</ref>. Each bar graph in Figure <ref type="figure" target="#fig_1">3</ref> is an average accuracy score of three GNN models (GCN, GraphSAGE, and GAT) on the respective datasets. Our results demonstrate that the NLP embeddings significantly increase the model's performance  Selecting Right Language Model For Embeddings The quality of embeddings generated by the language model can have a significant effect on the GNN model accuracy. To this end, we evaluated the GNN accuracy using a number of widely used language models (see Table <ref type="table" target="#tab_7">6</ref>) using the sentence transformer package provided by the Huggingface library <ref type="bibr" target="#b62">[64]</ref>. Table <ref type="table" target="#tab_7">6</ref> summarizes the average language model performance on NLP tasks, their respective model size, and generated text embedding dimensions.</p><p>Using these language models, we evaluate the performance of GNN models on the IGB-tiny dataset. To do this, we generated embeddings using each of the language models independently and trained all the GNN models using the respective language embeddings. Figure <ref type="figure" target="#fig_2">4</ref> shows the impact of these language models on the GNN accuracy. The average accuracy is calculated by averaging the reported accuracy of GCN, GraphSAGE, and GAT models. Although with this evaluation, it is not possible to concisely determine if the large performance range is due to the accuracy of the model or the embedding dimensions, it is clear that the language model itself has a direct impact on the GNN performance. In our next evaluation, we will isolate the impact of the model by reducing the dimensions of the embeddings.</p><p>Impact Of Embedding Dimension We normalized the differentsized embeddings from various language models using principal component analysis (PCA), a dimensionality reduction technique <ref type="bibr" target="#b19">[20]</ref>. We used the GPU implementation of PCA from the NVIDIA cuML library <ref type="bibr" target="#b45">[46]</ref>. The results of the experiments using GCN, Graph-SAGE, and GAT models with varying embedding dimensions are presented in Figure <ref type="figure">5</ref> and Figure <ref type="figure">6</ref>.</p><p>From Figure <ref type="figure">5</ref>, both mpnet <ref type="bibr" target="#b54">[56]</ref> and RoBERTa <ref type="bibr" target="#b37">[38]</ref> models outperform the other language models while training GNN. This is Figure <ref type="figure">6</ref>: GNN accuracy on RoBERTa <ref type="bibr" target="#b37">[38]</ref> embeddings when the dimensions are reduced from 1024 to 8 for IGB-small. Higher embedding dimensions provide better performance.</p><p>because both these NLP models are inherently better language models (see Table <ref type="table" target="#tab_7">6</ref>) and their respective accuracy improvements are reflected in the GNN model's accuracy.</p><p>From the figures, reducing the embedding dimensions using PCA had an impact on the accuracy across all the GNN models. This is not surprising as the PCA is a lossy pruning technique. For RoBERTa <ref type="bibr" target="#b37">[38]</ref> embeddings, reducing the dimensions from 1024 to 384 results in a 3.55%, 1.47%, and 0.58% reduction in accuracy for the GCN, GraphSAGE, and GAT models, respectively. The GAT model was found to be more resilient to reductions in dimensionality when compared to the GraphSAGE and GCN models. A similar observation is noted in other language models.</p><p>Reducing the dimensions from 1024 to 384 results in at least 2.67? memory capacity savings during training and inference operation with up to 3.55% loss in accuracy for RoBERTa embeddings. Further reducing the dimensionality offers significant memory footprint reduction but results in a significant drop in the GNN model performance as shown in Figure <ref type="figure">6</ref>. A similar set of observations are also seen with different IGB datasets including heterogeneous graphs and are not reported. Based on this analysis, the IGB dataset collection provides downloadable embeddings from the RoBERTa language model and provides a toolkit to generate other embeddings in our open-source codebase <ref type="bibr" target="#b29">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Input Language Influence</head><p>Scientific databases such as MAG <ref type="bibr">[49]</ref> and SemanticScholar <ref type="bibr" target="#b30">[31]</ref> consist of papers that are written in more than 80 languages. For instance, MAG <ref type="bibr">[49]</ref> database has more than 12 languages with over 1 million papers and only 50% of the total papers are written in English. In order to understand the impact of language on the accuracy of GNN models, it is important to consider the language models used to generate node embeddings, which are typically trained on web corpus data <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b37">38]</ref>.</p><p>Our goal is to determine if the language used in the dataset to train the NLP model makes any difference in the GNN performance. To do this, the study would require a language model trained exclusively on a specific language. However, finding such a pre-trained large language model is currently impossible. As a result, the study used multilingual language models that included or excluded specific languages.</p><p>We focused the evaluation on three languages: Japanese, Spanish, and French. Pre-trained multilingual models from the HuggingFace repository <ref type="bibr" target="#b62">[64]</ref> were used to create node embeddings. We used six different models, including MiniLM and mpnet, either trained in only English or multiple languages, including English.</p><p>We generated two types of embeddings: one using a language model trained in English and the other using a multi-lingual language model that included Japanese, Spanish, and French (multilingual embedding). The results, shown in Table <ref type="table" target="#tab_9">7</ref>, indicate that there is no significant performance improvement achieved by including specific languages, regardless of the model dimensions and language model used. We found similar results when running different embedding dimension models on the French and Spanish datasets. This makes us believe that the GNNs models are likely language agnostic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Overall Summary</head><p>The overall results of the IGB datasets for all the GNN models with two complex tasks are summarized in Table <ref type="table" target="#tab_10">8</ref> and Table <ref type="table" target="#tab_11">9</ref>. The IGB-tiny and small datasets are trained for 20 epochs while the rest are trained for three epochs due to the long training time. The accuracy of the models generally decreases in the more complex 2983 tasks as the model must identify finer differences in the nodes to classify them into a large number of classes. We observe up to 14.93% drop in performance for IGB homogeneous dataset. Longer training (more epochs) may help recover some of these accuracy drops, but ideally, better models are needed to handle the finer classification task.</p><p>Surprisingly the relational GNN models are tolerant to complex multi-class problems on the heterogeneous dataset. Despite an increase in task complexity from 19 classes to 2983 classes, the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Existing System Limitations</head><p>The existing system is unable to handle the complexity of training GNN models using the full IGB dataset. This is due to the large size of the embedding tables and graphs (see Table <ref type="table" target="#tab_3">3</ref> and<ref type="table" target="#tab_4">Table 4</ref>), which require large memory capacity. Despite the use of state-of-the-art systems and software, training GNN models on the full IGB dataset on a single system is still a challenge and time-consuming task. Let's briefly describe how GNN training occurs before discussing the system limitations.</p><p>GNN training operation can be split into three key stages: sampling, aggregation, and computation. The first stage in GNN training is to sample nodes or sub-graphs from the graph to form a minibatch. In the aggregation stage, information from the neighborhood of each node present in the mini-batch is aggregated to form a node representation. This process involves reading node embeddings and forming an aggregated representation for the node. In the final stage, the aggregated node representation is fed into a neural network to train the model.</p><p>As graphs and embeddings come in varying sizes, frameworks such as DGL <ref type="bibr" target="#b61">[63]</ref> and PyG <ref type="bibr" target="#b18">[19]</ref> provide a different mechanism to optimally place graphs and embeddings in the system for efficient training execution. If the dataset fits in the GPU memory (e.g. IGB(H)-tiny and IGB(H)-small), then both graphs and embeddings can be preloaded to the GPU memory and then GNN models can be trained. Our observation is that, when the datasets fit in the GPU memory, we achieve up to 80% GPU utilization (average 50%) during training.</p><p>If the dataset fits in the host CPU memory(e.g. IGB(H)-medium and IGB(H)-large), the graph is either placed in the GPU memory or the host memory, depending on the size of the graph, while the embeddings are loaded in the host memory. During the training operation, the GPU threads can directly sample the graphs from GPU memory and issue zero-copy memory-mapped I/O access to the embeddings using the DGL-UVA <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref> technique for efficient execution. Our measurement shows that, in this case, we achieve up to 80% GPU utilization (average 50%).</p><p>If the graph and embeddings do not fit in the host memory of a single system, as is the case of the IGB dataset, then embeddings can  be memory-mapped (mmap) and be stored in fast storage medium like NVMe SSDs. The intuition here is that, even though each mini-batch training requires a small working set size (?600MB for IGB(H)-full) in the GPU memory, the entire graph and embeddings need to be accessible during a training operation. This allows the frameworks like DGL <ref type="bibr" target="#b61">[63]</ref> and PyG <ref type="bibr" target="#b18">[19]</ref> to work on the embedding tables that exceed the host memory capacity of a single system. Figure <ref type="figure" target="#fig_3">7</ref> illustrates the distribution of the time spent for each of the three stages of training GCN and RGCN models on various IGBs graphs. As shown in Figure <ref type="figure" target="#fig_3">7</ref>, the use of mmap approach leads to a considerable slowdown, even when the graphs and embeddings fit in the host memory, as much as 4.5? (see IGB-large column). Unlike UVA, the mmap abstraction requires the CPU threads to perform the sampling and aggregation stages, accessing the embeddings stored in NVMe SSDs through the operating system page cache, leading to significant overhead. For IGB(H)-full graphs, the node aggregation stage consumes the most significant fraction of iteration time when using the mmap approach, causing low average GPU utilization, less than 5%. Profiling reveals that the mmap approach can only achieve up to 25% of storage bandwidth (1GBps) and is mainly limited by the system's page fault handler and page-cache throughput.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>This work introduces IGB, a research tool for deep learning practitioners to thoroughly evaluate and test GNN models with accuracy. It provides access to a large graph dataset that includes both homogeneous and heterogeneous graphs, with more than 40% of their nodes labeled. IGB has been designed to be flexible, offering options for the examination of various GNN architectures, embedding generation techniques, and analyzing system performance while training or inferencing GNN models. IGB is open-sourced, compatible with DGL and PyG frameworks, and includes raw text data that can inspire new research at the intersection of natural language processing and graph neural network research topics.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: IGB-Heterogeneous dataset schema.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: NLP embeddings help the GNN model to learn both structural and node properties while random embeddings can only learn from the structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: GNN accuracy on IGB-tiny using different NLP embedding models for initializing node embeddings.</figDesc><graphic url="image-1.png" coords="7,377.23,212.61,158.67,57.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Execution time breakdown of the three stages involved while training GCN and RGCN model. The majority of the time is spent either on the sampling operation or on the aggregation step.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>? (?) ? neighbor of node ? (for relation ? ? R). N (?) for ? = 1.</figDesc><table><row><cell>, where,</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Popular GNN models' update function comparison.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">SAMPLE</cell><cell></cell></row><row><cell>GCN [32]</cell><cell>? ? ? ? ?</cell><cell>??</cell><cell cols="2">(1/? ?? )W ?</cell><cell>? ?-1 ?</cell></row><row><cell></cell><cell></cell><cell cols="2">? ?N (?)</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">UPDATE</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">SAMPLE</cell></row><row><cell cols="4">GraphSage [24] ? ? ? ? ?W ? ???????? ?</cell><cell cols="2">? ?N ? (?) ? ?-1 ?</cell></row><row><cell></cell><cell cols="2">UPDATE</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">SAMPLE</cell><cell></cell></row><row><cell>GAT [59]</cell><cell cols="2">? ? ? ? ?</cell><cell>??</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">? ?N (?)</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison of IGB with the largest publicly disclosed existing dataset. *Labelled nodes as a percentage of total nodes. ????? implies the sizes are dependent on the values provided to the generator.</figDesc><table><row><cell>Dataset</cell><cell cols="3">Date Availability Type</cell><cell cols="3">#Nodes Labelled* #Edges</cell><cell cols="3">Dim RawText Flexibility</cell><cell>Task</cell></row><row><cell>PinSAGE [70]</cell><cell>2018</cell><cell>Private</cell><cell>Real</cell><cell>3000 M</cell><cell>UNK.</cell><cell>18 B</cell><cell>1024</cell><cell>No</cell><cell>No</cell><cell>Multi-class classification</cell></row><row><cell>papers100M [36]</cell><cell>2020</cell><cell>Public</cell><cell>Real</cell><cell>111 M</cell><cell>&lt; 1%</cell><cell>1.6 B</cell><cell>128</cell><cell>No</cell><cell>No</cell><cell>172-class classification</cell></row><row><cell>mag-240m [26]</cell><cell>2021</cell><cell>Public</cell><cell>Real</cell><cell>244 M</cell><cell>&lt; 1%</cell><cell>1.7 B</cell><cell>768</cell><cell>No</cell><cell>No</cell><cell>153-class classification</cell></row><row><cell>GraphWorld [43]</cell><cell>2022</cell><cell>Public</cell><cell>Syn</cell><cell>?????</cell><cell>UNK.</cell><cell>?????</cell><cell>N.A</cell><cell>No</cell><cell>No</cell><cell>System Design</cell></row><row><cell cols="2">SemanticScholar [31] 2023</cell><cell>Public</cell><cell>Real</cell><cell>205 M</cell><cell>UNK.</cell><cell>3 B</cell><cell>768</cell><cell>Yes</cell><cell>No</cell><cell>Multi-class classification</cell></row><row><cell>SynGen [13]</cell><cell>2023</cell><cell>Private</cell><cell>Syn</cell><cell>?????</cell><cell>N.A.</cell><cell>?????</cell><cell>N.A</cell><cell>No</cell><cell>Yes</cell><cell>System Design</cell></row><row><cell>IGB-Homogeneous</cell><cell>2023</cell><cell>Public</cell><cell cols="2">Real &gt; 260 M</cell><cell>&gt; 81%</cell><cell cols="2">4 B 128 to</cell><cell>Yes</cell><cell>Yes</cell><cell>19 or 2983-class classification</cell></row><row><cell>IGB-Heterogeneous</cell><cell>2023</cell><cell>Public</cell><cell cols="2">Real &gt; 547 M</cell><cell>&gt; 40%</cell><cell>6 B</cell><cell>1024</cell><cell>Yes</cell><cell>Yes</cell><cell>and System Design</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>IGB-Homogeneous dataset collection metrics. Maximum memory sizes are reported (embd.).</figDesc><table><row><cell>Dataset</cell><cell cols="2">#Nodes % Labeled</cell><cell>#Edges</cell><cell cols="2">Degree Homophily</cell><cell cols="2">Emb-dim #Classes Mem.(graph/embd./total)</cell></row><row><cell>IGB-tiny</cell><cell>100, 000</cell><cell>100</cell><cell>547, 416</cell><cell>234/1/5.47</cell><cell cols="2">56.79% 128 -1024 19/2983</cell><cell>6.9 MB/393 MB/400 MB</cell></row><row><cell>IGB-small</cell><cell>1, 000, 000</cell><cell>100</cell><cell>12, 070, 502</cell><cell>4, 292/1/12.07</cell><cell cols="2">47.75% 128 -1024 19/2983</cell><cell>185 MB/4.0 GB/4.1 GB</cell></row><row><cell>IGB-medium</cell><cell>10, 000, 000</cell><cell>100</cell><cell>120, 077, 694</cell><cell>22, 315/1/12.00</cell><cell cols="2">59.93% 128 -1024 19/2983</cell><cell>1.8 GB/39.0 GB/40.8 GB</cell></row><row><cell>IGB-large</cell><cell>100, 000, 000</cell><cell cols="2">100 1, 223, 571, 364</cell><cell>73, 248/1/12.20</cell><cell cols="2">58.27% 128 -1024 19/2983</cell><cell>19 GB/400 GB/401.8 GB</cell></row><row><cell>IGB</cell><cell>269, 346, 174</cell><cell cols="3">84.3 3, 995, 777, 033 277, 194/1/14.90</cell><cell cols="2">51.79% 128 -1024 19/2983</cell><cell>56 GB/1.1 TB/1.15 TB</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>IGB-Heterogeneous dataset collection metrics. Reported maximum memory sizes with 1024-dim embeddings (embd.).</figDesc><table><row><cell>Dataset</cell><cell cols="5">#Total Nodes #Paper Nodes #Author Nodes #Inst. Nodes #FoS Nodes</cell><cell cols="2">#Total Edges #Classes Mem.(graph/embd./total)</cell></row><row><cell>IGBH-tiny</cell><cell>549, 999</cell><cell>100, 000</cell><cell>357, 041</cell><cell>8, 738</cell><cell>84, 220</cell><cell>2, 062, 714 19/2983</cell><cell>31.5 MB/2.19 GB/2.2 GB</cell></row><row><cell>IGBH-small</cell><cell>3, 131, 266</cell><cell>1, 000, 000</cell><cell>1, 926, 066</cell><cell>14, 751</cell><cell>190, 449</cell><cell>26, 488, 616 19/2983</cell><cell>391 MB/12.16 GB/13 GB</cell></row><row><cell>IGBH-medium</cell><cell>25, 982, 964</cell><cell>10, 000, 000</cell><cell>15, 544, 654</cell><cell>23, 256</cell><cell>415, 054</cell><cell>249, 492, 193 19/2983</cell><cell>3.8 GB/100.8 GB/104 GB</cell></row><row><cell>IGBH-large</cell><cell>217, 636, 127</cell><cell>100, 000, 000</cell><cell>116, 959, 896</cell><cell>26, 524</cell><cell cols="2">649, 707 2, 104, 750, 425 19/2983</cell><cell>30.8 GB/844 GB/874 GB</cell></row><row><cell>IGBH</cell><cell>547, 306, 935</cell><cell>269, 346, 174</cell><cell>277, 220, 883</cell><cell>26, 918</cell><cell cols="2">712, 960 5, 812, 005, 639 19/2983</cell><cell>85 GB/2.2 TB/2.28 TB</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Impact of labeled nodes on IGB variants. Average accuracy across (R)GCN, (R)SAGE and (R)GAT models are reported for the 1/150 and full labeled (lbl) dataset.</figDesc><table><row><cell>Dataset</cell><cell cols="3"># Class Full-lbl acc 1/150 lbl acc</cell><cell>Diff.</cell></row><row><cell>IGB-medium</cell><cell>19</cell><cell>70.34%</cell><cell cols="2">58.15% 12.18</cell></row><row><cell>IGB-medium</cell><cell>2983</cell><cell>62.35%</cell><cell cols="2">49.39% 12.96</cell></row><row><cell>IGBH-medium</cell><cell>19</cell><cell>72.35%</cell><cell cols="2">61.66% 10.69</cell></row><row><cell>IGBH-medium</cell><cell>2983</cell><cell>72.46%</cell><cell>63.26%</cell><cell>9.20</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Selected Sentence Transformer Models</figDesc><table><row><cell cols="2">Dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="9">Emb dim Avg. Acc Model size</cell></row><row><cell cols="6">all-MiniLM-L6-v2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">384</cell><cell></cell><cell>58.80%</cell><cell></cell><cell></cell><cell></cell><cell>80 MB</cell></row><row><cell cols="10">distiluse-base-multilingual</cell><cell cols="2">512</cell><cell></cell><cell>45.59%</cell><cell></cell><cell></cell><cell cols="2">480 MB</cell></row><row><cell cols="6">all-mpnet-base-v2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">768</cell><cell></cell><cell>63.30%</cell><cell></cell><cell></cell><cell cols="2">420 MB</cell></row><row><cell cols="6">all-roberta-large-v1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">1, 024</cell><cell></cell><cell>61.64%</cell><cell></cell><cell cols="3">1, 360 MB</cell></row><row><cell></cell><cell>75</cell><cell></cell><cell></cell><cell></cell><cell cols="2">GCN</cell><cell cols="3">GraphSAGE</cell><cell>GAT</cell><cell></cell><cell cols="2">Avg</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GNN ACCURACY</cell><cell>55 60 65 70</cell><cell>66.33</cell><cell>70.57</cell><cell>67.6</cell><cell>68.17</cell><cell>64.51</cell><cell>68.03</cell><cell>65.09</cell><cell>65.88</cell><cell>67.79</cell><cell>71.24</cell><cell>68.26</cell><cell>69.1</cell><cell>68.06</cell><cell>71.87</cell><cell>68.92</cell><cell>69.62</cell></row><row><cell></cell><cell>50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="4">MINILM</cell><cell cols="4">DISTILUSE</cell><cell cols="4">MPNET</cell><cell cols="4">ROBERTA</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Average GNN accuracy with various languages. Language models have little effect on the accuracy of GNNs on different language inputs.</figDesc><table><row><cell>Model*</cell><cell cols="3">IGB-japanese IGB-spanish IGB-french</cell></row><row><cell>384 -???</cell><cell>62.89</cell><cell>59.77</cell><cell>60.39</cell></row><row><cell>384 -???</cell><cell>62.83</cell><cell>59.01</cell><cell>59.48</cell></row><row><cell>Average</cell><cell>62.86 ?0.03</cell><cell>59.39 ?0.38</cell><cell>59.94 ?0.46</cell></row><row><cell>768 -???</cell><cell>64.82</cell><cell>61.63</cell><cell>61.15</cell></row><row><cell>768 -???</cell><cell>64.74</cell><cell>61.77</cell><cell>61.25</cell></row><row><cell>Average</cell><cell>64.78 ?0.04</cell><cell>61.70 ?0.07</cell><cell>61.20 ?0.05</cell></row><row><cell>512 -?1</cell><cell>61.82</cell><cell>58.86</cell><cell>57.48</cell></row><row><cell>512 -?2</cell><cell>61.61</cell><cell>58.42</cell><cell>57.04</cell></row><row><cell>Average</cell><cell>61.72 ?0.11</cell><cell>58.64 ?0.22</cell><cell>57.26 ?0.22</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>IGB-Homogeneous datasets benchmark results using the same GNN model parameters (*trained for 3 epochs). N.G. stands for cannot finish training within time limit. ? .? . 64.89 ? .? . 64.59 ? .? . IGB* 48.59 ? .? . 54.95 ? .? . 55.51 ? .? . accuracy of relational GNN models does not decline. We believe the extra structural information present in the heterogeneous graph enables these models to classify with greater precision. Further investigation is necessary to fully comprehend this phenomenon.</figDesc><table><row><cell>Model</cell><cell cols="2">GCN</cell><cell cols="2">SAGE</cell><cell></cell><cell>GAT</cell></row><row><cell>Dataset (# class)</cell><cell>19</cell><cell>2983</cell><cell>19</cell><cell>2983</cell><cell>19</cell><cell>2983</cell></row><row><cell>IGB-tiny</cell><cell cols="6">68.06 53.13 71.87 59.49 68.92 51.74</cell></row><row><cell>IGB-small</cell><cell cols="6">70.46 63.28 75.49 68.70 70.93 63.70</cell></row><row><cell>IGB-medium*</cell><cell cols="6">70.63 62.70 70.35 62.55 70.06 61.81</cell></row><row><cell>IGB-large*</cell><cell>50.29</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>IGB-Heterogeneous datasets benchmark results using the same GNN model parameters(* trained for 3 epochs). N.G. stands for cannot finish training within time limit. .66 67.79 68.84 66.80 67.77 IGBH-small 71.35 71.64 72.54 72.60 72.61 72.27 IGBH-medium* 71.85 71.79 72.65 72.86 72.56 72.74 IGBH-large* ? .? . ? .? . ? .? . ? .? . ? .? . ? .? . IGBH* ? .? . ? .? . ? .? . ? .? . ? .? . ? .? .</figDesc><table><row><cell>Model</cell><cell></cell><cell cols="2">RGCN</cell><cell cols="2">RSAGE</cell><cell cols="2">RGAT</cell></row><row><cell cols="2">Dataset (# class)</cell><cell>19</cell><cell>2983</cell><cell>19</cell><cell>2983</cell><cell>19</cell><cell>2983</cell></row><row><cell cols="4">IGBH-tiny 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% Iteration Time Distribution 66.73 670% Sampling Aggregation</cell><cell cols="2">GPU Compute</cell><cell></cell><cell></cell></row><row><cell>UVA</cell><cell>MMAP</cell><cell>UVA</cell><cell>MMAP</cell><cell>UVA</cell><cell>MMAP</cell><cell>UVA</cell><cell>MMAP</cell></row><row><cell cols="2">IGB-large</cell><cell></cell><cell>IGB</cell><cell cols="2">IGBH-medium</cell><cell cols="2">IGBH</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>It is important to note that the MAG dataset is now fully deprecated and we thank the MAG database owners for giving us explicit open-sourcing permission.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Although IGB can generate edge embeddings, from the data management perspective it is quite challenging due to the sheer size of the generated dataset.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>RSAGE, a GraphSAGE extension to relational graphs is reported for the first time.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>We would like to acknowledge all of the help from members of the IMPACT research group, NVIDIA GNN team, and Amazon DGL team without which we could not have achieved any of the above reported results. Special thanks to <rs type="person">Dr. Zaid Qureshi</rs> from <rs type="funder">NVIDIA Research</rs> and <rs type="person">Jeongmin Park</rs> from the <rs type="funder">IMPACT</rs> research group for their valuable suggestion on writing and performance evaluations. We also would like to acknowledge feed-backs received anonymous reviewers and <rs type="person">Dr. Piotr Bigaj</rs> from <rs type="funder">NVIDIA</rs>. This work is partly funded by the <rs type="funder">Amazon Research Awards</rs>. This work uses GPUs donated by <rs type="funder">NVIDIA</rs>, and is partly supported by the <rs type="funder">IBM-ILLINOIS Center for Cognitive Computing Systems Research</rs> (<rs type="grantNumber">C3SR</rs>) and the <rs type="funder">IBM-ILLINOIS Discovery Accelerator Institute (IIDA)</rs>.</p></div>
			</div>
			<div type="funding">
<div><p>mei Hwu. 2023. IGB: Addressing The Gaps In Labeling, Features, Heterogeneity, and Size of Public Graph Datasets for Deep Learning Research. In Proceedings of in the 29th ACM SIGKDD Conference on Knowledge Discovery and <rs type="person">Data Mining</rs> (<rs type="affiliation">Under Review</rs>) (KDD). ACM, New York, NY, USA, 12 pages. https://doi.org/XXXXXXX.XXXXXXX</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_5adMCG5">
					<idno type="grant-number">C3SR</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<ptr target="https://arxiv.org/help/bulk_data" />
	</analytic>
	<monogr>
		<title level="j">ArXiv Bulk data</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hardware Acceleration of Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Auten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Tomei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rakesh</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="DOI">10.1109/DAC18072.2020.9218751</idno>
		<ptr target="https://doi.org/10.1109/DAC18072.2020.9218751" />
	</analytic>
	<monogr>
		<title level="m">2020 57th ACM/IEEE Design Automation Conference (DAC)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Graph Edit Distance Computation via Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Yunsheng</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.05689</idno>
		<ptr target="http://arxiv.org/abs/1808.05689" />
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1903.10676</idno>
		<ptr target="https://doi.org/10.48550/ARXIV.1903.10676" />
		<title level="m">SciBERT: A Pretrained Language Model for Scientific Text</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Kernel Methods for Mining Instance Data in Ontologies</title>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Bloehdorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">York</forename><surname>Sure</surname></persName>
		</author>
		<idno>ISWC&apos;07/ASWC&apos;07</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International The Semantic Web and 2nd Asian Conference on Asian Semantic Web Conference</title>
		<meeting>the 6th International The Semantic Web and 2nd Asian Conference on Asian Semantic Web Conference<address><addrLine>Busan, Korea; Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="58" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">970 Million Druglike Small Molecules for Virtual Screening in the Chemical Universe Database GDB-13</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Reymond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Chem. Soc</title>
		<imprint>
			<biblScope unit="volume">131</biblScope>
			<biblScope unit="page">8732</biblScope>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deep gaussian embedding of graphs: Unsupervised inductive learning via ranking</title>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.03815</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Translating Embeddings for Modeling Multirelational Data</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2013/file/1cecc" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">26</biblScope>
		</imprint>
	</monogr>
	<note>a77928ca8133fa24680a88d2f9-Paper.pdf</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Hello, It&apos;s GPT-2 -How Can I Help You? Towards the Use of Pretrained Language Models for Task-Oriented Dialogue Systems</title>
		<author>
			<persName><forename type="first">Pawe?</forename><surname>Budzianowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Vuli?</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1907.05774</idno>
		<ptr target="https://doi.org/10.48550/ARXIV.1907.05774" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Relational Graph Attention Networks</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Busbridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dane</forename><surname>Sherburn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Cavallo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nils</forename><forename type="middle">Y</forename><surname>Hammerla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.05811</idno>
		<ptr target="http://arxiv.org/abs/1904.05811" />
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">SPECTER: Document-level Representation Learning using Citationinformed Transformers</title>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2004.07180</idno>
		<ptr target="https://doi.org/10.48550/ARXIV.2004.07180" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Pawel Morkisz, and Alex Fit-Florea. 2022. A Framework for Large Scale Synthetic Graph Dataset Generation</title>
		<author>
			<persName><forename type="first">Sajad</forename><surname>Darabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bigaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawid</forename><surname>Majchrowski</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2210.01944</idno>
		<ptr target="https://doi.org/10.48550/ARXIV.2210.01944" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation with molecular orbital energies and hydrophobicity</title>
		<author>
			<persName><forename type="first">Asim</forename><surname>Kumar Debnath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rosa</forename><forename type="middle">L</forename><surname>Lopez De Compadre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gargi</forename><surname>Debnath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">J</forename><surname>Shusterman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corwin</forename><surname>Hansch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of medicinal chemistry</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="786" to="797" />
			<date type="published" when="1991">1991. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Convolutional 2D Knowledge Graph Embeddings</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<idno>AAAI&apos;18/IAAI&apos;18/EAAI&apos;18</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence</title>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Article 221, 8 pages</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1810.04805</idno>
		<ptr target="https://doi.org/10.48550/ARXIV.1810.04805" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">metapath2vec: Scalable representation learning for heterogeneous networks</title>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitesh</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 23rd ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="135" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Enhancing Graph Neural Network-based Fraud Detectors against Camouflaged Fraudsters</title>
		<author>
			<persName><forename type="first">Yingtong</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.08692</idno>
		<ptr target="https://arxiv.org/abs/2008.08692" />
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Fast Graph Representation Learning with PyTorch Geometric</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Eric Lenssen</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1903.02428</idno>
		<ptr target="https://doi.org/10.48550/ARXIV.1903.02428" />
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On lines and planes of closest fit to systems of points in space</title>
		<author>
			<persName><forename type="first">Karl</forename><surname>Pearson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R S</forename></persName>
		</author>
		<idno type="DOI">10.1080/14786440109462720</idno>
		<ptr target="https://doi.org/10.1080/14786440109462720" />
	</analytic>
	<monogr>
		<title level="j">The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="559" to="572" />
			<date type="published" when="1901">1901. 1901</date>
		</imprint>
	</monogr>
	<note>LIII</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distributed Deep Graph Learning at Scale</title>
		<author>
			<persName><forename type="first">Swapnil</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anand Padmanabha</forename><surname>Iyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Symposium on Operating Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">CiteSeer: An Automatic Citation Indexing System</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Lee</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><forename type="middle">D</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Lawrence</surname></persName>
		</author>
		<idno type="DOI">10.1145/276675.276685</idno>
		<ptr target="https://doi.org/10.1145/276675.276685" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third ACM Conference on Digital Libraries</title>
		<meeting>the Third ACM Conference on Digital Libraries<address><addrLine>Pittsburgh, Pennsylvania, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="89" to="98" />
		</imprint>
	</monogr>
	<note>DL &apos;98)</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Graphite: Optimizing Graph Neural Networks on CPUs through Cooperative Software-Hardware Techniques</title>
		<author>
			<persName><forename type="first">Zhangxiaowen</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houxiang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">W</forename><surname>Fletcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">J</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josep</forename><surname>Torrellas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual International Symposium on Computer Architecture</title>
		<meeting>the 49th Annual International Symposium on Computer Architecture<address><addrLine>New York, New York; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="916" to="931" />
		</imprint>
	</monogr>
	<note>ISCA &apos;22)</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02216</idno>
		<ptr target="http://arxiv.org/abs/1706.02216" />
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02216</idno>
		<ptr target="http://arxiv.org/abs/1706.02216" />
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maho</forename><surname>Nakata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2103.09430</idno>
		<ptr target="https://doi.org/10.48550/ARXIV.2103.09430" />
		<title level="m">OGB-LSC: A Large-Scale Challenge for Machine Learning on Graphs</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Open Graph Benchmark: Datasets for Machine Learning on Graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<ptr target="https://arxiv.org/abs/2005.00687" />
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">GPT-GNN: Generative Pre-Training of Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1145/3394486.3403237</idno>
		<ptr target="https://doi.org/10.1145/3394486.3403237" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery And Data Mining (Virtual Event, CA, USA) (KDD &apos;20)</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery And Data Mining (Virtual Event, CA, USA) (KDD &apos;20)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1857" to="1867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Iterative Refinement Graph Neural Network for Antibody Sequence-Structure Co-design</title>
		<author>
			<persName><forename type="first">Wengong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Wohlwend</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=LI2bhrE_2A" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Arpandeep</forename><surname>Khatua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikram</forename><surname>Sharma Mailthody</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhagyashree</forename><surname>Taleka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Mei</forename><surname>Hwu</surname></persName>
		</author>
		<ptr target="https://github.com/IllinoisGraphBenchmark/IGB-Datasets" />
		<title level="m">IGB Datasets for public release with leaderboard</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Rodney</forename><surname>Kinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chloe</forename><surname>Anastasiades</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russell</forename><surname>Authur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Bragg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Buraczynski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabel</forename><surname>Cachola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Candra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoganand</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miles</forename><surname>Crawford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Dunkelberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gorney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangzhou</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regan</forename><surname>Huff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Kohlmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bailey</forename><surname>Kuehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Langan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haokun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaron</forename><surname>Lochner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelsey</forename><surname>Macmillan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tyler</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Smita</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaurya</forename><surname>Rohatgi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Sayre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zejiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Soldaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivashankar</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amber</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">D</forename><surname>Wade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linda</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucy</forename><forename type="middle">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Wilhelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caroline</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangjiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angele</forename><surname>Zamarron</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2301.10140</idno>
		<ptr target="https://doi.org/10.48550/ARXIV.2301.10140" />
		<title level="m">The Semantic Scholar Open Data Platform</title>
		<imprint>
			<publisher>Madeleine Van Zuylen, and Daniel S. Weld</publisher>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<ptr target="http://arxiv.org/abs/1609.02907" />
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The suitesparse matrix collection website interface</title>
		<author>
			<persName><forename type="first">Mohsen</forename><surname>Scott P Kolodziej</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Aznaveh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jarrett</forename><surname>Bullock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Read</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><surname>Sandstrom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Open Source Software</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">1244</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rev2: Fraudulent user prediction in rating platforms</title>
		<author>
			<persName><forename type="first">Srijan</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Hooi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Disha</forename><surname>Makhija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName><surname>Vs Subrahmanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Eleventh ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="333" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">SmartSAGE: Training Large-Scale Graph Neural Networks Using in-Storage Processing Architectures</title>
		<author>
			<persName><forename type="first">Yunjae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinha</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minsoo</forename><surname>Rhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual International Symposium on Computer Architecture</title>
		<meeting>the 49th Annual International Symposium on Computer Architecture<address><addrLine>New York, New York; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="932" to="945" />
		</imprint>
	</monogr>
	<note>ISCA &apos;22)</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Krevl</surname></persName>
		</author>
		<ptr target="http://snap.stanford.edu/data" />
		<title level="m">SNAP Datasets: Stanford Large Network Dataset Collection</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Informative Pseudo-Labeling for Graph Neural Networks with Few Labels</title>
		<author>
			<persName><forename type="first">Yayong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.07951</idno>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1907.11692</idno>
		<ptr target="https://doi.org/10.48550/ARXIV.1907.11692" />
		<title level="m">RoBERTa: A Robustly Optimized BERT Pretraining Approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucy</forename><forename type="middle">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodney</forename><surname>Kinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.02782</idno>
		<title level="m">S2ORC: The semantic scholar open research corpus</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">EMOGI: Efficient Memory-access for Out-ofmemory Graph-traversal In GPUs</title>
		<author>
			<persName><forename type="first">Vikram</forename><surname>Seung Won Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaid</forename><surname>Sharma Mailthody</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinjun</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eiman</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Mei</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName><surname>Hwu</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2006.06890</idno>
		<ptr target="https://doi.org/10.48550/ARXIV.2006.06890" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">PyTorch-Direct: Enabling GPU Centric Data Access for Very Large Graph Neural Network Training with Irregular Accesses</title>
		<author>
			<persName><forename type="first">Seung</forename><surname>Won</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sitao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mert</forename><surname>Hidayeto?lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eiman</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Mei</forename><surname>Hwu</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2101.07956</idno>
		<ptr target="https://doi.org/10.48550/ARXIV.2101.07956" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">TUDataset: A collection of benchmark datasets for learning with graphs</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nils</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franka</forename><surname>Bause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristian</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petra</forename><surname>Mutzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08663</idno>
		<ptr target="https://arxiv.org/abs/2007.08663" />
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">GraphWorld: Fake Graphs Bring Real Insights for GNNs</title>
		<author>
			<persName><forename type="first">John</forename><surname>Palowitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Tsitsulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<idno type="DOI">10.1145/3534678.3539203</idno>
		<ptr target="https://doi.org/10.1145/3534678.3539203" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>K?pf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zach</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1912.01703</idno>
		<ptr target="https://doi.org/10.48550/ARXIV.1912.01703" />
		<title level="m">PyTorch: An Imperative Style, High-Performance Deep Learning Library</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Know-GNN: An Explainable Knowledge-Guided Graph Neural Network for Fraud Detection</title>
		<author>
			<persName><forename type="first">Yizhuo</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianya</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengyuan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoguang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongliang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhixian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wei</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-92307-5_19</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-92307-5_19" />
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="159" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Raschka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corey</forename><surname>Nolet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.04803</idno>
		<title level="m">Machine Learning in Python: Main developments and technology trends in data science, machine learning, and artificial intelligence</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Collective Opinion Spam Detection: Bridging Review Networks and Metadata</title>
		<author>
			<persName><forename type="first">Shebuti</forename><surname>Rayana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>Sydney, NSW, Australia; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="985" to="994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</title>
		<author>
			<persName><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.10084</idno>
		<ptr target="http://arxiv.org/abs/1908.10084" />
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Multi-scale Attributed Node Embedding</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Benedek Rozemberczki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rik</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><surname>Sarkar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.13021</idno>
		<ptr target="http://arxiv.org/abs/1909.13021" />
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Modeling Relational Data with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1703.06103</idno>
		<ptr target="https://doi.org/10.48550/ARXIV.1703.06103" />
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European semantic web conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="93" to="93" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Collective Classification in Network Data</title>
		<author>
			<persName><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Galileo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tina</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="93" to="106" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Pitfalls of Graph Neural Network Evaluation. Relational Representation Learning Workshop</title>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Mumme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2018</biblScope>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">MPNet: Masked and Permuted Pre-training for Language Understanding</title>
		<author>
			<persName><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2004.09297</idno>
		<ptr target="https://doi.org/10.48550/ARXIV.2004.09297" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">EquiBind: Geometric Deep Learning for Drug Binding Structure Prediction</title>
		<author>
			<persName><forename type="first">Hannes</forename><surname>St?rk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Octavian-Eugen</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lagnajit</forename><surname>Pattanaik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2202.05146</idno>
		<ptr target="https://doi.org/10.48550/ARXIV.2202.05146" />
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Representing text for joint embedding of text and knowledge bases</title>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pallavi</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Gamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 conference on empirical methods in natural language processing</title>
		<meeting>the 2015 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1499" to="1509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1710.10903</idno>
		<ptr target="https://doi.org/10.48550/ARXIV.1710.10903" />
		<title level="m">Graph Attention Networks</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">The Semantic Scholar Academic Graph (S2AG)</title>
		<author>
			<persName><forename type="first">Alex D</forename><surname>Wade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Companion Proceedings of the Web Conference</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
			<biblScope unit="page" from="739" to="739" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Next-Item Recommendation with Sequential Hypergraphs</title>
		<author>
			<persName><forename type="first">Jianling</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaize</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangjie</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Caverlee</surname></persName>
		</author>
		<idno type="DOI">10.1145/3397271.3401133</idno>
		<ptr target="https://doi.org/10.1145/3397271.3401133" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (Virtual Event, China) (SIGIR &apos;20)</title>
		<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (Virtual Event, China) (SIGIR &apos;20)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1101" to="1110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Improving graph-based label propagation algorithm with group partition for fraud detection</title>
		<author>
			<persName><forename type="first">Jiahui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinxiu</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minwei</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10489-020-01724-1</idno>
		<idno>10489- 020-01724-1</idno>
		<ptr target="https://doi.org/10.1007/s" />
	</analytic>
	<monogr>
		<title level="j">Applied Intelligence</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<date type="published" when="2020-10">2020. 10 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Deep Graph Library: A Graph-Centric, Highly-Performant Package for Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mufei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinjing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingfan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujie</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianjun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<idno>arXiv: Learning</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R?mi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1910.03771</idno>
		<ptr target="https://doi.org/10.48550/ARXIV.1910.03771" />
		<title level="m">HuggingFace&apos;s Transformers: State-of-the-art Natural Language Processing</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A Comprehensive Survey on Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNNLS.2020.2978386</idno>
		<ptr target="https://doi.org/10.1109/TNNLS.2020.2978386" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="4" to="24" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">How Powerful are Graph Neural Networks?</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<ptr target="http://arxiv.org/abs/1810.00826" />
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Deep Bidirectional Language-Knowledge Graph Pretraining</title>
		<author>
			<persName><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xikun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Local Higher-Order Graph Clustering</title>
		<author>
			<persName><forename type="first">Austin</forename><forename type="middle">R</forename><surname>Hao Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Benson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">F</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><surname>Gleich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>Halifax, NS, Canada; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="555" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">GNN Explainer: A Tool for Post-hoc Explanation of Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dylan</forename><surname>Bourgeois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.03894</idno>
		<ptr target="http://arxiv.org/abs/1903.03894" />
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Graph Convolutional Neural Networks for Web-Scale Recommender Systems</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01973</idno>
		<ptr target="http://arxiv.org/abs/1806.01973" />
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Performance-Adaptive Sampling Strategy Towards Fast and Accurate Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Minji</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Th?ophile</forename><surname>Gervet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baoxu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sufeng</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewon</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3447548.3467284</idno>
		<ptr target="https://doi.org/10.1145/3447548.3467284" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (Virtual Event, Singapore) (KDD &apos;21)</title>
		<meeting>the 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (Virtual Event, Singapore) (KDD &apos;21)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2046" to="2056" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<author>
			<persName><forename type="first">Seongjun</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minbyul</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raehyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunwoo</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1911.06455</idno>
		<ptr target="https://doi.org/10.48550/ARXIV.1911.06455" />
		<title level="m">Graph Transformer Networks</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">GCN-Based User Representation Learning for Unifying Robust Recommendation and Fraudster Detection</title>
		<author>
			<persName><forename type="first">Shijie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10150</idno>
		<ptr target="https://arxiv.org/abs/2005.10150" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Quoc Viet Hung Nguyen, Zi Huang, and Lizhen Cui</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">DistDGL: Distributed Graph Neural Network Training for Billion-Scale Graphs</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM 10th Workshop on Irregular Applications: Architectures and Algorithms (IA</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="36" to="44" />
			<date type="published" when="2020">2020. 2020</date>
			<publisher>IEEE Computer Society</publisher>
			<pubPlace>Los Alamitos, CA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Accelerating Large Scale Real-Time GNN Inference Using Channel Pruning</title>
		<author>
			<persName><forename type="first">Hongkuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajitesh</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanqing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajgopal</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viktor</forename><surname>Prasanna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB Endow</title>
		<meeting>VLDB Endow</meeting>
		<imprint>
			<date type="published" when="2021-10">2021. oct 2021</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1597" to="1605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Graph neural networks: A review of methods and applications</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ganqu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengding</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changcheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Open</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="57" to="81" />
			<date type="published" when="2020-02-26">2020. 2020. 26 February 2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
