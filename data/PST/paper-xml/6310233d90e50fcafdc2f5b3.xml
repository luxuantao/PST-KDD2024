<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Optimizing Bi-Encoder for Named Entity Recognition via Contrastive Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hao</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Optimizing Bi-Encoder for Named Entity Recognition via Contrastive Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present an efficient bi-encoder framework for named entity recognition (NER), which applies contrastive learning to map candidate text spans and entity types into the same vector representation space. Prior work predominantly approaches NER as sequence labeling or span classification. We instead frame NER as a metric learning problem that maximizes the similarity between the vector representations of an entity mention and its type. This makes it easy to handle nested and flat NER alike, and can better leverage noisy self-supervision signals. A major challenge to this bi-encoder formulation for NER lies in separating nonentity spans from entity mentions. Instead of explicitly labeling all non-entity spans as the same class Outside (O) as in most prior methods, we introduce a novel dynamic thresholding loss, which is learned in conjunction with the standard contrastive loss. Experiments show that our method performs well in both supervised and distantly supervised settings, for nested and flat NER alike, establishing new state of the art across standard datasets in the general domain (e.g., ACE2004, ACE2005) and highvalue verticals such as biomedicine (e.g., GENIA, NCBI, BC5CDR, JNLPBA).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Named entity recognition (NER) is the task of identifying text spans associated with proper names and classifying them into a predefined set of semantic classes such as person, location, organization, etc. As a fundamental component in information extraction systems <ref type="bibr" target="#b38">(Nadeau and Sekine, 2007)</ref>, NER has been shown to be of benefit for a number of downstream tasks such as relation extraction <ref type="bibr" target="#b37">(Mintz et al., 2009)</ref>, coreference resolution <ref type="bibr" target="#b3">(Chang et al., 2013)</ref>, and fine-grained opinion mining <ref type="bibr" target="#b5">(Choi et al., 2006)</ref>.</p><p>Inspired by its recent success of open-domain question answering <ref type="bibr" target="#b21">(Karpukhin et al., 2020)</ref>, and entity linking <ref type="bibr" target="#b56">(Wu et al., 2020;</ref><ref type="bibr">Zhang et al., 2021a)</ref>, we propose an efficient BI-encoder for NameD Entity Rcognition (BINDER). Our model employs two encoders to separately map text and entity types into the same vector space, and it is able to reuse the vector representations of text for different entity types (or vice versa), resulting in a faster training and inference speed. Based on the vector outputs of bi-encoder, we propose to use span-based contrastive learning for NER, which differs us from existing work that mostly formulates NER as a classification problem (e.g., <ref type="bibr" target="#b4">Chiu and Nichols, 2016;</ref><ref type="bibr" target="#b33">Ma and Hovy, 2016;</ref><ref type="bibr" target="#b27">Li et al., 2020;</ref><ref type="bibr" target="#b13">Fu et al., 2021)</ref>. <ref type="foot" target="#foot_0">1</ref> Through contrastive learning, we force the representation of entity type to be similar with the positive entity spans, and to be dissimilar with the negative text spans. On the other hand, existing work labels all non-entity tokens/spans as the same class Outside (O), which can introduce false negative noises when the training data is partially annotated <ref type="bibr" target="#b8">(Das et al., 2022;</ref><ref type="bibr" target="#b1">Aly et al., 2021)</ref>. Instead of explicitly labeling non-entity spans as O, we introduce a novel dynamic thresholding strategy in contrastive learning, which learn dynamic thresholds to distinguish entity spans from non-entity spans.</p><p>To the best of our knowledge, we are the first to optimize bi-encoder for NER via contrastive learning. We conduct extensive experiments to evaluate our method in both supervised and distantly supervised settings. The experimental results demonstrate that our method achieves the state of the art on a wide range of NER datasets, covering both the general domains and the biomedical domains. In supervised NER, compared to the previous state of the art, our method obtains a 1.4%-2.4% abso-lute improvement in F1 on standard nested NER benchmarks such as ACE2004 and ACE2005, and a 1.2%-1.9% absolute improvement on standard flat NER benchmarks such as BC5-chem, BC5disease, and NCBI. In distantly supervised NER, our method obtains a 1.5% absolute improvement in F1 on the BC5CDR dataset. We further study the impact of various choices of components in our method, and conduct breakdown analysis at entity type level and token level, which suggests further room for improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Supervised NER Early techniques for NER are based on hidden markov models (e.g., <ref type="bibr" target="#b65">Zhou and Su, 2002)</ref> and conditional random fields (CRFs) (e.g., <ref type="bibr" target="#b35">McDonald and Pereira 2005)</ref>. However, due to the inability to handle nested named entities, techniques such as cascaded CRFs <ref type="bibr" target="#b0">(Alex et al., 2007)</ref>, adpated constituency parsing <ref type="bibr" target="#b11">(Finkel and Manning, 2009)</ref>, and hypergraphs <ref type="bibr" target="#b31">(Lu and Roth, 2015)</ref> are developed for nested NER. More recently, with the advance in deep learning, a myriad of new techniques have been used in supervised NER. Depending on the way they formulate the task, these techniques can be categorized as NER as sequence labeling <ref type="bibr" target="#b4">(Chiu and Nichols, 2016;</ref><ref type="bibr" target="#b33">Ma and Hovy, 2016;</ref><ref type="bibr" target="#b22">Katiyar and Cardie, 2018)</ref>; NER as parsing <ref type="bibr" target="#b24">(Lample et al., 2016;</ref><ref type="bibr" target="#b61">Yu et al., 2020)</ref>; NER as span classification <ref type="bibr" target="#b46">(Sohrab and Miwa, 2018;</ref><ref type="bibr" target="#b57">Xia et al., 2019;</ref><ref type="bibr" target="#b40">Ouchi et al., 2020;</ref><ref type="bibr" target="#b13">Fu et al., 2021)</ref>; NER as a sequence-to-sequence problem <ref type="bibr" target="#b47">(Strakov? et al., 2019;</ref><ref type="bibr" target="#b58">Yan et al., 2021)</ref>; and NER as machine reading comprehension <ref type="bibr" target="#b27">(Li et al., 2020;</ref><ref type="bibr" target="#b36">Mengge et al., 2020)</ref>. Unlike previous work, we formulate NER as a metric learning problem. The span-based design of our bi-encoder and contrastive loss provides us with the flexibility to handle both nested and flat NER.</p><p>Distantly Supervised NER Distant supervision from external knowledge bases in conjunction with unlabeled text is generated by string matching <ref type="bibr" target="#b14">(Giannakopoulos et al., 2017)</ref> or heuristic rules <ref type="bibr" target="#b42">(Ren et al., 2015;</ref><ref type="bibr" target="#b12">Fries et al., 2017)</ref>. Due to the limited coverage of external knowledge bases, distant supervision often has a high false negative rate. To alleviate this issue, <ref type="bibr" target="#b43">Shang et al. (2018)</ref> design a new tagging scheme with an unknown tag specifically for false negatives; <ref type="bibr" target="#b34">Mayhew et al. (2019)</ref> iteratively detect false negatives and downweigh them in training; <ref type="bibr" target="#b41">Peng et al. (2019);</ref><ref type="bibr" target="#b66">Zhou et al. (2022)</ref> address overfitting to false negatives using Positive and Unlabeled (PU) learning; <ref type="bibr">Zhang et al. (2021b)</ref> identify dictionary biases via a structural causal model, and de-bias them using causal interventions. <ref type="bibr" target="#b29">Liu et al. (2021)</ref> introduce a calibrated confidence estimation method and integrate it into a self-training framework. Without replying on sophisticated de-noising designs, our biencoder framework can be directly used in distant supervision. Experiments in ?4.4 will show the robustness of our contrastive learning algorithm to the noise in distantly supervised NER.</p><p>Bi-Encoder The use of bi-encoder dates back to <ref type="bibr">Bromley et al. (1993)</ref> for signature verification and <ref type="bibr" target="#b6">Chopra et al. (2005)</ref> for face verification. These works and their descendants (e.g., <ref type="bibr" target="#b60">Yih et al., 2011;</ref><ref type="bibr" target="#b18">Hu et al., 2014)</ref> refer to the architecture as siamese networks since two similar inputs are encoded separately by two copies of the same network (all parameters are shared). Wsabie <ref type="bibr" target="#b53">(Weston et al., 2010)</ref> and StarSpace <ref type="bibr" target="#b55">(Wu et al., 2018)</ref> subsequently employ bi-encoder to learn embeddings for different data types. Using deep pretrained transformers as encoders, <ref type="bibr" target="#b20">Humeau et al. (2019)</ref> compare three different architectures, biencoder, poly-encoder, and cross-encoder. The biencoder architecture afterwards has been use in various tasks, e.g., information retrieval <ref type="bibr" target="#b19">(Huang et al., 2013;</ref><ref type="bibr" target="#b16">Gillick et al., 2018)</ref>, open-domain question answering <ref type="bibr" target="#b21">(Karpukhin et al., 2020)</ref>, and entity linking <ref type="bibr" target="#b15">(Gillick et al., 2019;</ref><ref type="bibr" target="#b56">Wu et al., 2020;</ref><ref type="bibr">Zhang et al., 2021a)</ref>. To our best knowledge, there is no previous work learning bi-encoder for NER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we present the design of BINDER, a novel architecture for NER tasks. As our model is built upon a bi-encoder framework, we first provide the necessary background for encoding both entity types and text using the Transformer-based <ref type="bibr" target="#b49">(Vaswani et al., 2017)</ref> bi-encoder. Then, we discuss our ways of deriving entity type and individual mention span representations using embeddings output from the bi-encoder. Based on that, we introduce two types of contrastive learning objectives for NER using the token and span-level similarity respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Bi-Encoder for NER</head><p>The overall architecture of BINDER is shown in Figure <ref type="figure">1</ref>. Our model is built upon a bi-encoder Vector Space</p><p>Figure <ref type="figure">1</ref>: The overall architecture of BINDER. The entity type and text encoder are isomorphic and fully decoupled Transformer models. In the vector space, the anchor point ( ) represents the special token [CLS] from the entity type encoder. Through contrastive learning, we maximize the similarity between the anchor and the positive token ( Jim), and minimize the similarity between the anchor and negative tokens. The dotted gray circle (delimited by the similarity between the anchor and [CLS] from the text encoder) represents a threshold that separates entity tokens from non-entity tokens. To reduce clutter, we do not draw data points that represent possible spans from the input text.</p><p>architecture which has been mostly explored for dense retrieval <ref type="bibr" target="#b21">(Karpukhin et al., 2020)</ref>. Following the recent work, our bi-encoder also consists of two isomorphic and fully decoupled Transformer models <ref type="bibr" target="#b49">(Vaswani et al., 2017)</ref>, i.e. an entity type encoder and a text encoder. For NER tasks, we consider two types of inputs, entity type descriptions and text to detect named entities. At the high level, the entity type encoder produces type representations for each entity of interests (e.g. person in Figure <ref type="figure">1</ref>) and the text encoder outputs representations for each input token in the given text where named entities are potentially mentioned (e.g. Jim in Figure <ref type="figure">1</ref>). Then, we enumerate all span candidates based on corresponding token representations and match them with each entity type in the vector space. As shown in Figure <ref type="figure">1</ref>, we maximize the similarity between the positive span and its corresponding entity type, and minimize the similarity of negative spans. We first formally discuss encoding both inputs using a pretrained Transformer model, BERT <ref type="bibr" target="#b9">(Devlin et al., 2019)</ref>. 2 Specifically, we use x 1 , . . . , x N to denote an input sequence of length N . When using BERT, there is a prepended to-2 Although different BERT variants are considered later in experiments, they all follow the same way of encoding discussed here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ken [CLS] and an appended token</head><formula xml:id="formula_0">[SEP]for all input sequences, i.e. [CLS], x 1 , . . . , x N [SEP].</formula><p>Then the output is a sequence of hidden states h [CLS] , h 1 , . . . , h N , h [SEP] ? R d from the last BERT layer for each input token, where d is the hidden dimension. Note that as [CLS]is always in the beginning, h 0 and h [CLS] are interchangeable here. Based on this, we then discuss the way of computing entity type and text token embeddings, which are the basic building blocks for deriving our NER constrative learning objectives later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Entity Type Embeddings</head><p>The goal of entity type encoder is to produce entity type embeddings that serve as anchors in the vector space for contrastive learning. In this work, we focus on a predefined set of entity types E = {E 1 , . . . , E K }, where each entity type has one or multiple natural language descriptions. The natural language description can be formal textual definitions based on the dataset annotation guideline or Wikipedia, and prototypical instances where a target type of named entities are mentioned. For simplicity, the discussion proceeds with one description per type and we use E k to denote a sequence of tokens for the k-th entity type description. For a given entity type E k , we use BERT as the entity type encoder (BERT E ) and add an additional linear projection to compute corresponding entity type embeddings in the following way:</p><formula xml:id="formula_1">h E k [CLS] = BERT E (E k ),<label>(1)</label></formula><formula xml:id="formula_2">e k = Linear E (h E k [CLS] ),<label>(2)</label></formula><p>where Linear is a learnable linear projection layer and e k ? R d is the final vector representation for entity type E k .</p><p>Text Token Embeddings Instead of using [CLS]embeddings as done in the recent biencoder work for entity retrieval <ref type="bibr" target="#b56">(Wu et al., 2020)</ref>, we consider using text token embeddings as the basic unit for computing similarity with entity span embeddings. As there are multiple potential named entities not known as a prior in the input text, naively using special markers <ref type="bibr" target="#b56">(Wu et al., 2020)</ref> incurs huge computation overhead for NER tasks. Similar to the entity type embeddings, we again use BERT as the text encoder (BERT T ) and a linear layer for computing token representations 3</p><formula xml:id="formula_3">h T 1 , . . . , h T N = BERT T (x 1 , . . . , x N ), (3) p n = Linear T (h T n ),<label>(4)</label></formula><p>where Linear T is a learnable linear layer and p n ? R d is the vector representation for the nth token in the input sentence. Again, as [CLS]is always at the start, we use p 0 to denote p [CLS] for simplicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">NER Contrastive Learning</head><p>Based on the entity type embeddings and text token embeddings discussed above, we then introduce two different contrastive learning objectives for NER in this part. Here, we assume a span (i, j) is a contiguous sequence of tokens in the input text with a start token in position i and an end token in position j. Throughout this work, we use the similarity function, sim(?,</p><formula xml:id="formula_4">?) = cos(?,?) ?</formula><p>, where ? is a scalar parameter.</p><p>As shown in Figure <ref type="figure">1</ref>, the overall goal of NER constrastive learning is to push the entity mention span representations close to their corresponding entity type embeddings (positive) and far away from irrelevant types (negative) in vector space, e.g. Person closer to Jim but away from Acme.</p><p>To achieve that, we propose a multi-objective formulation consisting of two objectives based on span and token embedding spaces respectively.</p><p>3 Here, we leave out special tokens for simplicity.</p><p>Span-based Objective Here, we consider the following way for deriving the vector representation for span (i, j)</p><formula xml:id="formula_5">s i,j = Linear S (h T i ? h T j ? D(j -i)),<label>(5</label></formula><p>) where s i,j ? R d , Linear S is a learnable linear layer, ? indicates the vector concatenation, D(ji) ? R m is a row from the span width embedding matrix D ? R L?m , and L is the maximum span width considered. Based on this, the span-based infoNCE <ref type="bibr">(Oord et al., 2018)</ref> can be defined as</p><formula xml:id="formula_6">span = -log exp(sim(s i,j , e k )) s ?S - k ?s i,j exp(sim(s ,e k ))</formula><p>,</p><p>where the span (i, j) belongs to entity type E k , S - k is the set of negative spans that are all possible spans from the input text, excluding entity spans from E k , and e k is the entity type embedding.</p><p>Position-based Objective One limitation of the span-based objective is that it penalizes negative spans in the same way, whether they are partially correct spans or spans that have no overlap with the gold entity span. Intuitively, it is more desirable to have partially correct spans, e.g., spans that have the same start or end token with the gold span. Therefore, we propose additional positionbased contrastive learning objectives. Specifically, we compute two additional entity type embeddings for E k by using additional linear layers</p><formula xml:id="formula_8">e B k = Linear E B (h E k [CLS] ) (7) e Q k = Linear E Q (h E k [CLS] ),<label>(8)</label></formula><p>where e B k , e Q k are the type embeddings for the start and end positions respectively, h E k [CLS] is from the entity type encoder (Equation <ref type="formula" target="#formula_1">1</ref>). Accordingly, we can use two addtional linear layers to compute the corresponding token embeddings for the start and end tokens respectively</p><formula xml:id="formula_9">u n = Linear T B (h T n ),<label>(9)</label></formula><formula xml:id="formula_10">v n = Linear T Q (h T n ). (<label>10</label></formula><formula xml:id="formula_11">)</formula><p>Using e k B , e k Q as anchors, we then define two position-based objectives</p><formula xml:id="formula_12">start = -log exp(sim(u i , e B k )) u ?U - k ?u i exp(sim(u , e B k )) (11) end = -log exp(sim(v j , e Q k )) v ?V - k ?v j exp(sim(v , e Q k )) ,<label>(12)</label></formula><p>where U - k , V - k are two sets of positions in the input text that do not belong to the start/end of any span of entity type k. Compared with Equation <ref type="formula" target="#formula_7">6</ref>, the main difference of position-based objectives comes from the corresponding negative sets where start and end positions are independent of each other. In other words, the position-based objectives can potentially help the model to make better start and end position predictions.</p><p>Thresholding for Non-Entity Cases Although the contrastive learning objectives defined above can effectively push the positive spans close to their corresponding entity type in vector space, it might be problematic for our model at test time to decide how close a span should be before it can be considered as positive. In other words, the model is not able to separate entity spans from non-entity spans properly. To address this issue, we use the similarity between the special token [CLS] and the entity type as a dynamic threshold (as shown in Figure <ref type="figure">1</ref>). Intuitively, the representation of [CLS] reads the entire input text and summarizes the contextual information, which could make it a good choice to compute the threshold to separate entity spans from non-entity spans. We compare it with several other thresholding choices in ?4.5.</p><p>To learn thresholding, we extend the original contrastive learning objectives with extra adaptive learning objectives for non-entity cases. Specifically, for the start loss (Equation <ref type="formula">11</ref>), the augmented start loss + start is</p><formula xml:id="formula_13">? start -(1 -?) log exp(sim(u [CLS] , e B k )) u ?U - k exp(sim(u ,e B k ))</formula><p>.</p><p>(13) An augmented end loss + end can be defined in a similar fashion. For the span loss (Equation <ref type="formula" target="#formula_7">6</ref>), we use the span embedding s 0,0 for deriving the augmented span loss</p><formula xml:id="formula_14">+ span ? span -(1 -?) log exp(sim(s 0,0 , e k )) s ?S - k ?s i,j exp(sim(s ,e k ))</formula><p>.</p><p>(14) Note that we use a single scalar hyperparameter ? for balancing the adaptive thresholding learning and original contrastive learning for all three cases.</p><p>Training Finally, we consider a multi-task contrastive formulation by combing three augmented contrastive learning discussed above, leading to our overall training objective</p><formula xml:id="formula_15">L = ? + start + ? + end + ? + joint ,<label>(15)</label></formula><p>where ?, ?, ? are all scalar hyperparameters.</p><p>Inference Strategy During inference, we enumerate all possible spans that are less than the length of L and compute three similarity scores based on the start/end/span cases for each entity type. We consider two prediction strategies, joint position-span and span-only predictions. In the joint position-span case, for entity type E k , we prune out spans (i, j) that have either start or end similar scores lower than the learned null threshold, i.e. sim(</p><formula xml:id="formula_16">u i , e B k ) &lt; sim(u [CLS] , e B k ) or sim(v j , e Q k ) &lt; sim(v [CLS] , e Q k )</formula><p>. Then, only those spans with span similarity scores higher than the span null threshold are predicted as positive ones i.e. sim(s i,j , e k ) &gt; sim(s 0,0 , e k ). For the spanonly strategy, we just rely on the span similarity score and keep all qualified spans as final predictions. The full inference algorithm is summarized in Algorithm 1. As we can see, the difference between joint position-span and span-only strategies is whether line 11 is used. As shown later in our experiments ( ?4.5), we find the span-only inference is more effective, because the joint inference is more likely to be affected by annotation artifacts. Also, for flat NER datasets, we further carry out a post-processing step to remove overlapping predictions (line 22 in Algorithm 1). Here, the post-processing is carried out in a greedy fashion where higher scored span predictions with earlier start and end positions are preferred.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate our method in both supervised and distantly supervised settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation Metrics</head><p>We follow the standard evaluation protocol and use micro F1 as the evaluation metric: a predicted entity span is considered correct if its span boundaries and the predicted entity type are both correct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>We implement our models based on the Hugging-Face Transformers library <ref type="bibr" target="#b54">(Wolf et al., 2020)</ref>. The base encoders are initialized using PubMedBERTbase-uncased <ref type="bibr" target="#b17">(Gu et al., 2021)</ref> or BioBERT <ref type="bibr" target="#b25">(Lee et al., 2019)</ref> for biomedical NER datasets, and Algorithm 1: Inference for BINDER.</p><p>Input: S = {(i, j)|i, j = 1, . . . , N, 0 ? j -i ? L} the set of spans , E = {E 1 , . . . , E K } the set of entity types, joint for whether using joint position-span inference, and flat for whether the inference is for flat NER. Function main(): M = {}; D = Dict() # a dictionary maps item in M to its similarity score; </p><formula xml:id="formula_17">for E k ? E do 6 calculate the threshold scores b null = sim(u [CLS] , e B k ), e null = sim(v [CLS] , e Q k ), s null = sim(s 0,0 , e k ); 8 for (i, j) ? S do calculate the similarity scores b = sim(u i , e B k ), e = sim(v j , e Q k ), s = sim(s i,j , e k );</formula><formula xml:id="formula_18">for (i, j, E k ) in D do if span (i, j) has no overlap in M then M = M ? {(i, j, E k )}; end end return M ;</formula><p>BERT-base-uncased <ref type="bibr" target="#b9">(Devlin et al., 2019)</ref> for other NER datasets. The linear layer output size is 128; the width embedding size is 128; the initial temperatures are 0.07. We train our models with the AdamW optimizer <ref type="bibr" target="#b30">(Loshchilov and Hutter, 2017)</ref> of a linear scheduler and dropout of 0.1. The entity start/end/span contrastive loss (Equation <ref type="formula" target="#formula_15">15</ref>) weights are set to ? = 0.2, ? = 0.2, ? = 0.6, and the same loss weights are chosen for thresholding contrastive learning. The contrastive losses for thresholding and entity are weighted equally in the final loss. For all experiments, we ignore sentence boundaries, and tokenize and split text into sequences with the maximum token length of 128 and a stride of 16. We train our models for 20 epochs with a learning rate of 3e-5 and a batch size of 8 sequences. The maximum token length for entity spans is set to 30. We use early stop with a patience of 10 in the distantly supervised setting.</p><p>Validation is done at every 50 steps of training, and we adopt the models that have the best performance on the development set. We report the median score of multiple runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Supervised NER Datasets</head><p>In the supervised setting, we evaluate our method on both nested and flat NER datasets. For nested NER, we consider ACE2004<ref type="foot" target="#foot_1">4</ref> , ACE2005<ref type="foot" target="#foot_2">5</ref> , and GENIA <ref type="bibr" target="#b23">(Kim et al., 2003)</ref>. ACE2004 and ACE2005 are collected from general domains (e.g., news and web). We follow <ref type="bibr" target="#b32">Luan et al. (2018)</ref> to split ACE2004 into 5 folds, and ACE2005 into train, development and test sets. GENIA is collected from the biomedical domain. We use its v3.0.2 corpus<ref type="foot" target="#foot_3">6</ref> and follow Finkel and <ref type="bibr" target="#b11">Manning (2009)</ref> and <ref type="bibr" target="#b31">Lu and Roth (2015)</ref> to split it into 80%/10%/10% train/dev/test splits. For flat NER, we consider five biomedical NER datasets from the BLURB benchmark<ref type="foot" target="#foot_4">7</ref> : BC5-chem/disease <ref type="bibr" target="#b26">(Li et al., 2016)</ref>, <ref type="bibr">NCBI (Dogan et al., 2014)</ref>, BC2GM <ref type="bibr" target="#b45">(Smith et al., 2008)</ref>, and JNLPBA <ref type="bibr" target="#b7">(Collier and Kim, 2004)</ref>. Preprocessing and splits follow <ref type="bibr" target="#b17">Gu et al. (2021)</ref>. Table <ref type="table" target="#tab_2">2</ref> shows the statistics of each dataset. and GENIA. We report precision, recall, and F1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>As is shown, our method achieves very strong performance on all three datasets. On ACE2004 and ACE2005, it outperforms all previous methods with 88.7% and 89.5% F1. Particularly, in comparison with the previous best method <ref type="bibr" target="#b48">(Tan et al., 2021)</ref>, our method significantly improves F1 by the absolute points of +1.4% and +2.4% respectively. On GENIA, our method is on par with the previous state of the art <ref type="bibr" target="#b61">(Yu et al., 2020)</ref>. Note that the previous methods are built on top of different encoders, e.g., LSTM, BERT-large, BARTlarge, and T5-base. Our method using a BERTbase encoder even outperforms the previous methods that use a larger encoder of BERT-large or BART-large. Overall, compared to the previous methods, our method has substantial gains on recall while maintaining a high precision. We conduct more detailed analysis in ?4.5.</p><p>Table <ref type="table" target="#tab_3">3</ref> compares our method with all previous submissions on the BLURB benchmark. We only report F1 due to unavailability of precision and recall of these submissions. The major difference among these submissions are encoders. A direct comparison can be made between our method and <ref type="bibr" target="#b17">Gu et al. (2021)</ref> which formulates NER as a sequential labeling task and fine-tunes a standard LSTM+CRF classifier on top of the pretrained transformer encoder. While both use PubMed-BERT as the encoder, our method significantly outperforms <ref type="bibr" target="#b17">Gu et al. (2021)</ref> across the board. Compared to the recent submissions, <ref type="bibr">Kanakarajan et al. (2021)</ref>; <ref type="bibr" target="#b59">Yasunaga et al. (2022)</ref>, our method also show substantial gains on BC5-chem (+1.2%), BC5-disease (+1.9%), and NCBI (1.5%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Distantly Supervised NER</head><p>Dataset In the distantly supervised setting, we consider BC5CDR <ref type="bibr" target="#b26">(Li et al., 2016)</ref> from the biomedical domain. It consists of 1,500 articles annotated with 15,935 chemical and 12,852 disease mentions. We use the standard train, development, and test splits. In train and development, we discard all human annotations and only keep the unlabeled articles. Their distant labels are generated using exact string matching against a dictionary released by <ref type="bibr" target="#b43">Shang et al. (2018)</ref>. 8 On the training set, the distant labels have high precision (97.99% for chemicals, and 98.34% for diseases), but low recall (63.14% for chemicals, and 46.73% for diseases).</p><p>Results Table <ref type="table" target="#tab_5">4</ref> compares test scores of our method and previous methods on BC5CDR. It presents a clear advantage of our method over all previous methods in the distantly supervised setting. The F1 score is improved by +1.5% over the previous best method <ref type="bibr" target="#b66">(Zhou et al., 2022)</ref>, which adapts positive and unlabeled (PU) learning to obtain a high recall yet at the loss of precision. In contrast, our method maintains a reasonable recall (comparable to <ref type="bibr" target="#b43">Shang et al., 2018;</ref><ref type="bibr" target="#b41">Peng et al., 2019)</ref> and substantially improves the precision. For reference, Table <ref type="table" target="#tab_3">3</ref>   <ref type="bibr" target="#b43">(Shang et al., 2018)</ref> 82.6 77.5 80.0 BNPU <ref type="bibr" target="#b41">(Peng et al., 2019)</ref> 48.1 77.1 59.2 BERT-ES <ref type="bibr" target="#b28">(Liang et al., 2020)</ref> 80.4 67.9 73.7 Conf-MPU <ref type="bibr" target="#b66">(Zhou et al., 2022)</ref>  Thresholding Strategies Our method uses dynamic similarity thresholds to distinguish entity spans from non-entity spans. We compare the impact of dynamic thresholds in our method with two straightforward variants: (1) Learned global thresholds replaces dynamic thresholds with global thresholds, one for each entity type. Specifically, we consider the global similarity thresholds as scalar parameters (initialized as 0).</p><p>During training, we replace the similarity function outputs sim(p [CLS] , e k ) in Equation <ref type="formula">13</ref>and sim(s 0,0 , e k ) in Equation <ref type="formula">14</ref>with the correspond-   <ref type="table" target="#tab_9">7</ref>, the majority of entity spans and non-entity spans are separable regardless of the thresholding strategy. This is true even when no thresholds are used during training and instead we tune global thresholds on dev. Dynamic thresholds (shown in green and best viewed in an extreme zoom-in) are advantageous among a small set of entity spans which can not be separated from non-entity spans using a global threshold. We also observe that the learned global thresholds tend to yield a smaller gap between en- tity spans and non-entity spans, and a denser similarity distribution of non-entity spans.</p><p>Performance Breakdown Table <ref type="table" target="#tab_8">6</ref> shows the test F1 scores on each entity type of ACE2005 and GENIA. We report four types of F1 scores: S-F1 span is the strict F1 based on the exact match of entity spans; S-F1 start is the strict F1 based on the exact match of entity start words; S-F1 end is the strict F1 based on the exact match of entity end words; L-F1 span is the loose F1 allowing the partial match of entity spans. We notice that S-F1 end is often substantially better than S-F1 span and S-F1 start . To explain this difference, we go through these partially corrected predictions and summarize the common errors in Table <ref type="table" target="#tab_10">8</ref>. The most common one is the inclusion or exclusion of modifiers. This could be due to annotation disagreement: in ACE2005, sometimes generic spans are preferred (e.g., "tomcats" vs. "f-14 tomcats"), while in some cases specific spans are preferred (e.g., "cruise ship" vs. "ship"). This issue is more common in the biomedical dataset GENIA, e.g., "human GR" is considered as wrong while "human lymphocytes" are correct, which explains the higher scores of S-F1 end than S-F1 start . Missing genitives for person names are another common errors in ACE2005. We also discover some annotation errors, where names of a single person, protein, or cell line are sometimes broken into two less meaningful spans, e.g., "Dr. Germ" is annotated as two person mentions "Dr" and "Germ", and "EBVtransformed human B cell line SKW6.4" is annotated as two separate cell lines "EBV-transformed human B cell line" and "SKW6.4".  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We present a bi-encoder framework for NER, which separately maps text and entity types into the same vector space. By formulating NER as a metric learning problem, we propose to use a novel contrastive loss to simultaneously learn entity span identification and classification. Experiments in both supervised and distantly supervised setting demonstrate the effectiveness and robustness of our method. We conduct extensive analysis to explain the success of our method and reveal the room for further improvement. Future directions include applying our method in self-supervised or zero-shot settings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>if joint is true and b &lt; b null or e &lt; e null then Continue; end if s &gt; s null then M = M {(i, j, E k )}; D[(i, j, E k )] = s; end end end if flat is true then return removeOverlap(D); end return M ; Function removeOverlap( D): M = {}; sort D by the similarity score in descending order and break the tie by ascending in start and end positions;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2</head><label>2</label><figDesc>Figure2visualizes the distributions of similarity scores between different text spans and entity types based on different thresholding strategies. Consistent with the scores in Table7, the majority of entity spans and non-entity spans are separable regardless of the thresholding strategy. This is true even when no thresholds are used during training and instead we tune global thresholds on dev. Dynamic thresholds (shown in green and best viewed in an extreme zoom-in) are advantageous among a small set of entity spans which can not be separated from non-entity spans using a global threshold. We also observe that the learned global thresholds tend to yield a smaller gap between en-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The kernel density estimation of similarity scores between different text spans (entity, non-entity, and threshold spans) and entity types (PER, ORG, GPE) on ACE2005 based on different thresholding strategies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Table1presents comparison of our method with all previous methods evaluated on three nested NER datasets, ACE2004, ACE2005, Test scores on three nested NER datasets. Bold and underline indicate the best and the second best respectively. The different encoders are used: L = LSTM, Bl = BERT-large, BioB = BioBERT, BAl = BART-large, T5b = T5-base, Bb = BERT-base. ? Original scores are not reproducible. We report the rerun of their code. Similar scores are also reported in<ref type="bibr" target="#b58">Yan et al. (2021)</ref>.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Encoder</cell><cell></cell><cell>ACE2004</cell><cell></cell><cell></cell><cell>ACE2005</cell><cell>GENIA</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell cols="2">Lu and Roth (2015)</cell><cell></cell><cell cols="7">-70.0 56.9 62.8 66.3 59.2 62.5 74.2 66.7 70.3</cell></row><row><cell cols="3">Katiyar and Cardie (2018)</cell><cell cols="7">L 73.6 71.8 72.7 70.6 70.4 70.5 77.7 71.8 74.6</cell></row><row><cell cols="3">Shibuya and Hovy (2020)</cell><cell cols="7">Bl 83.7 81.9 82.8 83.0 82.4 82.7 78.1 76.5 77.3</cell></row><row><cell cols="2">Wang et al. (2020)</cell><cell></cell><cell cols="7">Bl/BioB 86.1 86.5 86.3 84.0 85.4 84.7 79.5 78.9 79.2</cell></row><row><cell cols="2">Li et al. (2020)  ?</cell><cell></cell><cell cols="7">Bl/BioB 85.8 85.8 85.8 85.0 84.1 84.6 81.2 76.4 78.7</cell></row><row><cell>Yu et al. (2020)</cell><cell></cell><cell></cell><cell cols="7">Bl/BioB 87.3 86.0 86.7 85.2 85.6 85.4 81.8 79.3 80.5</cell></row><row><cell cols="2">Tan et al. (2021)</cell><cell></cell><cell cols="7">Bl/BioB 88.5 86.1 87.3 87.5 86.6 87.1 82.3 78.7 80.4</cell></row><row><cell cols="2">Yan et al. (2021)</cell><cell></cell><cell cols="7">BAl 87.3 86.4 86.8 83.2 86.4 84.7 78.6 79.3 78.9</cell></row><row><cell cols="2">Zhang et al. (2022)</cell><cell></cell><cell cols="7">T5b 86.5 84.5 85.4 83.3 86.6 84.9 81.0 77.2 79.1</cell></row><row><cell cols="2">Wan et al. (2022)</cell><cell></cell><cell cols="7">Bb 86.7 85.9 86.3 84.4 85.9 85.1 77.9 80.7 79.3</cell></row><row><cell>Ours</cell><cell></cell><cell></cell><cell cols="7">Bb/BioB 88.3 89.1 88.7 89.1 89.8 89.5 81.5 79.6 80.5</cell></row><row><cell>Dataset</cell><cell>|E|</cell><cell cols="2"># Annotations</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Train</cell><cell>Dev</cell><cell>Test</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ACE2004</cell><cell>7</cell><cell cols="2">22,735 (5-fold)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ACE2005</cell><cell>7</cell><cell cols="3">26,473 6,338 5,476</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GENIA</cell><cell>5</cell><cell cols="3">46,142 4,367 5,506</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BC5-chem</cell><cell>1</cell><cell cols="3">5,203 5,347 5,385</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BC5-disease</cell><cell>1</cell><cell cols="3">4,182 4,244 4,424</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>NCBI</cell><cell>1</cell><cell>5,134</cell><cell>787</cell><cell>960</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BC2GM</cell><cell>1</cell><cell cols="3">15,197 3,061 6,325</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>JNLPBA</cell><cell>1</cell><cell cols="3">46,750 4,551 8,662</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The statistics of supervised NER datasets.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Test F1 scores on five flat NER datasets from the BLURB benchmark (aka.ms/blurb). Bold and underline indicate the best and the second best respectively. All encoders use their base version.</figDesc><table><row><cell></cell><cell cols="6">Encoder BC5-chem BC5-disease NCBI BC2GM JNLPBA</cell></row><row><cell>Lee et al. (2019)</cell><cell>BioBERT</cell><cell>92.9</cell><cell>84.7</cell><cell>89.1</cell><cell>83.8</cell><cell>78.6</cell></row><row><cell>Gu et al. (2021)</cell><cell>PubMedBERT</cell><cell>93.3</cell><cell>85.6</cell><cell>87.8</cell><cell>84.5</cell><cell>79.1</cell></row><row><cell cols="2">Kanakarajan et al. (2021) BioELECTRA</cell><cell>93.6</cell><cell>85.8</cell><cell>89.4</cell><cell>84.7</cell><cell>80.2</cell></row><row><cell>Yasunaga et al. (2022)</cell><cell>LinkBERT</cell><cell>93.8</cell><cell>86.1</cell><cell>88.2</cell><cell>84.9</cell><cell>79.0</cell></row><row><cell>Ours</cell><cell>PubMedBERT</cell><cell>95.0</cell><cell>88.0</cell><cell>90.9</cell><cell>84.6</cell><cell>80.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>also includes the supervised state of the art. Our method in the supervised setting achieves the new state of the art at 91.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">9% F1,</cell></row><row><cell cols="4">outperforming Wang et al. (2021) by 1.0%. Com-</cell></row><row><cell cols="4">paring both settings, we observe that the distantly</cell></row><row><cell cols="4">supervised state of the art still has an over-10-point</cell></row><row><cell cols="4">gap with the supervised one, indicating a potential</cell></row><row><cell cols="3">to further reduce the false negative noise.</cell></row><row><cell></cell><cell></cell><cell>BC5CDR</cell></row><row><cell></cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell>Distantly Supervised</cell><cell></cell><cell></cell></row><row><cell>Dict/KB Matching</cell><cell cols="3">86.4 51.2 64.3</cell></row><row><cell>AutoNER</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Test scores on BC5CDR. The scores of previous methods in the distantly supervised setting are from<ref type="bibr" target="#b66">Zhou et al. (2022)</ref>.</figDesc><table><row><cell></cell><cell cols="3">76.6 83.8 80.1</cell></row><row><cell>Ours</cell><cell cols="3">87.6 76.3 81.6</cell></row><row><cell>Fully Supervised</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Nooralahzadeh et al. (2019)</cell><cell cols="3">92.1 87.9 89.9</cell></row><row><cell>Wang et al. (2021)</cell><cell>-</cell><cell>-</cell><cell>90.9</cell></row><row><cell>Ours</cell><cell cols="3">92.6 91.2 91.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>We observe performance degradation in all these variants. Shared linear layers uses the same linear layer for span and token embeddings, and the same linear layer for entity type embeddings, in the hope of projecting them into the same vector space and sharing their semantics. It leads to a slightly better precision but a drop of recall. Similar changes are observed in Joint positionspan inference, which adopts a more strict strategy to prune out spans -only keep spans whose start, end, and span scores are all above the thresholds. In contrast, No position-based objectives only optimizes the span-based objective, which penalizes partially corrected spans in the same way as other spans. It results in a marginal improvement of recall but a significant loss of precision.</figDesc><table><row><cell></cell><cell>ACE2005</cell><cell></cell></row><row><cell>P</cell><cell>R</cell><cell>F1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Test scores of model variants on ACE2005.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Test F1 score breakdowns on ACE2005 and GENIA. Columns compare F1 scores on different entity types. Rows compare F1 scores based on the entire entity span, or only the start or end of entity span. S-F1 denotes the strict F1 requiring the exact boundary match. L-F1 denotes the loose F1 allowing partial overlaps. The color signifies substantially better F1 scores than the corresponding entity span strict F1 scores. ing global thresholds. At test times, the global thresholds are used to separate entity spans from non-entity spans. (2) Global thresholds tuned on dev introduces global thresholds after the training is done and tune them on the development set. During training, instead of Equation 15, we optimize a simplified loss without thresholds, ?L start + ?L end + ?L joint . Table 7 compares their test scores on the ACE2005 dataset. Dynamic thresholds have the best scores overall. Learned global thresholds performs better than global thresholds tuned on the development set, indicating the necessity of learning thresholds during training. Note that the global thresholds tuned on dev still outperforms all the previous methods in Table 1, showing a clear advantage of our biencoder framework.</figDesc><table><row><cell></cell><cell>ACE2005</cell><cell></cell><cell></cell><cell cols="2">GENIA</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="7">PER GPE ORG FAC LOC VEH WEA ALL Prot. DNA CellT. CellL. RNA ALL</cell></row><row><cell cols="2">S-F1 span 93.4 91.2 79.7 81.0 78.7 84.8</cell><cell>82.1</cell><cell>89.5 82.9 77.6</cell><cell>74.5</cell><cell>76.3</cell><cell>87.9</cell><cell>80.5</cell></row><row><cell cols="2">S-F1 start 93.9 91.2 80.7 81.0 79.0 84.8</cell><cell>82.1</cell><cell>89.9 86.1 80.9</cell><cell>74.5</cell><cell>80.2</cell><cell>88.7</cell><cell>83.2</cell></row><row><cell>S-F1 end</cell><cell>93.9 91.2 81.9 83.1 79.0 86.8</cell><cell>82.1</cell><cell>90.3 87.6 82.6</cell><cell>83.7</cell><cell>82.8</cell><cell>91.0</cell><cell>85.8</cell></row><row><cell cols="2">L-F1 span 94.4 91.4 83.0 83.1 79.4 87.2</cell><cell>82.1</cell><cell>90.8 91.6 87.4</cell><cell>84.8</cell><cell>87.2</cell><cell>91.7</cell><cell>89.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Test scores of our method using different thresholding strategies on ACE2005.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Examples of common errors among the partially corrected predictions. Red indicates error spans. Blue indicates missing spans. The number after each span mean the span frequency in the training data.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p><ref type="bibr" target="#b8">Das et al. (2022)</ref> applies contrastive learning for NER in a few-shot setting. In this paper, we focus on supervised NER and distantly supervised NER.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>catalog.ldc.upenn.edu/LDC2005T09</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>catalog.ldc.upenn.edu/LDC2006T06</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3"><p>geniaproject.org/genia-corpus</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4"><p>aka.ms/blurb</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Recognising nested named entities in biomedical text</title>
		<author>
			<persName><forename type="first">Beatrice</forename><surname>Alex</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Grover</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biological, translational, and clinical language processing</title>
		<meeting><address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Leveraging type descriptions for zeroshot named entity recognition and classification</title>
		<author>
			<persName><forename type="first">Rami</forename><surname>Aly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.120</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1516" to="1528" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Eduard S?ckinger, and Roopak Shah. 1993. Signature verification using a&quot; siamese&quot; time delay neural network. Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">Jane</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A constrained latent variable model for coreference resolution</title>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajhans</forename><surname>Samdani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="601" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Named entity recognition with bidirectional LSTM-CNNs</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><surname>Nichols</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00104</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="357" to="370" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Joint extraction of entities and relations for opinion recognition</title>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Breck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2006 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="431" to="439" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005. 2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Introduction to the bio-entity recognition task at JNLPBA</title>
		<author>
			<persName><forename type="first">Nigel</forename><surname>Collier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin-Dong</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications</title>
		<meeting>the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications<address><addrLine>Geneva, Switzerland. COLING</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="73" to="78" />
		</imprint>
		<respStmt>
			<orgName>NLPBA/BioNLP</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">CON-TaiNER: Few-shot named entity recognition via contrastive learning</title>
		<author>
			<persName><forename type="first">Sarkar</forename><surname>Snigdha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarathi</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arzoo</forename><surname>Katiyar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rebecca</forename><surname>Passonneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.439</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6338" to="6353" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Special report: Ncbi disease corpus: A resource for disease name recognition and concept normalization</title>
		<author>
			<persName><forename type="first">Rezarta</forename><surname>Islamaj Dogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Leaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. of Biomedical Informatics</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Nested named entity recognition</title>
		<author>
			<persName><forename type="first">Jenny</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Rose</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="141" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Jason</forename><surname>Fries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ratner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>R?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.06360</idno>
		<title level="m">Swellshark: A generative model for biomedical named entity recognition without labeled data</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">SpanNER: Named entity re-/recognition as span prediction</title>
		<author>
			<persName><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.558</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="7183" to="7195" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised aspect term extraction with B-LSTM &amp; CRF using automatically labelled datasets</title>
		<author>
			<persName><forename type="first">Athanasios</forename><surname>Giannakopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudiu</forename><surname>Musat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreea</forename><surname>Hossmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Baeriswyl</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-5224</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</title>
		<meeting>the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="180" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning dense representations for entity retrieval</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sayali</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><surname>Lansing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Presta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Ie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Garcia-Olano</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K19-1049</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)</title>
		<meeting>the 23rd Conference on Computational Natural Language Learning (CoNLL)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="528" to="537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Presta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><forename type="middle">Singh</forename><surname>Tomar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08008</idno>
		<title level="m">End-to-end retrieval in continuous space</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Domain-specific language model pretraining for biomedical natural language processing</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Tinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naoto</forename><surname>Usuyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computing for Healthcare</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Convolutional neural network architectures for matching natural language sentences</title>
		<author>
			<persName><forename type="first">Baotian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning deep structured semantic models for web search using clickthrough data</title>
		<author>
			<persName><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><surname>Heck</surname></persName>
		</author>
		<idno type="DOI">10.1145/2505515.2505665</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM International Conference on Information &amp; Knowledge Management, CIKM &apos;13</title>
		<meeting>the 22nd ACM International Conference on Information &amp; Knowledge Management, CIKM &apos;13<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2333" to="2338" />
		</imprint>
	</monogr>
	<note>Association for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Polyencoders: Architectures and pre-training strate-gies for fast and accurate multi-sentence scoring</title>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Humeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.bionlp-1.16</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations. Kamal raj Kanakarajan, Bhuvana Kundumani, and Malaikannan Sankarasubbu</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019. 2021</date>
			<biblScope unit="page" from="143" to="154" />
		</imprint>
	</monogr>
	<note>Proceedings of the 20th Workshop on Biomedical Language Processing</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dense passage retrieval for open-domain question answering</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.550</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6769" to="6781" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Nested named entity recognition revisited</title>
		<author>
			<persName><forename type="first">Arzoo</forename><surname>Katiyar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1079</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="861" to="871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Genia corpus-a semantically annotated corpus for bio-textmining</title>
		<author>
			<persName><forename type="first">J-D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomoko</forename><surname>Ohta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuka</forename><surname>Tateisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun'ichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">suppl_1</biblScope>
			<biblScope unit="page" from="180" to="182" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1030</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="260" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">BioBERT: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName><forename type="first">Jinhyuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wonjin</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungdong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunkyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">So</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btz682</idno>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Biocreative v cdr task corpus: a resource for chemical disease relation extraction</title>
		<author>
			<persName><forename type="first">Jiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yueping</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniela</forename><surname>Sciaky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chih-Hsuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Leaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allan</forename><forename type="middle">Peter</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carolyn</forename><forename type="middle">J</forename><surname>Mattingly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">C</forename><surname>Wiegers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A unified MRC framework for named entity recognition</title>
		<author>
			<persName><forename type="first">Xiaoya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingrong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxian</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.519</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5849" to="5859" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bond: Bert-assisted open-domain named entity recognition with distant supervision</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siawpeng</forename><surname>Er</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruijia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3394486.3403149</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD &apos;20</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD &apos;20<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1054" to="1064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Noisy-labeled NER with confidence estimation</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mosha</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songfang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.269</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3437" to="3445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Joint mention extraction and classification with mention hypergraphs</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1102</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="857" to="867" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1360</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3219" to="3232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">End-toend sequence labeling via bi-directional LSTM-CNNs-CRF</title>
		<author>
			<persName><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1101</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1064" to="1074" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Named entity recognition with partially annotated training data</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Mayhew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Snigdha</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen-Tse</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K19-1060</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)</title>
		<meeting>the 23rd Conference on Computational Natural Language Learning (CoNLL)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="645" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Identifying gene and protein mentions in text using conditional random fields</title>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Coarseto-Fine Pre-training for Named Entity Recognition</title>
		<author>
			<persName><forename type="first">Xue</forename><surname>Mengge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingwen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.514</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6345" to="6354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP<address><addrLine>Suntec, Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A survey of named entity recognition and classification</title>
		<author>
			<persName><forename type="first">David</forename><surname>Nadeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satoshi</forename><surname>Sekine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lingvisticae Investigationes</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="26" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Association for Computational Linguistics. Aaron van den Oord, Yazhe Li, and Oriol Vinyals</title>
		<author>
			<persName><forename type="first">Farhad</forename><surname>Nooralahzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">Tore</forename><surname>L?nning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lilja</forename><surname>?vrelid</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-6125</idno>
		<idno type="arXiv">arXiv:1807.03748</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP</title>
		<meeting>the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2019. DeepLo 2019. 2018</date>
			<biblScope unit="page" from="225" to="233" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Representation learning with contrastive predictive coding</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Instance-based learning of span representations: A case study through named entity recognition</title>
		<author>
			<persName><forename type="first">Hiroki</forename><surname>Ouchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sosuke</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sho</forename><surname>Yokoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsuki</forename><surname>Kuribayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryuto</forename><surname>Konno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.575</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6452" to="6459" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Distantly supervised named entity recognition using positiveunlabeled learning</title>
		<author>
			<persName><forename type="first">Minlong</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1231</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2409" to="2419" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Clustype: Effective entity recognition and typing by relation phrase-based clustering</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>El-Kishky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangbo</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clare</forename><forename type="middle">R</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="995" to="1004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning named entity tagger using domain-specific dictionary</title>
		<author>
			<persName><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaotao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1230</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2054" to="2064" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Nested Named Entity Recognition via Second-best Sequence Learning and Decoding</title>
		<author>
			<persName><forename type="first">Takashi</forename><surname>Shibuya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00334</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="605" to="620" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Overview of biocreative ii gene mention recognition</title>
		<author>
			<persName><forename type="first">Larry</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorraine</forename><forename type="middle">K</forename><surname>Tanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng-Ju</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chun-Nan</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Shi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Klinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manabu</forename><surname>Torii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome biology</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep exhaustive model for nested named entity recognition</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Golam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sohrab</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1309</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2843" to="2849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Neural architectures for nested NER through linearization</title>
		<author>
			<persName><forename type="first">Jana</forename><surname>Strakov?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milan</forename><surname>Straka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Hajic</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1527</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5326" to="5331" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A sequence-toset network for nested named entity recognition</title>
		<author>
			<persName><forename type="first">Zeqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongliang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiming</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 30th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">? Ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems (NeurIPS)</title>
		<meeting>Advances in Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Nested named entity recognition with span-level graphs</title>
		<author>
			<persName><forename type="first">Juncheng</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyu</forename><surname>Ru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.63</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="892" to="903" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Pyramid: A layered model for nested named entity recognition</title>
		<author>
			<persName><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidan</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.525</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5918" to="5928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Improving named entity recognition by external context retrieving and cooperative learning</title>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nguyen</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.142</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1800" to="1812" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Large scale image annotation: learning to rank with joint word-image embeddings</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="35" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Transformers: Stateof-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keith</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<title level="m">Starspace: Embed all the things! In Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Scalable zero-shot entity linking with dense entity retrieval</title>
		<author>
			<persName><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Josifoski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.519</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6397" to="6407" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Multi-grained named entity recognition</title>
		<author>
			<persName><forename type="first">Congying</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fenglong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1138</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1430" to="1440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A unified generative framework for various NER subtasks</title>
		<author>
			<persName><forename type="first">Hang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junqi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qipeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.451</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5808" to="5822" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">LinkBERT: Pretraining language models with document links</title>
		<author>
			<persName><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.551</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="8003" to="8016" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Learning discriminative projections for text similarity measures</title>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Fifteenth Conference on Computational Natural Language Learning<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="247" to="256" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Named entity recognition as dependency parsing</title>
		<author>
			<persName><forename type="first">Juntao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimo</forename><surname>Poesio</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.577</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6470" to="6476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Tristan Naumann, Jianfeng Gao, and Hoifung Poon. 2021a. Knowledge-rich self-supervised entity linking</title>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shikhar</forename><surname>Vashishth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cliff</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.07887</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">De-bias for generative extraction in unified NER task</title>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongliang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiquan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiming</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.59</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="808" to="818" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">2021b. De-biasing distantly supervised named entity recognition via causal intervention</title>
		<author>
			<persName><forename type="first">Wenkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianpei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.371</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<biblScope unit="page" from="4803" to="4813" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Named entity recognition using an HMM-based chunk tagger</title>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Su</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073083.1073163</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="473" to="480" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Distantly supervised named entity recognition via confidence-based multi-class positive and unlabeled learning</title>
		<author>
			<persName><forename type="first">Kang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuepei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.498</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="7198" to="7211" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
