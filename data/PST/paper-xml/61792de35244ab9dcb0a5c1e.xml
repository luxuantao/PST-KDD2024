<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MolGPT: Molecular Generation Using a Transformer-Decoder Model</title>
				<funder>
					<orgName type="full">Kohli Center on Intelligent Systems, IIIT Hyderabad</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022">DECEMBER 01, 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Viraj</forename><surname>Bagal</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Rishal</forename><surname>Aggarwal</surname></persName>
						</author>
						<author>
							<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Vinod</surname></persName>
						</author>
						<author>
							<persName><forename type="first">U</forename><surname>Deva Priyakumar</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Emma</forename><forename type="middle">P</forename><surname>Tysinger</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Anton</forename><forename type="middle">V</forename><surname>Sinitskiy</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Satoshi</forename><surname>Noguchi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Junya</forename><surname>Inoue</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Fabio</forename><surname>Urbina</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sean</forename><surname>Ekins</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Lijuan</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Lei</forename><surname>Yang</surname></persName>
						</author>
						<title level="a" type="main">MolGPT: Molecular Generation Using a Transformer-Decoder Model</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022">DECEMBER 01, 2022</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1021/acs.jcim.1c00600</idno>
					<note type="submission">Received: May 26, 2021</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>From Reaction Informatics to Chemical Space</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Application of deep learning techniques for de novo generation of molecules, termed as inverse molecular design, has been gaining enormous traction in drug design. The representation of molecules in SMILES notation as a string of characters enables the usage of state of the art models in natural language processing, such as Transformers, for molecular design in general. Inspired by generative pre-training (GPT) models that have been shown to be successful in generating meaningful text, we train a transformerdecoder on the next token prediction task using masked self-attention for the generation of druglike molecules in this study. We show that our model, MolGPT, performs on par with other previously proposed modern machine learning frameworks for molecular generation in terms of generating valid, unique, and novel molecules. Furthermore, we demonstrate that the model can be trained conditionally to control multiple properties of the generated molecules. We also show that the model can be used to generate molecules with desired scaffolds as well as desired molecular properties by conditioning the generation on scaffold SMILES strings of desired scaffolds and property values. Using saliency maps, we highlight the interpretability of the generative process of the model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? INTRODUCTION</head><p>It has been postulated that the total number of potential drug like candidates range from 10 23 to 10 60 molecules, <ref type="bibr" target="#b0">1</ref> of which only about 10 8 molecules have been synthesized. <ref type="bibr" target="#b1">2</ref> Since it is difficult to screen a practically infinite chemical space, and there is a huge disparity between synthesized and potential molecules, generative models are used to model a distribution of molecules for the purpose of sampling molecules that have desirable properties. Deep generative models have made great strides in modeling data distributions in general data domains such as Computer Vision <ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4</ref> and Natural Language Processing (NLP). <ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6</ref> Such methods have also been adopted to model molecular distributions. <ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8</ref> Such models learn probability distributions over a large set of molecules and therefore are able to generate novel molecules by sampling from these distributions. <ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9</ref> The rapid adoption of the deep generative model has also led to the development of benchmark data sets such as the Molecular Sets (MOSES) <ref type="bibr" target="#b9">10</ref> and GuacaMol 9 data sets.</p><p>The representation of molecules in the Simplified Molecular Input Line Entry System (SMILES) <ref type="bibr" target="#b10">11</ref> notation as a string of characters enables the usage of modern NLP deep learning models for their computation. <ref type="bibr" target="#b11">12</ref> Some of the earliest deep learning architectures for molecular generation involved the usage of Recurrent Neural Networks (RNNs) on molecular SMILES. <ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14</ref> Such models have also previously been trained on a large corpus of molecules and then focused through the usage of reinforcement learning <ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16</ref> or transfer learning <ref type="bibr" target="#b12">13</ref> to generate molecules of desirable properties and activity.</p><p>Auto-Encoder variants such as the Variational Auto-Encoder (VAE) <ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref> and Adversarial Auto-Encoder (AAE) <ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref> have also been employed for molecular generation. These models contain an encoder that encodes molecules to a latent vector representation and a decoder that maps latent vectors back to molecules. Molecules can then be generated by sampling from these latent spaces. Randomization of SMILES strings <ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref> have also been employed in such models as a data augmentation strategy. Junction Tree VAE (JT-VAE), <ref type="bibr" target="#b19">20</ref> on the other hand, is an alternate solution for molecular generation that represents molecules as graph tree structures. JT-VAE also ensures 100% validity of generated molecules by maintaining a vocabulary of molecular components that can be added at each junction of the molecule tree. Conditional Variational Auto-Encoders have also been used to generate molecules with desired properties. <ref type="bibr" target="#b28">29</ref> Generative Adversarial Networks (GANs) have also gained traction for molecular design. <ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref> This is mainly because of their ability to generate highly realistic content. <ref type="bibr" target="#b3">4</ref> GANs are composed of generators and discriminators that work in opposition of each other. While the generator tries to generate realistic content, the discriminator tries to distinguish between generated and real content. ORGAN <ref type="bibr" target="#b30">31</ref> was the first usage of GANs for molecular generation. RANC <ref type="bibr" target="#b33">34</ref> introduced reinforcement learning alongside a GAN loss to generate molecules of desirable properties. LatentGAN 30 is a more recent method that uses latent vectors as input and outputs. These latent vectors are mapped to molecules by the decoder of a pretrained autoencoder. This ensures that the model can work with latent representations and does not have to handle SMILES syntax. Most of these methods have been benchmarked on either the MOSES <ref type="bibr" target="#b9">10</ref> or the GuacaMol 9 data set for easy comparison.</p><p>Often, methods use Bayesian optimization, <ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36</ref> reinforcement learning, <ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b33">34</ref> or other optimization methods <ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38</ref> to generate molecules exhibiting desirable properties. Mol-CycleGAN 39 is a generative model that utilizes the JT-VAE architecture and applies the CycleGAN 40 loss to generate molecules of required properties using given molecule templates. Only a few methods employ conditional generation based on user defined property values. Conditional RNNs, <ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b40">41</ref> Deep Learning Enabled Inorganic Material Generator (DING), <ref type="bibr" target="#b28">29</ref> and Conditional Adversarially Regularized Autoencoder (CARAE) <ref type="bibr" target="#b24">25</ref> are three such methods that sample molecules based on exact values. RNNs have also been previously used to generate molecules based on given scaffolds. <ref type="bibr" target="#b41">42</ref> A graph based method has been designed that ensures the presence of desired scaffolds while generating molecules with exact property values. <ref type="bibr" target="#b42">43</ref> A novel NLP architecture called the Transformer 5 has shown state-of-the-art performance in language translation tasks. Transformers consist of encoder and decoder modules. The encoder module gains context from all the input tokens through self-attention mechanisms. The decoder module gains context from both the encoder as well as previously generated tokens by attention. Using this context, the decoder is able to predict the next token. The decoder module has also been previously used independently for language modeling tasks and is known as the Generative Pre-Training Transformer model (GPT). <ref type="bibr" target="#b43">44</ref> The GPT model has been shown to develop better language embeddings 44 that model longer-distance connections. Due to this, the embeddings have shown top performance when used for multiple language modeling tasks such as natural language inference, question answering, sentence similarity, and classification. <ref type="bibr" target="#b44">45</ref> To yield the added benefits of this architecture, we train a GPT model, named MolGPT, to predict a sequence of SMILES tokens for molecular generation. To the best of our knowledge, this is the first work that has used the GPT architecture for molecular generation. For this, we use a regular expression (later referred to as a SMILES tokenizer) that breaks the SMILES strings into a set of relevant tokens which are used to train the model. Since predicted tokens are a result of attention applied to all previously generated tokens, we believe that the model easily learns the SMILES grammar and, therefore, can focus on higher level understanding of molecular properties. To this end, we also train our models conditionally to explicitly learn certain molecular properties. The model displays performance that is on par with other methods benchmarked on the MOSES and Guacamol data sets. Furthermore, we show that MolGPT controls user specified molecular properties and scaffolds with good accuracy, leading to our conclusion that it learns a good representation of the chemical space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? METHODS</head><p>In this section, we first present the data sets used for all the experiments. We discuss the properties used for conditional generation. This is then followed by the overview of the proposed model. A schematic of the training and generation pipeline is shown in this section. Finally, the details of the experiments and the metrics used for the evaluation of different models are provided.</p><p>Data Sets. In this work, we used two benchmark data sets, MOSES and GuacaMol, for training and evaluation of our model. MOSES is a data set composed of 1.9 million clean lead-like molecules from the Zinc data set <ref type="bibr" target="#b45">46</ref> with molecular weight ranging from 250 to 350 Da, number of rotatable bonds lower than 7, and XlogP below 3.5. GuacaMol on the other hand is a subset of the ChEMBL 24 <ref type="bibr" target="#b46">47</ref> database that contains 1.6 million molecules. We used the RDkit toolkit <ref type="bibr" target="#b47">48</ref> to calculate molecular properties and to extract Bemis-Murcko scaffolds. <ref type="bibr" target="#b48">49</ref> The MOSES data set was created mainly to represent lead like molecules and therefore has a distribution of molecules with ideal druglike properties. However, to test the models' control on conditional generation, we prefer the larger distribution of property values available in the Guacamol data set as can be seen in Figure <ref type="figure" target="#fig_0">1</ref>. This is preferred so that we can test the model's ability to generate molecules having very different property values. Therefore, we use the GuacaMol data set to test property conditional generation. The MOSES data set also provides a test set of scaffolds which we use to evaluate scaffold and property conditional generation.</p><p>The models were trained to learn some properties of the molecules for controlled generation and optimization. The properties used are the following:</p><p>? logP: The logarithm of the partition coefficient. The partition coefficient compares the solubilities of the solute in two immiscible solvents at equilibrium. If one of the solvents is water and the other is a nonpolar solvent, then logP is a measure of hydrophobicity.</p><p>? Synthetic Accessibility score (SAS 50 ): Measurement of the difficulty of synthesizing a compound. It is a score between 1 (easy to make) and 10 (very difficult to make).</p><p>? Topological Polar Surface Area (TPSA): The sum of surface area over all polar atoms. It measures the drug's ability to permeate cell membranes. Molecules with a TPSA greater than 140 ? 2 tend to be poor in permeating cell membranes.</p><p>Quantitative Estimate of Drug-likeness (QED 51 ): This quantifies drug-likeness by taking into account the main molecular properties. It ranges from 0 (all properties unfavorable) to 1 (all properties favorable).</p><p>Model Overview. The model schematic of MolGPT for training and generation is given in Figure <ref type="figure" target="#fig_1">2</ref>. For nonconditioned training, molecular SMILES are first tokenized using a SMILES tokenizer, and the model is then trained on  the next token prediction task. For property conditioned and scaffold conditioned training, we extract molecular properties and scaffolds from molecules using RDkit <ref type="bibr" target="#b47">48</ref> and pass them as conditions alongside the molecular SMILES. For generation, we feed the model a start token and the model then sequentially predicts the next token, thus generating a molecule. The start token is obtained via a weighted random sampling from a list of tokens that occur first in the SMILES strings of the training set. The weights of these tokens are determined by their frequency of occurrence in the first position of SMILES strings in the training set. Then, we provide the model a set of property and scaffold conditions along with the start token to sample a molecule.</p><p>Our model is illustrated in Figure <ref type="figure" target="#fig_2">3</ref>. The model is essentially a mini version of the Generative Pre-Training Transformer (GPT) model. <ref type="bibr" target="#b43">44</ref> Unlike GPT1 that has around 110 M parameters, MolGPT has only around 6 M parameters. MolGPT comprises stacked decoder blocks, each of which is composed of a masked self-attention layer and fully connected neural network. Each self-attention layer returns a vector of size 256 that is taken as input by the fully connected network. The hidden layer of the neural network outputs a vector of size 1024 and passes it through GELU activation layer. The final layer of the fully connected neural network returns a vector of size 256, that is then used as input for the next decoder block. MolGPT consists of eight such decoder blocks.</p><p>To keep track of the order of the input sequence, position value embeddings are assigned to each token. During conditional training, segment tokens are provided to distinguish between the condition and the SMILES tokens. Segment token embeddings represent whether a particular input is a condition or a molecule SMILES token for ease of differentiation between the two by the model. All the molecule SMILES tokens are mapped to a 256 dimensional vector using an embedding layer. Similarly, separate trainable embedding layers are used to map the position tokens and segment tokens to 256 dimensional vectors. These SMILES token embeddings, position embeddings, and segment token embeddings are then added, resulting in a vector of size 256 for each token of the SMILES string, which is then passed as input to the model.</p><p>GPT architectures work on a masked self-attention mechanism. Self-attention is calculated through "Scaled Dot Product Attention". This involves three sets of vectors, the query, key, and value vectors. Query vectors are used to query the weights of each individual value vector. They are first sent through a dot product with key vectors. These dot products are scaled by the dimensions of these vectors, and then a softmax function is applied to get the corresponding weights. The value vectors are multiplied by their respective weights and added. The query, key, and value vectors for each token are computed by weight matrices present in each decoder block. Attention can be represented by the following formula:</p><formula xml:id="formula_0">= i k j j j j j j y { z z z z z z Q K V QK d V Attention( , , ) softmax T k</formula><p>where Q, K, and V are query, key, and value vectors, respectively. d k here is the dimension of query and key vectors, and T is transpose of the matrix.</p><p>Self-attention provides attention to all the tokens of a sequence for prediction. However, this is not ideal when we are training a model to predict the next token in a sequence. It is because, during generation, unlike during training, the network would have access only to the tokens predicted in the previous time-steps. Therefore, masked self-attention is applied to mask attention to all sequence tokens that occur in future time steps. Moreover, instead of performing a single masked self-attention operation, each block performs multiple masked self-attention operations (multihead attention) in parallel and concatenates the output. Multihead attention provides better representations by attending to different representation subspaces at different positions.</p><p>We train this model on molecules represented as SMILES strings. For this, we use a SMILES tokenizer to break up the string into a sequence of relevant tokens. Property conditions are sent through a separate fully connected linear layer that maps the conditions to a vector of 256 dimensions to provide a representation of the properties in a higher dimension. The resultant vector is then concatenated at the start of the sequence of the embeddings of the SMILES tokens. For scaffold conditions, we use the same embedding layer as molecule SMILES to map each token of the scaffold string to a 256-dimensional vector. Similar to property conditions, the scaffold representation is then concatenated at the start of the sequence of the embeddings of the SMILES tokens. The model is trained such that the predicted tokens are a result of attention to both the previous molecule tokens as well as the conditions.</p><p>Training Procedure and Evaluation Metrics. Each model is trained for 10 epochs using the Adam optimizer with a learning rate of 6 ? 10 -4 . During generation, a start token (that is randomly sampled from the list of first tokens of molecules in the training set) is provided to the network along with the conditions.</p><p>We trained and tested MolGPT on both the MOSES <ref type="bibr" target="#b9">10</ref> and GuacaMol 9 data sets. We also conducted experiments to check MolGPT's capacity to control molecular properties and core structures. The models were trained on an NVIDIA 2080Ti GPU. Most of the models converged and showed best performance after 10 epochs. However, we noticed that training them for slightly fewer epochs led to similar results in terms of validity, novelty, and uniqueness of generated molecules, which are the metrics used here (details below).</p><p>? Validity: the fraction of a generated molecules that are valid. We use RDkit for validity check of molecules. Validity measures how well the model has learned the SMILES grammar and the valency of atoms.</p><p>? Uniqueness: the fraction of valid generated molecules that are unique. Low uniqueness highlights repetitive molecule generation and a low level of distribution learning by the model.</p><p>? Novelty: the fraction of valid unique generated molecules that are not in the training set. Low novelty is a sign of overfitting. We do not want the model to memorize the training data.</p><p>? Internal Diversity (IntDiv p ): measures the diversity of the generated molecules, which is a metric specially designed to check for mode collapse or whether the model keeps generating similar structures. This uses the power (p) mean of the Tanimoto similarity (T) between the fingerprints of all pairs of molecules (s1, s2) in the generated set (S). </p><formula xml:id="formula_1">? ? = - + ? + ? -? ? G D Tr FCD( , )<label>( 2(</label></formula><p>) )</p><formula xml:id="formula_2">G D G D G D 2 1/2</formula><p>where ? G is the mean and ? G is the covariance of the distribution G. For the Guacamol data set, the final FCD score is reported as</p><formula xml:id="formula_3">= - S exp( 0.2FCD)</formula><p>So, for the Guacamol data set, a higher S value is considered to be better.</p><p>? KL Divergence: Following Brown et al., <ref type="bibr" target="#b8">9</ref> KL divergence is calculated using numerous physicochemical descriptors of the generated and the reference sets. Lower values indicate that the model has learned the distribution of these properties very well. KL divergence between two distributions P and Q for any given property is a measure of how well Q approximates P and is calculated as follows:</p><formula xml:id="formula_4">? = D P Q P i P i Q i ( , ) ( ) log ( ) ( ) KL i</formula><p>Reported here is the aggregated final score S over all the properties k calculated as</p><formula xml:id="formula_5">? = - S k D 1 exp( ) i k i KL,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? RESULTS AND DISCUSSION</head><p>In this section, we first present the results on nonconditioned generation of molecules. MolGPT's performance is then compared with other state-of-the-art approaches, followed by some insights on the interpretability of our model. We then demonstrate our model's ability of conditional generation based on property alone and scaffold alone. This is followed with the results on conditional generation based on property and scaffold together. Finally, we show examples of our model being used for optimization of QED value of a starting molecule and optimization of SAS value maintaining the scaffold, TPSA, and logP values.</p><p>Nonconditioned Molecular Generation. The chemical space is practically infinite and unexplored, and so a good generative model should try to generate a greater number of novel, valid molecules so as to help us explore that chemical space. High values of these metrics would ensure that the models have learned the molecule grammar well and are not overfitting to the training data simultaneously. Internal diversity scores give an idea about the extent of chemical space traversed by different models. FCD and KL divergence measure how well the model captures the statistics and distribution of the features of the data set, respectively. So, MolGPT is compared with previous approaches on these criteria. All metrics except validity are computed on the set of valid molecules generated by the model. We compare the performance of MolGPT on the MOSES data set to that of CharRNN, VAE, AAE, LatentGAN, and JT-VAE. JT-VAE uses graphs as input, while the others use SMILES. The model performance with a temperature of 1.0 on each data set is reported in Table <ref type="table" target="#tab_0">1</ref> and<ref type="table">Table 2</ref>.</p><p>On the MOSES benchmark, MolGPT has the best FCD score for molecules as well as their scaffolds. This indicates that the model has learned the data set statistics very well. It performs on par with other models in terms of the two internal diversity metrics. In the case of validity, as mentioned earlier,  JT-VAE always generates a valid molecule because it checks validity at every step of generation. Barring JT-VAE, we observe that MolGPT performs best at generating valid and unique molecules. MolGPT has a near perfect validity score on the MOSES data set without the use of explicit constraints, indicating strong learning of the SMILES grammer and modeling of long-term dependencies that can be attributed to the attention mechanisms. MolGPT, however, has a low novelty score on the data set, being only slightly better than the AAE. On the GuacaMol benchmark, MolGPT demonstrates the best results on validity, novelty, and KL divergence, while its FCD is only 0.006 less than the RNN. So it is the preferred method when compared to other methods tested on it. It returns very high validity, uniqueness, and novelty scores on generation with a sampling temperature of 1.0. We believe this boost in novelty, as compared to MOSES, is due to a larger diversity in molecules in the GuacaMol data set (see Figure <ref type="figure" target="#fig_0">1</ref>). Moreover, even though the GuacaMol data set has larger molecules as compared to the MOSES data set, MolGPT generates molecules with very high validity, also indicating that this method handles long-range dependencies very well.</p><p>While it is important to develop machine learning frameworks and pipelines for making tasks more efficient, it is desirable to also demonstrate that these models allow for interpretation. We use saliency maps to visualize the molecular generation process for our model. The usage of saliency maps has been well established in the context of transformer models and have been used previously for synthesis prediction. <ref type="bibr" target="#b52">53</ref> Figure <ref type="figure" target="#fig_4">4</ref> shows input saliency maps for some of the generated tokens of the shown generated molecule. Input saliency methods assign a score to each input token that indicates the importance of that token in generating the next token. "(", "C", and "c" refer to the branching from chain, nonaromatic carbon, and aromatic carbon, respectively. From Figure <ref type="figure" target="#fig_4">4</ref>, we see that when generating the "O" atom in the first saliency map, the model attends to the previous double bond and "N" atoms. The double bond satisfies the valency of the oxygen atom, and the "N" atom participates in the formation of the tautomer (Lactam and Lactim), which increases the stability of the structure. When generating the "C" atom in the second saliency map, the model attends to "(" and ")" to check if they are balanced and also attends to the atoms in the nonaromatic ring. In the nonaromatic ring, it attends mostly to the immediate neighbors?"2" and "N" atoms. When generating the "2" token, it attends to the immediate previous "C" token and the tokens in the nonaromatic ring. When generating "c" tokens in the last and second to last row of the saliency maps, the model rightly attends to the atoms in the aromatic ring since that ring is still incomplete. Thus, these saliency maps provide some insight into the chemical interpretability of the generative process.  Generation-based on Single and Multiple Properties. Many processes in biology and chemistry require molecules to have certain property values in order to perform some functions. For example, for molecules to penetrate the blood-brain barrier (and thus act on receptors in the central nervous system), a TPSA value less than 90 ? 2 is usually needed. <ref type="bibr" target="#b53">54</ref> This motivates the need for models to have accurate conditional generation. So, the next objective is to evaluate the ability of MolGPT to generate molecules that exhibit specific properties (conditional generation). Since GuacaMol has a wider range in property values, we test the model's ability to control molecular properties trained on it. While only logP, SAS, TPSA, and QED are used for property control, we would like to note that the model can be trained to learn any property that is inferred from the molecule's 2D structure. For each condition, 10 000 molecules are generated to evaluate property control.</p><p>Distributions of molecular properties of MolGPT generated molecules while controlling a single property are depicted in Figure <ref type="figure" target="#fig_5">5</ref>. The mean average deviation (MAD), standard deviation (SD), validity, uniqueness, and novelty values for each property are reported in Table <ref type="table" target="#tab_1">3</ref>. As seen in Figure <ref type="figure" target="#fig_5">5</ref>, the distribution of properties is centered around the desired value. This is further exemplified by the low SD and MAD scores (relative to the range of the property values) in Table <ref type="table" target="#tab_1">3</ref>.  While generating molecules for specific purposes, in other words de novo design of molcules, it is necessary to optimize more than one property. For example, one may want molecules that have specific values for logP and TPSA. Hence, we check the model's capacity to control multiple properties simultaneously. For this, SAS, logP, and TPSA are used. We evaluate the model's ability to generate desired distributions using two and three property controls at a time. Generated distribution of molecule properties is depicted in Figure <ref type="figure" target="#fig_6">6</ref>. Well separated clusters centered at the desired property values are observed. As before, the low MAD and SD values for each property combination, reported in Table <ref type="table" target="#tab_2">4</ref> (as compared to the range of property values), indicate the strong control that MolGPT has over multiple properties for accurate generation.</p><p>Generation Based on Scaffold. The above section demonstrated the ability of MolGPT to generate molecules with desired properties. In certain exercises, for example, lead optimization, chemists intend to generate molecules containing a specific scaffold/skeleton and at the same time achieve desired property values. We evaluate the ability of MolGPT to generate structures with certain property values while maintaining the structure of the scaffold, the results of which are presented in this and the next sections. We conduct these experiments on the MOSES benchmark data set as it contains a set of test scaffolds that are non-overlapping with the set of scaffolds that are present in the training set. We select a random set of 100 test scaffolds, then generate 100 molecules for each scaffold followed by calculation of validity, uniqueness, novelty, and "similarity ratio". "Similarity ratio" is defined as the fraction of valid generated molecules having Tanimoto similarity of the scaffold of the generated molecule and the conditioned scaffold greater than 0.8. Initially, Murcko scaffolds are obtained from the generated molecules. Tanimoto similarity is then calculated for the fingerprints of the Murcko scaffolds of the generated molecule and the conditional scaffold. We use the RDkit fingerprints with default settings for the same. The distribution of each of the metrics in terms of box plot is shown in Figure <ref type="figure">7</ref>. From the boxplot, it can be seen that for all 100 scaffolds, the validity is greater than 0.8. Around 75% of scaffolds have uniqueness and novelty greater than 0.7. All of the scaffolds have a "similarity ratio" greater than 0.8, which suggests that most of the generated valid molecules have very similar scaffolds to the scaffold used for conditioning. The fraction of generated molecules that maintained the exact same scaffold structure as the conditioning is found to be 0.9897. Some examples of generated molecules for two scaffolds are given in Figure <ref type="figure" target="#fig_1">S2</ref> of the Supporting Information. In all of the generated molecules, the conditioned scaffold structure is maintained.</p><p>Generation Based on Scaffold and Property. We evaluate the models' ability to generate structures containing desired scaffolds while also controlling multiple molecular properties. For our experiments, five scaffolds of different sizes were randomly chosen from the MOSES test set (Figure <ref type="figure" target="#fig_0">S1</ref> of the Supporting Information). In these experiments, we define valid molecules as those molecular graphs that satisfy chemical valencies and contain scaffolds that have a Tanimoto similarity of at least 0.8 to the desired scaffold. The validity score of all scaffold based experiments is calculated based on this definition. Generated distributions for single property control can be seen in Figure <ref type="figure" target="#fig_7">8</ref>. Tanimoto similarity is calculated between the scaffold of the generated molecule and the conditional scaffold. Distribution of these Tanimoto similarity scores is also plotted in Figure <ref type="figure" target="#fig_7">8</ref>. The distribution plots peak at 1 for all of the scaffolds and properties. Since scaffold based generation is more constraining for property control, generated distributions are not as narrow and well separated as before. We also define a new metric called Same Scaffold Fraction (SSF) defined as the percentage of generated molecules that contain the same scaffold as the condition. The quantitative results along with SSF for single property control are reported in Table <ref type="table" target="#tab_0">S1</ref> in the Supporting Information. The low MAD and SD scores still show that MolGPT deviates only slightly from intended values  Journal of Chemical Information and Modeling despite the constraints. QED is a function that is dependent on multiple molecular properties simultaneously. Therefore, QED is greatly influenced by the structure of the scaffold itself, making it very hard to control under such constraints. We believe such competing objectives are the reason for large overlap between distributions generated for QED control. Figure <ref type="figure" target="#fig_2">S3</ref> of the Supporting Information shows the molecules conditioned on scaffold + logP and scaffold + SAS. MolGPT adds different functional groups to the scaffold in order to get the desired property value. Multiproperty control clusters are plotted in Figure <ref type="figure" target="#fig_8">9</ref>. Even when using multiple properties, we see the Tanimoto similarity distributions peaking at 1 in Figure <ref type="figure" target="#fig_9">10</ref>. Understandably, property-based clusters are not as well formed as before. However, there is a good separation between the clusters for two property control. The intended values of molecular properties are close to the centers of these clusters. This can further be verified by results reported for multiproperty control in Table <ref type="table">S2</ref> in the Supporting Information. For three property control, one of the clusters (red) is not well formed due to highly constraining property values. We see that the rest of the clusters are largely well formed and separated.</p><p>Next, we show examples where conditional generation could be used to optimize simple properties of a molecule. To demonstrate this, three scaffolds from the test set having a QED around 0.4 are sampled. Using these scaffolds and a QED of 0.9 as the condition, we generate molecules using MolGPT. Sample generated molecules are shown in Figure <ref type="figure" target="#fig_10">11</ref>. The scaffold structure is maintained in the generated molecules and their QED values are around 0.9. We also show other examples, where the TPSA, LogP, and scaffold strucuture are maintained and the SAS is improved to more desirable values in Figure <ref type="figure" target="#fig_11">12</ref>. We would like to note that the model here is used only to optimize simple molecular properties and does not ensure its usability in complex tasks such as lead optimization. The model design, however, points to a possible research direction for other conditional generation tasks, such as generating molecules with better docking scores, for which sufficiently large data sets can be created.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? CONCLUSION</head><p>In this work, we designed a Transformer-Decoder model called MolGPT for molecular generation. This model utilizes masked self-attention mechanisms that make it simpler to learn long-range dependencies between string tokens. This is especially useful to learn the semantics of valid SMILES strings that satisfy valencies and ring closures. We see through our benchmarking experiments that MolGPT shows very high validity and uniqueness scores for the MOSES and GuacaMol data sets. It also demonstrates good FCD and KL divergence scores on both the MOSES and Guacamol data sets. Furthermore, as shown, the generative process can be interpreted using saliency maps. Thus, MolGPT is able to show good performance on both data sets with it outperforming all other methods benchmarked on the GuacaMol data set in terms of validity and novelty.</p><p>We also show that the model learns higher level chemical representations through molecular property control. MolGPT is able to generate molecules with property values that deviate only slightly from the exact values that are passed by the user. It is also able to generate molecules containing user specified scaffolds while controlling these properties. It does this with good accuracy despite the constraining conditions of scaffoldbased drug design. Consequently, we believe that the MolGPT model should be considered a strong architecture to be used by itself or incorporated into other molecular generation techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? ASSOCIATED CONTENT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>* s? Supporting Information</head><p>The Supporting Information is available free of charge at https://pubs.acs.org/doi/10.1021/acs.jcim.1c00600.</p><p>Results of scaffold and property conditioning, figures of scaffolds, generated molecules from scaffold conditioning, as well as scaffold and property conditioning experiments (PDF)</p><p>? AUTHOR INFORMATION Corresponding Author U. Deva Priyakumar -International Institute of Information Technology, Hyderabad 500 032, India; orcid.org/0000-0001-7114-3955; Email: deva@iiit.ac.in</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Probability distributions of properties (log P, molecular weight, QED, SAS, TPSA, and SMILES length) of molecules in the MOSES and GuacaMol data sets.</figDesc><graphic url="image-3.png" coords="2,77.67,60.21,451.67,256.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Pipeline for training and generation using the MolGPT model.</figDesc><graphic url="image-4.png" coords="3,118.09,60.21,370.77,146.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. MolGPT model architecture.</figDesc><graphic url="image-5.png" coords="3,107.32,238.22,392.37,182.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>p</head><label></label><figDesc>Journal of Chemical Information and ModelingWe report IntDiv 1 (p = 1) and IntDiv 2 (p = 2) in this work.? Frechet ChemNet Distance 52 (FCD): calculated using the features of the generated molecules and the features of molecules in the data set. The features are obtained from the penultimate layer of the ChemNet model. Low FCD values indicate that the model has successfully captured the statistics of the data set. Mathematically, FCD between a generated distribution G and training data distribution D is defined as follows.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Input saliency maps for the shown generated molecule. The dark purple underlines are the tokens under consideration for saliency maps. The intensity of color of each token indicates the importance of that token for generating the underlined token.</figDesc><graphic url="image-6.png" coords="5,316.97,178.24,237.09,153.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Distribution of property of generated molecules conditioned on (a) logP, (b) TPSA, (c) SAS, and (d) QED. Distribution depicted using a solid red line corresponds to the whole data set. Trained on GuacaMol data set with temperature = 1.0.</figDesc><graphic url="image-7.png" coords="6,117.52,60.21,371.96,305.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Distribution of property of generated molecules conditioned on (a) TPSA + logP, (b) SAS + logP, (c) SAS + TPSA, and (d) TPSA + logP + SAS. The values that the generation is conditioned to are given in the legends of the panels.</figDesc><graphic url="image-8.png" coords="7,113.10,60.21,380.86,371.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Distribution of property of generated molecules conditioned on Scaffold + (a) logP, (c) SAS, (e) TPSA, and (g) QED. Distribution of Tanimoto similarity of the scaffolds of the generated molecules and the scaffold used for conditioning for (b) logP, (d) SAS, (f) TPSA, and (h) QED. Trained on MOSES data set.</figDesc><graphic url="image-10.png" coords="8,125.92,60.21,355.18,600.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Distribution of property of generated molecules conditioned on Scaffold + (a) TPSA + logP, (b) SAS + TPSA, (c) + logP, and (d) TPSA + logP + SAS. Trained on MOSES data set.</figDesc><graphic url="image-11.png" coords="9,117.01,60.21,372.93,359.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Distribution of Tanimoto similarity of the scaffolds of the generated molecules and the scaffold used for conditioning for (a) TPSA + logP, (b) SAS + TPSA, (c) SAS + logP, and (d) TPSA + logP + SAS. Trained on MOSES data set.</figDesc><graphic url="image-12.png" coords="10,117.30,60.21,372.47,300.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Optimization of QED value conditioned on the scaffold</figDesc><graphic url="image-13.png" coords="10,153.52,402.24,299.96,239.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 .</head><label>12</label><figDesc>Figure 12. SAS reduced, maintaining TPSA, logP, and scaffold structure.</figDesc><graphic url="image-14.png" coords="11,153.52,60.21,299.96,309.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison of the Different Metrics Corresponding to Nonconditioned Generation of Molecules Using Different Approaches Trained on MOSES Data Set</figDesc><table><row><cell>models</cell><cell>validity</cell><cell>unique@10K</cell><cell>novelty</cell><cell>IntDiv 1</cell><cell>IntDiv 2</cell><cell>FCD/Test</cell><cell>FCD/TestSF</cell></row><row><cell>CharRNN</cell><cell>0.975</cell><cell>0.999</cell><cell>0.842</cell><cell>0.856</cell><cell>0.85</cell><cell>0.0732</cell><cell>0.5204</cell></row><row><cell>VAE</cell><cell>0.977</cell><cell>0.998</cell><cell>0.695</cell><cell>0.856</cell><cell>0.85</cell><cell>0.099</cell><cell>0.567</cell></row><row><cell>AAE</cell><cell>0.937</cell><cell>0.997</cell><cell>0.793</cell><cell>0.856</cell><cell>0.85</cell><cell>0.555</cell><cell>1.057</cell></row><row><cell>LatentGAN</cell><cell>0.897</cell><cell>0.997</cell><cell>0.949</cell><cell>0.857</cell><cell>0.85</cell><cell>0.2968</cell><cell>0.8281</cell></row><row><cell>JT-VAE</cell><cell>1.0</cell><cell>0.999</cell><cell>0.914</cell><cell>0.855</cell><cell>0.849</cell><cell>0.395</cell><cell>0.938</cell></row><row><cell>MolGPT</cell><cell>0.994</cell><cell>1.0</cell><cell>0.797</cell><cell>0.857</cell><cell>0.851</cell><cell>0.067</cell><cell>0.507</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Comparison of Different Metrics while Generating Molecules Conditioned on Single Property Based on Training on GuacaMol Data Set (Temperature Value of 1.0 Was Used)</figDesc><table><row><cell>condition</cell><cell>validity</cell><cell>unique</cell><cell>novelty</cell><cell>MAD</cell><cell>SD</cell></row><row><cell>logP</cell><cell>0.971</cell><cell>0.998</cell><cell>1.0</cell><cell>0.23</cell><cell>0.31</cell></row><row><cell>TPSA</cell><cell>0.972</cell><cell>0.996</cell><cell>1.0</cell><cell>3.52</cell><cell>4.66</cell></row><row><cell>SAS</cell><cell>0.977</cell><cell>0.995</cell><cell>1.0</cell><cell>0.13</cell><cell>0.2</cell></row><row><cell>QED</cell><cell>0.975</cell><cell>0.998</cell><cell>1.0</cell><cell>0.056</cell><cell>0.075</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Multiproperty Conditional Training on GuacaMol Data Set</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MAD/SD</cell><cell></cell></row><row><cell>condition</cell><cell cols="3">validity unique novelty</cell><cell>TPSA</cell><cell>logP</cell><cell>SAS</cell></row><row><cell>SAS + logP</cell><cell>0.972</cell><cell>0.992</cell><cell>1.0</cell><cell></cell><cell>0.250/</cell><cell>0.140/</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.340</cell><cell>0.210</cell></row><row><cell>SAS + TPSA</cell><cell>0.971</cell><cell>0.988</cell><cell>1.0</cell><cell>3.760/</cell><cell></cell><cell>0.150/</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>4.940</cell><cell></cell><cell>0.220</cell></row><row><cell>TPSA + logP</cell><cell>0.965</cell><cell>0.994</cell><cell>1.0</cell><cell>3.710/</cell><cell>0.240/</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>4.770</cell><cell>0.320</cell><cell></cell></row><row><cell>TPSA + logP</cell><cell>0.973</cell><cell>0.969</cell><cell>1.0</cell><cell>3.790/</cell><cell>0.270/</cell><cell>0.180/</cell></row><row><cell>+ SAS</cell><cell></cell><cell></cell><cell></cell><cell>4.800</cell><cell>0.350</cell><cell>0.260</cell></row></table><note><p>Figure 7. Boxplot of the evaluation metrics for the scaffold conditioned results.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>https://doi.org/10.1021/acs.jcim.1c00600 J. Chem. Inf. Model. 2022, 62, 2064-2076</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>? ACKNOWLEDGMENTS</head><p>The authors thank <rs type="person">Manasa Kondamadugu</rs>, <rs type="person">Yashaswi Pathak</rs>, <rs type="person">Sarvesh Mehta</rs>, and <rs type="person">Manan Goel</rs> for their comments during the preparation of the manuscript. We thank <rs type="person">IHub-Data</rs>, <rs type="institution">IIIT Hyderabad</rs>, and <rs type="funder">Kohli Center on Intelligent Systems, IIIT Hyderabad</rs> for financial support. Data and method implementation is available at https://github.com/devalab/molgpt.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Estimation of the size of drug-like chemical space based on GDB-17 data</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Polishchuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">I</forename><surname>Madzhidov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Varnek</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10822-013-9672-4</idno>
	</analytic>
	<monogr>
		<title level="j">J. Comput.-Aided Mol. Des</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="675" to="679" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">PubChem substance and compound databases</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Thiessen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">E</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gindulyte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Shoemaker</surname></persName>
		</author>
		<idno type="DOI">10.1093/nar/gkv951</idno>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="1202" to="D1213" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Pretraining of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The rise of deep learning in drug discovery</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Engkvist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Olivecrona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Blaschke</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.drudis.2018.01.039</idno>
	</analytic>
	<monogr>
		<title level="j">Drug Discovery Today</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1241" to="1250" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Inverse molecular design using machine learning: Generative models for matter engineering</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sanchez-Lengeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.aat2663</idno>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">361</biblScope>
			<biblScope unit="page" from="360" to="365" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">GuacaMol: benchmarking models for de novo molecular design</title>
		<author>
			<persName><forename type="first">N</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fiscato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Segler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Vaucher</surname></persName>
		</author>
		<idno type="DOI">10.1021/acs.jcim.8b00839?urlappend=%3Fref%3DPDF&amp;jav=VoR&amp;rel=cite-as</idno>
	</analytic>
	<monogr>
		<title level="j">J. Chem. Inf. Model</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="1096" to="1108" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Molecular sets (moses): A benchmarking platform for molecular generation models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Polykovskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhebrak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sanchez-Lengeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Golovanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Tatanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belyaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kurbanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Artamonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Aladinskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Veselov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kadurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nikolenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhavoronkov</surname></persName>
		</author>
		<idno type="DOI">10.3389/fphar.2020.565644</idno>
	</analytic>
	<monogr>
		<title level="j">Front. Pharmacol</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules</title>
		<author>
			<persName><forename type="first">D</forename><surname>Weininger</surname></persName>
		</author>
		<idno type="DOI">10.1021/ci00057a005?urlappend=%3Fref%3DPDF&amp;jav=VoR&amp;rel=cite-as</idno>
	</analytic>
	<monogr>
		<title level="j">J. Chem. Inf. Model</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="31" to="36" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Chemically interpretable graph interaction network for prediction of pharmacokinetic properties of drug-like molecules</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laghuvarapu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">D</forename><surname>Priyakumar</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v34i01.5433</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="873" to="880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generating focused molecule libraries for drug discovery with recurrent neural networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Segler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kogej</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tyrchan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Waller</surname></persName>
		</author>
		<idno type="DOI">10.1021/acscentsci.7b00512?urlappend=%3Fref%3DPDF&amp;jav=VoR&amp;rel=cite-as</idno>
	</analytic>
	<monogr>
		<title level="j">ACS Cent. Sci</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="120" to="131" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generative recurrent networks for de novo drug design</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Huisman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Schneider</surname></persName>
		</author>
		<idno type="DOI">10.1002/minf.201700111</idno>
	</analytic>
	<monogr>
		<title level="j">Mol. Inf</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">1700111</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for de novo drug design</title>
		<author>
			<persName><forename type="first">M</forename><surname>Popova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Isayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tropsha</surname></persName>
		</author>
		<idno type="DOI">10.1126/sciadv.aap7885</idno>
	</analytic>
	<monogr>
		<title level="j">Sci. Adv</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7885</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Molecular de-novo design through deep reinforcement learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Olivecrona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Blaschke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Engkvist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1186/s13321-017-0235-x</idno>
	</analytic>
	<monogr>
		<title level="j">J. Cheminf</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">48</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Constrained graph variational autoencoders for molecule design</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gaunt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="7795" to="7804" />
		</imprint>
	</monogr>
	<note>Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Grammar variational autoencoder</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Paige</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hern?ndez-Lobato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01925</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Graphvae: Towards generation of small graphs using variational autoencoders. International Conference on Artificial Neural Networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01418-6_41</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11139</biblScope>
			<biblScope unit="page" from="412" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Junction tree variational autoencoder for molecular graph generation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04364</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Molecular generative model based on conditional variational autoencoder for de novo molecular design</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ryu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.1186/s13321-018-0286-7</idno>
	</analytic>
	<monogr>
		<title level="j">J. Cheminf</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">druGAN: an advanced generative adversarial autoencoder model for de novo generation of new molecules with desired molecular properties in silico</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kadurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nikolenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Khrabrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aliper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhavoronkov</surname></persName>
		</author>
		<idno type="DOI">10.1021/acs.molpharmaceut.7b00346?urlappend=%3Fref%3DPDF&amp;jav=VoR&amp;rel=cite-as</idno>
	</analytic>
	<monogr>
		<title level="j">Mol. Pharmaceutics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="3098" to="3104" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adversarial threshold neural computer for molecular de novo design</title>
		<author>
			<persName><forename type="first">E</forename><surname>Putin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Asadulaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Vanhaelen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ivanenkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Aladinskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aliper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhavoronkov</surname></persName>
		</author>
		<idno type="DOI">10.1021/acs.molpharmaceut.7b01137?urlappend=%3Fref%3DPDF&amp;jav=VoR&amp;rel=cite-as</idno>
	</analytic>
	<monogr>
		<title level="j">Mol. Pharmaceutics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="4386" to="4397" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Entangled conditional adversarial autoencoder for de novo drug discovery</title>
		<author>
			<persName><forename type="first">D</forename><surname>Polykovskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhebrak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ivanenkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Aladinskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mamoshina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bozdaganyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aliper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhavoronkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kadurin</surname></persName>
		</author>
		<idno type="DOI">10.1021/acs.molpharmaceut.8b00839?urlappend=%3Fref%3DPDF&amp;jav=VoR&amp;rel=cite-as</idno>
	</analytic>
	<monogr>
		<title level="j">Mol. Pharmaceutics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="4398" to="4405" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Molecular Generative Model Based on an Adversarially Regularized Autoencoder</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ryu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.1021/acs.jcim.9b00694?urlappend=%3Fref%3DPDF&amp;jav=VoR&amp;rel=cite-as</idno>
	</analytic>
	<monogr>
		<title level="j">J. Chem. Inf. Model</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="29" to="36" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Randomized SMILES strings improve the quality of molecular generative models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Aru?-Pous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Prykhodko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Bjerrum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tyrchan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Reymond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Engkvist</surname></persName>
		</author>
		<idno type="DOI">10.1186/s13321-019-0393-0</idno>
	</analytic>
	<monogr>
		<title level="j">J. Cheminf</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Bjerrum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07076</idno>
		<title level="m">SMILES enumeration as data augmentation for neural network modeling of molecules</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improving chemical autoencoder latent space and molecular de novo generation diversity with heteroencoders</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Bjerrum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sattarov</surname></persName>
		</author>
		<idno type="DOI">10.3390/biom8040131</idno>
	</analytic>
	<monogr>
		<title level="j">Biomolecules</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">131</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep learning enabled inorganic material generator</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Juneja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ehara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">D</forename><surname>Priyakumar</surname></persName>
		</author>
		<idno type="DOI">10.1039/D0CP03508D</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Chem. Chem. Phys</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="26935" to="26943" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A de novo molecular generation method using latent vector based generative adversarial network</title>
		<author>
			<persName><forename type="first">O</forename><surname>Prykhodko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-C</forename><surname>Kotsias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Aru?-Pous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Bjerrum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Engkvist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1186/s13321-019-0397-9</idno>
	</analytic>
	<monogr>
		<title level="j">J. Cheminf</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">74</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Objective-reinforced generative adversarial networks (ORGAN) for sequence generation models</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Guimaraes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sanchez-Lengeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Outeiral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L C</forename><surname>Farias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.10843</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Optimizing distributions over molecular space. An Objective-Reinforced Generative Adversarial Network for Inversedesign Chemistry (ORGANIC</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sanchez-Lengeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Outeiral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Guimaraes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ChemRxiv</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Cambridge Open Engage</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">MolGAN: An implicit generative model for small molecular graphs</title>
		<author>
			<persName><forename type="first">N</forename><surname>De Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kipf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.11973</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Reinforced adversarial neural computer for de novo molecular design</title>
		<author>
			<persName><forename type="first">E</forename><surname>Putin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Asadulaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ivanenkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Aladinskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sanchez-Lengeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhavoronkov</surname></persName>
		</author>
		<idno type="DOI">10.1021/acs.jcim.7b00690?urlappend=%3Fref%3DPDF&amp;jav=VoR&amp;rel=cite-as</idno>
	</analytic>
	<monogr>
		<title level="j">J. Chem. Inf. Model</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="1194" to="1204" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Automatic chemical design using a data-driven continuous representation of molecules</title>
		<author>
			<persName><forename type="first">R</forename><surname>G?mez-Bombarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hern?ndez-Lobato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>S?nchez-Lengeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sheberla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Aguilera-Iparraguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<idno type="DOI">10.1021/acscentsci.7b00572?urlappend=%3Fref%3DPDF&amp;jav=VoR&amp;rel=cite-as</idno>
	</analytic>
	<monogr>
		<title level="j">ACS Cent. Sci</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="268" to="276" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">MEMES: Machine learning framework for Enhanced MolEcular Screening</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laghuvarapu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sethi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alvala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">D D</forename><surname>Priyakumar</surname></persName>
		</author>
		<idno type="DOI">10.1039/D1SC02783B</idno>
	</analytic>
	<monogr>
		<title level="j">Chemical Science</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">11710</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Efficient multi-objective molecular optimization in a continuous latent space</title>
		<author>
			<persName><forename type="first">R</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Montanari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Steffen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Briem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>No?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-A</forename><surname>Clevert</surname></persName>
		</author>
		<idno type="DOI">10.1039/C9SC01928F</idno>
	</analytic>
	<monogr>
		<title level="j">Chemical science</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="8016" to="8024" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep-learning-based inverse design model for intelligent discovery of organic molecules</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-J</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hwang</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41524-018-0128-1</idno>
	</analytic>
	<monogr>
		<title level="j">Comput. Mater</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Mol-CycleGAN: a generative model for molecular optimization</title>
		<author>
			<persName><forename type="first">?</forename><surname>Maziarka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pocha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kaczmarczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rataj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Danel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Warcho?</surname></persName>
		</author>
		<idno type="DOI">10.1186/s13321-019-0404-1</idno>
	</analytic>
	<monogr>
		<title level="j">J. Cheminf</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Unpaired image-toimage translation using cycle-consistent adversarial networks</title>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.244</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2242" to="2251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Direct steering of de novo molecular generation with descriptor conditional recurrent neural networks</title>
		<author>
			<persName><forename type="first">P.-C</forename><surname>Kotsias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Aru?-Pous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Engkvist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tyrchan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Bjerrum</surname></persName>
		</author>
		<idno type="DOI">10.1038/s42256-020-0174-5</idno>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="254" to="265" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">SMILES-based deep generative scaffold decorator for de-novo drug design</title>
		<author>
			<persName><forename type="first">J</forename><surname>Aru?-Pous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Patronov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Bjerrum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tyrchan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Reymond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Engkvist</surname></persName>
		</author>
		<idno type="DOI">10.1186/s13321-020-00441-8</idno>
	</analytic>
	<monogr>
		<title level="j">J. Cheminf</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-Y</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.13639</idno>
		<title level="m">Scaffoldbased molecular design using graph generative model</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Language Models are Unsupervised Multitask Learners</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">ZINC-a free database of commercially available compounds for virtual screening</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Irwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Shoichet</surname></persName>
		</author>
		<idno type="DOI">10.1021/ci049714+?urlappend=%3Fref%3DPDF&amp;jav=VoR&amp;rel=cite-as</idno>
	</analytic>
	<monogr>
		<title level="j">J. Chem. Inf. Model</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="177" to="182" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">The ChEMBL database in 2017</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gaulton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hersey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nowotka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Bento</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mendez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mutowo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Atkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Bellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cibri?n-Uhalte</surname></persName>
		</author>
		<idno type="DOI">10.1093/nar/gkw1074</idno>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="945" to="D954" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">RDKit: A Software Suite for Cheminformatics, Computational Chemistry, and Predictive Modeling</title>
		<author>
			<persName><forename type="first">G</forename><surname>Landrum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">The properties of known drugs. 1. Molecular frameworks</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Bemis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Murcko</surname></persName>
		</author>
		<idno type="DOI">10.1021/jm9602928?urlappend=%3Fref%3DPDF&amp;jav=VoR&amp;rel=cite-as</idno>
	</analytic>
	<monogr>
		<title level="j">J. Med. Chem</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2887" to="2893" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Estimation of synthetic accessibility score of drug-like molecules based on molecular complexity and fragment contributions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ertl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schuffenhauer</surname></persName>
		</author>
		<idno type="DOI">10.1186/1758-2946-1-8</idno>
	</analytic>
	<monogr>
		<title level="j">J. Cheminf</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Quantifying the chemical beauty of drugs</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Bickerton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">V</forename><surname>Paolini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Besnard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Muresan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Hopkins</surname></persName>
		</author>
		<idno type="DOI">10.1038/nchem.1243</idno>
	</analytic>
	<monogr>
		<title level="j">Nat. Chem</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="90" to="98" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Fre?het ChemNet distance: a metric for generative models for molecules in drug discovery</title>
		<author>
			<persName><forename type="first">K</forename><surname>Preuer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Renz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Klambauer</surname></persName>
		</author>
		<idno type="DOI">10.1021/acs.jcim.8b00234?urlappend=%3Fref%3DPDF&amp;jav=VoR&amp;rel=cite-as</idno>
	</analytic>
	<monogr>
		<title level="j">J. Chem. Inf. Model</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="1736" to="1741" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Molecular transformer: a model for uncertaintycalibrated chemical reaction prediction</title>
		<author>
			<persName><forename type="first">P</forename><surname>Schwaller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Laino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gaudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bolgar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Hunter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bekas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1021/acscentsci.9b00576?urlappend=%3Fref%3DPDF&amp;jav=VoR&amp;rel=cite-as</idno>
	</analytic>
	<monogr>
		<title level="j">ACS Cent. Sci</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1572" to="1583" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Rapid calculation of polar molecular surface area and its application to the prediction of transport phenomena. 1. Prediction of intestinal absorption</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Clark</surname></persName>
		</author>
		<idno type="DOI">10.1021/acs.jcim.1c00600?urlappend=%3Fref%3DPDF&amp;jav=VoR&amp;rel=cite-as</idno>
		<ptr target="https://doi.org/10.1021/acs.jcim.1c00600" />
	</analytic>
	<monogr>
		<title level="j">J. Pharm. Sci</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="807" to="814" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">2076 Recommended by ACS Can We Quickly Learn to &quot;Translate</title>
		<idno type="DOI">10.1021/acs.jcim.2c01618?utm_campaign=RRCC_jcisd8&amp;utm_source=RRCC&amp;utm_medium=pdf_stamp&amp;originated=1703987934&amp;referrer_DOI=10.1021%2Facs.jcim.1c00600</idno>
	</analytic>
	<monogr>
		<title level="j">J. Chem. Inf. Model</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="2064" to="2076" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Bioactive Molecules. with Transformer Models?</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
