<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Scalability and Performance Evaluation of Edge Cloud Systems for Latency Constrained Applications</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sumit</forename><surname>Maheshwari</surname></persName>
							<email>sumitm@winlab.rutgers.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">WINLAB</orgName>
								<orgName type="institution" key="instit2">Rutgers University</orgName>
								<address>
									<addrLine>North Brunswick</addrLine>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dipankar</forename><surname>Raychaudhuri</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">WINLAB</orgName>
								<orgName type="institution" key="instit2">Rutgers University</orgName>
								<address>
									<addrLine>North Brunswick</addrLine>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ivan</forename><surname>Seskar</surname></persName>
							<email>seskar@winlab.rutgers.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">WINLAB</orgName>
								<orgName type="institution" key="instit2">Rutgers University</orgName>
								<address>
									<addrLine>North Brunswick</addrLine>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Francesco</forename><surname>Bronzino</surname></persName>
							<email>francesco.bronzino@inria.fr</email>
							<affiliation key="aff1">
								<orgName type="institution">Inria</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Scalability and Performance Evaluation of Edge Cloud Systems for Latency Constrained Applications</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AF87A4E7CA34F5E284CDF644CCF7168D</idno>
					<idno type="DOI">10.1109/SEC.2018.00028</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Cloud Computing</term>
					<term>Mobile Edge Cloud</term>
					<term>Fog Computing</term>
					<term>Real-time Applications</term>
					<term>Augmented Reality</term>
					<term>System Modeling</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents an analysis of the scalability and performance of an edge cloud system designed to support latency-sensitive applications. A system model for geographically dispersed edge clouds is developed by considering an urban area such as Chicago and co-locating edge computing clusters with known Wi-Fi access point locations. The model also allows for provisioning of network bandwidth and processing resources with specified parameters in both edge and the cloud. The model can then be used to determine application response time (sum of network delay, compute queuing and compute processing time), as a function of offered load for different values of edge and core compute resources, and network bandwidth parameters. Numerical results are given for the city-scale scenario under consideration to show key systemlevel trade-offs between edge cloud and conventional cloud computing. Alternative strategies for routing service requests to edge vs. core cloud clusters are discussed and evaluated. Key conclusions from the study are: (a) the core cloud-only system outperforms the edge-only system having low inter-edge bandwidth, (b) a distributed edge cloud selection scheme can approach the global optimal assignment when the edge has sufficient compute resources and high inter-edge bandwidth, and (c) adding capacity to an existing edge network without increasing the inter-edge bandwidth contributes to networkwide congestion and can reduce system capacity.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Edge clouds promise to meet the stringent latency requirements of emerging classes of real time applications such as augmented reality (AR) <ref type="bibr" target="#b0">[1]</ref> and virtual reality (VR) <ref type="bibr" target="#b1">[2]</ref> by bringing compute, storage and networking resources closer to user devices <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. Edge compute resources which are strategically placed near the users in the access network do not incur the irreducible propagation delays associated with offloading of compute intensive tasks to a distant data center. In addition, the use of edge computing can also lower wide-area backhaul costs associated with carrying user data back and forth from the central cloud. AR and VR applications enable users to view and interact with virtual objects in real time, hence requiring fast end-to-end delivery of compute services such as image analytics and video Research supported under NSF Future Internet Architecture -Next Phase (FIA-NP) Award CNS-134529 rendering. Previous studies <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b7">[8]</ref> have shown that latency associated with AR or gaming applications can be reduced by migrating some of the delay-sensitive tasks computing tasks to local servers, while maintaining global state in the core cloud.</p><p>While edge clouds have significant potential for improved system-level performance, there are some important tradeoffs between edge and core clouds that need to be considered. Specifically, core clouds implemented as large-scale data centers <ref type="bibr" target="#b8">[9]</ref> have the important advantage of service aggregation from large numbers of users, thus making the traffic volume predictable. Further, service requests entering a large data center can be handled in a close to optimal manner via centralized routing and load balancing <ref type="bibr" target="#b9">[10]</ref> algorithms. In contrast, edge clouds are intrinsically local and have a smaller scale and are thus subject to significantly larger fluctuations in offered traffic due to factors such as correlated events and user mobility. In addition, we note that edge computing systems by definition are distributed across multiple edge networks and hence are associated with considerable heterogeneity in bandwidth and compute resources. Moreover, the data center model of centralized control of resources is not applicable to a distributed system <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref> implemented across multiple edge network domains, possibly involving a multiplicity of service providers.</p><p>A general technology solution for edge clouds will thus require suitable distributed control algorithms and associated control plane protocols necessary for realization. The unique nature of the distributed edge cloud system poses key design challenges such as specification of a control plane for distributed edge, distributed or centralized resource assignment strategies, traffic load balancing, orchestration of computing functions and related network routing of data, mobility management techniques and so on. In order to address these challenges, a simulation based system model is the foundation for understanding performance and evaluating alternative strategies for any of the above design issues.</p><p>This paper presents an analysis of the scalability and performance of a general hybrid edge cloud system which supports latency-sensitive applications. The goal is to provide a better understanding of key system design parameters such as the proportion of resources in local cloud vs. data center, fronthaul and backhaul network bandwidth, relative latency/distance of core and edge clouds, and determine their impact on system level metrics such as average response time and service goodput. Using the model described here, we seek answers to the following questions: (a) How much load can an edge cloud network support without affecting the performance of an application; (b) How does the value of the application delay-constraint affects the capacity of the system; (c) What is the impact of offered load and resource distribution on goodput; (d) Under what circumstances can the core cloud perform better than an edge network and viceversa; and (e) What is the impact of inter-edge (fronthaul) and edge-to-core (backhaul) network bandwidth on system capacity?</p><p>We use a simulation model to study a city scale general multi-tier network as shown in Fig. <ref type="figure" target="#fig_0">1</ref> containing both edge and central cloud servers. The model is used to obtain system capacity and response time for an augmented reality application while analyzing the impact of key parameters resource distribution and fronthaul/backhaul bandwidth. A general optimization framework for the distributed system is proposed and compared with distributed algorithm approaches. The rest of paper is organized as follows. Section II demonstrates the augmented reality application with two use-cases and discusses the need of edge clouds to fulfill their low-latency requirements. Section III details the system model with an emphasis on system design, and performance model to analyze edge clouds using a city scale network including models for application, compute and latency. A baseline distributed resource allocation approach for selecting an edge cloud for an AR application is also detailed in Section III. Section IV presents the performance evaluation of the baseline approach. Section V proposes and evaluates a capacity enhancement heuristic (ECON) for realtime applications. Numerical results to compare ECON and the baseline are given in Section VI. Section VII provides related work in the field and finally, Section VIII concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. AUGMENTED REALITY AND EDGE CLOUDS</head><p>Augmented reality is gaining popularity in numerous fields such as healthcare, visualization, entertainment and ed-ucation. Most of the commercially available AR devices like Atheer AiR <ref type="bibr" target="#b12">[13]</ref>, Microsoft Hololens <ref type="bibr" target="#b13">[14]</ref> and Google Glass [15] have limited power, storage and on-chip computation capabilities for example currently Hololens has storage ∼64 GB and RAM ∼2GB. In turn, these devices often rely upon offloading storage as well as compute to an architecturally centralized cloud server while ensuring application response time.</p><p>The Quality of Experience (QoE) perceived by a user running an AR application using cloud services is a complex combination of network bandwidth, network traffic and compute capabilities of the cloud. First, the bandwidth from end-user to a cloud data center is the minimum bandwidth available across all the hops in the network path, which could be significant when cloud is located far from the user. Second, the network traffic depends upon the network load and congestion, and varies for each individual local network. Edge cloud computing (denoted as "edge" in the following discussions) promises to alleviate the shortcomings of the cloud server by bringing computation, networking and storage closer to the user and providing fast response, context awareness and mobility support <ref type="bibr" target="#b14">[16]</ref>. Therefore, edge computing can be viewed as having the same centralized cloud resources scattered at the mobile network edge and accessed through fast Wi-Fi or 5G access networks. This approach has the potential to provide tightly bounded service response time thereby creating a geographically distributed heterogeneous computing and communication system.</p><p>Edge computing does not replace but complements the cloud infrastructure as edge clouds are resource limited in terms of bandwidth and compute. The multifaceted edge system therefore must be studied in conjunction with the existing core cloud for different user requirements, application types, edge assignments and QoS constraints. Thus, for a resource constrained system it is required to allocate resources per request while taking system capacity into consideration. This leads to a nonlinear optimization problem <ref type="bibr" target="#b15">[17]</ref> due to multiple factors affecting the capacity including but not limited to network bandwidth, resource availability and application type. In order to understand the capacity constraints of a hybrid edge cloud system for a latency sensitive application, we first, analyze the system taking the AR application as an example and later generalize to other applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Use Case Scenario</head><p>Figure <ref type="figure">2</ref>(a) shows the process flow of our implementation of a demo AR application using Microsoft Hololens. A client sends a continuous video stream to the edge server which processes the information based upon application type and returns output to the client. The video stream (30 fps) is processed by OpenCV <ref type="bibr" target="#b16">[18]</ref> 3.3 running on Intel i7 CPU 980, 3.33GHz and 15GB RAM taking ∼20 ms time for processing each frame. The edge server is connected to the Smart Navigation. A user enters a building. The edge in the building has her contextual information from calender entries and GPS. As shown in Fig. <ref type="figure">2</ref>(b) the user is navigated to meet a person in the building using a set of cubes appearing on the device as she moves. Achievable latency is critical here because real-time activities of the user can be disrupted by late arrival of AR information.</p><p>Annotation based assistance. In this scenario, a user looks at an object having a set marker through Hololens with an intention to get supplementary information about the object. In Fig. <ref type="figure">2(c)</ref>, user looks at the printer and the status, ink level, number and current jobs are annotated on the user's display.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Application Flow Timing Diagram</head><p>Figures <ref type="figure">3(a</ref>) and (b) show (not to the scale) timing diagrams of a packet flow in the system for smart meeting and annotation based assistance application respectively. The network delay in both the cases is kept below 10 ms by deploying edge cloud services a single hop away from the AP. In both the scenarios, the processing delay, path finding in the navigation and OpenCV image processing in the annotation application, can be a major bottleneck. The following techniques are used in our implementation to lower the total response time as compared to the traditional core cloud based services: (i) reduction of network latency via higher bandwidth and closer edge cloud service; (ii) passing minimum processed information to the client such as end-to-end coordinates (8 Bytes) per query in case of the navigation and 64-1500 Bytes per frame processed for the annotation application, and (iii) offloading multiple tasks to the edge cloud to minimize local processing at the UE. The AR implementation serves as a guide to the parameters used in the system model described in the next section, which assumes a low-latency requirement (&lt; 50 ms) to run AR applications with acceptable subjective quality <ref type="bibr" target="#b7">[8]</ref>.</p><p>Using our deployed AR applications, this section confirms that: (a) the total application latency can be brought down by reducing the number of hops and increasing available access bandwidth, and (b) although edge cloud lowers the network latency, application processing latency contributes significantly to the total latency for AR applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. SYSTEM MODEL A. System Design</head><p>The system diagram of the hybrid edge cloud under consideration is shown in Fig. <ref type="figure" target="#fig_2">4</ref>. Each AP is equipped with an edge cloud with a configurable compute resource capacity. In general, a compute resource represents a machine or a group of machines (cluster) also known as cloud or edge rack. A rack has limited capacity to support users for their computational requirements. For instance, an AR application requires computation to process video/image stream and receive their response back from the server. The edge rack in our design has maximum five processors each having 3.33 GIPS processing speed. The central cloud server is placed at Salem, Oregon (OR; location chosen to relate with commercially available central clouds) which again has a configurable capacity. The compute capacity is defined as the number of servers available at the edge cloud and/or at the central cloud. The inter-edge bandwidth is varied from 1 Gbps to 100 Gbps and AP-Cloud bandwidth from 10 Gbps to 500 Gbps. The special case of unconstrained inter-edge and AP-cloud bandwidth is also considered. The central controller has the capability to collect network and compute parameters from all the edge clouds and the core cloud. The system design parameters are listed in Table <ref type="table">I</ref>.</p><p>In this study, the total amount of compute available at the edge clouds and core cloud is assumed to be fixed. This assumption holding the compute cost constant allows us to fairly analyze the impact of varying other key system parameters such as % of edge servers or core/edge bandwidth. In our simulation, we increase the resource density of already deployed edge clouds by removing and redistributing compute resources from the central cloud thereby keeping the overall compute resources for the whole system unchanged.</p><p>We use Chicago, the third most populous city in US, as a test-case considering locations of 11,00 WiFi APs <ref type="bibr" target="#b17">[19]</ref> as shown in Fig. <ref type="figure" target="#fig_3">5</ref>. The number of hops from Chicago to  OR varies from 10 to 20 (including switches) and takes around 5-6 hops to reach the cloud server gateway whereas the average latency in US ranges from 13 ms to 106 ms <ref type="bibr" target="#b18">[20]</ref> based on a simple ping test of 64 bytes packet from various locations. The mean local delay in Oregon is as low as 13 ms. It is to be noted that the AR application's bit rate increases rapidly with resolution for instance a 160x120 pixels video needs around 1.7 Mbps whereas a 640x480 pixels video requires 27 Mbps continuous uplink bandwidth (assuming 30 fps, 24 bit per pixel) which goes up to 432 Mbps for 1920x1080 video. For annotation based assistance, assuming each frame is processed for information, relevant data is queried from the database and sent to the user, the required downlink bandwidth varies from 54-600 Mbps.</p><p>The response from the server is sent to the UE as multiple packets (100-1500 Bytes) per frame processed. The uplink bandwidth is assumed to be from 27-300 Mbps as listed in Table <ref type="table">II</ref>. For the simulations in this paper, we used 1280x720 and 1026x576 video size chosen randomly for each user and maintained throughout. Note that the uplink bandwidth requirement for an AR application is more than the download bandwidth due to its uplink video/downlink processed information characteristic which is quite different from most web traffic today. We model the network based on the type of application and its latency requirement.</p><p>We run an AR application at the UE which sends a video stream to the server while server computes the contextual information and sends back the output to the user. The application is annotation-based assistance using AR wherein a user gets information about surrounding annotated on his AR device as described in Section II. Annotation-based assistance can be used in various application scenarios. For example, a policeman looks at a license plate of a car while driving and the information about the owner gets displayed on the device. The license plate can also be run against a list of stolen car and can be immediately reported to the policeman. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Performance Model</head><p>In this section, we describe system modeling aimed at evaluating user performance and system capacity as a function of key design parameters. A multi-tier edge-cloud system as shown in Fig. <ref type="figure" target="#fig_2">4</ref> can be divided into user, network (data and control) and computation plane. Our system design is a hierarchical composition of compute and network elements. The computation at edge or cloud is similar in functionality but different in terms of resources availability as the core cloud has a single big pool of shared resources while each edge cloud has limited resources closer to the user. The following discussion presents application, compute and latency modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Application:</head><p>In our model, the application is defined by a four tuple &lt; V, G, S, L &gt; where V denotes the computational task per unit time ranging from <ref type="bibr">[1, n]</ref>, n ∈ Z + . Each AR application requires these tasks to be completed within a specified real-time threshold latency in order to be useful to the AR application. In case a task is not completed within the application latency threshold, the goodput of system goes down. G denotes the geolocation of the UE. A city is considered to be a collection of G i blocks (assume as cells of a cellular network), i ∈ <ref type="bibr">[1, N]</ref> where N is the total number of geographical blocks. For simplicity, we divide the geographical area into square G i 's. Analyzing the users served by each block provides us meaningful information if we need to upgrade the capacity of an edge cloud in the block. Binary S ∈ {0, 1} denotes the availability of the edge cloud in the geographical area G of a user. Unavailability of an edge cloud may mean that there is no physical edge cloud present or the edge cloud of that region has run out of capacity in which case, a neighboring edge cloud can be chosen or the user can be routed to the central cloud. For delay-tolerant applications, routing a user to the central cloud frees resources at the edge to serve latency sensitive applications. Finally, L ∈ (0, d max ) represents the maximum tolerable latency for the said application.</p><p>2) Compute: The delay due to computation is modeled using a multi-server queuing model. The edge cloud is like a mini data center where tasks arrive from geographically distributed users, processed by the available resources in the edge cloud and depart. Therefore, as the number of transactions in the system increase when the system load rises these tasks are queued till they are processed. This scenario can be best represented by employing an M/M/C queuing model <ref type="bibr" target="#b19">[21]</ref>. Each edge or central cloud processes multiple service requests in a work-conserving FCFS queue with assumed infinite buffers. The overall latency is dependent on the arrival rate λ, service rate μ and the number of servers c. It can be noted that as the system computation power is constant, increasing capacity at the edge will mean removing equivalent resources from the central cloud implying a rise in queuing delay at the cloud. As the system load increases, the arrival rate, λ, rises thereby increasing the total computing latency per task V as d comp = 1/(cμλ) where μ = f/K, f being the rated speed in instructions per second and K is number of instructions required per task.</p><p>For a given set of static users, the system load is proportional to the number of active users and the rate of application requests per second. In our model, we assume 55K users and Load=1 is defined as 10% of the the users are running the application. Load=10 implies that all 55K users are running the AR application 100% of the time. In general, average time spent by a task in the server is the sum of transmission delay, queuing delay and processing delay, which is calculated using the M/M/c queuing model as given below in Eq. <ref type="bibr" target="#b0">(1)</ref><ref type="bibr" target="#b1">(2)</ref><ref type="bibr" target="#b2">(3)</ref>.</p><formula xml:id="formula_0">d node = W + 1 μ + t tx = P Q * ρ λ(1 -ρ) + 1 μ + t tx (1) P Q = (cρ) c c! 1 1 -ρ p 0<label>(2)</label></formula><formula xml:id="formula_1">p 0 = c k=0 (cρ) k k! + (cρ) c c! 1 1 -ρ -1 (3)</formula><p>Here, d node is the total time spent by a task V at the edge cloud or the core cloud, W is the wait time in the queue, P Q is the queuing probability, ρ is the server utilization, c are number of servers at each edge or total server at the cloud, p 0 is the initial probability, and t tx is the average transmission time for a task at an edge as noted in <ref type="bibr" target="#b20">[22]</ref> given by t tx = (N * r)</p><formula xml:id="formula_2">∞ j=1 j(1 -Φ) (j-1)</formula><p>Φ, where Φ is the nonoutage probability of a link implying available bandwidth for a task, r are the number of tasks per user per second and N is the total number of users in the system. In view of shared bandwidth on inter-edge links, t tx can be simplified as b link /r users where b link is the total bandwidth of a link and r users are number of total tasks run by all the users at an edge. For large c, to avoid involved calculations in Eq.</p><p>(2), we split cloud computing resources into set of uniform clusters where a selected cluster is one serving the lowest number of concurrent tasks.</p><p>3) Latency: The overall latency of an application has several components including irreducible propagation delay, the transmission delay, routing node delays and the cloud processing time. For a core cloud server, which carries aggregated traffic, there is also a Software Defined Networking (SDN) switching latency. As the number of users increase in a geographical region, the bandwidth is shared among them costing more transmission delay. For a cloud only model when there are no edge servers, the total cloud latency can be stated as:</p><formula xml:id="formula_3">L cloud = (α+δ) * D min(UE,AP s) +(β+γ) * D AP -cloud +d node (<label>4</label></formula><p>) Eq. 4 shows that a closest AP is chosen to route a user to the cloud. Here, α and δ are the proportionality constants for uplink and downlink bandwidth from UE to AP link respectively, and β and γ are the similar factors for AP to cloud uplink and downlink bandwidth respectively. D min(UE,AP s) is distance from UE to nearest AP and D AP -cloud is the distance from AP to the central cloud. It is noted that the uplink bandwidth usage for the AR application is much higher than that of the downlink as mentioned earlier. When resources are available at the edge, the total edge latency can be represented as:</p><formula xml:id="formula_4">L edge = (α + δ) * D min(UE,AP s) + d node + d s (5)</formula><p>In Eq. 5, d s ≥ 0 is the control plane switching latency from an edge at AP to another AP's edge in case of unavailable resources which is assumed to be between 1-5ms. The response time for an application is the sum of transmission delay, propagation delay, switching delay (if any), queuing delay and computation delays in both the cases.</p><p>A core cloud-only system is defined as one with no edge cloud available. The edge-only system does not have any core cloud and if the load exceeds the available computational resources, a request is queued until it is processed. We also consider hybrids of core and edge based on the percentage parameter that splits computing resources between the two.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Edge Selection for an AR Application</head><p>Edge selection in a system for a given traffic load can be achieved using multiple approaches depending upon whether the system has centralized or distributed control. The network routing information that is available to all the routers can be used to deliver the service request to the nearest edge cloud -the edge cloud then independently decide to serve the request based upon resource availability or can route the user to the central cloud. A queuing model (M/M/c) is used to predict the estimated service time for a request apart from networking delays (control plane), propagation delays and transmission delays (available bandwidth). This approach works well for scenarios with evenly distributed users and network resources. However, this simple nearest edge cloud routing strategy does not work well when the user distribution is not geographically uniform ascertained by our simulation showing only 10% improvement in the average system response time as compared to a cloud-only system.</p><p>An alternative distributed approach improves upon simple anycast by having routers maintain compute resource availability states of neighboring edge clouds. This may involve the use of overlay protocols to exchange cloud state in a distributed manner <ref type="bibr" target="#b21">[23]</ref>, <ref type="bibr" target="#b22">[24]</ref>. A user is routed to the nearest edge first which makes one of the following decisions: (i) serve the request, (ii) route to a neighboring edge with available resources, or (iii) route to the central cloud. The decision at the edge is based upon application requirement and traffic load. For an AR application, the decision metric selects the closest edge to the UE which can serve the UE in L edge ≤ d max . The algorithm for this approach is as detailed below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Baseline Approach</head><p>Algorithm 1 shows the pseudo-code for the baseline edge cloud selection approach adopted in our study. The algorithm is invoked whenever the default edge cloud is unable to serve the user's demand (line: 2). It then scans the states of neighboring edges to find the best edge which can serve the user within the specified latency threshold. This approach relies upon shared resource and bandwidth information among neighbors. The list of neighbors is defined as p closest edge Algorithm 1: Finding neighboring edge with available resources for an AR application This section detailed our system and performance model. A baseline algorithm which scans the states of neighboring edge clouds to find the best edge which can serve the user within the specified latency threshold is developed. Next section evaluates the performance of baseline algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. PERFORMANCE EVALUATION OF BASELINE SYSTEM</head><p>In this section we discuss the capacity of different edge cloud systems with respect to traffic load, resource distribution and inter-edge bandwidth. Consider a system with following compute resources: (i) core cloud only, (ii) edge cloud only, and (iii) core cloud plus edge cloud, where in each case, the total amount of resources are same. Major system parameters used in the simulation are summarized in Table <ref type="table">II</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Impact of Network Bandwidth Parameters</head><p>Figure <ref type="figure">6</ref> shows the average response time for core cloud only and edge only networks for different system load when there is no limit on inter-edge and edge-cloud bandwidth. As there is no bandwidth limitation, the queuing delay dominates and crosses the 50 ms response time threshold after the system load is more than 60% for edge only system without bandwidth constraints. In the case when the core cloud has infinite capacity we observed that the network latency affects the total application response time.</p><p>Figure <ref type="figure">7</ref> illustrates the impact of constraint bandwidth AP-cloud system on the average response time. Here, the total bandwidth limit is set between edge network and the core cloud cluster. For a 500 Gbps AP-cloud bandwidth, for given system, the average response time compares with that of an unconstrained bandwidth case while for 50 Gbps case, it rises exponentially as the load increases. In case of lower bandwidth cases like 10 Gbps and 25 Gbps, the system is unable to handle higher load. As a bandwidth-constrained Figure <ref type="figure" target="#fig_5">8</ref>(a) plots the average response time for the core cloud as well as edge only system with different inter-edge bandwidth. On one hand, the extreme fronthaul bandwidth of 100 Gbps edge-only compares with the unconstrained bandwidth edge-only system and therefore all the edge resources are utilized. On the other hand, after the system fills up at Load=7, core cloud only system outperforms the edge only system with 1 Gbps inter-edge bandwidth. The reason is that for the baseline case, when an edge fills up the capacity, it routes the request to a neighboring edge utilizing inter-edge bandwidth. As the finite inter-edge bandwidth is split between multiple application flows, the propagation delay and queuing delay rise which in turn increases the average response time for higher load. In the baseline approach, the edge decides whether to send the request to a neighboring edge or to the central cloud. For 1 Gbps inter-edge bandwidth, the average response time for Load=1 is as low as 30 ms while for Load=10 case, it rises to 170 ms as the bandwidth exhausts and queuing delay rises. A delay more than 100 ms is unsuitable for most of the AR applications. As the bandwidth doubles, for Load=10 case, the average response time is ∼120 ms. Increasing bandwidth lowers the average response time for a completely loaded system but beyond 10 Gbps there is no significant advantage visible for the baseline case as there are still significant queuing delays for a loaded edge at an AP (or neighboring AP). After a load point, there is no dip in response time irrespective of how good the fronthaul connectivity between edges is. In this case, there is a crossover around Load=7 so we compare the CDF of core cloud only and edge-only with the 1 Gbps case in Fig. <ref type="figure" target="#fig_5">8(b)</ref>. A linear rise in response time can be observed for the static load case implying that the inter-edge bandwidth of 1 Gbps is insufficient to run such a heavily loaded system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Impact of Resource Distribution</head><p>In this subsection, we analyze the impact of the compute resource distribution between the core cloud and edge cloud on the average response time. There are a total of 5.5K processors each having 3.33 GIPS speed, available as compute resources which are equivalent to 1.1K full edge racks. Figure <ref type="figure" target="#fig_6">9</ref> shows the baseline latency performance for a core cloud-only system, edge-only system and cloudedge (CE) system for the simulation parameters listed in Table <ref type="table">II</ref>. CE80-20 implies that 80% compute resources are available at the cloud and 20% are placed at the edge near the APs and so on. The inter-edge bandwidth has no limitation in this case. As expected, the edge only system outperforms irrespective of load. As the resources are moved from central cloud to the edge, the response time CDF moves towards the left close to the edge-only system. When the CE system does not find resources available at the neighboring edge using Algorithm 1, the request is routed to the core cloud. Therefore in each of these cases, except for the edgeonly case, a few requests are bound to have response time as close as core cloud-only case. As expected, increasing resources at the edge brings response time down in the case of unconstrained bandwidth. Next we consider more realistic scenarios with constrained bandwidth.</p><p>Figures <ref type="figure" target="#fig_0">10(a</ref>) and (b) compare average response time in CE28 and CE82 for the baseline with respect to interedge bandwidth and load respectively. Response times for inter-edge bandwidth of 10, 50 and 100 Gbps are close to each other for all the load cases for both scenarios. This implies that increasing inter-edge bandwidth indefinitely cannot improve the system performance when using the simple scheme of filling neighboring edge resources. Figure <ref type="figure" target="#fig_0">10</ref>(a) also highlights the fact that when edge resources are higher than the core cloud for a low inter-edge bandwidth, beyond a load point, the core cloud-only system performs better. This means that for a highly loaded system, if fast edge connectivity is unavailable, it is better to use the core cloud.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Impact of AR Application Traffic Parameters</head><p>Figure <ref type="figure" target="#fig_7">11</ref> establishes the fact that inter-edge bandwidth plays a crucial role in the system. For the CE28 case, when the cloud-edge resource distribution is 20%-80% and interedge bandwidth is 1 Gbps, average response time increases at a faster rate than that of the CE82 case. The reason is that in the baseline scenario for CE28, an edge might be able to find a neighbor with available capacity but the connectivity is not sufficient to reach to that neighbor. In the case of lower or no edge resources, the core cloud is immediately favored and therefore performs better than the edge cloud scenario as can be observed from the crossover point at Load=8 case.</p><p>One more point of interest in Fig. <ref type="figure" target="#fig_7">11</ref> is between Load=5 and Load=6 where all the CE cases intersect. Figure <ref type="figure" target="#fig_8">12</ref> shows the average response time with different inter-edge bandwidth and resource distribution for baseline when  Load=5. Here, for the CE82 case, increasing inter-edge bandwidth does not boost the system performance as compared to the CE28 case because for the low edge resources case, increasing inter-edge bandwidth cannot decrease the processing delays at the edge. For a system with high edge resources, a higher inter-edge bandwidth is therefore needed to maintain AR performance.</p><p>Similarly, for the Load=6 case, Fig. <ref type="figure" target="#fig_9">13</ref> plots average response time vs. resource distribution for different interedge bandwidths. Again, for a 50 Gbps inter-edge bandwidth system, a faster drop in the average response time can be observed for the CE28 case when 80% resources are at the edge. For a 1 Gbps inter-edge bandwidth system, the average response time is slightly higher for the CE28 system than for the CE46 system. Using our designed system and performance model, we make following observations for the baseline scenario: (a) for unconstrained compute resources, the edge cloud continues to perform better than the core cloud due to its vicinity to the users (lower network latency), (b) increasing core network bandwidth beyond a load point does not lower the total application latency as the compute latency takes over, (c) for higher system load, the propagation delay and queuing delay rise because finite inter-edge bandwidth is divided among multiple application flows, (d) indefinitely increasing fronthaul edge cloud connectivity does not improve the response time after a load level, and (e) for lower inter-edge bandwidth case, distributing more resources at the edge clouds only worsens the application performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. ECON: ENHANCED CAPACITY EDGE CLOUD NETWORK</head><p>The baseline approach considered in Section IV relies on distributed control to select the best available neighboring edge cloud which might be sub-optimal in terms of overall system capacity. A more general approach is to select an edge cloud based upon global information about network and compute resources available at a logically centralized point such as an SDN controller. The idea is to use the complete network view before assigning an application/user to an edge cloud or deciding to route it to the core cloud. We call this approach Enhanced Capacity Edge Cloud Network (ECON). This section describes the ECON method and compares its performance with the baseline method.</p><p>Definition 1: An edge or cloud is "usable" for a request i if the latency L a i for the user running an application a is below the latency threshold for given application L a T h i.e. L a i ≤ L a T h . Here, L a i is simply equal to L cloud or L edge with different d node and d s .</p><p>A "usable" server is best for a user request in terms of service quality whereas the overall system capacity might not be optimal with this assignment. For example consider a user's application latency threshold 110 ms which may be assigned to an edge server serving request within 30 ms. This assignment will hamper performance of another needy user who required 35 ms latency but cannot be accommodated due to unavailable resources at the edge.</p><p>Definition 2: "delay-constraint (%)" of an edge-cloud system is defined as the number of requests out of hundred served below the application response time threshold, L a T h . For a specific value of L a T h , the delay-constraint can also be interpreted as system capacity. For instance, a delayconstraint of 10% for a 15 ms threshold implies that system can accommodate only 10% of the total requests and 90% requests will only consume resources to lower the goodput. This means for 90% of the requests, the assigned edge resources are "not usable".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Percentage delay-constraint, C = n T h</head><p>N * 100, where n T h are requests served within threshold response time and N are the total number of requests in the system. A system with high C for a threshold is required to run latency sensitive applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. "Usable" Edge-Cloud Optimization</head><p>Assigning requests to a "usable" server is similar to capacity optimization of an edge-cloud system for given compute as well network resources and application delay fulfillment. This problem is equivalent to the maximum cardinal bin packing and hence is NP-hard <ref type="bibr" target="#b23">[25]</ref>, <ref type="bibr" target="#b24">[26]</ref>. We can model the global optimization to maximize usable server s for N requests, where each request i is assigned to the server s, as:</p><formula xml:id="formula_5">max s n∈N I {sn&gt;0} (<label>6</label></formula><formula xml:id="formula_6">)</formula><p>subject to:</p><formula xml:id="formula_7">L a i (s) ≤ L a T h , ∀s n &gt; 0, n ∈ N (<label>7</label></formula><formula xml:id="formula_8">)</formula><p>I {sn&gt;0} being the indicator function with values 1 or 0 depending upon if such a server is available or not for a given request which means if it can serve the request in application response time threshold. Mapping users to "usable" server is NP-hard problem as explained earlier thus requiring an alternative approach.</p><p>The total average processing delay, d comp , at the cloud or edge, comprise of a waiting delay in the queue and a processing delay associated to the type of application. At each node, there is a transmission time, t tx associated with each task V , adding which to d comp provides total time, d node , spent at a server. Therefore, for such a system, we can formulate Eq. ( <ref type="formula" target="#formula_5">6</ref>) as minimizing d node of the system for all the users, while compromising on the optimality, instead of a "usable" server problem as follows:</p><formula xml:id="formula_9">P 1 : min M i=1 ( N j=1 d j,i proc + d i tx + d i s )<label>(8)</label></formula><p>subject to:</p><formula xml:id="formula_10">L a j ≤ L a T h , ∀j ∈ N (9) b i,up min ≤ b a i ≤ b i,up max , ∀i ∈ M (10) b i,down min ≤ b a i ≤ b i,down max , ∀i ∈ M (11) M i=1 c i ≤C (<label>12</label></formula><formula xml:id="formula_11">)</formula><p>Equation 8 defines the optimization problem with Eq. ( <ref type="formula">9</ref>) as delay constraint, Eq. ( <ref type="formula">10</ref>) and Eq. ( <ref type="formula">11</ref>) as bandwidth constraints for uplink and downlink each user application and, Eq. ( <ref type="formula" target="#formula_10">12</ref>) as capacity constraint of each node respectively. As explained earlier, b a i can be computed as b i /r edge . Again, the problem is similar to maximum cardinality bin packing problem and is NP-hard. Therefore, to find the "usable" server, we need to fix a user to a nearby edge and find the Pareto optimal edge for the next user sequentially satisfying the application latency constraint. This can be done by omitting the switching delay. Therefore, the problem can be simplified as (with same constraints as above) follows assuming d i tx constraint is satisfied by bandwidth splitting for each request.  <ref type="formula">13</ref>establishes that for a latency sensitive AR application, finding the "usable" server for a user means we need to place the task to a server which is nearby to the user in strict network sense having low load, latency and high available bandwidth. The delay minimization objective function fills up the edge resources before routing a task to the central cloud. The latency and bandwidth of chosen server are estimated using the exponential moving average: x p * w x +(1-w x )x, with w x as weight factor for x, x p is the previous value, x is the previous average and x is latency or bandwidth parameter. We call this approach ECON and results are compared with the baseline in next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. ECON VS. BASELINE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Resource Distribution and Inter-edge Bandwidth</head><p>ECON relies upon filling up the edge resources before routing to the central cloud. Figures <ref type="figure" target="#fig_11">14(a</ref>) and (b) compare average response time for CE28 and CE82 cases when the inter-edge bandwidth is 1 Gbps. For an edge-favored CE28 scenario in Fig. <ref type="figure" target="#fig_11">14(a)</ref>, ECON and baseline have similar performance because finding an available resource in ECON is equivalent to finding a neighbor in the baseline which has high probability when edge resources are 80%. When the resources are cloud-favored i.e. CE82 in Fig. <ref type="figure" target="#fig_11">14(b)</ref>, for a lightly loaded system, ECON performs better as it is able to find the resources anywhere in the network without additional queuing delays at the edge. For a highly loaded system, finding an available edge is more expensive than routing the request to the cloud itself and therefore baseline outperforms ECON in case Load&gt;5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Application Delay Constraints</head><p>Figure <ref type="figure" target="#fig_3">15</ref> presents the delay-constraints for unlimited fronthaul bandwidth edge-cloud system for CE82 case when Load=1. As application latency threshold increases, delayconstraint rises meaning if an application has a latency threshold of 100ms, about 60% requests can be fulfilled by the cloud-only system whereas the edge-only system will be able to fulfill all the requests. As shown in the plot, without inter-edge bandwidth limits, ECON performs better Figures <ref type="figure" target="#fig_0">16(a</ref>) and (b) compare a 1 Gbps edge-favored (CE28) system with Load=1 and Load=10. For a lightly loaded system when the edge cloud has more resources, ECON and baseline have similar performance as both of these schemes are able to find an available resource at the edge and 1 Gbps bandwidth is sufficient to route the request to a neighboring edge. In the case of a heavy load scenario, both of these schemes again have similar performance but the core cloud-only system is able to serve more requests than any of these schemes when the application latency threshold is more than 140 ms. This study shows that for elastic applications such as email, a cloud-only system is sufficient and can even perform better when compared to an edge-cloud system with low bandwidth. Also, for the low bandwidth scenario, routing to the cloud is more helpful in improving application latency performance than maximizing usage of edge clouds as illustrated by Fig. <ref type="figure" target="#fig_0">16</ref>(b) as baseline outperforms ECON when application latency threshold is more than 100 ms.</p><p>Figures <ref type="figure" target="#fig_0">17(a</ref>) and (b) show the difference between ECON and baseline delay-constraint performance for Load=1 and Load=10 for CE82 case. For a lightly loaded system, and lower available inter-edge bandwidth, ECON is able to fill up edge clouds before routing to the cloud and therefore performs better than baseline. When the load is higher, when ECON tries to fill up all the edge resources which are only 20% here, with 1 Gbps inter-edge bandwidth connectivity, it introduces more transmission delays and therefore the baseline outperforms. In this specific case, the cloud-only system overtakes first ECON and later the baseline case when the application can withstand higher latencies.</p><p>1) Edge-favored vs. Cloud-favored: Figures <ref type="figure" target="#fig_5">18(a</ref>) and (b) compare edge and cloud favored resources respectively when inter-edge bandwidth is 10 Gbps. Figure <ref type="figure" target="#fig_5">18(a)</ref> shows that for an edge-favored case when most of the resources are available at the edge, a baseline neighbor selection scheme performs equally well as ECON which selects the best of all edge resources for the request. For the cloud favored resource case shown in Fig. <ref type="figure" target="#fig_5">18(b)</ref>, ECON performs better than baseline as each of the edges has sufficient bandwidth to reach a far away available edge resource. Therefore, when sufficient bandwidth is available, it is better to choose an edge even if there are fewer resources available as the queuing time at an edge can be compensated by faster request transfers. On the other hand, if the interedge bandwidth is low, instead of trying to maximize edge resource utilization, it is good to send the request to the cloud if the application can withstand the resulting delay.</p><p>2) Goodput: As discussed earlier, AR applications are delay sensitive and discard packets which arrive late. Goodput is defined as the number of useful (on time) bits per second delivered to UEs running the AR application. Therefore, even when the system throughput is high, the goodput could remain low due to high proportion of late arrivals. The capacity improvement can be studied by analyzing a geographic block, G i s level of goodput using our simulation tool. If goodput is lowest in a block, this is an indicative of a need to augment additional edge resources to the serving edge. Figure <ref type="figure" target="#fig_6">19</ref> shows the normalized goodput ratio of ECON and baseline for different resource distribution and load. For an unconstrained inter-edge bandwidth system, the goodput ratio of a cloud-favored system is more than that of an edge-favored one as ECON tries to find the best available edge resource as compared to the neighbor selection baseline scheme. In a cloud-favored system, the edge has minimal resources and therefore each edge requires sufficient bandwidth to transfer requests to other edges which may be far away. The edge-favored system cannot be significantly improved with ECON as there are ample neighboring edges available from the baseline and therefore finding a more optimal edge tends to increase the network delay. Also, as the system load increases, there is a rise in the queuing delay at the edge server and therefore the system performance is similar for ECON as well as baseline in this case.</p><p>This section compared baseline scenario with a global edge assignment approach called ECON. We found that: (a) for an edge-favored resource system, ECON and baseline have similar application response time performance, (b) for a cloud-favored resources and lightly loaded system, ECON performs better than the baseline, (c) maximizing edge clouds usage for lower inter-edge bandwidth hampers the average system response time, and (d) for elastic applications such as email, a cloud-only system is sufficient and can even perform better as compared to an edge-cloud system with low bandwidth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. RELATED WORK</head><p>Edge cloud solutions have been proposed for a number of emerging scenarios including Internet of Things (IoT) <ref type="bibr" target="#b25">[27]</ref>, Cloud of Things (CoT) <ref type="bibr" target="#b26">[28]</ref>- <ref type="bibr" target="#b29">[31]</ref>, health analytics <ref type="bibr" target="#b30">[32]</ref> and autonomous driving <ref type="bibr" target="#b31">[33]</ref>, <ref type="bibr" target="#b32">[34]</ref>. The term cloud is generically used to describe a remotely located on-demand computing and networking system along with its typical storage functionality. Architectures such as Mobile Edge Cloud (MEC) <ref type="bibr" target="#b15">[17]</ref>, <ref type="bibr" target="#b23">[25]</ref>, fog <ref type="bibr" target="#b33">[35]</ref> and edge <ref type="bibr" target="#b34">[36]</ref> computing bring these resources close to the user to support faster networking and ultra-low latency applications.</p><p>Serving IoT devices using edge clouds is proposed in <ref type="bibr" target="#b35">[37]</ref>- <ref type="bibr" target="#b37">[39]</ref> with or without virtualization techniques to provide local compute offload, nearby storage, and networking. Real-time applications such as autonomous driving, traffic monitoring/reporting, and online multi-player 3D gaming have also been considered, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b38">[40]</ref>- <ref type="bibr" target="#b40">[42]</ref>. Applications of ICN (Information Centric Networking) have been proposed in <ref type="bibr" target="#b41">[43]</ref> as a means to reduce network complexity through named services and content. A three-tier cloud of things (CoT) system is modeled in <ref type="bibr" target="#b42">[44]</ref> which identifies edge cloud is a key design element for time-constraint applications. Attempts are also made to provide hierarchical models of edge clouds thereby enabling aggregation capabilities similar to data center networks <ref type="bibr" target="#b43">[45]</ref>. Understanding network topology is a critical step in analyzing a cloud or edge network mainly due to effect of routing on latency and throughput. Attempts have been made to characterize the network using geographical properties in <ref type="bibr" target="#b44">[46]</ref> using data of autonomous system (ASes) and their relationships, to create a network topology for realistic analysis.</p><p>Motivated by faster compute and connectivity needs of newer AR/VR applications, an edge-centric computing is described in <ref type="bibr" target="#b45">[47]</ref>. A QoS-aware global optimal edge placement approach is described in <ref type="bibr" target="#b46">[48]</ref>. An energy efficient resource allocation strategy is proposed in <ref type="bibr" target="#b47">[49]</ref> considering link layer parameters. A small cell based multi-level cloud system is simulated in <ref type="bibr" target="#b48">[50]</ref>. Existing literature either relies on a central controller for an optimal edge placement or the use of new network hierarchy to realize improvements in system performance <ref type="bibr" target="#b49">[51]</ref>, <ref type="bibr" target="#b50">[52]</ref>. Studies aimed at determining the overall capacity of a edge cloud system to support multiple applications using a city-scale network are lacking in the existing literature. To the best of our knowledge, this is one of the early attempts to characterize such a hybrid system with respect to edge-cloud resource distribution, inter-edge bandwidth, AP-cloud bandwidth and system load.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSION</head><p>This paper provides a framework for modeling and analyzing capacity of a city-scale hybrid edge cloud system intended to serve augmented reality application with service time constraints. A baseline distributed decision scheme is compared with a centralized decision (ECON) approach for various system load, edge-cloud resource distribution, interedge bandwidth and edge-core bandwidth parameters. The results show that a core cloud only system outperforms the edge-only system when inter-edge fronthaul bandwidth is low. The system analysis results provide guidance for selecting right balance between edge and core cloud resources given a specified application delay constraint. We have shown that for the case with higher inter-edge bandwidth and edge computing resources, a distributed edge selection achieves performance close to centralized optimization, whereas with ample core cloud resources and no bandwidth constraints, ECON provides a lower average response time.</p><p>Our study shows that adding capacity to an existing edge resource without increasing internetwork bandwidth may actually increase network-wide congestion and can result in reduced system capacity. Future work includes evaluating alternative application profiles with task splitting and compute prediction, analyzing the impact of mobility on the system capacity and edge placement using the city-scale edge cloud testbeds such as COSMOS <ref type="bibr" target="#b51">[53]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. General Multi-tier Edge-cloud Network Architecture</figDesc><graphic coords="2,96.77,73.68,163.91,128.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .Figure 3 .</head><label>23</label><figDesc>Figure 2. AR Use-case Scenario Set-up: (a) AR Application Flow (b) Smart Meeting Application using Indoor Navigation and (c) Annotation based Assistance</figDesc><graphic coords="3,67.90,209.11,158.83,141.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Hybrid Edge Cloud System Diagram</figDesc><graphic coords="4,85.36,68.74,187.19,147.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Wi-Fi APs Placement in Chicago City</figDesc><graphic coords="4,360.00,74.18,140.63,121.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .Figure 7 .</head><label>67</label><figDesc>Figure 6. AR Application Average Response Time for Core Cloud only and Edge only Networks with Increasing System Load</figDesc><graphic coords="7,63.73,219.44,233.56,114.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Average Response Time Comparison for Core Cloud and Edge Only System, with Different Load and Inter-edge Bandwidth for Baseline</figDesc><graphic coords="7,314.27,68.70,233.70,114.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Response Time CDF for Different Resource Distribution for Baseline without Inter-edge Bandwidth Limit</figDesc><graphic coords="8,63.87,215.73,233.56,114.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Average Response Time for Edge Cloud System with Different Load and Resource Distribution for Baseline. Inter-edge Bandwidth=1Gbps.</figDesc><graphic coords="8,314.42,65.68,233.70,114.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 12 .</head><label>12</label><figDesc>Figure 12. Average Response Time for Edge Cloud System with Different Resource Distribution and Inter-edge Bandwidth for Baseline. Load=5.</figDesc><graphic coords="8,314.42,216.19,233.70,114.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 13 .</head><label>13</label><figDesc>Figure 13. Average Response Time for Edge Cloud System with Different Resource Distribution and Inter-edge Bandwidth for Baseline. Load=6.</figDesc><graphic coords="9,63.96,67.64,233.56,114.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 14 .</head><label>14</label><figDesc>Figure 14. Average Response Time Comparison for ECON and Baseline, for Different Load and 1 Gbps Inter-edge Bandwidth</figDesc><graphic coords="10,314.52,220.80,233.70,114.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 16 .Figure 17 .</head><label>1617</label><figDesc>Figure 16. Edge Cloud System Capacity for Different Load and Edge Favored Resources (Inter-edge BW=1 Gbps)</figDesc><graphic coords="11,63.73,218.14,233.56,114.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 18 .Figure 19 .</head><label>1819</label><figDesc>Figure 18. ECON and Baseline Comparison for Edge and Cloud Favored Resources (Inter-edge BW=10 Gbps)</figDesc><graphic coords="11,314.28,218.48,233.70,114.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>1 function AvailableNeighbor (a, b); Input : Neighbor resource and bandwidth s i and b i Output: T orF 2 Condition: T otalDelay Edge ≥ delay th 3 while(NeighborEdge) 4 if T otalDelay NeighborEdgei ≤ delay th then clouds from the current edge location. For finite p the order of state update messages to be exchanged is ∼ N * p 2 where N is the number of edge clouds, and is thus an acceptable overhead for small to moderate values of p.</figDesc><table><row><cell>5</cell><cell>return TRUE;</cell></row><row><cell cols="2">6 else</cell></row><row><cell>7</cell><cell>return FALSE;</cell></row><row><cell cols="2">8 end</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>We thank the anonymous reviewers and Prof. Ellen Zegura (shepherd for our submission) for valuable comments which have helped us to significantly improve the paper. This research is supported under NSF Future Internet Architecture -Next Phase (FIA-NP) Award CNS-134529.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A survey of augmented reality</title>
		<author>
			<persName><forename type="first">Ronald</forename><forename type="middle">T</forename><surname>Azuma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Presence: Teleoperators and Virtual Environments</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="355" to="385" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Virtual reality: exploring the brave new technologies</title>
		<author>
			<persName><forename type="first">Howard</forename><surname>Rheingold</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>Simon and Schuster Adult Publishing Group</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Decentralized edge clouds</title>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Weissman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Heintz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet Computing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="70" to="73" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">VL2: a scalable and flexible data center network</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Greenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGCOMM computer communication review</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2009">2009</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The growth of m-learning and the growth of mobile computing: Parallel developments</title>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">G</forename><surname>Caudill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Review of Research in Open and Distributed Learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Overlay: Practical mobile augmented reality</title>
		<author>
			<persName><forename type="first">Puneet</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Manweiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romit</forename><forename type="middle">Roy</forename><surname>Choudhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Annual International Conference on Mobile Systems, Applications, and Services</title>
		<meeting>the 13th Annual International Conference on Mobile Systems, Applications, and Services</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Managing latency in complex augmented reality systems</title>
		<author>
			<persName><forename type="first">Marco</forename><forename type="middle">C</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">A</forename><surname>Livingston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1997 symposium on Interactive 3D graphics</title>
		<meeting>the 1997 symposium on Interactive 3D graphics</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Towards efficient edge cloud augmentation for virtual reality MMOGs</title>
		<author>
			<persName><forename type="first">Wuyang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second ACM/IEEE Symposium on Edge Computing</title>
		<meeting>the Second ACM/IEEE Symposium on Edge Computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A view of cloud computing</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Armbrust</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="50" to="58" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cloud load balancing techniques: A step towards green computing</title>
		<author>
			<persName><forename type="first">Inderveer</forename><surname>Chana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nidhi</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kansal</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Science Issues (IJCSI)</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">238</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Service-Centric Networking for Distributed Heterogeneous Clouds</title>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Simoens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Communications Magazine</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dynamic service migration in mobile edge-clouds</title>
		<author>
			<persName><forename type="first">Shiqiang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IFIP Networking Conference (IFIP Networking)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Atheer</forename><surname>Air</surname></persName>
		</author>
		<ptr target="https://atheerair.com/" />
		<imprint>
			<date type="published" when="2018-04-03">April 03. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Detail of light relfecting on lens of HoloLens</title>
		<ptr target="https://www.microsoft.com/en-us/hololens" />
		<imprint>
			<date type="published" when="2018-04-03">April 03. 2018</date>
			<publisher>Microsoft</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mobile edge computing, fog et al.: A survey and analysis of security threats and challenges</title>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Roman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Javier</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masahiro</forename><surname>Mambo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Generation Computer Systems</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="680" to="698" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hybrid mobile edge computing: Unleashing the full potential of edge computing in mobile device use cases</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernd</forename><surname>Prnster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Zefferer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing</title>
		<meeting>the 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<ptr target="https://opencv.org/" />
		<title level="m">OpenCV library</title>
		<imprint>
			<date type="published" when="2018-04-04">April 04. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="middle">Net</forename><surname>Wigle</surname></persName>
		</author>
		<author>
			<persName><surname>Wigle</surname></persName>
		</author>
		<title level="m">Wireless Network Mapping. WiGLE: Wireless Network Mapping, www.wigle</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title/>
		<ptr target="www.cloudping.info/" />
	</analytic>
	<monogr>
		<title level="j">CloudPing.info. CloudPing.info</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hybrid method for minimizing service delay in edge cloud computing through VM migration and transmission power control</title>
		<author>
			<persName><forename type="first">Tiago</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName><surname>Gama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="810" to="819" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Delay-optimal computation task scheduling for mobile-edge computing systems</title>
		<author>
			<persName><forename type="first">Juan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Theory (ISIT), 2016 IEEE International Symposium</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hyperflow: A distributed control plane for openflow</title>
		<author>
			<persName><forename type="first">Amin</forename><surname>Tootoonchian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yashar</forename><surname>Ganjali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 internet network management conference on Research on enterprise networking</title>
		<meeting>the 2010 internet network management conference on Research on enterprise networking</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Onix: A distributed control platform for large-scale production networks</title>
		<author>
			<persName><forename type="first">Teemu</forename><surname>Koponen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OSDI</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Efficient multi-user computation offloading for mobile-edge cloud computing</title>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Networking</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="2795" to="2808" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Algorithms for the bin packing problem with conflicts</title>
		<author>
			<persName><forename type="first">Albert</forename><forename type="middle">E</forename><surname>Muritiba</surname></persName>
		</author>
		<author>
			<persName><surname>Fernandes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Informs Journal on computing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="401" to="415" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Edge analytics in the internet of things</title>
		<author>
			<persName><forename type="first">Mahadev</forename><surname>Satyanarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Pervasive Computing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="24" to="31" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fog computing and its role in the internet of things</title>
		<author>
			<persName><forename type="first">Flavio</forename><surname>Bonomi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the first edition of the MCC workshop on Mobile cloud computing</title>
		<meeting>the first edition of the MCC workshop on Mobile cloud computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cloud of Things: Integrating Internet of Things and cloud computing and the issues involved</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Aazam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applied Sciences and Technology (IBCAST), 2014 11th International Bhurban Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">IoT and cloud convergence: Opportunities and challenges</title>
		<author>
			<persName><forename type="first">Abdur</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raffaele</forename><surname>Rahim</surname></persName>
		</author>
		<author>
			<persName><surname>Giaffreda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Internet of Things (WF-IoT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Characterizing cloud federation in IoT</title>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Celesti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced Information Networking and Applications Workshops (WAINA), 2016 30th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Opportunities and challenges of cloud computing to improve health care services</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName><surname>Mu-Hsing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of medical Internet research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">3d lidar point cloud based intersection recognition for autonomous driving</title>
		<author>
			<persName><forename type="first">Quanwen</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A cloud-assisted design for autonomous driving</title>
		<author>
			<persName><forename type="first">Swarun</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shyamnath</forename><surname>Gollakota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dina</forename><surname>Katabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the first edition of the MCC workshop on Mobile cloud computing</title>
		<meeting>the first edition of the MCC workshop on Mobile cloud computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Finding your way in the fog: Towards a comprehensive definition of fog computing</title>
		<author>
			<persName><forename type="first">Luis</forename><forename type="middle">M</forename><surname>Vaquero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Rodero-Merino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGCOMM Computer Communication Review</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="27" to="32" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The promise of edge computing</title>
		<author>
			<persName><forename type="first">Weisong</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Schahram</forename><surname>Dustdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="78" to="81" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Future edge cloud and edge computing for internet of things applications</title>
		<author>
			<persName><forename type="first">Jianli</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Mcelhannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet of Things Journal</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="439" to="449" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Consolidate IoT Edge Computing with Lightweight Virtualization</title>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Morabito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Network</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="102" to="111" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">IIoTEED: An Enhanced, Trusted Execution Environment for Industrial IoT Edge Devices</title>
		<author>
			<persName><forename type="first">Sandro</forename><surname>Pinto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet Computing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="40" to="47" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Vehicle control system coordinated between cloud and mobile edge computing</title>
		<author>
			<persName><forename type="first">Kengo</forename><surname>Sasaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Society of Instrument and Control Engineers of Japan</title>
		<imprint>
			<biblScope unit="issue">SICE</biblScope>
			<date type="published" when="2016">2016. 2016</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
	<note>55th Annual Conference of the</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Big data caching for networking: Moving from cloud to edge</title>
		<author>
			<persName><forename type="first">Engin</forename><surname>Zeydan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Communications Magazine</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="36" to="42" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Optimal decision making for big data processing at edge-cloud environment: An sdn perspective</title>
		<author>
			<persName><forename type="first">Gagangeet</forename><surname>Aujla</surname></persName>
		</author>
		<author>
			<persName><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Informatics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="778" to="789" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Towards software defined icn based edge-cloud services</title>
		<author>
			<persName><forename type="first">Ravishankar</forename><surname>Ravindran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cloud Networking (CloudNet)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">System modelling and performance evaluation of a three-tier Cloud of Things</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Generation Computer Systems</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="104" to="125" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A hierarchical edge cloud architecture for mobile computing</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Communications, IEEE INFOCOM 2016-The 35th Annual IEEE International Conference</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">GeoTopo: A PoP-level Topology Generator for Evaluation of Future Internet Architectures</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Network Protocols (ICNP), 2015 IEEE 23rd International Conference on</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Edge-centric computing: Vision and challenges</title>
		<author>
			<persName><forename type="first">Garcia</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><surname>Pedro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGCOMM Computer Communication Review</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="37" to="42" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Segue: Quality of service aware edge cloud service migration</title>
		<author>
			<persName><forename type="first">Wuyang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cloud Computing Technology and Science (CloudCom), 2016 IEEE International Conference</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Energy-efficient resource allocation for mobile-edge computation offloading</title>
		<author>
			<persName><forename type="first">Changsheng</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Wireless Communications</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1397" to="1411" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Multilevel cloud based Tactile Internet system</title>
		<author>
			<persName><forename type="first">Abdelhamied</forename><forename type="middle">A</forename><surname>Ateya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced Communication Technology (ICACT), 2017 19th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A hybrid edge-cloud architecture for reducing on-demand gaming latency</title>
		<author>
			<persName><forename type="first">Sharon</forename><surname>Choy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="503" to="519" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Minimizing retrieval latency for content cloud</title>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Bjrkqvist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INFOCOM, 2011 Proceedings IEEE</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">COSMOS Project</title>
		<author>
			<orgName type="collaboration">Cosmos-lab.org</orgName>
		</author>
		<ptr target="http://www.cosmos-lab.org" />
		<imprint>
			<date type="published" when="2018-08-30">2018. 30 Aug. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
