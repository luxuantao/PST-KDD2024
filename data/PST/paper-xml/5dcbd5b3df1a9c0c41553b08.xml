<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LEARNING ALGORITHMS AND SIGNAL PROCESSING FOR BRAIN-INSPIRED COMPUTING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Bipin</forename><surname>Rajendran</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Abu</forename><surname>Sebastian</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Schmuker</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Narayan</forename><surname>Srinivasa</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Evangelos</forename><surname>Eleftheriou</surname></persName>
						</author>
						<title level="a" type="main">LEARNING ALGORITHMS AND SIGNAL PROCESSING FOR BRAIN-INSPIRED COMPUTING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C8256536111B773071C5B19E4460C9EC</idno>
					<idno type="DOI">10.1109/MSP.2019.2933719</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>achine learning has emerged as the dominant tool for implementing complex cognitive tasks that require supervised, unsupervised, and reinforcement learning. While the resulting machines have demonstrated in some cases even superhuman performance, their energy consumption has often proved to be prohibitive in the absence of costly supercomputers. Most state-of-the-art machine-learning solutions are based on memoryless models of neurons. This is unlike the neurons in the human brain that encode and process information using temporal information in spike events. The different computing principles underlying biological neurons and how they combine together to efficiently process information is believed to be a key factor behind their superior efficiency compared to current machine-learning systems.</p><p>Inspired by the time-encoding mechanism used by the brain, third-generation spiking neural networks (SNNs) are being studied for building a new class of information processing engines. Modern computing systems based on the von Neumann architecture, however, are ill suited for efficiently implementing SNNs since their performance is limited by the need to constantly shuttle data between physically separated logic and memory units. Hence, novel computational architectures that address the von Neumann bottleneck are necessary to build systems that can implement SNNs with low energy budgets. In this article, we review some of the architectural and system-level design aspects involved in developing a new class of brain-inspired information processing engines that mimic the time-based information encoding and processing aspects of the brain.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>While machine-learning algorithms based on deep NNs have demonstrated humanlike or even superhuman performance in several complex cognitive tasks, a significant gap exists between the energy and efficiency of the computational systems that implement these algorithms compared to the human brain. Most of these algorithms run on conventional computing systems, such as central processing units (CPUs), graphical processing units (GPUs), and field programmable gate arrays. Recently, digital or mixed-signal application-specific integrated circuits (ASICs) have also been developed for machine-learning acceleration. However, as Moore's law scaling is coming to an imminent end, the performance and power efficiency gains from the technology scaling of these conventional approaches are diminishing. Thus, there are significant research efforts worldwide toward developing a profoundly different approach to computing for artificial intelligence (AI) applications inspired by biological principles.</p><p>In the traditional von Neumann architecture, a powerful processing unit operates sequentially on data fetched from memory. In such machines, the von Neumann bottleneck is defined as the limitation on performance arising from the chokepoint between computation and data storage. Hence, the research focus has been not only on designing new AI algorithms, device technologies, integration schemes, and architectures but on overcoming the CPU/memory bottleneck in conventional computers. SNNs are the third generation of artificial neuron models that leverage the key time-based information encoding and processing aspects of the brain. Neuromorphic computing platforms aim to efficiently emulate SNNs in hardware by distributing both computation and memory among a large number of relatively simple computation units, namely, the neurons, each passing information via asynchronous spikes to hundreds or thousands of other neurons through synapses <ref type="bibr" target="#b0">[1]</ref>.</p><p>The event-driven characteristics of SNNs have led to highly efficient computing architectures with colocated memory and processing units, significantly increased parallelism, and reduced energy budgets. Such architectures have been demonstrated in neuromorphic implementations such as SpiNNaker, from the University of Manchester <ref type="bibr" target="#b1">[2]</ref>; IBM's TrueNorth <ref type="bibr" target="#b2">[3]</ref>; Intel's Loihi <ref type="bibr" target="#b3">[4]</ref>; BrainScaleS, built by Heidelberg University <ref type="bibr" target="#b4">[5]</ref>; NeuroGrid, from Stanford University <ref type="bibr" target="#b5">[6]</ref>; INI ZÃ¼rich's DYNAP <ref type="bibr" target="#b6">[7]</ref>; and ODIN, created by Catholic University Louvain <ref type="bibr" target="#b7">[8]</ref>. Moreover, breakthroughs in the area of nanoscale memristive devices have enabled further improvements in the area and energy efficiency of mixed digital-analog implementations of synapses and spiking neurons <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. In this article, we provide a high-level description of the design objectives and approaches that are currently being pursued for building energy-efficient neuromorphic computing platforms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Information processing in the brain</head><p>Computing in the brain follows a completely different paradigm than today's conventional computing systems. Whereas conventional systems are optimized to transmit and modify numerical representations of data, the brain operates on timed events called action potentials or spikes. Neurons receive these spikes via synapses, which convert them into small changes in the cell's membrane potential. The neuron integrates these changes in potential over time, and, under certain conditionsfor instance, when many spikes arrive within a short time-the neuron emits a spike. Spikes can be considered messages in the computing sense, except that they carry no information other than their time of generation and their source. Computing in the brain can thus be described as fully event driven, nonblocking, and imperatively concurrent, with very lightweight compute nodes (the neurons), that communicate via tiny messages, the spikes.</p><p>There are about 10 11 neurons in the human brain, and it is estimated that there are about 5,000-10,000 synapses per neuron in the human neocortex. Thus, connectivity is sparse, with a neuron receiving input from 10 -6 % of all other neurons (probably even less, considering that an axon may form multiple synapses on the same dendrite). At the same time, the total number of connections is huge, in the order of 10 15 . This is vastly different from the low fan-out connectivity that is common in conventional computers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Signal encoding in the brain</head><p>The event-driven nature of computing also applies to stimulus encoding by the sensory organs. Generally, sensory coding emphasizes changes in the stimulus rather than accurate coding of constant levels. These changes can be considered events that are translated into spikes for downstream processing. For example, ganglion cells in the retina transmit a spike if the change in local contrast in their receptive field exceeds a threshold. They then adapt to the new level of local contrast. More spikes are produced only when the local contrast rises further. This encoding scheme has three advantages over uniformly spaced signal sampling in conventional signal processing. 1) It produces a sparse code that transmits information only when the input signal is changing. 2) It is not limited by a fixed sample rate in the maximum frequency it can encode.</p><p>3) It can lead to extremely reactive systems since the latency for feature extraction is not limited by the time between two samples but only the minimum delay between two events, which is usually much shorter.</p><p>In neuromorphic devices, this encoding approach has been implemented via a set of thresholds that trigger events to be fired upon the signal crossing them [Figure <ref type="figure" target="#fig_1">1</ref>(b)] <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning in the brain</head><p>The connections in the brain are not fixed but can change as a function of their activity; this process is often called learning. Fundamental principles of synaptic changes have been uncovered that depend on the timing of spikes fired in the pre-and postsynaptic cells, thus termed spike-timing-dependent plasticity (STDP) <ref type="bibr" target="#b11">[12]</ref>. STDP implements a form of Hebbian learning, where a synapse is strengthened (potentiated in neuroscience terminology) if the presynaptic spike arrives within a certain time window preceding the spike of the postsynaptic cell or is weakened (depressed) if the temporal order is reversed [Figure <ref type="figure" target="#fig_1">1</ref> In biology, the amount and direction of weight change are often influenced by neuromodulators, such as dopamine or noradrenaline, which are released as a function of the reward received by the organism. Neuromodulators are thus a third factor in models of synaptic learning. They allow for the construction of powerful spiking learning rules that, for example, implement reinforcement learning <ref type="bibr" target="#b12">[13]</ref>. Taken together, the brain's massively parallel, event-driven computing and its extraordinary connectivity are probably the basis for its extremely high efficiency in signal processing, inference, and control. Furthermore, the action potentials used for communication as well as the synaptic currents in the brain have much smaller magnitudes than those of the electrical signals in silicon computers, as signals in the brain are encoded by the flow of a relatively smaller number of charge carriers (such as sodium, potassium, and calcium ions) through ion channels with highly selective, stochastic, and nonlinear conductance characteristics. As a result of all of these contributing factors, the brain is estimated to consume about 20 W, even during demanding tasks like mastering a multiplayer online computer game, whereas a conventional platform has been reported to use about 128,000 CPUs and 256 GPUs to achieve competitive performance (see https://blog.openai.com/openai-five).</p><p>This fundamental difference in computing architecture implies that porting algorithms from conventional machines to spike-based approaches can have only limited success. New algorithms are required that embrace the fundamentally eventbased nature of sensing, learning, and inference in the brain to leverage the full potential of spiking networks accelerated by neuromorphic hardware.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Building blocks of neuromorphic systems</head><p>Although biological neurons and synapses exhibit a wide variety of complex dynamical behaviors, most hardware designs need to mimic only the key aspects that are essential for computation. At a high level, this includes the integrate-and-fire (I&amp;F) dynamics of neurons and the spike-triggered communication and plasticity of synapses.</p><p>The central aspects of the I&amp;F dynamics of neurons are described by the Hodgkin-Huxley equations, which incorporate the voltage-dependent dynamics of sodium, potassium, and leaky ion channels to determine the evolution of the membrane potential. While biological neurons exhibit more complex behaviors, such as burst firing, chirping, postinhibitory rebound, and spike-frequency adaptation, and although the computational significance of these has not yet been clearly established, there have been hardware designs that mimic some of these behaviors, using both digital CMOS and subthreshold analog CMOS circuits.</p><p>Model-order reduction techniques have been used to reduce the complexity of the dynamical equations that describe these behaviors; some of the notable examples being the second-order Izhikevich model, the adaptive exponential I&amp;F model, and the linear leaky I&amp;F (LIF) model. These are simpler to implement in hardware and are more commonly used in large-scale neuromorphic designs. We describe the LIF model here, as it is the most commonly used spiking neuron model. The membrane potential V(t) evolves according to the differential equation: The brain employs a massively parallel computational network comprising ~10 11 neurons, each with some 5,000-10,000 synapses, in a system of distributed, asynchronous, event-driven computing that uses (b) event-driven signal encoding and (c) event-driven weight updates.</p><p>( )</p><formula xml:id="formula_0">C dt dV t g V t E I t V t E V t V if L L L T syn ! 2 = - - +<label>( ( ) ) ( ), ( ) , () ,</label></formula><p>where the input synaptic current ( ) I t syn is integrated across a leaky capacitor until the voltage exceeds a threshold , VT when a spike is issued, and the membrane potential is reset to its resting value .</p><p>EL C and gL model the membrane's capacitance and leak conductance, respectively. The refractory period seen in biological neurons can be implemented by holding the membrane potential at , ( )</p><formula xml:id="formula_2">V t EL =</formula><p>preventing current integration during that period.</p><p>Synaptic communication is triggered by the arrival of a spike, causing a current to flow into downstream neurons. This is also a highly complex process in the brain, involving the release of neurotransmitter molecules at the axon terminal, which then diffuse across the synaptic cleft and bind with receptor molecules at the dendrite, causing ionic currents to flow into the downstream neurons. These aspects are not modeled in most hardware implementations. Instead, the current through a synapse with strength w is calculated as = are some commonly used synaptic kernels. Note that this form of synaptic transmission also assumes that the current is independent of the postsynaptic neuron's potential, unlike in biology. Finally, to implement synaptic plasticity, the weight w is updated based on learning rules implemented in peripheral circuits, again in a spike-triggered fashion.</p><formula xml:id="formula_3">( ) ( ), I t w t t i i syn # a = - /<label>(2)</label></formula><p>While the hardware design of neuronal and synaptic circuits involves a variety of tradeoffs in area, power, reliability, and performance, the more challenging aspect of neuromorphic systems is supporting arbitrary connectivity patterns between spiking neurons. Since the computational advantage of NNs emerges from the high fan-out yet sparse connectivity of neurons, hardware designs also need to support this crucial aspect. Furthermore, several forms of structural plasticity are observed in the brain where neurons can form (or remove) new synaptic connections based on activity. In the following section, we describe some of the architectural design choices that enable the realization of some of these aspects in silicon substrates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System design principles and approaches</head><p>Colocation of memory and computation mitigates the von Neumann bottleneck in neuromorphic processors. Thus, inspired by neuroscience, the state-of-the-art architectural framework of SNN accelerators or coprocessors comprises a network of neurosynaptic cores <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> that can efficiently implement scale-out NNs, as shown in Figure <ref type="figure" target="#fig_2">2</ref>. Each core consists of a crossbar array, with electronic synapses at each cross-point Spikes are communicated between cores through a routing network using address event representation protocols <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b10">[11]</ref>.</p><p>and peripheral circuitry, including local static random-access memory (SRAM)-based lookup tables for routing information and storage of local data. The peripheral circuitry implements the neuronal functionality (typically the LIF neuron), the read/ write electronics, and the driver circuits that control the voltages on the input wires (axons, horizontal lines) and output wires (dendrites, vertical lines). Thus, a neurosynaptic core represents a layer of the SNN, and the passing of information between layers (cores) is enabled by a time-multiplexed communication fabric. Since neuron spiking rates are orders of magnitude slower than digital electronics, and jitter and delay through digital electronics (propagation and transition delay) are insignificant compared to axonal delays and neuron time constants, the networks typically used to multiplex information from one neurosynaptic core to another are packet-switched networks, using the so-called address event representation (AER) protocol. In this scheme, each neuron has a unique address, which is transmitted to the destination axons when it spikes; the time of spiking is hence encoded implicitly (Figure <ref type="figure" target="#fig_2">2</ref>). Note that the AER protocol also allows efficient and flexible off-chip interconnect for large-scale multichip network platforms.</p><p>The overall architecture is brain-inspired in the sense that the neurosynaptic core, via its crossbar array, mimics the dense local connectivity of neurons within the same layer, whereas the network interconnect topology with the AER protocol allows sparse long-range connections. From a pragmatic hardware-implementation-oriented viewpoint, such an SNN accelerator architecture with the appropriate core-to-core communications protocol is reminiscent of a dataflow engine, i.e., dataflow from core to core in an asynchronous fashion. See <ref type="bibr" target="#b10">[11]</ref> for a general review of neuromorphic design principles.</p><p>Although the core-to-core interconnect fabric is implemented with digital CMOS logic, the neurosynaptic core itself can be implemented by using analog/mixed-signal, digital, or memristive technologies. In its simplest form, the neuron membrane potential can be implemented in the analog domain as a voltage across a capacitor or as a multibit variable stored in digital latches. The analog design approaches have focused on two basic design methodologies: subthreshold current-mode circuits and above-threshold circuits. The former approach suffers from higher inhomogeneities (e.g., device mismatch) than the latter one, but it offers lower noise energy (noise power times bandwidth) and better energy efficiency (bandwidth over power) <ref type="bibr" target="#b10">[11]</ref>.</p><p>Alternatively, digital neurons can be realized using CMOS logic circuits, such as adders, multipliers, and counters <ref type="bibr" target="#b13">[14]</ref>. From a circuit design point of view, the synapses modulate the input spikes and transform them into a charge that consequently creates postsynaptic currents that are integrated at the membrane capacitance of the postsynaptic neuron <ref type="bibr" target="#b10">[11]</ref>. The implementation of silicon synapses typically follows the mixed-signal methodology, although they can also be implemented in the digital domain by employing SRAM cells <ref type="bibr" target="#b13">[14]</ref>. Moreover, neurons and synapses can be imple-mented using memristive technologies, as described in the section "Neuromorphic Computing With Memristive Devices."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>State-of-the-art neuromorphic hardware</head><p>In this section, we describe the salient features of state-of-theart silicon CMOS-based neuromorphic chips. Although research in this domain is more than three decades old, starting with the pioneering work of Carver Mead <ref type="bibr" target="#b0">[1]</ref>, the discussion in this article is limited to some of the recent demonstrations of large-scale neuromorphic platforms that integrate more than 1,000 neurons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SpiNNaker</head><p>SpiNNaker is a digital system that has been designed to efficiently simulate large spiking networks, approaching the complexity of the brain, in real time <ref type="bibr" target="#b1">[2]</ref>. Its building blocks are ARM9 cores that can access a small amount of local memory, plus some additional memory that is shared across one multicore chip. No global memory exists. Nodes can communicate via a high-throughput fabric that is optimized toward routing small messages (not larger than 72 bits) with high efficiency. SpiNNaker's event-driven design manifests itself in the messagehandling paradigm. A message arriving at a core triggers an interrupt that queues the packet for processing by that core. The system is optimized for small packet handler code that processes messages quickly, keeping queues short (i.e., not much larger than one). SpiNNaker thus implements fundamental concepts of the brain, such as event-driven computation, locality of information, high fan-in and fan-out connectivity, and communication with tiny messages.</p><p>The SpiNNaker system is constructed from processor cores, 18 of which are grouped on a die in a chip. Forty-eight such chips, with up to 864 cores (depending on the manufacturing yield) are assembled on one board. Chips on one board communicate using a custom protocol employing direct connections, wired in a hexagonal topology. Larger systems can be built by connecting 48-chip boards with fast serial interconnects. The largest system in existence today consists of 1 million processors, housed in 10 19-in racks at the University of Manchester, United Kingdom.</p><p>While SpiNNaker can be programmed directly, its software stack provides several levels of abstraction, with spiking network implementation being facilitated by PyNN. PyNN is a Python library that supports the portability of network designs between a variety of neuronal simulators and hardware, including the BrainScaleS system (described further on). SpiNNaker's PyNN interface provides several standard neuron models, such as LIF and Izhikevich's dynamical systems model, along with common algorithms for synaptic plasticity, including STDP. The successor of this chip, SpiNNaker 2, uses a more modern process technology, increases the number of cores per chip, and adds linear algebra accelerators and several other improvements. It has been used successfully for deep learning with sparse connectivity <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Colocation of memory and computation mitigates the von Neumann bottleneck in neuromorphic processors.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TrueNorth</head><p>TrueNorth is a million-neuron digital CMOS chip that IBM demonstrated using 28-nm process technology in 2014 <ref type="bibr" target="#b2">[3]</ref>. The chip is configured as a tiled array of 4,096 neurosynaptic cores, each containing 12.75 kilobytes of local SRAM memory to store the synapse states, neuron states and parameters, destination addresses of the fan-out neurons, and axonal delays. The digital neuron circuit in each core implements LIF dynamics and is time-multiplexed to emulate the operation of up to 256 neurons, which helps in amortizing the physical area and power budgets. Each core can support the fan-in and fan-out of 256 or less, and this connectivity can be configured such that a neuron in any core can communicate its spikes to one axon in any other core and then to any neuron in that core.</p><p>The spike-based communication and routing infrastructure also allows the integration of multiple chips; IBM recently demonstrated the NS16e board that comprises 16 TrueNorth chips. The chip integrates 256 million SRAM synapses, with the synaptic connectivity programmable to two values {0,1}, four programmable 9-bit signed integer weights per neuron, and a programmable spike delivery time at the destination axon in the range of 1-15 time steps. The spike routing is completed asynchronously in every time step, chosen to be 1 ms, to achieve real-time operation akin to biology, although faster synchronization clocks permit accelerated network emulation.</p><p>The corelet programming environment is used to map network parameters from software training to the TrueNorth processor. Thanks to the event-driven custom design, the colocation of memory and processing units in each core, and the use of low-leakage silicon CMOS technology, TrueNorth can perform 46 billion synaptic operations per second (SOPS) per watt for real-time operation, with 26 pJ per synaptic event. Its power density of 20 mW/cm 2 is about three orders of magnitude smaller than that of typical CPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loihi</head><p>Loihi is a neuromorphic learning chip developed by Intel in 2018 using their 14-nm fin field-effect transistor process <ref type="bibr" target="#b3">[4]</ref>. This multicore chip supports the implementation of axonal and synaptic delays, neuronal spiking threshold adaptation, and programmable synaptic learning rules based on spike timing and reward modulation. The chip has 128 neural cores, with each core having 1,024 spiking neurons and 2 megabits of SRAM to store the connectivity, configuration, and dynamic state of all neurons within the core. The chip also includes three embedded x86 processors, and 16 megabytes of synaptic memory implemented in SRAM, supporting a synaptic bit resolution of 1-9 bits. Thus, it supports roughly 130,000 neurons and 130 million synapses. Spikes are transported between the cores in the chip using packetized messages by an asynchronous network on chip and allows connection to 4,096 on-chip cores and up to 16,384 chips via hierarchical addressing.</p><p>To address the scaling of network connectivity to biological levels (i.e., a fan-out of 1,000), Loihi supports several features, including core-to-core multicast and population-based hierarchical connectivity. The cores in the chip can be programmed using microcodes to implement several forms of neuromorphic learning rules, such as pairwise STDP, triplet STDP, certain reinforcement learning protocols, and other rules that depend on spike rates as well as spike timing. Under nominal operating conditions, Loihi delivers 30 billion SOPS, consuming about 15 pJ per synaptic operation. A Python package for the Nengo neural simulator allows users to study the implementation of spiking networks on Loihi without accessing the hardware.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BrainScaleS-1</head><p>The BrainScaleS system is a mixed-signal platform that combines analog neuron circuits with digital communication <ref type="bibr" target="#b4">[5]</ref>. Its unique feature is a speedup factor of 10 3 -10 4 for spiking network emulations, meaning that a network model running for 1 s of wall time on the hardware emulates up to 10,000 s of biological simulation time. BrainScaleS supports the adaptive exponential I&amp;F model that can be parameterized to exhibit diverse firing patterns. The BrainScaleS system's smallest unit of silicon is the High-Input Count Analog Neuronal Network (HiCANN) chip <ref type="bibr" target="#b15">[16]</ref>. The number of neurons per chip can be configured within wide limits, following a tradeoff between the number of input synapses and the number of neurons. A single chip supports a maximum of 512 spiking neurons and up to about 14,000 synapses per neuron. Larger networks are supported by wafer-scale integration of HiCANN chips, which are wired directly on the silicon wafer without cutting it into discrete elements. A single wafer supports 4 10 7 # synapses and up to 180,000 neurons.</p><p>Prototypes of the next BrainScaleS generation support programmable plasticity via general-purpose processors embedded on the die alongside the neuromorphic circuitry <ref type="bibr" target="#b16">[17]</ref>. These processors have access to dedicated sensors at the synapses that measure the time interval between pre-and postsynaptic spikes. Arbitrary functions can be defined that compute updates to the synaptic weights from this information. This enables highly flexible learning rules to be implemented on BrainScaleS, including reward-based learning. BrainScaleS, like SpiNNaker, leverages the PyNN application programming interface (http://neuralensemble.org/PyNN) to allow the user to specify spiking networks for emulation on the hardware.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NeuroGrid/Braindrop</head><p>The goal of the NeuroGrid platform is to implement large-scale neural models and to emulate their function in real time <ref type="bibr" target="#b5">[6]</ref>. Hence, the system's memory and computing resources have time constants that are well matched to the signals that need to be processed. NeuroGrid employs analog/digital mixed-signal subthreshold circuits to model continuous-time neural processing elements. The physics of field-effect transistors operating in the subthreshold regime is used to directly emulate the various neuronal and synaptic functions.</p><p>NeuroGrid comprises a board with 16 standard CMOS NeuroCore chips connected in a tree network, with each chip consisting of a 256 256 # array of two compartmental neurons. Each neuron in the array can target multiple destinations by virtue of an asynchronous multicast tree routing digital infrastructure. The full NeuroGrid board can implement models of cortical networks of up to 1 million neurons and billions of synaptic connections with sparse long-range connections and dense local connectivity. A mixed-signal neurosynaptic core called Braindrop, with 4,096 neurons and 64 kilobytes of weight memory, has also been recently demonstrated, leveraging the variability of analog circuits for performing useful computations <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DYNAP</head><p>Dynamic neuromorphic asynchronous processors (DYNAP) is a family of mixed-signal neuromorphic chips from INI ZÃ¼rich. DYNAP-SE is one such chip, fabricated in 180-nm CMOS technology, integrating 1,024 neurons and 64,000 synapses <ref type="bibr" target="#b6">[7]</ref>. The chip is organized into four cores, each having 256 analog neurons. The temporal dynamics of the neurons are implemented using ultralow-power (subthreshold) analog circuits, whereas the asynchronous digital circuits allow for programming and reprogramming the network connectivity at runtime, enabling the configuration of recurrent networks, multilayer networks, and any arbitrary network topology. The analog circuits implement a wide range of features, including multiple types of synaptic and neural dynamics and the spike-frequency adaptation mechanisms that have recently been shown to be crucial in implementing long short-term memory (LSTM)-like networks with spiking neurons.</p><p>The asynchronous digital circuits implement a hierarchical routing scheme that combines the best features of all previously proposed approaches (e.g., NeuroGrid's tree-based method and SpiNNaker's 2D-grid mesh scheme), minimizing the memory requirements. Moreover, the device mismatch that is present in the analog circuits is exploited to implement neural sampling and reservoir-computing strategies that require variability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ODIN</head><p>ODIN is a 28-nm digital neuromorphic chip demonstrated by Catholic University Louvain in 2019 supporting simple forms of on-chip spike-driven synaptic plasticity <ref type="bibr" target="#b7">[8]</ref>. The core supports 256 neurons that can be configured to implement firstorder LIF dynamics as well as second-order Izhikevich dynamics. The neuronal parameters are stored in a 4-kilobyte SRAM array, and a global controller is used to time-multiplex the neuron logic circuit to implement the dynamics of the neurons in a sequential fashion. The core also integrates 3-bit 256 2 synapses, which are implemented as a 32-kilobyte SRAM array. An additional bit is used per synapse to enable or disable online learning. Using a subset of preprocessed images from the Modified National Institute of Standards and Technology database (commonly known as MNIST) data set, the chip demonstrated on-chip learning achieving 84.5% accuracy and consuming 15 nJ per inference with rank-order coding.</p><p>Table <ref type="table">1</ref> summarizes today's state-of-the-art neuromorphic chips, along with some of their key attributes. Note that representative numbers are reported in the table, and, in some instances, it is possible to exceed the performance metrics quoted here by operating the chip at higher frequencies or under other operating conditions. Furthermore, newer-generation prototype chips that form the building blocks of the larger systems have recently been reported, as alluded to in the text (especially for BrainScaleS and SpiNNaker), although we have included the specifications of the full-scale systems in this table.  The accumulative behavior of memristive devices can be exploited to emulate neuronal dynamics <ref type="bibr" target="#b9">[10]</ref>. In one approach using PCM devices, the internal state of the neuron is represented by the phase configuration of the device [Figure <ref type="figure">4(a)</ref>]. By translating the neuronal input to appropriate electrical signals, the firing frequency can be tuned in a highly controllable manner proportional to the strength of the input signal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1. A comparison of state-of-the-art neuromorphic chips, along with some performance attributes.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chip</head><p>In addition to the deterministic neuronal dynamics, stochastic neuronal dynamics also play a key role in signal encoding and transmission in biological NNs. One notable example is the use of neuronal populations to represent and transmit sensory and motor signals. The PCM-based neurons exhibit significant interneuronal as well as intraneuronal randomness, thus mimicking this stochastic neuronal behavior at the device level. Hence, multiple I&amp;F cycles in a single phase-change neuron could generate a distribution of interspike intervals, and this enables population-based computation. By exploiting this, fast signals were shown to be accurately represented by population coding, despite the rather slow firing rate of the individual neurons [Figure <ref type="figure">4(b)</ref>].</p><p>Memristive devices organized in a crossbar architecture can also be used to emulate the two essential synaptic attributes, namely, synaptic efficacy and plasticity (Figure <ref type="figure">5</ref>). Synaptic efficacy refers to the generation of a synaptic output based on the incoming neuronal activation; this can be realized using Ohm's law by measuring the current that flows through the device when an appropriate read voltage signal is applied. Synaptic plasticity, in contrast, is the ability of the synapse to change its weight, typically during the execution of a learning algorithm. The crossbar architecture is well suited to implement synaptic plasticity in a parallel and efficient manner by the application of suitable write pulses along the wires of the crossbar.</p><p>Although nanoscale devices offer exciting computational possibilities and scaling potential, several challenges need to be overcome to enable commercial products. PCM is based on the rapid and reversible phase transition of certain types of materials, such as germanium antimony telluride (Ge 2 Sb 2 Te 5 ). However, it is necessary to reduce the programming The accumulative behavior of memristive devices can be exploited to emulate neuronal dynamics. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FIGURE 4. (a)</head><p>A schematic illustration of a phase-change neuron that consists of the dendrites, the soma, and the axon. The key element is the neuronal membrane, which stores the membrane potential in the phase configuration of a PCM device. It is possible to connect the dendrites to plastic synapses that interface the neuron with other neurons in a network. (b) The representation of high-frequency signals via population coding of 500 slow-firing stochastic phase-change neurons. Also shown is the error in the representation of the stimulus by the population code. The population code captures the input signal despite all of the neurons in the population having their actual spiking frequency less than twice the base frequency of the input <ref type="bibr" target="#b9">[10]</ref>.</p><p>current as well as improve the temporal stability of the achieved conductance states. RRAM and CBRAM typically rely on the formation and rupture of nanoscale filaments to achieve multiple conductance values. This filamentary mechanism is particularly prone to inter-and intradevice variations, which is currently the major technical hurdle.</p><p>STT-MRAM devices consist of two magnetic layers separated by a tunnel barrier. These devices exhibit two resistive states, depending on whether the magnetization of the two layers is in a parallel or antiparallel direction. Devices based on STT-MRAM are expected to exhibit almost unlimited endurance and faster switching compared to those involving RRAM and PCM. However, the key challenge is a substantially lower dynamic range in programmable conductance states (typically a factor of two to three) as opposed to PCM and RRAM (which exhibit a dynamic range exceeding 100). It is crucial that new circuits and archi-tectures are developed that can mitigate these nonidealities <ref type="bibr" target="#b20">[21]</ref>.</p><p>There are also circuit-level challenges, such as the voltage drop across the long wires connecting the devices as well as the overhead introduced by data converters and other peripheral circuitry. These aspects would limit the size of the memristive crossbars that could be realized. However, in spite of these challenges, it is expected that significant gains in area and power efficiency are possible by employing nanoscale memristive devices in neuromorphic processors <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Signal processing applications</head><p>Neuromorphic processors strive to balance the efficiency of computation with the energy needed for this computation, similar to the human brain. Systems enabled by such processors are expected to have the first impact on smart edge devices, such as wearables, mobile devices, Internet of Things (IoT) Biological Synapses <ref type="bibr" target="#b11">[12]</ref> Single-Pulse Scheme FIGURE 5. Memristive devices organized in a crossbar configuration can be used to emulate synaptic communication and plasticity behaviors, such as STDP, by applying programming pulses from the periphery. Two devices, one for potentiation (LTP) and the other for depression (LTD), are used to represent the synapse in the array <ref type="bibr" target="#b19">[20]</ref>. (Adapted from <ref type="bibr" target="#b19">[20]</ref>.) it is expected that significant gains in area and power efficiency are possible by employing nanoscale memristive devices in neuromorphic processors.</p><p>sensors, and driverless vehicles, which have stringent constraints on size, weight, area, and power and are required to intelligently interact with the world autonomously for extended periods of time. Neuromorphic approaches could result in highly powerefficient devices capable of responding quickly in an intelligent manner in dynamic environments for such applications.</p><p>An illustrative example of this neuromorphic approach is the recent demonstration of a recurrent SNN that learns directly from data streams while using only a limited number of training examples and a relatively small memory for learning <ref type="bibr" target="#b23">[24]</ref>. Real-world data, represented in the form of spikes, were used for event-triggered learning, which was implemented in a microprocessor operating in the asynchronous mode <ref type="bibr" target="#b22">[23]</ref>. SNNs naturally enable sparse representations of data and can be efficiently implemented using asynchronous logic because data processing occurs only during spike events (Figure <ref type="figure" target="#fig_7">6</ref>). By implementing local learning rules on networks with sparse connectivity, both the memory required to store the network parameters and the time needed to train the model can be minimized significantly compared to traditional machine-learning techniques <ref type="bibr" target="#b23">[24]</ref>. This approach that implements SNNs in asynchronous processors has huge potential to enable edge devices that can learn and infer efficiently in the field.</p><p>Along with the development of neuromorphic processors, bioinspired event-driven sensors have emerged, which can further accelerate the development of intelligent edge devices <ref type="bibr" target="#b10">[11]</ref>. The most notable among them is the dynamic vision sensor (DVS) camera, inspired by the information processing mechanisms of the retina, and the silicon cochlea chip, inspired by how the inner ear encodes sound signals in the spike domain. These sensors have superior performance compared to conventional sensors in several respects. For instance, the DVS camera has a 1,000-times greater advantage in sampling rate compared to a conventional camera, which helps in capturing fastchanging events in the visual scene <ref type="bibr" target="#b24">[25]</ref>. In recent years, there have been several demonstrations of systems that combine such event-driven sensors with efficient neuromor phic processors, some of which will be discussed subsequently.</p><p>We now describe some of the proofof-concept demonstrations targeting signal processing applications using the neuromorphic platforms discussed in the section "State-of-the-Art Neuromorphic Hardware." In a notable example using IBM's TrueNorth chip, deep NNs were trained with a modified backpropagation rule, so that the weights and neuronal dynamics could be easily ported to the hardware, which supported only low-precision synaptic weights; software equivalent performance was achieved for several benchmark pattern classification tasks with this approach <ref type="bibr" target="#b25">[26]</ref>. In another instance, a convolutional network running on TrueNorth that received video input from a DVS camera was able to identify the onset of hand gestures with a latency of 105 ms while consuming less than 200 mW <ref type="bibr" target="#b26">[27]</ref>. Intel's Loihi has demonstrated an improvement of more than three orders of magnitude in energy-delay product compared to conventional solvers running on a CPU for least absolute shrinkage and selection operator (commonly known as LASSO) optimization problems by using a spiking convolutional implementation of the locally competitive algorithm <ref type="bibr" target="#b3">[4]</ref>.</p><p>Similarly, the SpiNNaker platform has already been used in several applications of spiking networks. A large spiking model of the visual cortex from a neural simulator running on a conventional compute cluster was recently ported to SpiN-Naker, demonstrating comparable accuracy and favorable speed and power consumption <ref type="bibr" target="#b27">[28]</ref>. Several neuromorphic applications have also been pioneered on the BrainScaleS system, in particular on Spikey, the predecessor of the HiCANN chip. These include several networks performing various computational neuroscience tasks <ref type="bibr" target="#b28">[29]</ref> and the first published assessment of pattern recognition on neuromorphic hardware <ref type="bibr" target="#b29">[30]</ref>. In an illustrative example with this approach, fewer than 2,000 spikes are needed to learn the features of an image that is conventionally represented using more than 100,000 pixels <ref type="bibr" target="#b22">[23]</ref>.</p><p>Neuromorphic approaches could result in highly power-efficient devices capable of responding quickly in an intelligent manner in dynamic environments.</p><p>brain-machine interface (BMI) application <ref type="bibr" target="#b30">[31]</ref>. A Kalman-filter-based decoder was implemented via an SNN and tested in BMI experiments with a rhesus monkey. The success of this closed-loop decoder shows the promise of neuromorphic chips for implementing signal processing algorithms in a power-efficient manner, which is a major enabling factor for the clinical translation of neural motor prostheses. The DYNAP-SE chip was recently used for reservoir computing that presented the first steps toward the design of a neuromorphic event-based neural processing system that can be directly interfaced to surface electromyography sensors for the online classification of motor neuron output activities <ref type="bibr" target="#b31">[32]</ref>.</p><p>In spite of being saddled with reliability and variability issues, memristive synapses have also demonstrated immense potential for signal processing applications. One noteworthy example was the demonstration of an SNN for detecting temporal correlations in an unsupervised fashion using plastic PCM synapses <ref type="bibr" target="#b20">[21]</ref> (see Fig ure <ref type="figure" target="#fig_8">7</ref>). The network consisted of a spiking neuron receiving an event-based data stream encoded as presynaptic input spikes arriving at PCM synapses. Most of the data streams were temporally uncorrelated, but a small subset was chosen to be mutually correlated. Postsynaptic currents were generated at the synapses that received a spike and were integrated by the neuron, which generated a spike when its membrane potential exceeded a threshold. An STDP rule was used to update the synaptic weights. Since the temporally correlated inputs are more likely to eventually govern the neuronal firing events, the conductance of synapses receiving correlated inputs should increase, whereas that of synapses whose inputs are uncorrelated should decrease. Hence, the final steady-state distribution of the weights should show a separation between synapses receiving correlated and uncorrelated inputs.</p><p>In the experiment, 144,000 input streams were fed through more than 1 million PCM devices representing the synapses. As shown in Figure <ref type="figure" target="#fig_8">7</ref>(b), well-separated synaptic distributions were achieved in the network at the end of the experiment, even though the devices exhibited significant deviceto-device variability and drift in the programmed conductance states. This demonstrated that nanoscale devices can enable complex computational capabilities in neuromorphic hardware platforms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Future outlook</head><p>It is widely believed that because of the added temporal dimension, SNNs should be computationally superior to and thus transcend second-generation deep NNs. The energy efficiency of neuromorphic systems based on SNNs makes them ideal candidates for embedded applications, such as mobile phones, robotics, the IoT, and personalized medicine, which are subject to strict power and area constraints. Moreover, as Moore's law for CMOS scaling is coming to an end, these systems offer unique opportunities to leverage new materials and device structures going beyond standard CMOS processing.</p><p>Going forward, there are algorithmic as well as technological challenges. From an algorithmic perspective, despite considerable advances, SNNs are yet to conclusively demonstrate superior  Bioinspired eventdriven sensors have emerged, which can further accelerate the development of intelligent edge devices.</p><p>performance compared to conventional deep learning, both in terms of accuracy and in many cases in terms of energy efficiency as well. This gap in performance could be attributed to the lack of efficient and scalable supervised SNN learning algorithms, the lack of efficient local learning rules that can reach the performance of backpropagation, and the reliance on rate coding as opposed to more energy-efficient temporal coding.</p><p>Very recently, promising alternatives to rate coding that enable efficient use of spike times have been introduced for SNNs <ref type="bibr" target="#b32">[33]</ref>. Moreover, it was shown that recurrent SNNs with adaptive neurons can achieve a classification performance comparable to state-of-the-art LSTM networks <ref type="bibr" target="#b33">[34]</ref>. Furthermore, efficient strategies have been demonstrated for converting deep NNs to spiking networks for complex problems, with negligible loss of accuracy <ref type="bibr" target="#b34">[35]</ref>. Recently, a novel recurrent artificial NN unit called Spiking Neural Unit was introduced that directly incorporates spiking neurons in deep-learning architectures and achieves competitive performance by using backpropagation through time <ref type="bibr" target="#b35">[36]</ref>.</p><p>Finally, it should be noted that some types of neuromorphic hardware support deep learning directly (i.e., without spikes), and powerful algorithms have been developed to leverage the specific advantages of the architectures <ref type="bibr" target="#b14">[15]</ref>. Also, modifications of traditional deep-learning algorithms have been proposed that enable their implementation in neuromorphic hardware using binary or ternary representations of neuronal activations and synaptic weights, although higher precision is required for gradient accumulation in these networks during training <ref type="bibr" target="#b36">[37]</ref>.</p><p>From a technology perspective, there are also numerous challenges associated with the use of memristive devices for neuromorphic computing. One key challenge applicable to all memristive technologies is related to the variations in the programmed conductance states with time and ambient temperature. The nonlinearity and stochasticity associated with the accumulative behavior also pose scaling challenges. Recent breakthroughs, such as multicell architectures, hold great promise to address these issues <ref type="bibr" target="#b20">[21]</ref>.</p><p>In summary, we believe that there will be two stages of innovations for the field of low-power, brain-inspired computing platforms. The near-term breakthroughs will come from neuromorphic accelerators built with conventional low-power, mixed-signal CMOS architectures, which are expected to lead to a period of transformative growth involving large neuromorphic platforms designed using ultralow-power computational memories that leverage nanoscale memristive technologies. However, it has to be emphasized that algorithmic exploration has to go hand-in-hand with advances in the hardware technologies.</p><p>Marie Curie Fellowship (European Commission). In 2016, he joined the University of Hertfordshire.</p><p>Narayan Srinivasa (physynapse@gmail.com) received his B.Tech degree from the Indian Institute of Technology, Varanasi, in 1988 and his Ph.D. degree from the University of Florida in Gainesville in 1994, both in mechanical engineering. He was a Beckman postdoctoral fellow with the Human-Computer Intelligent Interaction group at the Beckman Institute in University of Illinois at Urbana-Champaign from 1994 to 1997. He is currently the director of machine intelligence research programs at Intel Labs, Santa Clara, California. Prior to that, he was the chief technology officer at Eta Compute, Westlake Village, California, focusing on the development of ultralow-power artificial intelligence solutions for audio applications. From 2016 to 2017, he was chief scientist and senior principal engineer at Intel Labs. He has authored more than 90 papers in peer-reviewed journals and conferences and has been issued 66 U.S. patents.</p><p>Evangelos Eleftheriou (ele@zurich.ibm.com) received a diploma of engineering degree from the University of Patras, Greece, in 1979 and a master of engineering and Ph.D. degrees from Carleton University, Ottawa, Canada, in 1981 and 1985, respectively. He is currently responsible for the neuromorphic computing activities at IBM Research ZÃ¼rich. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(c)]. Pre-and postsynaptic Computing in the brain follows a completely different paradigm than today's conventional computing systems.timing are thus two factors that determine the change of a synaptic weight.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FIGURE 1 .</head><label>1</label><figDesc>FIGURE 1. (a)The brain employs a massively parallel computational network comprising ~1011 neurons, each with some 5,000-10,000 synapses, in a</figDesc><graphic coords="3,186.77,444.32,62.06,232.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>FIGURE 2 .</head><label>2</label><figDesc>FIGURE 2. The architecture and communication protocols used in neuromorphic systems. (a) A tiled array of neuromorphic cores, with each core integrating neurons and synapses locally. (b) Spikes are communicated between cores through a routing network using address event representation protocols<ref type="bibr" target="#b2">[3]</ref>,<ref type="bibr" target="#b10">[11]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Going beyond conventional CMOS, a new class of emerging nanoscale devices, namely, resistive memory, or memristive devices with their nonvolatile storage capability, is particularly well suited for developing computing substrates for SNNs. In these devices, information is stored in their resistance or conductance states. The four main types of memristive devices are phase-change memory (PCM), metal-oxidebased resistive RAM (RRAM), conductive bridge RAM (CBRAM), and spin-transfer-torque magnetic RAM (STT-MRAM) [see Figure3(a)]. The resistance values of these devices are altered by the application of appropriate electrical pulses through various physical mechanisms, such as phase transition, ionic drift, and spintronic effects. Besides this ability to achieve multiple resistance values, many of these devices also exhibit an accumulative behavior whereby the resistance values can be incrementally increased or decreased upon the application of successive programming pulses of the same amplitude. These attributes are key to their application in neuromorphic computing, as illustrated in Figure3(b) for PCM devices.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>FIGURE 3 .</head><label>3</label><figDesc>FIGURE 3. (a) A schematic illustration of memristive devices: STT-MRAM, PCM, RRAM, and CBRAM [19]. (b) The incremental programming of PCM conductance [10].</figDesc><graphic coords="8,279.17,471.44,227.18,182.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>ât (ms)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>FIGURE 6 .</head><label>6</label><figDesc>FIGURE 6. (a) Sparse, event-triggered signal encoding, which can be mapped efficiently to recurrent populations of spiking neurons with balanced spiking activity, i.e., between excitatory (blue) and inhibitory (red) neurons, can enable efficient learning. (b) In an illustrative example with this approach, fewer than 2,000 spikes are needed to learn the features of an image that is conventionally represented using more than 100,000 pixels<ref type="bibr" target="#b22">[23]</ref>.</figDesc><graphic coords="11,200.99,450.00,324.02,219.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>FIGURE 7 .</head><label>7</label><figDesc>FIGURE 7. (a) An SNN trained to perform the task of temporal correlation detection through unsupervised learning. (b) Synaptic weight evolution as a function of time and the synaptic weight distribution at the end of the experiment. Ten percent of the synapses receive correlated input data streams, with a correlation coefficient of 0.75<ref type="bibr" target="#b20">[21]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>He was a corecipient of the 2003 IEEE Communications Society Leonard G. Abraham Prize and the 2005 Technology Award of the Eduard Rhein Foundation. In 2009, he was a corecipient of the IEEE Control Systems Technology Award and the IEEE Transactions on Control Systems Technology Outstanding Paper Award. He was appointed an IBM fellow in 2005 and was inducted into the U.S. National Academy of Engineering as a Foreign Member in 2018. He is a Fellow of the IEEE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="1,-8.58,270.62,282.78,291.59" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Bipin Rajendran received partial support for this work from the U.S. National Science Foundation grant 1710009 and grant 2717.001 from the Semiconductor Research Corporation. Michael Schmuker received funding from the European Commission (H2020, Human Brain Project), grant 785907. Abu Sebastian acknowledges support from the European Research Council through the European Union's Horizon 2020 Research and Innovation Program under grant 682675.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Authors</head><p>Bipin <ref type="bibr">Rajendran</ref>  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neuromorphic electronic systems</title>
		<author>
			<persName><forename type="first">C</forename><surname>Mead</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="1629" to="1636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The SpiNNaker project</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Furber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Galluppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Temple</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Plana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="652" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A million spiking-neuron integrated circuit with a scalable communication network and interface</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Merolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">345</biblScope>
			<biblScope unit="issue">6197</biblScope>
			<biblScope unit="page" from="668" to="673" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Loihi: A neuromorphic manycore processor with on-chip learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Davies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="82" to="99" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A mixed-signal universal neuromorphic computing system</title>
		<author>
			<persName><forename type="first">K</forename><surname>Meier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Electron Devices Meeting</title>
		<meeting>IEEE Int. Electron Devices Meeting</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="461" to="464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">NeuroGrid: A mixed-analog-digital multichip system for large-scale neural simulations</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">V</forename><surname>Benjamin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="699" to="716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A scalable multicore architecture with heterogeneous memory structures for dynamic neuromorphic asynchronous processors (DYNAPs)</title>
		<author>
			<persName><forename type="first">S</forename><surname>Moradi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Stefanini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Indiveri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Circuits Syst</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="106" to="122" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A 0.086-mm 2 12.7-pJ/SOP 64k-synapse 256-neuron online-learning digital spiking neuromorphic processor in 28-nm CMOS</title>
		<author>
			<persName><forename type="first">C</forename><surname>Frenkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lefebvre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Legat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Circuits Syst</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="145" to="158" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Synaptic electronics: Materials, devices and applications</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kuzum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-S</forename><forename type="middle">P</forename><surname>Wong</surname></persName>
		</author>
		<idno type="DOI">10.1088/0957-4484/24/38/382001</idno>
	</analytic>
	<monogr>
		<title level="j">Nanotechnology</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">38</biblScope>
			<biblScope unit="page">382001</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Stochastic phase-change neurons</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tuma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pantazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Le</forename><surname>Gallo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Eleftheriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Nanotechnol</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="693" to="699" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Event-Based Neuromorphic Systems</title>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Delbruck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Indiveri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Whatley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Douglas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Wiley</publisher>
			<pubPlace>Hoboken, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Synaptic modifications in cultured hippocampal neurons: Dependence on spike timing, synaptic strength, and postsynaptic cell type</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">Q</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Poo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neurosci</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">24</biblScope>
			<biblScope unit="page" from="464" to="474" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Reinforcement learning in populations of spiking neurons</title>
		<author>
			<persName><forename type="first">R</forename><surname>Urbanczik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Senn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Neurosci</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="250" to="252" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A 45nm CMOS neuromorphic chip with a scalable architecture for learning in networks of spiking neurons</title>
		<author>
			<persName><forename type="first">J</forename><surname>Seo</surname></persName>
		</author>
		<idno type="DOI">10.1109/CICC.2011.6055293</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Custom Integrated Circuits Conf</title>
		<meeting>IEEE Custom Integrated Circuits Conf</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Memory-efficient deep learning on a SpiNNaker 2 prototype</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.3389/fnins.2018.00840</idno>
	</analytic>
	<monogr>
		<title level="j">Front. Neurosci</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<date type="published" when="2018-11">Nov. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A wafer-scale neuromorphic hardware system for large-scale neural modeling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schemmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>BrÃ¼derle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>GrÃ¼bl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Millner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Symp. Circuits and Systems</title>
		<meeting>IEEE Int. Symp. Circuits and Systems</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1947" to="1950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Demonstrating hybrid learning in a flexible neuromorphic hardware system</title>
		<author>
			<persName><forename type="first">S</forename><surname>Friedmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schemmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>GrÃ¼bl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hartel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Meier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Circuits Syst</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="128" to="142" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Braindrop: A mixed-signal neuromorphic architecture with a dynamical systems-based programming model</title>
		<author>
			<persName><forename type="first">A</forename><surname>Neckar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="144" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Memory leads the way to better computing</title>
		<author>
			<persName><forename type="first">H.-S</forename><forename type="middle">P</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Salahuddin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Nanotechnol</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="191" to="194" />
			<date type="published" when="2015-03">Mar. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Low-energy robust neuromorphic computation using synaptic devices</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kuzum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jeyasingh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-S</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Electron Devices</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3489" to="3494" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neuromorphic computing with multi-memristive synapses</title>
		<author>
			<persName><forename type="first">I</forename><surname>Boybat</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41467-018-04933-y</idno>
	</analytic>
	<monogr>
		<title level="j">Nature Commun</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Specifications of nanoscale devices &amp; circuits for neuromorphic computational systems</title>
		<author>
			<persName><forename type="first">B</forename><surname>Rajendran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gopalakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Electron Devices</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="246" to="253" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Micropowered intelligence for edge devices</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srinivasa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Raghavan</surname></persName>
		</author>
		<ptr target="https://www.embedded-computing.com/guest-blogs/micropower-intelligence-for-edge-devices" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Embedded Computing Des</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning to recognize actions from limited training examples using a recurrent spiking neural model</title>
		<author>
			<persName><forename type="first">P</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srinivasa</surname></persName>
		</author>
		<idno type="DOI">10.3389/fnins.2018.00126</idno>
	</analytic>
	<monogr>
		<title level="j">Front. Neurosci</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="2018-02">Mar. 2, 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Event-based, 6-DOF pose tracking for high-speed maneuvers</title>
		<author>
			<persName><forename type="first">E</forename><surname>Mueggler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/RSJ Int. Conf. Intelligent Robots and Systems</title>
		<meeting>IEEE/RSJ Int. Conf. Intelligent Robots and Systems</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2761" to="2768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Convolutional networks for fast, energy-efficient neuromorphic computing</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Esser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Nat. Acad. Sci</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">41</biblScope>
			<biblScope unit="page" from="441" to="452" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A low power, fully event-based gesture recognition system</title>
		<author>
			<persName><forename type="first">A</forename><surname>Amir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="7388" to="7397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Performance comparison of the digital neuromorphic hardware SpiNNaker and the neural network simulation software NEST for a fullscale cortical microcircuit model</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Van Albada</surname></persName>
		</author>
		<idno type="DOI">10.3389/fnins.2018.00291</idno>
	</analytic>
	<monogr>
		<title level="j">Front. Neurosci</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<date type="published" when="2018-05">May 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Six networks on a universal neuromorphic computing substrate</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pfeil</surname></persName>
		</author>
		<idno type="DOI">10.3389/fnins.2013.00011</idno>
	</analytic>
	<monogr>
		<title level="j">Front. Neurosci</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="2013-02">Feb. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A neuromorphic network for generic multivariate data classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schmuker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pfeil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Nawrot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Nat. Acad. Sci</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2081" to="2086" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A brain-machine interface operating with a real-time spiking neural network control algorithm</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dethier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nuyujukian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Eliasmith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Elasaad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">V</forename><surname>Shenoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Boahen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances Neural Information Processing Systems</title>
		<meeting>Advances Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2213" to="2221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Processing EMG signals using reservoir computing on an event-based neuromorphic system</title>
		<author>
			<persName><forename type="first">E</forename><surname>Donati</surname></persName>
		</author>
		<idno type="DOI">10.1109/BIOCAS.2018.8584674</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Biomedical Circuits and Systems Conf</title>
		<meeting>IEEE Biomedical Circuits and Systems Conf</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">STDPbased spiking deep convolutional neural networks for object recognition</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Kheradpisheh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ganjtabesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Thorpe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Masquelier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="56" to="67" />
			<date type="published" when="2018-03">Mar. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Long short-term memory and learning-to-learn in networks of spiking neurons</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bellec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Salaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Subramoney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Legenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Maass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Information Processing Systems</title>
		<meeting>Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="787" to="797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Going deeper in spiking neural networks: VGG and residual architectures</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Roy</surname></persName>
		</author>
		<idno type="DOI">10.3389/fnins.2019.00095</idno>
	</analytic>
	<monogr>
		<title level="j">Front. Neurosci</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<date type="published" when="2019-03">Mar. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Woz Â´niak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pantazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bohnstingl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Eleftheriou</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1812.07040" />
		<title level="m">Deep networks incorporating spiking neural dynamics</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Binaryconnect: Training deep neural networks with binary weights during propagations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-P</forename><surname>David</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 28th Int. Conf. Neural Information Processing Systems</title>
		<meeting>28th Int. Conf. Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="3123" to="3131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName><surname>Sp</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
