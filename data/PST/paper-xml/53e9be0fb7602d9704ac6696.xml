<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Objective Functions for Training New Hidden Units in Constructive Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tin-Yau</forename><surname>Kwok</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Science and Technology</orgName>
								<address>
									<settlement>Hong Kong, Kowloon, Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Science and Technology</orgName>
								<address>
									<settlement>Hong Kong, Kowloon, Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Objective Functions for Training New Hidden Units in Constructive Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9DC58EC02D1DAC54504D027CC37068C7</idno>
					<note type="submission">received November 18, 1995; revised June 30, 1996 and April 2, 1997. This work was supported in part by the Hong Kong Telecom Institute of Information Technology under Grant HKTIIT 92/93.002, and the Hong Kong Research Grants Council under Grant HKUST 15/91.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T04:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Cascade-correlation</term>
					<term>constructive algorithms</term>
					<term>convergence</term>
					<term>input weight freezing</term>
					<term>quickprop</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we study a number of objective functions for training new hidden units in constructive algorithms for multilayer feedforward networks. The aim is to derive a class of objective functions the computation of which and the corresponding weight updates can be done in O(N) time, where N is the number of training patterns. Moreover, even though input weight freezing is applied during the process for computational efficiency, the convergence property of the constructive algorithms using these objective functions is still preserved. We also propose a few computational tricks that can be used to improve the optimization of the objective functions under practical situations. Their relative performance in a set of two-dimensional regression problems is also discussed.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>I N recent years, many neural-network models have been proposed for pattern classification, function approximation, and regression problems. Among them, the class of multilayer feedforward networks is perhaps the most popular. Standard backpropagation performs gradient descent only in the weight space of a network with fixed topology. In general, it is useful only when the network architecture (i.e., model) is chosen correctly. Too small a network cannot learn the problem well, but a size too large will lead to over-generalization and thus poor performance. This can be easily understood by analogy to the problem of curve fitting using polynomials. Consider a data set generated from a smooth underlying function with additive noise on the outputs. A polynomial with too few coefficients will be unable to capture the underlying function from which the data was generated, while a polynomial with too many coefficients will fit the noise in the data and again result in a poor representation of the underlying function. For an optimal number of coefficients, the fitted polynomial will give the best representation of the function and also the best predictions for new data. A similar situation arises in the application of neural networks, where it is again necessary to match the network complexity to the problem being solved. Algorithms that can find an appropriate network architecture automatically are thus highly desirable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Matching the Network Complexity to the Problem</head><p>There are three major approaches to tackle this problem. The first involves using a larger than needed network and training it until an acceptable solution is found. After this, some hidden units or weights are removed if they are no longer actively used. Methods using this approach are called pruning algorithms. The second approach, which corresponds to constructive algorithms, starts with a small network and then grows additional hidden units and weights until a satisfactory solution is found. Review for pruning algorithms can be found in <ref type="bibr" target="#b0">[1]</ref>, while that for constructive algorithms in <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b3">[4]</ref>.</p><p>The constructive approach has a number of advantages over the pruning approach. First, for constructive algorithms, it is straightforward to specify an initial network, <ref type="foot" target="#foot_0">1</ref> whereas for pruning algorithms, one does not know in practice how big the initial network should be. Second, constructive algorithms always search for small network solutions first. They are thus more computationally economical than pruning algorithms, in which the majority of the training time is spent on networks larger than necessary. Third, as many networks with different sizes may be capable of implementing acceptable solutions, constructive algorithms are likely to find smaller network solutions than pruning algorithms. Smaller networks are more efficient in forward computation and can be described by a simpler set of rules. Functions of individual hidden units may also be more easily visualized. Moreover, by searching for small networks, the amount of training data required for good generalization may be reduced. Fourth, pruning algorithms usually measure the change in error when a hidden unit or weight in the network is removed. However, such changes can only be approximated for computational efficiency, and hence may introduce large errors, especially when many are to be pruned.</p><p>The third approach is regularization <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b6">[7]</ref>. However, regularization cannot alter the network structure, which must be specified in advance by the user. Although in principle the use of regularization should allow the network to be overly large, in practice using an overly large network makes optimization of the regularized error function with respect to its network weights more computationally intensive and difficult. Moreover, there is a delicate balance, controlled by a regularization parameter, between the error term and the penalty term. Early attempts either set this regularization parameter manually <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref> or by ad hoc procedures <ref type="bibr" target="#b6">[7]</ref>. A more disciplined and long respected method is cross-validation <ref type="bibr" target="#b7">[8]</ref>. However, this approach is usually very slow for nonlinear models like neural networks, because a large number of nonlinear optimization problems must be repeated. Recently, several researchers <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b11">[12]</ref> have incorporated Bayesian methods into neural-network learning. Regularization can then be accomplished by using appropriate priors that favor small network weights (such as the normal <ref type="bibr" target="#b9">[10]</ref> or Laplace <ref type="bibr" target="#b12">[13]</ref> distribution), and the regularization parameter can be automatically set. However, Bayesian inference mechanisms must often assume asymptotic normality of the posterior distributions, which may break down when the number of weights in the network is large compared to the training set size. <ref type="foot" target="#foot_1">2</ref> This scenario is more likely to occur in overly large networks, further escalating the problem of determining an appropriate initial network for use in regularization.</p><p>Hence, in this paper, we will focus mainly on constructive algorithms. Moreover, we will be particularly interested in regression problems, in which networks with linear output units are the most common. Note that classification problems can be considered as a special case of regression problems. Some constructive methods, such as those found in <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b17">[18]</ref>, that can only be applied to classification problems will not be discussed here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. How to Train New Hidden Units</head><p>There are three major problems involved in the design of constructive algorithms:</p><p>1) How to connect: How to connect the new hidden unit to the existing network? 2) How to train: How to determine the (new and existing) weights in the network after connections to the new hidden unit are made? 3) When to stop: When to stop the addition of new hidden units, or, in other words, what is the optimal number of hidden units to be installed in the network? The first issue has direct implications to the resultant network architecture. It is well known that no single class of network architectures is ideal for all problems <ref type="bibr" target="#b18">[19]</ref>, and so different connection strategies for the new hidden units are suitable for different problems. But a comprehensive comparison of different architectures is still lacking.</p><p>The third issue on model selection may be addressed using different techniques, such as by comparing evidence in a Bayesian framework <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b13">[14]</ref>, or, in a non-Bayesian framework, by the use of some information criteria <ref type="bibr" target="#b19">[20]</ref>- <ref type="bibr" target="#b24">[25]</ref> or data resampling methods <ref type="bibr" target="#b25">[26]</ref>- <ref type="bibr" target="#b27">[28]</ref>. This is still an active research topic. Interested readers may see the discussions in <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b28">[29]</ref>, and <ref type="bibr" target="#b29">[30]</ref>.</p><p>In this paper, we will focus on the second issue. In principle, it is fairly straightforward as one can simply train the whole network again (as in <ref type="bibr" target="#b30">[31]</ref> and <ref type="bibr" target="#b31">[32]</ref>). However, while in neural networks with fixed architecture the weights are trained only once, here in constructive algorithms, training must be repeated every time hidden units are added. Hence, computational efficiency, in terms of both time and space, becomes an important issue. Most optimization routines do not scale up well when the number of weights is large. For example, Newton's method requires computation of the Hessian matrix, entailing a space requirement of and a time requirement of in each iteration. Quasi-Newton methods still have a space requirement of , but do not require evaluating the Hessian matrix and are implementable in ways which only require arithmetic operations in each iteration. The price to pay is that instead of having the quadratic local convergence of Newton's method, quasi-Newton methods only exhibit a local superlinear convergence rate. Conjugate gradient methods do not require storage or maintenance of the Hessian, but their local convergence rate is roughly linear only. Simple gradient descent methods, though they only require space and time for each iteration, are notoriously slow when the network size is large <ref type="bibr" target="#b32">[33]</ref>. Thus, in short, techniques to improve computational efficiency are essential in practical constructive algorithms, and these will be discussed below.</p><p>On the other hand, a frequently overlooked issue concerning constructive algorithms is their convergence properties. The question of convergence can be stated as follows: Can the constructive algorithm produce a convergent sequence of network functions that, in the limit, approximates the target function as closely as desired? In other words, does the sequence so generated strongly converge<ref type="foot" target="#foot_3">3</ref> to as ? Apparently, the universal approximation capability 4  of a network structure is necessary for the convergence of its constructive algorithm. However, we will see in Section II why this may not be sufficient.</p><p>Improving Computational Complexity: A common technique used in constructive algorithms (like <ref type="bibr" target="#b38">[39]</ref>- <ref type="bibr" target="#b42">[43]</ref>) to simplify the optimization problem and thus improve computational complexity is to assume that the hidden units already existing in the network are useful in modeling part of the target function. Hence, we can keep the weights feeding into these hidden units fixed (input weight freezing), and allow only the weights connected to the new hidden unit and the output units to vary. The number of weights to be optimized, and the time and space requirements for each iteration, can thus be greatly reduced. This reduction is especially significant when there are already many hidden units installed in the network.</p><p>Training of the adaptable weights may then be performed by backpropagation as usual <ref type="bibr" target="#b41">[42]</ref>, but the computational requirement may be further reduced by proceeding in a layerby-layer manner. First, the weights feeding into the new hidden unit are trained (input training). They are then kept constant and the weights connecting the hidden units to the outputs are enough to contain the target function f or a good enough approximation of f. See, for example, <ref type="bibr" target="#b34">[35]</ref>- <ref type="bibr" target="#b37">[38]</ref> for universal approximation results on multilayer perceptrons and radial basis function networks, respectively. trained (output training). In this way, only one layer of weights needs to be optimized each time. There is never any need to backpropagate the error signals and hence is much faster.</p><p>During input training, the weights feeding into the new hidden unit are trained to optimize an objective function. The following objective functions have been used.</p><p>1) Projection index <ref type="bibr" target="#b43">[44]</ref> that finds "interesting" projections deviating from the Gaussian form. However, this is probably more suitable for exploratory data analysis <ref type="bibr" target="#b44">[45]</ref> rather than for regression or classification problems, as projections of the Gaussian form may also be useful as feature detectors.</p><p>2) The same error criterion, such as the squared error criterion, as used in output training <ref type="bibr" target="#b39">[40]</ref>. In this method, the weights in each layer are updated in turn, by keeping all the other weights unchanged, and the whole process is cycled many times. However, it will be shown later in this paper that such a criterion is inferior to the others proposed in Section II.</p><p>3) The covariance between the residual error and the new hidden unit activation, as in the cascade-correlation architecture <ref type="bibr" target="#b38">[39]</ref> and its variants. Without loss of generality, we assume that there is only one output unit in the network. New hidden units are added one at a time in a greedy manner by maximizing</p><p>where ranges over the training patterns, is the activation of the new hidden unit for pattern , is the residual error for pattern before the new hidden unit is added, and and are the corresponding values averaged over all patterns. Note that if there are training patterns, then the time complexities for computing (1) and its weight update are both of only. However, the design of is rather ad hoc. As mentioned in <ref type="bibr" target="#b38">[39]</ref>, early versions of the architecture used a true correlation measure, but the version of in (1) works better in most situations. 4) An objective function based on the use of projection matrices <ref type="bibr" target="#b45">[46]</ref>. This can be formally derived, but both the computation of this objective function and each weight update require multiplications. The space requirement is also . Ideally, this , the number of training patterns, should be large for good generalization performance, and should also scale up with problem complexity <ref type="bibr" target="#b46">[47]</ref>. Hence, this objective function may soon become infeasible for larger-scale problems. Moreover, its complex form makes it vulnerable to the problem of local optima in the optimization process. 5) Another very ad hoc objective function proposed in <ref type="bibr" target="#b47">[48]</ref>. Effect on the Convergence Property: Input weight freezing, though highly beneficial in improving computational efficiency, unfortunately also affects the convergence property of the constructive algorithm. Consider, without loss of generality, feedforward networks with one hidden layer, whose network functions can be described by the set where with being the weights connecting the inputs to the th hidden unit, the weight connecting this hidden unit to the output, and the hidden unit transfer function. In a certain function space (such as the space), universal approximation results state that when satisfies certain mild conditions (e.g., bounded and nonconstant <ref type="bibr" target="#b48">[49]</ref>), will be dense in that space, i.e., for any given target function in the space, there exists a sequence of network functions in such that strongly converges to . Notice that under this formulation, all parameters (weights) in any in the sequence are freely adjustable. However, when input weight freezing is used, the new network estimate , constructed from , cannot change the input weights of the hidden units associated with . This restriction may then impede the approximation capabilities of the sequence . A major aim of this paper is to find objective functions such that the convergence property will be preserved when these functions are used in the constructive algorithms. Recently, there are also some results on the convergence issue. We will postpone the discussion of them to Section III.</p><p>Also, note that the norm used in the convergence definition should be based on the whole input space, not just on the training patterns as is done in <ref type="bibr" target="#b47">[48]</ref> and <ref type="bibr" target="#b49">[50]</ref>. This is because one is usually more interested in the generalization performance rather than performance on the training set, and a perfect recall of the training set is always possible simply by having more hidden units. Hence, "convergence," in the sense of reducing the training error to zero <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b49">[50]</ref>, is inadequate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Organization of this Paper</head><p>Motivated by the success of the cascade-correlation architecture in a variety of problems <ref type="bibr" target="#b50">[51]</ref>- <ref type="bibr" target="#b54">[55]</ref>, the obscure design of the objective function and the importance of having convergence properties for constructive algorithms, we discuss in Section II a class of objective functions for training new hidden units in constructive algorithms, all of which have a time complexity of without compromising the convergence property. A comparison with related works on the issue of convergence will be discussed in Section III. Practical problems in optimizing the objective functions, together with some suggested remedies, will be discussed in Section IV. Experimental comparison in using these objective functions for a series of regression problems will be described in Section V, followed by some concluding remarks in the last section.</p><p>Proofs of the mathematical results and extension to the case of nonlinear output units are in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. DESIGN OF THE OBJECTIVE FUNCTIONS</head><p>In the following, we assume that the network has only one linear output unit. The case for nonlinear output units is dealt with in Appendix B. Extension to multiple output units is straightforward. Moreover, we will focus on the space, i.e., the space of all square integrable functions. For , the inner product 5  is defined by where is the (positive) input environment measure, is the whole input space which is a bounded measurable subset in the -dimensional Euclidean space . The norm in space will be denoted as . A network, having hidden units directly connected to the output unit, implements the function given by where represents the function implemented by the th hidden unit. Note that these 's may only be indirectly connected to the input units through intermediate hidden units, and thus the network is not restricted to having only one single hidden layer. Moreover, is the residual error function for the current network with hidden units. Addition of a new hidden unit proceeds in two steps: 1) Input training: Find and such that the resultant linear combination of with the current network, i.e., , gives minimum residual error . 2) Output training: Keeping fixed, adjust the values of so as to minimize the residual error. Note that, as will be shown later, can be found as a byproduct in Step 1), without requiring backpropagation of error. This can then be used as an initial value in performing Step 2).</p><p>Step 1) is performed by maximizing an objective function over from a set . This , having finite or infinite number of elements, contains all hidden unit functions that can possibly be implemented. It thus depends on the particular constructive algorithm that specifies how the new hidden unit is connected to the existing network, how the net input to the hidden unit is computed, and the transfer function of the new hidden unit, etc.</p><p>Step 2) is used to ensure that remains orthogonal to the subspace spanned by . This minimization can 5 An inner product <ref type="bibr" target="#b33">[34]</ref> in a real linear space R is a real function defined for every pair of elements u; v 2 R and is denoted by hu;vi, with the following properties:</p><p>1) hu;ui 0 where hu;ui = 0 if and only if u = 0; 2) hu;vi = hv;ui; 3) hu;vi = hu; vi; 2 &lt;; 4) hu;v + wi = hu;vi + hu;wi. be performed by using gradient descent or, when the output unit is linear and squared error criterion is used, by computing the pseudo-inverse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Derivation for and</head><p>We first address the problem in Step 1). Proposition 1: For a fixed , <ref type="foot" target="#foot_4">6</ref> the expression achieves its minimum iff However, in practice, (3) can only be calculated when the exact functional form of is available, which is obviously impossible as the true is unknown. A consistent <ref type="foot" target="#foot_5">7</ref> estimate of (3) using information from the training set is , where is the activation of the new hidden unit for pattern and is the corresponding residual error before this new hidden unit is added. Dropping the factor which is common for all candidate hidden unit functions, we obtain the following objective function:</p><formula xml:id="formula_1">(4)</formula><p>The corresponding update equation for connection weight feeding into this new hidden unit is Notice that the time complexities in computing this objective function and each weight update are of . Moreover, storage requirement is minimal as the 's and 's need not be stored.</p><p>The weight connecting the new hidden unit to the output unit is also determined as a by-product, given by as determined in (2). Compared to <ref type="bibr" target="#b39">[40]</ref> in which the same criterion is used for both the input and output training phases, they have to (randomly) fix an initial guess for before input training can proceed. Hence, the hidden unit found is likely to be suboptimal, and so the input and output training phases have to be iterated for several times, significantly lengthening the optimization process. In our approach, however, the optimal for the new hidden unit is automatically determined and its value is not required during input training. Moreover, the weight update process in <ref type="bibr" target="#b39">[40]</ref> can be regarded as a variant of the coordinate descent method <ref type="bibr" target="#b55">[56]</ref>, which is known to have poor convergence properties. Simulation results in Section V also confirm that this method used without iteration is inferior.</p><p>The convergence property of the constructive algorithm using (3) as the objective function during input training, with input weight freezing in effect, is assured by the following theorem.</p><p>Theorem 1: Given is dense in and for some . If is selected as to maximize , then . Requiring the set of hidden unit functions to satisfy the denseness requirement is not hard, as is supported by various universal approximation results for feedforward networks <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b56">[57]</ref>. Moreover, the boundedness assumption of holds for continuous transfer functions on bounded domains. Hence, Theorem 1 shows that the sequence of networks so constructed incrementally strongly converges to the target function.</p><p>If we further restrict the target function to be an exact summation of basis functions from , i.e., of the form <ref type="bibr" target="#b4">(5)</ref> for some , then, similar to <ref type="bibr" target="#b57">[58]</ref>, we can obtain the following convergence rate.</p><p>Proposition 2: If the target function is an exact summation of basis functions from as in ( <ref type="formula">5</ref>) and the network is constructed as in Theorem 1, then <ref type="bibr" target="#b5">(6)</ref> where . Or, in other words, . Because of the boundedness assumption of , the denominator in (3) can be dropped without affecting convergence.</p><p>Corollary 1: With the conditions in Theorem 1, if is selected as to maximize , then . And we arrive at the second objective function <ref type="bibr" target="#b6">(7)</ref> Again, the time complexities in computing this objective function and each weight update are of .</p><p>If we restrict the target function to be an exact summation of basis functions from , then we have the following proposition.</p><p>Proposition 3: If the target function is of the form in (5), , and the network is constructed as in Corollary 1, then <ref type="bibr" target="#b7">(8)</ref> where . Note that which implies i.e., the bound for in ( <ref type="formula">6</ref>) is smaller than that in <ref type="bibr" target="#b7">(8)</ref>. Of course, this does not necessarily mean that the network sequence constructed using as the objective function will converge faster than that using , as it also depends on how tight the bounds in ( <ref type="formula">6</ref>) and ( <ref type="formula">8</ref>) are. Nevertheless, empirical results in Section V tend to support this difference in convergence rates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Derivation for and</head><p>The objective function used in the cascadecorrelation architecture can also be derived from the above results. But let us first introduce a few notations.</p><p>Let . For , define</p><p>where Obviously, is a semi inner product <ref type="foot" target="#foot_6">8</ref> in . Here we use the notation for various spaces, which should be clear from context. The corresponding seminorm is also denoted as . To convert this semi inner product into an inner product, consider the set of all constant-valued functions in . The quotient space is the set with elements for each . For , define</p><p>where is as defined in <ref type="bibr" target="#b8">(9)</ref>. Obviously, defines an inner product in . We then have the following corollary.</p><p>Corollary 2: With the conditions in Theorem 1, if is selected as to maximize then . Hence, as and differ by at most a constant when if a constant (bias) term is included in the expansion of . The corresponding objective function to be maximized is <ref type="bibr" target="#b10">(11)</ref> where is the average value of the new hidden unit over all training patterns and is the corresponding average for the residual error. Again, only time is required for computing <ref type="bibr" target="#b10">(11)</ref> and its weight update.</p><p>Using Corollary 1, we can drop the denominator in ( <ref type="formula">11</ref>) and arrive at the fourth objective function which is the square of . Again, when the target function is restricted to be an exact summation of basis functions from , the convergence rates for or as objective functions are . Recall that the objective functions and are derived by assuming that is fixed at each step when the new hidden unit is optimized. In deriving and by using the quotient space, however, we have basically used the knowledge that the bias to the output unit will be corrected during output training, and hence the mean values of and may be ignored during input training. This is sometimes beneficial, as is illustrated by the simulation results in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. RELATED WORK ON THE CONVERGENCE ISSUE</head><p>The convergence issue of constructive algorithms has been discussed in <ref type="bibr" target="#b58">[59]</ref> and <ref type="bibr" target="#b59">[60]</ref>, originally in the context of projection pursuit regression <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b60">[61]</ref>. When the norms of the elements in are bounded (i.e., as we also assumed in Theorem 1), convergence is shown in <ref type="bibr" target="#b58">[59]</ref> when the target function is in the closure of the convex hull of . However, when the norm is bounded, the convex closure is no longer equal to the span of . Thus convergence to all functions is not guaranteed, whereas in our case, Theorem 1 holds for all functions. Moreover, in both <ref type="bibr" target="#b58">[59]</ref> and <ref type="bibr" target="#b59">[60]</ref>, the iterative sequence of network estimates is formed from a convex combination of the previous network function and the new basis function <ref type="bibr" target="#b11">(12)</ref> where . Whereas in our approach, the new is formed from full linear combination of the old and new hidden unit functions. The weights connecting the old hidden units to the output unit are thus not constrained as a group, as in <ref type="bibr" target="#b11">(12)</ref>.</p><p>Besides, in <ref type="bibr" target="#b11">(12)</ref>, has to be learned together with the new hidden unit , while in our case, the parameters of the new hidden unit are first learned and then the output layer weights. Training is thus performed for only one layer of weights at a time, making optimization of the objective function simpler. Moreover, the objective function to be minimized in <ref type="bibr" target="#b58">[59]</ref> and <ref type="bibr" target="#b59">[60]</ref> is different from that developed in the previous section, and thus results in <ref type="bibr" target="#b58">[59]</ref> and <ref type="bibr" target="#b59">[60]</ref> cannot be directly applied here.</p><p>On the other hand, while results in <ref type="bibr" target="#b58">[59]</ref> and <ref type="bibr" target="#b59">[60]</ref> and those we studied here are in the context of the space (or more general Hilbert spaces), Darken et al. <ref type="bibr" target="#b61">[62]</ref> extended the bounds on the rate of approximation to broader classes of spaces, including spaces where . However, they are more interested in showing the existence of a convergent network sequence, while we are more interested in the convergence properties of specific methods that are able to deliver such sequences.</p><p>Drago and Ridella <ref type="bibr" target="#b57">[58]</ref> also studied the convergence properties of using as the objective function for function approximation, but only for the case when the hidden unit transfer function is the hyperbolic tangent function and uniform environment measure. Our work here poses little restriction on the input environment measure or the hidden unit transfer function, so long as the hidden unit transfer function is bounded and the requirements for universal approximation are met. Moreover, a number of other objective functions, besides , are derived and discussed. Recently, Kurková and Beliczynski <ref type="bibr" target="#b62">[63]</ref> also devised the criterion in this paper as an objective function. However, their convergence proof follows from that in <ref type="bibr" target="#b58">[59]</ref>, and hence is restricted to target functions that are in the convex closure of .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. SOME PRACTICAL CONCERNS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Practical Problems</head><p>Before discussing the simulation results, we first discuss a few practical problems in optimizing the objective functions.</p><p>Problem with the Objective Function: In the following, we denote any objective function obtained in the previous section simply by . Its first and second partial derivatives with respect to the connection weight will be denoted by and , respectively.</p><p>As learning proceeds, hidden units are added to the network and the residual errors 's decrease with time. Observe that and are continuous with respect to , and when all the 's are zero. Hence as . This may pose a problem when the residual errors are small but still not acceptable. If gradient-ascent algorithms that use only first-order information (such as standard backpropagation) are used in the optimization, is changed at each step by an amount <ref type="bibr" target="#b12">(13)</ref> where is the learning rate. As is small, learning basically comes to a halt. By using optimization algorithms that use second-order information, we may be able to do a better job. Hence, in the experiments, the quickprop learning algorithm <ref type="bibr" target="#b32">[33]</ref> is used to implement this optimization. However, the problem is still not solved completely.</p><p>Problem with the Quickprop Algorithm: The quickprop algorithm is a second-order method. It assumes that the function, , to be optimized is locally quadratic with respect to each weight, and the Hessian matrix of in weight space is diagonal. Although these assumptions are quite "risky," the technique works well in practice <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b63">[64]</ref>.</p><p>Denoting at time by , the change for weight at time is given by: <ref type="bibr" target="#b13">(14)</ref> where is the weight update at time . There are cases when weight update by ( <ref type="formula">14</ref>) is not used. For example, since quickprop changes weights based on what happened during the previous weight update, <ref type="bibr" target="#b12">(13)</ref> is used to compute the first step. Another situation is when the current slope with respect to is in the same direction as that of the previous slope, and its magnitude is close to or even larger than the previous one. In this case, the next weight update is given by <ref type="bibr" target="#b14">(15)</ref> Although the aim of this restriction is to improve the stability of the algorithm, it may be problematic when one is exploring in a plateau <ref type="foot" target="#foot_7">9</ref> in the error surface. Consider the direction in the weight space. Taking the gradient ascent step in (13) at with <ref type="bibr" target="#b15">(16)</ref> Now, using Taylor series expansion and ( <ref type="formula">16</ref>) (17) <ref type="bibr" target="#b17">(18)</ref> where denotes at time . If falls in the region of a plateau, then Equation ( <ref type="formula">18</ref>) thus implies which satisfies the conditions for ( <ref type="formula">15</ref>) to be applied. The next weight update, using ( <ref type="formula">15</ref>) and ( <ref type="formula">16</ref>), is then Movement in the weight space is thus very slow.</p><p>This problem, however, is usually not that severe. Although the next weight change is small, and even if the conditions for <ref type="bibr" target="#b14">(15)</ref> to be applied continue to be satisfied, it can build up gradually given a sufficient number of training epochs. This can be seen clearly by applying ( <ref type="formula">15</ref>) repeatedly <ref type="bibr" target="#b18">(19)</ref> Thus, as , the difference between and will finally be large enough for the quadratic approximation to be applied.</p><p>However, in constructive neural-network algorithms, training is usually limited by a patience parameter <ref type="bibr" target="#b38">[39]</ref>, which means that the function to be optimized has to be improved by a certain fraction within a certain number of training epochs. Patience helps to end each learning phase when progress is excessively slow and thus saves time. Moreover, it is also experimentally shown to be able to improve generalization by avoiding overfitting of the training data <ref type="bibr" target="#b64">[65]</ref>. However, the value of this patience parameter is usually small. Hence, waiting for the weight slopes to slowly build up as in <ref type="bibr" target="#b18">(19)</ref> is not possible under such situations. More drastic action is needed to get out of the plateau in limited time. This situation is particularly acute in our case, as we have shown in the last section that when the residual errors are small, both the first and second derivatives of the objective function are likely to be small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Remedies</head><p>To alleviate the problems mentioned above, we aim at increasing the slope of the objective function to be optimized when the residual errors are small, such that the region to be searched is less likely to be a plateau. On the other hand, if we are so unfortunate that quickprop really searches in a plateau, we aim at finding a better method to get out of it quickly.</p><p>To avoid the first problem, Fahlman <ref type="bibr" target="#b32">[33]</ref> suggested to adjust by adding a small offset to the values of . However, this distorts the true surface of and, as noted by Crowder <ref type="bibr" target="#b65">[66]</ref>, this "confuses the correlation machinery" and cannot solve the problem.</p><p>Transforming the Objective Function: To increase the slope of when the residual errors are small, we transform by a (possibly nonlinear) functional , where is the space of all real-valued continuous functions such that when is small. Moreover, we require the function in to be strictly increasing. This is desirable so that locations of the local and global optima of are the same as those of .</p><p>An obvious choice of is This amounts to scaling up the dimension, or equivalently increasing the learning rate . However, increasing too much may cause oscillation, while increasing only a little may not be useful in improving the situation. Another simple choice is Now , and hence when . Thus, the slope is scaled up when is small. The smaller is, the larger is the scaling of the slope (Fig. <ref type="figure" target="#fig_1">1</ref>). Even when is large, which makes smaller than , it is not a problem because then quickprop will be mainly using the quadratic approximation <ref type="bibr" target="#b13">(14)</ref>.</p><p>Of course, there are other choices of , such as which basically changes the switch-over point where . But the basic idea of all these schemes is to dynamically alter the slope of the objective function during learning. This is similar to the idea of adaptive learning rate in backpropagation <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b66">[67]</ref>. However, we demonstrate here that a simple change in the objective function to be optimized can achieve the same goal, without requiring modification to the learning algorithm or continual updating of the learning rate which incurs additional computational burden. Finally, notice that this kind of modification to the objective functions does not affect the convergence rates of the constructive algorithm, as given in ( <ref type="formula">6</ref>) and <ref type="bibr" target="#b7">(8)</ref>.</p><p>Modification to the Quickprop Algorithm: For the second problem of enabling a faster escape from the plateau, a simple solution is to take large steps. From Section IV-A, we saw that the problem arises from always using the gradient ascent step when conditions to <ref type="bibr" target="#b14">(15)</ref> are satisfied. To alleviate this, we take the quadratic approximation in <ref type="bibr" target="#b13">(14)</ref> when the changes in all weight directions are very small, even under those conditions.</p><p>Following the analysis in Section IV-A, by taking the quadratic ascent step It is simple to verify that when , as when falls on a plateau, this will be of greater magnitude than the gradient ascent step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. SIMULATION</head><p>In this section, we compare the generalization performance of several networks constructed using the objective functions in (4), in <ref type="bibr" target="#b6">(7)</ref>, in <ref type="bibr" target="#b10">(11)</ref>, their modified versions (by taking the square root), in (1), the objective function used by Fujita in <ref type="bibr" target="#b45">[46]</ref>, and the squared error criterion 10 used in <ref type="bibr" target="#b39">[40]</ref>. Comparison is performed on a set of regression functions originally used in <ref type="bibr" target="#b39">[40]</ref>. Preliminary study on the improvements in generalization performance by the modifications discussed in Section IV-B has also been reported in <ref type="bibr" target="#b67">[68]</ref>, in the context of chaotic time series prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Setup</head><p>The networks under comparison all have linear output units and a single layer of hyperbolic tangent hidden units. Hidden units are added up to a maximum number of 15. Input training is performed by using the modified quickprop algorithm as described in Section IV-B, while output "training" is performed by computing the pseudoinverse exactly.</p><p>The regression problems used are the two-dimensional functions which have been used in <ref type="bibr" target="#b39">[40]</ref>. They are as follows.</p><p>• Simple interaction function:</p><p>• Radial function: 10 Notice that unlike the other objective functions, S sqr is always measured at the output nodes and is minimized instead of being maximized during both the input and output training phases.</p><p>Fig. <ref type="figure">2</ref>. f (1) : simple interaction.</p><p>Fig. <ref type="figure">3</ref>. f (2) : radial.</p><p>• Harmonic function:</p><p>or equivalently, with</p><p>• Additive function:</p><p>• Complicated interaction function:</p><p>Plots of these functions are shown in Figs. <ref type="figure">2</ref><ref type="figure">3</ref><ref type="figure" target="#fig_2">4</ref><ref type="figure" target="#fig_3">5</ref><ref type="figure" target="#fig_4">6</ref>.</p><p>We employ the same basic setup as in <ref type="bibr" target="#b39">[40]</ref>. Two sets of training data, one noiseless and the other noisy, are generated.   The noiseless training set has 225 points, and is generated from the uniform distribution . The same set of abscissa values ( 's) is used for experiments with all five functions. The test set, of size 10000, is generated from a regularly spaced grid on [0,1] , and is also the same for all five functions. The noisy training set is generated by adding independent and identically distributed (i.i.d.) Gaussian noise, with mean zero and standard deviation 0.25, to the noiseless training set. Its size is thus also 225. Whereas results in <ref type="bibr" target="#b39">[40]</ref> are based on only one specific set of training data, we want to get information on the variability due to the location of the 's. Hence, in the simulations below, we perform 100 independent trials each generating a different set of training data. The mean signalto-noise ratios (SNR's) for the five noisy functions are shown in Table <ref type="table" target="#tab_0">I</ref>.</p><p>As in <ref type="bibr" target="#b39">[40]</ref>, the fraction of variance unexplained (FVU) on the test set is used for comparison. It is defined as where . Note that the FVU is proportional to the commonly used mean squared error. Moreover, as mentioned in Section I-B, in this paper we focus on how to determine the network weights after hidden unit addition, rather than on the issue of determining the optimal number of hidden units to be installed in the network. Hence, in the following, we assume that some perfect criterion has been used to handle this issue and we will use for comparison the lowest testing FVU among the 15 networks ranging from one to 15 hidden units in each trial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Results</head><p>Boxplots for the best attainable testing (generalization) FVU's averaged over 100 independent trials are shown in Figs. <ref type="figure" target="#fig_6">8</ref> and<ref type="figure" target="#fig_7">9</ref>. In each boxplot, the horizontal line in the interior of the box is located at the median. The height of the box is equal to the interquartile distance, or IQD, which is the difference between the third quartile of the data and the first quartile. The whiskers (the dotted lines extending from the top and bottom of the box) extend to the extreme values or a distance from the center, whichever is less. Data points which fall outside the whiskers are indicated by horizontal lines.</p><p>The corresponding means and medians are shown in Tables <ref type="table" target="#tab_1">II</ref> and<ref type="table" target="#tab_2">III</ref>. A pairwise comparison of the generalization performance for different objective functions, on a test by test basis, by using the sign test <ref type="bibr" target="#b68">[69]</ref> at a 95% level of significance, is also reported in Tables IV-XIII. Each method (A, say) in the row is compared with each method (B, say) in the column. An asterisk in the corresponding entry indicates that the generalization performance of method A is significantly better than that of B. The following observations can be made.</p><p>• Poor results are obtained for the harmonic function using all the objective functions tested. This is, however, in line with the results by Donoho and Johnstone <ref type="bibr" target="#b69">[70]</ref>.</p><p>They studied the least-square approximation errors of ridge approximation and kernel approximation, which are approximation techniques analogous to feedforward neural networks with a single hidden layer of sigmoidal units and a single hidden layer of radial basis function units, respectively. Focusing on the two-dimensional input space with respect to the Gaussian measure, they showed that ridge approximation is better for radial functions while kernel approximation is better for harmonic functions. Hence, in this case, networks of size larger than 15 may be needed for successive approximation and results for this problem are not conclusive. • The testing performance of is poor for all the problems. But it can be significantly improved by using the modified version . Similar improvement also holds for and . This shows that transforming the objective functions as discussed in Section IV-B1 is beneficial.</p><p>• For the noiseless data, has the smallest testing FVU's for the simple interaction function and the radial function , and the second smallest for the additive function . Its superiority over other objective functions is also confirmed by the pairwise comparison (Tables IV, V, and VII). However, it is a bit inferior for the complicated interaction function . From Fig. <ref type="figure" target="#fig_6">8</ref>(e), one can see a number of outliers with high testing FVU's for . Moreover, the spreads of the testing FVU's for are also comparatively larger than the other objective functions in most datasets. This probably indicates that superiority of is marred by its sensitivity to the particular set of training data used.</p><p>• The testing performance of is much better than , and is generally better than . Except for the complicated interaction function is also better than . Hence, the claim in Section II on the difference in convergence rate is consistent with our experimental results.</p><p>• The superiority of over , and over also supports the claim in Section II that ignoring the bias to the output unit during input training is beneficial. • has competitive performance only for the simple interaction function, and is significantly worse than most other objective functions for all other functions. This is probably because of the complicated form of , making it hard to be optimized. The performance of is also poor, as is expected following the discussion in Section II, suggesting that optimization based on has to be repeated several times for satisfactory results, as has been done in <ref type="bibr" target="#b39">[40]</ref>.</p><p>• and have comparable testing performance. Moreover, the spreads of the testing FVU's are also comparable.</p><p>• For the noisy data, differences in performance between different objective functions become smaller. But, still and stand out among the others, as is evident from the pairwise comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, we study a number of objective functions for training new hidden units in constructive neural-network learning algorithms. The aim is to derive a class of objective functions whose value and the corresponding weight updates can be computed in time, where is the number of training patterns. Moreover, even though input weight freezing  (1) . (b) f (2) . (c) f (3) . (d) f (4) . (e) f (5)   is used in the constructive algorithm for computational efficiency, we require that the convergence property be preserved. This class of objective functions includes:</p><formula xml:id="formula_4">• • • • • • • .</formula><p>The results here are not tied to a particular network architecture (the cascade-correlation architecture), and are useful to situations when complete retraining of the whole network is infeasible after hidden unit addition. Moreover, the constructive algorithm can easily be generalized to adding a group of hidden units, instead of just one, to the network simultaneously, by requiring this group to optimize these same objective functions.</p><p>Besides, we also propose a few computational tricks that can be used to improve the optimization of these objective functions. Specifically, we address the problem of slow convergence in plateaus of the objective function, which is caused by both the form of the objective function and the quickprop algorithm. A simple transformation of the objective function to be optimized, although theoretically identical to the original one, may lead to significantly improved results in the numerical optimization process. Handling of plateaus in quickprop is also modified, which is especially important for constructive neural-network learning algorithms using the patience parameter.</p><p>In general, the generalization performance of networks using different objective functions can be influenced by a number of factors. First, most constructive algorithms (such as those studied in this paper) use a greedy approach, by grabbing the largest possible residual error every time a new hidden unit is added. However, being the nature of any greedy approach, even if the "best" hidden unit function, as judged by the "best" objective function, is selected at each step, this does  Second, the objective functions used here are sample versions of their theoretical counterparts, basing on information available from the training set. Their accuracy compared to the true population version is thus dependent on whether the training set has sufficiently sampled the whole input space, and whether the noise in the training data has severely corrupted its true value, etc. In the latter case when large errors are to be expected, it may be reasonable to use robust versions of the objective functions <ref type="bibr" target="#b70">[71]</ref>. Also, in some constructive algorithms (such as the cascade-correlation architecture <ref type="bibr" target="#b38">[39]</ref>), the number of input weights to the new hidden unit keeps on increasing. In this case, regularization methods that add a penalty term to the original objective function may be beneficial.</p><p>Third, practical optimization algorithms are prone to the local optimum problem. This plagues all objective functions being investigated here, but some are more vulnerable than the others, probably because of their complex functional form. Algorithms for finding global optima, such as simulated annealing, may of course be used, but the demand on computational resources will be much higher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Proofs</head><p>Proposition 1: For a fixed , the expression achieves its minimum iff  (1) . HERE, sc STANDS FOR S cascor ; s f FOR S fujita AND s s FOR S sqr TABLE V NOISELESS f (2)   Moreover, with and as defined above, , iff</p><p>Proof: Consider the case when all elements in are of unit norm For a fixed , the stated expression is minimized iff with From the set , the stated expression is minimized when is maximized over all . The result follows when the assumption of unit norm is dropped.</p><p>Theorem 1: Given is dense in and for some . If is selected as to maximize , then .    (5)   Proof: Let denote the version of before output training <ref type="bibr" target="#b19">(20)</ref> TABLE IX NOISY f (1)   TABLE X NOISY f (2)   TABLE XI NOISY f (3)   which is greater than zero as is dense (Lemma 3.3-7 of <ref type="bibr" target="#b71">[72]</ref>). So, is strictly decreasing, and bounded below by zero, thus it converges. That is, such that when   (5)   As is orthogonal to (Fig. <ref type="figure" target="#fig_5">7</ref>), therefore i.e., is a Cauchy sequence. Because is complete, such that . From the fact that maximizes (20) and , therefore</p><p>As strong convergence implies weak convergence, hence , which implies . Proposition 2: If the target function is an exact summation of basis functions from as in <ref type="bibr" target="#b4">(5)</ref>, and the network is constructed as in Theorem 1, then where .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>(</head><label></label><figDesc>Proofs are in Appendix A.) Thus, Proposition 1 suggests that the objective function to be optimized during input training is<ref type="bibr" target="#b2">(3)</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Plot of several choices of f .</figDesc><graphic coords="8,41.58,59.58,253.92,220.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. f (3) : harmonic.</figDesc><graphic coords="9,44.64,59.58,247.92,189.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. f (4) : additive.</figDesc><graphic coords="9,42.12,285.24,252.96,183.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. f(5) : complicated interaction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Illustration for Theorem 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Fig.8. Comparison of the logarithm of the testing FVU's on noiseless data. Here, s1 stands for S 1 , rs1 for p S 1 , s2 for S 2 , rs2 for p S 2 , s3 for S 3 , rs3 for p S 3 , fahlman for S cascor , fujita for S fujita , and sqr for S sqr (a) f<ref type="bibr" target="#b0">(1)</ref> . (b) f<ref type="bibr" target="#b1">(2)</ref> . (c) f (3) . (d) f<ref type="bibr" target="#b3">(4)</ref> . (e) f<ref type="bibr" target="#b4">(5)</ref> .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Fig.9. Comparison of the logarithm of the testing FVU's on noisy data. (a) f(1) . (b) f(2) . (c) f(3) . (d) f(4) . (e) f(5)   </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I MEAN</head><label>I</label><figDesc>SIGNAL-TO-NOISE RATIOS FOR THE FIVE REGRESSION PROBLEMS</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II COMPARISON</head><label>II</label><figDesc>OF MEAN TESTING FVU's IN 100 TRIALS ON NOISELESS TRAINING SETS (MEDIANS ARE IN BRACKETS)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III COMPARISON</head><label>III</label><figDesc>OF MEAN TESTING FVU's ON NOISY TRAINING SETS not necessarily guarantee that the final network will have the best performance.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV NOISELESS f</head><label>IVf</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VII NOISELESS</head><label>VII</label><figDesc></figDesc><table /><note><p><p>f</p>(4)   </p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VIII NOISELESS f</head><label>VIIIf</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE XIII NOISY f</head><label>XIIIf</label><figDesc></figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The smallest possible network to start with has no hidden units. If prior knowledge of the problem is available, an alternative initial state may be supplied by the user.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>As an example, MacKay<ref type="bibr" target="#b13">[14]</ref> reported that the Gaussian approximation in the evidence framework seemed to break down significantly for N=k &lt;</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p>361,where N is the number of training patterns and k is the number of weights in the network.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3"><p>A sequence ff n g strongly converges<ref type="bibr" target="#b33">[34]</ref> to f if lim n!1 kf 0f n k = 0, where k 1 k is the norm for the function space being considered.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>kgk = 0 implies g(x) 0 8x 2 X, which should obviously be excluded.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5"><p>Consistency follows as the estimate is a continuous function of the sample product moments.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6"><p>A semi inner product differs from an inner product in that hu; ui may be zero even when u 6 = 0.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7"><p>A plateau is a region where the first and second derivatives of the function to be optimized with respect to all the parameters are nearly zero.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Proof: The proof follows that of <ref type="bibr" target="#b57">[58]</ref>. Assume that the target function where . Consider as is orthogonal to . But is also equal to <ref type="bibr" target="#b20">(21)</ref> Hence, at least one of the terms in the summation of ( <ref type="formula">21</ref>) must be greater than or equal to , i.e., Hence, from <ref type="bibr" target="#b19">(20)</ref> in Theorem 1, we have <ref type="bibr" target="#b21">(22)</ref> Substituting , <ref type="bibr" target="#b21">(22)</ref> becomes</p><p>By induction, we have </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Nonlinear Output Units</head><p>In this section, we consider the case when the output unit transfer function is nonlinear. Nonlinear output units are commonly used in pattern classification tasks to restrict output values to ranges such as [0,1] or <ref type="bibr" target="#b0">[ 1,</ref><ref type="bibr" target="#b0">1]</ref>. In the following, we denote the function implemented by the network as , where is Fréchet differentiable. 11 The corresponding residual error function becomes . Proposition 4: For a fixed with , the expression achieves its minimum iff <ref type="bibr" target="#b22">(23)</ref> under first-order approximation using Taylor series expansion. Moreover, with as defined in (23) 11 Given X and Y are normed spaces. Suppose f : X ! Y is defined on a neighborhood of a 2 X. We say f (h) = o(h) if kf(h)k=khk ! 0 as h ! 0. A continuous linear operator L : X ! Y is said to be the Fréchet derivative <ref type="bibr" target="#b72">[73]</ref> </p><p>as h ! 0. We write L = f 0 (x).</p><p>iff Proof: Consider the case when all satisfy the condition <ref type="bibr" target="#b23">(24)</ref> Ignoring higher-order terms in Taylor series expansion,</p><p>. By linearity of and ( <ref type="formula">24</ref>), therefore Hence, for a fixed , the stated expression is minimized (subject to the approximation in truncating the full Taylor series) iff with From the set , the stated expression is minimized when is maximized over all . Result follows by removing the restriction <ref type="bibr" target="#b23">(24)</ref>.</p><p>Next, we compute the Fréchet derivative . Let be the output unit transfer function. Then , and Thus, , and the corresponding objective function to be maximized for nonlinear output units is where is the derivative of the output unit transfer function for pattern . In the special case when the output unit transfer function is linear, , Proposition 4 reduces to Proposition 1, and reduces to . However, unlike the case for linear output, convergence proof cannot be derived because approximation by Taylor series expansion is used.</p><p>Tin-Yau Kwok received the Ph.D. degree in computer science from the Hong Kong University of Science and Technology.</p><p>He is currently an Assistant Professor in the Hong Kong Baptist University. His research interests include the theory and applications of artificial neural networks, pattern recognition, machine learning, and data mining. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pruning algorithms-A survey</title>
		<author>
			<persName><forename type="first">R</forename><surname>Reed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="740" to="747" />
			<date type="published" when="1993-09">Sept. 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Comparative bibliography of ontogenic neural networks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Fiesler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Artificial Neural Networks</title>
		<meeting>Int. Conf. Artificial Neural Networks<address><addrLine>Sorrento, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994-05">May 1994</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="793" to="796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Constructive algorithms for structure learning in feedforward neural networks for regression problems</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Y</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1168" to="1183" />
			<date type="published" when="1996-09">Sept. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A taxonomy of neural-network optimality</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Rogers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Nat</title>
		<meeting>IEEE Nat<address><addrLine>Dayton, OH</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992-05">May 1992</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="894" to="899" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A backpropagation algorithm with optimal use of hidden units</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chauvin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Touretzky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 1</title>
		<meeting><address><addrLine>San Mateo, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="519" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Comparing biases for minimal network construction with backpropagation</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Hanson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Y</forename><surname>Pratt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Touretzky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 1</title>
		<meeting><address><addrLine>San Mateo, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="177" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generalization by weight-elimination with application to forecasting</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Weigend</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Huberman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Lippmann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Moody</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Touretzky</surname></persName>
		</editor>
		<meeting><address><addrLine>San Mateo, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="875" to="882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generalized cross-validation as a method for choosing a good ridge parameter</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Heath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wahba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="215" to="223" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bayesian backpropagation</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Buntine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Weigend</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Complex Syst</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="603" to="643" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bayesian interpolation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J C</forename><surname>Mackay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computa</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="415" to="447" />
			<date type="published" when="1992-05">May 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bayesian learning for neural networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dept. Comput. Sci., Univ. Toronto</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<pubPlace>Ont., Canada</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A review of Bayesian neural networks with an application to near infrared spectroscopy</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Thodberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="56" to="72" />
			<date type="published" when="1996-01">Jan. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bayesian regularization and pruning using a Laplace prior</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computa</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="117" to="143" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A practical Bayesian framework for backpropagation networks</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J C</forename><surname>Mackay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computa</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="448" to="472" />
			<date type="published" when="1992-05">May 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">GAL: Networks that grow when they learn and shrink when they forget</title>
		<author>
			<persName><forename type="first">E</forename><surname>Alpaydin</surname></persName>
		</author>
		<idno>TR 91-032</idno>
	</analytic>
	<monogr>
		<title level="j">Int. Comput. Sci. Instit</title>
		<imprint>
			<date type="published" when="1991-05">May 1991</date>
			<pubPlace>Berkeley, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neural units recruitment algorithm for generation of decision trees</title>
		<author>
			<persName><forename type="first">G</forename><surname>Deffuant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1990 IEEE Int. Joint Conf. Neural Networks</title>
		<meeting>1990 IEEE Int. Joint Conf. Neural Networks<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990-06">June 1990</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="637" to="642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The upstart algorithm: A method for constructing and training feedforward neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Frean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computa</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="198" to="209" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A convergence theorem for sequential learning in two-layer perceptrons</title>
		<author>
			<persName><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Golea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ruján</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Europhys. Lett</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="487" to="492" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An overview of predictive learning and function approximation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">From Statistics to Neural Networks: Theory and Pattern Recognition Applications</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wechsler</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A new look at the statistical model identification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Akaike</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Automat. Contr</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="716" to="723" />
			<date type="published" when="1974-12">Dec. 1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Predicted squared error: A criterion for automatic model selection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Barron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Self-Organizing Methods in Modeling, S. Farlow</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Marcel Dekker</publisher>
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Smoothing noisy data with spline functions: Estimating the correct degree of smoothing by the method of generalized cross-validation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Craven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wahba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Numer. Math</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="377" to="403" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Note on generalization, regularization, and architecture selection in nonlinear learning systems</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Moody</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1991 IEEE Wkshp. Neural Networks for Signal Processing</title>
		<editor>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Juang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Kung</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Kamm</surname></persName>
		</editor>
		<meeting>1991 IEEE Wkshp. Neural Networks for Signal essing<address><addrLine>Princeton, NJ</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991-09">Sept. 1991</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Modeling by shortest data description</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rissanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="465" to="471" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Estimating the dimension of a model</title>
		<author>
			<persName><forename type="first">G</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Statist</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="461" to="464" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Prediction risk and architecture selection for neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Moody</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">From Statistics to Neural Networks-Theory and Pattern Recognition Applications</title>
		<title level="s">NATO ASI Series F</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Cherkassky</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wechsler</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">136</biblScope>
			<biblScope unit="page" from="147" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cross-validatory choice and assessment of statistical predictions (with discussion)</title>
		<author>
			<persName><forename type="first">M</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Roy. Statist. Soc. Series B</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="111" to="147" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Evaluating neural-network predictors by bootstrapping</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Weigend</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lebaron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Neural Inform. Processing</title>
		<meeting>Int. Conf. Neural Inform. essing<address><addrLine>Seoul, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994-10">Oct. 1994</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1207" to="1212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Choosing network complexity</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Ripley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Probabilistic Reasoning and Bayesian Belief Networks</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Gammerman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ed</forename><forename type="middle">Alfred</forename><surname>Waller</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="97" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Statistical ideas for selecting network architectures</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Ripley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Artificial Intelligence and Industrial Applications</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Kappen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Gielen</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="183" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dynamic node creation in backpropagation networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Connection Sci</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="365" to="375" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Backpropagation algorithm which varies the number of hidden units</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hirose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yamashita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hijiya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="61" to="66" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Faster learning variations on backpropagation: An empirical study</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Fahlman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1988 Connectionist Models Summer School</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Touretzky</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</editor>
		<meeting>1988 Connectionist Models Summer School<address><addrLine>San Mateo, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1988">1988</date>
			<biblScope unit="page" from="38" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V</forename><surname>Fomin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Introductory Real Analysis</title>
		<imprint>
			<date type="published" when="1975">1975</date>
			<publisher>Dover</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Approximation by superpositions of a sigmoidal function</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cybenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Contr., Signials, and Syst</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="303" to="314" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Some new results on neural-network approximation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1069" to="1072" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Layered neural networks with Gaussian hidden units as universal approximations</title>
		<author>
			<persName><forename type="first">E</forename><surname>Hartman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Keeler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kowalski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computa</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="210" to="215" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Approximation and radial basis function networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Sandberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computa</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="305" to="316" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The cascade-correlation learning architecture</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Fahlman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lebiere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Touretzky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 2</title>
		<meeting><address><addrLine>San Mateo, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="524" to="532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Regression modeling in backpropagation and projection pursuit learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Lay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maechler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schimert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="342" to="353" />
			<date type="published" when="1994-05">May 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Cascade LLM networks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Littmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Artificial Neural Networks</title>
		<meeting>Int. Conf. Artificial Neural Networks<address><addrLine>Brighton, U.K.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992-09">Sept. 1992</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="253" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Approximation, dimension reduction, and nonconvex optimization using linear superpositions of Gaussians</title>
		<author>
			<persName><forename type="first">A</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Comput</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1222" to="1233" />
			<date type="published" when="1993-10">Oct. 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Some approximation properties of projection pursuit learning networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Atkeson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Moody</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Hanson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Lippmann</surname></persName>
		</editor>
		<meeting><address><addrLine>San Mateo, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="936" to="943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Forecasting demand for electric power</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Fine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Hanson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Cowan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</editor>
		<meeting><address><addrLine>San Mateo, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="739" to="746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Exploratory projection pursuit</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">397</biblScope>
			<biblScope unit="page" from="249" to="266" />
			<date type="published" when="1987-03">Mar. 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Optimization of the hidden unit function in feedforward neural networks</title>
		<author>
			<persName><forename type="first">O</forename><surname>Fujita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="755" to="764" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">What size net gives valid generalization?</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Baum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Haussler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Touretzky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 1</title>
		<meeting><address><addrLine>San Mateo, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="81" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A convergent generator of neural networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Courrieu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="835" to="844" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Approximation capabilities of multilayer feedforward networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="251" to="257" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A bias architecture with rank-expanding algorithm for neural networks supervised learning problem</title>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. World Congr</title>
		<meeting>World Congr<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994-06">June 1994</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="742" to="747" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A comparison of neural-network and fuzzy clustering techniques in segmenting magnetic resonance images of the brain</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">O</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Bensaid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Velthuizen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Silbiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Bezdek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="672" to="682" />
			<date type="published" when="1992-09">Sept. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Prediction of software reliability using feedforward and recurrent neural nets</title>
		<author>
			<persName><forename type="first">N</forename><surname>Karunanithi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Whitley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf. Neural Networks</title>
		<meeting>Int. Joint Conf. Neural Networks<address><addrLine>Baltimore, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992-06">June 1992</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="800" to="805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Cascade-correlation neural networks for the classification of cervical cells</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Mckenna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Ricketts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Cairns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Hussein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Inst. Elec. Eng. Colloquium on Neural Networks for Image Processing Applicat</title>
		<meeting><address><addrLine>London, U.K.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992-10">Oct. 1992</date>
			<biblScope unit="page" from="5" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Variations on the cascadecorrelation learning architecture for fast convergence in robot control</title>
		<author>
			<persName><forename type="first">N</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Corporaal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kerckhoffs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th Int. Conf. Neural Networks Applicat</title>
		<meeting>5th Int. Conf. Neural Networks Applicat<address><addrLine>Nimes, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992-11">Nov. 1992</date>
			<biblScope unit="page" from="455" to="464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A cascade-correlation model of balance scale phenomena</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Shultz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">C</forename><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 13th Annu</title>
		<meeting>13th Annu<address><addrLine>Hillsdale, NJ</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="635" to="640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Linear and Nonlinear Programming</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Luenberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Addison-Wesley</publisher>
			<biblScope unit="volume">2</biblScope>
			<pubPlace>Reading, MA</pubPlace>
		</imprint>
	</monogr>
	<note>nd ed.</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Universal approximation using radial basis function networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sandberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computa</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="246" to="257" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Convergence properties of cascade correlation in function approximation</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">P</forename><surname>Drago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ridella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput. Applicat</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="142" to="147" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Universal approximation bounds for superpositions of a sigmoidal function</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Barron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="930" to="945" />
			<date type="published" when="1993-05">May 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A simple lemma on greedy approximation in Hilbert space and convergence rates for projection pursuit regression and neuralnetwork training</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Statist</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="608" to="613" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">On a conjecture of Huber concerning the convergence of projection pursuit regression</title>
	</analytic>
	<monogr>
		<title level="j">Ann. Statist</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="880" to="882" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Rate of approximation results motivated by robust neural-network learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Darken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gurvits</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sontag</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994-04">Apr. 1994</date>
			<publisher>Siemens Corporate Res., Inc</publisher>
			<pubPlace>Princeton, NJ</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Incremental approximation by onehidden-layer neural networks</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kurková</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Beliczynski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Artificial Neural Networks</title>
		<meeting>Int. Conf. Artificial Neural Networks<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995-10">Oct. 1995</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="505" to="510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Optimization schemes for neural networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Jervis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Fitzgerald</surname></persName>
		</author>
		<idno>CUED/F- INFENG/TR 144</idno>
	</analytic>
	<monogr>
		<title level="j">Cambridge Univ. Eng. Dept., U.K</title>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Experimental analysis of aspects of the cascade-correlation learning architecture</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Squires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Shavlik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Sci. Dept., Univ. Wisconsin-Madison</title>
		<imprint>
			<biblScope unit="page" from="91" to="91" />
			<date type="published" when="1991">1991</date>
			<pubPlace>Madison, WI</pubPlace>
		</imprint>
	</monogr>
	<note>Machine Learning Res. Group Working Paper</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Cascor.c, C implementation of the cascade-correlation learning algorithm</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Crowder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Increased rates of convergence through learning rate adaptation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Constructive neural networks: Some practical considerations</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Y</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pro. IEEE Int. Conf. Neural Networks</title>
		<meeting><address><addrLine>Orlando, FL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994-06">June 1994</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="198" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">K</forename><surname>Rohatgi</surname></persName>
		</author>
		<title level="m">Statistical Inference</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Projection-based approximation and a duality with kernel methods</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Johnstone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Statist</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="58" to="106" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Huber</surname></persName>
		</author>
		<title level="m">Robust Statistics</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Kreyszig</surname></persName>
		</author>
		<title level="m">Introductory Functional Analysis with Applications</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Griffel</surname></persName>
		</author>
		<title level="m">Applied Functional Analysis</title>
		<meeting><address><addrLine>Chichester, U.K.</addrLine></address></meeting>
		<imprint>
			<publisher>Ellis Horwood</publisher>
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
