<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FLAT: Chinese NER Using Flat-Lattice Transformer</title>
				<funder ref="#_8MvcVH7 #_mudgpTB">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_ds2ARcq">
					<orgName type="full">National Key Research and Development Program of China</orgName>
				</funder>
				<funder ref="#_sSrWrMj">
					<orgName type="full">Shanghai Municipal Science and Technology Major Project</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-04-24">24 Apr 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiaonan</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Fudan University School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University lixiaonan</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hang</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Fudan University School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University lixiaonan</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
							<email>xpqiu@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Fudan University School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University lixiaonan</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
							<email>xjhuang@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Fudan University School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University lixiaonan</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">FLAT: Chinese NER Using Flat-Lattice Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-04-24">24 Apr 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2004.11795v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, the character-word lattice structure has been proved to be effective for Chinese named entity recognition (NER) by incorporating the word information. However, since the lattice structure is complex and dynamic, most existing lattice-based models are hard to fully utilize the parallel computation of GPUs and usually have a low inference-speed. In this paper, we propose FLAT: Flat-LAttice Transformer for Chinese NER, which converts the lattice structure into a flat structure consisting of spans. Each span corresponds to a character or latent word and its position in the original lattice. With the power of Transformer and well-designed position encoding, FLAT can fully leverage the lattice information and has an excellent parallelization ability. Experiments on four datasets show FLAT outperforms other lexicon-based models in performance and efficiency.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Named entity recognition (NER) plays an indispensable role in many downstream natural language processing (NLP) tasks <ref type="bibr" target="#b1">(Chen et al., 2015;</ref><ref type="bibr" target="#b4">Diefenbach et al., 2018)</ref>. Compared with English NER <ref type="bibr" target="#b9">(Lample et al., 2016;</ref><ref type="bibr" target="#b23">Yang et al., 2017;</ref><ref type="bibr" target="#b12">Liu et al., 2017;</ref><ref type="bibr" target="#b18">Sun et al., 2020)</ref>, Chinese NER is more difficult since it usually involves word segmentation.</p><p>Recently, the lattice structure has been proved to have a great benefit to utilize the word information and avoid the error propagation of word segmentation <ref type="bibr" target="#b25">(Zhang and Yang, 2018)</ref>. We can match a sentence with a lexicon to obtain the latent words in it, and then we get a lattice like in Figure <ref type="figure">1</ref>(a). The lattice is a directed acyclic graph, where each node is a character or a latent word. The lattice includes a sequence of characters and potential Chongqing Figure <ref type="figure">1</ref>: While lattice LSTM indicates lattice structure by dynamically adjusting its structure, FLAT only needs to leverage the span position encoding. In 1(c), , , denotes tokens, heads and tails, respectively.</p><p>words in the sentence. They are not ordered sequentially, and the word's first character and last character determine its position. Some words in lattice may be important for NER. For example, in Figure <ref type="figure">1</ref>(a), "????(Renhe Pharmacy)" can be used to distinguish between the geographic entity "??(Chongqing)" and the organization entity "? ??(Chongqing People)".</p><p>There are two lines of methods to leverage the lattice. (1) One line is to design a model to be compatible with lattice input, such as lattice LSTM <ref type="bibr" target="#b25">(Zhang and Yang, 2018)</ref> and LR-CNN <ref type="bibr">(Gui et al., 2019a)</ref>. In lattice LSTM, an extra word cell is employed to encode the potential words, and attention mechanism is used to fuse variable-number nodes at each position, as in Figure <ref type="figure">1</ref>(b). LR-CNN uses CNN to encode potential words at different window sizes. However, RNN and CNN are hard to model long-distance dependencies <ref type="bibr" target="#b19">(Vaswani et al., 2017)</ref>, which may be useful in NER, such as coreference <ref type="bibr" target="#b16">(Stanislawek et al., 2019)</ref>. Due to the dynamic lattice structure, these methods cannot fully utilize the parallel computation of GPU. (2) Another line is to convert lattice into graph and use a graph neural network (GNN) to encode it, such as Lexicon-based Graph Network (LGN) <ref type="bibr">(Gui et al., 2019b)</ref> and Collaborative Graph Network (CGN) <ref type="bibr" target="#b17">(Sui et al., 2019)</ref>. While sequential structure is still important for NER and graph is general counterpart, their gap is not negligible. These methods need to use LSTM as the bottom encoder to carry the sequential inductive bias, which makes the model complicated.</p><p>In this paper, we propose FLAT: Flat LAttice Transformer for Chinese NER. Transformer <ref type="bibr" target="#b19">(Vaswani et al., 2017)</ref> adopts fully-connected selfattention to model the long-distance dependencies in a sequence. To keep the position information, Transformer introduces the position representation for each token in the sequence. Inspired by the idea of position representation, we design an ingenious position encoding for the lattice-structure, as shown in Figure <ref type="figure">1</ref>(c). In detail, we assign two positional indices for a token (character or word): head position and tail position, by which we can reconstruct a lattice from a set of tokens. Thus, we can directly use Transformer to fully model the lattice input. The self-attention mechanism of Transformer enables characters to directly interact with any potential word, including self-matched words. To a character, its self-matched words denote words which include it. For example, in Figure <ref type="figure">1</ref>(a), self-matched words of "? (Drug)" are "??? ?(Renhe Pharmacy)" and "?? (Pharmacy)" <ref type="bibr" target="#b17">(Sui et al., 2019)</ref>. Experimental results show our model outperforms other lexicon-based methods on the performance and inference-speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>In this section, we briefly introduce the Transformer architecture. Focusing on the NER task, we only discuss the Transformer encoder. It is composed of self-attention and feedforward network (FFN) layers. Each sublayer is followed by residual connection and layer normalization. FFN is a position-wise multi-layer Perceptron with nonlinear transformation. Transformer performs self- attention over the sequence by H heads of attention individually and then concatenates the result of H heads. For simplicity, we ignore the head index in the following formula. The result of per head is calculated as:</p><formula xml:id="formula_0">0 1 2 3 4 5 0 2 4 -1 0 1 2 3 4 -1 1 3 -2 -1 0 1 2 3 -2 0 2 -3 -2 -1 0 1 2 -3 -1 1 -4 -3 -2 -1 0 1 -4 -2 0 -5 -4 -3 -2 -1 0 -5 -3 -1 0 1 2 3 4 5 0 2 4 -2 -1 0 1 2 3 -2 0 2 -4 -3 -2 -1 0 1 -4 -2 0 Embedding Self-Attention FFN ? (??)</formula><formula xml:id="formula_1">Att(A, V) = softmax(A)V,<label>(1)</label></formula><formula xml:id="formula_2">A ij = Q i K j T ? d head ,<label>(2)</label></formula><formula xml:id="formula_3">[Q, K, V] = Ex[Wq, W k , Wv],<label>(3)</label></formula><p>where E is the token embedding lookup table or the output of last Transformer layer.</p><formula xml:id="formula_4">W q , W k , W v ? R d model ?d head are learnable pa- rameters, and d model = H ? d head , d head is the dimension of each head.</formula><p>The vanilla Transformer also uses absolute position encoding to capture the sequential information. Inspired by <ref type="bibr" target="#b22">Yan et al. (2019)</ref>, we think commutativity of the vector inner dot will cause the loss of directionality in self-attention. Therefore, we consider the relative position of lattice also significant for NER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Converting Lattice into Flat Structure</head><p>After getting a lattice from characters with a lexicon, we can flatten it into flat counterpart. The flat-lattice can be defined as a set of spans, and a span corresponds to a token, a head and a tail, like in Figure <ref type="figure">1(c</ref>). The token is a character or word. The head and tail denote the position index of the token's first and last characters in the original sequence, and they indicate the position of the token in the lattice. For the character, its head and tail are the same. There is a simple algorithm to recover flat-lattice into its original structure. We can first take the token which has the same head and tail, to construct the character sequence. Then we use other tokens (words) with their heads and tails to build skip-paths. Since our transformation is recoverable, we assume flat-lattice can maintain the original structure of lattice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Relative Position Encoding of Spans</head><p>The flat-lattice structure consists of spans with different lengths. To encode the interactions among spans, we propose the relative position encoding of spans. For two spans x i and x j in the lattice, there are three kinds of relations between them: intersection, inclusion and separation, determined by their heads and tails. Instead of directly encoding these three kinds of relations, we use a dense vector to model their relations. It is calculated by continuous transformation of the head and tail information. Thus, we think it can not only represent the relation between two tokens, but also indicate more detailed information, such as the distance between a character and a word. Let head[i] and tail[i] denote the head and tail position of span x i . Four kinds of relative distances can be used to indicate the relation between x i and x j . They can be calculated as:</p><formula xml:id="formula_5">d (hh) ij = head[i] -head[j],<label>(4)</label></formula><formula xml:id="formula_6">d (ht) ij = head[i] -tail[j],<label>(5)</label></formula><formula xml:id="formula_7">d (th) ij = tail[i] -head[j],<label>(6)</label></formula><formula xml:id="formula_8">d (tt) ij = tail[i] -tail[j],<label>(7)</label></formula><p>where d</p><p>(hh) ij denotes the distance between head of</p><p>x i and tail of x j , and other d</p><formula xml:id="formula_9">(ht) ij , d (th) ij , d<label>(tt)</label></formula><p>ij have similar meanings. The final relative position encoding of spans is a simple non-linear transformation of the four distances:</p><formula xml:id="formula_10">Rij = ReLU(Wr(p d (hh) ij ? p d (th) ij ? p d (ht) ij ? p d (tt) ij )), (8)</formula><p>where W r is a learnable parameter, ? denotes the concatenation operator, and p d is calculated as in <ref type="bibr" target="#b19">Vaswani et al. (2017)</ref>,</p><formula xml:id="formula_11">p (2k) d = sin d/10000 2k/d model ,<label>(9)</label></formula><formula xml:id="formula_12">p (2k+1) d = cos d/10000 2k/d model , (<label>10</label></formula><formula xml:id="formula_13">)</formula><p>where d is d</p><formula xml:id="formula_14">(hh) ij , d (ht) ij , d (th) ij or d (tt)</formula><p>ij and k denotes the index of dimension of position encoding. Then we use a variant of self-attention <ref type="bibr" target="#b3">(Dai et al., 2019)</ref> to leverage the relative span position encoding as follows:  <ref type="bibr" target="#b25">Zhang and Yang (2018)</ref>. PLT denotes the porous lattice Transformer <ref type="bibr" target="#b13">(Mengge et al., 2019)</ref>. 'YJ' denotes the lexicon released by <ref type="bibr" target="#b25">Zhang and Yang (2018)</ref>, and 'LS' denotes the lexicon released by <ref type="bibr" target="#b11">Li et al. (2018)</ref>. The result of other models are from their original paper.</p><formula xml:id="formula_15">A * i,j = W q E x i Ex j W k,E + W q E x i RijW k,R + u Ex j W k,E + v RijW k,R ,<label>(11)</label></formula><p>Except that the superscript * means the result is not provided in the original paper, and we get the result by running the public source code. Subscripts 'msm' and 'mld' denote FLAT with the mask of self-matched words and long distance (&gt;10), respectively.</p><p>where W q , W k,R , W k,E ? R d model ?d head and u, v ? R d head are learnable parameters. Then we replace A with A * in Eq.( <ref type="formula" target="#formula_1">1</ref>). The following calculation is the same with vanilla Transformer.</p><p>After FLAT, we only take the character representation into output layer, followed by a Condiftional Random Field (CRF) <ref type="bibr" target="#b8">(Lafferty et al., 2001)</ref>   <ref type="bibr" target="#b7">Sun, 2016)</ref>. We show statistics of these datasets in Table <ref type="table" target="#tab_0">1</ref>. We use the same train, dev, test split as <ref type="bibr">Gui et al. (2019b)</ref>. We take BiLSTM-CRF and TENER <ref type="bibr" target="#b22">(Yan et al., 2019)</ref> as baseline models. TENER is a Transformer using relative position encoding for NER, without external information. We also compare FLAT with other lexicon-based methods. The embeddings and lexicons are the same as <ref type="bibr" target="#b25">Zhang and Yang (2018)</ref>. When comparing with CGN <ref type="bibr" target="#b11">(Li et al., 2018)</ref>, we use the same lexicon as CGN. The way to select hyper-parameters can be found in the supplementary material. In particular, we use only one layer Transformer encoder for our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Overall Performance</head><p>As shown in Table <ref type="table" target="#tab_1">2</ref>, our model outperforms baseline models and other lexicon-based models on four Chinese NER datasets. Our model outperforms TENER <ref type="bibr" target="#b22">(Yan et al., 2019</ref>) by 1.72 in average F1 score. For lattice LSTM, our model has an average F1 improvement of 1.51 over it. When using another lexicon <ref type="bibr" target="#b11">(Li et al., 2018)</ref>, our model also outperforms CGN by 0.73 in average F1 score. Maybe due to the characteristic of Transformer, the improvement of FLAT over other lexicon-based models on small datasets is not so significant like that on large datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Advantage of Fully-Connected Structure</head><p>We think self-attention mechanism brings two advantages over lattice LSTM: 1) All characters can directly interact with its self-matched words. 2) Long-distance dependencies can be fully modeled. Due to our model has only one layer, we can strip them by masking corresponding attention. In detail, we mask attention from the character to its self-matched word and attention between tokens whose distance exceeds 10. As shown in Table <ref type="table" target="#tab_1">2</ref>, the first mask brings a significant deterioration to FLAT while the second degrades performance slightly. As a result, we think leveraging information of self-matched words is important For Chinese NER. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Efficiency of FLAT</head><p>To verify the computation efficiency of our model, we compare the inference-speed of different lexicon-based models on Ontonotes. The result is shown in Figure <ref type="figure" target="#fig_2">3</ref>. GNN-based models outperform lattice LSTM and LR-CNN. But the RNN encoder of GNN-based models also degrades their speed. Because our model has no recurrent module and can fully leverage parallel computation of GPU, it outperforms other methods in running efficiency. In terms of leveraging batch-parallelism, the speedup ratio brought by batch-parallelism is 4.97 for FLAT, 2.1 for lattice LSTM, when batch size = 16. Due to the simplicity of our model, it can benefit from batch-parallelism more significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">How FLAT Brings Improvement</head><p>Compared with TENER, FLAT leverages lexicon resources and uses a new position encoding. To probe how these two factors bring improvement. We set two new metrics, 1) Span F: while the common F score used in NER considers correctness of both the span and the entity type, Span F only considers the former. 2) Type Acc: proportion of full-correct predictions to span-correct predictions.</p><p>Table <ref type="table" target="#tab_3">3</ref> shows two metrics of three models on the devlopment set of Ontonotes and MSRA. We can find: 1) FLAT outperforms TENER in two metrics significantly.</p><p>2) The improvement on Span F brought by FLAT is more significant than that on Type Acc. 3) Compared to FLAT, FLAT head 's deterioration on Span F is more significant than that on Type Acc. These show: 1) The new position encoding helps FLAT locate entities more accurately.</p><p>2) The pre-trained word-level embedding Comparision between BERT and BERT+FLAT. 'BERT' refers to the BERT+MLP+CRF architecture. 'FLAT+BERT' refers to FLAT using BERT embedding. We finetune BERT in both models during training. The BERT in the experiment is 'BERT-wwm' released by <ref type="bibr" target="#b2">Cui et al. (2019)</ref>. We use it by the BERTEmbedding in fastNLP 1 . makes FLAT more powerful in entity classification <ref type="bibr" target="#b0">(Agarwal et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Compatibility with BERT</head><p>We also compare FLAT equipped with BERT with common BERT+CRF tagger on four datasets, and Results are shown in Table <ref type="table" target="#tab_4">4</ref>. We find that, for large datasets like Ontonotes and MSRA, FLAT+BERT can have a significant improvement over BERT. But for small datasets like Resume and Weibo, the improvement of FLAT+BERT over BERT is marginal.</p><p>5 Related Work 5.1 Lexicon-based NER <ref type="bibr" target="#b25">Zhang and Yang (2018)</ref> introduced a lattice LSTM to encode all characters and potential words recognized by a lexicon in a sentence, avoiding the error propagation of segmentation while leveraging the word information. <ref type="bibr">Gui et al. (2019a)</ref> exploited a combination of CNN and rethinking mechanism to encode character sequence and potential words at different window sizes. Both models above suffer from the low inference efficiency and are hard to model long-distance dependencies. <ref type="bibr">Gui et al. (2019b)</ref> and <ref type="bibr" target="#b17">Sui et al. (2019)</ref> leveraged a lexicon and character sequence to construct graph, converting NER into a node classification task. However, due to NER's strong alignment of label and input, their model needs an RNN module for encoding. The main difference between our model and models above is that they modify the model structure according to the lattice, while we use a well-designed position encoding to indicate the lattice structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Lattice-based Transformer</head><p>For lattice-based Transformer, it has been used in speech translation and Chinese-source translation. The main difference between them is the way to 1 https://github.com/fastnlp/fastNLP indicate lattice structure. In Chinese-source translation, <ref type="bibr" target="#b21">Xiao et al. (2019)</ref> take the absolute position of nodes' first characters and the relation between each pair of nodes as the structure information. In speech translation, <ref type="bibr" target="#b15">Sperber et al. (2019)</ref> used the longest distance to the start node to indicate lattice structure, and <ref type="bibr" target="#b24">Zhang et al. (2019)</ref> used the shortest distance between two nodes. Our span position encoding is more natural, and can be mapped to all the three ways, but not vise versa. Because NER is more sensitive to position information than translation, our model is more suitable for NER. Recently, Porous Lattice Transformer <ref type="bibr" target="#b13">(Mengge et al., 2019)</ref> is proposed for Chinese NER. The main difference between FLAT and Porus Lattice Transformer is the way of representing position information. We use 'head' and 'tail' to represent the token's position in the lattice. They use 'head', tokens' relative relation (not distance) and an extra GRU. They also use 'porous' technique to limit the attention distribution. In their model, the position information is not recoverable because 'head' and relative relation can cause position information loss. Briefly, relative distance carries more information than relative relation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>In this paper, we introduce a flat-lattice Transformer to incorporate lexicon information for Chinese NER. The core of our model is converting lattice structure into a set of spans and introducing the specific position encoding. Experimental results show our model outperforms other lexiconbased models in the performance and efficiency. We leave adjusting our model to different kinds of lattice or graph as our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Appendices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Hyperparameters Selection</head><p>For MSRA and Ontonotes these two large datasets, we select the hyper-parameters based on the development experiment of Ontonotes. For two small datasets, Resume and Weibo, we find their optimal hyper-parameters by random-search. The Table <ref type="table" target="#tab_6">5</ref> lists the hyper-parameters obtained from the development experiment of Ontonotes.</p><p>The Table <ref type="table" target="#tab_7">6</ref> lists the range of hyper-parameters random-search for Weibo, Resume datasets. For the hyper-parameters which do not appear in it, they are the same as in Table <ref type="table" target="#tab_6">5</ref>   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The overall architecture of FLAT.</figDesc><graphic url="image-4.png" coords="2,401.21,76.43,81.98,81.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Inference-speed of different models, compared with lattice LSTM ?. ? denotes non-batchparallel version, and ? indicates the model is run in 16 batch size parallelly. For model LR-CNN, we do not get its batch-parallel version.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of four datasets. 'Train' is the size of training set. 'Char avg ', 'Word avg ', 'Entity avg ' are the average number of chars, words mateched by lexicon and entities in an instance.</figDesc><table><row><cell cols="2">Ontonotes</cell><cell>MSRA</cell><cell>Resume</cell><cell>Weibo</cell></row><row><cell>Train</cell><cell>15740</cell><cell>46675</cell><cell>3821</cell><cell>1350</cell></row><row><cell>Charavg</cell><cell>36.92</cell><cell>45.87</cell><cell>32.15</cell><cell>54.37</cell></row><row><cell>Wordavg</cell><cell>17.59</cell><cell>22.38</cell><cell>24.99</cell><cell>21.49</cell></row><row><cell>Entityavg</cell><cell>1.15</cell><cell>1.58</cell><cell>3.48</cell><cell>1.42</cell></row><row><cell></cell><cell cols="4">Lexicon Ontonotes MSRA Resume Weibo</cell></row><row><cell>BiLSTM</cell><cell>-</cell><cell>71.81</cell><cell cols="2">91.87 94.41 56.75</cell></row><row><cell>TENER</cell><cell>-</cell><cell>72.82</cell><cell cols="2">93.01 95.25 58.39</cell></row><row><cell>Lattice LSTM</cell><cell>YJ</cell><cell>73.88</cell><cell cols="2">93.18 94.46 58.79</cell></row><row><cell>CNNR</cell><cell>YJ</cell><cell>74.45</cell><cell cols="2">93.71 95.11 59.92</cell></row><row><cell>LGN</cell><cell>YJ</cell><cell>74.85</cell><cell cols="2">93.63 95.41 60.15</cell></row><row><cell>PLT</cell><cell>YJ</cell><cell>74.60</cell><cell cols="2">93.26 95.40 59.92</cell></row><row><cell>FLAT</cell><cell>YJ</cell><cell>76.45</cell><cell cols="2">94.12 95.45 60.32</cell></row><row><cell>FLATmsm</cell><cell>YJ</cell><cell>73.39</cell><cell cols="2">93.11 95.03 57.98</cell></row><row><cell>FLAT mld</cell><cell>YJ</cell><cell>75.35</cell><cell cols="2">93.83 95.28 59.63</cell></row><row><cell>CGN</cell><cell>LS</cell><cell>74.79</cell><cell cols="2">93.47 94.12  *  63.09</cell></row><row><cell>FLAT</cell><cell>LS</cell><cell>75.70</cell><cell cols="2">94.35 94.93 63.42</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Four datasets results (F1). BiLSTM results are from</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>.</figDesc><table><row><cell></cell><cell cols="2">Span F</cell><cell cols="2">Type Acc</cell></row><row><cell></cell><cell cols="4">Ontonotes MSRA Ontonotes MSRA</cell></row><row><cell>TENER</cell><cell>72.41</cell><cell cols="2">93.17 96.33</cell><cell>99.29</cell></row><row><cell>FLAT</cell><cell>76.23</cell><cell cols="2">94.58 97.03</cell><cell>99.52</cell></row><row><cell>FLAT head</cell><cell>75.64</cell><cell cols="2">94.33 96.85</cell><cell>99.45</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Two metrics of models. FLAT head means R ij in (11) is replaced by d</figDesc><table><row><cell>(hh) ij .</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell></cell><cell cols="3">Lexicon Ontonotes MSRA Resume Weibo</cell></row><row><cell>BERT</cell><cell>-</cell><cell>80.14</cell><cell>94.95 95.53 68.20</cell></row><row><cell>BERT+FLAT</cell><cell>YJ</cell><cell>81.82</cell><cell>96.09 95.86 68.55</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>.</figDesc><table><row><cell>batch</cell><cell>10</cell></row><row><cell>lr</cell><cell>1e-3</cell></row><row><cell>-decay</cell><cell>0.05</cell></row><row><cell>optimizer</cell><cell>SGD</cell></row><row><cell>-momentum</cell><cell>0.9</cell></row><row><cell>d m odel</cell><cell>160</cell></row><row><cell>head</cell><cell>8</cell></row><row><cell>FFN size</cell><cell>480</cell></row><row><cell cols="2">embed dropout 0.5</cell></row><row><cell cols="2">output dropout 0.3</cell></row><row><cell>warmup</cell><cell>10 (epoch)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Hyper-parameters for Ontonotes and MSRA.</figDesc><table><row><cell>batch</cell><cell>[8,10]</cell></row><row><cell>lr</cell><cell>[1e-3, 8e-4]</cell></row><row><cell>d h ead</cell><cell>[16,20]</cell></row><row><cell>head</cell><cell>[4,8,12]</cell></row><row><cell cols="2">warmup [1, 5, 10] (epoch)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>The range of hyper-parameters random-search for Weibo, Resume datasets.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We thank anonymous reviewers for their responsible attitude and helpful comments. We thank <rs type="person">Tianxiang Sun</rs>, <rs type="person">Yunfan Shao</rs> and <rs type="person">Lei Li</rs> for their help, such as drawing skill sharing, pre-reviewing, etc. This work is supported by the <rs type="funder">National Key Research and Development Program of China</rs> (No. <rs type="grantNumber">2018YFC0831103</rs>), <rs type="funder">National Natural Science Foundation of China</rs> (No. <rs type="grantNumber">U1936214</rs> and <rs type="grantNumber">61672162</rs>), <rs type="funder">Shanghai Municipal Science and Technology Major Project</rs> (No. <rs type="grantNumber">2018SHZDZX01</rs>) and ZJLab.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_ds2ARcq">
					<idno type="grant-number">2018YFC0831103</idno>
				</org>
				<org type="funding" xml:id="_8MvcVH7">
					<idno type="grant-number">U1936214</idno>
				</org>
				<org type="funding" xml:id="_mudgpTB">
					<idno type="grant-number">61672162</idno>
				</org>
				<org type="funding" xml:id="_sSrWrMj">
					<idno type="grant-number">2018SHZDZX01</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Interpretability analysis for named entity recognition to understand system predictions and how they can improve</title>
		<author>
			<persName><forename type="first">Oshin</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byron</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Event extraction via dynamic multipooling convolutional neural networks</title>
		<author>
			<persName><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P15-1017</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="167" to="176" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Pre-training with whole word masking for chinese BERT</title>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
		<idno>CoRR, abs/1906.08101</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno>CoRR, abs/1901.02860</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Core techniques of question answering systems over knowledge bases: a survey</title>
		<author>
			<persName><forename type="first">Dennis</forename><surname>Diefenbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vanessa</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamal</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Maret</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10115-017-1100-y</idno>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="529" to="569" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cnn-based chinese ner with lexicon rethinking</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruotian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lujun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Joint Conference on Artificial Intelligence, IJCAI&apos;19</title>
		<meeting>the 28th International Joint Conference on Artificial Intelligence, IJCAI&apos;19</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4982" to="4988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A lexicon-based graph neural network for Chinese NER</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yicheng</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlong</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1096</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1039" to="1049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">F-score driven max margin neural network for named entity recognition in chinese social media</title>
		<author>
			<persName><forename type="first">Hangfeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<idno>CoRR, abs/1611.04234</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Machine Learning, ICML &apos;01</title>
		<meeting>the Eighteenth International Conference on Machine Learning, ICML &apos;01<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1030</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="260" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The third international Chinese language processing bakeoff: Word segmentation and named entity recognition</title>
		<author>
			<persName><forename type="first">Gina-Anne</forename><surname>Levow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing</title>
		<meeting>the Fifth SIGHAN Workshop on Chinese Language Processing<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="108" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Analogical reasoning on Chinese morphological and semantic relations</title>
		<author>
			<persName><forename type="first">Shen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renfen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wensi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyong</forename><surname>Du</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-2023</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="138" to="143" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Empower sequence labeling with task-aware neural language model</title>
		<author>
			<persName><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><forename type="middle">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno>CoRR, abs/1709.04109</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Porous latticebased transformer encoder for chinese ner</title>
		<author>
			<persName><forename type="first">Xue</forename><surname>Mengge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Bowen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liu</forename><surname>Tingwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename><surname>Bin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Erli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Quangang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Named entity recognition for Chinese social media with jointly trained embeddings</title>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1064</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="548" to="554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Self-attentional models for lattice inputs</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Sperber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ngoc-Quan</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Waibel</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1115</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1185" to="1197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Named entity recognition -is there a glass ceiling?</title>
		<author>
			<persName><forename type="first">Tomasz</forename><surname>Stanislawek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Wr?blewska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alicja</forename><surname>W?jcicka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ziembicki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Przemyslaw</forename><surname>Biecek</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K19-1058</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)</title>
		<meeting>the 23rd Conference on Computational Natural Language Learning (CoNLL)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="624" to="633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Leverage lexical knowledge for Chinese named entity recognition via collaborative graph network</title>
		<author>
			<persName><forename type="first">Dianbo</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengping</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1396</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3821" to="3831" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning sparse sharing architectures for multiple tasks</title>
		<author>
			<persName><forename type="first">Tianxiang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfan</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaonan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Xipeng Qiu, and Xuanjing Huang</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">? Ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Ralph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linguistic</forename><surname>Weischedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Consortium</forename><surname>Data</surname></persName>
		</author>
		<title level="m">Ontonotes release 5.0. Title from disc label</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Lattice-based transformer encoder for neural machine translation</title>
		<author>
			<persName><forename type="first">Fengshun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangtong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kehai</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1298</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3090" to="3097" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Tener: Adapting transformer encoder for named entity recognition</title>
		<author>
			<persName><forename type="first">Hang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bocao</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaonan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Neural reranking for named entity recognition</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Dong</surname></persName>
		</author>
		<idno>CoRR, abs/1707.05127</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Lattice transformer for speech translation</title>
		<author>
			<persName><forename type="first">Pei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niyu</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boxing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Fan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1649</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6475" to="6484" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<idno>CoRR, abs/1805.02023</idno>
		<title level="m">Chinese NER using lattice LSTM</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
